{"title": [{"text": "Large-scale Machine Translation Evaluation of the iADAATPA Project", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.8050133387247721}]}], "abstractContent": [{"text": "This paper reports the results of an in-depth evaluation of 34 state-of-the-art domain-adapted machine translation (MT) systems that were built by four leading MT companies as part of the EU-funded iADAATPA project.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8550011038780212}]}, {"text": "These systems support a wide variety of languages for several domains.", "labels": [], "entities": []}, {"text": "The evaluation combined automatic metrics and human methods, namely assessments of adequacy, fluency, and comparative ranking.", "labels": [], "entities": []}, {"text": "The paper also discusses the most effective techniques to build domain-adapted MT systems for the relevant language combinations and domains .", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9239562749862671}]}], "introductionContent": [{"text": "The evaluation reported in this paper was conducted as part of the EU-funded iADAATPA (intelligent, Automatic Domain-Adapted Automated Translation for Public Administrations) project that ended in February 2019.", "labels": [], "entities": [{"text": "Domain-Adapted Automated Translation for Public Administrations)", "start_pos": 110, "end_pos": 174, "type": "TASK", "confidence": 0.7895785910742623}]}, {"text": "The evaluation was performed by the ADAPT Centre at Dublin City University (DCU) on 34 state-of-theart domain-adapted machine translation (MT) systems built by four leading MT companies KantanMT, Pangeanic, Prompsit and Tilde.", "labels": [], "entities": [{"text": "Dublin City University (DCU)", "start_pos": 52, "end_pos": 80, "type": "DATASET", "confidence": 0.9264907240867615}, {"text": "machine translation (MT)", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.8478256464004517}, {"text": "Prompsit", "start_pos": 207, "end_pos": 215, "type": "DATASET", "confidence": 0.7691829204559326}, {"text": "Tilde", "start_pos": 220, "end_pos": 225, "type": "DATASET", "confidence": 0.7600281834602356}]}, {"text": "These MT engines supported a wide range of language pairs, including under-resourced ones, for several domains.", "labels": [], "entities": [{"text": "MT", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.9573466181755066}]}, {"text": "The main objective of the project was to lower language barriers with a view to promoting truly multilingual services across EU Member States.", "labels": [], "entities": []}, {"text": "To this end, an innovative platform (MTHub) was developed to offer state-of-the-art domain-adapted MT engines to public administrations (PAs) in addition to the EU's own eTranslation service.", "labels": [], "entities": [{"text": "MT engines", "start_pos": 99, "end_pos": 109, "type": "TASK", "confidence": 0.8954790234565735}]}, {"text": "In this context, the technical partners of the project built MT engines for the language pairs and in the specific domains that were indicated as priorities by the PAs in the respective countries.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9835719466209412}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "The four MT companies involved in this study are presented in Sections 2 (KantanMT), 3 (Pangeanic), 4 (Prompsit), and 5 (Tilde), with a description of the systems that they developed, including the data that they used and how they customized their engines.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9466248750686646}, {"text": "Tilde", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.8025268912315369}]}, {"text": "Section 6 outlines the protocol that was followed for this large-scale automatic and human evaluation.", "labels": [], "entities": []}, {"text": "Section 7 reports the key results of the evaluation, and Section 8 concludes with a summary of the most important lessons learned and possibilities for further work in this area.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to space constraints, here we present average scores of the MT engines' AEM results grouped by partner (Engine) against average scores of GNMT (Baseline), pointing out particularly interesting aspects.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9616386890411377}, {"text": "AEM", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.7916571497917175}, {"text": "GNMT", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.8628447651863098}]}, {"text": "KantanMT's MT systems) score higher than GNMT in the majority of the cases, with the exception of the English-Italian system which does not outperform the GNMT system.", "labels": [], "entities": []}, {"text": "Pangeanic's MT systems score higher than GNMT in almost all cases.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9178059697151184}, {"text": "GNMT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.7409055233001709}]}, {"text": "The SpanishEnglish MT system is the only one that does not outperform the baseline by a statistically significant margin, possibly as a result of the Spanish PA's content being overly generic and thus competing on a general basis against GNMT as opposed to a customized engine.", "labels": [], "entities": [{"text": "SpanishEnglish MT system", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8359706203142802}]}, {"text": "Prompsit's MT systems) score higher than GNMT for the majority of the cases.", "labels": [], "entities": [{"text": "Prompsit", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9392049908638}, {"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.765040934085846}, {"text": "GNMT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.7738157510757446}]}, {"text": "Tilde's MT systems score higher than GNMT inmost cases, with Latvian-English and English-Latvian being the only the engines that do not outperform the baseline (by a statistical significant amount for Latvian\u2194English).", "labels": [], "entities": [{"text": "Tilde", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9151971936225891}, {"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9280361533164978}, {"text": "GNMT", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.7250633239746094}]}], "tableCaptions": [{"text": " Table 3: Data used to train Prompsit NMT systems", "labels": [], "entities": [{"text": "Prompsit NMT", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.7902669906616211}]}]}