{"title": [{"text": "A Simple Approach to Classify Fictional and Non-Fictional Genres", "labels": [], "entities": [{"text": "Classify Fictional and Non-Fictional Genres", "start_pos": 21, "end_pos": 64, "type": "TASK", "confidence": 0.7509702920913697}]}], "abstractContent": [{"text": "In this work, we deploy a logistic regression classifier to ascertain whether a given document belongs to the fiction or non-fiction genre.", "labels": [], "entities": []}, {"text": "For genre identification, previous work had proposed three classes of features, viz., low-level (character-level and token counts), high-level (lexical and syntactic information) and derived features (type-token ratio, average word length or average sentence length).", "labels": [], "entities": [{"text": "genre identification", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8803133070468903}]}, {"text": "Using the Recursive feature elimination with cross-validation (RFECV) algorithm, we perform feature selection experiments on an exhaustive set of nineteen features (belonging to all the classes mentioned above) extracted from Brown corpus text.", "labels": [], "entities": [{"text": "Recursive feature elimination with cross-validation (RFECV)", "start_pos": 10, "end_pos": 69, "type": "TASK", "confidence": 0.7082506492733955}, {"text": "Brown corpus text", "start_pos": 226, "end_pos": 243, "type": "DATASET", "confidence": 0.8687072396278381}]}, {"text": "As a result, two simple features viz., the ratio of the number of adverbs to adjectives and the number of adjectives to pronouns turnout to be the most significant.", "labels": [], "entities": []}, {"text": "Subsequently, our classification experiments aimed towards genre identification of documents from the Brown and Baby BNC corpora demonstrate that the performance of a classifier containing just the two aforemen-tioned features is at par with that of a classifier containing the exhaustive feature set.", "labels": [], "entities": [{"text": "genre identification", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.6981774121522903}, {"text": "Brown and Baby BNC corpora", "start_pos": 102, "end_pos": 128, "type": "DATASET", "confidence": 0.9177571892738342}]}], "introductionContent": [{"text": "Texts written in any human language can be classified in various ways, one of them being fiction and non-fiction genres.", "labels": [], "entities": []}, {"text": "These categories/genres can either refer to the actual content of the write-up or the writing style used, and in this paper, we use the latter meaning.", "labels": [], "entities": []}, {"text": "We associate fiction writings with literary perspectives, i.e., an imaginative form of writing which has its own purpose of communication, whereas non-fiction writings are written in a matter-of-fact manner, but the contents mayor may not refer to real life incidents).", "labels": [], "entities": []}, {"text": "The distinction between imaginative and informative prose is very important and can have several practical applications.", "labels": [], "entities": []}, {"text": "For example, one could use a software to identify news articles, which are expected to be written in a matter-of-fact manner, but tend to use an imaginative writing style to unfairly influence the reader.", "labels": [], "entities": []}, {"text": "Another application for such a software could be for publishing houses which can use it to automatically filter out article/novel submissions that do not meet certain expected aspects of fiction writing style.", "labels": [], "entities": []}, {"text": "The standard approach in solving such text classification problems is to identify a large enough set of relevant features and feed it into a machine learning algorithm.", "labels": [], "entities": [{"text": "text classification", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7149210721254349}]}, {"text": "In the genre identification literature, three types of linguistic features have been discussed i.e., high-level, lowlevel and derived features.", "labels": [], "entities": [{"text": "genre identification", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8336755931377411}]}, {"text": "High-level features include lexical and syntactic information whereas low-level features involve character-level and various types of token count information.", "labels": [], "entities": []}, {"text": "The lexical features deal with word frequency statistics such as frequency of content words, function words or specific counts of each pronoun, etc.", "labels": [], "entities": []}, {"text": "Similarly, the syntactic features incorporate statistics of parts of speech, i.e., noun, verb, adjectives, adverbs and grammatical functions such as active and passives voices or affective markers such as modal auxiliary verbs.", "labels": [], "entities": []}, {"text": "The character-level features involve punctuation usage, word count, word length, sentence length.", "labels": [], "entities": []}, {"text": "And, lastly, the derived features involve ratio metrics such as type-token ratio, average word length or average sentence length based information.", "labels": [], "entities": []}, {"text": "Majorly, all the previous work involved a combination of different features to represent a particular nature of the document and developing a model that classify different genres, sentiments or opinions.", "labels": [], "entities": []}, {"text": "Notably, researchers have adopted the frequentist approach and used lexical richness as a prominent cue for genre classification).", "labels": [], "entities": [{"text": "genre classification", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.742083340883255}]}, {"text": "These studies vouch that coming outwith statistical distribution from word frequencies would be the defacto-arbiter for document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.7477224469184875}]}, {"text": "In this regard, Stamatatos and colleagues have shown that most frequent words in the training corpus as well as in the entire English language are one of the good features for detecting the genre type).", "labels": [], "entities": []}, {"text": "With respect to syntactic and semantics properties of the text, previous studies have used various parts of speech counts in terms of number of types and tokens (.", "labels": [], "entities": []}, {"text": "Researchers have tried to investigate the efficacy of counts vs. ratio features and their impact on the classification model performance.", "labels": [], "entities": []}, {"text": "In general, a large number of features often tend to overfit the machine learning model performance.", "labels": [], "entities": []}, {"text": "Hence, concerning the derived ratio features, argues in his genre identification study that ratio features tend to eliminate over-fitting as well as high computational cost during training.", "labels": [], "entities": [{"text": "genre identification", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8461717963218689}]}, {"text": "Although these earlier approaches have made very good progress in text classification, and are very powerful from an algorithmic perspective, they do not provide many insights into the linguistic and cognitive aspects of these fiction and nonfiction genres.", "labels": [], "entities": [{"text": "text classification", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8143053650856018}]}, {"text": "The main objective of our work is to be able to extract the features that are most relevant to this particular classification problem and can help us in understanding the underlying linguistic properties of these genres.", "labels": [], "entities": []}, {"text": "We begin by extracting nineteen linguistically motivated features belonging to various types (described at the outset) from the Brown corpus and then perform feature selection experiments using Recursive feature elimination with cross-validation (RFECV) algorithm ().", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.7688276171684265}, {"text": "Recursive feature elimination", "start_pos": 194, "end_pos": 223, "type": "TASK", "confidence": 0.6357304155826569}]}, {"text": "Interestingly, we find that a classifier containing just two simple ratio features viz., the ratio of the number of adverbs to adjectives and number of adjectives to pronouns perform as well as a classifier containing an exhaustive set of features from prior work described above [96.31% and 100% classification accuracy for Brown and British National corpus (BNC, respectively].", "labels": [], "entities": [{"text": "accuracy", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.927937388420105}, {"text": "Brown", "start_pos": 325, "end_pos": 330, "type": "DATASET", "confidence": 0.9633891582489014}, {"text": "British National corpus (BNC", "start_pos": 335, "end_pos": 363, "type": "DATASET", "confidence": 0.9182469129562378}]}, {"text": "This is perhaps the best accuracy reported in the literature so far to the best of our knowledge.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9993597865104675}]}, {"text": "Essentially, we find that texts from the fiction genre tend to have a higher ratio of adverb to adjectives, and texts from the non-fiction genre tend to have a higher ratio of adjectives to pronouns.", "labels": [], "entities": []}, {"text": "We discuss the implications of this finding for style guides for non-fiction writing) as well as standard advice proffered to creative writers.", "labels": [], "entities": []}, {"text": "In Section 2, we share details about our linguistic features design, data set and experimental methodology.", "labels": [], "entities": []}, {"text": "Section 3 presents the experiments conducted as apart of our study and discusses their critical findings.", "labels": [], "entities": []}, {"text": "Finally, Section 4 summarizes the conclusions of the study and discusses the implications of our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our experiments aimed to classify texts into the fictional and non-fictional genres using machine learning.", "labels": [], "entities": []}, {"text": "The next subsection describes various linguistic features we deploy in detail and the use of feature selection to identify the most useful features.", "labels": [], "entities": []}, {"text": "Subsequently, Section 3.2 provides the results of our classification experiments.", "labels": [], "entities": []}, {"text": "As described in the previous section, we apply logistic regression to individual files of two data-sets (Brown Corpus and Baby British National Corpus) after extracting various low-level features and features encoding ratios of POS tags based on automatic tags emitted by the Stanford tagger (see).", "labels": [], "entities": [{"text": "Brown Corpus and Baby British National Corpus", "start_pos": 105, "end_pos": 150, "type": "DATASET", "confidence": 0.8477330463273185}]}, {"text": "We use a logistic regression classifier with ten-fold cross-validation and L1 regularization for training to carryout our analyses and report the accuracy achieved over the total number of files in our test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9991410970687866}]}, {"text": "We use the Scikit-learn 2 (Pedregosa et al., 2011) library for our classification experiments.", "labels": [], "entities": [{"text": "Scikit-learn 2 (Pedregosa et al., 2011) library", "start_pos": 11, "end_pos": 58, "type": "DATASET", "confidence": 0.5703999012708664}]}, {"text": "The individual performance by nonsignificant features has not been reported in our study.", "labels": [], "entities": []}, {"text": "We report results for three data sets after tagging them using the Stanford POS-tagger:  standard deviation for the same as well as for the most frequent baseline accuracy.", "labels": [], "entities": [{"text": "Stanford POS-tagger", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.8073362410068512}, {"text": "standard", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9659853577613831}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.7279767990112305}]}, {"text": "While for the third dataset, only one training and testing set possible exists, and therefore, we report the testing accuracy and the most frequent class baseline accuracy accordingly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9540867209434509}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.6199600100517273}]}, {"text": "The most frequent class baseline is the percentage accuracy obtained if a model labels all the data points as the most frequent class in the data (non-fiction in our study).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9752140641212463}]}, {"text": "Here, we also use another performance metric known as accuracy gain which is considered more rigorous and interpretable measure as compared to the standard measure of accuracy.", "labels": [], "entities": [{"text": "accuracy gain", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.9928697347640991}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9950297474861145}]}, {"text": "The accuracy gain percentage is calculated as: Accuracy Gain % = (acc \u2212 baseline) (100 \u2212 baseline) \u00d7100 (3) where 'acc' is the reported mean accuracy of model, whereas 'baseline' is the mean of most frequent class baseline.", "labels": [], "entities": [{"text": "accuracy gain", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9813146591186523}, {"text": "Accuracy Gain", "start_pos": 47, "end_pos": 60, "type": "METRIC", "confidence": 0.8142437934875488}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.694462239742279}]}, {"text": "We begin with the Brown Corpus and take 117 sample texts of non-fiction and 207 of fiction categories.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.987169086933136}]}, {"text": "Our training set consists of 60% of the total sample size whereas testing set comprises of remaining 40% of samples.", "labels": [], "entities": []}, {"text": "We have four combinations of the set of features (refer Row 1 of).", "labels": [], "entities": []}, {"text": "It can be noted that two features model performed better than the model corresponding to the six features and low-level ratio features and is performing as good as 19 features model.", "labels": [], "entities": []}, {"text": "To make the model more robust, we follow the same approach for the combination of Brown corpus and Baby BNC with 147 sample texts of non-fiction and 232 sample texts of fiction categories.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9772467613220215}, {"text": "Baby BNC", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.904701292514801}]}, {"text": "Baby BNC has been included to check the impact of British English on the performance of the model.", "labels": [], "entities": [{"text": "Baby BNC", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7296491265296936}]}, {"text": "One may observe that the model performed even better when exposed to Baby BNC.", "labels": [], "entities": [{"text": "Baby BNC", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9006344079971313}]}, {"text": "Similar observations can be made about the accuracy of the two features model (refer Row 2 of).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9995026588439941}]}, {"text": "In our final experiment, we use the Brown corpus for training and the Baby BNC for testing with the available set of features.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9413390755653381}, {"text": "Baby BNC", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.8235685229301453}]}, {"text": "In this case, the features obtained after feature selection on the exhaustive set of features results in 100% classification accuracy (Row 3 of).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9799162149429321}]}, {"text": "This result also signifies the universal applicability of the ratio features and high-level POS ratios are not something which is affected by bias due to the language variety (i.e., British vs. American English).", "labels": [], "entities": []}, {"text": "However, the low performance of the 19 features model (53% classification accuracy) shows how they are prone to overfitting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8361878395080566}]}, {"text": "The two most significant features, adverb/adjective ratio and adjective/pronoun ratio have regression coefficients 2.73 and -2.90 respectively.", "labels": [], "entities": []}, {"text": "Thus, fiction documents tend to have higher values for the ratio of number adverbs to adjectives and a lower value for the ratio of the number of adjectives to pronouns.", "labels": [], "entities": []}, {"text": "It is worth noting that the high accuracy scores of more than 95% we obtained by using 19 features in the case of the first two datasets are in the vicinity of the accuracy score given by only these two features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9987576007843018}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9988547563552856}]}, {"text": "Also, the fact that the F1 scores are close to the accuracy values signifies the fact that the results obtained are robust in nature.", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9994764924049377}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9994719624519348}]}, {"text": "Finally, in order to check the dominant tendencies in the behaviour of classifiers containing different feature sets, we examine the predictions of various classifiers using a separate test set consisting of 97 news documents in the Baby BNC corpus.", "labels": [], "entities": [{"text": "Baby BNC corpus", "start_pos": 233, "end_pos": 248, "type": "DATASET", "confidence": 0.9567513465881348}]}, {"text": "We also studied model predictions using different training sets.", "labels": [], "entities": [{"text": "model predictions", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.6915687024593353}]}, {"text": "Initially, we use the same data sets mentioned in the last two rows of Ta-: Percentage of documents classified as nonfiction in a test set of 97 Baby BNC news documents ble 4.", "labels": [], "entities": [{"text": "Baby BNC news documents ble 4", "start_pos": 145, "end_pos": 174, "type": "DATASET", "confidence": 0.8647163410981497}]}, {"text": "Apart from this, to check the bias of the model, we create anew test set after removing the news category from the non-fiction class of brown corpus.", "labels": [], "entities": []}, {"text": "Similarly, in the combined Brown+Baby BNC corpus, we later include news samples from Baby BNC to measure the improvement in the model's predictions.", "labels": [], "entities": [{"text": "Brown+Baby BNC corpus", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.854421067237854}, {"text": "Baby BNC", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9248598217964172}]}, {"text": "The results are shown in Table 5.", "labels": [], "entities": []}, {"text": "It can be observed that most of the samples are classified as non-fiction, as expected.", "labels": [], "entities": []}, {"text": "Also, removing news articles from the Brown corpus nonfiction category does not impact the results indicating the unbiased behavior of the model.", "labels": [], "entities": [{"text": "Brown corpus nonfiction category", "start_pos": 38, "end_pos": 70, "type": "DATASET", "confidence": 0.9263273328542709}]}, {"text": "However, an important conclusion one can draw from results is that both the two features as well as the six features model are pretty stable as compared to their 19-feature counterpart.", "labels": [], "entities": []}, {"text": "Even the introduction of news samples from Baby BNC in the training data does not seem to help the predictions of 19 features model.", "labels": [], "entities": [{"text": "Baby BNC", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8927741646766663}]}, {"text": "This shows the vulnerability of more complex models to a slight change in the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Brown corpus subgenre details", "labels": [], "entities": [{"text": "Brown corpus subgenre", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.9196688930193583}]}, {"text": " Table 4: Classification accuracy for Brown Corpus and Baby BNC with different feature sets (most frequent class  i.e., non-fiction baseline results reported).", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8727417588233948}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9726549386978149}, {"text": "Brown Corpus and Baby BNC", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8375030279159545}]}, {"text": " Table 5: Percentage of documents classified as non- fiction in a test set of 97 Baby BNC news documents", "labels": [], "entities": [{"text": "BNC news documents", "start_pos": 86, "end_pos": 104, "type": "DATASET", "confidence": 0.7481805483500162}]}]}