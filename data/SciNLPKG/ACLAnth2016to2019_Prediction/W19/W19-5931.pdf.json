{"title": [{"text": "Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU", "labels": [], "entities": [{"text": "HERMIT NLU", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.7486967444419861}]}], "abstractContent": [{"text": "We present anew neural architecture for wide-coverage Natural Language Understanding in Spoken Dialogue Systems.", "labels": [], "entities": [{"text": "wide-coverage Natural Language Understanding", "start_pos": 40, "end_pos": 84, "type": "TASK", "confidence": 0.582452304661274}]}, {"text": "We develop a hierarchical multi-task architecture, which delivers a multi-layer representation of sentence meaning (i.e., Dialogue Acts and Frame-like structures).", "labels": [], "entities": []}, {"text": "The architecture is a hierarchy of self-attention mechanisms and BiLSTM en-coders followed by CRF tagging layers.", "labels": [], "entities": [{"text": "CRF tagging", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.7535803020000458}]}, {"text": "We describe a variety of experiments, showing that our approach obtains promising results on a dataset annotated with Dialogue Acts and Frame Semantics.", "labels": [], "entities": []}, {"text": "Moreover, we demonstrate its applicability to a different, publicly available NLU dataset annotated with domain-specific intents and corresponding semantic roles, providing overall performance higher than state-of-the-art tools such as RASA, Di-alogflow, LUIS, and Watson.", "labels": [], "entities": []}, {"text": "For example, we show an average 4.45% improvement in entity tagging F-score over Rasa, Dialogflow and LUIS.", "labels": [], "entities": [{"text": "entity tagging", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7405809462070465}, {"text": "F-score", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.8134211301803589}]}], "introductionContent": [{"text": "Research in Conversational AI (also known as Spoken Dialogue Systems) has applications ranging from home devices to robotics, and has a growing presence in industry.", "labels": [], "entities": []}, {"text": "A key problem in real-world Dialogue Systems is Natural Language Understanding (NLU) -the process of extracting structured representations of meaning from user utterances.", "labels": [], "entities": [{"text": "Natural Language Understanding (NLU)", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.7957719266414642}, {"text": "extracting structured representations of meaning from user utterances", "start_pos": 101, "end_pos": 170, "type": "TASK", "confidence": 0.4431132636964321}]}, {"text": "In fact, the effective extraction of semantics is an essential feature, being the entry point of any Natural Language interaction system.", "labels": [], "entities": []}, {"text": "Apart from challenges given by the inherent complexity and ambiguity of human language, other challenges arise whenever the NLU has to operate over multiple domains.", "labels": [], "entities": []}, {"text": "In fact, interaction patterns, domain, and language vary depending on the device the user is interacting with.", "labels": [], "entities": []}, {"text": "For example, chit-chatting and instruction-giving for executing an action are different processes in terms of language, domain, syntax and interaction schemes involved.", "labels": [], "entities": []}, {"text": "And what if the user combines two interaction domains: \"play some music, but first what's the weather tomorrow\"?", "labels": [], "entities": []}, {"text": "In this work, we present HERMIT, a HiERarchical MultI-Task Natural Language Understanding architecture , designed for effective semantic parsing of domain-independent user utterances, extracting meaning representations in terms of high-level intents and frame-like semantic structures.", "labels": [], "entities": [{"text": "HiERarchical MultI-Task Natural Language Understanding", "start_pos": 35, "end_pos": 89, "type": "TASK", "confidence": 0.5702512860298157}, {"text": "semantic parsing of domain-independent user utterances", "start_pos": 128, "end_pos": 182, "type": "TASK", "confidence": 0.8601686060428619}]}, {"text": "With respect to previous approaches to NLU for SDS, HERMIT stands out for being a cross-domain, multi-task architecture, capable of recognising multiple intents/frames in an utterance.", "labels": [], "entities": [{"text": "SDS", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9409797787666321}, {"text": "HERMIT", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.8374398350715637}]}, {"text": "HERMIT also shows better performance with respect to current state-of-the-art commercial systems.", "labels": [], "entities": [{"text": "HERMIT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.585904061794281}]}, {"text": "Such a novel combination of requirements is discussed below.", "labels": [], "entities": []}, {"text": "Cross-domain NLU A cross-domain dialogue agent must be able to handle heterogeneous types of conversation, such as chit-chatting, giving directions, entertaining, and triggering domain/task actions.", "labels": [], "entities": []}, {"text": "A domain-independent and rich meaning representation is thus required to properly capture the intent of the user.", "labels": [], "entities": []}, {"text": "Meaning is modelled here through three layers of knowledge: dialogue acts, frames, and frame arguments.", "labels": [], "entities": []}, {"text": "Frames and arguments can be in turn mapped to domain-dependent intents and slots, or to Frame Semantics') structures (i.e. semantic frames and frame elements, respectively), which allow handling of heterogeneous domains and language.", "labels": [], "entities": []}, {"text": "Multi-task NLU Deriving such a multi-layered meaning representation can be approached through a multi-task learning approach.", "labels": [], "entities": []}, {"text": "Multitask learning has found success in several NLP problems, especially with the recent rise of Deep Learning.", "labels": [], "entities": []}, {"text": "Thanks to the possibility of building complex networks, handling more tasks at once has been proven to be a successful solution, provided that some degree of dependence holds between the tasks.", "labels": [], "entities": []}, {"text": "Moreover, multi-task learning allows the use of different datasets to train subparts of the network (.", "labels": [], "entities": []}, {"text": "Following the same trend, HERMIT is a hierarchical multitask neural architecture which is able to deal with the three tasks of tagging dialogue acts, frame-like structures, and their arguments in parallel.", "labels": [], "entities": [{"text": "HERMIT", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.6810286641120911}]}, {"text": "The network, based on self-attention mechanisms, seq2seq bi-directional Long-Short Term Memory (BiLSTM) encoders, and CRF tagging layers, is hierarchical in the sense that information output from earlier layers flows through the network, feeding following layers to solve downstream dependent tasks.", "labels": [], "entities": [{"text": "CRF tagging", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.6525446623563766}]}, {"text": "Multi-dialogue act and -intent NLU Another degree of complexity in NLU is represented by the granularity of knowledge that can be extracted from an utterance.", "labels": [], "entities": []}, {"text": "Utterance semantics is often rich and expressive: approximating meaning to a single user intent is often not enough to convey the required information.", "labels": [], "entities": [{"text": "Utterance semantics", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.918115109205246}]}, {"text": "As opposed to the traditional single-dialogue act and single-intent view in previous work, HERMIT operates on a meaning representation that is multi-dialogue act and multi-intent.", "labels": [], "entities": []}, {"text": "In fact, it is possible to model an utterance's meaning through multiple dialogue acts and intents at the same time.", "labels": [], "entities": []}, {"text": "For example, the user would be able both to request tomorrow's weather and listen to his/her favourite music with just a single utterance.", "labels": [], "entities": []}, {"text": "A further requirement is that for practical application the system should be competitive with stateof-the-art: we evaluate HERMIT's effectiveness by running several empirical investigations.", "labels": [], "entities": []}, {"text": "We perform a robust test on a publicly available NLUBenchmark (NLU-BM) () containing 25K cross-domain utterances with a conversational agent.", "labels": [], "entities": []}, {"text": "The results obtained show a performance higher than well-known off-the-shelf tools (i.e., Rasa, DialogueFlow, LUIS, and Watson).", "labels": [], "entities": []}, {"text": "The contribution of the different network components is then highlighted through an ablation study.", "labels": [], "entities": []}, {"text": "We also test HERMIT on the smaller Robotics-Oriented MUltitask Language UnderStanding (ROMULUS) corpus, annotated with Dialogue Acts and Frame Semantics.", "labels": [], "entities": [{"text": "HERMIT", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.6211947202682495}, {"text": "MUltitask Language UnderStanding (ROMULUS) corpus", "start_pos": 53, "end_pos": 102, "type": "DATASET", "confidence": 0.5180733757359641}]}, {"text": "HERMIT produces promising results for the application in areal scenario.", "labels": [], "entities": [{"text": "HERMIT", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7672227025032043}]}], "datasetContent": [{"text": "In order to assess the effectiveness of the proposed architecture and compare against existing off-theshelf tools, we run several empirical evaluations.", "labels": [], "entities": []}, {"text": "We tested the system on two datasets, different in size and complexity of the addressed language.", "labels": [], "entities": []}, {"text": "The first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains 25, 716 utterances annotated with targeted Scenario, Action, and involved Entities.", "labels": [], "entities": [{"text": "NLU-Benchmark (NLU-BM)", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.8608649522066116}]}, {"text": "For example, \"schedule a call with Lisa on Monday morning\" is labelled to contain a calendar scenario, where the set event action is instantiated through the entities [event name: a call with Lisa] and [date: Monday morning].", "labels": [], "entities": []}, {"text": "The Intent is then obtained by concatenating scenario and action labels (e.g., calendar set event).", "labels": [], "entities": [{"text": "Intent", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9830307364463806}]}, {"text": "This dataset consists of multiple home assistant task domains (e.g., scheduling, playing music), chit-chat, and commands to a robot (  The second dataset, RO-MULUS, is composed of 1, 431 sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided.", "labels": [], "entities": [{"text": "RO-MULUS", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.8464872241020203}]}, {"text": "This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns -e.g., chit-chat, command interpretation.", "labels": [], "entities": [{"text": "command interpretation", "start_pos": 210, "end_pos": 232, "type": "TASK", "confidence": 0.7565689384937286}]}, {"text": "The corpus is composed of different subsections, addressing heterogeneous linguistic phenomena, ranging from imperative instructions (e.g., \"enter the bedroom slowly, turn left and turn the lights off \") to complex requests for information (e.g., \"good morning I want to buy anew mobile phone is there any shop nearby?\") or open-domain chit-chat (e.g., \"nope thanks let's talk about cinema\").", "labels": [], "entities": []}, {"text": "A considerable number of utterances in the dataset is collected through Human-Human Interaction studies in robotic domain (\u224870%), though a small portion has been synthetically generated for balancing the frame distribution.", "labels": [], "entities": []}, {"text": "Note that while the NLU-BM is designed to have at most one intent per utterance, sentences are here tagged following the IOB2 sequence labelling scheme (see example of, that show an average number of dialogue acts, frames and frame elements always greater than 1 (i.e., 1.33, 1.41 and 3.54, respectively).", "labels": [], "entities": []}, {"text": "All the models are implemented with and as backend, and run on a Titan Xp.", "labels": [], "entities": [{"text": "Titan Xp", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9262093901634216}]}, {"text": "Experiments are performed in a 10-fold setting, using one fold for tuning and one for testing.", "labels": [], "entities": []}, {"text": "However, since HERMIT is designed to operate on dialogue acts, semantic frames and frame elements, the best hyperparameters are obtained over the ROMULUS dataset via a grid search using early stopping, and are applied also to the NLU-BM models.", "labels": [], "entities": [{"text": "ROMULUS dataset", "start_pos": 146, "end_pos": 161, "type": "DATASET", "confidence": 0.8052606582641602}]}, {"text": "This guarantees fairness towards other systems, that do not perform any fine-tuning on the training data.", "labels": [], "entities": []}, {"text": "We make use of pre-trained 1024-dim ELMo embeddings ( as word vector representations without re-training the weights.", "labels": [], "entities": []}, {"text": "This section shows the results obtained on the NLU-Benchmark (NLU-BM) dataset provided by (, by comparing HERMIT to off-the-shelf NLU services, namely: Rasa 4 , Dialogflow 5 , LUIS 6 and Watson 7 . In order to apply HERMIT to NLU-BM annotations, these have been aligned so that Scenarios are treated as DAs, Actions as FRs and Entities as ARs.", "labels": [], "entities": [{"text": "NLU-Benchmark (NLU-BM) dataset", "start_pos": 47, "end_pos": 77, "type": "DATASET", "confidence": 0.7548731207847595}, {"text": "FRs", "start_pos": 319, "end_pos": 322, "type": "METRIC", "confidence": 0.9488236904144287}]}, {"text": "To make our model comparable against other approaches, we reproduced the same folds as in (, where a resized version of the original dataset is used.", "labels": [], "entities": []}, {"text": "shows some statistics of the NLU-BM and its reduced version.", "labels": [], "entities": [{"text": "NLU-BM", "start_pos": 29, "end_pos": 35, "type": "DATASET", "confidence": 0.9155430197715759}]}, {"text": "Moreover, micro-averaged Precision, Recall and F1 are computed following the original paper to assure consistency.", "labels": [], "entities": [{"text": "Precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9322497844696045}, {"text": "Recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9976481795310974}, {"text": "F1", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.998435914516449}, {"text": "consistency", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9884506464004517}]}, {"text": "TP, FP and FN of intent labels are obtained as in any other multi-class task.", "labels": [], "entities": [{"text": "TP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9569854140281677}, {"text": "FP", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9658265113830566}, {"text": "FN", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9498739838600159}]}, {"text": "An entity is instead counted as TP if there is an overlap between the predicted and the gold span, and their labels match.", "labels": [], "entities": [{"text": "TP", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.8588312268257141}]}, {"text": "Experimental results are reported in Following (, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices  In this section we report the experiments performed on the ROMULUS dataset).", "labels": [], "entities": [{"text": "ROMULUS dataset", "start_pos": 229, "end_pos": 244, "type": "DATASET", "confidence": 0.8386133313179016}]}, {"text": "Together with the evaluation metrics used in (, we report the span F1, computed using the CoNLL-2000 shared task evaluation script, and the Exact Match (EM) accuracy of the entire sequence of labels.", "labels": [], "entities": [{"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.6839355826377869}, {"text": "CoNLL-2000 shared task evaluation script", "start_pos": 90, "end_pos": 130, "type": "DATASET", "confidence": 0.7943442106246948}, {"text": "Exact Match (EM) accuracy", "start_pos": 140, "end_pos": 165, "type": "METRIC", "confidence": 0.9069512089093527}]}, {"text": "It is worth noticing that the EM Combined score is computed as the conjunction of the three individual predictions -e.g., a match is when all the three sequences are correct.", "labels": [], "entities": [{"text": "EM Combined score", "start_pos": 30, "end_pos": 47, "type": "METRIC", "confidence": 0.7356555064519247}]}, {"text": "Results in terms of EM reflect the complexity of the different tasks, motivating their position within the hierarchy.", "labels": [], "entities": []}, {"text": "Specifically, dialogue act identification is the easiest task (89.31%) with respect to frame (82.60%) and frame element (79.73%), due to the shallow semantics it aims to catch.", "labels": [], "entities": [{"text": "dialogue act identification", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.8481329480806986}]}, {"text": "However, when looking at the span F1, its score (89.42%) is lower than the frame element identification task (92.26%).", "labels": [], "entities": [{"text": "frame element identification", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.6294580300649008}]}, {"text": "What happens is that even though the label set is smaller, dialogue act spans are supposed to be longer than frame element ones, sometimes covering the whole sentence.", "labels": [], "entities": []}, {"text": "Frame elements, instead, are often one or two tokens long, that contribute in increasing span based metrics.", "labels": [], "entities": []}, {"text": "Frame identification is the most complex task for several reasons.", "labels": [], "entities": [{"text": "Frame identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.971360594034195}]}, {"text": "First, lots of frame spans are interlaced or even nested; this contributes to increasing the network entropy.", "labels": [], "entities": []}, {"text": "Second, while the dialogue act label is highly related to syntactic structures, frame identification is often subject to the inherent ambiguity of language (e.g., get can evoke both Commerce buy and Arriving).", "labels": [], "entities": [{"text": "frame identification", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7384844422340393}]}, {"text": "We also report the metrics in () for consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9925518035888672}]}, {"text": "For dialogue act and frame tasks, scores provide just the extent to which the network is able to detect those labels.", "labels": [], "entities": []}, {"text": "In fact, the metrics do not consider any span information, essential to solve and evaluate our tasks.", "labels": [], "entities": []}, {"text": "However, the frame element scores are comparable to the benchmark, since the task is very similar.", "labels": [], "entities": []}, {"text": "Overall, getting back to the combined EM accuracy, HERMIT seems to be promising, with the network being able to reproduce all the three gold sequences for almost 70% of the cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.7084689736366272}, {"text": "HERMIT", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9925341606140137}]}, {"text": "The importance of this result provides an idea of the architecture behaviour over the entire pipeline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the NLU-Benchmark dataset (Liu  et al., 2019).", "labels": [], "entities": [{"text": "NLU-Benchmark dataset", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.9781118631362915}]}, {"text": " Table 2: Statistics of the ROMULUS dataset.", "labels": [], "entities": [{"text": "ROMULUS dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8676004707813263}]}, {"text": " Table 3: Comparison of HERMIT with the results obtained in (Liu et al., 2019) for Intents and Entity Types.", "labels": [], "entities": [{"text": "HERMIT", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9963449835777283}, {"text": "Intents and Entity Types", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6400066837668419}]}, {"text": " Table 4: Comparison of HERMIT with the results  in (Liu et al., 2019) by combining Intent and Entity.", "labels": [], "entities": [{"text": "HERMIT", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9874441027641296}]}, {"text": " Table 5: Ablation study of HERMIT on the NLU-BM.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.93877774477005}, {"text": "HERMIT", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9474008083343506}, {"text": "NLU-BM", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.9852650761604309}]}, {"text": " Table 6: HERMIT performance over the ROMULUS dataset. P,R and F1 are evaluated following (Liu et al., 2019)  metrics", "labels": [], "entities": [{"text": "HERMIT", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9969582557678223}, {"text": "ROMULUS dataset", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.7529842257499695}, {"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9979524612426758}]}]}