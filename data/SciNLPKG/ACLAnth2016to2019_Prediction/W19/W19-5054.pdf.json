{"title": [{"text": "UW-BHI at MEDIQA 2019: An Analysis of Representation Methods for Medical Natural Language Inference", "labels": [], "entities": [{"text": "UW-BHI at MEDIQA 2019", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.7677688300609589}, {"text": "Medical Natural Language Inference", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.5774351358413696}]}], "abstractContent": [{"text": "Recent advances in distributed language mod-eling have led to large performance increases on a variety of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "However, it is not well understood how these methods maybe augmented by knowledge-based approaches.", "labels": [], "entities": []}, {"text": "This paper compares the performance and internal representation of an Enhanced Sequential Inference Model (ESIM) between three experimental conditions based on the representation method: Bidirectional Encoder Representations from Transformers (BERT), Embeddings of Semantic Predications (ESP), or Cui2Vec.", "labels": [], "entities": []}, {"text": "The methods were evaluated on the Medical Natural Language Inference (MedNLI) sub-task of the MEDIQA 2019 shared task.", "labels": [], "entities": [{"text": "Medical Natural Language Inference (MedNLI)", "start_pos": 34, "end_pos": 77, "type": "TASK", "confidence": 0.6431770239557538}, {"text": "MEDIQA 2019 shared task", "start_pos": 94, "end_pos": 117, "type": "DATASET", "confidence": 0.7689966410398483}]}, {"text": "This task relied heavily on semantic understanding and thus served as a suitable evaluation set for the comparison of these representation methods .", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes our approach to the Natural Language Inference (NLI) subtask of the MEDIQA 2019 shared task.", "labels": [], "entities": [{"text": "MEDIQA 2019 shared task", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.51692995429039}]}, {"text": "As it is not yet clear the extent to which knowledge-based embeddings may provide task-specific improvement over recent advances in contextual embeddings, we provide an analysis of the differences in performance between these two methods.", "labels": [], "entities": []}, {"text": "Additionally, it is not yet clear from the literature the extent to which information stored in contextual embeddings overlaps with that in knowledge-based embeddings for which we provide a preliminary analysis of the attention weights of models that use these two representation methods as input.", "labels": [], "entities": []}, {"text": "We compare BERT fine-tuned to MIMIC-III (  and PubMed to Embeddings of Semantic Predications (ESP) trained on SemMedDB and a baseline that uses Cui2Vec embeddings trained on clinical and biomedical text.", "labels": [], "entities": [{"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9920352101325989}, {"text": "MIMIC-III", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.6459572315216064}]}, {"text": "Two recent advances in the unsupervised modeling of natural language, Embeddings of Language Models (ELMo) (  and Bidirectional Encoder Representations from Transformers (BERT), have led to drastic improvements across a variety of shared tasks.", "labels": [], "entities": []}, {"text": "Both of these methods use transfer learning, a method whereby a multi-layered language model is first trained on a large unlabeled corpus.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8933231830596924}]}, {"text": "The weights of the model are then frozen and used as input to a task specific model (.", "labels": [], "entities": []}, {"text": "This method is particularly well-suited for work in the medical domain where datasets tend to be relatively small due to the high cost of expert annotation.", "labels": [], "entities": []}, {"text": "However, whereas clinical free-text is difficult to access and share in bulk due to privacy concerns, the biomedical domain is characterized by a significant amount of manually-curated structured knowledge bases.", "labels": [], "entities": []}, {"text": "The BioPortal repository currently hosts 773 different biomedical ontologies comprised of over 9.4 million classes.", "labels": [], "entities": [{"text": "BioPortal repository", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8609302937984467}]}, {"text": "SemMedDB is a triple store that consists of over 94 million predications extracted from PubMed by SemRep, a semantic parser for biomedical text ().", "labels": [], "entities": []}, {"text": "These available resources make a strong case for the evaluation of knowledge-based methods for the Medical Natural Language Inference (MedNLI) task).", "labels": [], "entities": [{"text": "Medical Natural Language Inference (MedNLI) task", "start_pos": 99, "end_pos": 147, "type": "TASK", "confidence": 0.7116777747869492}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Results from chi-squared (with Yates' continuity correction) test of correct(+) and incorrect(-) predictions  by embedding and hypothesis focus type.", "labels": [], "entities": [{"text": "Yates' continuity correction) test", "start_pos": 41, "end_pos": 75, "type": "METRIC", "confidence": 0.7771524548530578}]}]}