{"title": [{"text": "The Fast and the Flexible: Training Neural Networks to Learn to Follow Instructions from Small Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Learning to follow human instructions is a long-pursued goal in artificial intelligence.", "labels": [], "entities": []}, {"text": "The task becomes particularly challenging if no prior knowledge of the employed language is assumed while relying only on a handful of examples to learn from.", "labels": [], "entities": []}, {"text": "Work in the past has relied on hand-coded components or manually engineered features to provide strong inductive biases that make learning in such situations possible.", "labels": [], "entities": []}, {"text": "In contrast, here we seek to establish whether this knowledge can be acquired automatically by a neural network system through a two phase training procedure: A (slow) offline learning stage where the network learns about the general structure of the task and a (fast) online adaptation phase where the network learns the language of anew given speaker.", "labels": [], "entities": []}, {"text": "Controlled experiments show that when the network is exposed to familiar instructions but containing novel words, the model adapts very efficiently to the new vocabulary.", "labels": [], "entities": []}, {"text": "Moreover, even for human speakers whose language usage can depart significantly from our artificial training language, our network can still make use of its automatically acquired inductive bias to learn to follow instructions more effectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to follow instructions from human speakers is a long-pursued goal in artificial intelligence, tracing back at least to Terry Winograd's work on SHRDLU.", "labels": [], "entities": []}, {"text": "This system was capable of interpreting and following natural language instructions about a world composed of geometric figures.", "labels": [], "entities": [{"text": "interpreting and following natural language", "start_pos": 27, "end_pos": 70, "type": "TASK", "confidence": 0.7300792932510376}]}, {"text": "While this first system relied on a set of hand-coded rules to process natural language, most of recent work aimed at using machine learning to map linguistic utterances into their semantic interpretations (.", "labels": [], "entities": []}, {"text": "Predominantly, they assumed that users speak all in the same natural language, and thus the systems could be trained offline once and for all.", "labels": [], "entities": []}, {"text": "However, recently departed from this assumption by proposing SHRDLURN, a coloured-blocks manipulation language game.", "labels": [], "entities": [{"text": "SHRDLURN", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.7281430959701538}]}, {"text": "There, users could issue instructions in any arbitrary language to a system that must incrementally learn to interpret it (see for an example).", "labels": [], "entities": []}, {"text": "This learning problem is particularly challenging because human users typically provide only a handful of examples for the system to learn from.", "labels": [], "entities": []}, {"text": "Therefore, the learning algorithms must incorporate strong inductive biases in order to learn effectively.", "labels": [], "entities": []}, {"text": "That is, they need to complement the scarce input with priors that would help the model make the right inferences even in the absence of positive data.", "labels": [], "entities": []}, {"text": "A way of giving the models a powerful inductive bias is by hand-coding features or operations that are specific to the given domain where the instructions must be interpreted.", "labels": [], "entities": []}, {"text": "For example, propose a log-linear semantic parser which crucially relies on a set of hand-coded functional primitives.", "labels": [], "entities": []}, {"text": "While effective, this strategy severely curtails the portability of a system: For every new domain, human technical expertise is required to adapt the system.", "labels": [], "entities": []}, {"text": "Instead, we would like these inductive biases to be learned automatically without human intervention.", "labels": [], "entities": []}, {"text": "That is, humans should be free from the burden of: Illustration of the SHRDLURN task of thinking what are useful primitives fora given domain, but still obtain systems that can learn fast from little data.", "labels": [], "entities": [{"text": "SHRDLURN task", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.6126560270786285}]}, {"text": "In this paper, we introduce a neural network system that learns domain-specific priors directly from data.", "labels": [], "entities": []}, {"text": "This system can then be used to quickly learn the language of new users online.", "labels": [], "entities": []}, {"text": "It uses a two phase regime: First, the network is trained offline on easy-to-produce artificial data to learn the mechanics of a given task.", "labels": [], "entities": []}, {"text": "Next, the network is deployed to real human users who will train it online with just a handful of examples.", "labels": [], "entities": []}, {"text": "While this implies that some of the manual effort needed to design useful primitive functions would go in developing the artificial data, we envision that in many real-world situations it could be easier to provide examples of expected interactions than thinking of what could be useful primitives involved in them.", "labels": [], "entities": []}, {"text": "On controlled experiments we show that our system can recover the meaning of sentences where some words where scrambled, even though it does not display evidence of compositional learning.", "labels": [], "entities": []}, {"text": "On the other hand, we show that the offline training phase allows it to learn faster from limited data, compared to a neural network system that did not go through this pre-training phase.", "labels": [], "entities": []}, {"text": "We hypothesize that this system learns useful inductive biases, such as the types of operations that are likely to be requested.", "labels": [], "entities": []}, {"text": "In this direction, we show that the performance of our best-performing system correlates with that of, where these operations were encoded by hand.", "labels": [], "entities": []}, {"text": "The work in this paper is organized as follows: We first start by creating a large artificially generated dataset to train the systems in the offline phase.", "labels": [], "entities": []}, {"text": "We then experiment with different neural network architectures to find which general learning system adapts best for this task.", "labels": [], "entities": []}, {"text": "Then, we propose how to adapt this network by training it online and confirm its effectiveness on recovering the meaning of scrambled words and on learning to process the language from human users, using the dataset introduced by.", "labels": [], "entities": [{"text": "recovering the meaning of scrambled words", "start_pos": 98, "end_pos": 139, "type": "TASK", "confidence": 0.7537689507007599}]}], "datasetContent": [{"text": "We seek to establish whether we can train a neural network system to learn the rules and structure of a task while communicating with a scripted teacher and then having it adapt to the particular nuances of each human user.", "labels": [], "entities": []}, {"text": "We tackled this question incrementally.", "labels": [], "entities": []}, {"text": "First, we explored what is the best architectural choice for solving the SHRDLURN task on our large artificially-constructed dataset.", "labels": [], "entities": [{"text": "SHRDLURN task", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.6822375059127808}]}, {"text": "Next, we ran multiple controlled experiments to investigate the adaptation skills of our online learning system.", "labels": [], "entities": []}, {"text": "In particular, we first tested whether the model was able to recover the original meaning of a word that had been replaced with anew arbitrary symbol -e.g. \"red\" becomes \"roze\"-on an online training regime.", "labels": [], "entities": []}, {"text": "Finally, we proceeded to learn from real human utterances using the dataset collected by.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model's accuracies (in percentages) evaluated on block configurations and utterances that were  completely unseen during offline training. Results expressed in percentages.", "labels": [], "entities": []}, {"text": " Table 2: Online accuracies (in percentages) for the word recovery task averaged over 7 sessions for  1 word, 17 for 2 words, 10 for 3 words and a single interaction for the all words condition, having 45  instructions each. \"Transfer\" stands for the components whose weights were saved (and not reinitialized)  from the offline training. \"Adapt\" stands for the components whose weights get updated during the online  training.", "labels": [], "entities": [{"text": "word recovery task", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8629656632741293}]}, {"text": " Table 3: For each (valid) combination of set of weights to re-use and weights to adapt online, we report  average online accuracy on", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9887009859085083}]}]}