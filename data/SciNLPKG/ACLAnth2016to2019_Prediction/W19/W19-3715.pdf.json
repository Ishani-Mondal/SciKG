{"title": [{"text": "Building English-to-Serbian Machine Translation System for IMDb Movie Reviews", "labels": [], "entities": [{"text": "English-to-Serbian Machine Translation", "start_pos": 9, "end_pos": 47, "type": "TASK", "confidence": 0.6237788001696268}, {"text": "IMDb Movie Reviews", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.851167619228363}]}], "abstractContent": [{"text": "This paper reports the results of the first experiment dealing with the challenges of building a machine translation system for user-generated content involving a complex South Slavic language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.764344722032547}]}, {"text": "We focus on translation of English IMDb user movie reviews into Ser-bian, in a low-resource scenario.", "labels": [], "entities": [{"text": "translation of English IMDb user movie reviews", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.8881469709532601}, {"text": "Ser-bian", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.8798503875732422}]}, {"text": "We explore potentials and limits of (i) phrase-based and neural machine translation systems trained on out-of-domain clean parallel data from news articles (ii) creating additional synthetic in-domain parallel corpus by machine-translating the English IMDb corpus into Serbian.", "labels": [], "entities": []}, {"text": "Our main findings are that morphology and syntax are better handled by the neural approach than by the phrase-based approach even in this low-resource mismatched domain scenario, however the situation is different for the lexical aspect , especially for person names.", "labels": [], "entities": []}, {"text": "This finding also indicates that in general, machine translation of person names into Slavic languages (es-pecially those which require/allow transcription) should be investigated more systematically .", "labels": [], "entities": [{"text": "machine translation of person names into Slavic languages", "start_pos": 45, "end_pos": 102, "type": "TASK", "confidence": 0.8423506580293179}]}], "introductionContent": [{"text": "Social media platforms have become hugely popular web-sites where Internet users can communicate and spread information worldwide.", "labels": [], "entities": []}, {"text": "Social media texts, such as user reviews and micro-blogs, are often short, informal, and noisy in terms of linguistic norms.", "labels": [], "entities": []}, {"text": "Usually, this noise does not pose problems for human understanding, but it can be challenging for NLP applications such as sentiment analysis or machine translation (MT).", "labels": [], "entities": [{"text": "human understanding", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6688684523105621}, {"text": "sentiment analysis", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.9637220501899719}, {"text": "machine translation (MT)", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.8263980507850647}]}, {"text": "Additional challenge for MT is sparseness of bilingual (translated) user-generated texts, especially for neural machine translation (NMT).", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9955241680145264}, {"text": "neural machine translation (NMT)", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.8381912509600321}]}, {"text": "The NMT approach has emerged in recent years and already replaced statistical phrase-based (PBMT) approach as state-of-the-art.", "labels": [], "entities": []}, {"text": "However, NMT is even more sensitive to the low-resource settings and domain mismatch (.", "labels": [], "entities": []}, {"text": "Therefore, the challenge of translating user-generated texts is threefold, and if the target language is complex, then fourfold.", "labels": [], "entities": [{"text": "translating user-generated texts", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8522983392079672}]}, {"text": "In this work, we focus on neural machine translation of English IMDb movie reviews into Serbian, a morpho-syntactically complex South Slavic language.", "labels": [], "entities": [{"text": "neural machine translation of English IMDb movie reviews", "start_pos": 26, "end_pos": 82, "type": "TASK", "confidence": 0.8243527486920357}]}, {"text": "To the best of our knowledge, this is the first experiment dealing with machine translation of user-generated content involving a South Slavic language.", "labels": [], "entities": [{"text": "machine translation of user-generated content involving a South Slavic language", "start_pos": 72, "end_pos": 151, "type": "TASK", "confidence": 0.7804324269294739}]}, {"text": "The main questions of our research described in this work are (i) What performance can be expected of an English-toSerbian machine translation system trained on news articles and applied to movie reviews?", "labels": [], "entities": [{"text": "English-toSerbian machine translation", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.6586340069770813}]}, {"text": "(ii) Can this performance be improved by translating the monolingual English movie reviews into Serbian thus creating additional synthetic in-domain bilingual data?", "labels": [], "entities": []}, {"text": "(iii) What are the main issues and what are the most important directions for the next experiments?", "labels": [], "entities": []}, {"text": "In order to answer these questions, we build a neural (NMT) machine system on the publicly available clean out-of-domain news corpus, and a phrase-based (PBMT) system trained on the same data in order to compare the two approaches in this specific scenario.", "labels": [], "entities": []}, {"text": "After that, we use these two systems to generate synthetic Serbian movie reviews thus creating additional in-domain bilingual data.", "labels": [], "entities": []}, {"text": "We then compare five different set-ups in terms of corpus statistics, overall automatic scores, and error analysis.", "labels": [], "entities": [{"text": "automatic scores", "start_pos": 78, "end_pos": 94, "type": "METRIC", "confidence": 0.8969916105270386}]}, {"text": "All our experiments were carried out on publicly available data sets.", "labels": [], "entities": []}, {"text": "In order to encourage further research on the topic, all Serbian human translations of IMDb reviews produced for purposes of this research are made publicly available, too 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiment, we have built one PBMT English-to-Serbian system using Moses toolkit ( and four English-toSerbian NMT models using OpenNMT () in the following way: \u2022 Train an out-of-domain PBMT system on the SEtimes corpus.", "labels": [], "entities": [{"text": "SEtimes corpus", "start_pos": 212, "end_pos": 226, "type": "DATASET", "confidence": 0.9590858817100525}]}, {"text": "\u2022 Train a baseline out-of-domain NMT system on the SEtimes corpus.", "labels": [], "entities": [{"text": "SEtimes corpus", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8705907166004181}]}, {"text": "\u2022 Translate the English IMDb training corpus into Serbian using the PBMT system, thus generating a synthetic parallel corpus IMDb pbmt . \u2022 Translate the English IMDb training corpus into Serbian using the baseline NMT system, thus generating a synthetic parallel corpus IMDb nmt . \u2022 Train anew NMT system on the SEtimes corpus enriched with the IMDb pbmt corpus.", "labels": [], "entities": [{"text": "SEtimes corpus", "start_pos": 312, "end_pos": 326, "type": "DATASET", "confidence": 0.9327364563941956}, {"text": "IMDb pbmt corpus", "start_pos": 345, "end_pos": 361, "type": "DATASET", "confidence": 0.7774914304415385}]}, {"text": "\u2022 Train another NMT system using SEtimes corpus enriched with the IMDb nmt corpus.", "labels": [], "entities": [{"text": "SEtimes corpus", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.8278084695339203}, {"text": "IMDb nmt corpus", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.947261393070221}]}, {"text": "\u2022 Train one more NMT system using SEtimes corpus enriched with both IMDb pbmt and IMDb nmt corpora (IMDb joint ).", "labels": [], "entities": [{"text": "SEtimes corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.7964872419834137}]}, {"text": "shows the statistics for each of the three training corpora (SEtimes, IMDb pbmt and IMDb nmt ), for the development set, as well as for the test set.", "labels": [], "entities": [{"text": "IMDb nmt", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.6019029915332794}]}, {"text": "First, it can be noticed that the IMDb training corpus contains more than twice segments and running words than the English part of the SEtimes corpus, and it has a much larger vocabulary.", "labels": [], "entities": [{"text": "IMDb training corpus", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8693264126777649}, {"text": "SEtimes corpus", "start_pos": 136, "end_pos": 150, "type": "DATASET", "confidence": 0.7849827110767365}]}, {"text": "Another fact is that, due to the rich morphology, the Serbian SEtimes vocabulary is almost twice as large as the English one.", "labels": [], "entities": [{"text": "Serbian SEtimes vocabulary", "start_pos": 54, "end_pos": 80, "type": "DATASET", "confidence": 0.8423735896746317}]}, {"text": "Nevertheless, this is not the case for the synthetic IMDb data, where the Serbian vocabulary is only barely larger or even comparable to the English one.", "labels": [], "entities": [{"text": "IMDb data", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.7761345207691193}]}, {"text": "This confirms the intuition about sub-optimal forward translation mentioned in the previous section -machine translated data generally exhibit less lexical and syntactic variety than natural data (Burlot and 2018), and here we are additionally dealing with a scarce out-of-domain MT system translating into a more complex language.", "labels": [], "entities": [{"text": "forward translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7558172941207886}, {"text": "MT", "start_pos": 280, "end_pos": 282, "type": "TASK", "confidence": 0.9690192937850952}]}, {"text": "For the development set, as intuitively expected, out-of-vocabulary rates are smaller for the indomain SEtimes corpus, and for the less morphologically complex English language.", "labels": [], "entities": []}, {"text": "As for the test set, the English part behaves in the same way, namely the OOV rates are smaller when compared to the in-domain IMDb training corpus.", "labels": [], "entities": [{"text": "OOV", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9928043484687805}, {"text": "IMDb training corpus", "start_pos": 127, "end_pos": 147, "type": "DATASET", "confidence": 0.7319402694702148}]}, {"text": "However, for the synthetic Serbian data, the OOV rates are comparable with those of the out-of-domain development corpus and much higher than for development corpus when compared to its in-domain the SEtimes corpus, which again illustrates the effects of sub-optimal synthetic data.", "labels": [], "entities": [{"text": "OOV", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9961661100387573}, {"text": "SEtimes corpus", "start_pos": 200, "end_pos": 214, "type": "DATASET", "confidence": 0.8330020606517792}]}, {"text": "We first evaluated all translation outputs using the following overall automatic MT evaluation metrics: BLEU (), METEOR (Lavie and Denkowski, 2009), TER (), chrF (Popovi\u00b4cPopovi\u00b4c, 2015) and characTER (.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.901911199092865}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9987610578536987}, {"text": "METEOR", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9933468103408813}, {"text": "TER", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9898221492767334}]}, {"text": "BLEU, ME-TEOR and TER are word-level metrics whereas chrF and characTER are character-based metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9891370534896851}, {"text": "ME-TEOR", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.9242802858352661}, {"text": "TER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9928682446479797}]}, {"text": "BLEU, METEOR and chrF are based on precision and/or recall, whereas TER and characTER are based on edit distance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9862960577011108}, {"text": "METEOR", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9909172654151917}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9991714954376221}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9989728927612305}, {"text": "TER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9958802461624146}]}, {"text": "The results both for the development as well as for the test set can be seen in.", "labels": [], "entities": []}, {"text": "The results for the development set are as it could intuitively be expected: the best option is to use a NMT system trained on the in-domain data (baseline), and using any kind of additional outof-domain data deteriorates all scores.", "labels": [], "entities": []}, {"text": "As for the test set, it could be expected that the scores will be worse than for the development set.", "labels": [], "entities": []}, {"text": "However, several interesting tendencies can be observed.", "labels": [], "entities": []}, {"text": "First of all, the baseline NMT system outperforms the baseline PBMT system despite the scarcity of the training corpus and domain mismatch (, however only in terms of word-level scores -both characterlevel scores are better for the PBMT system.", "labels": [], "entities": []}, {"text": "Furthermore, adding IMDb pbmt data deteriorates all word-level scores and improves both characterlevel scores.", "labels": [], "entities": [{"text": "IMDb pbmt data", "start_pos": 20, "end_pos": 34, "type": "DATASET", "confidence": 0.7917351921399435}]}, {"text": "On the other hand, adding IMDb nmt data improves all baseline scores, but the improvements of the character-based scores are smaller than those yielded by adding the IMDb pbmt corpus.", "labels": [], "entities": [{"text": "IMDb nmt data", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.6869554320971171}, {"text": "IMDb pbmt corpus", "start_pos": 166, "end_pos": 182, "type": "DATASET", "confidence": 0.8359880248705546}]}, {"text": "Finally, using all synthetic data IMDb joint improves all scores (except BLEU) over the baseline, however the improvements are smaller than the improvements of each individual synthetic data sets (IMDb nmt for word-level scores and IMDb pbmt for character-level scores).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9989107847213745}]}], "tableCaptions": [{"text": " Table 2: Overall word-level and character-level automatic evaluation scores for the development (SEtimes) and the  test (IMDb) corpus.", "labels": [], "entities": []}, {"text": " Table 3: Results of automatic error analysis including three error categories for the development (SEtimes) and  test (IMDb) corpus.", "labels": [], "entities": [{"text": "automatic error analysis", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.5694407125314077}]}]}