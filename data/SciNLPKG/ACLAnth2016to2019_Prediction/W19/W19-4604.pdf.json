{"title": [{"text": "Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects", "labels": [], "entities": [{"text": "Sentiment Analysis of Arabic Dialects", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.9287083864212036}]}], "abstractContent": [{"text": "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content.", "labels": [], "entities": [{"text": "Arabic sentiment analysis", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7037041187286377}]}, {"text": "These embeddings are usually composed via ordered , syntax-aware composition functions and learned within deep neural frameworks.", "labels": [], "entities": []}, {"text": "With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9569231271743774}]}, {"text": "Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9538397192955017}]}, {"text": "The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model.", "labels": [], "entities": []}, {"text": "Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.9458739757537842}]}, {"text": "The obtained results revealed that, our syntax-ignorant embeddings could outper-form word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.", "labels": [], "entities": []}], "introductionContent": [{"text": "According to the used features, existing Arabic Sentiment Analysis (ASA) systems can be classified into: (a) hand-crafted-based systems () where linguistic/stylistic and lexical features are generated by morphological analyzers and semantic resources and (b) text embeddings-based systems that adopt word/sentence embeddings using one of the composition models (.", "labels": [], "entities": [{"text": "Arabic Sentiment Analysis (ASA)", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.7870416243871053}]}, {"text": "While the first type of ASA systems provide a comparable performance, the generation of hand-crafted features is considered a laborintensive task that requires using language/dialectspecific NLP tools and techniques.", "labels": [], "entities": [{"text": "ASA", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9740894436836243}]}, {"text": "In contrast, text embeddings-based systems can use the raw unprocessed input content to generate expressive features to represent words or even longer pieces of text through using the composition models (.", "labels": [], "entities": []}, {"text": "Composition models aim to construct a phrase/sentence embeddings based on its constituent word embeddings and structural information.", "labels": [], "entities": []}, {"text": "Two main types of these models can be recognized: (a) Ordered models where the order and linguistic/grammatical structure of the input words do count while constructing the phrase/sentence vector and (b) Unordered models in which the word representations are combined irrespective of their order using algebraic operations (Sum of Word Embeddings (SOWE), average (Avg), mean and multiplication functions).", "labels": [], "entities": [{"text": "Sum of Word Embeddings (SOWE", "start_pos": 324, "end_pos": 352, "type": "TASK", "confidence": 0.8183630506197611}, {"text": "average (Avg)", "start_pos": 355, "end_pos": 368, "type": "METRIC", "confidence": 0.6827411949634552}]}, {"text": "Context words alongside their syntactic properties have been considered essential to build effective word embeddings able to infer the semantic/syntactic similarities among words, phrases or sentences.", "labels": [], "entities": []}, {"text": "Consequently, most of the recentlydeveloped SA systems adopted deep neural network architectures such as Convolutional Neural Networks (CNNs) and Recursive Neural Networks (RecNNs) where ordered composition models are employed to grasp the syntactic and linguistic relations between the words (Al).", "labels": [], "entities": []}, {"text": "These systems required more training time to learn words' order-aware embeddings due to the high computational complexity consumed at each layer of the model.", "labels": [], "entities": []}, {"text": "However, such embeddings resulting from ordered compositionality might not form discriminating features for the Arabic dialects; especially that these dialects have a free word order and varying syntactic/grammatical rules).", "labels": [], "entities": []}, {"text": "For instance, the dialectal (Levantine) sentence in-  vestigated in meaning \"I liked this idea\" can be represented by several word orders: VSO, SVO, OSV and OVS and yet, implies the same meaning and sentiment.", "labels": [], "entities": []}, {"text": "On the other hand, the Arabic dialects show phonological, morphological, lexical, and syntactic differences such that the same word might infer different syntactic information across different dialects.", "labels": [], "entities": []}, {"text": "To clarify that, reviews how the word \"\" has several Part Of Speech (POS) tags, multiple meanings and different sentiments across three Arabic dialects.", "labels": [], "entities": []}, {"text": "Thus, to handle such informality of DA, we propose an unordered composition model to construct sentence/phrase embeddings regardless of the order and the syntax of the context's words.", "labels": [], "entities": []}, {"text": "Nevertheless, when coming to the sentiment analysis task, sentence embeddings that are merely composed and learned based on the context words do not always infer the sentiment accurately.", "labels": [], "entities": [{"text": "sentiment analysis task", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8987398147583008}]}, {"text": "This is due to the fact that, some words of contradict sentiments might be mentioned within identical contexts which leads to map opposite words close to each other in the embedding space.", "labels": [], "entities": []}, {"text": "To clarify that, both sentences in Example 1 and Example 2 contain the same context words organized in the same order; yet the first sentence is of positive polarity while the second has a negative sentiment since the words \"\" and \"\" are antonyms that mean \"interesting\" and \"boring\", respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the model evaluation, Tw-StAR was employed to predict the sentiment in five publicly available datasets (See).", "labels": [], "entities": [{"text": "Tw-StAR", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.8985322117805481}]}, {"text": "Four of them were written in Eastern (Jordanian) and Western (Tunisian, Moroccan) Arabic dialects, while the fifth combined Eastern, Western and Gulf Arabic dialects.", "labels": [], "entities": []}, {"text": "They are as follows: \u2022 Arabic Twitter Dataset (ArTwitter): combines 2,000 positive/negative tweets mostly written in the Jordanian dialect ().", "labels": [], "entities": [{"text": "Arabic Twitter Dataset (ArTwitter)", "start_pos": 23, "end_pos": 57, "type": "DATASET", "confidence": 0.8130004902680715}]}, {"text": "\u2022 Tunisian Election Corpus (TEC): refers to 3,043 tweets positive/negative combining MSA and Tunisian dialect where Tunisian tweets form the majority of the data ().", "labels": [], "entities": [{"text": "Tunisian Election Corpus (TEC)", "start_pos": 2, "end_pos": 32, "type": "DATASET", "confidence": 0.8032902081807455}]}, {"text": "\u2022: F-measure values (%) obtained with dev sets for different window sizes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9984765648841858}]}, {"text": "\u2022 Moroccan Election Corpus (MEC): combines 10,253 positive/negative Facebook comments ().", "labels": [], "entities": [{"text": "Moroccan Election Corpus (MEC)", "start_pos": 2, "end_pos": 32, "type": "DATASET", "confidence": 0.8915693660577139}]}], "tableCaptions": [{"text": " Table 3: The statistics of the used datasets.", "labels": [], "entities": []}, {"text": " Table 4: F-measure values (%) obtained with dev  sets for different window sizes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.996023416519165}]}, {"text": " Table 5: Tw-StAR performances against baseline systems and word2vec/doc2vec for all datasets.  (*,**,***) refers to a significant difference at P-value<0.05, <0.01, <0.001, respectively, compared  to Tw-StAR.", "labels": [], "entities": []}]}