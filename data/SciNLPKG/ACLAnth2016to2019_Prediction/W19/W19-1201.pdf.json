{"title": [{"text": "The First Shared Task on Discourse Representation Structure Parsing", "labels": [], "entities": [{"text": "Discourse Representation Structure Parsing", "start_pos": 25, "end_pos": 67, "type": "TASK", "confidence": 0.7531777024269104}]}], "abstractContent": [{"text": "The paper presents the IWCS 2019 shared task on semantic parsing where the goal is to produce Discourse Representation Structures (DRSs) for English sentences.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.6183803826570511}]}, {"text": "DRSs originate from Discourse Representation Theory and represent scoped meaning representations that capture the semantics of negation, modals, quantification, and presupposition triggers.", "labels": [], "entities": []}, {"text": "Additionally, concepts and event-participants in DRSs are described with WordNet synsets and the thematic roles from VerbNet.", "labels": [], "entities": [{"text": "WordNet synsets", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9565969705581665}, {"text": "VerbNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9623029828071594}]}, {"text": "To measure similarity between two DRSs, they are represented in a clausal form, i.e. as a set of tuples.", "labels": [], "entities": []}, {"text": "Participant systems were expected to produce DRSs in this clausal form.", "labels": [], "entities": []}, {"text": "Taking into account the rich lexical information, explicit scope marking, a high number of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task.", "labels": [], "entities": [{"text": "scope marking", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.6491718143224716}, {"text": "DRS parsing", "start_pos": 188, "end_pos": 199, "type": "TASK", "confidence": 0.8583920001983643}]}, {"text": "The results of the shared task displayed improvements over the existing state-of-the-art parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing has been gaining in popularity in the last few years.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8516879379749298}]}, {"text": "There have been a series of shared tasks in semantic parsing organized, where each task requires to generate meaning representations of specific types: Broad-Coverage Broad-coverage Semantic Dependencies (, Abstract Meaning Representation, or Universal Conceptual Cognitive Annotation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7292242497205734}]}, {"text": "The Discourse Representation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference.", "labels": [], "entities": [{"text": "Discourse Representation Structure (DRS) parsing", "start_pos": 4, "end_pos": 52, "type": "TASK", "confidence": 0.8077737178121295}]}, {"text": "DRSs are meaning representations employed by Discourse Representation Theory (DRT,).", "labels": [], "entities": [{"text": "Discourse Representation Theory (DRT", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.8300380349159241}]}, {"text": "They have been successfully applied for wide-coverage semantic representations (, Natural Language Inference (, and Natural Language Generation (.", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.598428746064504}, {"text": "Natural Language Generation", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.6564127703507742}]}, {"text": "To the best of our knowledge, there has never been a shared task on scoped meaning representations.", "labels": [], "entities": [{"text": "scoped meaning representations", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.6332436203956604}]}, {"text": "There are only a few previous approaches to DRS parsing.", "labels": [], "entities": [{"text": "DRS parsing", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.8815191984176636}]}, {"text": "Traditionally, due to the complexity of the task, it has been the domain of symbolic and statistical approaches.", "labels": [], "entities": []}, {"text": "Recently, however, neural sequence-to-sequence systems achieved impressive performance on the task (, without relying on any external linguistic resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official evaluation set contains 600 instances that were not released previously.", "labels": [], "entities": []}, {"text": "They will not be released publicly, but are still available for (blind) scoring via the shared task website.", "labels": [], "entities": []}, {"text": "However, during the evaluation phase, we asked the participants to provide DRSs fora set of 12,606 short texts.", "labels": [], "entities": []}, {"text": "In addition to the raw texts (600) from the evaluation split, this set contained the train (4,597), development (682), and test (650) data from the PMB-2.2.0 release and the sentences (6,077) from the SICK dataset ().", "labels": [], "entities": [{"text": "PMB-2.2.0 release", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.9358500838279724}, {"text": "SICK dataset", "start_pos": 201, "end_pos": 213, "type": "DATASET", "confidence": 0.9091604351997375}]}, {"text": "The reason for providing the inflated set of raw texts was three-fold: (i) Disguise the raw texts of the evaluation set to make it hard to tune models on them; (ii) Obtain the complete information about the performance of the systems on the provided training, development and test sets; (iii) Carry out extrinsic evaluation of the participant systems on the natural language inference task.", "labels": [], "entities": []}, {"text": "Before comparing a system produced clausal form to the gold one, the produced form is checked on validity-whether it represents a DRS.", "labels": [], "entities": [{"text": "validity-whether", "start_pos": 97, "end_pos": 113, "type": "METRIC", "confidence": 0.9923547506332397}]}, {"text": "If the clausal form is invalid, it is replaced by a single nonmatching clause.", "labels": [], "entities": []}, {"text": "In the shared task, we include three baseline systems.", "labels": [], "entities": []}, {"text": "The evaluation and validation scripts and the baselines are publicly available.", "labels": [], "entities": []}, {"text": "The evaluation defines to what degree a system output clausal form is similar to the corresponding gold one.", "labels": [], "entities": []}, {"text": "To compare the system output and gold representations, we compute the F1-score over the clauses, following.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9990271329879761}]}, {"text": "We use the tool COUNTER, which is specifically designed to evaluate DRSs.", "labels": [], "entities": [{"text": "COUNTER", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.5676364302635193}]}, {"text": "It is based on the tool that is used to evaluate AMR parsers.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.8432691693305969}]}, {"text": "It is essentially a hill-climbing algorithm that finds the best variable mapping between the produced DRS and the gold standard.", "labels": [], "entities": []}, {"text": "To avoid local optima, we restart the procedure 10 times.", "labels": [], "entities": []}, {"text": "In order to prevent an inflated F-score, before searching the maximal matching, COUNTER discards those REF-clauses which are deemed redundant.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9758408665657043}, {"text": "REF-clauses", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.6443625688552856}]}, {"text": "A REF-clause b REF x is redundant if and only if its discourse referent x occurs with a concept predicate in a basic condition of the same box b -in other words, there exists a clause of the form b concept \"pos.nn\" x.", "labels": [], "entities": []}, {"text": "An example of comparing the clausal forms of two scoped meaning representations is shown in.", "labels": [], "entities": []}, {"text": "With respect to the optimal mapping, both, the sample system output and gold clauses, include three clauses that could not be matched with each other while four clauses are matched.", "labels": [], "entities": []}, {"text": "The optimal mapping gives us a precision and recall of 3 /7, resulting in an F-score of 42.9.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9998186230659485}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9996060729026794}, {"text": "F-score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9996299743652344}]}, {"text": "Similarly to AMR, we use micro-averaged F-score when evaluating a set of DRSs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9031312465667725}]}, {"text": "An aspect that is different from the AMR evaluation system is that we generalize over synonyms.", "labels": [], "entities": [{"text": "AMR evaluation", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7899328172206879}]}, {"text": "Ina preprocessing step of the evaluation, all word senses are converted to its WordNet 3.0 synset ID.", "labels": [], "entities": [{"text": "WordNet 3.0 synset ID", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.8993228077888489}]}, {"text": "For example, fox.n.02 and dodger.n.01 both get normalized to dodger.n.01 and are thus able to match.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the PMB release 2.2.0 and the shared task evaluation set.", "labels": [], "entities": [{"text": "PMB", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7928139567375183}]}, {"text": " Table 3: Official results of the shared task for the participating systems", "labels": [], "entities": []}, {"text": " Table 4: F-scores of fine-grained evaluation of the participating systems on the evaluation set. \"Winner  out of 5\" counts only those instances for which the parser obtained a higher score than all the rest, while  \"Highest out of 5\" allows ties.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9973183274269104}]}, {"text": " Table 5: F-scores of all systems compared to each other.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9972309470176697}]}]}