{"title": [{"text": "Enhancing biomedical word embeddings by retrofitting to verb clusters", "labels": [], "entities": [{"text": "Enhancing biomedical word embeddings", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8416924327611923}]}], "abstractContent": [{"text": "Verbs play a fundamental role in many biomed-ical tasks and applications such as relation and event extraction.", "labels": [], "entities": [{"text": "relation and event extraction", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.8145457059144974}]}, {"text": "We hypothesize that performance on many downstream tasks can be improved by aligning the input pretrained em-beddings according to semantic verb classes.", "labels": [], "entities": []}, {"text": "In this work, we show that by using semantic clusters for verbs, a large lexicon of verb classes derived from biomedical literature, we are able to improve the performance of common pretrained embeddings in downstream tasks by retrofitting them to verb classes.", "labels": [], "entities": []}, {"text": "We present a simple and computationally efficient approach using a widely-available \"off-the-shelf\" retrofitting algorithm to align pretrained embeddings according to semantic verb clusters.", "labels": [], "entities": []}, {"text": "We achieve state-of-the-art results on text classification and relation extraction tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8176166713237762}, {"text": "relation extraction", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7949664294719696}]}], "introductionContent": [{"text": "Core tasks in biomedical natural language processing (BioNLP) such as relation and event extraction, text classification, syntactic and semantic parsing, natural language inference, and entailment can all benefit from rich computational lexicons containing information about the behaviour and meaning of words in biomedical texts.", "labels": [], "entities": [{"text": "biomedical natural language processing (BioNLP)", "start_pos": 14, "end_pos": 61, "type": "TASK", "confidence": 0.7618961249079023}, {"text": "relation and event extraction", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.7040016055107117}, {"text": "text classification", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.795129805803299}, {"text": "syntactic and semantic parsing", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.6607125028967857}]}, {"text": "Verbs are especially important in many of these tasks; for example, describing proteinprotein interactions in biomedical text can often rely on a wide range of verbs, such as \"bind,\" \"activate,\" \"carry,\" \"facilitate,\" \"interact,\" etc.", "labels": [], "entities": []}, {"text": "in order to determine the specific type of interaction.", "labels": [], "entities": []}, {"text": "Lexical semantic classes for verbs can be used to abstract away from individual words, or to build a lexical structure (taxonomy) which predicts much of the behaviour of anew word by associating it with an appropriate class.", "labels": [], "entities": []}, {"text": "For example, the verbs \"assess,\" \"evaluate,\" \"estimate,\" \"explore,\" and \"analyze\" belong to the class examine, while the verbs \"utilize,\" \"employ,\" and \"exploit\" belong to the class use.", "labels": [], "entities": []}, {"text": "In addition to simple synonyms of verbs, semantic classes capture similarity in their use and behaviour in text by analysing their contexts.", "labels": [], "entities": []}, {"text": "In the past, lexical verb classes have been successfully shown to improve the performance classifiers in a variety of tasks and downstream applications in the biomedical domain; such as relation extraction, biomedical fact extraction (), text classification for cancer (, biomedical discourse analysis (, and biomedical information retrieval).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.8522731065750122}, {"text": "biomedical fact extraction", "start_pos": 207, "end_pos": 233, "type": "TASK", "confidence": 0.6528420150279999}, {"text": "text classification", "start_pos": 238, "end_pos": 257, "type": "TASK", "confidence": 0.7722258269786835}, {"text": "biomedical discourse analysis", "start_pos": 272, "end_pos": 301, "type": "TASK", "confidence": 0.649535338083903}, {"text": "biomedical information retrieval", "start_pos": 309, "end_pos": 341, "type": "TASK", "confidence": 0.6155012845993042}]}, {"text": "Lexical classes are useful for their ability to capture generalizations about a range of linguistic properties (); our hypothesis is therefore that by retrofitting embedded word representations to semantic verb classes, semanticallysimilar verbs (i.e. member verbs within the same lexical class) like \"suppress\" and \"inhibit\" will be pulled together in vector space, whereas verbs like \"collect\" and \"examine\" will not.", "labels": [], "entities": []}, {"text": "Consequently, this allows NLP systems to generalize away from individual verbs, alleviating the data sparseness problem of representing each verb in the corpus individually.", "labels": [], "entities": []}, {"text": "Retrofitting is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors.", "labels": [], "entities": []}, {"text": "It is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7330774664878845}]}, {"text": "It can be applied to any pretrained word embedding vectors.", "labels": [], "entities": []}, {"text": "The intuition behind retrofitting is to encourage the retrofitted vectors to be similar to the vectors of related word types and similar to their original distributional representations.", "labels": [], "entities": []}, {"text": "Using a standard \"off-the-shelf\" retrofitting algorithm, we apply the idea of retrofitting to verb clusters to two sets of widely-used pretrained embedding vectors in BioNLP (those by and by) to obtain improved embeddings.", "labels": [], "entities": [{"text": "BioNLP", "start_pos": 167, "end_pos": 173, "type": "DATASET", "confidence": 0.9263341426849365}]}, {"text": "We show that by doing nothing more than using this simple approach, we achieve state-of-the-art results on two text classification tasks (both tasks evaluated on document and sentence level classification), and a relation extraction task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7002120614051819}, {"text": "document and sentence level classification", "start_pos": 162, "end_pos": 204, "type": "TASK", "confidence": 0.6487267673015594}, {"text": "relation extraction", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.861409991979599}]}, {"text": "We make our retrofitted embeddings freely available to the BioNLP community along with our code.", "labels": [], "entities": []}, {"text": "The main contribution of this work is to be the first of its kind to apply verb-based retrofitting in the biomedical domain.", "labels": [], "entities": []}, {"text": "Retrofitting has thus far only been applied for aligning vectors to Medical Subject Headings (MeSH) (, and been validated only in an extrinsic setting.", "labels": [], "entities": [{"text": "Medical Subject Headings (MeSH)", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6646927247444788}]}, {"text": "We show that with very little effort, we can achieve state-of-the art results on various downstream tasks in a range of biomedical subdomains.", "labels": [], "entities": []}, {"text": "This paper will first describe relevant work on retrofitting to lexical resources in BioNLP; we then briefly give an overview of two verb cluster and lexicons that we use in our methodology, and then our task-based evaluation.", "labels": [], "entities": [{"text": "BioNLP", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.8795849680900574}]}, {"text": "We end with a discussion of the evaluation results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply retrofitting to incorporate the lexical information into word representations.", "labels": [], "entities": []}, {"text": "Then we evaluate the quality of the retrofitted-representation as features for two NLP tasks: text classification and relation classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8225691020488739}, {"text": "relation classification", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.8834088444709778}]}], "tableCaptions": [{"text": " Table 1: Linguistic constraint counts under each  class as obtained from the Korhonen's resource and  our automatically-created lexicon.", "labels": [], "entities": [{"text": "Korhonen's resource", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.9166176716486613}]}, {"text": " Table 2: Summary statistics of the Hallmarks of  Cancer (HOC) and the Chemical Exposure Assess- ment (EXP) datasets.", "labels": [], "entities": [{"text": "Chemical Exposure Assess- ment (EXP) datasets", "start_pos": 71, "end_pos": 116, "type": "TASK", "confidence": 0.6724537246757083}]}, {"text": " Table 3: Hyper-parameters used in (Baker and  Korhonen, 2017).", "labels": [], "entities": []}, {"text": " Table 4: Summary statistics of the Chemical- Protein interaction dataset (CHEMPROT).", "labels": [], "entities": [{"text": "Chemical- Protein interaction dataset (CHEMPROT)", "start_pos": 36, "end_pos": 84, "type": "DATASET", "confidence": 0.692169014364481}]}, {"text": " Table 5: Hyperparameters used by Bj\u00f6rne and  Salakoski (2018).", "labels": [], "entities": []}, {"text": " Table 6: Performance results for the Hallmarks of Cancer task (HOC) when different sets of lexicons are used  for retrofitting the baseline model. Baseline denotes a skip-gram model generated with our optimized training  settings. Scores are adopted from Baker and Korhonen (2017). All figures are micro-averages expressed as  percentages (Bold: the best score, *: statistically significant).", "labels": [], "entities": [{"text": "Hallmarks of Cancer task (HOC)", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6026495397090912}]}, {"text": " Table 7: Performance results for the Chemical Exposure Assessment task (EXP). Baseline denotes a skip- gram model generated with our optimized training settings. The \"No lexicon\" scores are from Baker and  Korhonen (2017). All figures are micro-averages expressed as percentages. (Bold: the best score, *: statistically  significant).", "labels": [], "entities": [{"text": "Chemical Exposure Assessment task (EXP)", "start_pos": 38, "end_pos": 77, "type": "TASK", "confidence": 0.8512939853327615}]}, {"text": " Table 8: Performance results for the Chemical- Protein Interaction (CHEMPROT) when different  sets of lexicons are used for retrofitting the baseline  model. Baseline denotes a skip-gram model gen- erated with our optimized training settings. SOTA  denotes the state-of-the-art result reported by Bj\u00f6rne  and Salakoski (2018) using the embeddings by  Pyysalo et al. (2013a). All figures are micro- averages expressed as percentages. (Bold: the best  score for the task, *: statistically significant).", "labels": [], "entities": [{"text": "Chemical- Protein Interaction (CHEMPROT)", "start_pos": 38, "end_pos": 78, "type": "TASK", "confidence": 0.7458206755774361}, {"text": "SOTA", "start_pos": 244, "end_pos": 248, "type": "METRIC", "confidence": 0.9006732106208801}]}]}