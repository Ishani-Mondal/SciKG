{"title": [{"text": "A Hierarchically-Labeled Portuguese Hate Speech Dataset", "labels": [], "entities": [{"text": "Portuguese Hate Speech Dataset", "start_pos": 25, "end_pos": 55, "type": "DATASET", "confidence": 0.6694930642843246}]}], "abstractContent": [{"text": "Over the past years, the amount of online offensive speech has been growing steadily.", "labels": [], "entities": []}, {"text": "To successfully cope with it, machine learning is applied.", "labels": [], "entities": []}, {"text": "However, ML-based techniques require sufficiently large annotated datasets.", "labels": [], "entities": [{"text": "ML-based", "start_pos": 9, "end_pos": 17, "type": "TASK", "confidence": 0.9654290080070496}]}, {"text": "In the last years, different datasets were published , mainly for English.", "labels": [], "entities": []}, {"text": "In this paper, we present anew dataset for Portuguese, which has not been in focus so far.", "labels": [], "entities": []}, {"text": "The dataset is composed of 5,668 tweets.", "labels": [], "entities": []}, {"text": "For its annotation , we defined two different schemes used by annotators with different levels of expertise.", "labels": [], "entities": []}, {"text": "First, non-experts annotated the tweets with binary labels ('hate' vs. 'no-hate').", "labels": [], "entities": []}, {"text": "Then, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception.", "labels": [], "entities": []}, {"text": "The hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections.", "labels": [], "entities": [{"text": "identification of different types of hate speech and their intersections", "start_pos": 105, "end_pos": 177, "type": "TASK", "confidence": 0.6233452588319779}]}, {"text": "To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pre-trained word embed-dings and LSTM on the binary classified data, with a state-of-the-art outcome.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.8149440884590149}]}], "introductionContent": [{"text": "The Internet is the source of an immense variety of knowledge repositories (Wikipedia, Wordnet, etc.) and applications (YouTube, Reddit, Twitter, etc.) that everybody can access and take advantage of; it is also the communication forum of our time and the most important instrument to ensure freedom of speech.", "labels": [], "entities": []}, {"text": "It allows us to freely state and disseminate our view on any private or public matter to vast audiences.", "labels": [], "entities": []}, {"text": "But unfortunately it also opens the door to manipulation of masses and defamation of specific individuals or groups of people.", "labels": [], "entities": []}, {"text": "One of these observed negative phenomena is the propagation of hate speech.", "labels": [], "entities": [{"text": "propagation of hate speech", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.8994501829147339}]}, {"text": "Hate speech leads to a negative self-image and social exclusion of the targeted individuals, groups or populations, and incites violence against them.", "labels": [], "entities": []}, {"text": "A clear example of the extreme harm that can be caused by hate speech is the 1994 Rwandan genocide; see fora detailed analysis.", "labels": [], "entities": []}, {"text": "The detection of online hate speech is thus a pressing problem that calls for solutions.", "labels": [], "entities": [{"text": "detection of online hate speech", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8544333457946778}]}, {"text": "Over the last decade, a considerable number of supervised machine learning-based works tackled the problem.", "labels": [], "entities": []}, {"text": "Most of them focused on English (, see also the overview by.", "labels": [], "entities": []}, {"text": "As a result, also many more annotated datasets, which are the precondition for the use of supervised machine learning, are available for English (e.g.,;;;) than for other languages.", "labels": [], "entities": []}, {"text": "However, hate speech is not a phenomenon that is observed only in English discourse; it is notorious in online media in other languages as well; cf., e.g.,), Italian (, or German (.", "labels": [], "entities": []}, {"text": "In this work, we aim to contribute to the field of hate speech detection.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.8624951044718424}]}, {"text": "Our contribution is twofold: (i) diversification of the research on hate speech by provision of anew dataset of hate speech in another language than English, namely Portuguese; (ii) introduction of a novel fine-grained hate speech typology that improves on the common state-of-the-art used typologies, which tend to disregard the existence of subtypes of hate speech and either consider hate speech recognition as a binary classification task, or take into account only a few classes, such as) -despite the fact that such broad distinctions unduly overgeneralize.", "labels": [], "entities": [{"text": "hate speech recognition", "start_pos": 387, "end_pos": 410, "type": "TASK", "confidence": 0.6683745483557383}]}, {"text": "For instance, by classifying discrimination against both black people and refugees simply as 'racism', we ignore that in this case, different characteristics with a different motivation are targeted (also reflected in a different language style).", "labels": [], "entities": []}, {"text": "In particular, we compile and annotate anew dataset composed of 5,668 tweets in Portuguese, which is one of the most commonly-used languages online.", "labels": [], "entities": []}, {"text": "Two types of annotations are carried out.", "labels": [], "entities": []}, {"text": "For the first, non-expert annotators classify the messages in a binary fashion ('hate' vs. 'no-hate').", "labels": [], "entities": []}, {"text": "For the second, we build a multilabel hate speech hierarchical annotation schema with 81 hate categories in total . To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pretrained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 246, "end_pos": 250, "type": "METRIC", "confidence": 0.8414360284805298}]}, {"text": "The remainder of the paper is structured as follows: Section 2 reviews the literature.", "labels": [], "entities": []}, {"text": "Section 3 describes our crawling procedure.", "labels": [], "entities": []}, {"text": "In Section 4, we present the two annotation schemas we work with: the binary and the hierarchical schema.", "labels": [], "entities": []}, {"text": "Section 5 discusses a baseline hate speech experiment that we carried out to validate our new dataset.", "labels": [], "entities": []}, {"text": "Section 6 presents some ethical considerations of this work.", "labels": [], "entities": []}, {"text": "In Section 7, finally, the conclusions of our work are presented.", "labels": [], "entities": []}, {"text": "analyze and compare several aggression-related concepts.", "labels": [], "entities": []}, {"text": "As a result of their analysis, they present the following definition of hate speech:", "labels": [], "entities": []}], "datasetContent": [{"text": "Several hate speech datasets are publicly available, e.g., for English (,, Italian), German (Ross et al., 2016), Hindi (, and Portuguese (de.", "labels": [], "entities": []}, {"text": "In this section, we analyze the data collection strategy, the annotation method and the dataset properties of three representative hate speech datasets: the Hate speech, Racism and Sexism dataset by, the Offensive Language Dataset by, and the Portuguese News Comments dataset by.", "labels": [], "entities": [{"text": "Offensive Language Dataset", "start_pos": 204, "end_pos": 230, "type": "DATASET", "confidence": 0.5636480251948038}, {"text": "Portuguese News Comments dataset", "start_pos": 243, "end_pos": 275, "type": "DATASET", "confidence": 0.9325314164161682}]}, {"text": "We have chosen the first two because they are the most widely used datasets for English hate speech automatic classification.", "labels": [], "entities": [{"text": "English hate speech automatic classification", "start_pos": 80, "end_pos": 124, "type": "TASK", "confidence": 0.7108076751232147}]}, {"text": "They show how Twitter can be used to retrieve information and how to conduct the manual classification relying on both expert and non-expert annotators.", "labels": [], "entities": [{"text": "manual classification", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.6143107265233994}]}, {"text": "The third is another annotated and published dataset for Portuguese, which is rather different from ours.", "labels": [], "entities": []}, {"text": "Hate speech, Racism and Sexism Dataset.", "labels": [], "entities": [{"text": "Hate speech", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7396374046802521}]}, {"text": "This dataset 2 (Waseem and Hovy, 2016) contains 16,914 tweets in English, which were classified by two annotators using the classes \"Racism\", \"Sexism\" and \"Neither\".", "labels": [], "entities": []}, {"text": "Regarding the tweet collection, an initial manual search was conducted on Twitter to collect common slurs and terms related to religious, sexual, gender, and ethnic minorities.", "labels": [], "entities": [{"text": "tweet collection", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7923065423965454}]}, {"text": "The authors identified frequently occurring terms in tweets that contain hate speech and used those terms to retrieve more messages.", "labels": [], "entities": []}, {"text": "The messages were then annotated by the main researcher, together with a gender studies student; in total, 3,383 tweets as sexist, 1,972 as racist, and 11,559 as neither sexist nor racist.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement had a Cohen's Kappa of 0.84.", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.7099363307158152}]}, {"text": "The authors of the study concluded that the use of n-grams provides good results in the task of automatic hate speech detection, and adding demographic information leads to little improvement in the performance of the classification model.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.734760582447052}]}, {"text": "annotated a dataset 3 with 14,510 tweets in English, using the classes \"Hate\", \"Offensive\" and \"Neither\".", "labels": [], "entities": []}, {"text": "Regarding the collection of the messages, they started with an English hate speech lexicon compiled by Hatebase.org, searching for tweets that contained terms from this lexicon.", "labels": [], "entities": []}, {"text": "The outcome was a collection of tweets written by 33,458 Twitter users.", "labels": [], "entities": []}, {"text": "The collected tweets were completed by further follow-up tweets of these users, which resulted in a corpus of 85.4 million tweets.", "labels": [], "entities": []}, {"text": "Finally, from this corpus, a random sample of 25,000 tweets containing terms from the lexicon has been extracted and manually annotated by CrowdFlower workers.", "labels": [], "entities": []}, {"text": "Three or more workers from CrowdFlower annotated each message.", "labels": [], "entities": []}, {"text": "The majority voting was used to assign a label to each tweet.", "labels": [], "entities": []}, {"text": "Tweets that did not have a majority class were discarded.", "labels": [], "entities": []}, {"text": "This resulted in a sample of 24,802 labeled tweets.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement score provided by CrowdFlower was 92%.", "labels": [], "entities": []}, {"text": "However, a total percentage of only 5% of tweets were labeled as hate speech by the majority of the workers.", "labels": [], "entities": []}, {"text": "de Pelle and Moreira (2017) collected a dataset 4 with 1,250 random comments from the Globo news site on politics and sports news.", "labels": [], "entities": [{"text": "Globo news site", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9304380218187968}]}, {"text": "Each comment was annotated by three annotators, who were asked to indicate whether it contained 'racism', 'sexism', 'homophobia', 'xenophobia', 'religious intolerance', or 'cursing'.", "labels": [], "entities": []}, {"text": "'Cursing' was the most frequent label, while the other labels had few instances in the corpus.", "labels": [], "entities": []}, {"text": "Regarding the annotator agreement, the value was 0.71.", "labels": [], "entities": []}, {"text": "In comparison to this work, the dataset that we have compiled provides more data and is not restricted to specific topics.", "labels": [], "entities": []}, {"text": "Additionally, our annotation focuses only on hate speech, instead of general offensive content.", "labels": [], "entities": []}, {"text": "We also use and provide a complete labeling schema.", "labels": [], "entities": []}, {"text": "Compared to the previous two datasets, our second annotation schema is considerably more finegrained.", "labels": [], "entities": []}, {"text": "As we will see below, our annotation procedure with the fine-grained schema is similar to that of Waseem and Hovy (2016).", "labels": [], "entities": []}, {"text": "After the annotation phase, we obtain a multilabeled dataset with 22% of hate speech instances.", "labels": [], "entities": []}, {"text": "The resulting hierarchy, the node depth (ND) and class frequencies (Freq) are presented in.", "labels": [], "entities": [{"text": "node depth (ND)", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.802107322216034}, {"text": "class frequencies (Freq)", "start_pos": 49, "end_pos": 73, "type": "METRIC", "confidence": 0.6652184784412384}]}, {"text": "As expected, the classes corresponding to nodes with a higher depth tend to have a smaller frequency.", "labels": [], "entities": []}, {"text": "Note that our schema also identifies categories that are less commonly mentioned in hate speech classification experiments, among them, e.g., 'fat people', 'fat women', 'ugly people', 'ugly women', 'men', 'feminists', 'people with left-wing ideology'.", "labels": [], "entities": [{"text": "hate speech classification", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.6638548970222473}]}, {"text": "Some of them (such as, e.g., 'men') may look neutral at the first glance, but, in reality, they group messages whose vocabulary and language style reflect negative expectations towards the corresponding collective (in the case of men those expectations reflect toxic masculinity norms  In order to obtain a first indicator of the usefulness of our dataset, we carryout a preliminary binary classification experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Annotator agreement by class, with the num- ber of messages annotated by each annotator.", "labels": [], "entities": []}, {"text": " Table 3: Hate subclasses (Class) and respective parent categories (Parent nodes) sorted by frequency (Freq). Infor- mation of the node depth is also provided (ND).", "labels": [], "entities": [{"text": "Freq", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9495326280593872}]}]}