{"title": [{"text": "May I Check Again? -A simple but efficient way to generate and use contextual dictionaries for Named Entity Recognition. Application to French Legal Texts", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8170079390207926}, {"text": "French Legal Texts", "start_pos": 136, "end_pos": 154, "type": "DATASET", "confidence": 0.8781603574752808}]}], "abstractContent": [{"text": "In this paper we present anew method to learn a model robust to typos fora Named Entity Recognition task.", "labels": [], "entities": [{"text": "Named Entity Recognition task", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.6395787596702576}]}, {"text": "Our improvement over existing methods helps the model to take into account the context of the sentence inside a court decision in order to recognize an entity with a typo.", "labels": [], "entities": []}, {"text": "We used state-of-the-art models and enriched the last layer of the neural network with high-level information linked with the potential of the word to be a certain type of entity.", "labels": [], "entities": []}, {"text": "More precisely, we utilized the similarities between the word and the potential entity candidates in the tagged sentence context.", "labels": [], "entities": []}, {"text": "The experiments on a dataset of French court decisions show a reduction of the relative F1-score error of 32%, upgrading the score obtained with the most competitive fine-tuned state-of-the-art system from 94.85% to 96.52%.", "labels": [], "entities": [{"text": "F1-score error", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.9897303283214569}]}], "introductionContent": [{"text": "Automatic Named Entity Recognition (NER) is a task that has been tackled and tackled over the years, because of the multitude of possible applications that flow from it.", "labels": [], "entities": [{"text": "Automatic Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7463213886533465}]}, {"text": "It can be useful for entity information extraction, for the creation of Knowledge Bases like DBPedia or for purposes of pseudonymisation (identification and replacement) insensitive documents from the medical or the legal domain (.", "labels": [], "entities": [{"text": "entity information extraction", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.70179283618927}, {"text": "DBPedia", "start_pos": 93, "end_pos": 100, "type": "DATASET", "confidence": 0.9449576139450073}, {"text": "pseudonymisation (identification and replacement", "start_pos": 120, "end_pos": 168, "type": "TASK", "confidence": 0.6423552453517913}]}, {"text": "In our application, the French Courts of Justice release 3800k court decisions each year.", "labels": [], "entities": [{"text": "French Courts of Justice release 3800k court", "start_pos": 24, "end_pos": 68, "type": "DATASET", "confidence": 0.932037217276437}]}, {"text": "The size of this number makes the manual de-identification of each court decision helpless.", "labels": [], "entities": []}, {"text": "Hence it is mandatory to use natural language processing NER tools to automatize the operation.", "labels": [], "entities": []}, {"text": "The domain of NER has considerably evolved in the last several years.", "labels": [], "entities": [{"text": "NER", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8463258147239685}]}, {"text": "The NER models can be rule-based systems using expert knowledge), hybrid models using linguistics and domain specific cues as features of a learning method ( or end-to-end deep learning models using distributed learned representations of words and characters (.", "labels": [], "entities": []}, {"text": "Each method has its own advantages and drawbacks.", "labels": [], "entities": []}, {"text": "The rule-based ones allow high precision but are nonetheless domain-specific, nor robust to noisy data and costly to design.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9949488639831543}]}, {"text": "Hybrid methods combine the robustness and the high accuracy of Machine Learning algorithms with the finegrained information of external dictionaries or linguistic rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9991796612739563}]}, {"text": "The deep learning approaches that achieve high performances relying on a big amount of training data are the most efficient nowadays.", "labels": [], "entities": []}, {"text": "Nevertheless, even the most efficient systems struggle to manage with some kind of noise: the typos and misspelling are common in real-world tasks, up to 15% of the search queries (, and lower the performances of the NER systems (.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew method called MICA (May I Check Again) that improves the performances of a state-of-the-art NER model using contextual information by automatically generating contextual dictionaries of entities.", "labels": [], "entities": []}, {"text": "We use a two-learning-step method that: learns a first NER neural network model, then uses the first network to create a list of potential entities that will be used to create new features for each word in the last layer of the second NER neural network model.", "labels": [], "entities": []}, {"text": "We chose last layer since those new features contain high-level information regarding our task and the level of complexity increases with the depth of neural network.", "labels": [], "entities": []}, {"text": "Nevertheless, this method can also be used with a simple NER system like Conditional Random Fields, and it shows interesting results although not state-ofthe-art.", "labels": [], "entities": []}, {"text": "The use of language-specific knowledgesource, dictionaries or gazetteers is very common for this type of task.", "labels": [], "entities": []}, {"text": "also proposed to learn dictionaries of entities for NER and we distinguish our work from theirs by several points.", "labels": [], "entities": [{"text": "NER", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6352267265319824}]}, {"text": "Our method does not aim to create a dictionary of entities but instead use the entities detected in the context of each sentence in order to enhance the NER model.", "labels": [], "entities": []}, {"text": "Finally, we also worked on the language model embeddings in order to adapt the language models and embeddings from general domain to the legal domain.", "labels": [], "entities": []}, {"text": "For that, we refined the BiLM Flair embeddings ( and trained the Fastext embeddings () on a dataset of 660,000 court decisions in order to adapt the language models and embeddings from general domain to the legal domain.", "labels": [], "entities": [{"text": "BiLM Flair embeddings", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.6553616225719452}, {"text": "Fastext embeddings", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.9139434099197388}]}], "datasetContent": [{"text": "We tested three kind of models, that were all build upon a state-of-the-art performing system for Named Entity Recognition () that we call Vanilla for reasons of simplicity.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.7195806503295898}]}, {"text": "The Vanilla model is a BiLSTM-CRF taking as input different kinds of embeddings learned on general text data.", "labels": [], "entities": []}, {"text": "We compared the Vanilla model with a model using embeddings that were fine-tuned or learned on legal text from the same domain that the text in our NER dataset.", "labels": [], "entities": [{"text": "NER dataset", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.9111307263374329}]}, {"text": "Eventually, we compare those baseline models to our models with the CRF layer enhanced with high-level similarity information.", "labels": [], "entities": []}, {"text": "All the models were compared on the same dataset, with the same split between the train, validation and test datasets.", "labels": [], "entities": []}, {"text": "Each set constitutes respectively approximately 80%, 10% and 10% of the full dataset.", "labels": [], "entities": []}, {"text": "All the models have been implemented using Pytorch () and based on the Flair toolbox ().", "labels": [], "entities": [{"text": "Pytorch", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.766269326210022}, {"text": "Flair toolbox", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9114474654197693}]}, {"text": "The simple CRF had been implemented using pycrfsuite).", "labels": [], "entities": []}, {"text": "Our dataset is composed of 94 of real court decisions fora total of 11,209 sentences and 276,705 tokens.", "labels": [], "entities": []}, {"text": "It has been manually annotated by a unique law expert regarding the following 4 types of entities: Following the protocol of CONLL-2003 (, the dataset has been annotated with the BIO scheme ( and separated into a train, a development and a test dataset.", "labels": [], "entities": [{"text": "CONLL-2003", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.9456682801246643}, {"text": "BIO", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.8742246031761169}]}, {"text": "The statistics of the subdivided sets are shown in.", "labels": [], "entities": []}, {"text": "Examples of decision after anonymization of the PER, LOC and DATE classes can be found on the Internet website of L\u00e9gifrance.", "labels": [], "entities": [{"text": "PER", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9877831339836121}, {"text": "LOC", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.864591121673584}, {"text": "DATE", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.8947818279266357}]}, {"text": "Due to the facts that a second annotation pass would be costly and that the court decisions follow a writing protocol familiar to the annotator (expert: Description of the dataset of French court decisions with the associated entities in law), there is no validation of the expert's annotations with an inter-agreement score.", "labels": [], "entities": []}, {"text": "Finally, it is important to note that it is a classical NER problem with four classes, nevertheless only the PER, LOC and DATE classes are useful for the de-identification problem.", "labels": [], "entities": [{"text": "NER problem", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8902671635150909}, {"text": "PER", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9876250624656677}, {"text": "LOC", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.770598292350769}, {"text": "DATE", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.7752313017845154}]}], "tableCaptions": [{"text": " Table 1: Description of the dataset of French court  decisions with the associated entities", "labels": [], "entities": []}, {"text": " Table 2: Results with the different models on the test dataset. The Context is in number of sentences.", "labels": [], "entities": []}]}