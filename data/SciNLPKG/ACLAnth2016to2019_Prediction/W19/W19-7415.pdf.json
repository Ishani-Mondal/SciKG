{"title": [], "abstractContent": [{"text": "This paper introduces our preliminary work in dictionary expansion by adding English and Chinese Wikipedia titles along with their linguistic features.", "labels": [], "entities": [{"text": "dictionary expansion", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8456684350967407}]}, {"text": "Parts-of-speech of Chinese titles are determined by the majority of heads of their Wikipedia categories.", "labels": [], "entities": []}, {"text": "Proper noun detection in English Wikipedia is done by checking the capitalization of the titles in the content of the articles.", "labels": [], "entities": [{"text": "noun detection", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.845785915851593}]}, {"text": "Title alternatives will be detected beforehand.", "labels": [], "entities": []}, {"text": "Chinese proper noun detection is done via interlanguage links and POS.", "labels": [], "entities": [{"text": "Chinese proper noun detection", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5188540443778038}]}, {"text": "The estimated accuracy of POS determination is 71.67% and the accuracy of proper noun detection is about 83.32%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9997827410697937}, {"text": "POS determination", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7447677552700043}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9997225403785706}, {"text": "proper noun detection", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.638819009065628}]}], "introductionContent": [{"text": "Dictionaries play an important role in many NLP researches.", "labels": [], "entities": []}, {"text": "A dictionary contains a list of words.", "labels": [], "entities": []}, {"text": "It can be used to provide candidates in Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.5805163184801737}]}, {"text": "If a dictionary also collect phrases, it can help to detect syntactic units when doing syntax parsing.", "labels": [], "entities": [{"text": "syntax parsing", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7209927141666412}]}, {"text": "Some dictionaries provide information about parts-of-speech or semantics, which is important for POS tagging and many other NLP applications (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.7392703294754028}]}, {"text": "A major issue of using dictionaries is the expansion of unknown words.", "labels": [], "entities": []}, {"text": "This issue is especially important in Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.5871073802312216}]}, {"text": "If the dictionary does not recognize many new words, it is impossible to segment an input sentence correctly.", "labels": [], "entities": []}, {"text": "It might be easy to collect unknown words from the Internet such as Wikipedia, but their parts-of-speech or other important linguistic features are not easy to be determined, because their sources are not designed for NLP purposes.", "labels": [], "entities": []}, {"text": "In NLP domain, there have been many researches about extracting information from Wikipedia in different aspects and methods.", "labels": [], "entities": [{"text": "extracting information from Wikipedia", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.8095234334468842}]}, {"text": "Popular researches include knowledge base expansion (), Wikipedia article similarity measurement by the hierarchy of categories (, infobox completion (, and soon.", "labels": [], "entities": [{"text": "knowledge base expansion", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7006520628929138}, {"text": "Wikipedia article similarity measurement", "start_pos": 56, "end_pos": 96, "type": "TASK", "confidence": 0.5243072062730789}]}, {"text": "Many NLP applications used Wikipedia as a resource, such as improving machine translation by Wikipedia interlanguage links (, measuring document similarity (), word sense disambiguation, annotating Wikipedia entries in documents (, and question answering (), including answer-type decision by Wikipedia (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7011382877826691}, {"text": "word sense disambiguation", "start_pos": 160, "end_pos": 185, "type": "TASK", "confidence": 0.6539066135883331}, {"text": "question answering", "start_pos": 236, "end_pos": 254, "type": "TASK", "confidence": 0.8136363923549652}]}, {"text": "This paper proposes methods to expand English and Chinese dictionaries by adding titles of Wikipedia articles, for they are new and constantly maintained.", "labels": [], "entities": []}, {"text": "Note that many of them are indeed multi-word phrases.", "labels": [], "entities": []}, {"text": "Methods to add linguistic features to these new words, such as parts-ofspeech, and proper nouns, are also discussed.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes how we preprocess data in Wikipedia.", "labels": [], "entities": []}, {"text": "Section 3 introduces our approach to determine parts-of-speech.", "labels": [], "entities": []}, {"text": "Section 4 proposes methods to decide whether a title is a proper noun or not.", "labels": [], "entities": []}, {"text": "Section 5 shows the experimental results and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental data are dumped files of English Wikipedia and Chinese Wikipedia on 2019/2/1.", "labels": [], "entities": [{"text": "English Wikipedia and Chinese Wikipedia on 2019/2/1", "start_pos": 42, "end_pos": 93, "type": "DATASET", "confidence": 0.7839150672609155}]}, {"text": "After removing administrative, disambiguation, and list pages, there are totally 5,679,503 English Wikipedia main articles and 995,294 Chinese Wikipedia main articles.", "labels": [], "entities": []}, {"text": "After further identifying person names, location names, letters and numbers, 3,905,050 English and 839,174 Chinese articles are waited to be processed as shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pre-Determined Wikipedia Pages.", "labels": [], "entities": []}, {"text": " Table 3: English Proper Noun Detection Accuracy.", "labels": [], "entities": [{"text": "English Proper Noun Detection Accuracy", "start_pos": 10, "end_pos": 48, "type": "TASK", "confidence": 0.671588534116745}]}]}