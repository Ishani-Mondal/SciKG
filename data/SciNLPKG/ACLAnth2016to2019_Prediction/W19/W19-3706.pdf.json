{"title": [{"text": "Creating a Corpus for Russian Data-to-Text Generation Using Neural Machine Translation and Post-Editing", "labels": [], "entities": [{"text": "Russian Data-to-Text Generation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6070261398951212}, {"text": "Neural Machine Translation", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.7015742460886637}]}], "abstractContent": [{"text": "In this paper, we propose an approach for semi-automatically creating a data-to-text (D2T) corpus for Russian that can be used to learn a D2T natural language generation model.", "labels": [], "entities": [{"text": "D2T natural language generation", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.580353245139122}]}, {"text": "An error analysis of the output of an English-to-Russian neural machine translation system shows that 80% of the automatically translated sentences contain an error and that 53% of all translation errors bear on named entities (NE).", "labels": [], "entities": [{"text": "English-to-Russian neural machine translation", "start_pos": 38, "end_pos": 83, "type": "TASK", "confidence": 0.677880585193634}]}, {"text": "We therefore focus on named entities and introduce two post-editing techniques for correcting wrongly translated NEs.", "labels": [], "entities": [{"text": "correcting wrongly translated NEs", "start_pos": 83, "end_pos": 116, "type": "TASK", "confidence": 0.7652897238731384}]}], "introductionContent": [{"text": "Data-to-text (D2T) generation is a key task in Natural Language Generation (NLG) which focuses on transforming data into text and permits verbalising the data contained in data-or knowledge bases.", "labels": [], "entities": [{"text": "Data-to-text (D2T) generation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6189287066459656}, {"text": "Natural Language Generation (NLG)", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.8176131447156271}]}, {"text": "However, creating the training data necessary to learn a D2T generation model is a major bottleneck as (i) naturally occurring parallel datato-text data does not commonly exist and (ii) manually creating such data is highly complex.", "labels": [], "entities": []}, {"text": "Moreover, the few parallel corpora that exist for D2T generation have been developed mainly for English.", "labels": [], "entities": [{"text": "D2T generation", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.8465755581855774}]}, {"text": "Methods that support the automatic creation of multi-lingual D2T corpora from these existing datasets would therefore be highly valuable.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a semi-automatic method for deriving a parallel data-to-text corpus for Russian from the D2T WebNLG corpus whose texts are in English.", "labels": [], "entities": [{"text": "D2T WebNLG corpus", "start_pos": 117, "end_pos": 134, "type": "DATASET", "confidence": 0.8277910749117533}]}, {"text": "Our method includes three main steps.", "labels": [], "entities": []}, {"text": "First, we use neural machine translation (NMT) model to translate WebNLG English texts into Russian.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.7978036006291708}, {"text": "WebNLG English texts", "start_pos": 66, "end_pos": 86, "type": "DATASET", "confidence": 0.79496963818868}]}, {"text": "Second, we perform a detailed error analysis on the output of the NMT model.", "labels": [], "entities": []}, {"text": "Third, we exploit two techniques for automatically post-editing the automatic translations.", "labels": [], "entities": []}, {"text": "As 53% of the translation errors bear on named entities, we focus on these in the present paper and leave other error types for further research.", "labels": [], "entities": []}, {"text": "The new corpus, error classification and scripts are available at https://gitlab.", "labels": [], "entities": [{"text": "error classification", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.6442387104034424}]}, {"text": "com/shimorina/bsnlp-2019.", "labels": [], "entities": []}], "datasetContent": [{"text": "The WebNLG data-to-text corpus () aligns knowledge graphs with textual descriptions verbalising the content of those graphs.", "labels": [], "entities": [{"text": "WebNLG data-to-text corpus", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.9404981732368469}]}, {"text": "The knowledge graphs are extracted from DBpedia ( sets of (one to seven) RDF triples of the form <subject, property, object>.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9520872235298157}]}, {"text": "Textual descriptions are in English, and due to the nature of the knowledge graphs, they have an abundance of named entities.", "labels": [], "entities": []}, {"text": "The first two lines of show an example of a WebNLG instance.", "labels": [], "entities": []}, {"text": "WebNLG provides textual descriptions for entities in fifteen DBpedia categories (Airport, Artist, Astronaut, Athlete, Building, CelestialBody, City, ComicsCharacter, Food, MeanOfTransportation, Monument, Politician, SportsTeam, University, WrittenWork).", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9290832281112671}, {"text": "MeanOfTransportation", "start_pos": 172, "end_pos": 192, "type": "DATASET", "confidence": 0.8143636584281921}]}, {"text": "The corpus possesses a hierarchical structure: if a set consisting of more than one triple is verbalised, then verbalisations of every single triple are to be found in the corpus.", "labels": [], "entities": []}, {"text": "Given the example in, the pairs {<Asterix, creator, Ren\u00e9 Goscinny>: Ren\u00e9 Goscinny created Asterix} and {<Ren\u00e9 Goscinny, nationality, French people>: Ren\u00e9 Goscinny is French} are also present in the WebNLG data.", "labels": [], "entities": [{"text": "WebNLG data", "start_pos": 198, "end_pos": 209, "type": "DATASET", "confidence": 0.9852618873119354}]}, {"text": "That structure allows propagating post-edits made in texts describing one triple to those verbalising triple sets of larger sizes.", "labels": [], "entities": []}, {"text": "Evaluation was carried out only on the rule-based method output, since it is more robust than the neural approach, and since the APE model did not yield better results.", "labels": [], "entities": [{"text": "APE", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.6005820631980896}]}, {"text": "We analysed a sample of total 66 lexicalisations in 4 categories: Astronaut, University, Monument (2-7 triples) and ComicsCharacter (2-5 triples).", "labels": [], "entities": [{"text": "Monument", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9184635877609253}, {"text": "ComicsCharacter", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.9652129411697388}]}, {"text": "Around two thirds of analysed named entities were replaced correctly.", "labels": [], "entities": []}, {"text": "Below we analyse common sources of errors for the erroneous NEs.", "labels": [], "entities": [{"text": "errors", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9663768410682678}, {"text": "NEs", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.8492422103881836}]}, {"text": "The most frequent case is unrecognised named entities.", "labels": [], "entities": []}, {"text": "In 62% of the cases the replacement was not performed, which includes 28% of Latin transcriptions kept, 27% of kept Cyrillic translations, and 7% of acronyms.", "labels": [], "entities": []}, {"text": "For the majority of these NEs, the original translations include unaccounted elements (not covered by the extracted rules) such as missing or wrongly inserted prepositions or punctuation marks.", "labels": [], "entities": []}, {"text": "Another common error is lack of grammatical adaptation of the NE.", "labels": [], "entities": []}, {"text": "Wrong case marking occurred in 23% of all NEs (cf. example 2), and gender and number agreement make about 6.5%.", "labels": [], "entities": [{"text": "Wrong case marking", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.546243945757548}, {"text": "NEs", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9501480460166931}]}, {"text": "The less frequent but important error categories are spelling errors, such as missing capitalisation, insertions of quotation marks, and gender or number agreement with anaphors, especially in texts verbalising 5-7 triples.", "labels": [], "entities": []}, {"text": "(2) En: 'The dean of Accademia di Architettura' MT: '\u0414\u0435\u043a\u0430\u043d Accademia di Projecttura' RPE: '\u0414\u0435\u043a\u0430\u043d \u0410\u043a\u0430\u0434\u0435\u043c\u0438\u044f nomn \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443-\u0440\u044b' Correct: '\u0414\u0435\u043a\u0430\u043d \u0410\u043a\u0430\u0434\u0435\u043c\u0438\u0438 gen \u0430\u0440\u0445\u0438\u0442\u0435\u043a-\u0442\u0443\u0440\u044b' To conclude, many errors are caused by irregularities in the translations (which, in turn, are often caused by misspelled input) and can be eliminated by introducing more variation to the replacement algorithm.", "labels": [], "entities": []}, {"text": "Grammatical adaptation of NEs, however, requires more careful further investigation.", "labels": [], "entities": [{"text": "Grammatical adaptation of NEs", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8679577261209488}]}], "tableCaptions": [{"text": " Table 2. Named enti- ties were treated as a separate category to high- light problems while applying the NMT system  on WebNLG. If a text contained more than one  mistake in a particular category, then each mis- take was tagged as an error. If a spotted mistake  concerned an NE, annotators were allowed to add  other categories to specify the error.", "labels": [], "entities": []}, {"text": " Table 3: Proportion of main error types in the manually  post-edited data and Cohen's \u03ba scores on the held-out  category Athlete.", "labels": [], "entities": [{"text": "Cohen's \u03ba scores", "start_pos": 79, "end_pos": 95, "type": "METRIC", "confidence": 0.7330113500356674}]}, {"text": " Table 4: Corpus statistics: number of post-edited (PE)  texts. Total corresponds to both PE and non-PE texts.", "labels": [], "entities": []}]}