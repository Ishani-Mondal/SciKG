{"title": [], "abstractContent": [{"text": "The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems.", "labels": [], "entities": [{"text": "WMT2019", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.8667117357254028}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.6856091022491455}]}, {"text": "In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model.", "labels": [], "entities": []}, {"text": "We train translation models in both language pairs: Nepali-English and Sinhala-English using provided parallel corpora.", "labels": [], "entities": []}, {"text": "To create the best possible translation model, we first join all provided parallel corpora (Nepali, Sinhala and Hindi to English) and after that, we applied bilingual cross-entropy selection for both language pairs (Nepali-English and Sinhala-English).", "labels": [], "entities": []}, {"text": "Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair.", "labels": [], "entities": []}, {"text": "We compute the smoothed BLEU score between the target sentence and generated hypothesis.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9728149175643921}]}, {"text": "In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score.", "labels": [], "entities": []}, {"text": "These heuristics are based on sentence length, source and target similarity and source language detection.", "labels": [], "entities": [{"text": "source language detection", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.628421276807785}]}, {"text": "We compare our results with the baseline published on the shared task website, which uses the Zip-porah model, over which we achieve significant improvements in one of the conditions in the shared task.", "labels": [], "entities": []}, {"text": "The designed filtering system is domain independent and all experiments are conducted using neural machine translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "A large amount of parallel corpora can be extracted using web-crawling.", "labels": [], "entities": []}, {"text": "This technique of data acquisition is very useful to increase the training set for low-resourced languages.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.749584436416626}]}, {"text": "Unfortunately, the extracted data can include noisy sentence pairs, such as unaligned sentences, partially translated pairs, or sentences containing different languages than those intended.", "labels": [], "entities": []}, {"text": "For these reasons the creation of systems for filtering of noisy parallel corpora are needed.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a filtering method for noisy parallel corpora based mainly on generating hypotheses for each sentence pair from noisy data and scoring based on hypothesis and target sentence similarity.", "labels": [], "entities": []}, {"text": "This technique consists of building the best possible translation engine for each language pair and generating a translation hypothesis for each sentence of the noisy data.", "labels": [], "entities": []}, {"text": "Once the hypotheses are generated, we compute the BLEU (), smoothed by adding one to both numerator and denominator from (), between each target and hypothesis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9991401433944702}]}, {"text": "To create a translation engine, which will be used for generating hypothesis for each sentence from noisy corpus, we select sentence pairs using bilingual cross-entropy selection) from all parallel corpora provided (Nepali, Sinhala, Hindi to English) jointly.", "labels": [], "entities": []}, {"text": "To apply bilingual cross-entropy, we first train language models using the provided monolingual corpora in Nepali, Sinhala and English.", "labels": [], "entities": []}, {"text": "In addition, we use some rules to discard useless sentences by filtering according to sentence length, Nepali and Sinhala characters detection, and BLEU scoring between source and target sentences.", "labels": [], "entities": [{"text": "Nepali and Sinhala characters detection", "start_pos": 103, "end_pos": 142, "type": "TASK", "confidence": 0.5056453585624695}, {"text": "BLEU scoring", "start_pos": 148, "end_pos": 160, "type": "METRIC", "confidence": 0.9628998637199402}]}, {"text": "The last rule is used to discard highly similar sentence pairs.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: Section 2 describes the shared task, the provided data, the subsampling process and the evaluation system.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the developed method for filtering noisy data.", "labels": [], "entities": []}, {"text": "We describe the experiments conducted and the results.", "labels": [], "entities": []}, {"text": "Conclusions and future work are drawn in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "As specified in the shared task, the evaluation of a selected subset of sentences is done using SMT and NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.8419058322906494}]}, {"text": "The SMT system is implemented using Moses () and the NMT system is built using the FAIRseq (Ott et al., 2019) toolkit.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9894864559173584}, {"text": "NMT", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.7754799127578735}, {"text": "FAIRseq (Ott et al., 2019) toolkit", "start_pos": 83, "end_pos": 117, "type": "DATASET", "confidence": 0.8198270234796736}]}, {"text": "Organisers provided scripts which allow for implementing the same translation system which will be used in the final evaluation.", "labels": [], "entities": []}, {"text": "However, we only conducted experiments using NMT.", "labels": [], "entities": []}, {"text": "The The learning rate is fixed to lr = 1e3, as described in).", "labels": [], "entities": []}, {"text": "The NMT system from the shared task is trained for 100 epochs and models are saved every 10 epochs.", "labels": [], "entities": []}, {"text": "The best model is chosen according to validation set loss function value.", "labels": [], "entities": []}, {"text": "The script which allowed us to reproduce the network used in the shared task can be found at https://github.com/facebookresearch/flores.", "labels": [], "entities": []}, {"text": "All experiments were performed using NVidia Titan Xp GPUs.", "labels": [], "entities": [{"text": "NVidia Titan Xp GPUs", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.9612446874380112}]}], "tableCaptions": [{"text": " Table 6: Validation sacreBLEU scores for bilin- gual cross-entropy selection results depending on the  number of training sentences for Nepali-English and  Sinhala-English. M denotes millions of elements. Best  system marked in bold.", "labels": [], "entities": []}, {"text": " Table 7: SacreBLEU scores for final NMT system  trained using sentences selected with different values  of threshold \u00b5 for Nepali-English.", "labels": [], "entities": []}, {"text": " Table 8: SacreBLEU scores for final NMT system  trained using sentences selected with different values  of threshold \u00b5 for Sinhala-English.", "labels": [], "entities": [{"text": "NMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8175554871559143}]}, {"text": " Table 9: Statistics of how many sentences of noisy cor- pus were set their final score as zero after applying dif- ferent rules. The number in parenthesis indicates the  rule described in the enumerated list above. k denotes  thousands of elements and M denotes millions of ele- ments.", "labels": [], "entities": []}, {"text": " Table 10: SacreBLEU scores for NMT system compar- ison with the Zipporah model.", "labels": [], "entities": []}]}