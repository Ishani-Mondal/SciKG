{"title": [{"text": "Gendered Pronoun Resolution using BERT and an extractive question answering formulation", "labels": [], "entities": [{"text": "Gendered Pronoun Resolution", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6537991464138031}, {"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9958367347717285}, {"text": "extractive question answering", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.6527511179447174}]}], "abstractContent": [{"text": "The resolution of ambiguous pronouns is a longstanding challenge in Natural Language Understanding.", "labels": [], "entities": [{"text": "resolution of ambiguous pronouns", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8674574196338654}, {"text": "Natural Language Understanding", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.6728770732879639}]}, {"text": "Recent studies have suggested gender bias among state-of-the-art coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9324145019054413}]}, {"text": "As an example, Google AI Language team recently released a gender-balanced dataset and showed that performance of these coreference resolvers is significantly limited on the dataset.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.8800513446331024}]}, {"text": "In this paper, we propose an extractive question answering (QA) formulation of pronoun resolution task that overcomes this limitation and shows much lower gender bias (0.99) on their dataset.", "labels": [], "entities": [{"text": "extractive question answering (QA)", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.7614020804564158}, {"text": "pronoun resolution task", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.7492123345534006}]}, {"text": "This system uses fine-tuned representations from the pre-trained BERT model and outperforms the existing baseline by a significant margin (22.2% absolute improvement in F1 score) without using any hand-engineered features.", "labels": [], "entities": [{"text": "BERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9933807849884033}, {"text": "F1 score", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9878886938095093}]}, {"text": "This QA framework is equally performant even without the knowledge of the candidate antecedents of the pronoun.", "labels": [], "entities": []}, {"text": "An ensemble of QA and BERT-based multiple choice and sequence classification models further improves the F1 (23.3% absolute improvement upon the baseline).", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9945580959320068}, {"text": "F1", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9996538162231445}]}, {"text": "This ensemble model was submitted to the shared task for the 1st ACL workshop on Gender Bias for Natural Language Processing.", "labels": [], "entities": [{"text": "ACL workshop on Gender Bias for Natural Language Processing", "start_pos": 65, "end_pos": 124, "type": "TASK", "confidence": 0.5769275095727708}]}, {"text": "It ranked 9th on the final official leaderboard.", "labels": [], "entities": [{"text": "final official leaderboard", "start_pos": 21, "end_pos": 47, "type": "DATASET", "confidence": 0.6253436903158823}]}], "introductionContent": [{"text": "Coreference resolution is a task that aims to identify spans in a text that refer to the same entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9104767739772797}]}, {"text": "This is central to Natural Language Understanding.", "labels": [], "entities": [{"text": "Natural Language Understanding", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.7131936351458231}]}, {"text": "We focus on a specific aspect of the coreference resolution that caters to resolving ambiguous pronouns in English.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9782187640666962}]}, {"text": "Recent studies have shown that state-of-the-art coreference resolution systems exhibit gender bias)) (.) released a dataset that contained an equal number of male and female examples to encourage gender-fair modeling on the pronoun resolution task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.8923419713973999}, {"text": "pronoun resolution task", "start_pos": 224, "end_pos": 247, "type": "TASK", "confidence": 0.7930921216805776}]}, {"text": "A shared task for this dataset was then published on Kaggle . The task involves classifying a specific ambiguous pronoun in a given Wikipedia passage as coreferring with one of the three classes: first candidate antecedent (hereby referred to as A), second candidate antecedent (hereby referred to as B) or neither of them (hereby referred to as N).", "labels": [], "entities": [{"text": "Kaggle", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9747613668441772}]}, {"text": "The authors show that even the best of the baselines such as,, () achieve an F1 score of just 66.9% on this dataset.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9901135265827179}]}, {"text": "The limited number of annotated labels available in this unbiased setting makes the modeling a challenging task.", "labels": [], "entities": []}, {"text": "To that end, we propose an extractive question answering formulation of the task that leverages BERT) pre-trained representations and significantly improves (22.2% absolute improvement in F1 score) upon the best baseline.", "labels": [], "entities": [{"text": "extractive question answering formulation", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.6874592006206512}, {"text": "BERT", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9968041181564331}, {"text": "F1 score)", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9779878457387289}]}, {"text": "In this formulation, the task is similar to a SQUAD () style question answering (QA) problem where the question is the context window (neighboring words) surrounding the pronoun to be resolved and the answer is the antecedent of the pronoun.", "labels": [], "entities": [{"text": "SQUAD () style question answering (QA)", "start_pos": 46, "end_pos": 84, "type": "TASK", "confidence": 0.7527172900736332}]}, {"text": "The answer is contained in the provided Wikipedia passage.", "labels": [], "entities": [{"text": "Wikipedia passage", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.9516662657260895}]}, {"text": "The intuition behind using the pronoun's context window as a question is that it allows the model to rightly identify the pronoun to be resolved as there can be multiple tokens that match the given pronoun in a passage.", "labels": [], "entities": []}, {"text": "There has been previous work that cast the coreference resolution as a Question Answering problem (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.9780226945877075}, {"text": "Question Answering problem", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7761249641577402}]}, {"text": "But the questions used in their approach take the form \"Who does \"she\" refer to?\".", "labels": [], "entities": []}, {"text": "This would necessitate including additional information such as an indicator vector to identify the exact pronoun to be re- solved when there are multiple of them in a given passage.", "labels": [], "entities": []}, {"text": "Furthermore, their approach doesn't impose that the answer should be contained within the passage or the question text.", "labels": [], "entities": []}, {"text": "() model the pronoun resolution task of the Winograd schema challenge () as a question answering problem by including the candidate antecedents as part of the question.", "labels": [], "entities": [{"text": "pronoun resolution task", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.808536171913147}, {"text": "question answering problem", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.7985075414180756}]}, {"text": "An unique feature of the question answering framework (referred to as CorefQA) we propose is that it doesn't require the knowledge of the candidate antecedents in order to produce an answer for the pronoun resolution task.", "labels": [], "entities": [{"text": "question answering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.858100563287735}, {"text": "pronoun resolution task", "start_pos": 198, "end_pos": 221, "type": "TASK", "confidence": 0.8179029226303101}]}, {"text": "The model \"learns\", from training on the QA version of the shared task dataset, the specific task of extracting the appropriate antecedent of the pronoun given just the Wikipedia passage and the pronoun's context window.", "labels": [], "entities": [{"text": "QA version of the shared task dataset", "start_pos": 41, "end_pos": 78, "type": "DATASET", "confidence": 0.6823570004531315}]}, {"text": "We also demonstrate other modeling variants for the shared task that use the knowledge of the candidate antecedents A and B.", "labels": [], "entities": []}, {"text": "The first variant (CorefQAExt) is an extension of the CorefQA model that uses its predictions to produce probabilities over A, B and N.", "labels": [], "entities": []}, {"text": "The second variant (CorefMulti) takes the formulation of a SWAG () style multiple choice classification and the final variant (CorefSeq) takes the standard sequence classification formulation.", "labels": [], "entities": []}, {"text": "An ensemble of CorefQAExt, CorefMulti and CorefSeq models shows further performance gains (23.3% absolute improvement in F1 score).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9913493394851685}]}], "datasetContent": [{"text": "We tried fine-tuning the BERT model in an unsupervised manner by training a language model on the texts extracted from the Wikipedia pages corresponding to the URLs provided in the dataset.", "labels": [], "entities": [{"text": "BERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9522759318351746}]}, {"text": "The idea behind this one was to see if we can get better BERT layer representations by tuning them to the shared task's dataset.", "labels": [], "entities": [{"text": "BERT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.8881915807723999}]}, {"text": "However, this is a computationally expensive step to run and we didn't see promising gains from initial runs.", "labels": [], "entities": []}, {"text": "We hypothesize that this maybe due to the fact that BERT representations were originally obtained by training on Wikipedia as one of the sources.", "labels": [], "entities": [{"text": "BERT", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9694560170173645}]}, {"text": "So, fine-tuning on the task's dataset which is also from Wikipedia might not have added an extra signal.", "labels": [], "entities": []}, {"text": "2. For the CorefMulti model, we tried adding to the token embedding vector, an additional entity embedding vector that encodes the wordpiece token level info of whether it belongs to one of A, B or P.", "labels": [], "entities": []}, {"text": "We hypothesized this should help the model focus its attention on the relevant entities to the coreference task.", "labels": [], "entities": []}, {"text": "But we weren't able to make a successful use of these embeddings to improve the model performance within the competition deadline.", "labels": [], "entities": []}, {"text": "However, this is a promising future direction.", "labels": [], "entities": []}, {"text": "3. For the CorefQAExt model, we appended the title extracted from the provided wikipedia page's URL into the input token sequence to evaluate if the page URL provides useful signal to the model.", "labels": [], "entities": []}, {"text": "This made the performance slightly worse.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Stage 1 and Stage 2 Dataset statistics.", "labels": [], "entities": []}, {"text": " Table 2: Stage 1 and Stage 2 Test Results. Bold indicates best performance.", "labels": [], "entities": []}]}