{"title": [{"text": "The Influence of Down-Sampling Strategies on SVD Word Embedding Stability", "labels": [], "entities": []}], "abstractContent": [{"text": "The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns.", "labels": [], "entities": []}, {"text": "We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.998230516910553}]}, {"text": "We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD PPMI-type embeddings.", "labels": [], "entities": []}, {"text": "This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior stability as well as accuracy on par with skip-gram embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9994719624519348}]}], "introductionContent": [{"text": "Word embedding algorithms implement the latest form of distributional semantics originating from the seminal work of or.", "labels": [], "entities": []}, {"text": "They generate dense vector space representations for words based on co-occurrences within a context window.", "labels": [], "entities": []}, {"text": "They sample word-context pairs, i.e., typically two cooccurring tokens, from a corpus and use these to generate vector representations of words and their context.", "labels": [], "entities": []}, {"text": "Changes to the algorithm's sampling mechanism can lead to new capabilities, e.g., processing dependency information instead of linear co-occurrences (), or increased performance, e.g., using word association values instead of raw co-occurrence counts.", "labels": [], "entities": []}, {"text": "Word embedding algorithms commonly downsample contexts to lessen the impact of highfrequency words (termed 'subsampling' in) or increase the relative importance of words closer to the center of a context window (called 'dynamic context window' in).", "labels": [], "entities": []}, {"text": "The effect of using such down-sampling strategies on accuracy in word similarity and analogy tasks was explored in several papers (e.g.,).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9957529306411743}, {"text": "word similarity and analogy tasks", "start_pos": 65, "end_pos": 98, "type": "TASK", "confidence": 0.7897793412208557}]}, {"text": "However, down-sampling and details of its implementation also have major effects on the stability of word embeddings (also known as 'reliability'), i.e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space.", "labels": [], "entities": []}, {"text": "This problem has lately raised severe concerns in the word embedding community (e.g.,;;) and is also of interest to the wider machine learning community due to the influence of probabilistic-and thus unstablemethods on experimental results, as well as replicability and reproducibility).", "labels": [], "entities": []}, {"text": "Stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e.g., tracking lexical change (.", "labels": [], "entities": []}, {"text": "Unstable word embeddings can lead to serious problems in such applications, as interpretations will depend on the luck of the draw.", "labels": [], "entities": []}, {"text": "This might also affect high-stake fields like medical informatics where patients could be harmed as a consequence of misleading results (.", "labels": [], "entities": []}, {"text": "In the light of these concerns, we here evaluate down-sampling strategies by modifying the SVD PPMI (Singular Value Decomposition of a Positive Pointwise Mutual Information matrix;) algorithm and comparing its results with those of two other embedding algorithms, namely, GLOVE () and SGNS (.", "labels": [], "entities": []}, {"text": "Our analysis is based on three corpora of different sizes and investigates effects on both accuracy and stability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.998995840549469}, {"text": "stability", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9529346227645874}]}, {"text": "The inclusion of accuracy measurements and the larger size of our training corpora exceed prior work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993205070495605}]}, {"text": "We show how the choice of down-sampling strategies, a seemingly minor detail, leads to major differences in the characterization of SVD PPMI in recent studies.", "labels": [], "entities": [{"text": "SVD PPMI", "start_pos": 132, "end_pos": 140, "type": "TASK", "confidence": 0.7063334286212921}]}, {"text": "We also present SVD WPPMI , a simple modification of SVD PPMI that replaces probabilistic down-sampling with weighting.", "labels": [], "entities": [{"text": "SVD WPPMI", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.5819590985774994}]}, {"text": "What, at first sight, appears to be a small change leads, nevertheless, to an unrivaled combination of stability and accuracy, making it particularly well-suited for the above-mentioned corpus linguistic applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9990634322166443}]}], "datasetContent": [{"text": "We compared five algorithm variants: GLOVE, SGNS, SVD PPMI without down-sampling, SVD PPMI with probabilistic down-sampling, and SVD WPPMI . While we could use SGNS 6 and GLOVE 7 implementations directly, we had to modify SVD PPMI 8 to support the weighted sampling used in SVD WPPMI . As proposed by, we further modified our SVD PPMI implementation to use random numbers generated with a non-fixed seed for probabilistic down-sampling.", "labels": [], "entities": [{"text": "SVD WPPMI", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.6989404857158661}]}, {"text": "A fixed seed would benefit reliability, but also act as a bias during all analyses-seed choice has been shown to cause significant differences in experimental results (.", "labels": [], "entities": [{"text": "reliability", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9807409644126892}]}, {"text": "Down-sampling strategies for df and ff can be chosen independently of each other, e.g., using probabilistic down-sampling for df together with weighted down-sampling for ff . However, we decided to use the same down-sampling strategies, e.g., weighting, for both factors, taking into ac-count computational limitations as well as results from pre-tests that revealed little benefit of mixed strategies.", "labels": [], "entities": []}, {"text": "We trained ten models for each algorithm variant and corpus.", "labels": [], "entities": []}, {"text": "In the case of subsampling, each model was trained on one of the independently drawn samples.", "labels": [], "entities": []}, {"text": "Stability was evaluated by selecting the 1k most frequent words in each non-bootstrap subsampled corpus as anchor words and calculating j@10 (see Equation 1).", "labels": [], "entities": []}, {"text": "Following Hellrich and Hahn (2016a,b), we did not only investigate stability, but also the accuracy of our models to gauge potential tradeoffs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9990843534469604}]}, {"text": "We measured the Spearman rank correlation between cosine-based word similarity judgments and human ones with four psycholinguistic test sets, i.e., the two crowdsourced test sets MEN () and), the especially strict SimLex-999 () and the widely used).", "labels": [], "entities": []}, {"text": "We also measured the percentage of correctly solved analogies (using the multiplicative formula from) with two test sets developed at Google () and Microsoft Research (MSR;).", "labels": [], "entities": []}, {"text": "shows the accuracy and stability for all tested combinations of algorithm and corpus variants.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995241165161133}]}, {"text": "Accuracy differences between test sets are inline with prior observations and general The strongest counterexample is a combination of probabilistic down-sampling for df and weighting for ff which lead to small, yet significant improvements in the MEN (0.703 \u00b1 0.001) and MTurk (0.568 \u00b1 0.015) similarity tasks (cf.).", "labels": [], "entities": [{"text": "MEN", "start_pos": 248, "end_pos": 251, "type": "METRIC", "confidence": 0.5161174535751343}, {"text": "MTurk", "start_pos": 272, "end_pos": 277, "type": "DATASET", "confidence": 0.8054664134979248}]}, {"text": "However, other accuracy tasks showed no improvements and the stability of this approach (0.475 \u00b1 0.001) was far closer to SVDPPMI with fully probabilistic down-sampling than to the perfect stability of SVDWPPMI.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9989834427833557}, {"text": "SVDPPMI", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.8667704463005066}, {"text": "SVDWPPMI", "start_pos": 202, "end_pos": 210, "type": "DATASET", "confidence": 0.9353821277618408}]}, {"text": "We used symmetric 5 word context windows for all models as well as frequent word down-sampling thresholds of 100 (GLOVE) and 10 \u22124 (others).", "labels": [], "entities": [{"text": "GLOVE", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9865053296089172}]}, {"text": "Default learning rates and numbers of iterations were used for all models.", "labels": [], "entities": []}, {"text": "Eigenvalues as well as context vectors were ignored for SVDPPMI embeddings.", "labels": [], "entities": []}, {"text": "5 negative samples were used for SGNS.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.9658786654472351}]}, {"text": "The minimum frequency threshold was 50 for COHA, 100 for NEWS and 750 for WIKI-increased thresholds were necessary due to SVDPPMI's memory consumption scaling quadratically with vocabulary size.", "labels": [], "entities": [{"text": "NEWS", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.8524507284164429}]}, {"text": "11 Stability calculation was not performed directly between all 10 models, as this would result in a single value and preclude significance tests.", "labels": [], "entities": [{"text": "Stability calculation", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.8974323868751526}]}, {"text": "Instead, we generated ten j@10 values by calculating the stability of all subsets formed by leaving out each model once in a jackknife procedure.", "labels": [], "entities": []}, {"text": "performance on WIKI is roughly in-line with the data reported in.", "labels": [], "entities": [{"text": "WIKI", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.9412321448326111}]}, {"text": "In general, corpus size does seem to have a positive effect on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9953020811080933}]}, {"text": "However, for MEN, MTurk and MSR the highest values are achieved with NEWS and not with WIKI.", "labels": [], "entities": [{"text": "MEN", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.7961009740829468}, {"text": "MTurk", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.5918733477592468}, {"text": "NEWS", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9210516810417175}, {"text": "WIKI", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.9035600423812866}]}, {"text": "SVD PPMI variants seem to be less hampered by small training corpora, matching observations by.", "labels": [], "entities": [{"text": "SVD PPMI", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.4459497183561325}]}, {"text": "Stability is clearly positively influenced by corpus size for all probabilistic algorithm variants except GLOVE, which, in contrast, benefits from small training corpora.", "labels": [], "entities": []}, {"text": "Also, randomly subsampling corpora has a negative effect on both accuracy and stability-this can be explained by the smaller corpus size for accuracy and the differences in training material (as subsampling was performed independently for each model) for stability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.998749852180481}, {"text": "stability-this", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.9622198939323425}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9983905553817749}]}, {"text": "illustrates the stability of all tested algorithm variants.", "labels": [], "entities": []}, {"text": "SVD WPPMI and SVD PPMI without down-sampling are the only systems which achieve perfect stability when trained on non-subsampled corpora.", "labels": [], "entities": [{"text": "SVD WPPMI", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8162286281585693}]}, {"text": "GLOVE is the third most reliable algorithm in this scenario, except for the large WIKI corpus.", "labels": [], "entities": [{"text": "GLOVE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7818145751953125}, {"text": "WIKI corpus", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9496189057826996}]}, {"text": "Corpus subsampling decreases the stability of all algorithms, with SVD WPPMI still performing better than all other alternatives.", "labels": [], "entities": [{"text": "SVD WPPMI", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.5031580477952957}]}, {"text": "The only exception is subsampled COHA where the otherwise suboptimal GLOVE narrowly (0.330 instead of 0.329; difference significant with p < .05 by two-sided t-test) outperforms SVD WPPMI . SVD WPPMI can achieve stability values on subsampled corpora that are competitive with those for SGNS and GLOVE trained on non-subsampled corpora.", "labels": [], "entities": [{"text": "SVD WPPMI", "start_pos": 178, "end_pos": 187, "type": "DATASET", "confidence": 0.6964814960956573}]}, {"text": "We found standard deviations for stability to be very low, the highest being 0.01 for GLOVE trained on nonsubsampled WIKI, probably due to the overlap in our jackknife procedure.", "labels": [], "entities": [{"text": "standard deviations", "start_pos": 9, "end_pos": 28, "type": "METRIC", "confidence": 0.9230136275291443}, {"text": "stability", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9819566011428833}, {"text": "GLOVE", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.5305673480033875}]}, {"text": "Finally, we tested 12 the overall performance of each algorithm variant by first performing a Quade test as a safeguard against type I: Stability for each combination of algorithm variant and corpus.", "labels": [], "entities": [{"text": "Quade", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9775574207305908}]}, {"text": "Measured with j@10 metric (higher is better).", "labels": [], "entities": []}, {"text": "Same data as in, standard deviations too small to display.", "labels": [], "entities": []}, {"text": "errors, thus confirming the existence of significant differences between algorithms (p = 1.3 \u00b7 10 \u22127 ).", "labels": [], "entities": []}, {"text": "We then used a pairwise Wilcoxon rank-sum test) in order to compare other algorithms with SVD WPPMI . We found it to be not significantly different inaccuracy from SGNS (p = 0.101), but significantly better than SVD PPMI without downsampling (corrected p = 5.4\u00b710 \u22126 ) or probabilistic down-sampling (corrected p = 0.015), as well as GLOVE (corrected p = 0.027).", "labels": [], "entities": [{"text": "SVD WPPMI", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.7340466678142548}, {"text": "GLOVE", "start_pos": 334, "end_pos": 339, "type": "METRIC", "confidence": 0.5696828365325928}]}, {"text": "Our results show SVD WPPMI to be both highly reliable and accurate, especially on COHA, which has a size common in both stability studies and corpus linguistic applications.", "labels": [], "entities": [{"text": "SVD WPPMI", "start_pos": 17, "end_pos": 26, "type": "TASK", "confidence": 0.5442573577165604}]}, {"text": "Diverging reports on SVD PPMI stability-described as perfectly reliable in, yet not in Antoniak and Mimno (2018)-can thus be explained by their difference in down-sampling options, i.e., no down-sampling or probabilistic down-sampling.", "labels": [], "entities": []}, {"text": "GLOVE's high stability in other studies) seems to be counterbalanced by its low accuracy and also appears to be limited to training on small corpora.", "labels": [], "entities": [{"text": "GLOVE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8497629761695862}, {"text": "stability", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9694631099700928}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9991494417190552}]}], "tableCaptions": [{"text": " Table 1: Performance of different algorithms and down-sampling strategies with models trained on corpora with  and without subsampling. Bold values are best or not significantly different by independent t-tests (with p < 0.05).", "labels": [], "entities": []}]}