{"title": [{"text": "Efficient Language Modeling with Automatic Relevance Determination in Recurrent Neural Networks", "labels": [], "entities": [{"text": "Efficient Language Modeling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.604522834221522}]}], "abstractContent": [{"text": "Reduction of the number of parameters is one of the most important goals in Deep Learning.", "labels": [], "entities": [{"text": "Deep Learning", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.8481077253818512}]}, {"text": "In this article we propose an adaptation of Doubly Stochastic Variational Inference for Automatic Relevance Determination (DSVI-ARD) for neural networks compression.", "labels": [], "entities": [{"text": "neural networks compression", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.7693931857744852}]}, {"text": "We find this method to be especially useful in language modeling tasks, where large number of parameters in the input and output layers is often excessive.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7500314116477966}]}, {"text": "We also show that DSVI-ARD can be applied together with encoder-decoder weight tying allowing to achieve even better sparsity and performance.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that more than 90% of the weights in both encoder and decoder layers can be removed with a minimal quality loss.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of neural networks compression has recently gained more interest as the number of parameters (and hence memory size) of modern neural networks increased drastically.", "labels": [], "entities": [{"text": "neural networks compression", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.729715903600057}]}, {"text": "Moreover, only a few weights prove to be relevant for prediction while the majority are de facto redundant (.", "labels": [], "entities": [{"text": "prediction", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.973920464515686}]}, {"text": "In this paper we suggest an adaptation of a Bayesian approach called Automatic Relevance Determination (ARD) for neural networks compression in language modeling tasks, where the first and the last linear layers often have enormous * These two authors contributed equally; the ordering of their names was chosen arbitrarily.", "labels": [], "entities": [{"text": "Automatic Relevance Determination (ARD)", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.6691798468430837}, {"text": "neural networks compression", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.7977098822593689}, {"text": "language modeling tasks", "start_pos": 144, "end_pos": 167, "type": "TASK", "confidence": 0.7576154371102651}]}, {"text": "The work was done when the first author was an intern at the Samsung R&D Institute. size.", "labels": [], "entities": [{"text": "Samsung R&D Institute", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.9397231817245484}]}, {"text": "We derive the Doubly Stochastic Variational Inference (DSVI) algorithm for non-iid (not independent and identically distributed) objects, a common casein language modeling, and use it to perform optimization of our models.", "labels": [], "entities": []}, {"text": "Furthermore, we extend this approach so that it could be applied together with the weight tying technique, i.e., using the same set of parameters for both weight matrices of the first and the last layers, which has been proved highly efficient.", "labels": [], "entities": [{"text": "weight tying", "start_pos": 83, "end_pos": 95, "type": "TASK", "confidence": 0.8169771432876587}]}], "datasetContent": [{"text": "We have conducted several experiments to test the DSVI-ARD compression approach in language modeling.", "labels": [], "entities": [{"text": "DSVI-ARD compression", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6879065036773682}, {"text": "language modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7060451507568359}]}, {"text": "We used LSTM and LSTM with tied weights models from () respectively as our baselines: the experiments involved the same LSTM architecture with two hidden layers of size 650 and two datasets: PTB ( and Wikitext2 (; also each mini-batch of objects was constructed from bs word sequences (bs = 10 and bs = 20 for evaluation and training respectively) of length bptt = 35.", "labels": [], "entities": [{"text": "PTB", "start_pos": 191, "end_pos": 194, "type": "DATASET", "confidence": 0.8761163353919983}]}, {"text": "We applied dropout after the embedding (except for the tied-weight ARD models because ARD can be regarded as a special form of regularization by itself) and hidden layers, with a dropout rate as a hyperparameter.", "labels": [], "entities": []}, {"text": "We used stochastic gradient descent (SGD) as an optimization procedure, with adaptive learning rate decreasing from the starting value by a multiplicative factor (both are hyperparameters) each time validation perplexity has stopped improving.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 8, "end_pos": 41, "type": "TASK", "confidence": 0.7870828409989675}]}, {"text": "We also compared our approach to other compression techniques: matrix decompositionbased ( and VDbased (.", "labels": [], "entities": []}, {"text": "For the last one we used a similar model: a network with one LSTM layer of 256 hidden units.", "labels": [], "entities": []}, {"text": "The whole set of parameters of a model with DSVI-ARD layers can be divided into the varia-: Plots of validation cross-entropy (red line) of a LSTM model with a DSVI-ARD softmax layer on the PTB dataset and its corresponding sparsity (blue line) for different possible threshold log \u03bb thresh values (top) and the distribution histogram of its prior log-variances log \u03bb ij (bottom).", "labels": [], "entities": [{"text": "PTB dataset", "start_pos": 190, "end_pos": 201, "type": "DATASET", "confidence": 0.9762341380119324}]}, {"text": "We display the density on a log scale due to a very sparse distribution.", "labels": [], "entities": []}, {"text": "The threshold chosen for further model evaluation (the best in terms of perplexity on the validation set) log \u03bb opt thresh is marked with a green dashed line.", "labels": [], "entities": []}, {"text": "tional parameters \u00b5, \u03c3 and all the other network parameters (including biases of the DSVI-ARD layers).", "labels": [], "entities": []}, {"text": "Variational optimization is performed with the DSVI-ARD algorithm, which, in turn, only requires gradients of the log-likelihood and KL-divergence.", "labels": [], "entities": []}, {"text": "Therefore, overall model training is a standard gradient optimization of parameters based on backpropagation (specifically, BPTT in the RNN case) with negative ELBO as the loss function.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.8797311782836914}, {"text": "ELBO", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9948737025260925}]}, {"text": "For more efficient training we applied the KLcost annealing technique (.", "labels": [], "entities": []}, {"text": "The idea is to multiply the KL-term in ELBO by a variable weight, called the KL-weight, at training time.", "labels": [], "entities": [{"text": "ELBO", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7652817964553833}, {"text": "KL-weight", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.811636745929718}]}, {"text": "The weight gradually increases from zero to one during the first several epochs of training.", "labels": [], "entities": [{"text": "weight", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9639001488685608}]}, {"text": "This technique allows achieving better final performance of the model because such a train-: Language modeling experiments results.", "labels": [], "entities": []}, {"text": "We provide the number of parameters left after pruning (in millions) and the achieved compression ratios (in percents) of the whole network and the softmax layer alone along with the final quality (perplexity and accuracy) on the test set for each evaluated model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9989620447158813}]}, {"text": "The original (uncompressed) models quality is provided for comparison.", "labels": [], "entities": []}, {"text": "Figure 2: Distribution histogram of the prior logvariances log \u03bb ij obtained fora LSTM model with a DSVI-ARD softmax layer on the PTB dataset.", "labels": [], "entities": [{"text": "PTB dataset", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.9866195619106293}]}, {"text": "We provide the standard-scaled density to justify the usage of a log scale in. ing procedure can be considered as pre-training on data (when the data term in ELBO dominates) and then starting fair optimization of the true ELBO (when the KL-weight reaches one).", "labels": [], "entities": []}, {"text": "We used a simple linear KL-weight increasing strategy with a step selected as a hyperparameter.", "labels": [], "entities": [{"text": "KL-weight increasing", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7336238324642181}]}, {"text": "During the evaluation of our models we do not sample parameters as we do in the training phase but instead set the approximated posterior mean \u00b5 as DSVI-ARD layers weights.", "labels": [], "entities": []}, {"text": "Then we zero out the weights with the corresponding logarithms of prior variances lower than a certain threshold log \u03bb thresh (a hyperparameter selected on validation): This procedure essentially provides the desired sparsity as redundant weights are being literally removed from the network.", "labels": [], "entities": []}, {"text": "Each experiment was conducted as follows.", "labels": [], "entities": []}, {"text": "We trained several models for some number of epochs with different hyperparameter initialization (such as dropout rate, learning rate, etc.).", "labels": [], "entities": []}, {"text": "Then we picked the best model in terms of crossentropy (log-perplexity) on the validation set at the last training epoch.", "labels": [], "entities": []}, {"text": "We did not zero weights during evaluation at this phase, in other words, log \u03bb thresh = \u2212\u221e in equation.", "labels": [], "entities": []}, {"text": "After that, we started threshold selection for the picked model: we iterated over possible values of log \u03bb thresh from the \"leave-all\" to the \"remove-all\" extreme values and chose the one (denoted by log \u03bb opt thresh ) at which the best validation perplexity was obtained.", "labels": [], "entities": []}, {"text": "Finally, we evaluated the model on the test set using the chosen optimal threshold log \u03bb opt thresh . In our results we report the achieved compres- , perplexity and accuracy 2 on the test set.", "labels": [], "entities": [{"text": "compres-", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9791257083415985}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9994150400161743}]}, {"text": "concludes all the results obtained during our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language modeling experiments results. We provide the number of parameters left after pruning (in  millions) and the achieved compression ratios (in percents) of the whole network and the softmax layer alone  along with the final quality (perplexity and accuracy) on the test set for each evaluated model. The original  (uncompressed) models quality is provided for comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 264, "end_pos": 272, "type": "METRIC", "confidence": 0.9985548853874207}]}]}