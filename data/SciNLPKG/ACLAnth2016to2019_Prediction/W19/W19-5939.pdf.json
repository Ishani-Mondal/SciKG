{"title": [{"text": "Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we aim to predict dialogue success and user satisfaction as well as emotion on a turn level.", "labels": [], "entities": []}, {"text": "To achieve this, we investigate the use of spectrogram representations , extracted from audio files, in combination with several types of convolutional neural networks.", "labels": [], "entities": []}, {"text": "The experiments were performed on the Let's Go V2 database, comprising 5065 audio files and having labels for subjective and objective dialogue turn success, as well as the emotional state of the user.", "labels": [], "entities": [{"text": "Let's Go V2 database", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8212552070617676}]}, {"text": "Results show that by using only audio, it is possible to predict turn success with very high accuracy for all three labels (90%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9981491565704346}]}, {"text": "The best performing input representation were 1s long mel-spectrograms in combination with a CNN with a bottleneck architecture.", "labels": [], "entities": []}, {"text": "The resulting system has the potential to be used real-time.", "labels": [], "entities": []}, {"text": "Our results significantly surpass the state of the art for dialogue success prediction based only on audio.", "labels": [], "entities": [{"text": "dialogue success prediction", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.9142454266548157}]}], "introductionContent": [{"text": "Spoken Statistical Dialogue Systems (SDS) have gained much popularity in the last years, especially due to the widespread need for applications such as assisted living, phone banking (, intelligent virtual agents () and health care).", "labels": [], "entities": [{"text": "Spoken Statistical Dialogue Systems (SDS)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7442375676972526}, {"text": "phone banking", "start_pos": 169, "end_pos": 182, "type": "TASK", "confidence": 0.6915157437324524}]}, {"text": "An important part of an SDS is spoken language, which is used to communicate directly with the virtual agent in order to pose questions and reply to the agent output.", "labels": [], "entities": []}, {"text": "Ina modular spoken SDS system, the speech part is converted to text through Automatic Speech Recognition Systems (ASR), which is then analysed using Natural Language Processing (NLP) methods.", "labels": [], "entities": []}, {"text": "However, the audio part, which could be of low audio quality, is usually then discarded while the extracted text is fed forward to the SDS.", "labels": [], "entities": []}, {"text": "In our view (and this is an important part of our motivation), when looking at dialogue success prediction, this can be seen as a waste of possible resources, since the speech part can contain useful information regarding the emotional state of the user, or verbal cues which can indicate if the user is satisfied with the system performance.", "labels": [], "entities": [{"text": "dialogue success prediction", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.7555869023005167}]}, {"text": "The prediction or recognition of such cues can be very helpful for supporting a dialogue management system, which can make better assessments as to what the next steps should be.", "labels": [], "entities": []}, {"text": "Taking this thought one step further, we want to assess if it is possible to predict dialogue success based only on the audio, in order to find a light-weight, real-time method to manage the user expectations and, eventually, to build more efficient and user-friendly spoken SDS.", "labels": [], "entities": []}, {"text": "A final motivation of this work is that we wanted to experiment with spectrogram input representations and convolutional neural networks (CNNs) as classifiers.", "labels": [], "entities": []}, {"text": "Although there have been several examples of such uses for other topics, especially in image processing () and music information retrieval, this approach remains underrepresented in the area of dialogue success prediction.", "labels": [], "entities": [{"text": "image processing", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7523487508296967}, {"text": "music information retrieval", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.6836987733840942}, {"text": "dialogue success prediction", "start_pos": 194, "end_pos": 221, "type": "TASK", "confidence": 0.778341273466746}]}, {"text": "Therefore, our research closes this gap and attempts to evaluate how well such approaches can function for dialogue success prediction.", "labels": [], "entities": [{"text": "dialogue success prediction", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.9075619777043661}]}, {"text": "Considering related works, the use of neural networks in the wider area of modular SDS has been gaining some popularity the last years.", "labels": [], "entities": []}, {"text": "For example, neural networks have been utilised for dialogue state tracking.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.8568829894065857}]}, {"text": "use CNNs in order to track the user's goal over the whole dialogue without the use of handcrafted semantic dictionaries and achieve high accuracy for their task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9971218705177307}]}, {"text": "similarly employ recurrent neural networks to map the results of ASR directly to a dialogue state and also report high performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9827761650085449}]}, {"text": "Another approach uses deep reinforcement learning to discover dialogue states and outperform a standard baseline.", "labels": [], "entities": []}, {"text": "An additional deep reinforcement learning approach ( shows that using both RNNs and CNNs with turn level-features (non-audio) can be useful in predicting dialogue success.", "labels": [], "entities": []}, {"text": "Research from shows that deep learning can be useful in creating more natural conversation task-oriented SDS, whereas use CNNs and RNNs for dialogue topic tracking.", "labels": [], "entities": [{"text": "dialogue topic tracking", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.7217093904813131}]}, {"text": "As the listing of the related previous work shows, the use of neural networks with audio spectrograms or waveforms for the analysis of the audio part of the SDS and its consequent use for tasks such as dialogue success prediction has not been researched adequately.", "labels": [], "entities": [{"text": "dialogue success prediction", "start_pos": 202, "end_pos": 229, "type": "TASK", "confidence": 0.8444660703341166}]}, {"text": "Only a limited number of papers () exist which explore the possibility of dialogue success prediction using audio features extracted from speech paired with standard machine learning techniques such as support vector machines.", "labels": [], "entities": [{"text": "dialogue success prediction", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.92437611023585}]}, {"text": "These approaches have shown promising results, especially for creating away to reliably estimate the user satisfaction.", "labels": [], "entities": []}, {"text": "Additionally, they are able to do so in real-time or near-real-time and subsequently enable suitable next steps for the dialogue policy.", "labels": [], "entities": []}, {"text": "Moreover, the recent success of deep learning approaches for audio tasks suggests that using these can bring an advantage: By exploiting input representations such as spectrograms, the estimation of task success can take place even at an ever finer time resolution level (e.g., very short audio frames), providing the possibility for even faster processing and reaction.", "labels": [], "entities": []}, {"text": "Furthermore, data augmentation methods can provide a possibility to achieve higher accuracy rates.", "labels": [], "entities": [{"text": "accuracy rates", "start_pos": 83, "end_pos": 97, "type": "METRIC", "confidence": 0.9815427958965302}]}, {"text": "Since CNNs combined with audio spectrograms as input have been shown to provide very good results in a multitude of tasks (for example for tempo estimation ( and beat tracking), we choose to employ them for the creation of an experimental setup for dialogue success prediction.", "labels": [], "entities": [{"text": "tempo estimation", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.6763755232095718}, {"text": "beat tracking", "start_pos": 162, "end_pos": 175, "type": "TASK", "confidence": 0.6992827653884888}, {"text": "dialogue success prediction", "start_pos": 249, "end_pos": 276, "type": "TASK", "confidence": 0.9053629040718079}]}, {"text": "In that sense, we frame our task as an emotion recognition one: As dialogue success is expected to show a high correlation with user satisfaction, which in turn is closely related with the user's emotional state, we investigated similar works using neural networks for speech emotion recognition.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7781448662281036}, {"text": "speech emotion recognition", "start_pos": 269, "end_pos": 295, "type": "TASK", "confidence": 0.7414068977038065}]}, {"text": "Such works include those of who use a Long-Short-Term-Memory (LSTM) network on top of a CNN in order to extract information and consider contextual information from raw audio data (waveforms), outperforming existing systems for speech emotion recognition.", "labels": [], "entities": [{"text": "speech emotion recognition", "start_pos": 228, "end_pos": 254, "type": "TASK", "confidence": 0.7510964075724283}]}, {"text": "Similar work has been performed by, where audio waveforms are used in combination with a CNN followed by an LSTM for speech emotion recognition, achieving high results for arousal and valence.", "labels": [], "entities": [{"text": "speech emotion recognition", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.6443604330221812}]}, {"text": "In the work of, a CNN is used to predict emotions based on speech spectrograms fora virtual elderly companion agent with very good results.", "labels": [], "entities": []}, {"text": "create a multimodal framework with text and speech for emotion recognition.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.737479031085968}]}, {"text": "For the audio part, besides hand-crafted features, spectrograms with CNNs and LSTMs are used and fused with text features to predict 5 emotions and achieve better results than all other methods.", "labels": [], "entities": []}, {"text": "Another interesting method comes from, where spectrograms of different sizes are used as an input fora CNN, achieving very good results for 4 emotional states.", "labels": [], "entities": []}, {"text": "study the impacts of input features, signal length and speech type, using spectrogram or raw waveform input and CNNs, achieving state of the art results and reaching very useful conclusions for speech emotion recognition: input representation is not as important as the model architecture, which in turn is task and speech type specific.", "labels": [], "entities": [{"text": "speech emotion recognition", "start_pos": 194, "end_pos": 220, "type": "TASK", "confidence": 0.787041187286377}]}, {"text": "also achieve very good results in speech emotion recognition using a simple deep neural network and spectrograms as input.", "labels": [], "entities": [{"text": "speech emotion recognition", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.8104519248008728}]}, {"text": "A similar strategy is employed by for successful prediction of emotion, as well as gender and age on an utterance level, showing that even simple deep architectures can provide good results for speech emotion recognition.", "labels": [], "entities": [{"text": "prediction of emotion", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8875420292218527}, {"text": "speech emotion recognition", "start_pos": 194, "end_pos": 220, "type": "TASK", "confidence": 0.750941534837087}]}, {"text": "CNNs have also been used with success for general audio classification (, which is a broader task, hinting at the suitability of this architecture for the task at hand in this paper.", "labels": [], "entities": [{"text": "general audio classification", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.8027811646461487}]}, {"text": "For this paper, we decided -for the sake of simplicity and due to the not enormous size of the dataset -to resort to only CNNs and determine which architectures, input representation forms and parameters provide good classification results for this task.", "labels": [], "entities": []}, {"text": "Another reason for the use of CNNs is not only their aforementioned success in many tasks, but also the possibility to establish a better understanding of the suitability of this approach for the task of dialogue success prediction.", "labels": [], "entities": [{"text": "dialogue success prediction", "start_pos": 204, "end_pos": 231, "type": "TASK", "confidence": 0.7577744921048483}]}, {"text": "The latter is slightly different than speech emotion recognition per se because the user's emotional state is not the only factor that affects the final success label.", "labels": [], "entities": [{"text": "speech emotion recognition", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7158638636271158}]}, {"text": "Finally, this way we can establish a very fast and simple pipeline, which can also be used in a real-time setting to provide useful auxiliary information about the dialogue success, so as to inform the dialogue manager.", "labels": [], "entities": []}, {"text": "This approach is compared to a baseline, involving hand-crafted audio features as in (, which have been shown to provide satisfactory results.", "labels": [], "entities": []}, {"text": "Experiments are performed on the publicly available Let's Go V2 Database (, which contains three kind of labels (for objective and subjective dialogue success and the emotional state of the user, for more information see 2.3).", "labels": [], "entities": [{"text": "Let's Go V2 Database", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.5882319986820221}]}, {"text": "This paper is structured as follows: In the next section the used methods are presented in depth, whereas in section 3, the results of the classification are shown and discussed.", "labels": [], "entities": []}, {"text": "We close with conclusions and suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The spoken dialogue corpus used in this study is based on the the CMU Let's Go Bus Information System () (from this point on referred to as the Let's Go V2 dataset).", "labels": [], "entities": [{"text": "CMU Let's Go Bus Information System", "start_pos": 66, "end_pos": 101, "type": "DATASET", "confidence": 0.942157404763358}, {"text": "Let's Go V2 dataset", "start_pos": 144, "end_pos": 163, "type": "DATASET", "confidence": 0.5996544837951661}]}, {"text": "This has been developed by the university of Ulm in order to evaluate dialogue quality, user emotion and task success for an SDS which was used as an information system for bus itinerary search.", "labels": [], "entities": [{"text": "bus itinerary search", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.6310032904148102}]}, {"text": "The database contains 9083 system-user exchanges (to which we will refer as interactions in the following).", "labels": [], "entities": []}, {"text": "For our experiments, we kept a total of 5065 audio files for the interactions, for which all labels where available, so as to be able to compare between the results using the different label sets.", "labels": [], "entities": []}, {"text": "Each interaction has been rated with three labels.", "labels": [], "entities": []}, {"text": "The first is an emotional label, signifying the emotional state of the user.", "labels": [], "entities": []}, {"text": "The label has four levels, ranging from non-angry to very angry.", "labels": [], "entities": []}, {"text": "This label was assigned from the users themselves.", "labels": [], "entities": []}, {"text": "Another label shows the subjective dialogue success, dubbed IQ (Interaction Quality) in the corpus annotation (, indicating whether the user was satisfied with the interaction.", "labels": [], "entities": [{"text": "IQ (Interaction Quality)", "start_pos": 60, "end_pos": 84, "type": "METRIC", "confidence": 0.8591569304466248}]}, {"text": "This label ranges from satisfied to extremely unsatisfied and has five levels and was agreed on by three individual external raters.", "labels": [], "entities": []}, {"text": "We refer to it here as subjective label.", "labels": [], "entities": []}, {"text": "Finally, the objective labels indicate whether the goal of the dialogue was reached, i.e., the information looked for was actually provided by the system.", "labels": [], "entities": []}, {"text": "This label also exists on an interaction level and has two levels (successful or not).", "labels": [], "entities": []}, {"text": "In order to simplify the classification, we choose to create a binary model which results from taking the most highly ranked result of each label set as the positive label, and all the other results pooled together as the negative label.", "labels": [], "entities": []}, {"text": "In that way, it was possible to create an almost balanced dataset for the subjective labels (53% negative and 47% negative ones), but not for the other two labels sets (having correspondingly a distribution of 65% positive/35% negative for the emotional labels and 85% positive/15% negative for the objective samples).", "labels": [], "entities": []}, {"text": "Therefore, we then created a balanced version of the dataset for the emotional and the objective labels by taking the smaller class and randomly choosing as many examples for the other class.", "labels": [], "entities": []}, {"text": "The balanced subjective set contained 5065 samples, the balanced objective one 1146 and the balanced emotional one 3660 samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification results, training set, average accuracy over 3 folds and corresponding loss for 1 and 2 s  segments. All datasets are balanced, the prior is 0.5.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9520518779754639}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.928123414516449}]}, {"text": " Table 2: Classification results, validation set, average accuracy over 3 folds and corresponding loss for 1 and 2 s  segments. All datasets are balanced, the prior is 0.5.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9322654008865356}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.943466067314148}]}]}