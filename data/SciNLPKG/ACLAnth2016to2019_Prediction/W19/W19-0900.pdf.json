{"title": [{"text": "Proceedings of the IWCS Workshop Vector Semantics for Discourse and Dialogue", "labels": [], "entities": []}], "abstractContent": [{"text": "Vector models of meaning-both strictly distributional models derived directly from co-occurrence statistics, and embeddings learned using representations such as those learnt by a neural network-have revolutionised computational linguistics via their ability to reflect semantic similarities and regularities while providing flexibility to model dynamics and change.", "labels": [], "entities": []}, {"text": "However, while there has been much recent interest in extending these models from the level of words to that of larger phrases and sentences, with its own scalability and transparency problems, and there is relatively little progress in understanding how they might apply beyond the sentence: in the realm of discourse and dialogue.", "labels": [], "entities": []}, {"text": "This requires a shift in perspective, moving beyond the static word/sentence view of language as a jigsaw of pieces, to a dynamic perspective seeing language as a set of mechanisms for interaction in real time, encompassing a whole range of actions both sub-and supra-sentential.", "labels": [], "entities": []}, {"text": "At a sub-sentential level, dialogue is highly incremental: individuals can interrupt, extend, correct, or request clarification mid-turn, in effect constructing joint utterances without any sense of breakdown in the dialogue exchange.", "labels": [], "entities": []}, {"text": "And at the suprasentential level, we have the challenges not only of establishing coreference but of modelling the vast array of speech act effects in dialogue, and rhetorical effects in text discourse, and how they evolve.", "labels": [], "entities": []}, {"text": "The inaugural Vector Semantics for Discourse and Dialogue workshop brings together researchers using vector space methods for distributional semantics, word and sentence embeddings, and dialogue and discourse, to discuss these challenges and fill this gap.", "labels": [], "entities": []}, {"text": "We received 13 submissions, each receiving at least two reviews, and all contributions were accepted.", "labels": [], "entities": []}, {"text": "Invited Talks 1 Gemma Boleda (Universitat Pompeu Fabra) \"Talking about you: Deep Learning models of linguistic reference.\"", "labels": [], "entities": [{"text": "Invited Talks 1 Gemma Boleda (Universitat Pompeu Fabra)", "start_pos": 0, "end_pos": 55, "type": "DATASET", "confidence": 0.5785872459411621}]}, {"text": "We use language to talk about things.", "labels": [], "entities": []}, {"text": "For instance, within the TV series Friends, \"the brother of Monica Geller\" can be used to refer to the character Ross Geller.", "labels": [], "entities": []}, {"text": "Yet, most computational work on language lacks this connection to the reality that it is about: in Machine Translation, we get for instance \"le fr\u00e8re de Monica Geller\", with no link to the person it refers to.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.808608889579773}]}, {"text": "Instead, we want to model language in context.", "labels": [], "entities": []}, {"text": "Our hypothesis is that jointly learning to represent language and the entities referred to will improve computational models of both.", "labels": [], "entities": []}, {"text": "I will report on ongoing research testing this hypothesis with Deep Learning models.", "labels": [], "entities": []}, {"text": "2 Raquel Fern\u00e1ndez (University of Amsterdam) \"Analysing Language in Use with Vector Representations .\" Distributed vector representations have become ubiquitous in computational semantics.", "labels": [], "entities": []}, {"text": "Yet their application to aspects related to language interaction and dialogue is still limited.", "labels": [], "entities": []}, {"text": "In this talk, I will present recent work on two lines of research connected to this.", "labels": [], "entities": []}, {"text": "In the first part, I will focus on lexical meaning within online communities of practice.", "labels": [], "entities": []}, {"text": "In the second part, I will zoom into dyadic interaction and present a case study on using Recurrent Neural Networks on synthetic dialogue data to investigate incremental language understanding.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}