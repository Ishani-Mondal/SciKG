{"title": [{"text": "Toward automatic improvement of language produced by non-native language learners", "labels": [], "entities": []}], "abstractContent": [{"text": "It is important for language learners to practice speaking and writing in realistic scenarios.", "labels": [], "entities": []}, {"text": "The learners also need feedback on how to express themselves better in the new language.", "labels": [], "entities": []}, {"text": "In this paper, we perform automatic paraphrase generation on language-learner texts.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7744417786598206}]}, {"text": "Our goal is to devise tools that can help language learners write more correct and natural sounding sentences.", "labels": [], "entities": []}, {"text": "We use a pivoting method with a character-based neural machine translation system trained on subtitle data to paraphrase and improve learner texts that contain grammatical errors and other types of noise.", "labels": [], "entities": [{"text": "character-based neural machine translation", "start_pos": 32, "end_pos": 74, "type": "TASK", "confidence": 0.6887465864419937}]}, {"text": "We perform experiments in three languages: Finnish, Swedish and English.", "labels": [], "entities": []}, {"text": "We experiment with monolingual data as well as error-augmented monolingual and bilingual data in addition to parallel subtitle data during training.", "labels": [], "entities": []}, {"text": "Our results show that our baseline model trained only on parallel bilingual data sets is surprisingly robust to different types of noise in the source sentence, but introducing artificial errors can improve performance.", "labels": [], "entities": []}, {"text": "In addition to error correction, the results show promise for using the models to improve fluency and make language-learner texts more idiomatic.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is difficult to express oneself well in anew language.", "labels": [], "entities": []}, {"text": "Language students can learn grammar and vocabulary by filling in blanks in carefully prepared exercise sentences, but the students also need to practice speaking and writing in realistic scenarios.", "labels": [], "entities": []}, {"text": "When students write their own texts, they need corrective feedback.", "labels": [], "entities": []}, {"text": "We are interested in finding out to what extent computers can provide the necessary corrections, a task traditionally performed by human teachers.", "labels": [], "entities": []}, {"text": "However, human teachers are not always available and the students will want to carry on using the language outside the language class.", "labels": [], "entities": []}, {"text": "A tool helping language learners to produce more correct and more natural sounding expressions can enhance the learning process and encourage the students to use the new language in real situations.", "labels": [], "entities": []}, {"text": "In addition, findings since the 1980s suggest that language students that receive corrective feedback from computers rather than human teachers learn better and perceive the feedback as more neutral and encouraging.", "labels": [], "entities": []}, {"text": "In this paper, we study automatic paraphrasing methods on sentences produced by learners of three languages: Finnish, Swedish and English.", "labels": [], "entities": []}, {"text": "A paraphrase is an alternate way of expressing a meaning using other words than in the original utterance, such as the sentence pair: \"Why don't you watch your mouth?\"", "labels": [], "entities": []}, {"text": "\u2194 \"Take care what you say.\"", "labels": [], "entities": [{"text": "Take", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9610914587974548}]}, {"text": "Our goal is to discover to what extent we can improve the spelling, grammar and naturalness of text written by non-native language users.", "labels": [], "entities": []}, {"text": "We are not primarily interested in creating spell or grammar checkers, but we are interested in seeing whether it is possible to make \"noisy\" nonstandard sentences sound more natural.", "labels": [], "entities": [{"text": "spell or grammar checkers", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6129052340984344}]}, {"text": "Nonnative users maybe struggling to find fluent, natural sounding idiomatic expressions.", "labels": [], "entities": []}, {"text": "Paraphrase generation maybe away to \"translate\" sentences produced by language learners to sentences that are grammatically correct and sound more authentic to native speakers.", "labels": [], "entities": [{"text": "Paraphrase generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9279882907867432}]}, {"text": "In the present work, we do not set out to explicitly mark the errors made by the learners or suggest corrections to each of the errors separately.", "labels": [], "entities": []}, {"text": "Rather, for each sentence produced by the nonnative language user, we propose an alternative, corrected sentence.", "labels": [], "entities": []}, {"text": "The proposed sentence can differ significantly, or not at all, from the original sentence, depending on the quality of the original input.", "labels": [], "entities": []}, {"text": "By comparing the original and altered sentence, the language learner can identify errors and learn new expressions.", "labels": [], "entities": []}, {"text": "Our work is closely related to the field of grammatical error correction (GEC), although our focus is broader.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.7636413425207138}]}, {"text": "We are not only interested in grammar, but also in fluency and naturalness in a broader sense.", "labels": [], "entities": []}, {"text": "Furthermore, the concepts of error and correction are too narrow, in our opinion, since we are interested in better, or more effective, ways of conveying a message.", "labels": [], "entities": [{"text": "error", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9671303033828735}]}, {"text": "Nonetheless, from our point of view, GEC can provide us with useful data sets, methods, as well as evaluation guidelines and metrics.", "labels": [], "entities": [{"text": "GEC", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.637679398059845}]}, {"text": "introduce the NUCLE corpus, which was used in the CoNLL-2014 shared task on Grammatical Error Correction (.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.9470248520374298}, {"text": "Grammatical Error Correction", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.6186135411262512}]}, {"text": "NUCLE is an annotated corpus of English texts written by non-native English speakers.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9694930911064148}]}, {"text": "Twentyeight error types have been annotated manually, such as incorrect preposition or verb tense.", "labels": [], "entities": []}, {"text": "present JFLEG, an English parallel corpus incorporating fluency edits, in order not only to correct grammatical errors but also make the original text more native sounding.", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.7991035580635071}]}, {"text": "add Spanish translations to the JFLEG corpus.", "labels": [], "entities": [{"text": "JFLEG corpus", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9501015245914459}]}, {"text": "Grammatical error correction systems are typically evaluated using metrics that compare the corrections suggested by the system to a set of gold standard corrections.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7651457985242208}]}, {"text": "The MaxMatch (M 2 ) algorithm) matches the system output to the gold standard and computes the sequence of edit operations that has maximal overlap with the gold standard annotation.", "labels": [], "entities": []}, {"text": "This set of corrections is then scored using the F 1 measure.", "labels": [], "entities": [{"text": "corrections", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9443010687828064}, {"text": "F 1 measure", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9857822060585022}]}, {"text": "In the CoNLL-2014 shared task (), the M 2 scorer is revised.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.7619927326838175}, {"text": "M 2 scorer", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.907508373260498}]}, {"text": "In order to emphasize the precision of the suggested corrections twice as much as recall, the F 0.5 measure is used instead of F 1 . propose another metric, the I measure, which addresses some shortcomings of M 2 , such as not distinguising between not proposing an edit versus proposing the wrong edit.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9988343119621277}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9979922771453857}, {"text": "F 0.5 measure", "start_pos": 94, "end_pos": 107, "type": "METRIC", "confidence": 0.9770650466283163}, {"text": "F 1", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9611695408821106}, {"text": "I measure", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.8840006291866302}]}, {"text": "develop the Generalized Language Evaluation Understanding metric (GLEU) inspired by BLEU (), which seems to correlate better with the human ranking than the F and I measures.", "labels": [], "entities": [{"text": "Generalized Language Evaluation Understanding metric (GLEU)", "start_pos": 12, "end_pos": 71, "type": "METRIC", "confidence": 0.6295283883810043}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9961510896682739}]}, {"text": "When it comes to methods utilized in GEC, abroad range of approaches exist.", "labels": [], "entities": [{"text": "GEC", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9054632186889648}]}, {"text": "The participants in the CoNLL-2014 shared task ( propose systems based on classifiers (Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, averaged perceptron, maximum entropy), statistical language models, phrase-based and factored translation models, rule-based approaches, as well as combinations of these methods.", "labels": [], "entities": []}, {"text": "More recently, machine translation has been the predominant framework.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.807370126247406}]}, {"text": "Sentences containing errors are translated into corrected sentences.", "labels": [], "entities": []}, {"text": "Neural machine translation (NMT) generally requires large amounts of training data and has been shown to be sensitive to noisy data (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8680389821529388}]}, {"text": "Therefore approaches have been suggested where \"noise\" of the desired characteristics are incorporated in the training data, such that the system learns to remove the noise in the translation (.", "labels": [], "entities": []}, {"text": "Combining neural machine translation with statistical machine translation (SMT) is also claimed to produce better results ( ).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7204068104426066}, {"text": "statistical machine translation (SMT)", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.7466256668170294}]}, {"text": "Furthermore, GEC can be studied as a low-resource machine translation task, wherein addition to adding source-side noise other techniques are used: domain adaptation, a GEC-specific training-objective, transfer learning with monolingual data, and ensembling of independently trained GEC models and language models , noisy channel models) and unsupervised SMT (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7598350942134857}, {"text": "domain adaptation", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.7535682022571564}, {"text": "SMT", "start_pos": 355, "end_pos": 358, "type": "TASK", "confidence": 0.9316144585609436}]}, {"text": "We are interested in the Nordic languages Finnish and Swedish.", "labels": [], "entities": []}, {"text": "In addition, we perform experiments on English data.", "labels": [], "entities": []}, {"text": "We use neural machine translation to produce paraphrases of original sentences written by non-native language learners.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.8003161152203878}]}, {"text": "We are especially interested in the low-resource scenario, where in-domain, task-specific training data is scarce or non-existent, which is the case with Finnish and Swedish.", "labels": [], "entities": []}, {"text": "Our approach uses multilingual character-level NMT in combination with out-of-domain machine translation data to deal with the lack of task-specific data.", "labels": [], "entities": []}, {"text": "The data sets used for training and testing are described in Section 2.", "labels": [], "entities": []}, {"text": "Our machine translation model and training process are described in Section 3.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8064329624176025}]}, {"text": "We then turn to our experiments in Section 4.", "labels": [], "entities": []}, {"text": "The models are evaluated using qualitative analysis and manual annotation, and the results are described in Section 5.", "labels": [], "entities": []}, {"text": "Finally we conclude with a discussion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments on translation models trained in five different setups.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9659065008163452}]}, {"text": "All setups are built on our baseline model, which can translate from any of the three languages Finnish, Swedish or English to any of the same three languages.", "labels": [], "entities": []}, {"text": "Our test sets do not contain gold standard reference sentences, and therefore we cannot use automated metrics to evaluate our models.", "labels": [], "entities": []}, {"text": "Instead we will attempt to analyze the output of our models qualitatively and we also perform manual annotation of the generated sentences in two of the setups.", "labels": [], "entities": []}, {"text": "As expected, the baseline model (Section 4.1) performs poorly in a zero-shot translation scenario.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.586043119430542}]}, {"text": "The model is generally unable to produce a paraphrase with the same semantic content as the source sentence, and many of the produced sentences contain artifacts that can be traced back to one of the other languages, and the multilingual nature of the model.", "labels": [], "entities": []}, {"text": "Examples of such artifacts are producing mixed language or incorrectly translating false friends, such as: \"Siin\u00e4 on Teid\u00e4n perheen valokuva.\"", "labels": [], "entities": []}, {"text": "\u2192 \"Siin\u00e4 on er\u00e4\u00e4n familjen valokuva.\", \"Thank you fora gift.\"", "labels": [], "entities": []}, {"text": "\u2192 \"Thank you fora poison.\"", "labels": [], "entities": []}, {"text": "(The Swedish word for family has been inserted into a Finnish sentence, and the English word gift means poison in Swedish.)", "labels": [], "entities": []}, {"text": "Pivoting through another language works better as the model now only needs to translate between language pairs explicitly trained on.", "labels": [], "entities": []}, {"text": "Examples of the intermediate steps (pivot languages) and the final paraphrases can be seen in.", "labels": [], "entities": []}, {"text": "Many of the errors in the original source sentences have been corrected, although some sentences retain incorrect sentence structure or word forms from the source.", "labels": [], "entities": []}, {"text": "Distortion of the source sentence semantics can also be seen in some cases.", "labels": [], "entities": []}, {"text": "In the pivot scenario we also deal with the problem of compounding errors because of the two separate translation steps.", "labels": [], "entities": []}, {"text": "We now turn to the models trained on monolingual data in addition to bilingual parallel data.", "labels": [], "entities": []}, {"text": "A general trend emerges with all three models where monolingual data was used (Sections 4.2, 4.3, and 4.4).", "labels": [], "entities": []}, {"text": "The models will most of the time simply copy the source, including the errors present in the sentence.", "labels": [], "entities": []}, {"text": "While this is somewhat expected of the model where clones were used, it is surprising that even the model with paraphrase data exhibits this behavior.", "labels": [], "entities": []}, {"text": "The Opusparcus paraphrase corpus does not contain pairs with identical source and target sentences.", "labels": [], "entities": [{"text": "Opusparcus paraphrase corpus", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.9208774367968241}]}, {"text": "The error-augmented monolingual data seems to aid in correcting some typographical errors in the source sentences but does not correct bad inflection to the same extent.", "labels": [], "entities": []}, {"text": "The sentence structure of the generated paraphrase is generally identical to that of the source sentence: \"I wiss that you move the other plase and you can sleep very well\" \u2192 \"I wish that you move the other place and you can sleep very well\" Guided by the results from pivot-based methods and the attempts to use monolingual data in training, our final setup incorporates error-augmented bilingual data instead of monolingual data (Section 4.5).", "labels": [], "entities": []}, {"text": "A look at the generated phrases does not reveal consistent improvements over the baseline model, as shown in.", "labels": [], "entities": []}, {"text": "The baseline model already corrects most typos, and while there are examples of phrases where the baseline model generates an incorrect word or inflection and the error-augmented model a correct one, the converse is true in other cases.", "labels": [], "entities": []}, {"text": "We will compare the quality of the two models using manual annotations in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Illustration of the baseline pivoting method augmented with artificial errors for the three source  texts in Table 1. The Finnish and Swedish texts have been translated to English (in small font) and back  to Finnish and Swedish (in larger font). The English text has been translated to Finnish (small font) and  back to English (larger font).", "labels": [], "entities": []}, {"text": " Table 4: Manual annotation results for the Base- line model and the Baseline model with error- augmented bilingual data (+Errors). Proportion  [%] of generated sentences annotated as one of the  four categories.", "labels": [], "entities": [{"text": "Errors", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9205839037895203}]}]}