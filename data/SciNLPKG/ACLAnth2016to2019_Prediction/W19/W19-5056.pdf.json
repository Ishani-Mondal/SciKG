{"title": [{"text": "IITP at MEDIQA 2019: Systems Report for Natural Language Inference, Question Entailment and Question Answering", "labels": [], "entities": [{"text": "IITP at MEDIQA 2019", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8034296929836273}, {"text": "Natural Language Inference", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6430503726005554}, {"text": "Question Entailment", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7584859728813171}, {"text": "Question Answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8286730945110321}]}], "abstractContent": [{"text": "This paper presents the experiments accomplished as apart of our participation in the MEDIQA challenge, an (Abacha et al., 2019) shared task.", "labels": [], "entities": [{"text": "MEDIQA challenge", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.4840630739927292}]}, {"text": "We participated in all the three tasks defined in this particular shared task.", "labels": [], "entities": []}, {"text": "The tasks are viz. i. Natural Language Inference (NLI) ii.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7898719807465872}]}, {"text": "Recognizing Question En-tailment(RQE) and their application in medical Question Answering (QA).", "labels": [], "entities": [{"text": "Recognizing Question En-tailment(RQE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8020381977160772}, {"text": "medical Question Answering (QA)", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.7701115608215332}]}, {"text": "We submitted runs using multiple deep learning based systems (runs) for each of these three tasks.", "labels": [], "entities": []}, {"text": "We submitted five system results in each of the NLI and RQE tasks, and four system results for the QA task.", "labels": [], "entities": [{"text": "RQE", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.43633410334587097}, {"text": "QA task", "start_pos": 99, "end_pos": 106, "type": "TASK", "confidence": 0.7473466992378235}]}, {"text": "The systems yield encouraging results in all the three tasks.", "labels": [], "entities": []}, {"text": "The highest performance obtained in NLI, RQE and QA tasks are 81.8%, 53.2%, and 71.7%, respectively .", "labels": [], "entities": [{"text": "QA tasks", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.5041642040014267}]}], "introductionContent": [{"text": "Natural Language Processing (NLP) in biomedical domain is an essential and challenging task.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7448119620482127}]}, {"text": "With the availability of the data in electronic form it is possible to apply Artificial intelligence (AI), machine learning and deep learning technologies to build data driven automated tools.", "labels": [], "entities": []}, {"text": "These automated tools will be helpful in the field of medical science.", "labels": [], "entities": []}, {"text": "An ACL-BioNLP 2019 shared task, namely the MEDIQA challenge aims to attract further research efforts in NLI, RQE and their application in QA in medical domain.", "labels": [], "entities": []}, {"text": "The motivation of this shared task is in a need to develop relevant methods, techniques, and gold standard data for inference and recognizing question entailment in medical domain and their application to improve domain specific Information Retrieval (IR) and Question Answering (QA) systems.", "labels": [], "entities": [{"text": "recognizing question entailment", "start_pos": 130, "end_pos": 161, "type": "TASK", "confidence": 0.757018248240153}, {"text": "domain specific Information Retrieval (IR) and Question Answering (QA)", "start_pos": 213, "end_pos": 283, "type": "TASK", "confidence": 0.7538147018505976}]}, {"text": "The MEDIQA has defined several tasks related to Natural Language Inference, Question Entailment and Question Answering in medical domain.", "labels": [], "entities": [{"text": "MEDIQA", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8711017370223999}, {"text": "Natural Language Inference", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.6332705517609915}, {"text": "Question Entailment", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.783343106508255}, {"text": "Question Answering", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8614112138748169}]}, {"text": "We participated in all the three tasks defined in this shared task.", "labels": [], "entities": []}, {"text": "We offer multiple systems for each the tasks.", "labels": [], "entities": []}, {"text": "The workshop comprises of three tasks namely viz. i. Natural Language Inference (NLI): This task involves in identifying three inference relations between two sentences: i.e. Entailment, ii.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7412241895993551}]}, {"text": "Recognizing Question Entailment (RQE): This task focuses on identifying entailment relation between two questions in the context of QA.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.809919868906339}]}, {"text": "The definition of question entailment is as follows: \"a question A entails a question B if every answer to B is also a complete and/or partial answer to A\" (Abacha and Demner-Fushman, 2019) and iii.", "labels": [], "entities": [{"text": "question entailment", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.6986760795116425}]}, {"text": "Question Answering (QA): The goal of this task is to filter and improve the ranking of automatically retrieved answers.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8551897048950196}]}, {"text": "The existing medical QA system namely CHiQA is applied to generate the input ranks.", "labels": [], "entities": []}, {"text": "(. We participated in all the three tasks defined above and submitted the results.", "labels": [], "entities": []}, {"text": "Our proposed systems produce encouraging results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We submitted system results (runs) for all the three tasks.", "labels": [], "entities": []}, {"text": "In all these tasks, we make use of the dataset released as apart of this shared task.", "labels": [], "entities": []}, {"text": "In the following we discuss the dataset, evaluation results and the necessary analysis of the results obtained.", "labels": [], "entities": []}, {"text": "Data: In the NLI task, the training and test instances are having 14049 and 405 number of sentence pairs, respectively.", "labels": [], "entities": []}, {"text": "In task 2 (i.e. RQE), the training set is having 8588 number of pairs Task 1(NLI): In the first task, we propose five runs.", "labels": [], "entities": []}, {"text": "In all the tasks, we make use of either BERT or BioBERT models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9945611357688904}, {"text": "BioBERT", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.7540402412414551}]}, {"text": "We merge the input sentences pairs into a single sequence having maximum length of 128.", "labels": [], "entities": []}, {"text": "They are separated by a special token ().", "labels": [], "entities": []}, {"text": "The first token of every sequence is always a special classification token ().", "labels": [], "entities": []}, {"text": "The final hidden state (i.e., output of Transformer) corresponding to this token is used as the aggregate sequence representation for the classification tasks.", "labels": [], "entities": []}, {"text": "This final hidden state is a 768 dimensional vector (for bert-base) representing the input sentence pair.", "labels": [], "entities": []}, {"text": "This vector is fed subsequently into one or more feed-forward layers with soft-max activation at the end for 3-way classification (Entailment, Neutral or Contradiction).", "labels": [], "entities": []}, {"text": "The results for this task are shown in the.", "labels": [], "entities": []}, {"text": "We have discussed the way we can use a BERT model to perform sentence classification in medical domain.", "labels": [], "entities": [{"text": "BERT", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9816409945487976}, {"text": "sentence classification", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7606328427791595}]}, {"text": "It is observed that an absolute improvement of 5.4% inaccuracy has been achieved by using a BioBERT (pre-trained on PubMed abstracts) model in run 3 instead of using the original BERT-base-uncased model (Pre-trained on Wikipedia and Book Corpus (as used in run 2)).", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9875310063362122}, {"text": "BERT-base-uncased", "start_pos": 179, "end_pos": 196, "type": "METRIC", "confidence": 0.8043408393859863}, {"text": "Wikipedia and Book Corpus", "start_pos": 219, "end_pos": 244, "type": "DATASET", "confidence": 0.7618058174848557}]}, {"text": "The increase in result maybe the effect of BioBERT, because the other experimental setup remain same.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.989537239074707}]}, {"text": "The reason for using 1 feed forward layer at the end of BERT models in all the runs except the run 1 (no fine tuning), using only one feed forward layer was putting the model into an under fitting state.", "labels": [], "entities": [{"text": "BERT", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.497469425201416}]}, {"text": "While in case of fine tuning a large model, one feed forward is enough  as suggested by).", "labels": [], "entities": []}, {"text": "Up to run 3, we make use of 11232 sentence pairs for the training.", "labels": [], "entities": []}, {"text": "Those sentence pairs are same as the one used to train several models used in.", "labels": [], "entities": []}, {"text": "We use the remaining 2817 sentence pairs for validation.", "labels": [], "entities": []}, {"text": "The validation set accuracy is always around 3-4% higher than the test case accuracy for all the runs up to run 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.6073602437973022}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.6799322962760925}]}, {"text": "For getting the higher accuracy we combine all the 14049 pairs in the subsequent run.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995086193084717}]}, {"text": "We get the accuracy of 81.8 % which is the highest among all the proposed methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9998249411582947}]}, {"text": "As per our knowledge, in the official results of NLI task we stand at 12th position among the 17 official teams which participated for the NLI task.", "labels": [], "entities": []}, {"text": "Task 2 (RQE): In the second task i.e. task of Recognising Question Entailment, we propose five runs.", "labels": [], "entities": [{"text": "Recognising Question Entailment", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.9090358018875122}]}, {"text": "The results are shown in the.", "labels": [], "entities": []}, {"text": "It is interesting to note the variation inaccuracy for the different runs.", "labels": [], "entities": []}, {"text": "Siamese architecture performs much better here.", "labels": [], "entities": []}, {"text": "Another peculiarity is that fine tuning BERT hurts the performance while using pre-trained BERT embedding without fine tuning seems to be more useful.", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9800392985343933}, {"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9095265865325928}]}, {"text": "This is concluded by observing the results of run 2 and run 3.", "labels": [], "entities": []}, {"text": "In run 2, we used only pre-trained BERT embedding for () token for classification, whereas in run 3, we fine tuned the BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9421244263648987}, {"text": "BERT", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.8737531900405884}]}, {"text": "The highest accuracy is achieved by a Siamese Model consisting of 2 Bi-LSTMs with shared weights and a dense layers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992490410804749}]}, {"text": "In this task, 12 teams submitted their systems, and we stood the 10th position.", "labels": [], "entities": []}, {"text": "Task 3 (QA): In this task, we offer 4 runs to tackle the problem.", "labels": [], "entities": []}, {"text": "The results for this are shown in the: Results obtained in all the four runs for the QA Task (, where, MRR: Mean Reciprocal Rank predicted scores using BM25.", "labels": [], "entities": [{"text": "MRR", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9883864521980286}, {"text": "Mean Reciprocal Rank predicted scores", "start_pos": 108, "end_pos": 145, "type": "METRIC", "confidence": 0.9093791723251343}, {"text": "BM25", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.6756364703178406}]}, {"text": "The BM25 part of the system is same for all the runs.", "labels": [], "entities": [{"text": "BM25", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7679261565208435}]}, {"text": "In this task, participants are encouraged to compute the Mean Reciprocal Rank (MRR), Precision, and Spearman's Rank Correlation Coefficient as the evaluation measures in addition to Accuracy.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 57, "end_pos": 83, "type": "METRIC", "confidence": 0.963090310494105}, {"text": "Precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9966267347335815}, {"text": "Spearman's Rank Correlation Coefficient", "start_pos": 100, "end_pos": 139, "type": "METRIC", "confidence": 0.8465081334114075}, {"text": "Accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9987984895706177}]}, {"text": "We actually used BioBERT instead of original BERT from the run 3, which increases the accuracy with an absolute margin of 2.7% (65.1 to 67.8%).", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9591710567474365}, {"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9972634315490723}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9997238516807556}]}, {"text": "Using BioBERT we observe an improvement in MRR by 5.5%.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.7894929647445679}, {"text": "MRR", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.8380414247512817}]}, {"text": "Our best run with an accuracy of 71.7% attains the position of 6th among 10 teams in the official result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9996341466903687}]}], "tableCaptions": [{"text": " Table 1: Submission results of all the five runs for the  NLI task (Task-1)", "labels": [], "entities": []}, {"text": " Table 2: Submission results in all the five runs for the  RQE Task (Task-2)", "labels": [], "entities": [{"text": "RQE Task", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.6889746785163879}]}, {"text": " Table 3: Results obtained in all the four runs for the QA  Task (", "labels": [], "entities": [{"text": "QA  Task", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.6028258353471756}]}]}