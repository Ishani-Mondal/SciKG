{"title": [{"text": "An Exploration of Placeholding in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7070313294728597}]}], "abstractContent": [{"text": "Phrase-based machine translation provides the system developer with controls that enable fine-grained control over machine translation output.", "labels": [], "entities": [{"text": "Phrase-based machine translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6294108132521311}]}, {"text": "One approach to provide similar control in neural machine translation is placeholding (herein called masking), which replaces input tokens with masks which are replaced with the original input text in post-processing.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.669208695491155}]}, {"text": "But is this a good idea?", "labels": [], "entities": []}, {"text": "We undertake an exploration of masking in French-English and Japanese-English using Transformer archi-tectures.", "labels": [], "entities": []}, {"text": "We attempt to quantify whether (and where) masking is necessary with analysis of a baseline system, and then explore numerous parameterization of masking, including post-processing techniques for replacing the masks.", "labels": [], "entities": []}, {"text": "Our analysis shows this to be a thorny matter; masks solve some problems but are not perfectly translated themselves.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation generally produces higher quality output than phrase-based machine translation, especially in high-resource training settings and on in-domain data.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8089428742726644}, {"text": "phrase-based machine translation", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.6734484434127808}]}, {"text": "However, this improvement has come at the expense of a certain loss of control over how words get translated, since there is no longer a direct link between source words, their translation options, and the ordered decoder output.", "labels": [], "entities": []}, {"text": "While nearly everyone has considered this trade to be worthwhile, there lingers src En 2017 Bernard Arnault a gagn\u00e9...", "labels": [], "entities": []}, {"text": "mask En NUM NAME NAME a gagn\u00e9...", "labels": [], "entities": []}, {"text": "out In NUM NAME NAME a gagn\u00e9...", "labels": [], "entities": []}, {"text": "align In NUM 1 NAME 1 NAME 2 a gagn\u00e9...", "labels": [], "entities": []}, {"text": "detok In 2017 Bernard Arnault won...", "labels": [], "entities": []}, {"text": "a concern about the stability and dependency of NMT performance.", "labels": [], "entities": []}, {"text": "Input words are not all equally important, and there are many settings where one would be willing to sacrifice translation quality fora translation guarantee that certain input tokens be translated with perfect recall.", "labels": [], "entities": []}, {"text": "Common examples include prices on a product page, names and places in a news article, or contact and location information, and other data types, such as URLs.", "labels": [], "entities": []}, {"text": "One attempt to providing these guarantees is the use of placeholders (or masks, the term we will use in this paper), where input tokens in a category are replaced by a masked label token ().", "labels": [], "entities": []}, {"text": "These are then passed through to the output and replaced with the correct translation in post-processing.", "labels": [], "entities": []}, {"text": "This ostensibly guarantees that the input term (or its preferred translation) will correctly appear in the output, while at the same time restoring a capability that was easily handled in the old phrase-based paradigm.", "labels": [], "entities": []}, {"text": "At the same time, doing so reflects alack of confidence in the decoder to get this right.", "labels": [], "entities": []}, {"text": "This approach has not received much attention in the research literature.", "labels": [], "entities": []}, {"text": "In this paper, we look at this topic in more detail.", "labels": [], "entities": []}, {"text": "We focus our attention on copy or pass-through tokens, which is to say, input tokens that are not translated, but which are simply copied to the output sentence.", "labels": [], "entities": []}, {"text": "This includes many different token types that can be recognized by regular expressions (numbers, URLs, email addresses, and Emojis), as well as types for which we can provide a dictionary.", "labels": [], "entities": []}, {"text": "We ask the following questions: \u2022 Are translation guarantees necessary for these types?", "labels": [], "entities": []}, {"text": "\u2022 How effective is masking at producing these guarantees?", "labels": [], "entities": []}, {"text": "We experiment in both high resource (FR\u2192EN) and low-resource (JA\u2192EN) language settings.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Pre-tokenization data sizes in sentence and English words for FR-EN and JA-EN training (top), validation (middle),  and testing (bottom).", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.4592127203941345}]}, {"text": " Table 3: Entity counts across all data. For training data, the counts are \"true\" counts, that is, they are only counted for tokens  that appeared on both the source and target sides of the data. For test sets, the counts are produced by matching only against the  source. For most entity types, data is quite sparse.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores on test sets. The score take from Michel & Neubig is the system not trained on MTNT/train, since we did  not train on that in this paper, instead reserving it for analysis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990012049674988}, {"text": "MTNT/train", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.671027660369873}]}, {"text": " Table 5: FR-EN baseline recall scores (against the reference)  for each data type when decoding with the baseline system.  Hyphens (-) indicate no data being available, and italics in- dicate counts for which there were fewer than 50 instances", "labels": [], "entities": [{"text": "FR-EN baseline recall scores", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8076062947511673}]}, {"text": " Table 10: Demasked permutations for the attention-based  (left) and alignment-based (right) approaches. Mono/not de- notes whether the text of the decoder output (rows) and refer- ence (columns) was monotonic with respect to the input.", "labels": [], "entities": []}]}