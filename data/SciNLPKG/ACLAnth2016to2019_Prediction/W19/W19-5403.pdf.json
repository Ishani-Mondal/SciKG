{"title": [{"text": "Evaluation for MEDLINE Abstracts and Biomedical Terminologies", "labels": [], "entities": [{"text": "MEDLINE Abstracts", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.8445031046867371}]}], "abstractContent": [{"text": "In the fourth edition of the WMT Biomedical Translation task, we considered a total of six languages, namely Chinese (zh), English (en), French (fr), German (de), Portuguese (pt), and Spanish (es).", "labels": [], "entities": [{"text": "WMT Biomedical Translation task", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.8404739201068878}]}, {"text": "We performed an evaluation of automatic translations fora total of 10 language directions, namely, zh/en, en/zh, fr/en, en/fr, de/en, en/de, pt/en, en/pt, es/en, and en/es.", "labels": [], "entities": []}, {"text": "We provided training data based on MEDLINE abstracts for eight of the 10 language pairs and test sets for all of them.", "labels": [], "entities": [{"text": "MEDLINE abstracts", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.7826976180076599}]}, {"text": "In addition to that, we offered anew sub-task for the translation of terms in biomedical terminologies for the en/es language direction.", "labels": [], "entities": [{"text": "translation of terms", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8825621406237284}]}, {"text": "Higher BLEU scores (close to 0.5) were obtained for the es/en, en/es and en/pt test sets, as well as for the terminology sub-task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9993351101875305}]}, {"text": "After manual validation of the primary runs, some submis-* The author list is alphabetical and does not reflect the respective author contributions.", "labels": [], "entities": []}, {"text": "The task was coordinated by Mariana Neves.", "labels": [], "entities": []}, {"text": "sions were judged to be better than the reference translations, for instance, for de/en, en/es and es/en.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) holds the promise to unlock access to textual content in a wide range of languages.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8986157417297364}]}, {"text": "In the biomedical domain, the bulk of the literature is available in English, which provides two interesting applications for machine translation: first, providing patients, scientists and health professionals with access to the literature in their native language and second, assisting scientist and health professionals in writing reports in English, when it is not their primary language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7238541692495346}]}, {"text": "Furthermore, important health information can be found in the free text of electronic health records and social media.", "labels": [], "entities": []}, {"text": "As these sources are increasingly available to patients and health professionals, MT can be leveraged to widen access beyond language barriers.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9889398217201233}]}, {"text": "Other situations in the healthcare domain, such as emergency response communications, have expressed the need for translation technologies to improve patient-provider communication (.", "labels": [], "entities": [{"text": "emergency response communications", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.7001327872276306}]}, {"text": "However, the recurring conclusion of practical studies is that progress is still needed.", "labels": [], "entities": []}, {"text": "The goal of this shared task is to bring machine translation of biomedical text to a level of performance that can help with these medical challenges.", "labels": [], "entities": [{"text": "machine translation of biomedical text", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.8296961188316345}]}, {"text": "In recent years, many parallel corpora in the biomedical domain have been made available, which are valuable resources for training and evaluating MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 147, "end_pos": 149, "type": "TASK", "confidence": 0.993099570274353}]}, {"text": "Examples of such corpora include), Scielo ( , Full-Text Scientific Articles from Scielo (), MeSpEn, thesis and dissertations, and clinical trials.", "labels": [], "entities": [{"text": "MeSpEn", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8778448104858398}]}, {"text": "These corpora cover a variety of language pairs and document types, such as scientific articles, clinical trials, and academic dissertations.", "labels": [], "entities": []}, {"text": "Many previous efforts have addressed MT for the biomedical domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9965772032737732}]}, {"text": "Interesting previous work includes a comparison of performance in biomedical MT to Google Translate for English, French, German, and Spanish ().", "labels": [], "entities": [{"text": "biomedical MT", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.5504257380962372}]}, {"text": "Pecina et al. applied MT for the task of multilingual information retrieval in the medical domain ().", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9838511347770691}, {"text": "multilingual information retrieval", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6310778756936392}]}, {"text": "They compared various MT systems, including Moses, Google Translate, and Bing Translate.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9444935321807861}]}, {"text": "Later, Pecina et al. utilized domain adaptation of statistical MT for English,.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.8461915850639343}]}, {"text": "The field of MT has experienced considerable improvements in the performance of systems, and this is also the case for biomedical MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9890736937522888}, {"text": "biomedical MT", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.5728128552436829}]}, {"text": "Our more recent shared tasks show an increasing number of teams that relied on neural machine translation (NMT) to tackle the problem).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.7980166872342428}]}, {"text": "We found some commonalities in the work above.", "labels": [], "entities": []}, {"text": "On the one hand, clinical vocabularies are underdevelopment, as well as data sets based on scientific publications.", "labels": [], "entities": []}, {"text": "On the other hand, there is little or no work on languages that do not have typical Indo-European morphology, e.g. in the isolating direction (no Chinese), and in the agglutinating direction (no Hungarian, Turkish, Finnish, Estonian).", "labels": [], "entities": []}, {"text": "There is also little previous research in MT for electronic health records (EHR).", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9948691129684448}]}, {"text": "The translation of technical texts requires considerable specific knowledge, not only about linguistic rules, but also about the subject of the text that is being translated.", "labels": [], "entities": [{"text": "translation of technical texts", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8685419857501984}]}, {"text": "The advantage of terminology management can be seen in its important role in the process of acquiring, storing and applying linguistic and subject-specific knowledge related to the production of the target text.", "labels": [], "entities": [{"text": "terminology management", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.9348752498626709}]}, {"text": "Terminologies can also be extremely useful in data mining pipelines, where one might be interested in identifying specific terms or diseases, for example.", "labels": [], "entities": [{"text": "data mining pipelines", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7730412781238556}]}, {"text": "In addition, terminologies can be used to improve the quality of machine translation and help in the normalization of vocabulary use.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7428194284439087}]}, {"text": "Terminological resources in the field of biomedicine and clinic are of crucial importance for the development of natural language processing systems and language technologies in the field of health, among them the semantic network called Unified Medical Language System (UMLS).", "labels": [], "entities": []}, {"text": "This resource contains terminological subsets of a wide variety of subject areas and specialties such as health sciences, life sciences and pharmacology.", "labels": [], "entities": []}, {"text": "For instance, at present only 13% of the concepts included in UMLS have entries for Spanish, while the vast majority of concepts have an equivalent in English.", "labels": [], "entities": []}, {"text": "Therefore, one of the coverage expansion strategies is based on the translation of terms related to UMLS entries from English into Spanish.", "labels": [], "entities": [{"text": "coverage expansion", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8394029140472412}]}, {"text": "Over the past three years, the aim of the biomedical task at WMT has been to focus the attention of the community on health as a specialized domain for the application of MT (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8026241064071655}, {"text": "MT", "start_pos": 171, "end_pos": 173, "type": "TASK", "confidence": 0.9672825336456299}]}, {"text": "This forum has provided a unique opportunity to review existing parallel corpora in the biomedical domain and to further develop resources in language pairs such as English and Chinese, French, Spanish, Portuguese.", "labels": [], "entities": []}, {"text": "In this edition of the shared task, we continued this effort and we addressed five language pairs in two translation directions, as follows: Chinese/English (zh/en and en/zh), French/English (fr/en and en/fr), German/English (de/en and en/de), Portuguese/English (pt/en and en/pt), and Spanish/English (es/en and en/es).", "labels": [], "entities": []}], "datasetContent": [{"text": "For each language pair, we compared the submitted translations to the reference translations.", "labels": [], "entities": []}, {"text": "BLEU scores were calculated using the MULTI-EVAL tool and tokenization as provided in Moses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9874776601791382}]}, {"text": "For Chinese, character-level tokenization was used via a minor modification to the tool.", "labels": [], "entities": []}, {"text": "Although an ideal tokenization would take into account that Chinese words consist of a varying number of characters, achieving such an ideal tokenization requires a sophisticated dictionary () -including biomedical terms -and is beyond the scope of this shared task.", "labels": [], "entities": []}, {"text": "Further, using character-level tokenization for BLEU purposes is in accordance with current practice (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.6397845149040222}]}, {"text": "shows BLEU scores for all language pairs when considering all sentences in our test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9993784427642822}]}, {"text": "only considers the sentences that have been manually classified as being correctly aligned (cf. Section 2).", "labels": [], "entities": []}, {"text": "As expected, certain results improve considerably (by more than 10 BLEU points) when only considering the sentences that are correctly aligned.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9994162321090698}]}, {"text": "Most teams outperformed the three baselines, except the NRPU team's submissions for en/fr and fr/en.", "labels": [], "entities": [{"text": "NRPU", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9293974041938782}]}, {"text": "Baseline1, trained using Marian NMT, obtained results not far behind the best performing team, while the two other baselines were not very competitive.", "labels": [], "entities": [{"text": "Baseline1", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.918761134147644}, {"text": "Marian NMT", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8982847630977631}]}, {"text": "We rank the various runs according to the results that they obtained followed by a short discussion of the results with regard to the methods that they used.", "labels": [], "entities": []}, {"text": "All submitted runs from both ARC and UCAM teams outperformed our three baselines.", "labels": [], "entities": [{"text": "ARC and UCAM teams", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.7693619132041931}]}, {"text": "The runs from ARC were slightly superior to those from UCAM.", "labels": [], "entities": [{"text": "ARC", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9356589317321777}, {"text": "UCAM", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9579088687896729}]}, {"text": "Both teams used Transformer models but the ARC also used BERT multilingual embeddings.", "labels": [], "entities": [{"text": "ARC", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8825422525405884}, {"text": "BERT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.8367092609405518}]}, {"text": "We observed no significant difference between the submissions from team ARC but runs based on the ensemble of models from team UCAM (i.e. runs 2 and 3) obtained a higher score than their single best systems.", "labels": [], "entities": [{"text": "UCAM", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.9662853479385376}]}, {"text": "Results were similar to those for en/de: the runs from team ARC outperformed the runs from team UCAM.", "labels": [], "entities": [{"text": "ARC", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9363827109336853}, {"text": "UCAM", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.9789898991584778}]}, {"text": "Similarly, we observed no difference between the runs from team ARC and slightly higher scores for the runs based on ensemble systems from team UCAM.", "labels": [], "entities": [{"text": "ARC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.835713267326355}, {"text": "UCAM", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.9582412838935852}]}, {"text": "All submitted runs outperformed our baselines.", "labels": [], "entities": []}, {"text": "The best performing systems from the Talp upc, UCAM, and BSC teams were Transformer models, the one based on Marian NMT from the MT-UOC-UPF team, an finally the SMT Moses systems from UHH-DS.", "labels": [], "entities": [{"text": "Talp upc", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9393873810768127}, {"text": "UCAM", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.5240748524665833}, {"text": "BSC", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.604765772819519}, {"text": "UHH-DS", "start_pos": 184, "end_pos": 190, "type": "DATASET", "confidence": 0.9571022391319275}]}, {"text": "We did not observe significant differences between the various runs from single teams, except for run1 from Talp upc team (terminology-aware segmentation, shared source and target vocabularies and shared encoder-decoder embedding weights), which outperformed their other two runs.", "labels": [], "entities": [{"text": "Talp upc team", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.8999669750531515}]}, {"text": "All submitted runs outperformed our baselines.", "labels": [], "entities": []}, {"text": "As opposed to results for en/es, the Transformer system from the UCAM team slightly outperformed the one developed by the Talp upc team, which obtained a similar performance to the OpenNMT system from the BSC team.", "labels": [], "entities": [{"text": "UCAM team", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9680857360363007}, {"text": "Talp upc team", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.8852835694948832}, {"text": "BSC team", "start_pos": 205, "end_pos": 213, "type": "DATASET", "confidence": 0.9487077295780182}]}, {"text": "Baselines 2 and 3 were outperformed by all submitted runs, whereas baseline 1, which is trained using Marian, was only outperformed by team ARC, whose system uses the Transformer model.", "labels": [], "entities": [{"text": "ARC", "start_pos": 140, "end_pos": 143, "type": "DATASET", "confidence": 0.934463620185852}]}, {"text": "We observed no significant difference between the three runs from the ARC team.", "labels": [], "entities": [{"text": "ARC team", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9769667088985443}]}, {"text": "Similar to fr/en, baselines 2 and 3 were outperformed by all submitted runs, while baseline 1 was similar to the run from the KU team, which uses the Transformer model.", "labels": [], "entities": [{"text": "KU team", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.913159042596817}]}, {"text": "All runs from the ARC team outperformed our baseline 1.", "labels": [], "entities": [{"text": "ARC team", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.9628982841968536}]}, {"text": "Run1 from the ARC performed significantly better than the other two runs, although details about the difference between the runs do not seem to be available.", "labels": [], "entities": [{"text": "ARC", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9096558690071106}]}, {"text": "The run from the BSC team based on OpenNMT performed slightly better than baseline 1.", "labels": [], "entities": [{"text": "BSC team", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.961154043674469}, {"text": "OpenNMT", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9440115094184875}]}, {"text": "However, their performance was far superior to baselines 2 and 3, which were also trained using OpenNMT but only trained on the Medline training data.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9488948583602905}, {"text": "Medline training data", "start_pos": 128, "end_pos": 149, "type": "DATASET", "confidence": 0.9847924113273621}]}, {"text": "Results for en/pt from the BSC were almost 10 points higher than the ones for pt/en.", "labels": [], "entities": [{"text": "BSC", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7781839966773987}]}, {"text": "The run from the BSC team based on OpenNMT outperfomed with some difference the baseline based on Marian NMT, maybe because of the many resources that the team trained its system on.", "labels": [], "entities": [{"text": "BSC team", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.9510533511638641}, {"text": "OpenNMT", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9359387159347534}, {"text": "Marian NMT", "start_pos": 98, "end_pos": 108, "type": "DATASET", "confidence": 0.9491554200649261}]}, {"text": "Further, they were much superior to the baselines 2 and 3 also based on OpenNMT but only trained on the Medline training data.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9671884179115295}, {"text": "Medline training data", "start_pos": 104, "end_pos": 125, "type": "DATASET", "confidence": 0.9820218483606974}]}, {"text": "All submitted runs outperformed the only baseline that we prepared.", "labels": [], "entities": []}, {"text": "The three best-performing teams's submissions were Transformer models.", "labels": [], "entities": []}, {"text": "The system developed by the OOM team slightly outperformed ARC's submission.", "labels": [], "entities": [{"text": "OOM", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.7956292033195496}, {"text": "ARC", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.7972816824913025}]}, {"text": "Little difference in the results for the runs for the two teams was observed.", "labels": [], "entities": []}, {"text": "A significant difference, however, was observed between results from the ARC and OOM teams and the Transformer system of the KU team.", "labels": [], "entities": []}, {"text": "The Transformer-based system from team OOM significantly outperformed the transformer systems of team ARC.", "labels": [], "entities": []}, {"text": "The latter had a similar performance to the runs for the other two teams (Radiant and peace) for which we do not know the details.", "labels": [], "entities": []}, {"text": "presents the results of the automatic evaluation of the terminology test set.", "labels": [], "entities": []}, {"text": "The evaluation considered the accuracy of translation (on lower-cased terms), rather than BLEU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.999592125415802}, {"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9407117366790771}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9986425042152405}]}, {"text": "The choice of accuracy was due to the fact that the terms are usually very short and having at least one different word from the reference translation can lead to a complete different meaning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9991939663887024}]}, {"text": "For the Medline test sets, we performed manual evaluation of the primary runs, as identified by the participants, for all teams and language pairs.", "labels": [], "entities": [{"text": "Medline test sets", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9640069206555685}]}, {"text": "We carried out pairwise comparisons of translations taken either from a sample of the translations from the selected primary runs or the reference translations.", "labels": [], "entities": []}, {"text": "Specifically, sets of translation pairs, consisting of either two automatic translations fora given sentence (derived from submitted results), or one automatic translation and the reference translation fora sentence, were prepared for evaluation.", "labels": [], "entities": []}, {"text": "presents the primary runs that we considered from each team.", "labels": [], "entities": []}, {"text": "We performed a total of 62 validations of pairwise datasets.", "labels": [], "entities": []}, {"text": "We relied on human validators who were native speakers of the target languages and who were either members of the participating teams or colleagues from the research community.", "labels": [], "entities": []}, {"text": "We also preferred to use validators who were familiar enough with the source language so that the original text could be consulted in case of questions about the translations, and for most language pairs this was the case.", "labels": [], "entities": []}, {"text": "We carried out the so-called 3-way ranking task in our installation of the Appraise tool).", "labels": [], "entities": [{"text": "Appraise tool", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.6960888206958771}]}, {"text": "14 . For each pairwise dataset, we checked a total of 100 randomly-chosen sentence pairs.", "labels": [], "entities": []}, {"text": "The validation consisted of reading the two translation sentences (A and B) and choosing one of the options listed below: \u2022 A<B: the quality of translation B is higher than translation A; \u2022 A=B: both translations have similar quality; \u2022 A>B: the quality of translation A was higher than translation B; \u2022 Flag error: the translations do not seem to come from the same source sentence, probably due to errors in the corpus alignment.", "labels": [], "entities": [{"text": "Flag error", "start_pos": 304, "end_pos": 314, "type": "METRIC", "confidence": 0.7746767997741699}]}, {"text": "summarizes the manual evaluation for the Medline test sets.", "labels": [], "entities": [{"text": "Medline test sets", "start_pos": 41, "end_pos": 58, "type": "DATASET", "confidence": 0.9806403517723083}]}, {"text": "We did not perform manual evaluation for any of our baselines.", "labels": [], "entities": []}, {"text": "We ranked the runs and reference translations among themselves based on the number of times that one validation was carried out by the evaluators.", "labels": [], "entities": []}, {"text": "When the superiority of a team (or reference translation) over another team was not very clear, we decided to put both of them together in a block without the \"lower than\" sign (<) between them.", "labels": [], "entities": []}, {"text": "However, in these situations, the items are listed in ascending order of possible superiority in relation to the others.", "labels": [], "entities": []}, {"text": "We discuss differences that we found in the discussion of the results for each language pair below.", "labels": [], "entities": []}, {"text": "The reference translations and the runs from teams ARC and UCAM were of similar quality and we did not observe huge differences between them.", "labels": [], "entities": [{"text": "ARC", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9754472374916077}, {"text": "UCAM", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.7627918124198914}]}, {"text": "For this reason, we have grouped them into a single block, ordering them according to increasing performance.", "labels": [], "entities": []}, {"text": "The UCAM team's submission was seen to be marginally better than the reference translations (33 vs. 23).", "labels": [], "entities": [{"text": "UCAM team", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9651471972465515}]}, {"text": "We did not observe any differences in the respective order of teams compared to that of the automatic evaluation.", "labels": [], "entities": []}, {"text": "The reference translation was clearly superior to the runs from the ARC and UCAM teams (41 vs. 19, and 44 vs. 16, respectively).", "labels": [], "entities": [{"text": "ARC", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9748782515525818}, {"text": "UCAM", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.5393751263618469}]}, {"text": "The translations from the ARC submission were more frequently judged better than the ones from the UCAM team (37 vs. 16).", "labels": [], "entities": [{"text": "ARC submission", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.7861375212669373}, {"text": "UCAM team", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.9601081907749176}]}, {"text": "While we found no significant difference in the BLEU scores for teams ARC and UCAM, the manual evaluation showed that translations from team ARC were of superior quality to those of team UCAM.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9993546605110168}, {"text": "ARC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9674029350280762}, {"text": "UCAM", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9025502800941467}, {"text": "ARC", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.9185032844543457}, {"text": "UCAM", "start_pos": 187, "end_pos": 191, "type": "DATASET", "confidence": 0.9622948169708252}]}, {"text": "The runs from the MT-UOC-UPF and BSC teams were judged as of similar quality to the reference translations, while the ones from Talp upc and UCAM were deemed superior to the reference translations.", "labels": [], "entities": [{"text": "MT-UOC-UPF and BSC teams", "start_pos": 18, "end_pos": 42, "type": "DATASET", "confidence": 0.7384768128395081}, {"text": "Talp upc", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9602842628955841}, {"text": "UCAM", "start_pos": 141, "end_pos": 145, "type": "DATASET", "confidence": 0.6441668272018433}]}, {"text": "The manual validation did not indicate much difference between runs from teams BSC, Talp upc and UCAM.", "labels": [], "entities": [{"text": "BSC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.9878829121589661}, {"text": "Talp upc", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9070968329906464}, {"text": "UCAM", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.9028162956237793}]}, {"text": "The ranking of the teams did not change significantly between that of the automatic evaluation.", "labels": [], "entities": []}, {"text": "The reference translations were clearly superior to the runs from the KU and the NRPU teams, however, they were found only marginally superior to the ARC run.", "labels": [], "entities": [{"text": "KU", "start_pos": 70, "end_pos": 72, "type": "DATASET", "confidence": 0.8401716947555542}, {"text": "NRPU", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.8494372367858887}, {"text": "ARC run", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.9375781118869781}]}, {"text": "We therefore decided to put the ARC runs and reference translations in a single block.", "labels": [], "entities": []}, {"text": "As for the comparison of the ARC runs to the KU and NRPU runs, superiority of ARC was higher when compared to the NRPU team (82 vs. 2) than for team KU (42 vs. 21).", "labels": [], "entities": [{"text": "NRPU runs", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.8681776523590088}, {"text": "ARC", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.7116140723228455}, {"text": "NRPU", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.927849531173706}]}, {"text": "Indeed, the translations from the KU team were validated as far superior (73 vs. 9) to team NRPU.", "labels": [], "entities": [{"text": "KU team", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.8832460045814514}, {"text": "NRPU", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9715970754623413}]}, {"text": "We did not observe any differences in the ranking of teams with respect to the automatic evaluation.", "labels": [], "entities": []}, {"text": "We could not rank the runs from the various teams because of inconsistencies when comparing results from the various pairwise validations.", "labels": [], "entities": []}, {"text": "For instance, the translations from the OOM team were judged better than the the reference translations, and the latter better that the ones from the ARC team.", "labels": [], "entities": [{"text": "OOM team", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.748902827501297}, {"text": "ARC team", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.8305257558822632}]}, {"text": "However, the translation from the ARC team were considered better than the ones from the OOM team.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9512595534324646}, {"text": "ARC team", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.8572514355182648}, {"text": "OOM team", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8127550184726715}]}, {"text": "We also found differences in the rankings found in the automatic validation.", "labels": [], "entities": []}, {"text": "For instance, the team that obtained the lowest BLEU scores (peace), had their translation judged to be as good as the ones from the Radiant and OOM teams, two of the teams that obtained high BLEU scores.", "labels": [], "entities": [{"text": "BLEU scores (peace)", "start_pos": 48, "end_pos": 67, "type": "METRIC", "confidence": 0.8033274292945862}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9947172999382019}]}, {"text": "The translations from the BSC team were validated as slightly superior) to the reference translations.", "labels": [], "entities": [{"text": "BSC team", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9479966759681702}]}, {"text": "We therefore grouped both of them in a single block.", "labels": [], "entities": []}, {"text": "The reference translations were judged as of similar quality to the ones from the Talp upc teams, followed by the translations from the BSC and UCAM teams.", "labels": [], "entities": [{"text": "Talp upc teams", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.9559340675671896}, {"text": "BSC and UCAM teams", "start_pos": 136, "end_pos": 154, "type": "DATASET", "confidence": 0.8009181171655655}]}, {"text": "The only difference to the ranking from the automatic evaluation was that the runs from the Talp upc were considered better than those from the UCAM team while the latter obtained a higher BLEU score.: Overview of the primary runs that were considered for manual validation.", "labels": [], "entities": [{"text": "Talp upc", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.9077944457530975}, {"text": "UCAM team", "start_pos": 144, "end_pos": 153, "type": "DATASET", "confidence": 0.9743821322917938}, {"text": "BLEU score.", "start_pos": 189, "end_pos": 200, "type": "METRIC", "confidence": 0.9845779836177826}]}, {"text": "The last columns shows the number of runs that we validated for each team.", "labels": [], "entities": []}, {"text": "The last rows in the tables show the total number of runs and of pairwise combinations between runs and the reference translations.", "labels": [], "entities": []}, {"text": "The reference translations were consistently validated as superior to the one from team NRPU's submissions, whereas the ones from team ARC were judged to be better than the reference translations.", "labels": [], "entities": [{"text": "NRPU", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.9379860162734985}, {"text": "ARC", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.8942522406578064}]}, {"text": "The reference translations were validated as slightly superior (29 vs. 24) to the ones from team BSC.", "labels": [], "entities": [{"text": "BSC", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9696977734565735}]}, {"text": "Therefore, we grouped both of them in a single block.", "labels": [], "entities": []}, {"text": "Only the translation from the OOM team, the runs that obtained the highest BLEU scores, were judged as of similar quality to the reference translations.", "labels": [], "entities": [{"text": "OOM team", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.8962197005748749}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9989340901374817}]}, {"text": "The only main difference compared to the ranking from the automatic translation was with regard to team peace's submission, which obtained the lowest BLEU score, but for which the translations were ranked higher than the ones from the KU team and of similar quality to the ARC team according to the manual evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 150, "end_pos": 160, "type": "METRIC", "confidence": 0.9821551144123077}, {"text": "ARC team", "start_pos": 273, "end_pos": 281, "type": "DATASET", "confidence": 0.8434217572212219}]}], "tableCaptions": [{"text": " Table 1: Number of documents, sentences, and terms in the training and test sets.", "labels": [], "entities": []}, {"text": " Table 3: List of the participating teams.", "labels": [], "entities": []}, {"text": " Table 4: Overview of the submissions from all teams and test sets. We identify submissions to the MEDLINE test  sets with an \"M\" and to the terminology test set with a \"T\". The value next to the letter indicates the number of  runs for the corresponding test set, language pair, and team.", "labels": [], "entities": [{"text": "MEDLINE test  sets", "start_pos": 99, "end_pos": 117, "type": "DATASET", "confidence": 0.9638160665829977}, {"text": "terminology test set", "start_pos": 141, "end_pos": 161, "type": "DATASET", "confidence": 0.7192461689313253}]}, {"text": " Table 6: BLEU scores when considering all sentences in the test sets. Runs are presented in alphabetical order of the team's name, while the baseline results are shown at the  bottom of the table. * indicates the primary run, as indicated by the participants, in the case of multiple runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988372921943665}]}, {"text": " Table 7: BLEU scores when considering only the correctly aligned sentences in the test sets. Runs are presented in alphabetical order of the team's name, while the baseline  results are shown at the bottom of the table. * indicates the primary run, as indicated by the participants, in the case of multiple runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989581108093262}]}, {"text": " Table 9: Overview of the primary runs that were considered for manual validation. The last columns shows the  number of runs that we validated for each team. The last rows in the tables show the total number of runs and of  pairwise combinations between runs and the reference translations.", "labels": [], "entities": []}, {"text": " Table 10: Results for the manual validation for the Medline test sets. Values are absolute numbers (not percent- ages). They might not sum up to 100 due to the skipped sentences.", "labels": [], "entities": [{"text": "Medline test sets", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.9601968924204508}]}]}