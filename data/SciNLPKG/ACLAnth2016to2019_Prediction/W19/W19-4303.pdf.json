{"title": [{"text": "Generative Adversarial Networks for text using word2vec intermediaries", "labels": [], "entities": [{"text": "Generative Adversarial Networks", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7689358194669088}]}], "abstractContent": [{"text": "Generative adversarial networks (GANs) have shown considerable success, especially in the realistic generation of images.", "labels": [], "entities": []}, {"text": "In this work, we apply similar techniques for the generation of text.", "labels": [], "entities": [{"text": "generation of text", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8479500214258829}]}, {"text": "We propose a novel approach to handle the discrete nature of text, during training, using word embeddings.", "labels": [], "entities": []}, {"text": "Our method is agnostic to vocabulary size and achieves competitive results relative to methods with various discrete gradient estimators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Generation (NLG) is often regarded as one of the most challenging tasks in computation.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7648233970006307}]}, {"text": "It involves training a model to do language generation fora series of abstract concepts, represented either in some logical form or as a knowledge base.", "labels": [], "entities": []}, {"text": "Goodfellow introduced generative adversarial networks (GANs) () as a method of generating synthetic, continuous data with realistic attributes.", "labels": [], "entities": [{"text": "generative adversarial networks (GANs)", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.8158343136310577}]}, {"text": "The model includes a discriminator network (D), responsible for distinguishing between the real and the generated samples, and a generator network (G), responsible for generating realistic samples with the goal of fooling the D.", "labels": [], "entities": []}, {"text": "This setup leads to a minimax game where we maximize the value function with respect to D, and minimize it with respect to G.", "labels": [], "entities": []}, {"text": "The ideal optimal solution is the complete replication of the real distributions of data by the generated distribution.", "labels": [], "entities": []}, {"text": "GANs, in this original setup, often suffer from the problem of mode collapse -where the G manages to find a few modes of data that resemble real data, using them consistently to fool the D.", "labels": [], "entities": [{"text": "GANs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.6922697424888611}]}, {"text": "Workarounds for this include updating the loss function to incorporate an element of multidiversity.", "labels": [], "entities": []}, {"text": "An optimal D would provide G with the information to improve, however, if at the current stage of training it is not doing that yet, the gradient of G vanishes.", "labels": [], "entities": []}, {"text": "Additionally, with this loss function, there is no correlation between the metric and the generation quality, and the most common workaround is to generate targets across epochs and then measure the generation quality, which can bean expensive process.", "labels": [], "entities": []}, {"text": "W-GAN () rectifies these issues with its updated loss.", "labels": [], "entities": [{"text": "W-GAN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9715607166290283}]}, {"text": "Wasserstein distance is the minimum cost of transporting mass in converting data from distribution Pr to P g . This loss forces the GAN to perform in a min-max, rather than a max-min, a desirable behavior as stated in, potentially mitigating modecollapse problems.", "labels": [], "entities": [{"text": "Wasserstein distance", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.8975925743579865}]}, {"text": "The loss function is given by: where Dis the set of 1-Lipschitz functions and P g is the model distribution implicitly defined by\u02dcx by\u02dc by\u02dcx = G(z), z \u223c p(z).", "labels": [], "entities": []}, {"text": "A differentiable function is 1-Lipschtiz iff it has gradients with norm at most 1 everywhere.", "labels": [], "entities": []}, {"text": "Under an optimal D minimizing the value function with respect to the generator parameters minimizes the W(p r , pg ), where Wis the Wasserstein distance, as discussed in.", "labels": [], "entities": [{"text": "W", "start_pos": 104, "end_pos": 105, "type": "METRIC", "confidence": 0.9722214937210083}]}, {"text": "To enforce the Lipschitz constraint, the authors propose clipping the weights of the gradient within a compact space.", "labels": [], "entities": []}, {"text": "( show that even though this setup leads to more stable training compared to the original GAN loss function, the architecture suffers from exploding and vanishing gradient problems.", "labels": [], "entities": []}, {"text": "They introduce the concept of gradient penalty as an alternative way to enforce the Lipschitz constraint, by penalizing the gradient norm directly in the loss.", "labels": [], "entities": []}, {"text": "The loss function is given by: x are random samples drawn from P x , and L critic is the loss defined in Equation 1.", "labels": [], "entities": [{"text": "L critic", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9644971489906311}]}, {"text": "Empirical results of GANs over the past year or so have been impressive.", "labels": [], "entities": [{"text": "GANs", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.9730409979820251}]}, {"text": "GANs have gotten stateof-the-art image-generation results on datasets like ImageNet ( and LSUN.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9361200332641602}, {"text": "LSUN", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.9546931385993958}]}, {"text": "Such GANs are fully differentiable and allow for back-propagation of gradients from D through the samples generated by G. However, if the data is discrete, as, in the case of text, the gradient cannot be propagated back from D to G, without some approximation.", "labels": [], "entities": []}, {"text": "Workarounds to this problem include techniques from reinforcement learning (RL), such as policy gradients to choose a discrete entity and reparameterization to represent the discrete quantity in terms of an approximated continuous function).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7172064661979676}]}], "datasetContent": [{"text": "The Chinese Poetry dataset, introduced by) presents simple 4-line poems in Chinese with a length of 5 or 7 tokens (henceforth referred to Poem 5 and Poem 7 respectively).", "labels": [], "entities": [{"text": "Chinese Poetry dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9375142852465311}]}, {"text": "Following previous work by and (, we treat every line as a separate data point.", "labels": [], "entities": []}, {"text": "We modify the Poem 5 dataset to add start and end of tokens, to ensure the model captures (at least) that pattern through the corpus (given our lack of Chinese knowledge).", "labels": [], "entities": [{"text": "Poem 5 dataset", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.7562215924263}]}, {"text": "This setup allows us to use identical architectures for both the Poem 5 and Poem 7 datasets.", "labels": [], "entities": [{"text": "Poem 5 and Poem 7 datasets", "start_pos": 65, "end_pos": 91, "type": "DATASET", "confidence": 0.7070066134134928}]}, {"text": "We also modify the GAN2vec loss function with the objective in Eq.", "labels": [], "entities": [{"text": "GAN2vec loss function", "start_pos": 19, "end_pos": 40, "type": "METRIC", "confidence": 0.6243391732374827}, {"text": "Eq", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.8687568306922913}]}, {"text": "2, and report the results below.", "labels": [], "entities": []}, {"text": "CMU-SE 3 is a pre-processed collections of simple English sentences, consisting of 44,016 sentences and a vocabulary of 3,122-word types.", "labels": [], "entities": [{"text": "CMU-SE 3", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8812198042869568}]}, {"text": "For purposes of our experiments here, we limit the number of sentences to 7, chosen empirically to capture a significant share of the examples.", "labels": [], "entities": []}, {"text": "For the sake of simplicity in these experiments, for the real corpus, sentences with fewer than seven words are ignored, and those with more than seven words are cut-off at the seventh word.", "labels": [], "entities": []}, {"text": "presents sentences generated by the original GAN2vec model.", "labels": [], "entities": []}, {"text": "Appendix A.2 includes additional examples.", "labels": [], "entities": [{"text": "Appendix A.2", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8713926672935486}]}, {"text": "While this is a small subset of randomly sampled examples, on a relatively simple dataset, the text quality appears competitive to the work of () on this corpus.", "labels": [], "entities": []}, {"text": "<s> will you have two moment ? </s> <s> how is the another headache ? </s> <s> what sin the friday food ? ?", "labels": [], "entities": []}, {"text": "</s> <s> id like to fax a newspaper . </s> GAN2vec <s> i dropped my camera . </s> <s> i 'd like to transfer it <s> i 'll take that car , <s> prepare whisky and coffee , please: Example sentences generated by the original GAN2vec.", "labels": [], "entities": [{"text": "GAN2vec", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9568081498146057}, {"text": "GAN2vec", "start_pos": 221, "end_pos": 228, "type": "DATASET", "confidence": 0.9670438170433044}]}, {"text": "We report example sentences from and from our GAN2vec model on CMU-SE.", "labels": [], "entities": [{"text": "CMU-SE", "start_pos": 63, "end_pos": 69, "type": "DATASET", "confidence": 0.9405560493469238}]}, {"text": "The Coco Dataset is used to train and generate synthetic data as a common dataset for all the best-performing models over the last two years.", "labels": [], "entities": [{"text": "Coco Dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9384942054748535}]}, {"text": "In Texygen, the authors set the sentence length to 20.", "labels": [], "entities": [{"text": "Texygen", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9718962907791138}]}, {"text": "They train an oracle that generates 20,000 sentences, with one half used as the training set and the rest as the test set.", "labels": [], "entities": []}, {"text": "All the models in this benchmark are trained for 180 epochs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Examples of sentences generated by the conditional GAN. We report examples of sentences with our  model conditioned on sentence type, i.e., question or sentence.", "labels": [], "entities": []}, {"text": " Table 5: Model BLEU scores on Train Set of the Coco  Dataset (higher is better).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9992473125457764}, {"text": "Coco  Dataset", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9680289328098297}]}, {"text": " Table 6: Model BLEU scores on Test Set of the Coco  Dataset (higher is better).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9988828301429749}, {"text": "Coco  Dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9499337375164032}]}, {"text": " Table 7: Self BLEU scores of the models trained on the  Coco dataset (lower is better).", "labels": [], "entities": [{"text": "Self", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.915485680103302}, {"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.8923712372779846}, {"text": "Coco dataset", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9536891281604767}]}]}