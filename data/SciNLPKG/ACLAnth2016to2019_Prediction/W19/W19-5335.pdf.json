{"title": [{"text": "Findings of the 2016 Conference on Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7005055695772171}]}], "abstractContent": [{"text": "The paper describes the development process of Tilde's NMT systems for the WMT 2019 shared task on news translation.", "labels": [], "entities": [{"text": "WMT 2019 shared task on news translation", "start_pos": 75, "end_pos": 115, "type": "TASK", "confidence": 0.6163443412099566}]}, {"text": "We trained systems for the English-Lithuanian and Lithuanian-English translation directions in constrained and unconstrained tracks.", "labels": [], "entities": []}, {"text": "We build upon the best methods of the previous year's competition and combine them with recent advancements in the field.", "labels": [], "entities": []}, {"text": "We also present anew method to ensure source domain adherence in back-translated data.", "labels": [], "entities": []}, {"text": "Our systems achieved a shared first place inhuman evaluation..", "labels": [], "entities": []}, {"text": "findings of the 2017 conference on machine translation (wmt17)..", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.795015424489975}]}, {"text": "findings of the 2018 conference on machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.832091212272644}]}], "introductionContent": [{"text": "Since the paradigm-shifting success of neural machine translation (NMT) systems at the 2016 Conference on Machine Translation (WMT) (, NMT methods and neural network architectures applied in NMT have been annually improved.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.8602678676446279}, {"text": "2016 Conference on Machine Translation (WMT)", "start_pos": 87, "end_pos": 131, "type": "TASK", "confidence": 0.7883770912885666}]}, {"text": "In 2016, the best-performing systems were based on recurrent neural networks with gated recurrent units (GRU) (.", "labels": [], "entities": []}, {"text": "In 2017, deep GRU models ( and models based on shallow multiplicative long short-term memory units (MLSTM; () allowed achieving the best results (Bojar et al., a).", "labels": [], "entities": []}, {"text": "In 2018, the majority of best-performing systems were based on self-attentional () (Transformer) models (.", "labels": [], "entities": []}, {"text": "A year has passed, and the majority of bestperforming systems submitted to the shared task on news translation of WMT 2019 are still based on Transformer networks.", "labels": [], "entities": [{"text": "news translation of WMT 2019", "start_pos": 94, "end_pos": 122, "type": "TASK", "confidence": 0.6931831419467926}]}, {"text": "However, improvements are evident in other areas (e.g., usage of document-level context, very deep models, distillation by ensemble teachers, etc.)", "labels": [], "entities": []}, {"text": "Quite a few of 1 http://matrix.statmt.org the submissions indicate that substantial amounts of computational resources may have been utilised in order to achieve such results.", "labels": [], "entities": []}, {"text": "As we do not have access to large GPU clusters, our strategy for participating at the shared task on news translation of the 2019 Conference on Machine Translation was comprised of combining different methods that showed promising results in scientific publications published in 2018, and analysing whether the methods allowed increasing the overall quality of NMT systems when training NMT models using just modest hardware (with access to one or two graphical processing units) and with the goal of producing models suitable for production.", "labels": [], "entities": [{"text": "news translation of the 2019 Conference on Machine Translation", "start_pos": 101, "end_pos": 163, "type": "TASK", "confidence": 0.8593238128556145}]}, {"text": "In our experiments, we investigated methods for corpora filtering (the Tilde MT parallel data filtering (TMTF) and normalisation workflow together with dual conditional cross-entropy filtering (DCCEF)), training data pre-processing using the methods described by, anew optimisation method, the quasi-hyperbolic Adam, proposed by, back-translation with sampling-based decoding (e.g., as done by ) and by targeting rare words and in-domain subsets of the monolingual data, and automatic linguistically informed post-editing of named entities and non-translatable phrases.", "labels": [], "entities": [{"text": "corpora filtering", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7448111474514008}, {"text": "Tilde MT parallel data filtering (TMTF", "start_pos": 71, "end_pos": 109, "type": "TASK", "confidence": 0.6773811153003148}]}, {"text": "This year, Tilde participated in the shared task on news translation for the English\u2194Lithuanian language pair.", "labels": [], "entities": [{"text": "news translation", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.67708820104599}]}, {"text": "We trained constrained and unconstrained systems for both translation directions.", "labels": [], "entities": []}, {"text": "The paper is further structured as follows: Section 2 describes the data used for training, Section 3 describes the main NMT model training experiments, Section 4 describes our experiments on automatic post-editing of named entities, Section 5 summarises our automatic evaluation results, and the paper is concluded in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used NMT model adaptation through backtranslation ( to adapt NMT systems to the news domain.", "labels": [], "entities": [{"text": "NMT model adaptation", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.656845211982727}]}, {"text": "We applied two iterations of back-translation and the subsequent system training to incrementally improve the backtranslated data set.", "labels": [], "entities": []}, {"text": "We also analysed methods for selection of the data for backtranslation.", "labels": [], "entities": []}, {"text": "The methods are discussed further.", "labels": [], "entities": []}, {"text": "In the figures further, if not specified in the name of each system, the proportion between parallel and back-translated data is 1-to-1.", "labels": [], "entities": []}], "tableCaptions": []}