{"title": [{"text": "Improving Sentiment Classification in Slovak Language", "labels": [], "entities": [{"text": "Improving Sentiment Classification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.9102221528689066}]}], "abstractContent": [{"text": "Using different neural network architectures is widely spread for many different NLP tasks.", "labels": [], "entities": []}, {"text": "Unfortunately, most of the research is performed and evaluated only in English language and minor languages are often omitted.", "labels": [], "entities": []}, {"text": "We believe using similar architectures for other languages can show interesting results.", "labels": [], "entities": []}, {"text": "In this paper, we present our study on methods for improving sentiment classification in Slovak language.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.9390093982219696}]}, {"text": "We performed several experiments for two different datasets, one containing customer reviews, the other one general Twitter posts.", "labels": [], "entities": []}, {"text": "We show comparison of performance of different neural network architectures and also different word representations.", "labels": [], "entities": []}, {"text": "We show that another improvement can be achieved by using a model ensemble.", "labels": [], "entities": []}, {"text": "We performed experiments utilizing different methods of model ensemble.", "labels": [], "entities": []}, {"text": "Our proposed models achieved better results than previous models for both datasets.", "labels": [], "entities": []}, {"text": "Our experiments showed also other potential research areas.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For evaluation of our models, we used two different datasets.", "labels": [], "entities": []}, {"text": "The first dataset (Reviews3) consists of customer reviews of various services, which were manually labeled by 2 annotators.", "labels": [], "entities": []}, {"text": "Since many reviews were only slightly positive or negative and agreement between annotators were not very high, we can categorize reviews into three different classes, where we consider positive, negative and neutral class (contains slightly positive or negative reviews).", "labels": [], "entities": [{"text": "agreement", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9876567125320435}]}, {"text": "The second dataset (Twitter) consists of tweets in Slovak language, which were also labeled manually.", "labels": [], "entities": []}, {"text": "Since some of the tweets from the original dataset did not exist anymore, we provide only evaluation on tweets available via standard Twitter API.", "labels": [], "entities": []}, {"text": "The descriptive statistics of both datasets is shown in  To evaluate quality of our models we use F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9893092513084412}]}, {"text": "Since all datasets can be considered as highly unbalanced, we evaluate micro and macro F1 score separately.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9639894366264343}]}, {"text": "One of the problems of the Reviews3 dataset is its size.", "labels": [], "entities": [{"text": "Reviews3 dataset", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9731095135211945}]}, {"text": "Since it contains approximately 5000 annotated reviews, we need to perform complete cross-validation, where the dataset is split in ratio 8:1:1 for train, valid and test set.", "labels": [], "entities": []}, {"text": "For the Twitter dataset we split dataset in ratio 8:1:1 for train, valid and test set without any cross-validation.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9507293403148651}]}, {"text": "We also provide twitter ids for each set to preserve further reproducibility of experiments.", "labels": [], "entities": []}, {"text": "The only preprocessing used for our experiments is escaping punctuation to improve quality of tokenization of spaCy tokenizer in Slovak language.", "labels": [], "entities": []}, {"text": "We also provide list of further hyper-parameters and techniques used for training our models: dropout after embedding layer 0.5; dropout after recurrent and attention layer 0.3, negative log likelihood loss, Adam optimizer.", "labels": [], "entities": [{"text": "negative log likelihood loss", "start_pos": 178, "end_pos": 206, "type": "METRIC", "confidence": 0.7415788471698761}]}], "tableCaptions": [{"text": " Table 2: Statistics of used datasets.", "labels": [], "entities": []}, {"text": " Table 3: Results of sentiment classification for dataset  Reviews3.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.9627291560173035}, {"text": "dataset  Reviews3", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.5979860424995422}]}, {"text": " Table 4: Results of sentiment classification for dataset  Twitter.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.9569952487945557}]}, {"text": " Table 5: Comparison of sentiment classification for  dataset Reviews 3.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.8867889940738678}]}, {"text": " Table 6: Comparison of best performing model and  different types of model ensemble for dataset Twitter.  * -indicates differences in used dataset", "labels": [], "entities": []}]}