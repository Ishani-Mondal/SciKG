{"title": [{"text": "IT-IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed Models for Inflection", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents the Instituto de Telecomunica\u00e7Telecomunica\u00e7\u02dcTelecomunica\u00e7\u00f5es-Instituto Superior T\u00e9cnico submission to Task 1 of the SIGMORPHON 2019 Shared Task.", "labels": [], "entities": [{"text": "SIGMORPHON 2019 Shared Task", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.6805051565170288}]}, {"text": "Our models combine sparse sequence-to-sequence models with a two-headed attention mechanism that learns separate attention distributions for the lemma and inflectional tags.", "labels": [], "entities": []}, {"text": "Among submissions to Task 1, our models rank second and third.", "labels": [], "entities": []}, {"text": "Despite the low data setting of the task (only 100 in-language training examples), they learn plausible inflection patterns and often concentrate all probability mass into a small set of hypotheses, making beam search exact.", "labels": [], "entities": [{"text": "beam search", "start_pos": 206, "end_pos": 217, "type": "TASK", "confidence": 0.8658258616924286}]}], "introductionContent": [{"text": "Morphological inflection is the task of producing an inflected form, given a lemma and a set of inflectional tags.", "labels": [], "entities": [{"text": "Morphological inflection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8635521531105042}]}, {"text": "A widespread approach to the task is the attention-based sequence-to-sequence model (seq2seq;; such models perform well but are difficult to interpret.", "labels": [], "entities": []}, {"text": "To mitigate this shortcoming, we employ an alternative architecture which combines sparse seq2seq modeling () with two-headed attention that attends separately to the lemma and inflectional tags.", "labels": [], "entities": []}, {"text": "The attention and output distributions are computed with the sparsemax function and models are trained to minimize sparsemax loss.", "labels": [], "entities": []}, {"text": "Sparsemax, unlike softmax, can assign exactly zero attention weight to irrelevant source tokens and exactly zero probability to implausible hypotheses.", "labels": [], "entities": []}, {"text": "We apply our models to Task 1 at the, which extends morphological inflection to a cross-lingual setting.", "labels": [], "entities": []}, {"text": "We present two sparse seq2seq architectures: \u2022 DOUBLEATTN (it-ist-01-1) is a reimplementation of the two-headed attention model ( \u00b4 Acs, 2018) which substitutes sparsemax and its loss for softmax and cross entropy loss.", "labels": [], "entities": [{"text": "DOUBLEATTN", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9812371134757996}]}, {"text": "It uses separate encoders and attention heads for the lemma and inflections, and concatenates the outputs of the attention heads.", "labels": [], "entities": []}, {"text": "\u2022 GATEDATTN (it-ist-02-1) replaces the attention concatenation with a sparse gate which interpolates the lemma and inflection attention.", "labels": [], "entities": [{"text": "GATEDATTN", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.6490378379821777}]}, {"text": "The intuition is that the lemma and inflectional tags are not likely to be equally important at all time steps.", "labels": [], "entities": []}, {"text": "For example, in a suffixing language, the first several generated characters are likely to be identical to the lemma; inflectional tags are not relevant.", "labels": [], "entities": []}, {"text": "The sparse gate allows the model to learn to shift focus between the two attentions while ignoring the other at a given time step.", "labels": [], "entities": []}, {"text": "GATEDATTN and DOUBLEATTN rank second and third, respectively, among submissions to Task 1.", "labels": [], "entities": [{"text": "GATEDATTN", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.5469698905944824}, {"text": "DOUBLEATTN", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9730312824249268}]}, {"text": "In addition, their behavior is highly interpretable: they mostly learn to attend to a single lemma hidden state at a time, progressing monotonically from left to right, while their inflection attention learns patterns which reflect underlying morphological structure.", "labels": [], "entities": []}, {"text": "The sparse output layer often allows the model to concentrate all probability mass into a single hypothesis, providing a certificate that decoding is exact.", "labels": [], "entities": []}, {"text": "Our analysis shows that sparsity is also highly predictive of performance on the shared task metrics, showing that the models \"know what they know\".", "labels": [], "entities": []}], "datasetContent": [{"text": "Each model was trained with early stopping fora maximum of 30 epochs with a batch size of 64.", "labels": [], "entities": []}, {"text": "We used the Adam optimizer () with a learning rate of 10 \u22123 , which was halved when validation accuracy failed to improve for three consecutive epochs.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9531720578670502}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9524450302124023}]}, {"text": "We tuned the dropout and the number of inflection encoder layers on a separate grid for each language pair.", "labels": [], "entities": []}, {"text": "Our hyperparameter ranges are shown in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task 1 test results on the SIGMORPHON  2019 Shared Task, averaged across language pairs.", "labels": [], "entities": [{"text": "SIGMORPHON  2019 Shared Task", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.5275099724531174}]}, {"text": " Table 2: Hyperparameters for all models. Bracketed  values were tuned individually for each language pair.", "labels": [], "entities": [{"text": "Bracketed", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.997165858745575}]}, {"text": " Table 3: Average number of positions with nonzero attention per target time step on the Task 1 development sets,  grouped by the family of the low resource language. For DOUBLEATTN, this is simply averaged over all target  time steps. For GATEDATTN, the lemma attention nonzeros are summed only over time steps in which the gate  is active for the lemma, and similarly for the inflection attention nonzeros. The 'Total' column for GATEDATTN  indicates the average number of nonzeros over all time steps after accounting for the gate.", "labels": [], "entities": [{"text": "Total' column", "start_pos": 414, "end_pos": 427, "type": "METRIC", "confidence": 0.9465074936548868}]}]}