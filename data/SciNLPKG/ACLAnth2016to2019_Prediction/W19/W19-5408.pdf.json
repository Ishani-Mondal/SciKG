{"title": [{"text": "MIPT System for World-Level Quality Estimation", "labels": [], "entities": [{"text": "World-Level Quality Estimation", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.6665507157643636}]}], "abstractContent": [{"text": "We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation.", "labels": [], "entities": [{"text": "WMT 19 shared task", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.5438199639320374}, {"text": "word-level quality estimation of automatic translation", "start_pos": 71, "end_pos": 125, "type": "TASK", "confidence": 0.6048838496208191}]}, {"text": "We start with a model similar to Shef-bRNN (Ive et al., 2018), which we modify by using conditional random fields (CRFs) (Lafferty et al., 2001) for sequence labelling.", "labels": [], "entities": []}, {"text": "Additionally, we use a different approach for labelling gaps and source words.", "labels": [], "entities": []}, {"text": "We further develop this model by including features from different sources such as BERT (Devlin et al., 2018), baseline features for the task (Specia et al., 2018) and transformer encoders (Vaswani et al., 2017).", "labels": [], "entities": [{"text": "BERT", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9967305660247803}]}, {"text": "We evaluate the performance of our models on the English-German dataset for the corresponding task.", "labels": [], "entities": [{"text": "English-German dataset", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.7154407054185867}]}], "introductionContent": [{"text": "Current methods of assessing the quality of machine translation, like BLEU (), are based on comparing the output of a machine translation system with several gold reference translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7714319825172424}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9960805773735046}]}, {"text": "The tasks of quality estimation at the WMT 19 conference aims at detecting errors in automatic translation without a reference translation at various levels (word-level, sentencelevel and document-level).", "labels": [], "entities": [{"text": "WMT 19 conference", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.7166415452957153}]}, {"text": "In this work we predict word-level quality.", "labels": [], "entities": []}, {"text": "In the task the participants are given a source sentence and its automatic translation and are asked to label the words in the machine translation as OK or BAD.", "labels": [], "entities": [{"text": "BAD", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9144381880760193}]}, {"text": "The machine translation system could have omitted some words in the translated sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7026156038045883}]}, {"text": "To detect such errors participants are also asked to label the gaps in the automatic translation.", "labels": [], "entities": []}, {"text": "A target sentence has a gap between every pair of neighboring words, one gap in the beginning of the sentence and one gap at the end of the sentence.", "labels": [], "entities": []}, {"text": "We are also interested in detecting the words in the source sentence that led to errors in the translation.", "labels": [], "entities": []}, {"text": "For this purpose participants are also asked to label the words in source sentences.", "labels": [], "entities": []}, {"text": "The source labels were obtained based on the alignments between the source and the postedited target sentences.", "labels": [], "entities": []}, {"text": "If a target token is labeled as BAD in the translation, then all source tokens aligned to it are labeled as BAD as well.", "labels": [], "entities": [{"text": "BAD", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9520299434661865}, {"text": "BAD", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9683828949928284}]}, {"text": "In section 2 we introduce our base model, which is a modified version of phrase-level Shef-bRNN (, and further develop it by using different methods of extracting features from the input alongside the bi-RNN features.", "labels": [], "entities": []}, {"text": "In section 3 we write about our experimental setup and in section 4 we present the scores achieved by our models.", "labels": [], "entities": []}, {"text": "In section 5 we summarize our work and propose ways for further development.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train and evaluate our models on the WMT 19 Word-Level Quality Estimation Task EnglishGerman dataset.", "labels": [], "entities": [{"text": "WMT 19 Word-Level Quality Estimation Task EnglishGerman dataset", "start_pos": 40, "end_pos": 103, "type": "DATASET", "confidence": 0.8229863122105598}]}, {"text": "In our experiments we did not utilize pre-training or multi-task learning unlike some versions of Shef-bRNN.", "labels": [], "entities": []}, {"text": "All our models were implemented in PyTorch, the code is available online.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9143127202987671}]}, {"text": "For RNN feature extraction we use OpenNMT () bi-LSTM encoder implementation with 300 hidden units in both backward and forward LSTMs for models that label words and 150 hidden units for models that label gaps.", "labels": [], "entities": [{"text": "RNN feature extraction", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8911502162615458}]}, {"text": "We used FastText models () for English and German languages to produce word embeddings.", "labels": [], "entities": []}, {"text": "Baseline features were provided with the dataset.", "labels": [], "entities": []}, {"text": "In our experiments we used min occurs = 4 when building baseline feature vocabularies.", "labels": [], "entities": [{"text": "min occurs", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9452382922172546}]}, {"text": "Pretrained BERT model was provided by the pytorch-pretrained-bert package.", "labels": [], "entities": [{"text": "Pretrained", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9913116693496704}, {"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.8025844693183899}]}, {"text": "In our experiments we used the bert-base-multilingual-cased version of BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9831041097640991}]}, {"text": "We used the OpenNMT () transformer encoder implementation with the following parameters: num layers = 3, d model = 300, heads = 4, d ff = 600 (or d ff = 300 for gap labelling), dropout = 0.1.", "labels": [], "entities": []}, {"text": "We trained our models using PyTorch implementation of the ADADELTA algorithm) with all parameters, except the learning rate, set to their default values.", "labels": [], "entities": [{"text": "ADADELTA", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9073373675346375}]}, {"text": "For the train loss to converge we used the learning rate of 1 for the RNN and Transformer models, the learning rate of 0.3 for the RNN+Baseline model and the learning rate of 0.1 for RNN+Bert, RNN+Baseline+Bert and Transformer+Baseline+Bert models.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.9653812050819397}, {"text": "learning rate", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.9558143019676208}]}, {"text": "The inputs were fed into the model in mini-batches of 10 samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Final results on the test dataset.", "labels": [], "entities": []}, {"text": " Table 2: Models scores on WMT 19 English-German dataset, target prediction. The baseline scores are taken from  (Specia et al., 2018) and the Shef-bRNN scores are taken from (Ive et al., 2018)", "labels": [], "entities": [{"text": "WMT 19 English-German dataset", "start_pos": 27, "end_pos": 56, "type": "DATASET", "confidence": 0.9088607281446457}]}, {"text": " Table 3: Models scores on WMT 19 English-German dataset, source prediction. The Shef-bRNN scores are taken  from (Ive et al., 2018)", "labels": [], "entities": [{"text": "WMT 19 English-German dataset", "start_pos": 27, "end_pos": 56, "type": "DATASET", "confidence": 0.8899877220392227}]}, {"text": " Table 4: Models scores on WMT 19 English-German dataset, gap prediction. The Shef-bRNN scores are taken  from (Ive et al., 2018)", "labels": [], "entities": [{"text": "WMT 19 English-German dataset", "start_pos": 27, "end_pos": 56, "type": "DATASET", "confidence": 0.8730035722255707}, {"text": "gap prediction", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.6379455924034119}]}]}