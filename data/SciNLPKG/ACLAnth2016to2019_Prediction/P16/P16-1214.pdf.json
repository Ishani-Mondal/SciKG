{"title": [{"text": "On Approximately Searching for Similar Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We discuss an approximate similarity search for word embeddings, which is an operation to approximately find em-beddings close to a given vector.", "labels": [], "entities": []}, {"text": "We compared several metric-based search algorithms with hash-, tree-, and graph-based indexing from different aspects.", "labels": [], "entities": []}, {"text": "Our experimental results showed that a graph-based indexing exhibits robust performance and additionally provided useful information, e.g., vector normalization achieves an efficient search with cosine similarity.", "labels": [], "entities": [{"text": "vector normalization", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.7392557263374329}]}], "introductionContent": [{"text": "An embedding or distributed representation of a word is a real-valued vector that represents its \"meaning\" on the basis of distributional semantics, where the meaning of a word is determined by its context or surrounding words.", "labels": [], "entities": []}, {"text": "For a given meaning space, searching for similar embeddings is one of the most basic operations in natural language processing and can be applied to various applications, e.g., extracting synonyms, inferring the meanings of polysemous words, aligning words in two sentences in different languages, solving analogical reasoning questions, and searching for documents related to a query.", "labels": [], "entities": [{"text": "extracting synonyms", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.8764570951461792}, {"text": "solving analogical reasoning questions", "start_pos": 298, "end_pos": 336, "type": "TASK", "confidence": 0.5774396881461143}]}, {"text": "In this paper, we address how to quickly and accurately find similar embeddings in a continuous space for such applications.", "labels": [], "entities": []}, {"text": "This is important from a practical standpoint, e.g., when we want to develop a real-time query expansion system on a search engine on the basis of an embedding similarity.", "labels": [], "entities": []}, {"text": "A key difference from the existing work is that embeddings are not high-dimensional sparse (traditional count) vectors, but (relatively) low-dimensional dense vectors.", "labels": [], "entities": []}, {"text": "We therefore need to use approximate search methods instead of inverted-index-based methods ().", "labels": [], "entities": []}, {"text": "Three types of indexing are generally used in approximate similarity search: hash-, tree-, and graph-based indexing.", "labels": [], "entities": [{"text": "approximate similarity search", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7363707224527994}]}, {"text": "Hash-based indexing is the most common in natural language processing due to its simplicity, while tree/graph-based indexing is preferred in image processing because of its performance.", "labels": [], "entities": [{"text": "Hash-based indexing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7183915376663208}]}, {"text": "We compare several algorithms with these three indexing types and clarify which algorithm is most effective for similarity search for word embeddings from different aspects.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.8094199001789093}]}, {"text": "To the best of our knowledge, no other study has compared approximate similarity search methods focusing on neural word embeddings.", "labels": [], "entities": [{"text": "approximate similarity search", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.6029594739278158}]}, {"text": "Although one study has compared similarity search methods for (count-based) vectors on the basis of distributional semantics), our study advances this topic and makes the following contributions: (a) we focus on neural word embeddings learned by a recently developed skip-gram model, (b) show that a graph-based search method clearly performs better than the best one reported in the Gorman and Curran study from different aspects, and (c) report the useful facts that normalizing vectors can achieve an effective search with cosine similarity, the search performance is more strongly related to a learning model of embeddings than its training data, the distribution shape of embeddings is a key factor relating to the search performance, and the final performance of a target application can be far different from the search performance.", "labels": [], "entities": []}, {"text": "We believe that our timely results can lead to the practical use of embeddings, especially for real-time applications in the real world.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly survey hash-, tree-, and graph-based indexing methods for achieving similarity search in a metric space.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.6686265766620636}]}, {"text": "In Section 3, we compare several similarity search algorithms from different aspects and discuss the results.", "labels": [], "entities": []}, {"text": "Finally, Section 4 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we focused on the pure similarity search task of word embeddings rather than complex application tasks for avoiding extraneous factors, since many practical tasks can be formulated as k-nearest neighbor search.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6872927248477936}]}, {"text": "For example, assuming search engines, we can formalize query expansion, term deletion, and misspelling correction as finding frequent similar words, infrequent similar words, and similar words with different spellings, respectively.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7119700312614441}, {"text": "term deletion", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.6491399705410004}]}, {"text": "We chose FLANN from the tree-based methods and NGT from the graph-based methods since they are expected to be suitable for practical use.", "labels": [], "entities": [{"text": "FLANN", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9800323247909546}]}, {"text": "FLANN and NGT are compared with SASH, which was the best method reported in a previous study).", "labels": [], "entities": [{"text": "FLANN", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9289366006851196}, {"text": "NGT", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6001477837562561}, {"text": "SASH", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9393142461776733}]}, {"text": "In addition, we consider LSH only for confirmation, since it is widely used in natural language processing, although several studies have reported that LSH performed worse than SASH and FLANN.", "labels": [], "entities": [{"text": "FLANN", "start_pos": 186, "end_pos": 191, "type": "METRIC", "confidence": 0.6094937920570374}]}, {"text": "We used the E2LSH package, which includes an implementation of a practical LSH algorithm.", "labels": [], "entities": [{"text": "E2LSH package", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.9266606569290161}]}], "tableCaptions": [{"text": " Table 1: Indexing time of SASH, FLANN, NGT,  and LSH using the normalized, angular, Euclidean  distance functions.", "labels": [], "entities": [{"text": "SASH", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.39107128977775574}, {"text": "FLANN", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9470388889312744}, {"text": "NGT", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8561407327651978}, {"text": "LSH", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9026214480400085}]}, {"text": " Table 2: Variance and kurtosis of English  Wikipedia (EW), GN, CW, and GV embeddings.", "labels": [], "entities": [{"text": "English  Wikipedia (EW)", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.8454841375350952}]}]}