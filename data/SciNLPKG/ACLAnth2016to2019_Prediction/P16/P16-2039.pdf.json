{"title": [{"text": "Domain Specific Named Entity Recognition Referring to the Real World by Deep Neural Networks", "labels": [], "entities": [{"text": "Domain Specific Named Entity Recognition Referring to the Real World", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.7245049208402634}]}], "abstractContent": [{"text": "In this paper, we propose a method for referring to the real world to improve named entity recognition (NER) specialized fora domain.", "labels": [], "entities": [{"text": "named entity recognition (NER) specialized fora domain", "start_pos": 78, "end_pos": 132, "type": "TASK", "confidence": 0.8066062463654412}]}, {"text": "Our method adds a stacked auto-encoder to a text-based deep neural network for NER.", "labels": [], "entities": []}, {"text": "We first train the stacked auto-encoder only from the real world information , then the entire deep neural network from sentences annotated with NEs and accompanied by real world information.", "labels": [], "entities": []}, {"text": "In our experiments, we took Japanese chess as the example.", "labels": [], "entities": []}, {"text": "The dataset consists of pairs of a game state and commentary sentences about it annotated with game-specific NE tags.", "labels": [], "entities": []}, {"text": "We conducted NER experiments and showed that referring to the real world improves the NER accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9657594561576843}]}], "introductionContent": [{"text": "In recent years there has been a surge of interest in relating natural language to the real world.", "labels": [], "entities": []}, {"text": "And more and more language resources accompanied by nonlinguistic data are becoming available.", "labels": [], "entities": []}, {"text": "Typical examples are image descriptions) and video ().", "labels": [], "entities": []}, {"text": "summarized many other image and video datasets.", "labels": [], "entities": []}, {"text": "These datasets allow us to attempt the task of connecting language expressions to the real world, which is called symbol grounding.", "labels": [], "entities": [{"text": "symbol grounding", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.748794436454773}]}, {"text": "proposed methods for acquiring multimodal representations by applying SVD to distributional semantics and bag-of-visual-words (BoVW).", "labels": [], "entities": []}, {"text": "proposed unsupervised multimodal learning based on deep restricted boltzmann machines (RBMs).", "labels": [], "entities": []}, {"text": "In the field of natural language processing (NLP) research, * This work was done when the first author was at Ehime University.", "labels": [], "entities": [{"text": "natural language processing (NLP) research", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.8043230431420463}, {"text": "Ehime University", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9122671782970428}]}, {"text": "proposed to acquire bilingual lexicon based on visual similarity.", "labels": [], "entities": []}, {"text": "describe a method for predicting a preposition referring to positions in the image.", "labels": [], "entities": [{"text": "predicting a preposition referring to positions", "start_pos": 22, "end_pos": 69, "type": "TASK", "confidence": 0.8306735754013062}]}, {"text": "In this paper, we propose a method for enhancing a named entity (NE) recognizer referring to the real world.", "labels": [], "entities": []}, {"text": "Because of the lack of datasets consisting of sentences annotated with the general NE tags such as names of people, organizations, and times, with accompanying real world data, we take game states as the counterpart of the language and the NE tag set specialized for game commentaries such as defense formations and opening names.", "labels": [], "entities": [{"text": "NE tag set", "start_pos": 240, "end_pos": 250, "type": "DATASET", "confidence": 0.6822461585203806}]}, {"text": "Similar to bio-medical NEs), these NEs are useful for applications in the game domain.", "labels": [], "entities": []}, {"text": "Our method could be used to improve automatic game commentary systems () or to build a state search method that uses natural language queries instead of state notations ().", "labels": [], "entities": [{"text": "game commentary", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.715659499168396}]}, {"text": "In addition to these interesting applications, game states have another advantage for NLP research.", "labels": [], "entities": []}, {"text": "They are much easier to recognize than images and video, which allows us to concentrate on the NLP problem.", "labels": [], "entities": []}, {"text": "In order to incorporate the real world, i.e. game states, into NE recognition (NER), we propose to use deep neural networks (DNNs), which have been reported to be successful in various NLP tasks such as word embedding (, part-of-speech tagging), parsing), parsing), NER , sentiment analysis () and machine translation.", "labels": [], "entities": [{"text": "NE recognition (NER)", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.9146139979362488}, {"text": "part-of-speech tagging", "start_pos": 221, "end_pos": 243, "type": "TASK", "confidence": 0.7200098484754562}, {"text": "parsing", "start_pos": 246, "end_pos": 253, "type": "TASK", "confidence": 0.9606199264526367}, {"text": "parsing", "start_pos": 256, "end_pos": 263, "type": "TASK", "confidence": 0.9909245371818542}, {"text": "NER", "start_pos": 266, "end_pos": 269, "type": "TASK", "confidence": 0.9751971960067749}, {"text": "sentiment analysis", "start_pos": 272, "end_pos": 290, "type": "TASK", "confidence": 0.8839351534843445}, {"text": "machine translation", "start_pos": 298, "end_pos": 317, "type": "TASK", "confidence": 0.80452099442482}]}, {"text": "First we build a normal NE recognizer by referring only to the text information based on DNN.", "labels": [], "entities": [{"text": "NE recognizer", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.8215145468711853}]}, {"text": "Each unit of its output layer corresponds to a BIO tag for the word (see Section 3).", "labels": [], "entities": []}, {"text": "We use post processing based on the Viterbi algorithm to choose the best tag sequence by discarding inconsistent ones.", "labels": [], "entities": []}, {"text": "This design allows us to train the model from partially annotated sentences, in which only some words are annotated with NE tags (.", "labels": [], "entities": []}, {"text": "Next we extend the text-based DNN with a module that refers to game states.", "labels": [], "entities": []}, {"text": "This module is a stacked-auto-encoder (SAE) ( and we first train it only from game states.", "labels": [], "entities": []}, {"text": "The pre-training allows the model to learn game state embedding which abstracts game state information.", "labels": [], "entities": []}, {"text": "Then we fine-tune the entire DNN for NER, consisting of both text-based DNN and SAE.", "labels": [], "entities": [{"text": "NER", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.7538858652114868}]}, {"text": "As we show in later section of this paper, we end up with an NE recognizer that refers to real world information in addition to text information, which increases its accuracy.", "labels": [], "entities": [{"text": "NE recognizer", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.7626897990703583}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9969395399093628}]}], "datasetContent": [{"text": "In this section we describe the NER experiments we conducted to evaluate our method.", "labels": [], "entities": [{"text": "NER", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9229574799537659}]}, {"text": "The corpus we used is the game commentary corpus (Mori et al., 2016) described in Section 3 briefly.", "labels": [], "entities": [{"text": "game commentary corpus", "start_pos": 26, "end_pos": 48, "type": "DATASET", "confidence": 0.6162199079990387}]}, {"text": "shows the number of dimensions in each layer for game state embeddings in pre-training.", "labels": [], "entities": []}, {"text": "We set the number of layers in the SAE (Subsection 4.2) to four, with which we could maximize the accuracy on the development set held-out from the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9994207620620728}]}], "tableCaptions": [{"text": " Table 3: Game commentary corpus specifications.", "labels": [], "entities": []}, {"text": " Table 4: Dimensions of the SAE layers.", "labels": [], "entities": []}]}