{"title": [{"text": "Together We Stand: Siamese Networks for Similar Question Retrieval", "labels": [], "entities": [{"text": "Similar Question Retrieval", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7190130849679311}]}], "abstractContent": [{"text": "Community Question Answering (cQA) services like Yahoo!", "labels": [], "entities": [{"text": "Community Question Answering (cQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7231341898441315}]}, {"text": "Answers 1 , Baidu Zhidao 2 , Quora 3 , StackOverflow 4 etc.", "labels": [], "entities": []}, {"text": "provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.", "labels": [], "entities": []}, {"text": "The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives.", "labels": [], "entities": [{"text": "cQA archives", "start_pos": 141, "end_pos": 153, "type": "DATASET", "confidence": 0.9803527891635895}]}, {"text": "The main challenge in this task is the \"lexico-syntactic\" gap between the current and the previous questions.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach called \"Siamese Convolutional Neural Network for cQA (SCQA)\" to find the semantic similarity between the current and the archived questions.", "labels": [], "entities": []}, {"text": "SCQA consist of twin convolu-tional neural networks with shared parameters and a contrastive loss function joining them.", "labels": [], "entities": []}, {"text": "SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA forum archives.", "labels": [], "entities": [{"text": "cQA forum archives", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.9373901089032491}]}, {"text": "The model projects semantically similar question pairs nearer to each other and dissimilar question pairs farther away from each other in the semantic space.", "labels": [], "entities": []}, {"text": "Experiments on large scale real-life \"Yahoo!", "labels": [], "entities": []}, {"text": "Answers\" dataset reveals that SCQA outperforms current state-of-the-art approaches based on translation models , topic models and deep neural network 1 https://answers.yahoo.com/ 2", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8952483534812927}]}], "introductionContent": [{"text": "The cQA forums have emerged as popular and effective means of information exchange on the Web.", "labels": [], "entities": []}, {"text": "Users post queries in these forums and receive precise and compact answers instead of a list of documents.", "labels": [], "entities": []}, {"text": "Unlike in Web search, opinion based queries are also answered hereby experts and users based on their personal experiences.", "labels": [], "entities": []}, {"text": "The question and answers are also enhanced with rich metadata like categories, subcategories, user expert level, user votes to answers etc.", "labels": [], "entities": []}, {"text": "One of the serious concerns in cQA is \"question-starvation\") where a question does not get immediate answer from any user.", "labels": [], "entities": []}, {"text": "When this happens, the question may take several hours and sometimes days to get satisfactory answers or may not get answered at all.", "labels": [], "entities": []}, {"text": "This delay in response maybe avoided by retrieving semantically related questions from the cQA archives.", "labels": [], "entities": [{"text": "cQA archives", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9720420241355896}]}, {"text": "If a similar question is found in the database of previous questions, then the corresponding best answer can be provided without any delay.", "labels": [], "entities": []}, {"text": "However, the major challenge associated with retrieval of similar questions is the lexicosyntactic gap between them.", "labels": [], "entities": []}, {"text": "Two questions may mean the same thing but they may differ lexically and syntactically.", "labels": [], "entities": []}, {"text": "For example the queries \"Why are yawns contagious?\" and \"Why do we yawn when we see somebody else yawning?\" convey the same meaning but differ drastically from each other in terms of words and syntax.", "labels": [], "entities": []}, {"text": "Several techniques have been proposed in the literature for similar question retrieval and they could be broadly classified as follows: models like BM 25 and Language modeling for Information Retrieval (LM IR) () score the similarity based on the weights of the matching text terms between the questions.", "labels": [], "entities": [{"text": "similar question retrieval", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.6282415091991425}, {"text": "BM 25", "start_pos": 148, "end_pos": 153, "type": "DATASET", "confidence": 0.7230972051620483}, {"text": "Information Retrieval (LM IR)", "start_pos": 180, "end_pos": 209, "type": "TASK", "confidence": 0.8054166038831075}]}], "datasetContent": [{"text": "Labs Webscope . Each question in the dataset contains title, description, best answer, most voted answers and meta-data like categories, sub categories etc.", "labels": [], "entities": [{"text": "Labs Webscope", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9006549119949341}]}, {"text": "For training dataset, we randomly selected 2 million data and extracted question-relevant answer pairs and question-irrelevant answer pairs from them to train SCQA.", "labels": [], "entities": [{"text": "SCQA", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.7854782938957214}]}, {"text": "Similarly, our validation dataset contains 400,000 question answer pairs.", "labels": [], "entities": []}, {"text": "The hyperparameters of the network are tuned on the validation dataset.", "labels": [], "entities": []}, {"text": "The values of the hyperparameters for which we obtained the best results is shown in Table 1.", "labels": [], "entities": []}, {"text": "We used the annotated survey dataset of 1018 questions released by as testset for all the models.", "labels": [], "entities": []}, {"text": "On this gold data, we evaluated the performance of the models with three evaluation criteria: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at K (P@K).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 94, "end_pos": 122, "type": "METRIC", "confidence": 0.9704969922701517}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 124, "end_pos": 150, "type": "METRIC", "confidence": 0.9596129953861237}, {"text": "Precision at K (P@K)", "start_pos": 155, "end_pos": 175, "type": "METRIC", "confidence": 0.9221362918615341}]}, {"text": "Each question and answer was pre-processed by lower-casing, stemming, stopword and special character removal.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameters of SCQA.", "labels": [], "entities": []}]}