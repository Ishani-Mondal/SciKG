{"title": [{"text": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "labels": [], "entities": [{"text": "Character-Based Distributed Representations", "start_pos": 11, "end_pos": 54, "type": "TASK", "confidence": 0.6527290840943655}]}], "abstractContent": [{"text": "Text from social media provides a set of challenges that can cause traditional NLP approaches to fail.", "labels": [], "entities": []}, {"text": "Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches.", "labels": [], "entities": []}, {"text": "We propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences.", "labels": [], "entities": []}, {"text": "The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences.", "labels": [], "entities": []}, {"text": "Our tweet2vec encoder is publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "We understand from Zipf's Law that in any natural language corpus a majority of the vocabulary word types will either be absent or occur in low frequency.", "labels": [], "entities": []}, {"text": "Estimating the statistical properties of these rare word types is naturally a difficult task.", "labels": [], "entities": []}, {"text": "This is analogous to the curse of dimensionality when we deal with sequences of tokens -most sequences will occur only once in the training data.", "labels": [], "entities": []}, {"text": "Neural network architectures overcome this problem by defining non-linear compositional models over vector space representations of tokens and hence assign non-zero probability even to sequences not seen during training.", "labels": [], "entities": []}, {"text": "In this work, we explore a similar approach to learning distributed representations of social media posts by https://github.com/bdhingra/tweet2vec composing them from their constituent characters, with the goal of generalizing to out-of-vocabulary words as well as sequences attest time.", "labels": [], "entities": []}, {"text": "Traditional Neural Network Language Models (NNLMs) treat words as the basic units of language and assign independent vectors to each word type.", "labels": [], "entities": []}, {"text": "To constrain memory requirements, the vocabulary size is fixed before-hand; therefore, rare and out-of-vocabulary words are all grouped together under a common type 'UNKNOWN'.", "labels": [], "entities": []}, {"text": "This choice is motivated by the assumption of arbitrariness in language, which means that surface forms of words have little to do with their semantic roles.", "labels": [], "entities": []}, {"text": "Recently, () challenge this assumption and present a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for composing word vectors from their constituent characters which can memorize the arbitrary aspects of word orthography as well as generalize to rare and out-of-vocabulary words.", "labels": [], "entities": [{"text": "bidirectional Long Short Term Memory (LSTM)", "start_pos": 53, "end_pos": 96, "type": "METRIC", "confidence": 0.6632417477667332}]}, {"text": "Encouraged by their findings, we extend their approach to a much larger unicode character set, and model long sequences of text as functions of their constituent characters (including whitespace).", "labels": [], "entities": []}, {"text": "We focus on social media posts from the website Twitter, which are an excellent testing ground for character based models due to the noisy nature of text.", "labels": [], "entities": []}, {"text": "Heavy use of slang and abundant misspellings means that there are many orthographically and semantically similar tokens, and special characters such as emojis are also immensely popular and carry useful semantic information.", "labels": [], "entities": []}, {"text": "In our moderately sized training dataset of 2 million tweets, there were about 0.92 million unique word types.", "labels": [], "entities": []}, {"text": "It would be expensive to capture all these phenomena in a word based model in terms of both the memory requirement (for the increased vocabulary) and the amount of training data required for effective learning.", "labels": [], "entities": []}, {"text": "Additional benefits of the character based approach include language independence of the methods, and no requirement of NLP preprocessing such as word-segmentation.", "labels": [], "entities": []}, {"text": "A crucial step in learning good text representations is to choose an appropriate objective function to optimize.", "labels": [], "entities": []}, {"text": "Unsupervised approaches attempt to reconstruct the original text from its latent representation (.", "labels": [], "entities": []}, {"text": "Social media posts however, come with their own form of supervision annotated by millions of users, in the form of hashtags which link posts about the same topic together.", "labels": [], "entities": []}, {"text": "A natural assumption is that the posts with the same hashtags should have embeddings which are close to each other.", "labels": [], "entities": []}, {"text": "Hence, we formulate our training objective to maximize cross-entropy loss at the task of predicting hashtags fora post from its latent representation.", "labels": [], "entities": []}, {"text": "We propose a Bi-directional Gated Recurrent Unit (Bi-GRU) () neural network for learning tweet representations.", "labels": [], "entities": [{"text": "learning tweet representations", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.7043968737125397}]}, {"text": "Treating white-space as a special character itself, the model does a forward and backward pass over the entire sequence, and the final GRU states are linearly combined to get the tweet embedding.", "labels": [], "entities": []}, {"text": "Posterior probabilities over hashtags are computed by projecting this embedding to a softmax output layer.", "labels": [], "entities": []}, {"text": "Compared to a word-level baseline this model shows improved performance at predicting hashtags fora held-out set of posts.", "labels": [], "entities": []}, {"text": "Inspired by recent work in learning vector space text representations, we name our model tweet2vec.", "labels": [], "entities": [{"text": "learning vector space text representations", "start_pos": 27, "end_pos": 69, "type": "TASK", "confidence": 0.615450519323349}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Model sizes and training time/epoch", "labels": [], "entities": []}, {"text": " Table 3: Hashtag prediction results. Best numbers  for each test set are in bold.", "labels": [], "entities": [{"text": "Hashtag prediction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8536359071731567}]}, {"text": " Table 4: Precision @1 as training data size and  number of output labels is increased. Note that the  test set is different for each setting.", "labels": [], "entities": [{"text": "Precision @1", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.947663426399231}]}]}