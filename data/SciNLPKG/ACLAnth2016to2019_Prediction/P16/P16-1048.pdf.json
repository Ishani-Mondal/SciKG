{"title": [{"text": "CSE: Conceptual Sentence Embeddings based on Attention Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Most sentence embedding models typically represent each sentence only using word surface, which makes these models indis-criminative for ubiquitous homonymy and polysemy.", "labels": [], "entities": []}, {"text": "In order to enhance representation capability of sentence, we employ conceptualization model to assign associated concepts for each sentence in the text corpus, and then learn conceptual sentence embedding (CSE).", "labels": [], "entities": []}, {"text": "Hence, this semantic representation is more expressive than some widely-used text representation models such as latent topic model, especially for short-text.", "labels": [], "entities": []}, {"text": "Moreover, we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction.", "labels": [], "entities": []}, {"text": "In the experiments , we evaluate the CSE models on two tasks, text classification and information retrieval.", "labels": [], "entities": [{"text": "text classification", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8331650197505951}, {"text": "information retrieval", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.8455635905265808}]}, {"text": "The experimental results show that the proposed models outperform typical sentence embedding models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language processing applications require the input text to be represented as a fixedlength feature, of which sentence representation is very important.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.7387423515319824}]}, {"text": "Perhaps the most common fixedlength vector representation for texts is the bag-ofwords or bag-of-n-grams.", "labels": [], "entities": []}, {"text": "However, they suffer severely from data sparsity and high dimensionality, and have very little sense about the semantics of words or the distances between the words.", "labels": [], "entities": []}, {"text": "Recently, in sentence representation and classification, deep neural network (DNN) approaches have achieved state-of-the-art results (Le * The contact author. and Mikolov, 2014;).", "labels": [], "entities": [{"text": "sentence representation and classification", "start_pos": 13, "end_pos": 55, "type": "TASK", "confidence": 0.7459702044725418}]}, {"text": "Despite of their usefulness, recent sentence embeddings face several challenges: (i) Most sentence embedding models represent each sentence only using word surface, which makes these models indiscriminative for ubiquitous polysemy; (ii) For short-text, however, neither parsing nor topic modeling works well because there are simply not enough signals in the input; (iii) Setting window size of context words is very difficult.", "labels": [], "entities": [{"text": "Setting window size of context words", "start_pos": 372, "end_pos": 408, "type": "TASK", "confidence": 0.816229502360026}]}, {"text": "To solve these problems, we must derive more semantic signals from the input sentence, e.g., concepts.", "labels": [], "entities": []}, {"text": "Besides, we should assigned different attention for different contextual word, to enhance the influence of words that are relevant for each prediction.", "labels": [], "entities": []}, {"text": "This paper proposed Conceptual Sentence Embedding (CSE), an unsupervised framework that learns continuous distributed vector representations for sentence.", "labels": [], "entities": [{"text": "Conceptual Sentence Embedding (CSE)", "start_pos": 20, "end_pos": 55, "type": "TASK", "confidence": 0.7066399951775869}]}, {"text": "Specially, by innovatively introducing concept information, this concept-level vector representations of sentence are learned to predict the surrounding words or target word in contexts.", "labels": [], "entities": []}, {"text": "Our research is inspired by the recent work in learning vector representations of words using deep learning strategy ().", "labels": [], "entities": []}, {"text": "More precisely, we first obtain concept distribution of the sentence, and generate corresponding concept vector.", "labels": [], "entities": []}, {"text": "Then we concatenate or average the sentence vector, contextual word vectors with concept vector of the sentence, and predict the target word in the given context.", "labels": [], "entities": []}, {"text": "All of the sentence vectors and word vectors are trained by the stochastic gradient descent and backpropagation.", "labels": [], "entities": []}, {"text": "At prediction time, sentence vectors are inferred by fixing the word vectors and observed sentence vectors, and training the new sentence vector until convergence.", "labels": [], "entities": []}, {"text": "In parallel, the concept of attention has gained popularity recently in neural natural language processing researches, which allowing models to learn alignments between different modalities (.", "labels": [], "entities": []}, {"text": "In this work, we further propose the extensions to CSE, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word.", "labels": [], "entities": []}, {"text": "The main intuition behind the extended model is that prediction of a word is mainly dependent on certain words surrounding it.", "labels": [], "entities": [{"text": "prediction of a word", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8663791567087173}]}, {"text": "In summary, the basic idea of CSE is that, we allow each word to have different embeddings under different concepts.", "labels": [], "entities": []}, {"text": "Taking word apple into consideration, it may indicate a fruit under the concept food, and indicate an IT company under the concept information technology.", "labels": [], "entities": []}, {"text": "Hence, concept information significantly contributes to the discriminative of sentence vector.", "labels": [], "entities": []}, {"text": "Moreover, an important advantage of the proposed conceptual sentence embeddings is that they could be learned from unlabeled data.", "labels": [], "entities": []}, {"text": "Another advantage is that we take the word order into account, in the same way of ngram model, while bag-of-n-grams model would create a very high-dimensional representation that tends to generalize poorly.", "labels": [], "entities": []}, {"text": "To summarize, this work contributes on the following aspects: We integrate concepts and attention-based strategy into basic sentence embedding representation, and allow the resulting conceptual sentence embedding to model different meanings of a word under different concept.", "labels": [], "entities": []}, {"text": "The experimental results on text classification task and information retrieval task demonstrate that this concept-level sentence representation is robust.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8656410376230875}, {"text": "information retrieval task", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.8400371670722961}]}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 surveys related researches.", "labels": [], "entities": []}, {"text": "Section 3 formally de-scribes the proposed model of conceptual sentence embedding.", "labels": [], "entities": []}, {"text": "Corresponding experimental results are shown in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we show experiments on two text understanding problems, text classification and information retrieval, to evaluate related models in several aspects.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7789809703826904}, {"text": "text classification", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8144254386425018}, {"text": "information retrieval", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7898933589458466}]}, {"text": "These tasks are always used to evaluate the performance of sentence embedding methods ().", "labels": [], "entities": []}, {"text": "The source codes and datasets of this paper are publicly available 1 .  We utilize four datasets for training and evaluating.", "labels": [], "entities": []}, {"text": "For text classification task, we use three datasets: NewsTile, TREC and Twitter.", "labels": [], "entities": [{"text": "text classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8602781891822815}, {"text": "NewsTile", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9791737794876099}, {"text": "TREC", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.5493503212928772}]}, {"text": "Dataset Tweet11 is used for evaluation in information retrieval task.", "labels": [], "entities": [{"text": "Dataset Tweet11", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.7750691771507263}, {"text": "information retrieval task", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.8679536183675131}]}, {"text": "Moreover, we construct dataset Wiki to fully train topic model-based models.", "labels": [], "entities": []}, {"text": "NewsTitle: The news articles are extracted from a large news corpus, which contains about one million articles searched from Web pages.", "labels": [], "entities": [{"text": "NewsTitle", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9235483407974243}]}, {"text": "We organize volunteers to classify these news articles manually into topics according its article content ( , and we select six topics: company, health, entertainment, food, politician, and sports.", "labels": [], "entities": []}, {"text": "We randomly select 3,000 news articles in each topic, and only keep its title and its first one line of article.", "labels": [], "entities": []}, {"text": "The average word count of titles is 9.41.", "labels": [], "entities": []}, {"text": "TREC: It is the corpus for question classification on TREC (), which is widely used as benchmark in text classification task.", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6754862070083618}, {"text": "question classification", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8163945972919464}, {"text": "text classification task", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.8442906141281128}]}, {"text": "There are 5,952 sentences in the entire dataset, classified into the 6 categories as follows: person, abbreviation, entity, description, location and numeric.", "labels": [], "entities": []}, {"text": "Tweet11: This is the official tweet collections used in TREC Microblog.", "labels": [], "entities": [{"text": "TREC Microblog", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9303045868873596}]}, {"text": "Using the official API, we crawled a set of local copies of the corpus.", "labels": [], "entities": []}, {"text": "Our local Tweets11 collection has a sample of about 16 million tweets, and a set of 49 (TMB2011) and 60 (TMB2012) timestamped topics.", "labels": [], "entities": [{"text": "Tweets11 collection", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.7406804859638214}]}, {"text": "Twitter: This dataset is constructed by manually labeling the previous dataset Tweet11.", "labels": [], "entities": []}, {"text": "Similar to dataset NewsTitle, we ask our volunteers to label these tweets.", "labels": [], "entities": []}, {"text": "After manually labeling, the dataset contains 12,456 tweets which are in four categories: company, country, entertainment, and device.", "labels": [], "entities": []}, {"text": "The average length of the tweets is 13.16 words.", "labels": [], "entities": []}, {"text": "Because of its noise and sparsity, this social media dataset is very challenging for the comparative models.", "labels": [], "entities": []}, {"text": "Moreover, we also construct a Wikipedia dataset (denoted as Wiki) for training.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9389415085315704}]}, {"text": "We preprocess the Wikipedia articles 2 with the following rules.", "labels": [], "entities": []}, {"text": "First, we remove the articles less than 100 words, as well as the articles less than 10 links.", "labels": [], "entities": []}, {"text": "Then we remove all the category pages and disambiguation pages.", "labels": [], "entities": []}, {"text": "Moreover, we move the content to the right redirection pages.", "labels": [], "entities": []}, {"text": "Finally we obtain about 3.74 million Wikipedia articles for indexing and training.", "labels": [], "entities": []}, {"text": "The details about parameter settings of the comparative algorithms are described in this section, respectively.", "labels": [], "entities": []}, {"text": "For TWE, CSE-1, CSE-2 and their attention variants aCSE-1, and aCSE-2, the structure of the hierarchical softmax is a binary Huffman tree (, where short codes are assigned to frequent words.", "labels": [], "entities": [{"text": "aCSE-2", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.8977576494216919}]}, {"text": "This is a good speedup trick because common words are accessed quickly ().We set the dimensions of sentence, word, topic and concept embeddings as 5,000, which is like the number of concept clusters in Probase ().", "labels": [], "entities": [{"text": "Probase", "start_pos": 202, "end_pos": 209, "type": "DATASET", "confidence": 0.9707772135734558}]}, {"text": "Meanwhile, we have done many experiments on choosing the context window size (k).", "labels": [], "entities": []}, {"text": "We perform experiments on increasing windows size from 3 to 11, and different size works differently on different dataset with different average length of short-texts.", "labels": [], "entities": []}, {"text": "And we choose the result of windows size of 5 present here, because it performs best in almost datasets.", "labels": [], "entities": []}, {"text": "Usually, in project layer, the sentence vector, the context vector and the concept vectors could be averaged or concatenated for combination to predict the next word in a context.", "labels": [], "entities": []}, {"text": "We perform experiments following these two strategies respectively, and report the better of the two.", "labels": [], "entities": []}, {"text": "In fact, the concatenation performs better since averaging different types of vectors may cause loss of information somehow.", "labels": [], "entities": []}, {"text": "For BOW and LDA, we remove stop words by using InQuery stop-word list.", "labels": [], "entities": [{"text": "BOW", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.45263510942459106}]}, {"text": "For BOW, we select top 50,000 words according to TF-IDF scores as features.", "labels": [], "entities": []}, {"text": "For both LDA and TWE, in the text classification task, we set the topic number to be the cluster number or twice, and report the better of the two; while in the information retrieval task, we experimented with a varying number of topics from 100 to 500, which gives similar performance, and we report the final results of using 500 topics.", "labels": [], "entities": [{"text": "TWE", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.6798353791236877}, {"text": "text classification task", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.8652394413948059}, {"text": "information retrieval task", "start_pos": 161, "end_pos": 187, "type": "TASK", "confidence": 0.8277450799942017}]}, {"text": "In summary, we use the sentence vectors generated by each algorithm as features and run a linear classifier using) for evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results of multi-class text classification task.", "labels": [], "entities": [{"text": "multi-class text classification task", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.7198862433433533}]}, {"text": " Table 2: Results of information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.875958114862442}]}]}