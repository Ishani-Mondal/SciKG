{"title": [{"text": "Event Nugget Detection with Forward-Backward Recurrent Neural Networks", "labels": [], "entities": [{"text": "Event Nugget Detection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5454807380835215}]}], "abstractContent": [{"text": "Traditional event detection methods heavily rely on manually engineered rich features.", "labels": [], "entities": [{"text": "event detection", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.8204258382320404}]}, {"text": "Recent deep learning approaches alleviate this problem by automatic feature engineering.", "labels": [], "entities": []}, {"text": "But such efforts, like tradition methods, have so far only focused on single-token event mentions, whereas in practice events can also be a phrase.", "labels": [], "entities": []}, {"text": "We instead use forward-backward recurrent neural networks (FBRNNs) to detect events that can be either words or phrases.", "labels": [], "entities": []}, {"text": "To the best our knowledge, this is one of the first efforts to handle multi-word events and also the first attempt to use RNNs for event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.7290986478328705}]}, {"text": "Experimental results demonstrate that FBRNN is competitive with the state-of-the-art methods on the ACE 2005 and the Rich ERE 2015 event detection tasks.", "labels": [], "entities": [{"text": "FBRNN", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.776191771030426}, {"text": "ACE 2005", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.9548127055168152}, {"text": "Rich ERE 2015", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.9500361680984497}, {"text": "event detection", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.7724825441837311}]}], "introductionContent": [{"text": "Automatic event extraction from natural text is an important and challenging task for natural language understanding.", "labels": [], "entities": [{"text": "Automatic event extraction from natural text", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8038699179887772}, {"text": "natural language understanding", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.6814666390419006}]}, {"text": "Given a set of ontologized event types, the goal of event extraction is to identify the mentions of different event types and their arguments from natural texts.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7614221572875977}]}, {"text": "In this paper we focus on the problem of extracting event mentions, which can be in the form of a single word or multiple words.", "labels": [], "entities": []}, {"text": "In the current literature, events have been annotated in two different forms: \u2022 Event trigger: a single token that is considered to signify the occurrence of an event.", "labels": [], "entities": []}, {"text": "Here a token is not necessarily a word, for example, in order to capture a death event, the phrase \"kick the bucket\" is concatenated into a single token \"kick the bucket\".", "labels": [], "entities": []}, {"text": "This scheme has been used in the ACE and Light ERE data and has been followed inmost studies on event extraction.", "labels": [], "entities": [{"text": "ACE", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8775994181632996}, {"text": "Light ERE data", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.8965489466985067}, {"text": "event extraction", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.8122336864471436}]}, {"text": "\u2022 Event nugget: a word or a phrase of multiple words that most clearly expresses the occurrence of an event.", "labels": [], "entities": []}, {"text": "This scheme is recently introduced to remove the limitation of singletoken event triggers and has been adopted by the rich ERE data for event annotation.", "labels": [], "entities": [{"text": "ERE data", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.8713084161281586}]}, {"text": "Existing event extraction work often heavily relies on a rich set of hand-designed features and utilizes existing NLP toolkits and resources).", "labels": [], "entities": [{"text": "event extraction", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7229619175195694}]}, {"text": "Consequently, it is often challenging to adapt prior methods to multi-lingual or nonEnglish settings since they require extensive linguistic knowledge for feature engineering and mature NLP toolkits for extracting the features without severe error propagation.", "labels": [], "entities": []}, {"text": "By contrast, deep learning has recently emerged as a compelling solution to avoid the aforementioned problems by automatically extracting meaningful features from raw text without relying on existing NLP toolkits.", "labels": [], "entities": []}, {"text": "There have been some limited attempts in using deep learning for event detection) which apply Convolutional Neural Networks (CNNs) to a window of text around potential triggers to identify events.", "labels": [], "entities": [{"text": "event detection", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7438871413469315}]}, {"text": "These efforts outperform traditional methods, but there remain two major limitations: \u2022 So far they have, like traditional methods, only focused on the oversimplified scenario of single-token event detection.", "labels": [], "entities": [{"text": "single-token event detection", "start_pos": 179, "end_pos": 207, "type": "TASK", "confidence": 0.6283116439978281}]}, {"text": "\u2022 Such CNN-based approaches require a fixed size window.", "labels": [], "entities": []}, {"text": "In practice it is often unclear how large this window needs to be in order to capture necessary context to make decision for an event candidate.", "labels": [], "entities": []}, {"text": "Recurrent Neural Networks (RNNs), by contrast, is a natural solution to both problems above because it can be applied to inputs of variable length which eliminates both the requirement of single-token event trigger and the need fora fixed window size.", "labels": [], "entities": []}, {"text": "Using recurrent nodes with Long Short Term Memory (LSTM) or Gated Recurrent Units (GRU) (), RNN is potentially capable of selectively deciding the relevant context to consider for detecting events.", "labels": [], "entities": []}, {"text": "In this paper we present a forward-backward recurrent neural network (FBRNN) to extract (possibly multi-word) event mentions from raw text.", "labels": [], "entities": []}, {"text": "Although RNNs have been studied extensively in other NLP tasks, to the best of our knowledge, this is the first work to use RNNs for event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.7817421853542328}]}, {"text": "This is also one of the first efforts to handle multi-word event nuggets.", "labels": [], "entities": []}, {"text": "Experimental results confirm that FBRNN is competitive compared to the state-ofthe-art on the ACE 2005 dataset and the Rich ERE 2015 event detection task.", "labels": [], "entities": [{"text": "FBRNN", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.7851443886756897}, {"text": "ACE 2005 dataset", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9731072982152303}, {"text": "Rich ERE 2015", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.9408493836720785}, {"text": "event detection task", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.6984623670578003}]}], "datasetContent": [{"text": "In this section, we first empirically examine some design choices for our model and then compare the proposed model to the current state-of-the-art on two different event detection datasets.", "labels": [], "entities": [{"text": "event detection", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.7273588478565216}]}, {"text": "We experiment on two different corpora, ACE 2005 and Rich ERE 2015.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9471350610256195}, {"text": "Rich ERE 2015", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9002584417661031}]}, {"text": "\u2022 ACE 2005: The ACE 2005 corpus is annotated with single-token event triggers and has eight event types and 33 event subtypes that, along with the \"non-event\" class, constitutes a 34-class classification problem.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 2, "end_pos": 10, "type": "DATASET", "confidence": 0.8968137800693512}, {"text": "ACE 2005 corpus", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.879953404267629}]}, {"text": "In our experiments we used the same train, development and test sets as the previous studies on this dataset).", "labels": [], "entities": []}, {"text": "Candidate generation for this corpus is based on a list of candidate event trigger words created from the training data and the PPDB paraphrase database.", "labels": [], "entities": [{"text": "Candidate generation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7374877631664276}, {"text": "PPDB paraphrase database", "start_pos": 128, "end_pos": 152, "type": "DATASET", "confidence": 0.91207883755366}]}, {"text": "Given a sentence, we go over each token and extract the tokens that appear in this high-recall list as event candidates, which we then classify with our proposed FBRNN model.", "labels": [], "entities": [{"text": "FBRNN", "start_pos": 162, "end_pos": 167, "type": "DATASET", "confidence": 0.7129352688789368}]}, {"text": "\u2022 Rich ERE 2015: The Rich ERE 2015 corpus was released in the TAC 2015 competition and annotated at the nugget level, thus addressing phrasal event mentions.", "labels": [], "entities": [{"text": "Rich ERE 2015 corpus", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.8784675449132919}, {"text": "TAC 2015 competition", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.7650572260220846}]}, {"text": "The Rich ERE 2015 corpus has nine event types and 38 event subtypes, forming a 39-class classification problem (considering \"non-event\" as an additional class).", "labels": [], "entities": [{"text": "Rich ERE 2015 corpus", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9668732285499573}]}, {"text": "We utilized the same train and test sets that have been used in the TAC 2015 event nugget detection competition.", "labels": [], "entities": [{"text": "TAC 2015 event nugget detection competition", "start_pos": 68, "end_pos": 111, "type": "TASK", "confidence": 0.6925628284613291}]}, {"text": "A subset of the provided train set was set aside as our development set.", "labels": [], "entities": []}, {"text": "To generate event nugget candidates, we first followed the same strategy that we used for the ACE 2005 dataset experiment to identify singletoken event candidates.", "labels": [], "entities": [{"text": "ACE 2005 dataset experiment", "start_pos": 94, "end_pos": 121, "type": "DATASET", "confidence": 0.9431868493556976}]}, {"text": "We then expand the single-token event candidates using a heuristic rule based on POS tags.", "labels": [], "entities": []}, {"text": "There area number of hyper-parameters for our model, including the dimension of the branch embedding, the number of recurrent layers in each RNN, the size of the RNN outputs, the dropout rates for training the networks.", "labels": [], "entities": []}, {"text": "We tune these parameters using the development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on the development set with  different configurations on Rich ERE 2015.", "labels": [], "entities": [{"text": "Rich ERE 2015", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9725605845451355}]}, {"text": " Table 2: Comparison with reported performance  by event detection systems without using gold en- tity mentions and types on the ACE 2005 corpus.", "labels": [], "entities": [{"text": "event detection", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7495438754558563}, {"text": "ACE 2005 corpus", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.9782535036404928}]}, {"text": " Table 3: Performance of FBRNN compared with  reported top results in TAC competition (Mita- mura et al., 2015) on Rich ERE 2015.", "labels": [], "entities": [{"text": "FBRNN", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.7222442626953125}, {"text": "TAC competition (Mita- mura et al., 2015) on Rich ERE 2015", "start_pos": 70, "end_pos": 128, "type": "DATASET", "confidence": 0.7070407291253408}]}]}