{"title": [{"text": "Situation entity types: automatic classification of clause-level aspect", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the first robust approach to automatically labeling clauses with their situation entity type (Smith, 2003), capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure.", "labels": [], "entities": []}, {"text": "Previous work on this task used a small data set from a limited domain, and relied mainly on words as features, an approach which is impractical in larger settings.", "labels": [], "entities": []}, {"text": "We provide anew corpus of texts from 13 genres (40,000 clauses) annotated with situation entity types.", "labels": [], "entities": []}, {"text": "We show that our sequence labeling approach using distributional information in the form of Brown clusters, as well as syntactic-semantic features targeted to the task, is robust across genres, reaching accuracies of up to 76%.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6871569454669952}, {"text": "accuracies", "start_pos": 203, "end_pos": 213, "type": "METRIC", "confidence": 0.9938188195228577}]}], "introductionContent": [{"text": "Clauses in text have different aspectual properties, and thus contribute to the discourse in different ways.", "labels": [], "entities": []}, {"text": "Distinctions that have been made in the linguistic and semantic theory literature include the classification of states, events and processes, and whether clauses introduce particular eventualities or report regularities generalizing either over events or members of a kind (.", "labels": [], "entities": [{"text": "classification of states, events and processes", "start_pos": 94, "end_pos": 140, "type": "TASK", "confidence": 0.7758954848561969}]}, {"text": "Such aspectual distinctions are relevant to natural language processing tasks requiring text understanding such as information extraction) or temporal processing.", "labels": [], "entities": [{"text": "text understanding such as information extraction", "start_pos": 88, "end_pos": 137, "type": "TASK", "confidence": 0.6397315015395483}, {"text": "temporal processing", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7324696481227875}]}, {"text": "In this paper, we are concerned with automatically identifying the type of a situation entity (SE), which we assume to be expressed by a clause.", "labels": [], "entities": [{"text": "automatically identifying the type of a situation entity (SE)", "start_pos": 37, "end_pos": 98, "type": "TASK", "confidence": 0.5716796544465151}]}, {"text": "Specifically, we present a system for automatically labeling clauses using the inventory of STATE: The colonel owns the farm.", "labels": [], "entities": [{"text": "STATE", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.8758013248443604}]}, {"text": "EVENT: John won the race.", "labels": [], "entities": [{"text": "EVENT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9510418176651001}]}, {"text": "REPORT: \"...\", said Obama.", "labels": [], "entities": []}, {"text": "GENERIC SENTENCE: Generalizations over kinds.", "labels": [], "entities": [{"text": "GENERIC SENTENCE", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5567699074745178}]}, {"text": "The lion has a bushy tail.", "labels": [], "entities": []}, {"text": "GENERALIZING SENTENCE: Generalizations over events (habituals).", "labels": [], "entities": [{"text": "GENERALIZING SENTENCE", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5452315360307693}]}, {"text": "Mary often fed the cat last year.", "labels": [], "entities": []}, {"text": "QUESTION: Who wants to come?", "labels": [], "entities": []}, {"text": "IMPERATIVE: Hand me the pen!: SE types, adapted from.", "labels": [], "entities": [{"text": "IMPERATIVE", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9797977209091187}, {"text": "SE", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9946541786193848}]}, {"text": "SE types shown in).", "labels": [], "entities": []}, {"text": "The original motivation for the above inventory of SE types is the observation that different modes of discourse, a classification of linguistic properties of text at the passage level, have different distributions of SE types.", "labels": [], "entities": []}, {"text": "For example, EVENTs and STATEs are predominant in narrative passages, while GENERIC SENTENCEs occur frequently in information passages.", "labels": [], "entities": [{"text": "EVENTs", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9946505427360535}, {"text": "STATEs", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.899286150932312}]}, {"text": "A previous approach to automatically labeling SE types) -referred to here as UT07 -captures interesting insights, but is trained and evaluated on a relatively small amount of text (about 4300 clauses), mainly from one rather specialized subsection of the Brown corpus.", "labels": [], "entities": [{"text": "UT07", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8518938422203064}, {"text": "Brown corpus", "start_pos": 255, "end_pos": 267, "type": "DATASET", "confidence": 0.9006012976169586}]}, {"text": "The data shows a highly skewed distribution of SE types and was annotated in an intuitive fashion with only moderate agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9664134383201599}]}, {"text": "In addition, the UT07 system relies mostly on part-of-speech tags and words as features.", "labels": [], "entities": []}, {"text": "The latter are impractical when dealing with larger data sets and capture most of the corpus vocabulary, overfitting the model to the data set.", "labels": [], "entities": []}, {"text": "Despite this overfitting, the system's accuracy is only around 53%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.999729573726654}]}, {"text": "We address these shortcomings, developing a robust system that delivers high performance compared to the human upper bound across a range of genres.", "labels": [], "entities": []}, {"text": "Our approach uses features which increase robustness: Brown clusters and syntactic-semantic features.", "labels": [], "entities": []}, {"text": "Our models for labeling texts with the aspectual properties of clauses in the form of SE types reach accuracies of up to 76%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9628973007202148}]}, {"text": "In an oracle experiment, show that including the gold labels of the previous clauses as features into their maximum entropy model is beneficial.", "labels": [], "entities": []}, {"text": "We implement the first true sequence labeling model for SE types, using conditional random fields to find the globallybest sequence of labels for the clauses in a document.", "labels": [], "entities": []}, {"text": "Performance increases by around 2% absolute compared to predicting labels for clauses separately; much of this effect stems from the fact that GENERIC SENTENCEs often occur together.", "labels": [], "entities": [{"text": "GENERIC SENTENCEs", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.6398779898881912}]}, {"text": "Moving well beyond the single-domain setting, our models are trained and evaluated on a multigenre corpus of approximately 40,000 clauses from MASC ( and Wikipedia which have been annotated with substantial agreement.", "labels": [], "entities": [{"text": "MASC", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.9127780199050903}]}, {"text": "We train and test our models both within genres and across genres, highlighting differences between genres and creating models that are robust across genres.", "labels": [], "entities": []}, {"text": "Both the corpus and the code for an SE type labeler are freely available.", "labels": [], "entities": []}, {"text": "1 These form the basis for future research on SE types and related aspectual phenomena and will enable the inclusion of SE type information as a preprocessing step into various NLP tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we present our experiments on SE type classification, beginning with a (near) replication of the UT07 system, and moving onto evaluate our new approach from multiple perspectives.", "labels": [], "entities": [{"text": "SE type classification", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9299904902776083}, {"text": "UT07 system", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.923121839761734}]}, {"text": "We develop our models using 10-fold cross validation (CV) on 80% (counted in terms of the number of SEs) of the MASC and Wikipedia data (a total of 32,855 annotated SEs), keeping the remaining 20% as a held-out test set.", "labels": [], "entities": [{"text": "MASC and Wikipedia data", "start_pos": 112, "end_pos": 135, "type": "DATASET", "confidence": 0.7571766525506973}]}, {"text": "Development and test sets each contain distinct sets of documents; the documents of each MASC genre and of Wikipedia are distributed over the folds.", "labels": [], "entities": []}, {"text": "We report results in terms of macro-average precision, macro-average recall, macro-average F1-measure (harmonic mean of macro-average precision and macro-average recall), and accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9011048078536987}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9368431568145752}, {"text": "F1-measure", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.7459859848022461}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.901842474937439}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9989546537399292}]}, {"text": "We apply McNemar's test) with p < 0.01 to test significance of differences inaccuracy.", "labels": [], "entities": [{"text": "significance", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9577792286872864}]}, {"text": "In the following tables, we mark numerically-close scores with the same symbols if they are found to be significantly different.", "labels": [], "entities": []}, {"text": "Upper bound: human performance.", "labels": [], "entities": []}, {"text": "Labeling clauses with their SE types is a non-trivial task even for humans, as there are many borderline cases (see Sections 4 and 8).", "labels": [], "entities": [{"text": "Labeling clauses", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.892179548740387}]}, {"text": "We compute an upper bound for system performance by iterating overall clauses: for each pair of human annotators, two entries are added to a co-occurrence matrix (similar to a confusion matrix), with each label serving once as \"gold standard\" and once as the \"prediction.\"", "labels": [], "entities": []}, {"text": "From this matrix, we can compute scores in the same manner as for system predictions.", "labels": [], "entities": []}, {"text": "Precision and recall scores are symmetric in this case, and accuracy corresponds to observed agreement.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9900882840156555}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9808660745620728}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9995226860046387}]}], "tableCaptions": [{"text": " Table 1: SE-labeled corpora: size and agreement.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of SE types in gold stan- dard (%). *Krippendorff's diagnostics.", "labels": [], "entities": [{"text": "SE", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9630959630012512}]}, {"text": " Table 5: Accuracy on Brown. Test set majority  class (STATE) is 35.3%. LB = results for best look- back settings in MaxEnt. 787 test instances.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981666803359985}, {"text": "Test set majority  class (STATE)", "start_pos": 29, "end_pos": 61, "type": "METRIC", "confidence": 0.7033507781369346}, {"text": "LB", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9921959638595581}, {"text": "MaxEnt. 787 test instances", "start_pos": 117, "end_pos": 143, "type": "DATASET", "confidence": 0.8967043280601501}]}, {"text": " Table 6: Impact of different feature sets.  Wiki+MASC dev set, CRF, 10-fold CV.", "labels": [], "entities": [{"text": "Wiki+MASC dev set", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.7978588342666626}, {"text": "CRF", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.7224193811416626}]}, {"text": " Table 7: Results on MASC+Wiki held-out test set  (7937 test instances).", "labels": [], "entities": [{"text": "MASC+Wiki held-out test set", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.9002492527167002}]}, {"text": " Table 8: Impact of feature groups: ablation  Wiki+MASC dev set, CRF, 10-fold CV. All accu- racies for ablation settings are significantly differ- ent from A+B.", "labels": [], "entities": [{"text": "CRF", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8165509700775146}]}, {"text": " Table 9: Impact of sequence information: (F1  by SE type): CRF, Masc+Wiki, 10-fold CV.", "labels": [], "entities": [{"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9807084202766418}, {"text": "SE", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.8805367946624756}, {"text": "CRF", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8551718592643738}]}, {"text": " Table 10: Impact of in-genre training data. F1- score by SE type, CRF, MASC+Wiki dev.", "labels": [], "entities": [{"text": "F1- score", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9877587159474691}, {"text": "SE type", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.967646598815918}, {"text": "MASC", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.5065389275550842}]}]}