{"title": [], "abstractContent": [{"text": "A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those that yield the correct denotation-from a combina-torially large space.", "labels": [], "entities": []}, {"text": "To control the search space, previous work relied on restricted set of rules, which limits expressivity.", "labels": [], "entities": []}, {"text": "In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms.", "labels": [], "entities": []}, {"text": "Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance.", "labels": [], "entities": []}, {"text": "To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms.", "labels": [], "entities": []}, {"text": "On the WIKITABLEQUESTIONS dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms.", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS dataset", "start_pos": 7, "end_pos": 33, "type": "DATASET", "confidence": 0.9750163555145264}]}], "introductionContent": [{"text": "Consider the task of learning to answer complex natural language questions (e.g., \"Where did the last 1st place finish occur?\") using only question-answer pairs as supervision.", "labels": [], "entities": []}, {"text": "Semantic parsers map the question into a logical form (e.g., R.argmax) that can be executed on a knowledge source to obtain the answer (denotation).", "labels": [], "entities": []}, {"text": "Logical forms are very expressive since they can be recursively composed, but this very expressivity makes it more difficult to search over the space of logical forms.", "labels": [], "entities": []}, {"text": "Previous work sidesteps this obstacle by restricting the set of possible logical form compositions, but this is limiting.", "labels": [], "entities": []}, {"text": "For instance, for the system in, in only 53.5% of the examples was the correct logical form even in the set of generated logical forms.", "labels": [], "entities": []}, {"text": "The goal of this paper is to solve two main challenges that prevent us from generating more expressive logical forms.", "labels": [], "entities": []}, {"text": "The first challenge is computational: the number of logical forms grows exponentially as their size increases.", "labels": [], "entities": []}, {"text": "Directly enumerating overall logical forms becomes infeasible, and pruning techniques such as beam search can inadvertently prune out correct logical forms.", "labels": [], "entities": []}, {"text": "The second challenge is the large increase in spurious logical forms-those that do not reflect the semantics of the question but coincidentally execute to the correct denotation.", "labels": [], "entities": []}, {"text": "For example, while logical forms z 1 , . .", "labels": [], "entities": []}, {"text": ", z 5 in are all consistent (they execute to the correct answer y), the logical forms z 4 and z 5 are spurious and would give incorrect answers if the table were to change.", "labels": [], "entities": []}, {"text": "We address these two challenges by solving two interconnected tasks.", "labels": [], "entities": []}, {"text": "The first task, which addresses the computational challenge, is to enumerate the set Z of all consistent logical forms given a question x, a knowledge source w (\"world\"), and the target denotation y (Section 4).", "labels": [], "entities": []}, {"text": "Observing that the space of possible denotations grows much more slowly than the space of logical forms, we perform dynamic programming on denotations (DPD) to make search feasible.", "labels": [], "entities": []}, {"text": "Our method is guaranteed to find all consistent logical forms up to some bounded size.", "labels": [], "entities": []}, {"text": "Given the set Z of consistent logical forms, the second task is to filter out spurious logical forms from Z (Section 5).", "labels": [], "entities": []}, {"text": "Using the property that spurious logical forms ultimately give a wrong answer when the data in the world w changes, we create x: \"Where did the last 1st place finish occur?\" y: Thailand Consistent Correct z1: R.argmax(Position.1st, Index) Among rows with Position = 1st, pick the one with maximum index, then return the Venue of that row.", "labels": [], "entities": [{"text": "Thailand", "start_pos": 177, "end_pos": 185, "type": "DATASET", "confidence": 0.8378299474716187}, {"text": "Venue", "start_pos": 320, "end_pos": 325, "type": "METRIC", "confidence": 0.9814261794090271}]}, {"text": "z2: R.Index.max(R.Position.1st) Find the maximum index of rows with Position = 1st, then return the Venue of the row with that index.", "labels": [], "entities": [{"text": "R.Index.max", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.935706615447998}, {"text": "Venue", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9726922512054443}]}, {"text": "The first five are consistent: they execute to the correct answer y.", "labels": [], "entities": []}, {"text": "Of those, correct logical forms z 1 , z 2 , and z 3 are different ways to represent the semantics of x, while spurious logical forms z 4 and z 5 get the right answer y for the wrong reasons.", "labels": [], "entities": []}, {"text": "fictitious worlds to test the denotations of the logical forms in Z.", "labels": [], "entities": []}, {"text": "We use crowdsourcing to annotate the correct denotations on a subset of the generated worlds.", "labels": [], "entities": []}, {"text": "To reduce the amount of annotation needed, we choose the subset that maximizes the expected information gain.", "labels": [], "entities": []}, {"text": "The pruned set of logical forms would provide a stronger supervision signal for training a semantic parser.", "labels": [], "entities": []}, {"text": "We test our methods on the WIKITABLEQUES-TIONS dataset of complex questions on Wikipedia tables.", "labels": [], "entities": [{"text": "WIKITABLEQUES-TIONS dataset of complex questions on Wikipedia tables", "start_pos": 27, "end_pos": 95, "type": "DATASET", "confidence": 0.9000184088945389}]}, {"text": "We define a simple, general set of deduction rules (Section 3), and use DPD to confirm that the rules generate a correct logical form in . .", "labels": [], "entities": [{"text": "DPD", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9360724091529846}]}, {"text": "76% of the examples, up from the 53.5% in Pasupat and Liang (2015).", "labels": [], "entities": []}, {"text": "Moreover, unlike beam search, DPD is guaranteed to find all consistent logical forms up to a bounded size.", "labels": [], "entities": [{"text": "beam search", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8935663402080536}]}, {"text": "Finally, by using annotated data on fictitious worlds, we are able to prune out 92.1% of the spurious logical forms.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments, we use the training portion of the WIKITABLEQUESTIONS dataset, which consists of 14,152 questions on 1,679 Wikipedia tables gathered by crowd workers.", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS dataset", "start_pos": 56, "end_pos": 82, "type": "DATASET", "confidence": 0.9549057185649872}]}, {"text": "Answering these complex questions requires different types of operations.", "labels": [], "entities": []}, {"text": "The same operation can be phrased in different ways (e.g., \"best\", \"top ranking\", or \"lowest ranking number\") and the interpretation of some phrases depend on the context (e.g., \"number of \" could be a table lookup or a count operation).", "labels": [], "entities": []}, {"text": "The lexical content of the questions is also quite diverse: even excluding numbers and symbols, the 14,152 training examples contain 9,671 unique words, only 10% of which appear more than 10 times.", "labels": [], "entities": []}, {"text": "We attempted to manually annotate the first 300 examples with lambda DCS logical forms.", "labels": [], "entities": []}, {"text": "We successfully constructed correct logical forms for 84% of these examples, which is a good number considering the questions were created by humans who could use the table however they wanted.", "labels": [], "entities": []}, {"text": "The remaining 16% reflect limitations in our setupfor example, non-canonical table layouts, answers appearing in running text or images, and commonsense reasoning (e.g., knowing that \"Quarterfinal\" is better than \"Round of 16\").", "labels": [], "entities": []}], "tableCaptions": []}