{"title": [], "abstractContent": [{"text": "We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods.", "labels": [], "entities": []}, {"text": "Our results show that contrary to the common belief that the number of latent states for each nontermi-nal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nontermi-nals.", "labels": [], "entities": [{"text": "parsing", "start_pos": 171, "end_pos": 178, "type": "TASK", "confidence": 0.9651323556900024}]}, {"text": "In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean , Polish and Swedish.", "labels": [], "entities": []}, {"text": "Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.8573294281959534}]}, {"text": "They were introduced in the NLP community by and, with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them.", "labels": [], "entities": []}, {"text": "Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers.", "labels": [], "entities": [{"text": "syntactic parsing of English", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.8262061327695847}]}, {"text": "showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser -the number of latent states associated with the various nonterminals in the grammar.", "labels": [], "entities": []}, {"text": "As such, they introduced a coarse-to-fine technique to estimate the grammar.", "labels": [], "entities": []}, {"text": "It splits and merges nonterminals (with latent state information) with the aim to optimize the likelihood of the training data.", "labels": [], "entities": []}, {"text": "Together with other types of fine tuning of the parsing model, this led to state-of-the-art results for English parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.972170889377594}, {"text": "English parsing", "start_pos": 104, "end_pos": 119, "type": "TASK", "confidence": 0.5342158228158951}]}, {"text": "In more recent work, described a different family of estimation algorithms for L-PCFGs.", "labels": [], "entities": []}, {"text": "This so-called \"spectral\" family of learning algorithms is compelling because it offers a rigorous theoretical analysis of statistical convergence, and sidesteps local maxima issues that arise with the EM algorithm.", "labels": [], "entities": [{"text": "statistical convergence", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.7946058213710785}]}, {"text": "While spectral algorithms for L-PCFGs are compelling from a theoretical perspective, they have been lagging behind in their empirical results on the problem of parsing.", "labels": [], "entities": []}, {"text": "In this paper we show that one of the main reasons for that is that spectral algorithms require a more careful tuning procedure for the number of latent states than that which has been advocated for until now.", "labels": [], "entities": []}, {"text": "Ina sense, the relationship between our work and the work of is analogous to the relationship between the work by and the work by: we suggest a technique for optimizing the number of latent states for spectral algorithms, and test it on eight languages.", "labels": [], "entities": []}, {"text": "Our results show that when the number of latent states is optimized using our technique, the parsing models the spectral algorithms yield perform significantly better than the vanilla-estimated models, and for most of the languages -better than the Berkeley parser of.", "labels": [], "entities": []}, {"text": "As such, the contributions of this parser are twofold: \u2022 We describe a search algorithm for optimiz-ing the number of latent states for spectral learning.", "labels": [], "entities": [{"text": "spectral learning", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7423697710037231}]}, {"text": "\u2022 We describe an analysis of spectral algorithms on eight languages (until now the results of L-PCFG estimation with spectral algorithms for parsing were known only for English).", "labels": [], "entities": []}, {"text": "Our parsing algorithm is rather language-generic, and does not require significant linguistically-oriented adjustments.", "labels": [], "entities": []}, {"text": "In addition, we dispel the common wisdom that more data is needed with spectral algorithms.", "labels": [], "entities": []}, {"text": "Our models yield high performance on treebanks of varying sizes from 5,000 sentences (Hebrew and Swedish) to 40,472 sentences (German).", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In \u00a72 we describe notation and background.", "labels": [], "entities": []}, {"text": "\u00a73 further investigates the need for an optimization of the number of latent states in spectral learning and describes our optimization algorithm, a search algorithm akin to beam search.", "labels": [], "entities": []}, {"text": "In \u00a74 we describe our experiments with natural language parsing for Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7449756662050883}]}, {"text": "We conclude in \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our setup for parsing experiments on a range of languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9652935862541199}]}, {"text": "Datasets We experiment with nine treebanks consisting of eight different morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish.", "labels": [], "entities": []}, {"text": "shows the statistics of 9 different treebanks with their splits into training, development and test sets. and Swedish) are taken from the workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL;).", "labels": [], "entities": [{"text": "Statistical Parsing of Morphologically Rich Languages (SPMRL", "start_pos": 150, "end_pos": 210, "type": "TASK", "confidence": 0.8642454519867897}]}, {"text": "The German corpus in the SPMRL workshop is taken from the TiGer corpus (German-T,).", "labels": [], "entities": [{"text": "German corpus", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.820457935333252}, {"text": "TiGer corpus", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.8672604560852051}]}, {"text": "We also experiment with another German corpus, the NEGRA corpus (German-N,, in a standard evaluation split.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9323780536651611}]}, {"text": "Words in the SPMRL datasets are annotated with their morphological signatures, whereas the NEGRA corpus does not contain any morphological information.", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.7556296586990356}, {"text": "NEGRA corpus", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9371289014816284}]}, {"text": "Data preprocessing and treatment of rare words We convert all trees in the treebanks to a binary form, train and run the parser in that form, and then transform back the trees when doing evaluation using the PARSEVAL metric.", "labels": [], "entities": [{"text": "Data preprocessing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6205311715602875}, {"text": "PARSEVAL", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.6849288940429688}]}, {"text": "In addition, we collapse unary rules into unary chains, so that our trees are fully binarized.", "labels": [], "entities": []}, {"text": "The column \"#nts\" in shows the number of nonterminals after binarization in the various treebanks.", "labels": [], "entities": []}, {"text": "Before binarization, we also drop all functional information from the nonterminals.", "labels": [], "entities": []}, {"text": "We use fine tags for all languages except Korean.", "labels": [], "entities": []}, {"text": "For Korean, there are 2,825 binarized nonterminals making it impractical to use our optimization algorithm, so we use the coarse tags.", "labels": [], "entities": []}, {"text": "have shown that the morphological signatures for rare words are useful to improve the performance of the Berkeley parser.", "labels": [], "entities": []}, {"text": "In our preliminary experiments with na\u00a8\u0131vena\u00a8\u0131ve spectral estimation, we preprocess rare words in the training set in two ways: (i) we replace them with their corresponding POS tags, and (ii) we replace them with their corresponding POS+morphological signatures.", "labels": [], "entities": [{"text": "na\u00a8\u0131vena\u00a8\u0131ve spectral estimation", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.6047525703907013}]}, {"text": "We follow and consider a word to be rare if it occurs less than 20 times in the training data.", "labels": [], "entities": []}, {"text": "We experimented both with aversion of the parser that does not ignore and does ignore letter cases, and discovered that the parser behaves better when case is not ignored.", "labels": [], "entities": []}, {"text": "Spectral algorithms: subroutine choices The latent state optimization algorithm will work with either the clustering estimation algorithm of or the spectral algorithm of.", "labels": [], "entities": []}, {"text": "In our setup, we first run the latent state optimization algorithm with the clustering algorithm.", "labels": [], "entities": [{"text": "latent state optimization", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6180224219957987}]}, {"text": "We then run the spectral algorithm once with the optimized f from the clustering algorithm.", "labels": [], "entities": []}, {"text": "We do that because the clustering algorithm is significantly faster to iteratively parse the development set, because it leads to sparse estimates.", "labels": [], "entities": []}, {"text": "Our optimization algorithm is sensitive to the initialization of the number of latent states assigned to each nonterminals as it sequentially goes through the list of nonterminals and chooses latent state numbers for each nonterminal, keeping latent state numbers for other nonterminals fixed.", "labels": [], "entities": []}, {"text": "In our setup, we start our search algorithm with the best model from the clustering algorithm, controlling for all hyperparameters; we tune f , the function which maps each nonterminal to a fixed number of latent states m, by running the vanilla version with different values of m for different languages.", "labels": [], "entities": []}, {"text": "Based on our preliminary experiments, we set m to 4 for Basque, Hebrew, Polish and Swedish; 8 for German-N; 16 for German-T, Hungarian and Korean; and 24 for French.", "labels": [], "entities": []}, {"text": "We use the same features for the spectral methods as in for German-N. For the SPMRL datasets we do not use the head features.", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9032002985477448}]}, {"text": "These require linguistic understanding of the datasets (because they require head rules for propagating leaf nodes in the tree), and we discovered that simple heuristics for constructing these rules did not yield an increase in performance.", "labels": [], "entities": []}, {"text": "We use the kmeans function in Matlab to do the clustering for the spectral algorithm of.", "labels": [], "entities": [{"text": "Matlab", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9564818739891052}]}, {"text": "We experimented with several versions of k-means, and discovered that the version that works best in a set of preliminary experiments is hard k-means.", "labels": [], "entities": []}, {"text": "Decoding and evaluation For efficiency, we use abase PCFG without latent states to prune marginals which receive a valueless than 0.00005 in the dynamic programming chart.", "labels": [], "entities": []}, {"text": "This is just a bare-bones PCFG that is estimated using maximum likelihood estimation (with frequency count).", "labels": [], "entities": []}, {"text": "The parser takes part-of-speech tagged sentences as input.", "labels": [], "entities": []}, {"text": "We tag the German-N data using the Turbo Tagger (.", "labels": [], "entities": [{"text": "German-N data", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.971468448638916}]}, {"text": "For the languages in the SPMRL data we use the MarMot tagger of to jointly predict the POS and morphological tags.", "labels": [], "entities": [{"text": "SPMRL data", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8704015016555786}]}, {"text": "The parser itself can assign different part-of-speech tags to words to avoid parse failure.", "labels": [], "entities": []}, {"text": "This is also particularly important for constituency parsing with morphologically rich languages.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8875675797462463}]}, {"text": "It helps mitigate the problem of the taggers to assign correct tags when long-distance dependencies are present.", "labels": [], "entities": []}, {"text": "For all results, we report the F 1 measure of the PARSEVAL metric (.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.985131025314331}, {"text": "PARSEVAL metric", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9054294228553772}]}, {"text": "We use the EVALB program 7 with the parameter file COLLINS.prm) for the German-N data and the SPMRL parameter file, spmrl.prm, for the SPMRL data (.", "labels": [], "entities": [{"text": "EVALB program 7", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.8833453059196472}, {"text": "COLLINS.prm", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9045024514198303}, {"text": "German-N data", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9768351316452026}, {"text": "SPMRL data", "start_pos": 135, "end_pos": 145, "type": "DATASET", "confidence": 0.8022588789463043}]}, {"text": "In this setup, the latent state optimization algorithm terminates in few hours for all datasets except French and German-T. The German-T data has 762 nonterminals to tune over a large development set consisting of 5,000 sentences, whereas, the French data has a high average sentence length of 31.43 in the development set.", "labels": [], "entities": [{"text": "German-T data", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.8351271152496338}]}, {"text": "Following, we further improve our results by using multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model.", "labels": [], "entities": []}, {"text": "Using the optimized f , we estimate lang.", "labels": [], "entities": []}, {"text": "Basque French German-N German-: Results on the development datasets.", "labels": [], "entities": [{"text": "Basque French German-N German-", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.8679440498352051}]}, {"text": "\"Bk\" makes use of the Berkeley parser with its coarse-to-fine mechanism to optimize the number of latent states ().", "labels": [], "entities": []}, {"text": "For Bk, \"van\" uses the vanilla treatment of rare words using signatures defined by, whereas \"rep.\" uses the morphological signatures instead.", "labels": [], "entities": []}, {"text": "\"Cl\" uses the algorithm of Narayan and Cohen (2015) and \"Sp\" uses the algorithm of.", "labels": [], "entities": []}, {"text": "In Cl, \"van (pos)\" and \"van (rep)\" are vanilla estimations (i.e., each nonterminal is mapped to fixed number of latent states) replacing rare words by POS or POS+morphological signatures, respectively.", "labels": [], "entities": []}, {"text": "The best of these two models is used with our optimization algorithm in \"opt\".", "labels": [], "entities": []}, {"text": "For Sp, \"van\" uses the best setting for unknown words as Cl.", "labels": [], "entities": []}, {"text": "Best result in each column from the first seven rows is in bold.", "labels": [], "entities": []}, {"text": "In addition, our best performing models from rows 3-7 are marked with * . \"Bk multiple\" shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking).", "labels": [], "entities": [{"text": "Bk multiple", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9605468213558197}]}, {"text": "\"Cl multiple\" gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding: Results on the test datasets.", "labels": [], "entities": []}, {"text": "\"Bk\" denotes the best Berkeley parser result reported by the shared task organizers).", "labels": [], "entities": [{"text": "Bk", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9544332027435303}]}, {"text": "For the German-N data, Bk results are taken from Petrov (2010).", "labels": [], "entities": [{"text": "German-N data", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9663273990154266}, {"text": "Bk", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9846301674842834}]}, {"text": "\"Cl van\" shows the performance of the best vanilla models from on the test set.", "labels": [], "entities": [{"text": "Cl van\"", "start_pos": 1, "end_pos": 8, "type": "DATASET", "confidence": 0.8227503498395284}]}, {"text": "\"Cl opt\" and \"Sp opt\" give the result of our algorithm on the test set.", "labels": [], "entities": []}, {"text": "We also include results from, and.", "labels": [], "entities": []}, {"text": "80 models for each of noise induction mechanisms in Narayan and Cohen: Dropout, Gaussian (additive) and Gaussian (multiplicative).", "labels": [], "entities": []}, {"text": "To decode with multiple noisy models, we train the MaxEnt reranker of.", "labels": [], "entities": []}, {"text": "Hierarchical decoding with \"maximal tree coverage\" over MaxEnt models, further improves our accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9980193376541138}]}, {"text": "See Narayan and Cohen (2015) for more details on the estimation of a diverse set of models, and on decoding with them.", "labels": [], "entities": []}, {"text": "estimates than the dense estimates of.", "labels": [], "entities": []}, {"text": "10 Implementation: https://github.com/BLLIP/ bllip-parser.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 3, "end_pos": 17, "type": "METRIC", "confidence": 0.9523913860321045}, {"text": "BLLIP", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9217635989189148}]}, {"text": "More specifically, we used the programs extract-spfeatures, cvlm-lbfgs and best-indices.", "labels": [], "entities": []}, {"text": "extract-spfeatures uses head features, we bypass this for the SPMRL datasets by creating a dummy heads.cc file.", "labels": [], "entities": [{"text": "SPMRL datasets", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.8141634166240692}]}, {"text": "cvlm-lbfgs was used with the default hyperparameters from the Makefile.", "labels": [], "entities": [{"text": "Makefile", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9906805157661438}]}, {"text": "give the results for the various languages.", "labels": [], "entities": []}, {"text": "Our main focus is on comparing the coarse-to-fine Berkeley parser () to our method.", "labels": [], "entities": []}, {"text": "However, for the sake of completeness, we also present results for other parsers, such as parsers of,.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the different datasets used in our experiments for the training (\"train\"), development (\"dev\") and test", "labels": [], "entities": []}, {"text": " Table 2: Results on the development datasets. \"Bk\" makes use of the Berkeley parser with its coarse-to-fine mechanism to", "labels": [], "entities": []}, {"text": " Table 3: Results on the test datasets. \"Bk\" denotes the best Berkeley parser result reported by the shared task organizers", "labels": [], "entities": [{"text": "Bk", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9341559410095215}]}, {"text": " Table 5: A comparison of the number of latent states for each preterminal for the German-N model, before (\"b.\") running the", "labels": [], "entities": []}]}