{"title": [{"text": "Evaluating the Utility of Vector Differences for Lexical Relation Learning", "labels": [], "entities": [{"text": "Lexical Relation", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.9016034007072449}]}], "abstractContent": [{"text": "Recent work has shown that simple vector subtraction over word embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision.", "labels": [], "entities": []}, {"text": "Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated.", "labels": [], "entities": [{"text": "word analogy prediction formulation", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.8670807480812073}]}, {"text": "In this paper, we carryout such an evaluation in two learning settings: (1) spectral clustering to induce word relations , and (2) supervised learning to classify vector differences into relation types.", "labels": [], "entities": []}, {"text": "We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to abroad range of relations, including over unseen lexical items.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to identify lexical relations is a fundamental task in natural language processing (\"NLP\"), and can contribute to many NLP applications including paraphrasing and generation, machine translation, and ontology building (.", "labels": [], "entities": [{"text": "paraphrasing and generation", "start_pos": 155, "end_pos": 182, "type": "TASK", "confidence": 0.737650195757548}, {"text": "machine translation", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.8221534490585327}, {"text": "ontology building", "start_pos": 209, "end_pos": 226, "type": "TASK", "confidence": 0.7964199483394623}]}, {"text": "Recently, attention has been focused on identifying lexical relations using word embeddings, which are dense, low-dimensional vectors obtained either from a \"predict-based\" neural network trained to predict word contexts, or a \"countbased\" traditional distributional similarity method combined with dimensionality reduction.", "labels": [], "entities": []}, {"text": "The skipgram model of and other similar language models have been shown to perform well on an analogy completion task (, in the space of relational similarity prediction, where the task is to predict the missing word in analogies such as A:B :: C: -?-.", "labels": [], "entities": [{"text": "relational similarity prediction", "start_pos": 137, "end_pos": 169, "type": "TASK", "confidence": 0.7319010297457377}]}, {"text": "A well-known example involves predicting the vector queen from the vector combination king \u2212 man + woman, where linear operations on word vectors appear to capture the lexical relation governing the analogy, in this case OPPOSITE-GENDER.", "labels": [], "entities": []}, {"text": "The results extend to several semantic relations such as CAPITAL-OF (paris\u2212france+poland \u2248 warsaw) and morphosyntactic relations such as PLURALISATION (cars \u2212 car + apple \u2248 apples).", "labels": [], "entities": [{"text": "PLURALISATION", "start_pos": 137, "end_pos": 150, "type": "METRIC", "confidence": 0.9738551378250122}]}, {"text": "Remarkably, since the model is not trained for this task, the relational structure of the vector space appears to bean emergent property.", "labels": [], "entities": []}, {"text": "The key operation in these models is vector difference, or vector offset.", "labels": [], "entities": []}, {"text": "For example, the paris \u2212 france vector appears to encode CAPITAL-OF, presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (.", "labels": [], "entities": []}, {"text": "The success of the simple offset method on analogy completion suggests that the difference vectors (\"DIFFVEC\" hereafter) must themselves be meaningful: their direction and/or magnitude encodes a lexical relation.", "labels": [], "entities": [{"text": "analogy completion", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8730863034725189}, {"text": "DIFFVEC", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.8981843590736389}]}, {"text": "Previous analogy completion tasks used with word embeddings have limited coverage of lexical relation types.", "labels": [], "entities": [{"text": "analogy completion tasks", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.8016189138094584}]}, {"text": "Moreover, the task does not explore the full implications of DIFFVECs as meaningful vector space objects in their own right, because it only looks fora one-best answer to the particular lexical analogies in the test set.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew, larger dataset covering many well-known lexical relation types from the linguistics and cognitive science literature.", "labels": [], "entities": []}, {"text": "We then apply DIFFVECs to two new tasks: unsupervised and supervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7074963450431824}]}, {"text": "First, we cluster the DIFFVECs to test whether the clusters map onto true lexical relations.", "labels": [], "entities": []}, {"text": "We find that the clustering works remarkably well, although syntactic relations are captured better than semantic ones.", "labels": [], "entities": []}, {"text": "Second, we perform classification over the DIFFVECs and obtain remarkably high accuracy in a closed-world setting (over a predefined set of word pairs, each of which corresponds to a lexical relation in the training data).", "labels": [], "entities": [{"text": "DIFFVECs", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8547757863998413}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9989277720451355}]}, {"text": "When we move to an open-world setting including random word pairs -many of which do not correspond to any lexical relation in the training data -the results are poor.", "labels": [], "entities": []}, {"text": "We then investigate methods for better attuning the learned class representation to the lexical relations, focusing on methods for automatically synthesising negative instances.", "labels": [], "entities": []}, {"text": "We find that this improves the model performance substantially.", "labels": [], "entities": []}, {"text": "We also find that hyper-parameter optimised count-based methods are competitive with predictbased methods under both clustering and supervised relation classification, inline with the findings of.", "labels": [], "entities": [{"text": "supervised relation classification", "start_pos": 132, "end_pos": 166, "type": "TASK", "confidence": 0.6598128378391266}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The pre-trained word embeddings used in  our experiments, with the number of dimensions  and size of the training data (in word tokens). The  models trained on English Wikipedia (\"wiki\") are  in the lower half of the table.", "labels": [], "entities": []}, {"text": " Table 3: The entropy for each lexical relation over  the clustering output for each set of pre-trained  word embeddings.", "labels": [], "entities": []}, {"text": " Table 4: F-scores (F) for CLOSED-WORLD classi- fication, for a baseline method based on clustering  + majority-class labelling, a multiclass linear SVM  trained on w2v, w2v wiki and SVD wiki DIFFVEC  inputs.", "labels": [], "entities": [{"text": "F-scores (F)", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9119588136672974}]}, {"text": " Table 5: Precision (P) and recall (R) for OPEN- WORLD classification, using the binary classifier  without (\"Orig\") and with (\"+neg\") negative sam- ples .", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9422495812177658}, {"text": "recall (R)", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9481060355901718}, {"text": "OPEN- WORLD classification", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.7126402258872986}]}]}