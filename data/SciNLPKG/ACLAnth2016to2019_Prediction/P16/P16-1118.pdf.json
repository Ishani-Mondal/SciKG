{"title": [{"text": "Addressing Limited Data for Textual Entailment Across Domains", "labels": [], "entities": [{"text": "Textual Entailment Across Domains", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.8091620951890945}]}], "abstractContent": [{"text": "We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7702483534812927}]}, {"text": "To that end, we first create (for experimental purposes) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains.", "labels": [], "entities": []}, {"text": "We then explore self-training and active learning strategies to address the lack of labeled data.", "labels": [], "entities": []}, {"text": "With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data.", "labels": [], "entities": [{"text": "ENT", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.6104663014411926}, {"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.999221682548523}, {"text": "F-score", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9988439083099365}]}, {"text": "On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Textual entailment is the task of automatically determining whether a natural language hypothesis can be inferred from a given piece of natural language text.", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7577067911624908}]}, {"text": "The RTE challenges) have spurred considerable research in textual entailment over newswire data.", "labels": [], "entities": [{"text": "RTE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.717289388179779}, {"text": "textual entailment", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7196823358535767}]}, {"text": "This, along with the availability of large-scale datasets labeled with entailment information (, has resulted in a variety of approaches for textual entailment recognition.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 141, "end_pos": 171, "type": "TASK", "confidence": 0.790797770023346}]}, {"text": "* This work was conducted during an internship at IBM A variation of this task, dubbed textual entailment search, has been the focus of RTE-5 and subsequent challenges, where the goal is to find all sentences in a corpus that entail a given hypothesis.", "labels": [], "entities": [{"text": "textual entailment search", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.7143666446208954}, {"text": "RTE-5", "start_pos": 136, "end_pos": 141, "type": "TASK", "confidence": 0.4672362804412842}]}, {"text": "The mindshare created by those challenges and the availability of the datasets has spurred many creative solutions to this problem.", "labels": [], "entities": []}, {"text": "However, the evaluations have been restricted primarily to these datasets, which are in the newswire domain.", "labels": [], "entities": []}, {"text": "Thus, much of the existing state-of-the-art research has focused on solutions that are effective in this domain.", "labels": [], "entities": []}, {"text": "It is easy to see though, that entailment search has potential applications in other domains too.", "labels": [], "entities": [{"text": "entailment search", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7765650749206543}]}, {"text": "For instance, in the clinical domain we imagine entailment search can be applied for clinical trial matching as one example.", "labels": [], "entities": [{"text": "clinical trial matching", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.6627206007639567}]}, {"text": "Inclusion criteria fora clinical trial (for e.g., patient is a smoker) become the hypotheses, and the patient's electronic health records are the text for entailment search.", "labels": [], "entities": []}, {"text": "Clearly, an effective textual entailment search system could possibly one day fully automate clinical trial matching.", "labels": [], "entities": [{"text": "clinical trial matching", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.5810502767562866}]}, {"text": "Developing an entailment system that works well in the clinical domain and, thus, automates this matching process, requires lots of labeled data, which is extremely scant in the clinical domain.", "labels": [], "entities": []}, {"text": "Generating such a dataset is tedious and costly, primarily because it requires medical domain expertise.", "labels": [], "entities": []}, {"text": "Moreover, there are always privacy concerns in releasing such a dataset to the community.", "labels": [], "entities": []}, {"text": "Taking this into consideration, we investigate the problem of textual entailment in a low-resource setting.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7834343314170837}]}, {"text": "We begin by creating a dataset in the clinical domain, and a supervised entailment system that is competitive on multiple domains -newswire as well as clinical.", "labels": [], "entities": []}, {"text": "We then present our work on selftraining and active learning to address the lack of a large-scale labeled dataset.", "labels": [], "entities": []}, {"text": "Our self-training system results in significant gains in performance on clinical (+13% F-score) and on newswire (+15% F-score) data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9735329151153564}, {"text": "F-score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9615095257759094}]}, {"text": "Further, we show that active learning with uncertainty sampling reduces the number of required annotations for the entailment search task by more than 90% in both domains.", "labels": [], "entities": [{"text": "entailment search task", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7741003235181173}]}], "datasetContent": [{"text": "In this section, we describe the data sets from two domains, newswire and clinical, that we use in the development and evaluation of our work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of datasets", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.828007698059082}]}, {"text": " Table 4: System performance on test data (* indicates statistical significance)", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 55, "end_pos": 79, "type": "METRIC", "confidence": 0.8539170622825623}]}, {"text": " Table 5: Self-training results on test data (* indicates statistical significance)", "labels": [], "entities": []}]}