{"title": [{"text": "Mining Paraphrasal Typed Templates from a Plain Text Corpus", "labels": [], "entities": [{"text": "Paraphrasal Typed Templates", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.7910622954368591}]}], "abstractContent": [{"text": "Finding paraphrases in text is an important task with implications for generation , summarization and question answering , among other applications.", "labels": [], "entities": [{"text": "summarization", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.980248749256134}, {"text": "question answering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8735465109348297}]}, {"text": "Of particular interest to those applications is the specific formulation of the task where the paraphrases are templated, which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities.", "labels": [], "entities": []}, {"text": "Previous work has fo-cused on mining paraphrases from parallel and comparable corpora, or mining very short sub-sentence synonyms and paraphrases.", "labels": [], "entities": []}, {"text": "In this paper we present an approach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates, utilizing a rich type system for the slots, from a plain text corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the main difficulties in Natural Language Generation (NLG) is the surface realization of messages: transforming a message from its internal representation to a natural language phrase, sentence or larger structure expressing it.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.8250490029652914}]}, {"text": "Often the simplest way to realize messages is though the use of templates.", "labels": [], "entities": []}, {"text": "For example, any message about the birth year and place of any person can be expressed with the template \" was born in in\".", "labels": [], "entities": []}, {"text": "Templates have the advantage that the generation system does not have to deal with the internal syntax and coherence of each template, and can instead focus on document-level discourse coherence and on local coreference issues.", "labels": [], "entities": []}, {"text": "On the other hand, templates have two major disadvantages.", "labels": [], "entities": []}, {"text": "First, having a human manually compose a template for each possible message is costly, especially when a generation system is relatively openended or is expected to deal with many domains.", "labels": [], "entities": []}, {"text": "In addition, a text generated using templates often lacks variation, which means the system's output will be repetitive, unlike natural text produced by a human.", "labels": [], "entities": []}, {"text": "In this paper, we are concerned with a task aimed at solving both problems: automatically mining paraphrasal templates, i.e. groups of templates which share the same slot types and which, if their slots are filled with the same entities, result in paraphrases.", "labels": [], "entities": []}, {"text": "We introduce an unsupervised approach to paraphrasal template mining from the text of Wikipedia articles.", "labels": [], "entities": [{"text": "paraphrasal template mining from the text of Wikipedia articles", "start_pos": 41, "end_pos": 104, "type": "TASK", "confidence": 0.8078751232888963}]}, {"text": "Most previous work on paraphrase detection focuses either on a corpus of aligned paraphrase candidates or on such candidates extracted from a parallel or comparable corpus.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.983543872833252}]}, {"text": "In contrast, we are concerned with a very large dataset of templates extracted from a single corpus, where any two templates are potential paraphrases.", "labels": [], "entities": []}, {"text": "Specifically, paraphrasal templates can be extracted from sentences which are not in fact paraphrases; for example, the sentences \"The population of Missouri includes more than 1 million African Americans\" and \"Roughly 185,000 Japanese Americans reside in Hawaii\" can produce the templated paraphrases \"The population of [american state] includes more than [ethnic group]\" and \"Roughly [ethnic group] reside in [american state]\".", "labels": [], "entities": []}, {"text": "Looking for paraphrases among templates, instead of among sentences, allows us to avoid using an aligned corpus.", "labels": [], "entities": []}, {"text": "Our approach consists of three stages.", "labels": [], "entities": []}, {"text": "First, we process the entire corpus and determine slot locations, transforming the sentences to templates (Section 4).", "labels": [], "entities": []}, {"text": "Next, we find most approriate type for each slot using a large taxonomy, and group together templates which share the same set of types as potential paraphrases (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we cluster the templates in each group into sets of paraphrasal templates (Section 6).", "labels": [], "entities": []}, {"text": "We apply our approach to six corpora representing diverse subject domains, and show through a crowd-sourced evaluation that we can achieve a high precision of over 80% with a reasonable similarity threshold setting.", "labels": [], "entities": [{"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9950947761535645}]}, {"text": "We also show that our threshold parameter directly controls the trade-off between the number of paraphrases found and the precision, which makes it easy to adjust our approach to the needs of various applications.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9996616840362549}]}], "datasetContent": [{"text": "To evaluate our method, we applied it to the six domains described in.", "labels": [], "entities": []}, {"text": "We tried to choose a set of domains that are diverse in topic, size and degree of repeated structure across documents.", "labels": [], "entities": []}, {"text": "For each domain, we collected a corpus composed of relevant Wikipedia articles (as described in the table) and used the method described in Sections 4-6 to extract paraphrasal templates.", "labels": [], "entities": []}, {"text": "We used Wikipedia for convenience, since it allows us to easily select domain corpora, but there is nothing in our approach that is specific to Wikipedia; it can be applied to any text corpus.", "labels": [], "entities": []}, {"text": "We sampled 400 pairs of paraphrases extracted from each domain and used this set of 2400 pairs to conduct a crowd-sourced human evaluation on CrowdFlower.", "labels": [], "entities": []}, {"text": "For each template pair, we randomly selected one and used its original entities in both templates to create two sentences about the same set of entities.", "labels": [], "entities": []}, {"text": "The annotators were presented with this pair and asked to score the extent to which they are paraphrases on a scale from 1 to 5.", "labels": [], "entities": []}, {"text": "shows the labels and a brief version of the explanations provided for each.", "labels": [], "entities": []}, {"text": "To ensure the quality of annotations, we used a set of hidden test questions throughout the evaluation and rejected the contributions of annotators which did not get at least 70% of the test questions correctly.", "labels": [], "entities": []}, {"text": "Of those that did perform well on the test questions, we had three annotators score each pair and used the average as the final score for the pair.", "labels": [], "entities": []}, {"text": "In 39.4% of the cases, all three annotators agreed; two annotators agreed in another 47% of the cases, and in the remaining 13.6% there was complete disagreement.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement for the two annotators that had the highest overlap (27 annotated pairs), using Cohen's Kappa, was \u03ba = 0.35.", "labels": [], "entities": []}, {"text": "The overall results are shown in.", "labels": [], "entities": []}, {"text": "Note that because of our clustering approach, we have a choice of similarity threshold.", "labels": [], "entities": []}, {"text": "The results are shown across a range of thresholds from 8 to 11 -it is clear from the figure that the threshold provides away to control the trade-off between the number of paraphrases generated and their precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 205, "end_pos": 214, "type": "METRIC", "confidence": 0.9969802498817444}]}, {"text": "shows the results with our preferred threshold of 9.5.", "labels": [], "entities": []}, {"text": "The number of paraphrase clusters found changes with the threshold.", "labels": [], "entities": []}, {"text": "For the 9.5 threshold we find 512 clusters overall domains, a little over 60% of the number of paraphrases.", "labels": [], "entities": []}, {"text": "The distribution of their sizes is Zipfian: a few very large clus-: Size, average score, % of pairs with a score above 3 (paraphrases), and % of pairs with a score above 4 (high quality paraphrases) for the different domains with a 9.5 threshold ters, dozens of increasingly smaller medium-sized ones and along tail of clusters that contain only two templates.", "labels": [], "entities": [{"text": "average score", "start_pos": 74, "end_pos": 87, "type": "METRIC", "confidence": 0.939494252204895}]}, {"text": "The vast majority of paraphrase pairs come from sentences that were not originally paraphrases (i.e, sentences that originally had different entities).", "labels": [], "entities": []}, {"text": "With a 9.5 threshold, 86% of paraphrases answer that criteria.", "labels": [], "entities": []}, {"text": "While that number varies somewhat across thresholds, it is always above 80% and does not consistently increase or decrease as the threshold increases.", "labels": [], "entities": [{"text": "that number", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.7478713989257812}]}, {"text": "While we wanted to show a meaningful comparison with another method from previous work, none of them do what we are doing here -extraction of sentence-size paraphrasal templates from a non-aligned corpus -and so a comparison using the same data would not be fair (and inmost cases, not possible).", "labels": [], "entities": []}, {"text": "While it seems that providing the results of human evaluation without comparison to prior methods is the norm inmost relevant prior work (, we wanted to at least get some sense of where we stand in comparison to other methods, and so we provide a list of (not directly comparable) results reported by other authors in.", "labels": [], "entities": []}, {"text": "While it is impossible to meaningfully compare and rate such different methods, these numbers support the conclusion that our singlecorpus, domain-agnostic approach achieves a precision that is similar to or better than other methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.998016357421875}]}, {"text": "We also include the paraphrase per sentence (PPS) value -the ratio of paraphrases extracted to the number of input sentences of the corpus -for each method in the table.", "labels": [], "entities": [{"text": "paraphrase per sentence (PPS) value", "start_pos": 20, "end_pos": 55, "type": "METRIC", "confidence": 0.721583638872419}]}, {"text": "We intend this figure as the closest thing to recall that we can conceive for mining paraphrases.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9976599216461182}]}, {"text": "However, keep in mind that it is not a comparable figure across the methods, since different corpora are used.", "labels": [], "entities": []}, {"text": "In particular, it is expected to be significantly higher for parallel corpora, where the entire corpus consists of potential paraphrases (and that fact is reflected in, where some methods that use parallel corpora have a PPS that is an order of magnitude higher than other methods).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation domains. Source article links are preceded by https://en.wikipedia.org/wiki/", "labels": [], "entities": []}, {"text": " Table 2: Annotation score labels and explanations", "labels": [], "entities": []}, {"text": " Table 4: Comparison with the precision and paraphrases generated per input sentence (PPS) of relevant  prior work", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9986082911491394}]}]}