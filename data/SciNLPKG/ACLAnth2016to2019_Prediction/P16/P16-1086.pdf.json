{"title": [{"text": "Text Understanding with the Attention Sum Reader Network", "labels": [], "entities": [{"text": "Text Understanding", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8597210943698883}]}], "abstractContent": [{"text": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test.", "labels": [], "entities": [{"text": "CNN and Daily Mail news data", "start_pos": 94, "end_pos": 122, "type": "DATASET", "confidence": 0.8137227992216746}, {"text": "Children's Book Test", "start_pos": 131, "end_pos": 151, "type": "DATASET", "confidence": 0.8794914782047272}]}, {"text": "Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches.", "labels": [], "entities": []}, {"text": "We present anew, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models.", "labels": [], "entities": []}, {"text": "This makes the model particularly suitable for question-answering problems where the answer is a single word from the document.", "labels": [], "entities": []}, {"text": "Ensemble of our models sets new state of the art on all evaluated datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most of the information humanity has gathered up to this point is stored in the form of plain text.", "labels": [], "entities": []}, {"text": "Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence.", "labels": [], "entities": []}, {"text": "One way of testing the level of text understanding is simply to ask the system questions for which the answer can be inferred from the text.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7391229569911957}]}, {"text": "A well-known example of a system that could make use of a huge collection of unstructured documents to answer questions is for instance IBM's Watson system used for the Jeopardy challenge.", "labels": [], "entities": []}, {"text": "Cloze style questions, i.e. questions formed by removing a phrase from a sentence, are an appealing form of such questions (for example see).", "labels": [], "entities": []}, {"text": "While the task is easy to evaluate, one can vary the context, the question Document: What was supposed to be a fantasy sports car ride at Walt Disney World Speedway turned deadly when a Lamborghini crashed into a guardrail.", "labels": [], "entities": []}, {"text": "The crash took place Sunday at the Exotic Driving Experience, which bills itself as a chance to drive your dream car on a racetrack.", "labels": [], "entities": [{"text": "Exotic Driving Experience", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.6716649134953817}]}, {"text": "The Lamborghini's passenger, 36-year-old Gary Terry of Davenport, Florida, died at the scene, Florida Highway Patrol said.", "labels": [], "entities": [{"text": "Florida Highway Patrol", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.9244646628697714}]}, {"text": "The driver of the Lamborghini, 24-year-old Tavon Watson of Kissimmee, Florida, lost control of the vehicle, the Highway Patrol said.", "labels": [], "entities": []}, {"text": "Question: Officials say the driver, 24-year-old Tavon Watson, lost control of a Answer candidates: Tavon Watson, Walt Disney World Speedway, Highway Patrol, Lamborghini, Florida, (...)", "labels": [], "entities": [{"text": "Highway Patrol", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.776334673166275}]}, {"text": "Answer: Lamborghini: Each example consists of a context document, question, answer cadidates and, in the training data, the correct answer.", "labels": [], "entities": [{"text": "Lamborghini", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.8718748688697815}]}, {"text": "This example was taken from the CNN dataset (.", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9810121059417725}]}, {"text": "Anonymization of this example that makes the task harder is shown in sentence or the specific phrase missing in the question to dramatically change the task structure and difficulty.", "labels": [], "entities": []}, {"text": "One way of altering the task difficulty is to vary the word type being replaced, as in (.", "labels": [], "entities": []}, {"text": "The complexity of such variation comes from the fact that the level of context understanding needed in order to correctly predict different types of words varies greatly.", "labels": [], "entities": []}, {"text": "While predicting prepositions can easily be done using relatively simple models with very little context knowledge, predicting named entities requires a deeper understanding of the context.", "labels": [], "entities": [{"text": "predicting prepositions", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.9246814548969269}, {"text": "predicting named entities", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.8809263308842977}]}, {"text": "Also, as opposed to selecting a random sentence from a text (as done in), the questions can be formed from a specific part of a document, such as a short summary or a list of and the statistics provided with the CBT data set. tags.", "labels": [], "entities": [{"text": "CBT data set. tags", "start_pos": 212, "end_pos": 230, "type": "DATASET", "confidence": 0.9776747941970825}]}, {"text": "Since such sentences often paraphrase in a condensed form what was said in the text, they are particularly suitable for testing text comprehension ().", "labels": [], "entities": []}, {"text": "An important property of cloze style questions is that a large amount of such questions can be automatically generated from real world documents.", "labels": [], "entities": []}, {"text": "This opens the task to data-hungry techniques such as deep learning.", "labels": [], "entities": []}, {"text": "This is an advantage compared to smaller machine understanding datasets like MCTest () that have only hundreds of training examples and therefore the best performing systems usually rely on handcrafted features.", "labels": [], "entities": []}, {"text": "In the first part of this article we introduce the task at hand and the main aspects of the relevant datasets.", "labels": [], "entities": []}, {"text": "Then we present our own model to tackle the problem.", "labels": [], "entities": []}, {"text": "Subsequently we compare the model to previously proposed architectures and finally describe the experimental results on the performance of our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we introduce the task that we are seeking to solve and relevant large-scale datasets that have recently been introduced for this task.", "labels": [], "entities": []}, {"text": "We will now briefly summarize important features of the datasets.", "labels": [], "entities": []}, {"text": "In this section we evaluate our model on the CNN, Daily Mail and CBT datasets.", "labels": [], "entities": [{"text": "CNN", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9722727537155151}, {"text": "Daily Mail and CBT datasets", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.8009859085083008}]}, {"text": "We show that despite the model's simplicity its ensembles achieve state-of-the-art performance on each of these datasets.", "labels": [], "entities": []}, {"text": "We evaluated the proposed model both as a single model and using ensemble averaging.", "labels": [], "entities": []}, {"text": "Although the model computes attention for every word in the document we restrict the model to select an answer from a list of candidate answers associated with each question-document pair.", "labels": [], "entities": []}, {"text": "For single models we are reporting results for the best model as well as the average of accuracies for the best 20% of models with best performance on validation data since single models display considerable variation of results due to random weight initialization    the best performing model according to validation performance.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.962151825428009}]}, {"text": "Then in each step we tried adding the best performing model that had not been previously tried.", "labels": [], "entities": []}, {"text": "We kept it in the ensemble if it did improve its validation performance and discarded it otherwise.", "labels": [], "entities": [{"text": "validation", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.9523927569389343}]}, {"text": "This way we gradually tried each model once.", "labels": [], "entities": []}, {"text": "We call the resulting model a greedy ensemble.", "labels": [], "entities": []}, {"text": "Time per epoch CNN 10h 22min Daily Mail 25h 42min CBT Named Entity 1h 5min CBT Common Noun 0h 56min: Average duration of one epoch of training on the four datasets.", "labels": [], "entities": [{"text": "CNN 10h 22min Daily Mail 25h 42min CBT Named Entity 1h 5min CBT Common Noun 0h 56min", "start_pos": 15, "end_pos": 99, "type": "DATASET", "confidence": 0.9048067226129419}]}, {"text": "The hyperparameters, namely the recurrent hidden layer dimension and the source embedding dimension, were chosen by grid search.", "labels": [], "entities": []}, {"text": "We started with a range of 128 to 384 for both parameters and subsequently kept increasing the upper bound by 128 until we started observing a consistent decrease in validation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.904424786567688}]}, {"text": "The region of the parameter space that we explored together with the parameters of the model with best validation accuracy are summarized in: Subfigure (a) shows the model accuracy when the correct answer is among n most frequent named entities for n \u2208.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.6882897019386292}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9838108420372009}]}, {"text": "Subfigure (b) shows the number of test examples for which the correct answer was the n-th most frequent entity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the 4 data sets used to evaluate the model. CBT CN stands for CBT Common  Nouns and CBT NE stands for CBT Named Entites. Statistics were taken from (Hermann et al., 2015)  and the statistics provided with the CBT data set.", "labels": [], "entities": [{"text": "CBT data set", "start_pos": 233, "end_pos": 245, "type": "DATASET", "confidence": 0.9638220469156901}]}, {"text": " Table 2: Results of our AS Reader on the CNN and Daily Mail datasets. Results for models marked  with  \u2020 are taken from (Hermann et al., 2015), results of models marked with  \u2021 are taken from (Hill et al.,  2015) and results marked with are taken from (Kobayashi et al., 2016). Performance of  \u2021 and models  was evaluated only on CNN dataset.", "labels": [], "entities": [{"text": "CNN and Daily Mail datasets", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.8282853603363037}, {"text": "CNN dataset", "start_pos": 331, "end_pos": 342, "type": "DATASET", "confidence": 0.9894342124462128}]}, {"text": " Table 3: Results of our AS Reader on the CBT datasets. Results marked with  \u2021 are taken from (Hill et  al., 2015). ( * ) Human results were collected on 10% of the test set.", "labels": [], "entities": [{"text": "AS Reader", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.6205616891384125}, {"text": "CBT datasets", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9570876359939575}]}, {"text": " Table 4: Proportion of test examples for which the  top k answers proposed by the greedy ensemble  included the correct answer.", "labels": [], "entities": []}, {"text": " Table 6: Dimension of the recurrent hidden layer  and of the source embedding for the best model  and the range of values that we tested. We report  number of hidden units of the unidirectional GRU;  the bidirectional GRU has twice as many hidden  units.", "labels": [], "entities": []}]}