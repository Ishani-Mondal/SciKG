{"title": [{"text": "Improved Semantic Parsers For If-Then Statements", "labels": [], "entities": [{"text": "Improved Semantic Parsers", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8277227481206259}]}], "abstractContent": [{"text": "Digital personal assistants are becoming both more common and more useful.", "labels": [], "entities": []}, {"text": "The major NLP challenge for personal assistants is machine understanding: translating natural language user commands into an executable representation.", "labels": [], "entities": [{"text": "machine understanding", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7958987355232239}]}, {"text": "This paper focuses on understanding rules written as If-Then statements, though the techniques should be portable to other semantic parsing tasks.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.7429608702659607}]}, {"text": "We view understanding as structure prediction and show improved models using both conventional techniques and neural network models.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.755536675453186}]}, {"text": "We also discuss various ways to improve generalization and reduce overfitting: synthetic training data from paraphrase, grammar combinations , feature selection and ensembles of multiple systems.", "labels": [], "entities": []}, {"text": "An ensemble of these techniques achieves anew state of the art result with 8% accuracy improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995421171188354}]}], "introductionContent": [{"text": "The ability to instruct computers using natural language clearly allows novice users to better use modern information technology.", "labels": [], "entities": []}, {"text": "Work in semantic parsing has explored mapping natural language to some formal domain-specific programming languages such as database queries, commands to robots), operating systems, and spreadsheets ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7596033215522766}]}, {"text": "This paper explores the use of neural network models (NN) and conventional models for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.8342081308364868}]}, {"text": "Recently approaches using neural networks have shown great improvements in a number of areas such as parsing (, machine translation, and image captioning).", "labels": [], "entities": [{"text": "parsing", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.9777130484580994}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7980620563030243}, {"text": "image captioning", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.7374912053346634}]}, {"text": "We are among the first to apply neural network methods to semantic parsing tasks (.", "labels": [], "entities": [{"text": "semantic parsing tasks", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.8289761145909628}]}, {"text": "There are several benchmark datasets for semantic parsing, the most well known of which is Geoquery (. We target an If-Then dataset) for several reasons.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7836489677429199}]}, {"text": "First, it is both directly applicable to the end-user task of training personal digital assistants.", "labels": [], "entities": []}, {"text": "Second, the training data, drawn from the site http://ifttt.com, is comparatively quite large, containing nearly 100,000 recipe-description pairs.", "labels": [], "entities": []}, {"text": "That said, it is several orders of magnitude smaller than the data for other tasks where neural networks have been successful.", "labels": [], "entities": []}, {"text": "Machine translation datasets, for instance, may contain billions of tokens.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7257662415504456}]}, {"text": "NN methods appear \"data-hungry\".", "labels": [], "entities": []}, {"text": "They require larger datasets to outperform sparse linear approaches with careful feature engineering, as evidenced in work on syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.7456496059894562}]}, {"text": "This makes it interesting to compare NN models with conventional models on this dataset.", "labels": [], "entities": []}, {"text": "As inmost prior semantic parsing attempts, we model natural language understanding as a structure prediction problem.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7953133881092072}, {"text": "natural language understanding", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.6682743728160858}, {"text": "structure prediction", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7666383683681488}]}, {"text": "Each modeling decision predicts some small component of the target structure, conditioned on the whole input and all prior decisions.", "labels": [], "entities": []}, {"text": "Because this is a real-world task, the vocabulary is large and varied, with many words appearing only rarely.", "labels": [], "entities": []}, {"text": "Overfitting is a clear danger.", "labels": [], "entities": []}, {"text": "We explore several methods to improve generalization.", "labels": [], "entities": [{"text": "generalization", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.97385174036026}]}, {"text": "A classic method is to apply feature selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7258501350879669}]}, {"text": "Synthetic data generated by paraphrasing helps augment the data available.", "labels": [], "entities": []}, {"text": "Adjusting the conditional structure of our model also makes sense, as does creating ensembles of the best performing approaches.", "labels": [], "entities": []}, {"text": "An ensemble of the resulting systems achieves anew state-of-the-art result, with an absolute improvement of 8% inaccuracy.", "labels": [], "entities": []}, {"text": "We compare the performance of a neural network model with logistic regression, and explore in detail the contribution of each of them, and why the logistic regression is performing better than the neural network.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a semantic parsing dataset collected from http://ifttt.com, first introduced in.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7295548319816589}]}, {"text": "This website publishes a large set of recipes in the form of If-Then rules.", "labels": [], "entities": []}, {"text": "Each recipe was authored by a website user to automate simple tasks.", "labels": [], "entities": []}, {"text": "For instance, a recipe could send you a message every time you are tagged on a picture on Facebook.", "labels": [], "entities": []}, {"text": "From a natural language standpoint, the most interesting part of this data is that alongside each recipe, there is a short natural language description intended to name or advertise the task.", "labels": [], "entities": []}, {"text": "This provides a naturalistic albeit often noisy source of parallel data for training semantic parsing systems.", "labels": [], "entities": [{"text": "training semantic parsing", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6210049092769623}]}, {"text": "Some of these descriptions faithfully represent the program.", "labels": [], "entities": []}, {"text": "Others are underspecified or suggestive, with many details of the recipe are not uniquely specified or omitted altogether.", "labels": [], "entities": []}, {"text": "The task is to predict the correct If-Then code given a natural language description.", "labels": [], "entities": []}, {"text": "As for the code, If-Then statements follow the format I f T r i g g er Ch an n e l . T r i g g er F u n ct i on ( a r g s ) Then Ac ti on Ch an n e l . Ac ti on F u n ct i on ( a r g s ) Every If-Then statement has exactly one trigger and one action.", "labels": [], "entities": [{"text": "Ac", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.9577764272689819}]}, {"text": "Each trigger and action consist of both a channel and a function.", "labels": [], "entities": []}, {"text": "The channel represents a connection to a service, website, or device (e.g., Facebook, Android, or ESPN) and provides a set of functions relevant to that channel.", "labels": [], "entities": []}, {"text": "Finally, each of these functions may take a number of arguments: to receive a trigger when it becomes sunny, we need to specify the location to watch.", "labels": [], "entities": []}, {"text": "The resulting dataset after cleaning and separation contains 77,495 training recipes, 5,171 development recipes and 4,294 testing recipes.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the systems by providing the model with descriptions unseen during training.", "labels": [], "entities": []}, {"text": "Free parameters of the models were tuned using the development set.", "labels": [], "entities": []}, {"text": "The separation of data into training, development, and test follows.", "labels": [], "entities": []}, {"text": "Two evaluation metrics are used: accuracy on just channel selection and accuracy of both channel and function.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9993079900741577}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9991776347160339}]}, {"text": "Two major families of approaches are considered: a baseline logistic regression classifier from scikit-learn, as well as a feed-forward neural network.", "labels": [], "entities": []}, {"text": "We explore a number of variations, including feature selection and grammar formulation.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7395107448101044}, {"text": "grammar formulation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7496469020843506}]}], "tableCaptions": [{"text": " Table 1: Accuracy of the Neural Network (NN) and Logistic Regression (LR) implementations of our  system with various configurations. Channel-only and full tree (channel+function) accuracies are listed.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99886155128479}]}, {"text": " Table 2: System comparisons on various subsets  of the data. Following Quirk et al.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of NN and LR limited to word  unigram features, with three vocabulary sizes: all  words, words occurring at least twice in the train- ing data (13,971 words), and those occurring at  least three times in the training data (8,974 words).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9937916398048401}]}, {"text": " Table 4: Count of error cases by type for NN and  LR models, in their default configurations. This  table only counts those instances in the most clean  set (where three or more turkers agree with the  gold program) where exactly one system made an  error.", "labels": [], "entities": []}]}