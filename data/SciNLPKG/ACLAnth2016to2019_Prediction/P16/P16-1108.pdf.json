{"title": [{"text": "Leveraging Inflection Tables for Stemming and Lemmatization", "labels": [], "entities": []}], "abstractContent": [{"text": "We present several methods for stemming and lemmatization based on discrimina-tive string transduction.", "labels": [], "entities": [{"text": "stemming", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.9642348885536194}]}, {"text": "We exploit the paradigmatic regularity of semi-structured inflection tables to identify stems in an un-supervised manner with over 85% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9973211884498596}]}, {"text": "Experiments on English, Dutch and German show that our stemmers substantially outperform Snowball and Morfes-sor, and approach the accuracy of a supervised model.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.9859175086021423}, {"text": "Morfes-sor", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.8275488018989563}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.999474823474884}]}, {"text": "Furthermore, the generated stems are more consistent than those annotated by experts.", "labels": [], "entities": []}, {"text": "Our direct lemmatiza-tion model is more accurate than Morfette and Lemming on most datasets.", "labels": [], "entities": []}, {"text": "Finally, we test our methods on the data from the shared task on morphological reinflection.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many languages contain multiple inflected forms that correspond to the same dictionary word.", "labels": [], "entities": []}, {"text": "Inflection is a grammatical procedure that has little impact on the meaning of the word.", "labels": [], "entities": []}, {"text": "For example, the German words in all refer to the action of giving.", "labels": [], "entities": []}, {"text": "When working with these languages, it is often beneficial to establish a consistent representation across a set of inflections.", "labels": [], "entities": []}, {"text": "This is the task that we address here.", "labels": [], "entities": []}, {"text": "There are two principal approaches to inflectional simplification: stemming and lemmatization.", "labels": [], "entities": []}, {"text": "Stemming aims at removing inflectional affixes from a word form.", "labels": [], "entities": [{"text": "Stemming", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9515166878700256}]}, {"text": "It can be viewed as a kind of word segmentation, in which the boundaries of the stem are identified within the word; no attempt is made to restore stem changes that may occur as part of the inflection process.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7062731981277466}]}, {"text": "The goal of lemmatization is to map any inflected form to its unique lemma, which is typically the word form that rep-: Examples of German word-forms corresponding to the lemma geben.", "labels": [], "entities": []}, {"text": "resents a set of related inflections in a dictionary.", "labels": [], "entities": []}, {"text": "Unlike stemming, lemmatization must always produce an actual word form.", "labels": [], "entities": []}, {"text": "In this paper, we present a discriminative string transduction approach to both stemming and lemmatization.", "labels": [], "entities": []}, {"text": "Supervised stemmers require morphologically annotated corpora, which are expensive to build.", "labels": [], "entities": []}, {"text": "We remove this constraint by extracting stems from semi-structured inflection tables, such as the one shown in, in an unsupervised manner.", "labels": [], "entities": []}, {"text": "We design two transduction models that are trained on such stems, and evaluate them on unseen forms against a supervised model.", "labels": [], "entities": []}, {"text": "We then extend our stemming models to perform the lemmatization task, and to incorporate an unannotated corpus.", "labels": [], "entities": []}, {"text": "We evaluate them on several datasets.Our best system improves the state of the art for Dutch, German, and Spanish.", "labels": [], "entities": []}, {"text": "Finally, we test our methods on the data from the shared task on morphological reinflection.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present an overview of prior work on inflectional simplification.", "labels": [], "entities": [{"text": "inflectional simplification", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6247834861278534}]}, {"text": "In Section 3, we describe our stemming methodology, followed by three types of evaluation experiments in Section 4.", "labels": [], "entities": [{"text": "stemming", "start_pos": 30, "end_pos": 38, "type": "TASK", "confidence": 0.9648136496543884}]}, {"text": "In Section 5, we describe our approach to lemmatization, followed by both intrinsic and extrinsic experiments in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Precise evaluation of stemming methods requires morphologically annotated lexicons, which are rare.", "labels": [], "entities": []}, {"text": "Unlike lemmas, stems are abstract representations, rather than actual word forms.", "labels": [], "entities": []}, {"text": "Unsurprisingly, annotators do not always agree on the segmentation of a word.", "labels": [], "entities": []}, {"text": "In this section, we describe three experiments for evaluating stem extraction, intrinsic accuracy, and consistency.", "labels": [], "entities": [{"text": "stem extraction", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.8629460632801056}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9792947173118591}, {"text": "consistency", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9946314096450806}]}, {"text": "We evaluate our methods against three systems that are based on very different principles.", "labels": [], "entities": []}, {"text": "Snowball 1 is a rule-based program based on the methodology of the Porter Stemmer.", "labels": [], "entities": [{"text": "Snowball 1", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.905664324760437}, {"text": "Porter Stemmer", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9493061900138855}]}, {"text": "Morfessor FlatCat (, is a fully-supervised system that represents the current state of the art.", "labels": [], "entities": [{"text": "Morfessor FlatCat", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.7987432479858398}]}, {"text": "First, we evaluate our unsupervised segmentation approach, which serves as the basis for our basic and joint models, on the union of the training and development parts of the CELEX dataset.", "labels": [], "entities": [{"text": "CELEX dataset", "start_pos": 175, "end_pos": 188, "type": "DATASET", "confidence": 0.9911217391490936}]}, {"text": "We are interested how often the stems induced by the method described in Section 3.3 match the stem annotations in the CELEX database.", "labels": [], "entities": [{"text": "CELEX database", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9757803380489349}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "Our method is substantially more accurate than either Snowball or Morfessor.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.982415497303009}, {"text": "Morfessor", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.8262765407562256}]}, {"text": "Snowball, despite being called a stemming algorithm, often eliminates derivational affixes; e.g. able in unbearable.", "labels": [], "entities": [{"text": "Snowball", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.929415225982666}]}, {"text": "Morfessor makes similar mistakes, although less often.", "labels": [], "entities": []}, {"text": "Our method tends to prefer longer stems and shorter affixes.", "labels": [], "entities": []}, {"text": "For example, it stems verwandtestem, as verwandte, while CELEX has verwandt.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9379996657371521}]}, {"text": "We evaluate lemmatization using word accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9225288033485413}]}, {"text": "In cases where a surface word-form without a morphological tag may correspond to multiple lemmas, we judge the prediction as correct if it matches any of the lemmas.", "labels": [], "entities": []}, {"text": "For example, both the noun Schrei and the verb schreien are considered to be correct lemmas for the German word schreien.", "labels": [], "entities": []}, {"text": "The results without the use of a corpus are shown in.", "labels": [], "entities": []}, {"text": "Thanks to its tag awareness, the stemma-based method is more accurate than the stem-based method, except on the verb-only Spanish Wiktionary dataset.", "labels": [], "entities": [{"text": "Spanish Wiktionary dataset", "start_pos": 122, "end_pos": 148, "type": "DATASET", "confidence": 0.5507329156001409}]}, {"text": "However, our best method is the direct word-to-lemma model, which outperforms both Morfette and Lemming on most datasets.", "labels": [], "entities": []}, {"text": "We interpret the results as the evidence for the effectiveness of our discriminative string transduction approach.", "labels": [], "entities": []}, {"text": "The direct model is superior to the stemma-based model because it avoids any information loss that may occur during an intermediate stemming step.", "labels": [], "entities": []}, {"text": "However, it is still able to take advantage of the tag that it generates together with the target lemma.", "labels": [], "entities": []}, {"text": "For example, Lemming incorrectly lemmatizes the German noun form Verdienste \"earnings\" as verdien because +ste is a superlative adjective suffix.", "labels": [], "entities": []}, {"text": "Our direct model, however, considers dien to bean unlikely ending for an adjective, and instead produces the correct lemma Verdienst.", "labels": [], "entities": []}, {"text": "The results with the use of a corpus are shown The capitalization of German nouns is ignored.  in.", "labels": [], "entities": []}, {"text": "We omit the results on Spanish Wiktionary and on both English datasets, which are almost identical to those in.", "labels": [], "entities": [{"text": "Spanish Wiktionary", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.857658177614212}, {"text": "English datasets", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.830891877412796}]}, {"text": "We observe that both the stemma-based and direct methods achieve a substantial error rate reduction on the Dutch and German datasets, while Lemming improvements are minimal.", "labels": [], "entities": [{"text": "error rate", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9642522037029266}, {"text": "Dutch and German datasets", "start_pos": 107, "end_pos": 132, "type": "DATASET", "confidence": 0.6187297105789185}]}, {"text": "The Spanish CoNLL results are different: only the stem-based and stemma-based methods benefit noticeably from reranking.", "labels": [], "entities": [{"text": "Spanish CoNLL", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8122105896472931}]}, {"text": "Error analysis indicates that the re-ranker is able to filter non-existent lemmas, such as wint for Winter, and endstadie for Endstadien, instead of Endstadium.", "labels": [], "entities": [{"text": "endstadie", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9821244478225708}, {"text": "Endstadien", "start_pos": 126, "end_pos": 136, "type": "DATASET", "confidence": 0.7573865056037903}, {"text": "Endstadium", "start_pos": 149, "end_pos": 159, "type": "DATASET", "confidence": 0.6968522071838379}]}, {"text": "In general, the degree of improvement seems to depend on the set of randomly selected instances in the held-out set used for training the re-ranker.", "labels": [], "entities": []}, {"text": "If abase model achieves a very high accuracy on the held-out set, the re-ranker tends to avoid correcting the predictions on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9966673254966736}, {"text": "re-ranker", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9912392497062683}]}, {"text": "When stemming is used for inflectional simplification, it should ideally produce the same stem for all word-forms that correspond to a given lemma.", "labels": [], "entities": []}, {"text": "In many cases, this is not an attainable goal because of internal stem changes (cf. Table 1).", "labels": [], "entities": []}, {"text": "However, most inflected words follow regular paradigms, which involve no stem changes.", "labels": [], "entities": []}, {"text": "For example, all forms of the Spanish verb cantar contain the substring cant, which is considered the common stem.", "labels": [], "entities": []}, {"text": "We quantify the extent to   sets.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The stems-per-table average tends to reflect the morphological complexity of a language.", "labels": [], "entities": []}, {"text": "All systems achieve excellent consistency on English, but the Dutch and German results paint a different picture.", "labels": [], "entities": [{"text": "consistency", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9941169023513794}]}, {"text": "The supervised system falls somewhat short of emulating the gold segmentations, which maybe due to the confusion between different parts of speech.", "labels": [], "entities": []}, {"text": "In terms of consistency, the stems generated by our unsupervised methods are superior to those of Snowball and Morfessor, and even to the gold stems.", "labels": [], "entities": [{"text": "consistency", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9874663949012756}, {"text": "Snowball and Morfessor", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.7669191559155782}]}, {"text": "We attribute this surprising result to the fact that the EM-based alignment of the training data favors consistency in both stems and affixes, although this may not always result in the correct segmentation.", "labels": [], "entities": [{"text": "consistency", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.9891548752784729}]}, {"text": "Unlike stemming, lemmatization is a completely consistent process: all word-forms within an inflection table correspond to the same lemma.", "labels": [], "entities": []}, {"text": "In this section, we describe intrinsic and extrinsic experiments to evaluate the quality of the lemmas generated by our systems, and compare the results against the current state of the art.", "labels": [], "entities": []}, {"text": "We perform our final evaluation experiment on the German dataset 10 from the SIGMORPHON shared task on morphological reinflection (Cot-: Accuracy on the German dataset from the shared task on morphological reinflection. terell et al., 2016).", "labels": [], "entities": [{"text": "German dataset 10", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.9068126877148946}, {"text": "German dataset", "start_pos": 153, "end_pos": 167, "type": "DATASET", "confidence": 0.8296751379966736}]}, {"text": "The task of inflection generation (Task 1) is to produce a word-form given a lemma and an abstract inflectional tag.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.8255330324172974}]}, {"text": "The task of unlabeled reinflection (Task 3) takes as input an unannotated inflected form instead of a lemma.", "labels": [], "entities": []}, {"text": "We evaluate four different methods that combine the models introduced in this paper.", "labels": [], "entities": []}, {"text": "For Task 1, the stem-based method composes a lemma-tostem and a stem-to-word models; the stemmabased method is similar, but pivots on stemmas instead; and the source-target method is a lemmato-word model.", "labels": [], "entities": []}, {"text": "For Task 3, a word-to-lemma model is added in front of both the stem-based and stemma-based methods; the lemma-based method composes a word-to-lemma and a lemma-to-word models; and the source-target method is a wordto-word model.", "labels": [], "entities": []}, {"text": "In addition, we compare with a method that is similar to our stem-based method, but pivots on Chipmunk-generated stems instead.", "labels": [], "entities": []}, {"text": "As a baseline, we run the transduction method provided by the task organizers.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "On Task 1, none of the stemming approaches is competitive with a direct lemma-to-word model.", "labels": [], "entities": []}, {"text": "First, the lemmatic suffixes provide information regarding part-of-speech.", "labels": [], "entities": []}, {"text": "Second, the stemmers fail to take into account the fact that the source word-forms are lemmas.", "labels": [], "entities": []}, {"text": "For example, the German word\u00fcberhitzendword\u00a8word\u00fcberhitzend \"overheated\" can either bean adjective, or the present participle of the verb\u00fcberhitzen verb\u00a8verb\u00fcberhitzen; if the word is a lemma, it is obviously the former.", "labels": [], "entities": []}, {"text": "The lemma-based method is the best performing one on Task 3.", "labels": [], "entities": []}, {"text": "One advantage that it has over the word-to-word model lies in the ability to reduce the potentially quadratic number of transduction operations between various related word- We use the development sets for this evaluation because the target sides of the test sets have not been publicly released.", "labels": [], "entities": []}, {"text": "forms to a linear number of transduction operations between the word-forms and their lemmas, and vice-versa.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: The number of words and distinct inflec- tions for each language in the CELEX datasets.", "labels": [], "entities": [{"text": "CELEX datasets", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.9857748448848724}]}, {"text": " Table 6: Unsupervised stemming accuracy of the  CELEX training set.", "labels": [], "entities": [{"text": "stemming", "start_pos": 23, "end_pos": 31, "type": "TASK", "confidence": 0.6305155158042908}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9677342772483826}, {"text": "CELEX training set", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.9695144097010294}]}, {"text": " Table 8: German stemming accuracy of systems  trained on Wiktionary data, and tested on the  CELEX data.", "labels": [], "entities": [{"text": "German stemming", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.4633105993270874}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9840606451034546}, {"text": "Wiktionary data", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.8751611411571503}, {"text": "CELEX data", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.9867747724056244}]}, {"text": " Table 9: Stemming accuracy of systems trained  and tested on the Chipmunk data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9906884431838989}, {"text": "Chipmunk data", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9672622680664062}]}, {"text": " Table 10: Average number of stems per lemma.", "labels": [], "entities": [{"text": "Average", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9629561901092529}]}, {"text": " Table 11: Lemmatization results without the use of a corpus.", "labels": [], "entities": []}, {"text": " Table 12: Lemmatization results boosted with a  raw corpus.", "labels": [], "entities": []}, {"text": " Table 13: Accuracy on the German dataset from  the shared task on morphological reinflection.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9951390027999878}, {"text": "German dataset", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.9481678307056427}]}]}