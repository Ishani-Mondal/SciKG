{"title": [{"text": "MMFEAT: A Toolkit for Extracting Multi-Modal Features", "labels": [], "entities": [{"text": "MMFEAT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6460153460502625}]}], "abstractContent": [{"text": "Research at the intersection of language and other modalities, most notably vision, is becoming increasingly important in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.6551603476206461}]}, {"text": "We introduce a toolkit that can be used to obtain feature representations for visual and auditory information.", "labels": [], "entities": []}, {"text": "MMFEAT is an easy-to-use Python toolkit, which has been developed with the purpose of making non-linguistic modalities more accessible to natural language processing researchers.", "labels": [], "entities": [{"text": "MMFEAT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9536300897598267}]}], "introductionContent": [{"text": "Distributional models are built on the assumption that the meaning of a word is represented as a distribution over others, which implies that they suffer from the grounding problem.", "labels": [], "entities": []}, {"text": "That is, they do not account for the fact that human semantic knowledge is grounded in the perceptual system.", "labels": [], "entities": []}, {"text": "There has been a lot of interest within the Natural Language Processing community for making use of extra-linguistic perceptual information, much of it in a subfield called multi-modal semantics.", "labels": [], "entities": []}, {"text": "Such multi-modal models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness), improving lexical entailment (), predicting compositionality (Roller and Schulte im, bilingual lexicon induction (Bergsma and Van Durme, 2011) and metaphor identification (.", "labels": [], "entities": [{"text": "predicting compositionality", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.9485465586185455}, {"text": "bilingual lexicon induction (Bergsma and Van Durme, 2011)", "start_pos": 220, "end_pos": 277, "type": "TASK", "confidence": 0.7943280095403845}, {"text": "metaphor identification", "start_pos": 282, "end_pos": 305, "type": "TASK", "confidence": 0.8824237585067749}]}, {"text": "Although most of this work has relied on vision for the perceptual input, recent approaches have also used auditory ( and even olfactory () information.", "labels": [], "entities": []}, {"text": "In this demonstration paper, we describe MM-FEAT, a Python toolkit that makes it easy to obtain images and sound files and extract visual or auditory features from them.", "labels": [], "entities": []}, {"text": "The toolkit includes two standalone command-line tools that do not require any knowledge of the Python programming language: one that can be used for automatically obtaining files from a variety of sources, including Google, Bing and FreeSound (miner.py); and one that can be used for extracting different types of features from directories of data files (extract.py).", "labels": [], "entities": []}, {"text": "In addition, the package comes with code for manipulating multi-modal spaces and several demos to illustrate the wide range of applications.", "labels": [], "entities": []}, {"text": "The toolkit is open source under the BSD license and available at https: //github.com/douwekiela/mmfeat.", "labels": [], "entities": [{"text": "mmfeat", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.9335751533508301}]}], "datasetContent": [], "tableCaptions": []}