{"title": [{"text": "Document-level Sentiment Inference with Social, Faction, and Discourse Context", "labels": [], "entities": [{"text": "Document-level Sentiment Inference", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7002305189768473}]}], "abstractContent": [{"text": "We present anew approach for document-level sentiment inference, where the goal is to predict directed opinions (who feels positively or negatively towards whom) for all entities mentioned in a text.", "labels": [], "entities": [{"text": "document-level sentiment inference", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.699232151110967}]}, {"text": "To encourage more complete and consistent predictions , we introduce an ILP that jointly models (1) sentence-and discourse-level sentiment cues, (2) factual evidence about entity factions, and (3) global constraints based on social science theories such as homophily, social balance, and reciprocity.", "labels": [], "entities": []}, {"text": "Together, these cues allow for rich inference across groups of entities, including for example that CEOs and the companies they lead are likely to have similar sentiment towards others.", "labels": [], "entities": []}, {"text": "We evaluate performance on new, densely labeled data that provides supervision for all pairs, complementing previous work that only labeled pairs mentioned in the same sentence.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that the global model outperforms sentence-level baselines, by providing more coherent predictions across sets of related entities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Documents often present a complex web of facts and opinions that hold among the entities they describe.", "labels": [], "entities": []}, {"text": "Consider the international relations story in.", "labels": [], "entities": [{"text": "international relations", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8924380540847778}]}, {"text": "Representatives from three countries form factions and create a network of sentiment.", "labels": [], "entities": []}, {"text": "While some opinions are relatively directly stated (e.g., Russia criticizes Belarus), many others must be inferred based on the factual ties among entities (e.g., Moscow, Gryzlov, and Russia probably share the same sentiment towards other entities) and known social context (e.g., Russia probably Russia criticized Belarus for permitting Georgian President Mikheil Saakhashvili to appear on Belorussian television.", "labels": [], "entities": []}, {"text": "\"The appearance was an unfriendly step towards Russia,\" the speaker of Russian parliament Boris Gryzlov said.", "labels": [], "entities": []}, {"text": "Saakhashvili announced Thursday that he did not understand Russia's claims.", "labels": [], "entities": []}, {"text": "Moscow refused to have any business with Georgia's president after the armed conflict in 2008 . .", "labels": [], "entities": []}, {"text": "level sentiment graph we aim to recover.", "labels": [], "entities": []}, {"text": "The graph includes edges with direct textual support (e.g., from Russian to Belarus given the verb \"criticized\") as well as ones that must be inferred at the whole-document level (e.g., from Gryzlov to Saakhashvili given the web of relationships and opinions between them, Georgia, Russian, and Belarus).", "labels": [], "entities": []}, {"text": "dislikes Saakhashvili since Russia criticized Belarus for supporting him).", "labels": [], "entities": []}, {"text": "In this paper, we show that jointly reasoning about all of these factors can provide more complete and consistent documentlevel sentiment predictions.", "labels": [], "entities": []}, {"text": "More concretely, we present a global model for document-level entity-to-entity sentiment, i.e., who feels positively (or negatively) towards whom.", "labels": [], "entities": [{"text": "document-level entity-to-entity sentiment", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.5475127299626669}]}, {"text": "Our goal is to make exhaustive predictions overall entity pairs, including those that require crosssentence inference.", "labels": [], "entities": []}, {"text": "We present a Integer Linear Programming (ILP) model that combines three complementary types of evidence: entity-pair sentiment classification, template-based faction extraction, and sentiment dynamics in social groups.", "labels": [], "entities": [{"text": "entity-pair sentiment classification", "start_pos": 105, "end_pos": 141, "type": "TASK", "confidence": 0.6828526854515076}, {"text": "template-based faction extraction", "start_pos": 143, "end_pos": 176, "type": "TASK", "confidence": 0.6013766924540201}]}, {"text": "Together, they allow for recovering more complete predictions of both the explicitly stated and im- (a) shows explicitly stated sentiment, (b) shows faction relationships and (c) shows all edges for Georgia and its representative Saakhashvili.", "labels": [], "entities": []}, {"text": "Through Saakhasvili's relationship with Belarus, Georgia forms an alliance with Belarus, providing evidence for an inferred negative stance towards Russia.", "labels": [], "entities": []}, {"text": "Green dotted edges represent positive sentiment, red are negative, and blue dashed lines show faction relationship.", "labels": [], "entities": []}, {"text": "plicit sentiment, while preserving consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9579538106918335}]}, {"text": "The sentiment dynamics in social groups, motivated by social science theories, are encoded as soft ILP constraints.", "labels": [], "entities": []}, {"text": "They include a notion of homophily, that entities in the same group tend to have similar opinions.", "labels": [], "entities": []}, {"text": "For example, shows directed faction edges, where one entity is likely to agree with the other's opinions.", "labels": [], "entities": []}, {"text": "They also encode dyadic social constraints (i.e., the likely reciprocity of opinions) and triadic social dynamics following social balance theory.", "labels": [], "entities": []}, {"text": "For example, from Russia's criticism on Belarus and Belarus' positive attitude towards Saakhashvilli (in, we can infer that Russia is negative towards Saakhashvilli (in.", "labels": [], "entities": []}, {"text": "When considered in aggregate, these constraints can greatly improve the consistency over the overall document-level predictions.", "labels": [], "entities": [{"text": "consistency", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9954243302345276}]}, {"text": "Our work stands in contrast to previous approaches in three aspects.", "labels": [], "entities": []}, {"text": "First, we apply social dynamics motivated by social science theories to entity-entity sentiment analysis in unstructured text.", "labels": [], "entities": [{"text": "entity-entity sentiment analysis", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.8035860459009806}]}, {"text": "In contrast, most previous studies focused on social media or dialogue data with overt social network structure when integrating social dynamics).", "labels": [], "entities": []}, {"text": "Second, we aim to recover sentiment that can be inferred through partial evidence that spans multiple sentences.", "labels": [], "entities": []}, {"text": "This complements prior efforts for accessing implied sentiment where the key evidence is, by and large, at the sentence level ().", "labels": [], "entities": [{"text": "accessing implied sentiment", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.7977572679519653}]}, {"text": "Finally, we present the first approach to model the relationship between factual and subjective relations.", "labels": [], "entities": []}, {"text": "We evaluate the approach on a newly gathered corpus with dense document-level sentiment labels in news articles.", "labels": [], "entities": []}, {"text": "1 This data includes comprehensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that the global model significantly improves performance over a pairwise classifier and other strong baselines.", "labels": [], "entities": []}, {"text": "We also perform a detailed ablation and error analysis, showing cases where the global constraints contribute and pointing towards important areas for future work.", "labels": [], "entities": [{"text": "ablation and error analysis", "start_pos": 27, "end_pos": 54, "type": "METRIC", "confidence": 0.7637612074613571}]}], "datasetContent": [{"text": "Data and Metrics We randomly split the densely labeled KBP document set, using half as a test data and half as a development data.", "labels": [], "entities": [{"text": "KBP document set", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9046899875005087}]}, {"text": "One half of the development set was used to tune hyper parameters, 8 and the other for error analysis and ablations.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.8118188977241516}]}, {"text": "After development, we ran on the test sets composed of KBP documents and MPQA documents.", "labels": [], "entities": [{"text": "KBP documents", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9460506439208984}, {"text": "MPQA documents", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.8797909617424011}]}, {"text": "For MPQA we did not create a separate development set and reserved all of the relatively modest amount of data fora more reliable test set.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.831730842590332}]}, {"text": "For the pairwise classifier, we report development results using five-fold cross validation on the training data.", "labels": [], "entities": []}, {"text": "We report macro-averaged precision, recall, and F-measure for both sentiment labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9764356017112732}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995966553688049}, {"text": "F-measure", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9992759823799133}]}, {"text": "Comparison Systems We compare performance to two simple baselines and two adaptations of existing sentiment classifiers.", "labels": [], "entities": []}, {"text": "The baselines include our base pairwise classifier (Pair) and randomly assigning labels according to their empirical distribution (Random).", "labels": [], "entities": []}, {"text": "The first existing method adaptation (Sentence) uses the publicly released sentence-level RNN sentiment model from Socher et al.", "labels": [], "entities": [{"text": "Sentence)", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.9581604599952698}]}, {"text": "For each entity pair, we collect sentiment labels from sentences they co-occur in and assign a positive label if a positive-labeled sentence exists, negative if there exists more than one sentence with a negative label and no positives.", "labels": [], "entities": []}, {"text": "We also report a proxy for doing similar aggregation over a state-of-the-art entity-entity sentiment classifier.", "labels": [], "entities": []}, {"text": "Here, because we added our new labels to the original KBP and MPQA3.0 annotations, we can simply predict the union of the original gold annotations using mention string overlap to align the entities (KM Gold).", "labels": [], "entities": [{"text": "KBP", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9313370585441589}, {"text": "MPQA3.0", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.640335738658905}, {"text": "KM Gold", "start_pos": 200, "end_pos": 207, "type": "DATASET", "confidence": 0.6117189228534698}]}, {"text": "This provides a reasonable upper bound on the performance of any extractor trained on this data.", "labels": [], "entities": []}, {"text": "10 Implementation Details We use CPLEX4 11 to solve the ILP described in Sec.", "labels": [], "entities": []}, {"text": "2. For computational efficiency and to avoid erroneous propagation, soft constraints associated with reciprocity and balance theory are introduced only on pairs for which a high-precision classifier assigned polarity.", "labels": [], "entities": []}, {"text": "For the pairwise classifier, we use a classweighted linear SVM.", "labels": [], "entities": []}, {"text": "We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset.", "labels": [], "entities": []}, {"text": "We made two versions of pairwise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search.", "labels": [], "entities": []}, {"text": "One is tuned for high precision to be used as abase classifier for ILP (ILP base), and the other is tuned for the best F1 (Pairwise).", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9960896968841553}, {"text": "F1", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.997805655002594}]}, {"text": "shows results on the evaluation datasets.", "labels": [], "entities": []}, {"text": "The global model achieves the best F1 on both labels.", "labels": [], "entities": [{"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9996445178985596}]}, {"text": "All systems do significantly better than the random baseline but, overall, we see that entityentity sentiment detection is challenging, requir-: Pairwise classifier feature ablation study.", "labels": [], "entities": [{"text": "entityentity sentiment detection", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.7991326451301575}]}], "tableCaptions": [{"text": " Table 3: Sentiment Label Statistics. Each count  represents the average number per document.", "labels": [], "entities": []}, {"text": " Table 4: Inter-annotator Agreement. Cohen's  kappa score: Exact counts only exact matches,  Strict counts allows NOT NEG labels to match  POS, and Relaxed allows NOT NEG to match POS  or UNBIASED (analogously for negative).", "labels": [], "entities": []}, {"text": " Table 5: Percentage of entity pairs that do not co- occur in a sentence.", "labels": [], "entities": []}, {"text": " Table 6: Percentage of labels marked as inferred.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.988731324672699}]}, {"text": " Table 7: Performance on the evaluation datasets: including implicit and explicit sentiment.", "labels": [], "entities": []}, {"text": " Table 8: ILP constraints ablation study.", "labels": [], "entities": []}, {"text": " Table 9: Pairwise classifier feature ablation study.", "labels": [], "entities": [{"text": "Pairwise classifier feature ablation", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7394447326660156}]}, {"text": " Table 10: Error Analysis on the development set.", "labels": [], "entities": [{"text": "Error", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9458947777748108}]}]}