{"title": [{"text": "Generating Natural Questions About an Image", "labels": [], "entities": [{"text": "Generating Natural Questions About an Image", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8535121977329254}]}], "abstractContent": [{"text": "There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering questions about images.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7409260272979736}, {"text": "video transcription", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.730685219168663}, {"text": "answering questions about images", "start_pos": 147, "end_pos": 179, "type": "TASK", "confidence": 0.8462789803743362}]}, {"text": "These tasks have fo-cused on literal descriptions of the image.", "labels": [], "entities": []}, {"text": "To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image.", "labels": [], "entities": [{"text": "Visual Question Generation (VQG)", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.7559666732947031}]}, {"text": "We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art cap-tioning systems thus far.", "labels": [], "entities": []}, {"text": "We train and test several generative and retrieval models to tackle the task of VQG.", "labels": [], "entities": [{"text": "generative and retrieval", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8403191963831583}, {"text": "VQG", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.6147458553314209}]}, {"text": "Evaluation results show that while such models ask reasonable questions fora variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics.", "labels": [], "entities": []}, {"text": "Our proposed task offers anew challenge to the community which we hope furthers interest in exploring deeper connections between vision & language.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are witnessing a renewed interest in interdisciplinary AI research in vision & language, from descriptions of the visual input such as image captioning () and video  transcription, to testing computer understanding of an image through question answering ().", "labels": [], "entities": [{"text": "image captioning", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.7137239277362823}, {"text": "question answering", "start_pos": 238, "end_pos": 256, "type": "TASK", "confidence": 0.777685284614563}]}, {"text": "The most established work in the vision & language community is 'image captioning', where the task is to produce a literal description of the image.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7262873202562332}]}, {"text": "It has been shown) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.743333637714386}]}, {"text": "Furthermore, although this task has a great value for communities of people who are low-sighted or cannot see in all or some environments, for others, the description does not add anything to what a person has already perceived.", "labels": [], "entities": []}, {"text": "The popularity of the image sharing applications in social media and user engagement around images is evidence that commenting on pictures is a very natural task.", "labels": [], "entities": []}, {"text": "A person might respond to an image with a short comment such as 'cool', 'nice pic' or ask a question.", "labels": [], "entities": []}, {"text": "Imagine someone has shared the image in.", "labels": [], "entities": []}, {"text": "What is the very first question that comes to mind?", "labels": [], "entities": []}, {"text": "Your question is most probably very similar to the questions listed next to the image, expressing concern about the motorcyclist (who is not even present in the image).", "labels": [], "entities": []}, {"text": "As you can tell, natural questions are not about what is seen, the policemen or the motorcycle, but rather about what is inferred given these objects, e.g., an accident or injury.", "labels": [], "entities": []}, {"text": "As such, questions are often about abstract concepts, i.e., events or states, in contrast to the concrete terms 1 used in image captioning.", "labels": [], "entities": []}, {"text": "It is clear that the corresponding automatically generated caption 2 for presents only a literal description of objects.", "labels": [], "entities": []}, {"text": "To move beyond the literal description of image content, we introduce the novel task of Visual Question Generation (VQG), where given an image, the system should 'ask a natural and engaging question'.", "labels": [], "entities": [{"text": "Visual Question Generation (VQG)", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.7577699522177378}]}, {"text": "Asking a question that can be answered simply by looking at the image would be of interest to the Computer Vision community, but such questions are neither natural nor engaging fora person to answer and so are not of interest for the task of VQG.", "labels": [], "entities": [{"text": "Computer Vision", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.6748791635036469}, {"text": "VQG", "start_pos": 242, "end_pos": 245, "type": "DATASET", "confidence": 0.8630841970443726}]}, {"text": "Learning to ask questions is an important task in NLP and is more than a syntactic transformation of a declarative sentence.", "labels": [], "entities": []}, {"text": "Deciding what to ask about demonstrates understanding and as such, question generation provides an indication of machine understanding, just as some educational methods assess students' understanding by their ability to ask relevant questions . Furthermore, training a system to ask a good question (not only answer a question) may imbue the system with what appears to be a cognitive ability unique to humans among other primates).", "labels": [], "entities": [{"text": "question generation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7600753307342529}]}, {"text": "Developing the ability to ask relevant and to-the-point questions can bean essential component of any dynamic learner which seeks information.", "labels": [], "entities": []}, {"text": "Such an ability can bean integral component of any conversational agent, either to engage the user in starting a conversation or to elicit taskspecific information.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summarized as follows: (1) in order to enable the VQG research, we carefully created three datasets with a total of 75,000 questions, which range from object-to event-centric images, where we show that VQG covers a wide range of abstract terms including events and states.", "labels": [], "entities": []}, {"text": "we collected 25,000 gold captions for our eventcentric dataset and show that this dataset presents Concrete terms are the ones that can be experienced with five senses.", "labels": [], "entities": []}, {"text": "Abstract terms refer to intangible things, such as feelings, concepts, and qualities 2 Throughout this paper we use the state-of-the-art captioning system), henceforth MSR captioning system https://www.captionbot.ai/, to generate captions.", "labels": [], "entities": []}, {"text": "3 http://rightquestion.org/ challenges to the state-of-the-art image captioning models (Section 3.3).", "labels": [], "entities": [{"text": "image captioning", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.7481838464736938}]}, {"text": "(3) we perform analysis of various generative and retrieval approaches and conclude that end-to-end deep neural models outperform other approaches on our most-challenging dataset (Section 4).", "labels": [], "entities": [{"text": "generative and retrieval", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8989444971084595}]}, {"text": "(4) we provide a systematic evaluation methodology for this task, where we show that the automatic metric \u2206BLEU strongly correlates with human judgments (Section 5.3).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.6654974222183228}]}, {"text": "The results show that while our models learn to generate promising questions, there is still a large gap to match human performance, making the generation of relevant and natural questions an interesting and promising new challenge to the community.", "labels": [], "entities": []}], "datasetContent": [{"text": "While in VQG the set of possible questions is not limited, there is consensus among the natural questions (discussed in Section 3.1) which enables meaningful evaluation.", "labels": [], "entities": [{"text": "VQG", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8802111744880676}]}, {"text": "Although human evaluation is the ideal form of evaluation, it is important to find an automatic metric that strongly correlates with human judgment in order to benchmark progress on the task.", "labels": [], "entities": []}, {"text": "The quality of the evaluation is in part determined by how the evaluation is presented.", "labels": [], "entities": []}, {"text": "For instance, it is important for the human judges to see various system hypotheses at the same time in order to give a calibrated rating.", "labels": [], "entities": []}, {"text": "We crowdsourced our human evaluation on AMT, asking three crowd workers to each rate the quality of candidate questions on a three-point semantic scale.", "labels": [], "entities": [{"text": "AMT", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.5965961217880249}]}, {"text": "The goal of automatic evaluation is to measure the similarity of system-generated question hypotheses and the crowdsourced question references.", "labels": [], "entities": []}, {"text": "To capture n-gram overlap and textual similarity between hypotheses and references, we use standard Machine Translation metrics, BLEU) and METEOR).", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.6647463887929916}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9960195422172546}, {"text": "METEOR", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.8888483047485352}]}, {"text": "We use BLEU with equal weights up to 4-grams and default setting of METEOR version 1.5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9978482723236084}, {"text": "METEOR version 1.5", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.7569157282511393}]}, {"text": "Additionally we use \u2206BLEU (  which is specifically tailored towards generation tasks with diverse references, such as conversations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9139102101325989}]}, {"text": "\u2206BLEU requires rating per reference, distinguishing between the quality of the references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.9949885010719299}]}, {"text": "For this purpose, we crowd-sourced three human ratings (on a scale of 1-3) per reference and used the majority rating.", "labels": [], "entities": []}, {"text": "The pairwise correlational analysis of human and automatic metrics is presented in, where we report on Pearson's r, Spearman's \u03c1 and Kendall's \u03c4 correlation coefficients.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.8468751907348633}, {"text": "Spearman's \u03c1 and Kendall's \u03c4 correlation", "start_pos": 116, "end_pos": 156, "type": "METRIC", "confidence": 0.6575818471610546}]}, {"text": "As this table reveals, \u2206BLEU strongly correlates with human judgment and we suggest it as the main evaluation metric for testing a VQG system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.8832456469535828}]}, {"text": "It is important to note that BLEU is also very competitive with \u2206BLEU, showing strong correlations with human judgment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9979325532913208}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9971131086349487}]}, {"text": "Hence, we recommend using BLEU for any further benchmarking and optimization purposes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9966095685958862}]}, {"text": "BLEU can also be used as a proxy for \u2206BLEU for evaluation purposes whenever rating per reference are not available.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9825275540351868}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9774013757705688}]}], "tableCaptions": [{"text": " Table 2: Statistics of crowdsourcing task, aggre- gating all three datasets.", "labels": [], "entities": []}, {"text": " Table 3: Image captioning results", "labels": [], "entities": [{"text": "Image captioning", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7719535529613495}]}, {"text": " Table 5: Results of evaluating various models according to different metrics. X represents training on the  corresponding dataset in the row. Human score per model is computed by averaging human score across  multiple images, where human score per image is the median rating across the three raters.", "labels": [], "entities": []}]}