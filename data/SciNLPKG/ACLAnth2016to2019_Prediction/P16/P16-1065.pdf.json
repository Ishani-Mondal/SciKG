{"title": [{"text": "A Discriminative Topic Model using Document Network Structure", "labels": [], "entities": []}], "abstractContent": [{"text": "Document collections often have links between documents-citations, hyperlinks, or revisions-and which links are added is often based on topical similarity.", "labels": [], "entities": []}, {"text": "To model these intuitions, we introduce anew topic model for documents situated within a network structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topic-and word-level features.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 188, "end_pos": 203, "type": "TASK", "confidence": 0.7253869473934174}]}, {"text": "Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 178, "end_pos": 193, "type": "TASK", "confidence": 0.7018969804048538}]}], "introductionContent": [{"text": "Documents often appear within a network structure: social media mentions, retweets, and follower relationships; Web pages by hyperlinks; scientific papers by citations.", "labels": [], "entities": []}, {"text": "Network structure interacts with the topics in the text, in that documents linked in a network are more likely to have similar topic distributions.", "labels": [], "entities": []}, {"text": "For instance, a citation link between two papers suggests that they are about a similar field, and a mentioning link between two social media users often indicates common interests.", "labels": [], "entities": []}, {"text": "Conversely, documents' similar topic distributions can suggest links between them.", "labels": [], "entities": []}, {"text": "For example, topic model ( and block detection papers) are relevant to our research, so we cite them.", "labels": [], "entities": [{"text": "block detection", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.7416595220565796}]}, {"text": "Similarly, if asocial media user A finds another user B with shared interests, then A is more likely to follow B.", "labels": [], "entities": []}, {"text": "Our approach is part of a natural progression of network modeling in which models integrate more information in more sophisticated ways.", "labels": [], "entities": []}, {"text": "Some past methods only consider the network itself (, which loses the rich information in text.", "labels": [], "entities": []}, {"text": "In other cases, methods take both links and text into account (), but they are modeled separately, not jointly, limiting the model's ability to capture interactions between the two.", "labels": [], "entities": []}, {"text": "The relational topic model goes further, jointly modeling topics and links, but it considers only pairwise document relationships, failing to capture network structure at the level of groups or blocks of documents.", "labels": [], "entities": []}, {"text": "We propose anew joint model that makes fuller use of the rich link structure within a document network.", "labels": [], "entities": []}, {"text": "Specifically, our model embeds the weighted stochastic block model to identify blocks in which documents are densely connected.", "labels": [], "entities": []}, {"text": "WSBM basically categorizes each item in a network probabilistically as belonging to one of L blocks, by reviewing its connections with each block.", "labels": [], "entities": [{"text": "WSBM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9022645354270935}]}, {"text": "Our model can be viewed as a principled probabilistic extension of, who identify blocks in a document network deterministically as strongly connected components (SCC).", "labels": [], "entities": []}, {"text": "Like them, we assign a distinct Dirichlet prior to each block to capture its topical commonalities.", "labels": [], "entities": []}, {"text": "Jointly, a linear regression model with a discriminative, max-margin objective function () is trained to reconstruct the links, taking into account the features of documents' topic and word distributions (, block assignments, and inter-block link rates.", "labels": [], "entities": []}, {"text": "We validate our approach on a scientific paper abstract dataset and collection of webpages, with citation links and hyperlinks respectively, to predict links among previously unseen documents and from those new documents to training documents.", "labels": [], "entities": []}, {"text": "Embedding the WSBM in a network/topic model leads to substantial improvements in link prediction over previous models; it also improves block detection and topic interpretability.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7012558281421661}, {"text": "block detection", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.749732106924057}, {"text": "topic interpretability", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.7110916674137115}]}, {"text": "The key advantage in embedding WSBM is its flexibility and robustness in the face of noisy links.", "labels": [], "entities": []}, {"text": "Our results also lend additional support for using maxmargin learning fora \"downstream\" supervised topic model, and that predictions from lexical as well as topic features improves performance.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces two previous link-modeling methods, WSBM and RTM.", "labels": [], "entities": [{"text": "WSBM", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.7556952238082886}]}, {"text": "Section 3 presents our methods to incorporate block priors in topic modeling and include various features in link prediction, as well as the aggregated discriminative topic model whose posterior inference is introduced in Section 4.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.7700026333332062}, {"text": "link prediction", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.7678152918815613}]}, {"text": "In Section 5 we show how our model can improve link prediction and (often) improve topic coherence.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.794986754655838}]}], "datasetContent": [{"text": "We evaluate using the two datasets.", "labels": [], "entities": []}, {"text": "The first one is CORA dataset ().", "labels": [], "entities": [{"text": "CORA dataset", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9313559234142303}]}, {"text": "After removing stopwords and words that appear in fewer than ten documents, as well as documents with no  We treat all links as undirected.", "labels": [], "entities": []}, {"text": "Both datasets are split into 5 folds, each further split into a development and test set with approximately the same size when used for evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Predictive Link Rank Results", "labels": [], "entities": [{"text": "Predictive Link Rank", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9227348963419596}]}, {"text": " Table 2: PLR of the citation link between example  documents K and A (described in Section 5.2)", "labels": [], "entities": [{"text": "PLR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9937884211540222}]}, {"text": " Table 3: Average Association Scores of Topics", "labels": [], "entities": [{"text": "Average Association Scores", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8830587863922119}]}, {"text": " Table 4: Statistics of Blocks 1 (learning theory)  and 2 (Bayes nets), which are merged in SCC.", "labels": [], "entities": []}]}