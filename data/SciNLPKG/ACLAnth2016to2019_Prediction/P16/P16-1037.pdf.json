{"title": [{"text": "News Citation Recommendation with Implicit and Explicit Semantics", "labels": [], "entities": [{"text": "News Citation Recommendation", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.9157742261886597}]}], "abstractContent": [{"text": "In this work, we focus on the problem of news citation recommendation.", "labels": [], "entities": [{"text": "news citation recommendation", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.8293015559514364}]}, {"text": "The task aims to recommend news citations for both authors and readers to create and search news references.", "labels": [], "entities": []}, {"text": "Due to the sparsity issue of news citations and the engineering difficulty in obtaining information on authors, we focus on content similarity-based methods instead of col-laborative filtering-based approaches.", "labels": [], "entities": []}, {"text": "In this paper, we explore word embedding (i.e., implicit semantics) and grounded entities (i.e., explicit semantics) to address the variety and ambiguity issues of language.", "labels": [], "entities": []}, {"text": "We formulate the problem as a re-ranking task and integrate different similarity measures under the learning to rank framework.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a real-world dataset.", "labels": [], "entities": []}, {"text": "The experimental results show the efficacy of our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "When an author writes an online news article, s/he often cites previously published news reports to elaborate a mentioned event or support his/her point of view.", "labels": [], "entities": []}, {"text": "For the convenience of the readers, the editor usually associates the words with hyperlinks.", "labels": [], "entities": []}, {"text": "Through the links the readers can directly access the referenced articles to know more details about the events.", "labels": [], "entities": []}, {"text": "If there is no reference fora mentioned event, the readers may search the related news reports for further reading.", "labels": [], "entities": []}, {"text": "Hence, it is valuable to have automatic news citation recommendations for authors and readers to create or search news references.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the problem of news citation recommendation.", "labels": [], "entities": [{"text": "news citation recommendation", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.8400337298711141}]}, {"text": "As shown in, * Work done during internship at Microsoft Research.", "labels": [], "entities": []}, {"text": "given a snippet of citing context (left), the task aims to retrieve a list of news articles (right) as references.", "labels": [], "entities": []}, {"text": "This task differs from traditional recommendation tasks, e.g., citation recommendation for scientific papers, in that: (a) based on the statistics from our dataset, the number of references per news article is 4.56 on average, much less than the number of citations per academic paper (typically dozens); (b) the author-topic information is usually unavailable, since it is technically difficult to obtain author information from news articles.", "labels": [], "entities": [{"text": "citation recommendation for scientific papers", "start_pos": 63, "end_pos": 108, "type": "TASK", "confidence": 0.8543194651603698}]}, {"text": "These differences make the collaborative filteringbased methods, which have been widely applied to paper citation recommendation, less available in our scenario.", "labels": [], "entities": [{"text": "paper citation recommendation", "start_pos": 99, "end_pos": 128, "type": "TASK", "confidence": 0.7322912712891897}]}, {"text": "Therefore, in this paper we focus on content similarity-based methods to deal with the task of news citation recommendation.", "labels": [], "entities": [{"text": "news citation recommendation", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.7892321348190308}]}, {"text": "Previous studies use string-based overlap (), machine translation measures (, and dependency syntax (; ) to model text similarity.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7320198714733124}]}, {"text": "More recent work focuses on neural network methods (.", "labels": [], "entities": []}, {"text": "There are two major challenges rendering these approaches not suitable for this task: (i) the variety and (ii) the ambiguity of language.", "labels": [], "entities": []}, {"text": "By variety, we mean that the same meaning maybe expressed with different phrases.", "labels": [], "entities": []}, {"text": "Taking the first row in for example, Vlaar in the citing context refers to Ron Vlaar, a Dutch football player, who is referred to as Dutch star and Netherlands international in the cited article.", "labels": [], "entities": []}, {"text": "By ambiguity, we mean that the same expression may have different meanings in different contexts.", "labels": [], "entities": []}, {"text": "In the second example in, the mention tiger refers to tiger the mammal.", "labels": [], "entities": []}, {"text": "By contrast, in \"Detroit Tigers links: The Tigers are in trouble\" for example, the word Tiger is the name of a team.", "labels": [], "entities": []}, {"text": "In this paper, we explore both implicit and explicit semantics to ad-", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments, we set the TF-IDF as the baseline, and incrementally add different groups of features to the system.", "labels": [], "entities": []}, {"text": "The word embedding is pretrained with skipgram model ( on Wikipedia corpus and then fine-tuned using the method proposed in on PPDB ().", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.970505028963089}, {"text": "PPDB", "start_pos": 127, "end_pos": 131, "type": "DATASET", "confidence": 0.94940185546875}]}, {"text": "The embedding fine-tuned with paraphrase pairs can better capture the semantic relatedness of different phrase.", "labels": [], "entities": []}, {"text": "In the experiments, we observe a 1% \u2212 2% improvement by the finetuned word representations compared to vanilla skip-gram vectors.", "labels": [], "entities": []}, {"text": "We use the linear model in RankLib for the learning to rank implementation.", "labels": [], "entities": []}, {"text": "Coordinate ascent is used for parameter optimization.", "labels": [], "entities": [{"text": "Coordinate ascent", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6865692138671875}, {"text": "parameter optimization", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7214021384716034}]}, {"text": "The model is trained to directly optimize the evaluation metrics, Precision@1, Precision@5, NDCG@5 and MAP, respectively.", "labels": [], "entities": [{"text": "Precision@1", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9178008635838827}, {"text": "Precision@5", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.8755098779996237}, {"text": "MAP", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9760896563529968}]}, {"text": "For NDCG@5 measure, we set a binary relevance score, i.e., the scores equal to 1 for ground truths, 0 for negative samples.", "labels": [], "entities": [{"text": "NDCG@5 measure", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.6807263344526291}, {"text": "binary relevance score", "start_pos": 29, "end_pos": 51, "type": "METRIC", "confidence": 0.7332276105880737}]}, {"text": "gives the performance of the baselines and the systems using different groups of features on test and validation sets.", "labels": [], "entities": []}, {"text": "The results show that WMD brings a consistent improvement over its TF-IDF baseline, and so do grounded entities compared to ungrounded mentions.", "labels": [], "entities": []}, {"text": "Individually added to the TF-IDF baseline, WMD has the largest performance boost, followed by grounded entity features.", "labels": [], "entities": [{"text": "TF-IDF baseline", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.7783551514148712}]}, {"text": "Besides, the additional information from grounded entity knowledge helps the model outperform the ungrounded mentions, with a consistent margin of 1.0%-2.0% NDCG@5.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 157, "end_pos": 161, "type": "METRIC", "confidence": 0.9851040244102478}]}, {"text": "We further compare the performance of the models when using features from headlines+lead paragraphs only and those from full passages.", "labels": [], "entities": []}, {"text": "As shown in, the former brings much better performance on each metric compared to the latter.", "labels": [], "entities": []}, {"text": "It's worth noting that there are ground truths mis-labeled as irrelevant in the dataset.", "labels": [], "entities": []}, {"text": "A primary: Experimental results in percentage on S and\u02dcSand\u02dc and\u02dcS.", "labels": [], "entities": []}, {"text": "S is a randomly constructed subset of the test set, and\u02dcSand\u02dc and\u02dcS is obtained by manually labeling samples in S. reason is that news sites sometimes individually publish different reports on a certain event.", "labels": [], "entities": []}, {"text": "And the articles don't necessarily share the same title.", "labels": [], "entities": []}, {"text": "To see how this affects the model, we randomly build a subset S of the test set and manually label the selected samples, which gives\u02dcSgives\u02dc gives\u02dcS 4 . compares the model's performance on S and\u02dcSand\u02dc and\u02dcS under Headline+Lead paragraph setting.", "labels": [], "entities": []}, {"text": "There is a consistent improvement of NDCG@5 score o\u00f1o\u00f1 S compared to that on S.", "labels": [], "entities": [{"text": "NDCG@5 score o\u00f1o\u00f1 S", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.8344273964564005}]}, {"text": "Besides that, on manually labeled data, the model's performance across different feature settings is almost in accord with that on the full test set.", "labels": [], "entities": []}, {"text": "These results show that there are indeed mis-labeled ground truths in the dataset, but they have little influence when comparing different groups of features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Experimental results in percentage on the dataset collected from Bing News.", "labels": [], "entities": [{"text": "dataset collected from Bing News", "start_pos": 52, "end_pos": 84, "type": "DATASET", "confidence": 0.7377065420150757}]}, {"text": " Table 4: Experimental results in percentage on S and\u02dcSand\u02dc and\u02dcS. S is a  randomly constructed subset of the test set, and\u02dcSand\u02dc and\u02dcS is obtained  by manually labeling samples in S.", "labels": [], "entities": []}]}