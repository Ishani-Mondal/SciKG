{"title": [{"text": "WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia", "labels": [], "entities": [{"text": "Large-scale Language Understanding Task", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.7160209119319916}]}], "abstractContent": [{"text": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances.", "labels": [], "entities": [{"text": "natural language understanding task", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.7344785183668137}]}, {"text": "The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles.", "labels": [], "entities": [{"text": "Wikidata", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9507750272750854}]}, {"text": "The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs).", "labels": [], "entities": [{"text": "classification and extraction", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.7622619469960531}]}, {"text": "We compare various state-of-the-art DNN-based architectures for document classification , information extraction, and question answering.", "labels": [], "entities": [{"text": "document classification", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.7387440204620361}, {"text": "information extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8458159267902374}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8884953856468201}]}, {"text": "We find that models supporting a rich answer space, such as word or character sequences, perform best.", "labels": [], "entities": []}, {"text": "Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9995131492614746}]}], "introductionContent": [{"text": "A growing amount of research in natural language understanding (NLU) explores end-to-end deep neural network (DNN) architectures for tasks such as text classification (, relation extraction, and question answering ( ).", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.822733610868454}, {"text": "text classification", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7648167312145233}, {"text": "relation extraction", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.8414387404918671}, {"text": "question answering", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.854274719953537}]}, {"text": "These models offer the potential to remove the intermediate steps traditionally involved in processing natural language data by operating on increasingly raw forms of text input, even unprocessed character or byte sequences.", "labels": [], "entities": []}, {"text": "Furthermore, while these tasks are often studied in isolation, DNNs have the potential to combine multiple forms of reasoning within a single model.", "labels": [], "entities": []}, {"text": "Supervised training of DNNs often requires a large amount of high-quality training data.", "labels": [], "entities": []}, {"text": "To this end, we introduce a novel prediction task and accompanying large-scale dataset with a range of sub-tasks combining text classification and information extraction.", "labels": [], "entities": [{"text": "text classification", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7646756768226624}, {"text": "information extraction", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.7719742655754089}]}, {"text": "The dataset is made publiclyavailable at http://goo.gl/wikireading.", "labels": [], "entities": []}, {"text": "The task, which we call WIKIREADING, is to predict textual values from the open knowledge base given text from the corresponding articles on Wikipedia (.", "labels": [], "entities": []}, {"text": "Example instances are shown in, illustrating the variety of subject matter and sub-tasks.", "labels": [], "entities": []}, {"text": "The dataset contains 18.58M instances across 884 sub-tasks, split roughly evenly between classification and extraction (see Section 2 for more details).", "labels": [], "entities": []}, {"text": "In addition to its diversity, the WIKIREADING dataset is also at least an order of magnitude larger than related NLU datasets.", "labels": [], "entities": [{"text": "WIKIREADING dataset", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.9265307188034058}, {"text": "NLU datasets", "start_pos": 113, "end_pos": 125, "type": "DATASET", "confidence": 0.9503689706325531}]}, {"text": "Many natural language datasets for question answering (QA), such as WIKIQA (, have only thousands of examples and are thus too small for training end-to-end models.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.8997184038162231}, {"text": "WIKIQA", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.9157101511955261}]}, {"text": "proposed a task similar to QA, predicting entities in news summaries from the text of the original news articles, and generated a NEWS dataset with 1M instances.", "labels": [], "entities": [{"text": "predicting entities in news summaries from the text of the original news articles", "start_pos": 31, "end_pos": 112, "type": "TASK", "confidence": 0.8704461684593787}, {"text": "NEWS dataset", "start_pos": 130, "end_pos": 142, "type": "DATASET", "confidence": 0.9406631886959076}]}, {"text": "The bAbI dataset ) requires multiple forms of reasoning, but is composed of synthetically generated documents.", "labels": [], "entities": [{"text": "bAbI dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8971487283706665}]}, {"text": "WIKIQA and NEWS only involve pointing to locations within the document, and text classification datasets often have small numbers of output classes.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9505511522293091}, {"text": "NEWS", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.8314893841743469}, {"text": "text classification", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7046739459037781}]}, {"text": "In contrast, WIKIREADING has a rich output space of millions of answers, making it a challenging benchmark for state-of-the-art DNN architectures for QA or text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7065204828977585}]}, {"text": "We implemented a large suite of recent models, and for the first time evaluate them on common grounds, placing the complexity of the task in context and illustrating the tradeoffs inherent in each: Examples instances from WIKIREADING.", "labels": [], "entities": [{"text": "WIKIREADING", "start_pos": 222, "end_pos": 233, "type": "DATASET", "confidence": 0.8553275465965271}]}, {"text": "The task is to predict the answer given the document and property.", "labels": [], "entities": []}, {"text": "Answer tokens that can be extracted are shown in bold, the remaining instances require classification or another form of inference. approach.", "labels": [], "entities": []}, {"text": "The highest score of 71.8% is achieved by a sequence to sequence model) operating on word-level input and output sequences, with special handing for out-of-vocabulary words.", "labels": [], "entities": []}], "datasetContent": [{"text": "We constructed the WIKIREADING dataset from Wikidata and Wikipedia as follows: We consolidated all Wikidata statements with the same item and property into a single (item, property, answer) triple, where answer is a set of values.", "labels": [], "entities": [{"text": "WIKIREADING dataset", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.8905162811279297}]}, {"text": "Replacing each item with the text of the linked Wikipedia article (discarding unlinked items) yields a dataset of 18.58M (document, property, answer) instances.", "labels": [], "entities": []}, {"text": "Importantly, all elements in each instance are human-readable strings, making the task entirely textual.", "labels": [], "entities": []}, {"text": "The only modification we made to these strings was to convert timestamps into a human-readable format (e.g., \"4 July 1776\").", "labels": [], "entities": [{"text": "\"4 July 1776\")", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.7348138570785523}]}, {"text": "The WIKIREADING task, then, is to predict the answer string for each tuple given the document and property strings.", "labels": [], "entities": []}, {"text": "This setup can be seen as similar to information extraction, or question answering where the property acts as a \"question\".", "labels": [], "entities": [{"text": "information extraction", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8327467143535614}, {"text": "question answering", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8057000935077667}]}, {"text": "We assigned all instances for each document randomly to either training (12.97M instances), validation (1.88M), and test (3.73M ) sets following a 70/10/20 distribution.", "labels": [], "entities": []}, {"text": "This ensures that, during validation and testing, all documents are unseen.", "labels": [], "entities": []}, {"text": "We evaluated all methods from Section 3 on the full test set with a single scoring framework.", "labels": [], "entities": []}, {"text": "An answer is correct when there is an exact string match between the predicted answer and the gold answer.", "labels": [], "entities": []}, {"text": "However, as describe in Section 2.2, some answers are composed from a set of values (e.g. third example in).", "labels": [], "entities": []}, {"text": "To handle this, we define the Mean F1 score as follows: For each instance, we compute the F1-score (harmonic mean of precision and recall) as a measure of the degree of overlap between the predicted answer set and the gold set fora given instance.", "labels": [], "entities": [{"text": "Mean F1 score", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.8235090970993042}, {"text": "F1-score", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9984822869300842}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.8539915680885315}, {"text": "recall)", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9851772785186768}]}, {"text": "The resulting perinstance F1 scores are then averaged to produce a single dataset-level score.", "labels": [], "entities": [{"text": "perinstance F1 scores", "start_pos": 14, "end_pos": 35, "type": "METRIC", "confidence": 0.7512011130650839}]}, {"text": "This allows a method to obtain partial credit for an instance when it answers with at least one value from the golden set.", "labels": [], "entities": []}, {"text": "In this paper, we only consider methods for answering with a single value, and most answers in the dataset are also composed of a single value, so this Mean F1 metric is closely related to accuracy.", "labels": [], "entities": [{"text": "Mean F1 metric", "start_pos": 152, "end_pos": 166, "type": "METRIC", "confidence": 0.8582163651784261}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9977073669433594}]}, {"text": "More precisely, a method using a single value as answer is bounded by a Mean F1 of 0.963.", "labels": [], "entities": [{"text": "Mean F1", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.8338195979595184}]}], "tableCaptions": [{"text": " Table 2: Training set frequency and scaled answer entropy  for the 10 most frequent properties.", "labels": [], "entities": []}, {"text": " Table 3: Results for all methods described in Section 3 on the test set. F1 is the Mean F1 score described in 4. Bound is the  upper bound on Mean F1 imposed by constraints in the method (see text for details). The remaining columns provide score  breakdowns by property type and the number of model parameters.", "labels": [], "entities": [{"text": "F1", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9991063475608826}, {"text": "Mean F1 score", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.9093553225199381}, {"text": "Bound", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9948220252990723}, {"text": "Mean F1", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.950553297996521}]}, {"text": " Table 4: Structural model parameters. Note that the Para- graph Vector method uses the output from a separate, unsu- pervised model as a document encoding, which is not counted  in these parameters.", "labels": [], "entities": []}, {"text": " Table 5: Property-level Mean F1 scores on the test set for selected methods and properties. For each property type, the two  most frequent properties are shown followed by two less frequent properties to illustrate long-tail behavior.", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.5745035409927368}]}]}