{"title": [{"text": "Chinese Zero Pronoun Resolution with Deep Neural Networks", "labels": [], "entities": [{"text": "Chinese Zero Pronoun Resolution", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6591660976409912}]}], "abstractContent": [{"text": "While unsupervised anaphoric zero pronoun (AZP) resolvers have recently been shown to rival their supervised counterparts in performance, it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features.", "labels": [], "entities": [{"text": "anaphoric zero pronoun (AZP) resolvers", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.6720230621950967}]}, {"text": "To address these weaknesses, we propose a supervised approach to AZP resolution based on deep neural networks, taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.92433100938797}]}, {"text": "Our approach achieves state-of-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus.", "labels": [], "entities": [{"text": "OntoNotes corpus", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.8233464658260345}]}], "introductionContent": [{"text": "A zero pronoun (ZP) is a gap in a sentence that is found when a phonetically null form is used to refer to a real-world entity.", "labels": [], "entities": []}, {"text": "An anaphoric zero pronoun (AZP) is a ZP that corefers with one or more preceding mentions in the associated text.", "labels": [], "entities": []}, {"text": "Below is an example taken from the Chinese Treebank (CTB), where the ZP (denoted as *pro*) refers to \u4fc4\u7f57\u65af (Russia).", "labels": [], "entities": [{"text": "Chinese Treebank (CTB)", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.9556281566619873}]}, {"text": "[\u4fc4\u7f57\u65af] \u4f5c\u4e3a\u7c73\u6d1b\u820d\u592b\u7ef4\u5947\u4e00\u8d2f\u7684\u652f\u6301\u8005\uff0c *pro* \u66fe\u7ecf\u63d0\u51fa\u8c03\u505c\u8fd9\u573a\u653f\u6cbb\u5371\u673a\u3002", "labels": [], "entities": []}, {"text": "( is a consistent supporter of Milo\u0161evi\u0107, *pro* has proposed to mediate the political crisis.)", "labels": [], "entities": []}, {"text": "As we can see, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender.", "labels": [], "entities": [{"text": "overt pronoun resolution", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.832105795542399}]}, {"text": "This makes ZP resolution more challenging than overt pronoun resolution.", "labels": [], "entities": [{"text": "ZP resolution", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.7951128780841827}, {"text": "overt pronoun resolution", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.7802431384722391}]}, {"text": "Automatic ZP resolution is typically composed of two steps.", "labels": [], "entities": [{"text": "ZP resolution", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8163741230964661}]}, {"text": "The first step, AZP identification, involves extracting ZPs that are anaphoric.", "labels": [], "entities": [{"text": "AZP identification", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.857732355594635}]}, {"text": "The second step, AZP resolution, aims to identify an antecedent of an AZP.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.834565281867981}]}, {"text": "State-of-the-art ZP resolvers have tackled both of these steps in a supervised manner, training one classifier for AZP identification and another for AZP resolution (e.g.,,).", "labels": [], "entities": [{"text": "ZP resolvers", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.83883997797966}, {"text": "AZP identification", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7094950675964355}, {"text": "AZP resolution", "start_pos": 150, "end_pos": 164, "type": "TASK", "confidence": 0.6795625984668732}]}, {"text": "More recently, have proposed unsupervised probabilistic AZP resolution models (henceforth the CN14 model and the CN15 model, respectively) that rival their supervised counterparts in performance.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.6078485548496246}]}, {"text": "An appealing aspect of these unsupervised models is that their language-independent generative process enables them to be applied to languages where data annotated with ZP links are not readily available.", "labels": [], "entities": []}, {"text": "Though achieving state-of-the-art performance, these models have several weaknesses.", "labels": [], "entities": []}, {"text": "First, a lot of manual efforts need to be spent on engineering the features for generative probabilistic models, as these models are sensitive to the choice of features.", "labels": [], "entities": [{"text": "generative probabilistic", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.8996654748916626}]}, {"text": "For instance, having features that are (partially) dependent on each other could harm model performance.", "labels": [], "entities": []}, {"text": "Second, in the absence of labeled data, it is difficult, though not impossible, for these models to profitably employ lexical features (e.g., word pairs, syntactic patterns involving words), as determining which lexical features are useful and how to combine the potentially large number of lexical features in an unsupervised manner is a very challenging task.", "labels": [], "entities": []}, {"text": "In fact, the unsupervised models proposed by are unlexicalized, presumably owing to the aforementioned reasons.", "labels": [], "entities": []}, {"text": "Unfortunately, as shown in previous work (e.g,,), the use of lexical features contributed significantly to the performance of state-of-the-art supervised AZP resolvers.", "labels": [], "entities": [{"text": "AZP resolvers", "start_pos": 154, "end_pos": 167, "type": "TASK", "confidence": 0.6569450944662094}]}, {"text": "Finally, owing to the lack of labeled data, the model parameters are learned to maximize data likelihood, which may not correlate well with the desired evaluation measure (i.e., F-score).", "labels": [], "entities": [{"text": "F-score", "start_pos": 178, "end_pos": 185, "type": "METRIC", "confidence": 0.9965853691101074}]}, {"text": "Hence, while unsupervised resolvers have achieved stateof-the-art performance, these weaknesses together suggest that it is very challenging to scale these models up so that they can achieve the next level of performance.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to improve the state of the art in AZP resolution.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.7951573729515076}]}, {"text": "Motivated by the aforementioned weaknesses, we propose a novel approach to AZP resolution using deep neural networks, which we believe has three key advantages over competing unsupervised counterparts.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.8829459846019745}]}, {"text": "First, deep neural networks are particularly good at discovering hidden structures from the input data and learning task-specific representations via successive transformations of the input vectors, where different layers of a network correspond to different levels of abstractions that are useful for the target task.", "labels": [], "entities": []}, {"text": "For the task of AZP resolution, this is desirable.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7456980049610138}]}, {"text": "Traditionally, it is difficult to correctly resolve an AZP if its context is lexically different from its antecedent's context.", "labels": [], "entities": [{"text": "resolve an AZP", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.6303366223971049}]}, {"text": "This is especially the case for unsupervised resolvers.", "labels": [], "entities": []}, {"text": "In contrast, a deep network can handle difficult cases like this via learning representations that make lexically different contexts look similar.", "labels": [], "entities": []}, {"text": "Second, we train our deep network in a supervised manner.", "labels": [], "entities": []}, {"text": "1 In particular, motivated by recent successes of applying the mention-ranking model to entity coreference resolution (e.g.,,,,,), we propose to employ a ranking-based deep network, which is trained to assign the highest probability to the correct antecedent of an AZP given a set of candidate antecedents.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 88, "end_pos": 117, "type": "TASK", "confidence": 0.8044535517692566}]}, {"text": "This contrasts with existing supervised AZP resolvers, all of which are classification-based.", "labels": [], "entities": [{"text": "AZP resolvers", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.613798975944519}]}, {"text": "Optimizing this objective function is better than maximizing data likelihood, as the former is more tightly coupled with the desired evaluation metric (F-score) than the latter.", "labels": [], "entities": [{"text": "F-score)", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9228733479976654}]}, {"text": "Finally, given that our network is trained in a supervised manner, we can extensively employ lex-1 Note that deep neural networks do not necessarily have to be trained in a supervised manner.", "labels": [], "entities": []}, {"text": "In fact, in early research on extending semantic modeling using auto-encoders, the networks were trained in an unsupervised manner, where the model parameters were optimized for the reconstruction of the input vectors.", "labels": [], "entities": [{"text": "semantic modeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8053590953350067}]}, {"text": "ical features and use them in combination with other types of features that have been shown to be useful for AZP resolution.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.7159028947353363}]}, {"text": "However, rather than employing words directly as features, we employ word embeddings trained in an unsupervised manner.", "labels": [], "entities": []}, {"text": "The goal of the deep network will then be to take these task-independent word embeddings as input and convert them into embeddings that would work best for AZP resolution via supervised learning.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.9179769158363342}]}, {"text": "We call our approach an embedding matching approach because the underlying deep network attempts to compare the embedding learned for an AZP with the embedding learned for each of its antecedents.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first approach to AZP resolution based on deep networks.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.9268025457859039}]}, {"text": "When evaluated on the Chinese portion of the OntoNotes 5.0 corpus, our embedding matching approach to AZP resolution outperforms the CN15 model, achieving state-of-the-art results.", "labels": [], "entities": [{"text": "Chinese portion of the OntoNotes 5.0 corpus", "start_pos": 22, "end_pos": 65, "type": "DATASET", "confidence": 0.7676537505217961}, {"text": "AZP resolution", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.6872888803482056}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 overviews related work on zero pronoun resolution for Chinese and other languages.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7075434923171997}]}, {"text": "Section 3 describes our embedding matching approach, specifically the network architecture and the way we train and apply the network.", "labels": [], "entities": [{"text": "embedding matching", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7296087741851807}]}, {"text": "We present our evaluation results in Section 4 and our conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ the Chinese portion of the OntoNotes 5.0 corpus that was used in the official CoNLL-2012 shared task ().", "labels": [], "entities": [{"text": "OntoNotes 5.0 corpus", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.7966722846031189}]}, {"text": "In the CoNLL-2012 data, the training set and the development set contain ZP coreference annotations, but the test set does not.", "labels": [], "entities": [{"text": "CoNLL-2012 data", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9634303748607635}]}, {"text": "Therefore, we train our models on the training set and perform evaluation on the development set.", "labels": [], "entities": []}, {"text": "Statistics on the datasets are shown in.", "labels": [], "entities": []}, {"text": "Following, we evaluate our model in three settings.", "labels": [], "entities": []}, {"text": "In Setting 1, we assume the availability of gold syntactic parse trees and gold AZPs.", "labels": [], "entities": [{"text": "AZPs", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.860028088092804}]}, {"text": "In Setting 2, we employ gold syntactic parse trees and system (i.e., automatically identified) AZPs.", "labels": [], "entities": []}, {"text": "Finally, in Setting 3, we employ system syntactic parse trees and system AZPs.", "labels": [], "entities": []}, {"text": "The gold and system syntactic parse trees, as well as the gold AZPs, are obtained from the CoNLL-2012 shared task dataset, while the system AZPs are identified by a learning-based AZP identifier described in the Appendix.", "labels": [], "entities": [{"text": "CoNLL-2012 shared task dataset", "start_pos": 91, "end_pos": 121, "type": "DATASET", "confidence": 0.8689704686403275}]}, {"text": "As our baseline, we employ Chen and Ng's (2015) system, which has achieved the best result on our test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics on the training and test sets.", "labels": [], "entities": []}, {"text": " Table 5: AZP resolution results of the baseline and our model on the test set.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.7905560433864594}]}, {"text": " Table 6: Ablation results of AZP resolution on the whole test set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9966415166854858}, {"text": "AZP resolution", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9011618494987488}]}]}