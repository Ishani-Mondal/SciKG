{"title": [{"text": "Hidden Softmax Sequence Model for Dialogue Structure Analysis", "labels": [], "entities": [{"text": "Dialogue Structure Analysis", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.814439078172048}]}], "abstractContent": [{"text": "We propose anew unsupervised learning model, hidden softmax sequence model (HSSM), based on Boltzmann machine for dialogue structure analysis.", "labels": [], "entities": [{"text": "dialogue structure analysis", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7648255030314127}]}, {"text": "The model employs three types of units in the hidden layer to discovery dialogue latent structures: softmax units which represent latent states of utterances; binary units which represent latent topics specified by dialogues ; and a binary unit that represents the global general topic shared across the whole dialogue corpus.", "labels": [], "entities": []}, {"text": "In addition, the model contains extra connections between adjacent hidden softmax units to formulate the dependency between latent states.", "labels": [], "entities": []}, {"text": "Two different kinds of real world dialogue corpora, Twitter-Post and AirTicketBook-ing, are utilized for extensive comparing experiments, and the results illustrate that the proposed model outperforms sate-of-the-art popular approaches.", "labels": [], "entities": [{"text": "AirTicketBook-ing", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9268701672554016}]}], "introductionContent": [{"text": "Dialogue structure analysis is an important and fundamental task in the natural language processing domain.", "labels": [], "entities": [{"text": "Dialogue structure analysis", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8444156249364217}]}, {"text": "The technology provides essential clues for solving real-world problems, such as producing dialogue summaries (;, controlling conversational agents), and designing interactive dialogue systems etc.", "labels": [], "entities": []}, {"text": "The study of modeling dialogues always assumes that for each dialogue there exists an unique latent structure (namely dialogue structure), which consists of a series of latent states.", "labels": [], "entities": []}, {"text": "Some past works mainly rely on supervised or semi-supervised learning, which always involve extensive human efforts to manually construct latent state inventory and to label training samples.", "labels": [], "entities": []}, {"text": "developed an inventory of latent states specific to E-mail in an office domain by inspecting a large corpus of e-mail.", "labels": [], "entities": []}, {"text": "employed semi-supervised learning to transfer latent states from labeled speech corpora to the Internet media and e-mail.", "labels": [], "entities": []}, {"text": "Involving extensive human efforts constrains scaling the training sample size (which is essential to supervised learning) and application domains.", "labels": [], "entities": []}, {"text": "In recent years, there has been some work on modeling dialogues with unsupervised learning methods which operate only on unlabeled observed data.", "labels": [], "entities": []}, {"text": "employed Dirichlet process mixture clustering models to recognize latent states for each utterance in dialogues from a travel-planning domain, but they do not inspect dialogues' sequential structure.", "labels": [], "entities": []}, {"text": "proposed a hidden Markov model (HMM) based dialogue analysis model to study structures of task-oriented conversations from indomain dialogue corpus.", "labels": [], "entities": [{"text": "dialogue analysis", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7543298304080963}]}, {"text": "More recently, extended the HMM based conversation model by introducing additional word sources for topic learning process.", "labels": [], "entities": []}, {"text": "assumed words in an utterance are emitted from topic models under HMM framework, and topics were shared across all latent states.", "labels": [], "entities": []}, {"text": "All these dialogue structure analysis models are directed generative models, in which the HMMs, language models and topic models are combined together.", "labels": [], "entities": []}, {"text": "In this study, we attempt to develop a Boltzmann machine based undirected generative model for dialogue structure analysis.", "labels": [], "entities": [{"text": "dialogue structure analysis", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.8427412311236063}]}, {"text": "As for the document modeling using undirected generative model, proposed a general framework, replicated soft-max model (RSM), for topic modeling based on restricted Boltzmann machine (RBM).", "labels": [], "entities": []}, {"text": "The model focuses on the document-level topic analysis, it cannot be applied for the structure analysis.", "labels": [], "entities": [{"text": "document-level topic analysis", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.5841975212097168}]}, {"text": "We propose a hidden softmax sequence model (HSSM) for the dialogue modeling and structure analysis.", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.8454272449016571}, {"text": "structure analysis", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7231496125459671}]}, {"text": "HSSM is a two-layer special Boltzmann machine.", "labels": [], "entities": [{"text": "HSSM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9347070455551147}]}, {"text": "The visible layer contains softmax units used to model words in a dialogue, which are the same with the visible layer in RSM.", "labels": [], "entities": []}, {"text": "However, the hidden layer has completely different design.", "labels": [], "entities": []}, {"text": "There are three kinds of hidden units: softmax hidden units, which is utilized for representing latent states of dialogues; binary units used for representing dialogue specific topics; and a special binary unit used for representing the general topic of the dialogue corpus.", "labels": [], "entities": []}, {"text": "Moreover, unlike RSM whose hidden binary units are conditionally independent when visible units are given, HSSM has extra connections utilized to formulate the dependency between adjacent softmax units in the hidden layer.", "labels": [], "entities": []}, {"text": "The connections are the latent states of two adjacent utterances.", "labels": [], "entities": []}, {"text": "Therefore, HSSM can be considered as a special Boltzmann machine.", "labels": [], "entities": [{"text": "HSSM", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.7385047078132629}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces two real world dialogue corpora utilized in our experiments.", "labels": [], "entities": []}, {"text": "Section 3 describes the proposed hidden softmax sequence model.", "labels": [], "entities": []}, {"text": "Experimental results and discussions are presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "It's not easy to evaluate the performance of a dialogue structure analysis model.", "labels": [], "entities": [{"text": "dialogue structure analysis", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.7530623277028402}]}, {"text": "In this study, we examined our model via qualitative visualization and quantitative analysis as done in ().", "labels": [], "entities": []}, {"text": "We implemented five conventional models to conduct an extensive comparing study on the two corpora: Twitter-Post and AirTicketBooking.", "labels": [], "entities": [{"text": "AirTicketBooking", "start_pos": 117, "end_pos": 133, "type": "DATASET", "confidence": 0.9565398693084717}]}, {"text": "Conventional models include: LMHMM, LMH-MMS (, TMHMM, TMHMMS, and TMHMMSS).", "labels": [], "entities": []}, {"text": "In our experiments, for each corpus we randomly select 80% dialogues for training, and use the rest 20% for testing.", "labels": [], "entities": []}, {"text": "We select three different number of latent states to evaluate all the models.", "labels": [], "entities": []}, {"text": "In TMHMM, TMHMMS and TMH-MMSS, the number of \"topics\" in the latent states and a dialogue is a hyper-parameter.", "labels": [], "entities": [{"text": "TMHMMS", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.8317450284957886}, {"text": "TMH-MMSS", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.837101936340332}]}, {"text": "We conducted a series of experiments with varying numbers of topics, and the results illustrated that 20 is the best choice on the two corpora.", "labels": [], "entities": []}, {"text": "So, for all the following experimental results of TMHMM, TMHMMS and TMHMMSS, the corresponding topic configurations are set to 20.", "labels": [], "entities": [{"text": "TMHMM", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8514933586120605}, {"text": "TMHMMS", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.779667854309082}, {"text": "TMHMMSS", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.819413423538208}]}, {"text": "The number of estimation iterations for all the models on training sets is set to 10,000; and on held-out test sets, the numver of iterations for inference is set to 1000.", "labels": [], "entities": []}, {"text": "In order to speed-up the learning of HSSM, datasets are divided into minibatches, each has 15 dialogues.", "labels": [], "entities": [{"text": "HSSM", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.8173138499259949}]}, {"text": "In addition, the learning rate and momentum are set to 0.1 and 0.9, respectively.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.9546886682510376}, {"text": "momentum", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9653021693229675}]}, {"text": "Dialogues in Twitter-Post always begin with three latent states: broadcasting what they (Twitter users) are doing now (\"Status\"), broadcasting an interesting link or quote to their followers (\"Reference Broadcast\"), or asking a question to their followers (\"Question to Followers\").", "labels": [], "entities": []}, {"text": "We find that structures discoverd by HSSM and LMHMMS with 10 latent states are most reasonable to interpret.", "labels": [], "entities": [{"text": "HSSM", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8410310745239258}]}, {"text": "For example, after the initiating state (\"Status\", \"Reference Broadcast\", or \"Question to Followers\"), it was often followed a \"Reaction\" to \"Reference Broadcast\" (or \"Status\"), or a \"Comment\" to \"Status\", or a \"Question\" to \"Status\" ( \"Reference Broadcast\", or \"Question to Followers\"') etc.", "labels": [], "entities": []}, {"text": "Compared with LMHMMS, besides obtaining similar latent states, HSSM exhibits powerful ability in learning sequential dependency relationship between latent states.", "labels": [], "entities": []}, {"text": "Take the following simple Twitter dialogue session as an example: : rt i like katy perry lt lt we see tht lol LMHMMS labelled the second utterance (\"lol gd morning \") and the third utterance (\"lol good morning how u \" ) into the same latent state, while HSSM treats them as two different latent states (Though they both have almost the same words).", "labels": [], "entities": [{"text": "HSSM", "start_pos": 254, "end_pos": 258, "type": "DATASET", "confidence": 0.9527125358581543}]}, {"text": "The result is reasonable: the first \"gd morning\" is a greeting, while the second \"gd morning\" is a response.", "labels": [], "entities": []}, {"text": "For AirTicketBooking dataset, the statetransition diagram generated with our model under the setting of 10 latent states is presented in.", "labels": [], "entities": [{"text": "AirTicketBooking dataset", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9814538955688477}]}, {"text": "And several utterance examples corresponding to the latent staes are also showed in.", "labels": [], "entities": []}, {"text": "In general, conversations begin with sever agent's short greeting, such as \"Hi, very glad to be of service.\", and then transit to checking the passenger's identity information or inquiring the passenger's air ticket demand; or it's directly interrupted by the passenger with booking demand which is always associated with place information.", "labels": [], "entities": []}, {"text": "After that, conversations are carried outwith other booking related issues, such as checking ticket price or flight time.", "labels": [], "entities": []}, {"text": "The flowchart produced by HSSM can be reasonably interpreted with knowledge of air ticket booking domain, and it most consistent with the agent's real workflow of the Ticket Booking Corporation 3 compared with other models.", "labels": [], "entities": [{"text": "HSSM", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.8929958343505859}, {"text": "Ticket Booking Corporation 3", "start_pos": 167, "end_pos": 195, "type": "DATASET", "confidence": 0.6665492877364159}]}, {"text": "We notice that conventional models cannot clearly distinguish some relevant latent states from each other.", "labels": [], "entities": []}, {"text": "For example, these baseline models always confound the latent state \"Price Info\" with the latent state \"Reservation\", due to certain words assigned large weights in the two states, such as \" (discount)\", and \" (credit card)\" etc.", "labels": [], "entities": [{"text": "Reservation", "start_pos": 104, "end_pos": 115, "type": "METRIC", "confidence": 0.7432783842086792}]}, {"text": "Furthermore, Only HSSM and LMHMMS have dialogue specific topics, and experimental results illustrate that HSSM can learn much better than LMHMMS which always mis-recognize corpus general words as belonging to dialogue specific topic (An example is presented in).", "labels": [], "entities": []}, {"text": "For quantitative evaluation, we examine HSSM and traditional models with log likelihood and an ordering task on the held-out test set of TwitterPost and AirTicketBooking.", "labels": [], "entities": [{"text": "TwitterPost", "start_pos": 137, "end_pos": 148, "type": "DATASET", "confidence": 0.9678030610084534}, {"text": "AirTicketBooking", "start_pos": 153, "end_pos": 169, "type": "DATASET", "confidence": 0.9514098167419434}]}, {"text": "We hide the corporation's real name for privacy reasons.", "labels": [], "entities": []}, {"text": "Log Likelihood The likelihood metric measures the probability of generating the test set using a specified model.", "labels": [], "entities": []}, {"text": "The likelihood of LMHMM and TMHMM can be directed computed with the forward algorithm.", "labels": [], "entities": [{"text": "TMHMM", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.6686221361160278}]}, {"text": "However, since likelihoods of LMHMMS, TMHMMS and TMHMMSS are intractable to compute due to the local dependencies with respect to certain latent variables, Chibstyle estimating algorithms ( are employed in our experiments.", "labels": [], "entities": [{"text": "Chibstyle estimating", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.6405600607395172}]}, {"text": "For HSSM, the partition function is a key problem for calculating the likelihood, and it can be effectively estimated by Annealed Importance Sampling (AIS).", "labels": [], "entities": [{"text": "likelihood", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9709067344665527}, {"text": "Annealed Importance Sampling", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.5654190679391226}]}, {"text": "presents the likelihood of different models on the two held-out datasets.", "labels": [], "entities": []}, {"text": "We can observe that HSSM achieves better performance on likelihood than all the other models under different number of latent states.", "labels": [], "entities": []}, {"text": "On Twitter-Post dataset our model slightly surpasses LMHMMS, and it performs much better than all traditional models on AirTicketBooking dataset.", "labels": [], "entities": [{"text": "Twitter-Post dataset", "start_pos": 3, "end_pos": 23, "type": "DATASET", "confidence": 0.9089659750461578}, {"text": "AirTicketBooking dataset", "start_pos": 120, "end_pos": 144, "type": "DATASET", "confidence": 0.9858425259590149}]}], "tableCaptions": []}