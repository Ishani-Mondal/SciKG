{"title": [{"text": "Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction", "labels": [], "entities": [{"text": "Integrating Distributional Lexical Contrast", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7904514223337173}, {"text": "Antonym-Synonym Distinction", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.9795378148555756}]}], "abstractContent": [{"text": "We propose a novel vector representation that integrates lexical contrast into distri-butional vectors and strengthens the most salient features for determining degrees of word similarity.", "labels": [], "entities": []}, {"text": "The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs).", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9882262349128723}]}, {"text": "Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model.", "labels": [], "entities": []}, {"text": "The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms.", "labels": [], "entities": []}], "introductionContent": [{"text": "Antonymy and synonymy represent lexical semantic relations that are central to the organization of the mental lexicon.", "labels": [], "entities": []}, {"text": "While antonymy is defined as the oppositeness between words, synonymy refers to words that are similar in meaning.", "labels": [], "entities": []}, {"text": "From a computational point of view, distinguishing between antonymy and synonymy is important for NLP applications such as Machine Translation and Textual Entailment, which go beyond a general notion of semantic relatedness and require to identify specific semantic relations.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8026263415813446}, {"text": "Textual Entailment", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.6856378614902496}]}, {"text": "However, due to interchangeable substitution, antonyms and synonyms often occur in similar contexts, which makes it challenging to automatically distinguish between them.", "labels": [], "entities": []}, {"text": "Distributional semantic models (DSMs) offer a means to represent meaning vectors of words and to determine their semantic \"relatedness\".", "labels": [], "entities": [{"text": "Distributional semantic models (DSMs", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6738839089870453}]}, {"text": "They rely on the distributional hypothesis, in which words with similar distributions have related meaning.", "labels": [], "entities": []}, {"text": "For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context.", "labels": [], "entities": []}, {"text": "However, DSMs tend to retrieve both synonyms (such as formal-conventional) and antonyms (such as formal-informal) as related words and cannot sufficiently distinguish between the two relations.", "labels": [], "entities": []}, {"text": "In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies.", "labels": [], "entities": []}, {"text": "For example, used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns 'from X to Y' or 'either X or Y' significantly often.", "labels": [], "entities": []}, {"text": "assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites.", "labels": [], "entities": []}, {"text": "showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. and aimed to identify the most salient dimensions of meaning in vector representations and reported anew average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations).", "labels": [], "entities": []}, {"text": "Lately, antonym-synonym distinction has also been a focus of word embedding models.", "labels": [], "entities": [{"text": "antonym-synonym distinction", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.6938992291688919}]}, {"text": "For example, integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms.", "labels": [], "entities": []}, {"text": "pro-posed thesaurus-based word embeddings to capture antonyms.", "labels": [], "entities": []}, {"text": "They proposed two models: the WE-T model that trains word embeddings on thesaurus information; and the WE-TD model that incorporated distributional information into the WE-T model.", "labels": [], "entities": []}, {"text": "introduced the multitask lexical contrast model (mLCM) by incorporating WordNet into a skip-gram model to optimize semantic vectors to predict contexts.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9493029117584229}]}, {"text": "Their model outperformed standard skip-gram models with negative sampling on both general semantic tasks and distinguishing antonyms from synonyms.", "labels": [], "entities": []}, {"text": "In this paper, we propose two approaches that make use of lexical contrast information in distributional semantic space and word embeddings for antonym-synonym distinction.", "labels": [], "entities": [{"text": "antonym-synonym distinction", "start_pos": 144, "end_pos": 171, "type": "TASK", "confidence": 0.7315912544727325}]}, {"text": "Firstly, we incorporate lexical contrast into distributional vectors and strengthen those word features that are most salient for determining word similarities, assuming that feature overlap in synonyms is stronger than feature overlap in antonyms.", "labels": [], "entities": []}, {"text": "Secondly, we propose a novel extension of a skip-gram model with negative sampling () that integrates the lexical contrast information into the objective function.", "labels": [], "entities": []}, {"text": "The proposed model optimizes the semantic vectors to predict degrees of word similarity and also to distinguish antonyms from synonyms.", "labels": [], "entities": []}, {"text": "The improved word embeddings outperform state-of-the-art models on antonymsynonym distinction and a word similarity task.", "labels": [], "entities": [{"text": "antonymsynonym distinction", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.731240302324295}, {"text": "word similarity", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7130791544914246}]}], "datasetContent": [{"text": "The corpus resource for our vector representations is one of the currently largest web corpora: EN-COW14A (, containing approximately 14.5 billion tokens and 561K distinct word types.", "labels": [], "entities": [{"text": "EN-COW14A", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.8908268213272095}]}, {"text": "As distributional information, we used a window size of 5 tokens for both the original vector representation and the word embeddings models.", "labels": [], "entities": []}, {"text": "For word embeddings models, we trained word vectors with 500 dimensions; k negative sampling was set to 15; the threshold for sub-sampling was set to 10 \u22125 ; and we ignored all words that occurred < 100 times in the corpus.", "labels": [], "entities": []}, {"text": "The parameters of the models were estimated by backpropagation of error via stochastic gradient descent.", "labels": [], "entities": []}, {"text": "The learning rate strategy was similar to in which the initial learning rate was set to 0.025.", "labels": [], "entities": []}, {"text": "For the lexical contrast information, we used WordNet and Wordnik 1 to collect antonyms and synonyms, obtaining a total of 363,309 synonym and 38,423 antonym pairs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.977809727191925}, {"text": "Wordnik 1", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9350610077381134}]}], "tableCaptions": [{"text": " Table 1: AP evaluation on DSMs.", "labels": [], "entities": [{"text": "AP evaluation", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.826574981212616}]}, {"text": " Table 2: Spearman's \u03c1 on SimLex-999.", "labels": [], "entities": []}, {"text": " Table 3: AUC scores for identifying antonyms.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.932633638381958}]}]}