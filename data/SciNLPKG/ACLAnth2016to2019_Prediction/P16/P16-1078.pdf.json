{"title": [], "abstractContent": [{"text": "Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.827735036611557}]}, {"text": "We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure.", "labels": [], "entities": []}, {"text": "Our model has an attention mechanism that enables the de-coder to generate a translated word while softly aligning it with phrases as well as words of the source sentence.", "labels": [], "entities": []}, {"text": "Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.", "labels": [], "entities": [{"text": "WAT'15 English-to-Japanese dataset", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.687952995300293}, {"text": "SMT", "start_pos": 232, "end_pos": 235, "type": "TASK", "confidence": 0.9210589528083801}]}], "introductionContent": [{"text": "Machine Translation (MT) has traditionally been one of the most complex language processing problems, but recent advances of Neural Machine Translation (NMT) make it possible to perform translation using a simple end-to-end architecture.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9067144751548767}, {"text": "Neural Machine Translation (NMT)", "start_pos": 125, "end_pos": 157, "type": "TASK", "confidence": 0.8258926073710123}]}, {"text": "In the Encoder-Decoder model (), a Recurrent Neural Network (RNN) called the encoder reads the whole sequence of source words to produce a fixedlength vector, and then another RNN called the decoder generates the target words from the vector.", "labels": [], "entities": []}, {"text": "The Encoder-Decoder model has been extended with an attention mechanism (, which allows the model to jointly learn the soft alignment between the source language and the target language.", "labels": [], "entities": []}, {"text": "NMT models have achieved state-of-the-art results in English-to-French and English-to-German trans-: Alignment between an English phrase and a Japanese word.", "labels": [], "entities": [{"text": "English-to-German trans-: Alignment between an English phrase and a Japanese word", "start_pos": 75, "end_pos": 156, "type": "TASK", "confidence": 0.7604957452187171}]}, {"text": "However, it is yet to be seen whether NMT is competitive with traditional Statistical Machine Translation (SMT) approaches in translation tasks for structurally distant language pairs such as English-to-Japanese.", "labels": [], "entities": [{"text": "NMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.7999815940856934}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.7863325029611588}, {"text": "translation tasks", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.8981370329856873}]}, {"text": "shows a pair of parallel sentences in English and Japanese.", "labels": [], "entities": []}, {"text": "English and Japanese are linguistically distant in many respects; they have different syntactic constructions, and words and phrases are defined in different lexical units.", "labels": [], "entities": []}, {"text": "In this example, the Japanese word \"\" is aligned with the English words \"green\" and \"tea\", and the English word sequence \"a cup of\" is aligned with a special symbol \"null\", which is not explicitly translated into any Japanese words.", "labels": [], "entities": []}, {"text": "One way to solve this mismatch problem is to consider the phrase structure of the English sentence and align the phrase \"a cup of green tea\" with \"\".", "labels": [], "entities": []}, {"text": "In SMT, it is known that incorporating syntactic constituents of the source language into the models improves word alignment) and translation accuracy (;).", "labels": [], "entities": [{"text": "SMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9912697076797485}, {"text": "word alignment", "start_pos": 110, "end_pos": 124, "type": "TASK", "confidence": 0.7356709539890289}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.8811408281326294}]}, {"text": "However, the existing NMT models do not allow us to perform this kind of alignment.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel attentional NMT model to take advantage of syntactic infor-mation.", "labels": [], "entities": []}, {"text": "Following the phrase structure of a source sentence, we encode the sentence recursively in a bottom-up fashion to produce a vector representation of the sentence and decode it while aligning the input phrases and words with the output.", "labels": [], "entities": []}, {"text": "Our experimental results on the WAT'15 English-toJapanese translation task show that our proposed model achieves state-of-the-art translation accuracy.", "labels": [], "entities": [{"text": "WAT'15 English-toJapanese translation task", "start_pos": 32, "end_pos": 74, "type": "TASK", "confidence": 0.9030684232711792}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.8918577432632446}]}], "datasetContent": [{"text": "We evaluated the models by two automatic evaluation metrics, RIBES () and BLEU () following WAT'15.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9969731569290161}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9993956089019775}, {"text": "WAT'15", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8369910717010498}]}, {"text": "We used the KyTea-based evaluation script for the translation results.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9613981246948242}]}, {"text": "The RIBES score is a metric based on rank correlation coefficients with word precision, and the BLEU score is based on n-gram word precision and a Brevity Penalty (BP) for outputs shorter than the references.", "labels": [], "entities": [{"text": "RIBES score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.759321928024292}, {"text": "word precision", "start_pos": 72, "end_pos": 86, "type": "METRIC", "confidence": 0.6240646690130234}, {"text": "BLEU score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9774481356143951}, {"text": "word precision", "start_pos": 126, "end_pos": 140, "type": "METRIC", "confidence": 0.617237776517868}, {"text": "Brevity Penalty (BP)", "start_pos": 147, "end_pos": 167, "type": "METRIC", "confidence": 0.9868961930274963}]}, {"text": "RIBES is known to have stronger correlation with human judgements than BLEU in translation between English and Japanese as discussed in. shows the perplexity, BLEU, RIBES, and the training time on the development data with the Attentional NMT (ANMT) models trained on the small dataset.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9662608504295349}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.998379111289978}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.999030351638794}, {"text": "RIBES", "start_pos": 165, "end_pos": 170, "type": "METRIC", "confidence": 0.9895556569099426}]}, {"text": "We conducted the experiments with our proposed method using BlackOut and softmax.", "labels": [], "entities": []}, {"text": "We decoded a translation by our proposed beam search with abeam size of 20.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9705666899681091}]}, {"text": "As shown in, the results of our proposed model with BlackOut improve as the number of negative samples K increases.", "labels": [], "entities": []}, {"text": "Although the result of softmax is better than those of BlackOut), the training time of softmax per epoch is about three times longer than that of BlackOut even with the small dataset.", "labels": [], "entities": []}, {"text": "As to the results of the ANMT model, reversing the word order in the input sentence decreases the scores in English-to-Japanese translation, which contrasts with the results of other language pairs reported in previous work.", "labels": [], "entities": [{"text": "English-to-Japanese translation", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.6390881538391113}]}, {"text": "By taking syntactic information into consideration, our proposed model improves the scores, compared to the sequential attention-based approach.", "labels": [], "entities": []}, {"text": "We found that better perplexity does not always lead to better translation scores with BlackOut as shown in   by the modified unigram-based negative sampling where frequent words can be treated as the negative samples multiple times at each training step.", "labels": [], "entities": [{"text": "translation", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.9447351098060608}]}, {"text": "Effects of the proposed beam search shows the results on the development data of proposed method with BlackOut (K = 2000) by the simple beam search and our proposed beam search.", "labels": [], "entities": []}, {"text": "The beam size is set to 6 or 20 in the simple beam search, and to 20 in our proposed search.", "labels": [], "entities": []}, {"text": "We can see that our proposed search outperforms the simple beam search in both scores.", "labels": [], "entities": []}, {"text": "Unlike RIBES, the BLEU score is sensitive to the beam size and becomes lower as the beam size increases.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.39192938804626465}, {"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9860601127147675}]}, {"text": "We found that the BP had a relatively large impact on the BLEU score in the simple beam search as the beam size increased.", "labels": [], "entities": [{"text": "BP", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9961178302764893}, {"text": "BLEU score", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9822216928005219}]}, {"text": "Our search method works better than the simple beam search by keeping long sentences in the candidates with a large beam size.", "labels": [], "entities": []}, {"text": "Effects of the sequential LSTM units We also investigated the effects of the sequential LSTMs at the leaf nodes in our proposed tree-based encoder.", "labels": [], "entities": []}, {"text": "shows the result on the development data of our proposed encoder and that of an attentional tree-based encoder without sequential LSTMs with BlackOut (K = 2000).", "labels": [], "entities": []}, {"text": "The results show that our proposed encoder considerably out-7 For this evaluation, we used the 1,789 sentences that were successfully parsed by Enju because the encoder without sequential LSTMs always requires a parse tree.", "labels": [], "entities": [{"text": "Enju", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.9078519344329834}]}, {"text": "performs the encoder without sequential LSTMs, suggesting that the sequential LSTMs at the leaf nodes contribute to the context-aware construction of the phrase representations in the tree.", "labels": [], "entities": []}, {"text": "shows the experimental results of RIBES and BLEU scores achieved by the trained models on the large dataset.", "labels": [], "entities": [{"text": "RIBES", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9797508120536804}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9987819790840149}]}, {"text": "We decoded the target sentences by our proposed beam search with the beam size of 20.", "labels": [], "entities": []}, {"text": "The results of the other systems are the ones reported in.", "labels": [], "entities": []}, {"text": "All of our proposed models show similar performance regardless of the value of d.", "labels": [], "entities": []}, {"text": "Our ensemble model is composed of the three models with d = 512, 768, and 1024, and it shows the best RIBES score among all systems.", "labels": [], "entities": [{"text": "RIBES score", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9101144075393677}]}, {"text": "As for the time required for training, our implementation needs about one day to perform one epoch on the large training dataset with d = 512.", "labels": [], "entities": []}, {"text": "It would take about 11 days without using the BlackOut sampling.", "labels": [], "entities": [{"text": "BlackOut sampling", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9672270119190216}]}, {"text": "Comparison with the NMT models The model of Zhu", "labels": [], "entities": [{"text": "NMT", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.8028193712234497}]}], "tableCaptions": [{"text": " Table 1: Dataset in ASPEC corpus.", "labels": [], "entities": [{"text": "ASPEC corpus", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.8458737730979919}]}, {"text": " Table 2: Training dataset and the vocabulary sizes.", "labels": [], "entities": [{"text": "Training dataset", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.6542460024356842}]}, {"text": " Table 3: Evaluation results on the development data using the small training data. The training time per  epoch is also shown, and K is the number of negative samples in BlackOut.", "labels": [], "entities": []}, {"text": " Table 4: Effects of the Beam Search (BS) on the  development data.", "labels": [], "entities": [{"text": "Beam Search (BS)", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.5810380280017853}]}, {"text": " Table 5: Effects of the sequential LSTMs in our  proposed tree-based encoder on the development  data.", "labels": [], "entities": []}, {"text": " Table 6: Evaluation results on the test data.", "labels": [], "entities": []}]}