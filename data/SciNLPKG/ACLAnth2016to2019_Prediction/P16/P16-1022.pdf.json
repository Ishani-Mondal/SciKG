{"title": [{"text": "Compressing Neural Language Models by Sparse Word Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural networks are among the state-of-the-art techniques for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7197383344173431}]}, {"text": "Existing neural language models typically map discrete words to distributed, dense vector representations.", "labels": [], "entities": []}, {"text": "After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word.", "labels": [], "entities": []}, {"text": "Such approaches are time-and memory-intensive because of the large numbers of parameters for word embeddings and the output layer.", "labels": [], "entities": []}, {"text": "In this paper, we propose to compress neural language models by sparse word representations.", "labels": [], "entities": []}, {"text": "In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible.", "labels": [], "entities": []}, {"text": "Moreover, our approach not only reduces the parameter space to a large extent , but also improves the performance in terms of the perplexity measure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) play an important role in a variety of applications in natural language processing (NLP), including speech recognition and document recognition.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.770527164141337}, {"text": "speech recognition", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7821452021598816}, {"text": "document recognition", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7931334972381592}]}, {"text": "In recent years, neural network-based LMs have achieved significant breakthroughs: they can model language more precisely than traditional n-gram statistics (Mikolov et al., 2011); it is even possible to generate new sentences from a neural LM, benefiting various downstream tasks like machine translation, summarization, and dialogue systems).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 286, "end_pos": 305, "type": "TASK", "confidence": 0.7412899732589722}, {"text": "summarization", "start_pos": 307, "end_pos": 320, "type": "TASK", "confidence": 0.6589796543121338}]}, {"text": "Code released on https://github.com/chenych11/lm Existing neural LMs typically map a discrete word to a distributed, real-valued vector representation (called embedding) and use a neural model to predict the probability of each word in a sentence.", "labels": [], "entities": []}, {"text": "Such approaches necessitate a large number of parameters to represent the embeddings and the output layer's weights, which is unfavorable in many scenarios.", "labels": [], "entities": []}, {"text": "First, with a wider application of neural networks in resourcerestricted systems, such approach is too memory-consuming and may fail to be deployed in mobile phones or embedded systems.", "labels": [], "entities": []}, {"text": "Second, as each word is assigned with a dense vector-which is tuned by gradient-based methods-neural LMs are unlikely to learn meaningful representations for infrequent words.", "labels": [], "entities": []}, {"text": "The reason is that infrequent words' gradient is only occasionally computed during training; thus their vector representations can hardly been tuned adequately.", "labels": [], "entities": []}, {"text": "In this paper, we propose a compressed neural language model where we can reduce the number of parameters to a large extent.", "labels": [], "entities": []}, {"text": "To accomplish this, we first represent infrequent words' embeddings with frequent words' by sparse linear combinations.", "labels": [], "entities": []}, {"text": "This is inspired by the observation that, in a dictionary, an unfamiliar word is typically defined by common words.", "labels": [], "entities": []}, {"text": "We therefore propose an optimization objective to compute the sparse codes of infrequent words.", "labels": [], "entities": []}, {"text": "The property of sparseness (only 4-8 values for each word) ensures the efficiency of our model.", "labels": [], "entities": []}, {"text": "Based on the pre-computed sparse codes, we design our compressed language model as follows.", "labels": [], "entities": []}, {"text": "A dense embedding is assigned to each common word; an infrequent word, on the other hand, computes its vector representation by a sparse combination of common words' embeddings.", "labels": [], "entities": []}, {"text": "We use the long short term memory (LSTM)-based recurrent neural network (RNN) as the hidden layer of our model.", "labels": [], "entities": []}, {"text": "The weights of the output layer are also compressed in a same way as embeddings.", "labels": [], "entities": []}, {"text": "Consequently, the number of trainable neural parameters is a constant regardless of the vocabulary size if we ignore the biases of words.", "labels": [], "entities": []}, {"text": "Even considering sparse codes (which are very small), we find the memory consumption grows imperceptibly with respect to the vocabulary.", "labels": [], "entities": []}, {"text": "We evaluate our LM on the Wikipedia corpus containing up to 1.6 billion words.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.9393807053565979}]}, {"text": "During training, we adopt noise-contrastive estimation (NCE) ( to estimate the parameters of our neural LMs.", "labels": [], "entities": [{"text": "noise-contrastive estimation (NCE)", "start_pos": 26, "end_pos": 60, "type": "METRIC", "confidence": 0.7283529162406921}]}, {"text": "However, different from, we tailor the NCE method by adding a regression layer (called ZRegressoion) to predict the normalization factor, which stabilizes the training process.", "labels": [], "entities": []}, {"text": "Experimental results show that, our compressed LM not only reduces the memory consumption, but also improves the performance in terms of the perplexity measure.", "labels": [], "entities": []}, {"text": "To sum up, the main contributions of this paper are three-fold.", "labels": [], "entities": []}, {"text": "(1) We propose an approach to represent uncommon words' embeddings by a sparse linear combination of common ones'.", "labels": [], "entities": []}, {"text": "(2) We propose a compressed neural language model based on the pre-computed sparse codes.", "labels": [], "entities": []}, {"text": "The memory increases very slowly with the vocabulary size (4-8 values for each word).", "labels": [], "entities": [{"text": "memory", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9981001019477844}]}, {"text": "(3) We further introduce a ZRegression mechanism to stabilize the NCE algorithm, which is potentially applicable to other LMs in general.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this part, we first describe our dataset in Subsection 4.1.", "labels": [], "entities": []}, {"text": "We evaluate our learned sparse codes of rare words in Subsection 4.2 and the compressed language model in Subsection 4.3.", "labels": [], "entities": []}, {"text": "Subsection 4.4 provides in-depth analysis of the ZRegression mechanism.", "labels": [], "entities": []}, {"text": "We used the freely available Wikipedia 4 dump (2014) as our dataset.", "labels": [], "entities": [{"text": "Wikipedia 4 dump (2014)", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.964407871166865}]}, {"text": "We extracted plain sentences from the dump and removed all markups.", "labels": [], "entities": []}, {"text": "We further performed several steps of preprocessing such as text normalization, sentence splitting, and tokenization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7847372591495514}, {"text": "sentence splitting", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.80544114112854}, {"text": "tokenization", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.9633166790008545}]}, {"text": "Sentences were randomly shuffled, so that no information across sentences could be used, i.e., we did not consider cached language models.", "labels": [], "entities": []}, {"text": "The resulting corpus contains about 1.6 billion running words.", "labels": [], "entities": []}, {"text": "The corpus was split into three parts for training, validation, and testing.", "labels": [], "entities": []}, {"text": "As it is typically timeconsuming to train neural networks, we sampled a subset of 100 million running words to train neural LMs, but the full training set was used to train the backoff n-gram models.", "labels": [], "entities": []}, {"text": "We chose hyperparameters by the validation set and reported model performance on the test set.", "labels": [], "entities": []}, {"text": "presents some statistics of our dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Perplexity of our compressed language  models and baselines.  \u2020 Trained with the full cor- pus of 1.6 billion running words.", "labels": [], "entities": []}, {"text": " Table 4: Memory reduction (%) by our proposed  methods in comparison with the uncompressed  model LSTM-z. The memory of sparse codes are  included.", "labels": [], "entities": [{"text": "Memory reduction", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9163088202476501}]}]}