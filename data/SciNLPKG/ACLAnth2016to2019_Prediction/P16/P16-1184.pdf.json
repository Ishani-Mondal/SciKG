{"title": [{"text": "Cross-Lingual Morphological Tagging for Low-Resource Languages", "labels": [], "entities": [{"text": "Cross-Lingual Morphological Tagging", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7853449583053589}]}], "abstractContent": [{"text": "Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools.", "labels": [], "entities": []}, {"text": "We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision.", "labels": [], "entities": []}, {"text": "Our approach extends existing approaches of projecting part-of-speech tags across languages , using bitext to infer constraints on the possible tags fora given word type or token.", "labels": [], "entities": []}, {"text": "We propose a tagging model using Wsabie, a discriminative embedding-based model with rank-based learning.", "labels": [], "entities": [{"text": "tagging", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.962875485420227}]}, {"text": "In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable.", "labels": [], "entities": []}, {"text": "Multilingual experiments show that the method performs best when projecting between related language pairs.", "labels": [], "entities": []}, {"text": "Despite the inherently lossy projection , we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average.", "labels": [], "entities": [{"text": "LAS", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9933266639709473}]}], "introductionContent": [{"text": "Morphologically rich languages pose significant challenges for Natural Language Processing (NLP) due to data-sparseness caused by large vocabularies.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.7684138715267181}]}, {"text": "Intermediate processing is often required to address the limitations of only using surface forms, especially for small datasets.", "labels": [], "entities": []}, {"text": "Common morphological processing tasks include segmentation (, paradigm learning) and morphological tagging.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.6981095969676971}]}, {"text": "In this paper we focus on the latter.", "labels": [], "entities": []}, {"text": "Parts-of-speech (POS) tagging is the most common form of syntactic annotation.", "labels": [], "entities": [{"text": "Parts-of-speech (POS) tagging", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.611269760131836}]}, {"text": "However, the granularity of POS varies across languages and annotation-schemas, and tagsets have often been extended to include tags for morphologicallymarked properties such as number, case or degree.", "labels": [], "entities": []}, {"text": "To enable cross-lingual learning, a small set of universal (coarse-grained) POS tags have been proposed ().", "labels": [], "entities": []}, {"text": "For morphological processing this can be complemented with a set of attribute-feature values that makes the annotation more fine-grained).", "labels": [], "entities": []}, {"text": "Tagging text with morphologically-enriched labels has been shown to benefit downstream tasks such as parsing () and semantic role labelling (.", "labels": [], "entities": [{"text": "Tagging text", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8920552134513855}, {"text": "parsing", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.9715169668197632}, {"text": "semantic role labelling", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.6495967904726664}]}, {"text": "In generation tasks such as machine translation these tags can help to generate the right form of a word and to model agreement (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.798084169626236}]}, {"text": "Morphological information can also benefit automatic speech recognition for low-resource languages ().", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.634715348482132}]}, {"text": "However, annotating sufficient data to learn accurate morphological taggers is expensive and relies on linguistic expertise, and is therefore currently only feasible for the world's most widelyused languages.", "labels": [], "entities": []}, {"text": "In this paper we are interested in learning morphological taggers without the availability of supervised data.", "labels": [], "entities": [{"text": "learning morphological taggers", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.7035379012425741}]}, {"text": "A successful paradigm for learning without direct supervision is to make use of word-aligned parallel text, with a resourcerich language on one side and a resource-poor language on the other side (.", "labels": [], "entities": []}, {"text": "In this paper we extend these methods, that have mostly been proposed for universal POS-taggers, to learn weakly-supervised morphological taggers.", "labels": [], "entities": []}, {"text": "Our approach is based on projecting token and type constraints across parallel text, learning a tagger in a weakly-supervised manner from the projected constraints.", "labels": [], "entities": []}, {"text": "We propose an embedding-based model trained with the Wsabie algorithm), and compare this approach against a baseline HMM model.", "labels": [], "entities": []}, {"text": "We evaluate the projected tags fora set of languages for which morphological tags are available in the Universal Dependency corpora.", "labels": [], "entities": []}, {"text": "To show the feasibility of our approach, and to compare the performance of different models, we use English as source language.", "labels": [], "entities": []}, {"text": "Then we perform an evaluation on all language pairs in the set of target languages which shows that the best performance is obtained when projecting between genealogically related languages.", "labels": [], "entities": []}, {"text": "As an extrinsic evaluation of our approach, we show that NLP models can benefit from using these induced tags even if they are not as accurate as tags produced by supervised models, by evaluating the effect of features obtained from tags predicted by the induced morphological taggers in dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 288, "end_pos": 306, "type": "TASK", "confidence": 0.7965575456619263}]}], "datasetContent": [{"text": "We evaluate our model in two settings.", "labels": [], "entities": []}, {"text": "The first evaluation measures the accuracy of the crosslingual taggers on language pairs where annotated data is available for both languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9992677569389343}]}, {"text": "The annotated target language data is used only during evaluation and not for training.", "labels": [], "entities": []}, {"text": "Second, we perform a downstream evaluation by including the morphological attributes predicted by the tagger as features in a dependency parser to guage the effectiveness of our approach in a setting where one does not have access to gold morphological annotations.", "labels": [], "entities": []}, {"text": "As source of parallel training data we use Europarl 2 (Koehn, 2005) version 7.", "labels": [], "entities": [{"text": "Europarl 2 (Koehn, 2005) version", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9561055228114128}]}, {"text": "Sentences are tokenized but not lower-cased, and sentences longer than 80 words are excluded.", "labels": [], "entities": []}, {"text": "In our experiments we learn taggers fora set of 11 European languages that have both UD training data with morphological features, and parallel data in Europarl: Bulgarian, Czech, Danish, Dutch, Finnish, Italian, Polish, Portuguese, Slovene, Spanish and Swedish.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 152, "end_pos": 160, "type": "DATASET", "confidence": 0.9785599112510681}]}, {"text": "We train cross-lingual models in two setups: The first uses English as source language; in the second we train models with different source languages for each target language.", "labels": [], "entities": []}, {"text": "Word alignments over the parallel data are obtained using FastAlign (.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6428950130939484}]}, {"text": "High-confidence bidirectional word alignments are constructed by intersecting the alignments in the two directions and including alignment points only if the posterior probabilities in both directions are above the alignment threshold \u03b1.", "labels": [], "entities": []}, {"text": "For each language pair all the word-aligned parallel data available (between 10 and 50 million target-side tokens per language) are used to extract the type constraints, and the models are trained on a subset of 2 million target-side tokens (optionally with their token constraints).", "labels": [], "entities": []}, {"text": "The number of distinct attribute-value pairs appearing in the tagsets depends on the language pair and ranges between 35 and 79, with 54 on average (including POS tags).", "labels": [], "entities": []}, {"text": "The number of distinct composite morphological tags is 423 on average, with a much larger range, between 81 and 1483.", "labels": [], "entities": []}, {"text": "The English UD data has 116 tags composed out of 51 distinct attribute-value pairs.", "labels": [], "entities": [{"text": "English UD data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9267874757448832}]}, {"text": "Therefore, we can project a reasonable number of morpho-syntactic attributes from English, although the number of attribute combinations that occur in the data is less than for morphologically richer languages.", "labels": [], "entities": []}, {"text": "The source text is tagged with supervised taggers, trained with Wsabie on the UD training data for each of the source languages used.", "labels": [], "entities": [{"text": "UD training data", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.8408522009849548}]}, {"text": "For each language pair, we train a distinct source-side model covering only the attribute types appearing in both languages.", "labels": [], "entities": []}, {"text": "This is meant to obtain a maximally accurate source-side tagger, while accepting that our approach cannot predict target-side attributes that are absent from the source language.", "labels": [], "entities": [{"text": "source-side tagger", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.6683584153652191}]}, {"text": "The average accuracy of the English taggers on the UD test data is 94.96%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993222951889038}, {"text": "UD test data", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9532756408055624}]}, {"text": "The source-side taggers overall the language pairs we experiment on have an average accuracy of 95.75%, with a minimum of 89.14% and a maximum of 98.59%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9994168281555176}]}, {"text": "In order to evaluate the induced taggers on the annotated UD data for the target languages, we define two settings that circumvent mismatches between source and target language annotations to different degrees.", "labels": [], "entities": []}, {"text": "The STANDARD setting involves first making minor corrections to certain predicted POS values to account for inconsistencies in the original annotated data.", "labels": [], "entities": [{"text": "STANDARD", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.6137099862098694}]}, {"text": "When predicted by the model, the POS tag values absent from the target language training corpus are deterministically mapped to the mostrelated value present in the target language in the following way: PROPN to NOUN; SYM and INTJ to X; SYM and X to PUNCT.", "labels": [], "entities": []}, {"text": "Besides POS, the evaluation considers only those attribute types that appear in both languages' training corpora, i.e., the set of attributes for which the model was trained.", "labels": [], "entities": []}, {"text": "Note that this leaves cases intact where the model predicts certain attribute values that appear only in one of the two languages; it is thus penalised for making mistakes on values that it cannot learn under our projection approach.", "labels": [], "entities": []}, {"text": "The second evaluation setting, INTERSECTED, relaxes the latter aspect: it only considers attribute-value pairs appearing in the training corpora of both languages.", "labels": [], "entities": [{"text": "INTERSECTED", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9570004940032959}]}, {"text": "The motivation for this is to get a better measurement of the accuracy of our method, assuming that the tagsets are consistent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9994352459907532}]}, {"text": "In both settings we report macro-averaged F1 scores overall the considered attribute types.", "labels": [], "entities": [{"text": "F1", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9656468629837036}]}, {"text": "Results for Wsabie are averaged over 3 random restarts because it uses stochastic optimization during training.", "labels": [], "entities": [{"text": "Wsabie", "start_pos": 12, "end_pos": 18, "type": "TASK", "confidence": 0.9335333108901978}]}, {"text": "To evaluate the effect of our models on a downstream task, we apply the cross-lingual taggers induced using English as source language to dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.839438408613205}]}, {"text": "This is applicable to a scenario where a language might have a corpus annotated with dependency trees and universal POS, but not morphological attributes.", "labels": [], "entities": []}, {"text": "We want to determine how much of the performance gain from features based on supervised morphological tags we can recover with the tags predicted by our model.", "labels": [], "entities": []}, {"text": "As: Dependency parsing results (LAS) with no, projected and supervised morphological tags.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7265615165233612}]}, {"text": "Zhang and Nivre (2011), an arc-eager transitionbased dependency parser with a rich feature-set, with beam-size 8, trained for 10 epochs with a structured perceptron.", "labels": [], "entities": []}, {"text": "We assume that universal POS tags are available, using a supervised SVM POS tagger for training and evaluation.", "labels": [], "entities": []}, {"text": "To include the morphology, we add features based on the predicted tags of the word on top of the stack and the first two words on the buffer.", "labels": [], "entities": []}, {"text": "Parsing results are given in.", "labels": [], "entities": []}, {"text": "We report labelled attachment scores (LAS) for the baseline with no morphological tags, the model with features predicted by Wsabie with projected type constraints, and the model with features predicted by the supervised morphological tagger.", "labels": [], "entities": [{"text": "labelled attachment scores (LAS)", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8018771956364313}]}, {"text": "We obtain improvements in parsing accuracies for all languages except Bulgarian when adding the induced morphological tags.", "labels": [], "entities": []}, {"text": "Using the projected tags as features recovers 24.67% (0.6 LAS absolute) of the average gain that supervised morphology features delivers over the baseline parser.", "labels": [], "entities": [{"text": "LAS absolute)", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9836518963177999}]}, {"text": "The parser with features from the supervised tagger trained on 1000 tokens obtains 73.63 LAS on average.", "labels": [], "entities": [{"text": "LAS", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9951352477073669}]}, {"text": "This improvement of +0.15 LAS over the baseline versus the +0.6 of our method shows that the tags predicted by our projected models are more useful as features than those predicted by a small supervised model.", "labels": [], "entities": [{"text": "LAS", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9868906736373901}]}, {"text": "To investigate the effect of source language choice for the projected models in this evaluation, we trained a model for Swedish using Danish as source language.", "labels": [], "entities": []}, {"text": "The parsing performance is insignificantly different from using English as source, despite the accuracy of the tags projected   from Danish being higher.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9720486998558044}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9992038607597351}]}, {"text": "show that features from induced morpho-syntactic lexicons can also improve dependency parsing accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8238615095615387}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8786360621452332}]}, {"text": "However, their method relies on having a seed lexicon of 1000 annotated word types, while our method does not require any morphological annotations in the target language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-lingual morphological tagging from English: Macro F1 scores averaged across 11 lan- guages. All the results except for the first two rows are for Wsabie models. The standard deviation over  3 runs is given in brackets.", "labels": [], "entities": [{"text": "Cross-lingual morphological tagging", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7446538607279459}, {"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9116119742393494}]}, {"text": " Table 2: Cross-lingual morphological tagging results (STANDARD F1 scores) per source and target lan- guage, Wsabie projected model with type constraints. Rows indicate source language and columns target  language.", "labels": [], "entities": [{"text": "Cross-lingual morphological tagging", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.6434166928132375}, {"text": "STANDARD F1 scores)", "start_pos": 55, "end_pos": 74, "type": "METRIC", "confidence": 0.9142304509878159}, {"text": "Wsabie projected model", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.8383025129636129}]}, {"text": " Table 3: Comparison of the performance of the  most accurate cross-lingual taggers for each target  language, compared to having English as source  language.", "labels": [], "entities": []}, {"text": " Table 4: Wsabie projected model with type con- straints, POS accuracy with English and the best  language for each target as source.", "labels": [], "entities": [{"text": "POS", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9739382266998291}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.7907677292823792}]}, {"text": " Table 6: Dependency parsing results (LAS) with  no, projected and supervised morphological tags.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7210112363100052}]}, {"text": " Table 5: Cross-lingual tagging results (F1 scores) per language and per attribute (not showing POS and  a small number of attribute types that only appear with 1 or 2 language pairs), for Wsabie projected with  type constraints. English and best source language.", "labels": [], "entities": [{"text": "Cross-lingual tagging", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7212131321430206}, {"text": "F1 scores)", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9681066672007242}]}]}