{"title": [{"text": "Question Answering on Freebase via Relation Extraction and Textual Evidence", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7487962543964386}, {"text": "Freebase", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8545063734054565}, {"text": "Relation Extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7132506221532822}]}], "abstractContent": [{"text": "Existing knowledge-based question answering systems often rely on small annotated training data.", "labels": [], "entities": [{"text": "question answering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7555313408374786}]}, {"text": "While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.9091624617576599}, {"text": "semantic parsing", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.730967178940773}]}, {"text": "Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9312576055526733}]}, {"text": "We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9594424962997437}, {"text": "Wikipedia", "start_pos": 128, "end_pos": 137, "type": "DATASET", "confidence": 0.945235550403595}]}, {"text": "Experiments on the WebQuestions question answering dataset show that our method achieves an F 1 of 53.3%, a substantial improvement over the state-of-the-art.", "labels": [], "entities": [{"text": "WebQuestions question answering dataset", "start_pos": 19, "end_pos": 58, "type": "DATASET", "confidence": 0.8146809339523315}, {"text": "F 1", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9926961958408356}]}], "introductionContent": [{"text": "Since the advent of large structured knowledge bases (KBs) like Freebase (, and, answering natural language questions using those structured KBs, also known as KBbased question answering (or KB-QA), is attracting increasing research efforts from both natural language processing and information retrieval communities.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.9789576530456543}, {"text": "KBbased question answering", "start_pos": 160, "end_pos": 186, "type": "TASK", "confidence": 0.5741313000520071}]}, {"text": "The state-of-the-art methods for this task can be roughly categorized into two streams.", "labels": [], "entities": []}, {"text": "The first is based on semantic parsing, which typically learns a grammar that can parse natural language to a sophisticated meaning representation language.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7421533763408661}]}, {"text": "But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase.", "labels": [], "entities": []}, {"text": "Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem ().", "labels": [], "entities": []}, {"text": "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction) or distributed representations (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7746954262256622}]}, {"text": "Designing large training datasets for these methods is relatively easy).", "labels": [], "entities": []}, {"text": "These methods are often good at producing an answer irrespective of their correctness.", "labels": [], "entities": []}, {"text": "However, handling compositional questions that involve multiple entities and relations, still remains a challenge.", "labels": [], "entities": []}, {"text": "Consider the question what mountain is the highest in north america.", "labels": [], "entities": []}, {"text": "Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9525538980960846}]}, {"text": "To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry.", "labels": [], "entities": []}, {"text": "We propose a method based on textual evidence which can answer such questions without solving the mathematic functions implicitly.", "labels": [], "entities": []}, {"text": "Knowledge bases like Freebase capture real world facts, and Web resources like Wikipedia provide a large repository of sentences that validate or support these facts.", "labels": [], "entities": []}, {"text": "For example, a sentence in Wikipedia says, Denali (also known as Mount McKinley, its former official name) is the highest mountain peak in North America, with a summit elevation of 20,310 feet (6,190 m) above sea level.", "labels": [], "entities": []}, {"text": "To answer our example question against a KB using a relation extractor, we can use this sentence as external evidence, filter out wrong answers and pick the correct one.", "labels": [], "entities": []}, {"text": "Using textual evidence not only mitigates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.9392470419406891}]}, {"text": "Consider the question, who was queen isabella's mother.", "labels": [], "entities": []}, {"text": "Answering this question involves predicting two constraints hidden in the word mother.", "labels": [], "entities": []}, {"text": "One constraint is that the answer should be the parent of Isabella, and the other is that the answer's gender is female.", "labels": [], "entities": []}, {"text": "Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by ).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7381473481655121}, {"text": "relation extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8342213332653046}]}, {"text": "Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female.", "labels": [], "entities": []}, {"text": "Whereas the textual evidence from Wikipedia, . .", "labels": [], "entities": []}, {"text": "her mother was Isabella of Barcelos . .", "labels": [], "entities": []}, {"text": ", can act as a further constraint to answer the question correctly.", "labels": [], "entities": []}, {"text": "We present a novel method for question answering which infers on both structured and unstructured resources.", "labels": [], "entities": [{"text": "question answering", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8854358494281769}]}, {"text": "Our method consists of two main steps as outlined in \u00a72.", "labels": [], "entities": []}, {"text": "In the first step we extract answers fora given question using a structured KB (here Freebase) by jointly performing entity linking and relation extraction ( \u00a73).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.738006517291069}]}, {"text": "In the next step we validate these answers using an unstructured resource (here Wikipedia) to prune out the wrong answers and select the correct ones ( \u00a74).", "labels": [], "entities": []}, {"text": "Our evaluation results on a benchmark dataset WebQuestions show that our method outperforms existing state-ofthe-art models.", "labels": [], "entities": []}, {"text": "Details of our experimental setup and results are presented in \u00a75.", "labels": [], "entities": []}, {"text": "Our code, data and results can be downloaded from https://github.", "labels": [], "entities": []}, {"text": "com/syxu828/QuestionAnsweringOverFB.", "labels": [], "entities": [{"text": "QuestionAnsweringOverFB", "start_pos": 12, "end_pos": 35, "type": "DATASET", "confidence": 0.9529460668563843}]}, {"text": "gives an overview of our method for the question \"who did shaq first play for\".", "labels": [], "entities": []}, {"text": "We have two main steps: (1) inference on Freebase (KB-QA box); and (2) further inference on Wikipedia (Answer Refinement box).", "labels": [], "entities": [{"text": "Freebase (KB-QA box", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.8324656933546066}, {"text": "Wikipedia", "start_pos": 92, "end_pos": 101, "type": "DATASET", "confidence": 0.9565187096595764}]}, {"text": "Let us take a close look into step 1.", "labels": [], "entities": []}, {"text": "Here we perform entity linking to identify a topic entity in the question and its possible Freebase entities.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7243954688310623}]}, {"text": "We employ a relation extractor to predict the potential Freebase relations that could exist between the entities in the question and the answer entities.", "labels": [], "entities": []}, {"text": "Later we perform a joint inference step over the entity linking and relation extraction   results to find the best entity-relation configuration which will produce a list of candidate answer entities.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7156233638525009}]}, {"text": "In the step 2, we refine these candidate answers by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we introduce the experimental setup, the main results and detailed analysis of our system.", "labels": [], "entities": []}, {"text": "We use the WebQuestions () dataset, which contains 5,810 questions crawled via Google Suggest service, with answers annotated on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "WebQuestions () dataset", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.8132172028223673}]}, {"text": "The questions are split into training and test sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively.", "labels": [], "entities": []}, {"text": "We further split the training questions into 80%/20% for development.", "labels": [], "entities": []}, {"text": "To train the MCCNNs and the joint inference model, we need the gold standard relations of the questions.", "labels": [], "entities": [{"text": "MCCNNs", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.8176851272583008}]}, {"text": "Since this dataset contains only questionanswer pairs and annotated topic entities, instead of relying on gold relations we rely on surrogate gold relations which produce answers that have the highest overlap with gold answers.", "labels": [], "entities": []}, {"text": "Specifically, fora given question, we first locate the topic entity e in the Freebase graph, then select 1-hop and 2-hop relations connected to the topic entity as relation candidates.", "labels": [], "entities": [{"text": "Freebase graph", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9645672738552094}]}, {"text": "The 2-hop relations refer to the n-ary relations of Freebase, i.e., first hop from the subject to a mediator node, and the second from the mediator to the object node.", "labels": [], "entities": []}, {"text": "For each relation candidate r, we issue the query (e, r, ?) to the KB, and label the relation that produces the answer with minimal F 1 -loss against the gold answer, as the surrogate gold relation.", "labels": [], "entities": [{"text": "F 1 -loss", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9262454062700272}]}, {"text": "From the training set, we collect 461 relations to train the MCCNN, and the target prediction during testing time is over these relations.", "labels": [], "entities": [{"text": "MCCNN", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8393475413322449}]}, {"text": "We have 6 dependency tree patterns based onto decompose the question into subquestions (See Appendix).", "labels": [], "entities": []}, {"text": "We initialize the word embeddings with's word representations with dimensions set to 50.", "labels": [], "entities": []}, {"text": "The hyper parameters in our model are tuned using the development set.", "labels": [], "entities": []}, {"text": "The window size of MCCNN is set to 3.", "labels": [], "entities": [{"text": "MCCNN", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.7092916369438171}]}, {"text": "The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively.", "labels": [], "entities": [{"text": "MCCNN channels", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.845432311296463}]}, {"text": "We use the Freebase version of, containing 4M entities and 5,323 relations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test set.", "labels": [], "entities": []}, {"text": " Table 2: Impact of the joint inference on the devel- opment set", "labels": [], "entities": []}, {"text": " Table 3: Impact of different MCCNN channels on  the development set.", "labels": [], "entities": []}]}