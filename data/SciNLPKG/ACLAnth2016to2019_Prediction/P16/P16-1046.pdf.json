{"title": [{"text": "Neural Summarization by Extracting Sentences and Words", "labels": [], "entities": [{"text": "Neural Summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8837924003601074}]}], "abstractContent": [{"text": "Traditional approaches to extractive summarization rely heavily on human-engineered features.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8612086474895477}]}, {"text": "In this work we propose a data-driven approach based on neural networks and continuous sentence features.", "labels": [], "entities": []}, {"text": "We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.5763648450374603}]}, {"text": "This architecture allows us to develop different classes of summarization models which can extract sentences or words.", "labels": [], "entities": [{"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9676047563552856}]}, {"text": "We train our models on large scale corpora containing hundreds of thousands of document-summary pairs 1.", "labels": [], "entities": []}, {"text": "Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The need to access and digest large amounts of textual data has provided strong impetus to develop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content.", "labels": [], "entities": []}, {"text": "Much effort in automatic summarization has been devoted to sentence extraction, where a summary is created by identifying and subsequently concatenating the most salient text units in a document.", "labels": [], "entities": [{"text": "summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.7771202921867371}, {"text": "sentence extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7794604897499084}]}, {"text": "Most extractive methods to date identify sentences based on human-engineered features.", "labels": [], "entities": []}, {"text": "These include surface features such as sentence position and length ), the words in the title, the presence of proper nouns, content features such as word frequency (), and event features such as action nouns).", "labels": [], "entities": []}, {"text": "Sentences are typically assigned a score indicating the strength of presence of these features.", "labels": [], "entities": []}, {"text": "Several methods have been used in order to select the summary sentences ranging from binary classifiers (, to hidden Markov models), graph-based algorithms (, and integer linear programming.", "labels": [], "entities": []}, {"text": "In this work we propose a data-driven approach to summarization based on neural networks and continuous sentence features.", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9865757822990417}]}, {"text": "There has been a surge of interest recently in repurposing sequence transduction neural network architectures for NLP tasks such as machine translation), question answering (, and sentence compression (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7492273449897766}, {"text": "question answering", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.9085168838500977}, {"text": "sentence compression", "start_pos": 180, "end_pos": 200, "type": "TASK", "confidence": 0.7853461503982544}]}, {"text": "Central to these approaches is an encoderdecoder architecture modeled by recurrent neural networks.", "labels": [], "entities": []}, {"text": "The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence.", "labels": [], "entities": []}, {"text": "An attention mechanism ( is often used to locate the region of focus during decoding.", "labels": [], "entities": []}, {"text": "We develop a general framework for singledocument summarization which can be used to extract sentences or words.", "labels": [], "entities": [{"text": "singledocument summarization", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6555544137954712}]}, {"text": "Our model includes a neural network-based hierarchical document reader or encoder and an attention-based content extractor.", "labels": [], "entities": [{"text": "attention-based content extractor", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.6464025775591532}]}, {"text": "The role of the reader is to derive the meaning representation of a document based on its sentences and their constituent words.", "labels": [], "entities": []}, {"text": "Our models adopt a variant of neural attention to extract sentences or words.", "labels": [], "entities": []}, {"text": "Contrary to previous work where attention is an intermediate step used to blend hidden units of an encoder to a vector propagating additional information to the decoder, our model applies attention directly to select sentences or words of the input document as the output summary.", "labels": [], "entities": []}, {"text": "Similar neural attention architectures have been previously used for geometry reasoning , under the name Pointer Networks.", "labels": [], "entities": [{"text": "geometry reasoning", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8881266415119171}]}, {"text": "One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.7362189292907715}]}, {"text": "Inspired by previous work on summarization and reading comprehension ( we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website.", "labels": [], "entities": [{"text": "summarization", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.9923630952835083}, {"text": "DailyMail website", "start_pos": 160, "end_pos": 177, "type": "DATASET", "confidence": 0.9539865851402283}]}, {"text": "Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see for an example).", "labels": [], "entities": []}, {"text": "Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.7661145925521851}, {"text": "word extraction", "start_pos": 204, "end_pos": 219, "type": "TASK", "confidence": 0.793681263923645}]}, {"text": "Previous approaches have used small scale training data in the range of a few hundred examples.", "labels": [], "entities": []}, {"text": "Our work touches on several strands of research within summarization and neural sequence modeling.", "labels": [], "entities": [{"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9925320744514465}, {"text": "neural sequence modeling", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.5936214327812195}]}, {"text": "The idea of creating a summary by extracting words from the source document was pioneered in who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words.", "labels": [], "entities": [{"text": "summarization", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.9862251281738281}, {"text": "statistical machine translation", "start_pos": 142, "end_pos": 173, "type": "TASK", "confidence": 0.6423846880594889}]}, {"text": "Our word-based model is similar in spirit, however, it operates over continuous representations, produces multi-sentence output, and jointly selects summary words and organizes them into sentences.", "labels": [], "entities": []}, {"text": "A few recent studies () perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7859399318695068}]}, {"text": "Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more directly to perform the actual summarization task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 173, "end_pos": 186, "type": "TASK", "confidence": 0.9752237200737}]}, {"text": "propose a neural attention model for abstractive sentence compression which is trained on pairs of headlines and first sentences in an article.", "labels": [], "entities": [{"text": "abstractive sentence compression", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.6788703302542368}]}, {"text": "In contrast, our model summarizes documents rather than individual sentences, producing multi-sentential discourse.", "labels": [], "entities": []}, {"text": "A major architectural difference is that our decoder selects output symbols from the document of interest rather than the entire vocabulary.", "labels": [], "entities": []}, {"text": "This effectively helps us sidestep the difficulty of searching for the next output symbol under a large vocabulary, with lowfrequency words and named entities whose representations can be challenging to learn. and propose a similar \"copy\" mechanism in sentence compression and other tasks; their model can accommodate both generation and extraction by selecting which sub-sequences in the input sequence to copy in the output.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 252, "end_pos": 272, "type": "TASK", "confidence": 0.727101519703865}, {"text": "generation and extraction", "start_pos": 323, "end_pos": 348, "type": "TASK", "confidence": 0.7045306662718455}]}, {"text": "We evaluate our models both automatically (in terms of ROUGE) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9785994291305542}, {"text": "DUC 2002 document summarization corpus", "start_pos": 107, "end_pos": 145, "type": "DATASET", "confidence": 0.9140525817871094}, {"text": "DailyMail news highlights corpus", "start_pos": 158, "end_pos": 190, "type": "DATASET", "confidence": 0.9471154063940048}]}, {"text": "Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing handengineered features and sophisticated linguistic constraints.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental setup for assessing the performance of our summarization models.", "labels": [], "entities": []}, {"text": "We discuss the datasets used for A simpler model would use hard attention to select a sentence first and then a few words from it as a summary, but this would render the system non-differentiable for training.", "labels": [], "entities": []}, {"text": "Although hard attention can be trained with the REINFORCE algorithm, it requires sampling of discrete actions and could lead to high variance.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9807386994361877}]}, {"text": "We empirically found that feeding the previous sentencelevel attention vector as additional input to the LSTM would lead to small performance improvements.", "labels": [], "entities": []}, {"text": "This is not shown in the equation.", "labels": [], "entities": []}, {"text": "training and evaluation, give implementation details, briefly introduce comparison models, and explain how system output was evaluated.", "labels": [], "entities": []}, {"text": "Datasets We trained our sentence-and wordbased summarization models on the two datasets created from DailyMail news.", "labels": [], "entities": [{"text": "DailyMail news", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9718549847602844}]}, {"text": "Each dataset was split into approximately 90% for training, 5% for validation, and 5% for testing.", "labels": [], "entities": []}, {"text": "We evaluated the models on the DUC-2002 single document summarization task.", "labels": [], "entities": [{"text": "DUC-2002 single document summarization task", "start_pos": 31, "end_pos": 74, "type": "TASK", "confidence": 0.7621086359024047}]}, {"text": "In total, there are 567 documents belonging to 59 different clusters of various news topics.", "labels": [], "entities": []}, {"text": "Each document is associated with two versions of 100-word 7 manual summaries produced by human annotators.", "labels": [], "entities": []}, {"text": "We also evaluated our models on 500 articles from the DailyMail test set (with the human authored highlights as goldstandard).", "labels": [], "entities": [{"text": "DailyMail test set", "start_pos": 54, "end_pos": 72, "type": "DATASET", "confidence": 0.9847714503606161}]}, {"text": "We sampled article-highlight pairs so that the highlights include a minimum of 3 sentences.", "labels": [], "entities": []}, {"text": "The average byte count for each document is 278.", "labels": [], "entities": []}, {"text": "Implementation Details We trained our models with Adam () with initial learning rate 0.001.", "labels": [], "entities": [{"text": "Implementation Details", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6452798843383789}]}, {"text": "The two momentum parameters were set to 0.99 and 0.999 respectively.", "labels": [], "entities": []}, {"text": "We performed mini-batch training with a batch size of 20 documents.", "labels": [], "entities": []}, {"text": "All input documents were padded to the same length with an additional mask variable storing the real length for each document.", "labels": [], "entities": []}, {"text": "The size of word, sentence, and document embeddings were set to 150, 300, and 750, respectively.", "labels": [], "entities": []}, {"text": "For the convolutional sentence model, we followed and used a list of kernel sizes {1, 2, 3, 4, 5, 6, 7}.", "labels": [], "entities": []}, {"text": "For the recurrent document model and the sentence extractor, we used as regularization dropout with probability 0.5 on the LSTM input-to-hidden layers and the scoring layer.", "labels": [], "entities": [{"text": "sentence extractor", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7180793434381485}]}, {"text": "The depth of each LSTM module was 1.", "labels": [], "entities": [{"text": "depth", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.96180659532547}]}, {"text": "All LSTM parameters were randomly initialized over a uniform distribution within [-0.05, 0.05].", "labels": [], "entities": []}, {"text": "The word vectors were initialized with 150 dimensional pre-trained embeddings.", "labels": [], "entities": []}, {"text": "Proper nouns pose a problem for embeddingbased approaches, especially when these are rare or unknown (e.g., attest time).", "labels": [], "entities": []}, {"text": "address this issue by adding anew set of features and a log-linear model component to their system.", "labels": [], "entities": []}, {"text": "As our model enjoys the advantage of generation by extraction, we can force the model to inspect the context surrounding an entity and its relative position in the sentence in order to discover extractive patterns, placing less emphasis on the meaning representation of the entity itself.", "labels": [], "entities": []}, {"text": "Specifically, we perform named entity recognition with the package provided by and maintain a set of randomly initialized entity embeddings.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.6461169421672821}]}, {"text": "During training, the index of the entities is permuted to introduce some noise but also robustness in the data.", "labels": [], "entities": []}, {"text": "A similar data augmentation approach has been used for reading comprehension (.", "labels": [], "entities": []}, {"text": "A common problem with extractive methods based on sentence labeling is that there is no constraint on the number of sentences being selected attest time.", "labels": [], "entities": [{"text": "sentence labeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7143183201551437}]}, {"text": "We address this by reranking the positively labeled sentences with the probability scores obtained from the softmax layer (rather than the label itself).", "labels": [], "entities": []}, {"text": "In other words, we are more interested in is the relative ranking of each sentence rather than their exact scores.", "labels": [], "entities": []}, {"text": "This suggests that an alternative to training the network would be to employ a ranking-based objective or a learning to rank algorithm.", "labels": [], "entities": []}, {"text": "However, we leave this to future work.", "labels": [], "entities": []}, {"text": "We use the three sentences with the highest scores as the summary (also subject to the word or byte limit of the evaluation protocol).", "labels": [], "entities": []}, {"text": "Another issue relates to the word extraction model which is challenging to batch since each document possesses a distinct vocabulary.", "labels": [], "entities": [{"text": "word extraction", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7881923317909241}]}, {"text": "We sidestep this during training by performing negative sampling) which trims the vocabulary of different documents to the same length.", "labels": [], "entities": []}, {"text": "At each decoding step the model is trained to differentiate the true target word from 20 noise samples.", "labels": [], "entities": []}, {"text": "At test time we still loop through the words in the input document (and a stop-word list) to decide which word to output next.", "labels": [], "entities": []}, {"text": "System Comparisons We compared the output of our models to various summarization methods.", "labels": [], "entities": []}, {"text": "These included the standard baseline of simply selecting the \"leading\" three sentences from each document as the summary.", "labels": [], "entities": []}, {"text": "We also built a sentence extraction baseline classifier using logistic regression and human engineered features.", "labels": [], "entities": [{"text": "sentence extraction baseline classifier", "start_pos": 16, "end_pos": 55, "type": "TASK", "confidence": 0.8449060320854187}]}, {"text": "The classifier was trained on the same datasets as our neural network models with the following features: sentence length, sentence position, number of entities in the sentence, sentence-tosentence cohesion, and sentence-to-document relevance.", "labels": [], "entities": []}, {"text": "Sentence-to-sentence cohesion was computed by calculating for every document sentence its embedding similarity with every other sentence in the same document.", "labels": [], "entities": [{"text": "Sentence-to-sentence cohesion", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9055028259754181}]}, {"text": "The feature was the normalized sum of these similarity scores.", "labels": [], "entities": [{"text": "similarity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9569473266601562}]}, {"text": "Sentence embeddings were obtained by averaging the constituent word embeddings.", "labels": [], "entities": []}, {"text": "Sentence-to-document relevance was computed similarly.", "labels": [], "entities": [{"text": "relevance", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.49629539251327515}]}, {"text": "We calculated for each sentence its embedding similarity with the document (represented as bag-of-words), and normalized the score.", "labels": [], "entities": []}, {"text": "The word embeddings used in this baseline are the same as the pre-trained ones used for our neural models.", "labels": [], "entities": []}, {"text": "In addition, we included a neural abstractive summarization baseline.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.6630087494850159}]}, {"text": "This system has a similar architecture to our word extraction model except that it uses an open vocabulary during decoding.", "labels": [], "entities": [{"text": "word extraction", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7727391123771667}]}, {"text": "It can also be viewed as a hierarchical documentlevel extension of the abstractive sentence summarizer proposed by.", "labels": [], "entities": []}, {"text": "We trained this model with negative sampling to avoid the excessive computation of the normalization constant.", "labels": [], "entities": []}, {"text": "Finally, we compared our models to three previously published systems which have shown competitive performance on the DUC2002 single document summarization task.", "labels": [], "entities": [{"text": "DUC2002 single document summarization task", "start_pos": 118, "end_pos": 160, "type": "TASK", "confidence": 0.7498730182647705}]}, {"text": "The first approach is the phrase-based extraction model of.", "labels": [], "entities": [{"text": "phrase-based extraction", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7720933556556702}]}, {"text": "Their system learns to produce highlights from parsed input (phrase structure trees and dependency graphs); it selects salient phrases and recombines them subject to length, coverage, and grammar constraints enforced via integer linear programming (ILP).", "labels": [], "entities": []}, {"text": "Like ours, this model is trained on document-highlight pairs, and produces telegraphic-style bullet points rather than full-blown summaries.", "labels": [], "entities": []}, {"text": "The other two systems, TGRAPH) and URANK, produce more typical summaries and represent the state of the art.", "labels": [], "entities": [{"text": "TGRAPH", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.44475987553596497}, {"text": "URANK", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.8146297335624695}]}, {"text": "TGRAPH is a graph-based sentence extraction model, where the graph is constructed from topic models and the optimization is performed by constrained ILP.", "labels": [], "entities": [{"text": "TGRAPH", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6122114062309265}, {"text": "sentence extraction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7191997021436691}]}, {"text": "URANK adopts a unified ranking system for both single-and multidocument summarization.", "labels": [], "entities": [{"text": "URANK", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9390808343887329}]}, {"text": "Evaluation We evaluated the quality of the summaries automatically using ROUGE ( lap (ROUGE-1,2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9963422417640686}, {"text": "lap (ROUGE-1,2)", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.8011812716722488}, {"text": "longest common subsequence (ROUGE-L)", "start_pos": 145, "end_pos": 181, "type": "METRIC", "confidence": 0.5903670340776443}]}, {"text": "In addition, we evaluated the generated summaries by eliciting human judgments for 20 randomly sampled DUC 2002 test documents.", "labels": [], "entities": [{"text": "DUC 2002 test documents", "start_pos": 103, "end_pos": 126, "type": "DATASET", "confidence": 0.9584090709686279}]}, {"text": "Participants were presented with a news article and summaries generated by a list of systems.", "labels": [], "entities": []}, {"text": "These include two neural network systems (sentenceand word-based extraction), the neural abstractive system described earlier, the lead baseline, the phrase-based ILP model 10 of, and the human authored summary.", "labels": [], "entities": [{"text": "sentenceand word-based extraction", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.6151223679383596}]}, {"text": "Subjects were asked to rank the summaries from best to worst (with ties allowed) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?).", "labels": [], "entities": []}, {"text": "We elicited human judgments using Amazon's Mechanical Turk crowdsourcing platform.", "labels": [], "entities": []}, {"text": "Participants (self-reported native English speakers) saw 2 random articles per session.", "labels": [], "entities": []}, {"text": "We collected 5 responses per document.: Rankings (shown as proportions) and mean ranks given to systems by human participants (lower is better).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE evaluation (%) on the DUC-2002  and 500 samples from the DailyMail.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9988735318183899}, {"text": "DUC-2002", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9207621216773987}, {"text": "DailyMail", "start_pos": 73, "end_pos": 82, "type": "DATASET", "confidence": 0.9466912150382996}]}]}