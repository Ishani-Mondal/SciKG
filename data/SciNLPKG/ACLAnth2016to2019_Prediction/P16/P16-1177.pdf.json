{"title": [{"text": "Learning Text Pair Similarity with Context-sensitive Autoencoders", "labels": [], "entities": [{"text": "Learning Text Pair Similarity", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5997881218791008}]}], "abstractContent": [{"text": "We present a pairwise context-sensitive Autoencoder for computing text pair similarity.", "labels": [], "entities": []}, {"text": "Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs.", "labels": [], "entities": []}, {"text": "Our model outperforms the state-of-the-art models in two semantic retrieval tasks and a contextual word similarity task.", "labels": [], "entities": []}, {"text": "For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outper-forms them.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (.", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9554692506790161}]}, {"text": "Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (.", "labels": [], "entities": []}, {"text": "Recent research has made substantial progress on semantic similarity using neural networks (.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8642741739749908}]}, {"text": "In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs.", "labels": [], "entities": []}, {"text": "We represent context information as low dimensional vectors that will be injected to deep autoencoders.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work that enables integrating context into autoencoders.", "labels": [], "entities": []}, {"text": "In representation learning, context may appear in various forms.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.9518970549106598}]}, {"text": "For example, the context of a current sentence in a document could be either its neighboring sentences (;, topics associated with the sentence, the document that contains the sentence (, as well as their combinations (.", "labels": [], "entities": []}, {"text": "It is important to integrate context into neural networks because these models are often trained with only local information about their individual inputs.", "labels": [], "entities": []}, {"text": "For example, recurrent and recursive neural networks only use local information about previously seen words in a sentence to predict the next word or composition.", "labels": [], "entities": []}, {"text": "On the other hand, context information (such as topical information) often capture global information that can guide neural networks to generate more accurate representations.", "labels": [], "entities": []}, {"text": "We investigate the utility of context information in three semantic similarity tasks: contextual word sense similarity in which we aim to predict semantic similarity between given word pairs in their sentential context, question ranking in which we aim to retrieve semantically equivalent questions with respect to a given test question), and answer ranking in which we aim to rank single-sentence answers with respect to a given question.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: (1) integrating context information into deep autoencoders and showing that such integration improves the representation performance of deep autoencoders across several different semantic similarity tasks.", "labels": [], "entities": []}, {"text": "Our model outperforms the state-of-the-art su-pervised baselines in three semantic similarity tasks.", "labels": [], "entities": []}, {"text": "Furthermore, the unsupervised version of our autoencoder show comparable performance with the supervised baseline models and in some cases outperforms them.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at \u03b1 = 0.05.", "labels": [], "entities": [{"text": "asterisk mark", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9640607535839081}, {"text": "significance", "start_pos": 89, "end_pos": 101, "type": "METRIC", "confidence": 0.972184419631958}, {"text": "\u03b1", "start_pos": 105, "end_pos": 106, "type": "METRIC", "confidence": 0.8392700552940369}]}], "tableCaptions": [{"text": " Table 2: Spearman's \u03c1 correlation between model  predictions and human judgments in contextual  word similarity. (LC: local context only, LGC: lo- cal and global context.)", "labels": [], "entities": []}, {"text": " Table 4: Answer ranking in supervised setting", "labels": [], "entities": [{"text": "Answer ranking", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8679172396659851}]}, {"text": " Table 5: Answer ranking in unsupervised setting.", "labels": [], "entities": [{"text": "Answer ranking", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9448544979095459}]}, {"text": " Table 6: Question ranking in supervised setting", "labels": [], "entities": [{"text": "Question ranking", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8943334221839905}]}, {"text": " Table 7: Question ranking in unsupervised setting", "labels": [], "entities": [{"text": "Question ranking", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8698727488517761}]}]}