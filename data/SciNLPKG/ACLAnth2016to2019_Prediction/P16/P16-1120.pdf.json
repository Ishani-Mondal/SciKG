{"title": [], "abstractContent": [{"text": "This study proposes the bilingual segmented topic model (BiSTM), which hierarchically models documents by treating each document as a set of segments, e.g., sections.", "labels": [], "entities": []}, {"text": "While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al., 2009; Ni et al., 2009), consider only cross-lingual alignments between entire documents, the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments.", "labels": [], "entities": []}, {"text": "This study also presents a method for simultaneously inferring latent topics and segmen-tation boundaries, incorporating unsuper-vised topic segmentation (Du et al., 2013) into BiSTM.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed model significantly out-performs BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy).", "labels": [], "entities": [{"text": "BiLDA", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.7679534554481506}, {"text": "translation pair extraction", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.8057071169217428}, {"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.7918732762336731}]}], "introductionContent": [{"text": "Probabilistic topic models, such as probabilistic latent semantic analysis (PLSA)) and latent Dirichlet allocation (LDA) ( , are generative models for documents that have been used as unsupervised frameworks to discover latent topics in document collections without prior knowledge.", "labels": [], "entities": []}, {"text": "These topic models were originally applied to monolingual data; however, various recent studies have proposed the use of probabilistic topic models in multilingual set- Most multilingual topic models, including bilingual LDA (BiLDA) (, model a document-aligned comparable corpus, such as a collection of Wikipedia articles, where aligned documents are topically similar but are not direct translations 2 . In particular, these models assume that the documents in each tuple share the same topic distribution and that each cross-lingual topic has a language-specific word distribution.", "labels": [], "entities": []}, {"text": "Existing multilingual topic models consider only document-level alignments.", "labels": [], "entities": []}, {"text": "However, most documents are hierarchically structured, i.e., a document comprises segments (e.g., sections and paragraphs) that can be aligned across languages.", "labels": [], "entities": []}, {"text": "shows a Wikipedia article example, which contains a set of sections.", "labels": [], "entities": []}, {"text": "Sections 1, 2, and 3 in the English article correspond topically to sections 4, 2, and 3 in the Japanese counterpart, re-spectively.", "labels": [], "entities": []}, {"text": "To date, such segment-level alignments have been ignored; however, we consider that such corresponding segments must share the same topic distribution.", "labels": [], "entities": []}, {"text": "have shown that segment-level topics and their dependencies can improve modeling accuracy in a monolingual setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9638010263442993}]}, {"text": "Based on that research, we expect that segment-level topics can also be useful for modeling multilingual data.", "labels": [], "entities": []}, {"text": "This study proposes a bilingual segmented topic model (BiSTM) that extends BiLDA to capture segment-level alignments through a hierarchical structure.", "labels": [], "entities": []}, {"text": "In particular, BiSTM considers each document as a set of segments and models a document as a document-segment-word structure.", "labels": [], "entities": []}, {"text": "The topic distribution of each segment (per-segment topic distribution) is generated using a PitmanYor process (PYP), in which the base measure is the topic distribution of the related document (per-document topic distribution).", "labels": [], "entities": []}, {"text": "In addition, BiSTM introduces a binary variable that indicates whether two segments in different languages are aligned.", "labels": [], "entities": []}, {"text": "If two segments are aligned, their per-segment topic distributions are shared; if they are not aligned, they are independently generated.", "labels": [], "entities": []}, {"text": "BiSTM leverages existing segments from a given segmentation.", "labels": [], "entities": []}, {"text": "However, a segmentation is not always given, and a given segmentation might not be optimal for statistical modeling.", "labels": [], "entities": []}, {"text": "Therefore, this study also presents a model, BiSTM+TS, that incorporates unsupervised topic segmentation into BiSTM.", "labels": [], "entities": [{"text": "TS", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.3986824154853821}, {"text": "topic segmentation", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7500438988208771}]}, {"text": "BiSTM+TS integrates point-wise boundary sampling into BiSTM in a manner similar to that proposed by and infers segmentation boundaries and latent topics jointly.", "labels": [], "entities": []}, {"text": "Experiments using an English-Japanese and English-French Wikipedia corpus show that the proposed models (BiSTM and BiSTM+TS) significantly outperform the standard bilingual topic model (BiLDA) in terms of perplexity, and that they improve performance in translation extraction (up to +0.083 top 1 accuracy).", "labels": [], "entities": [{"text": "translation extraction", "start_pos": 254, "end_pos": 276, "type": "TASK", "confidence": 0.9729718863964081}, {"text": "accuracy", "start_pos": 297, "end_pos": 305, "type": "METRIC", "confidence": 0.9414777159690857}]}, {"text": "The experiments also reveal that BiSTM+TS is comparable to BiSTM, which uses manually provided segmentation, i.e., section boundaries in Wikipedia articles.", "labels": [], "entities": [{"text": "TS", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.704346239566803}]}], "datasetContent": [{"text": "We evaluated the proposed models in terms of perplexity and performance in translation pair extraction, which is a well-known application that uses a bilingual topic model.", "labels": [], "entities": [{"text": "translation pair extraction", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.9073884685834249}]}, {"text": "We used a document-aligned comparable corpus comprising 3,995 document pairs, each of which is a Japanese Wikipedia article in the Kyoto Wiki Corpus 5 and its corresponding English Wikipedia article 6 . Note that the English articles were collected from the English Wikipedia database dump (2 June 2015) based on inter-language links, even though the original Kyoto Wiki corpus is a parallel corpus, in which each sentence in the Japanese articles is manually translated into English.", "labels": [], "entities": [{"text": "Kyoto Wiki Corpus 5", "start_pos": 131, "end_pos": 150, "type": "DATASET", "confidence": 0.9496445059776306}, {"text": "English Wikipedia database dump (2 June 2015)", "start_pos": 258, "end_pos": 303, "type": "DATASET", "confidence": 0.8595817618899875}, {"text": "Kyoto Wiki corpus", "start_pos": 360, "end_pos": 377, "type": "DATASET", "confidence": 0.8590208689371744}]}, {"text": "Thus, our experimental data is not a parallel corpus.", "labels": [], "entities": []}, {"text": "We extracted texts from the collected English articles using an open-source script . All Japanese and English texts were segmented using MeCab and TreeTagger, respectively.", "labels": [], "entities": [{"text": "MeCab", "start_pos": 137, "end_pos": 142, "type": "DATASET", "confidence": 0.9558236002922058}]}, {"text": "Then, function words were removed, and the remaining words were lemmatized to reduce data sparsity.", "labels": [], "entities": []}, {"text": "For translation extraction experiments, we automatically created a gold-standard translation set according to.", "labels": [], "entities": [{"text": "translation extraction", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9767650067806244}]}, {"text": "We first computed p(w e |w f ) and p(w f |w e ) by running IBM Model 4 on the original Kyoto Wiki corpus, which is a parallel corpus, using GIZA++, and then extracted word pairs ( \u02c6 we , \u02c6 w f ) that satisfy both of the following conditions: \u02c6 we = argmax we p(w e |w f = \u02c6 w f ) and\u02c6w and\u02c6 and\u02c6w f = argmax w f p(w f |w e = \u02c6 we ).", "labels": [], "entities": [{"text": "Kyoto Wiki corpus", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.9333186944325765}]}, {"text": "Finally, we eliminated word pairs that do not appear in the document pairs in the document-aligned comparable corpus.", "labels": [], "entities": []}, {"text": "We used all 7,930 Japanese words in the resulting gold-standard set as the evaluation input.", "labels": [], "entities": []}, {"text": "We evaluated the predictive performance of each model by computing the test set perplexity based on 5-fold cross validation.", "labels": [], "entities": []}, {"text": "A lower perplexity indicates better generalization performance.", "labels": [], "entities": []}, {"text": "shows the perplexity of each model.", "labels": [], "entities": []}, {"text": "As can be seen, BiSTM and BiSTM+TS are better than BiLDA in terms of perplexity.", "labels": [], "entities": [{"text": "BiSTM+TS", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.5495034356911978}]}, {"text": "We measured the performance of translation extraction with top N accuracy (ACC N ), the number of test words whose top N translation candidates contain a correct translation over the total number of test words.", "labels": [], "entities": [{"text": "translation extraction", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9655390083789825}, {"text": "accuracy (ACC N )", "start_pos": 65, "end_pos": 82, "type": "METRIC", "confidence": 0.8286845922470093}]}, {"text": "summarizes ACC 1 and ACC 10 for each model.", "labels": [], "entities": [{"text": "ACC 1", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9555620551109314}, {"text": "ACC 10", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8630631864070892}]}, {"text": "As can be seen, Cue/Liu(BiSTM) and Cue/Liu(BiSTM+TS) significantly outperform Cue/Liu(BiLDA) (p < 0.01 in the sign test).", "labels": [], "entities": []}, {"text": "This indicates that BiSTM and BiSTM+TS improve the performance of translation extraction for both the Cue and Liu methods by assigning more suitable topics.", "labels": [], "entities": [{"text": "BiSTM+TS", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.6091297666231791}, {"text": "translation extraction", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9853418171405792}]}, {"text": "Both experiments prove that capturing segmentlevel alignments is effective for modeling bilingual data.", "labels": [], "entities": []}, {"text": "In addition, these experiments show that BiSTM+TS is comparable with BiSTM, indicat-", "labels": [], "entities": [{"text": "TS", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.5250517725944519}]}], "tableCaptions": [{"text": " Table 2: Test Set Perplexity", "labels": [], "entities": []}, {"text": " Table 3: Performance of Translation Extraction", "labels": [], "entities": [{"text": "Translation Extraction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.9088800251483917}]}, {"text": " Table 6: Performance on an English-French  Wikipedia Corpus (K = 2, 000)", "labels": [], "entities": [{"text": "English-French  Wikipedia Corpus", "start_pos": 28, "end_pos": 60, "type": "DATASET", "confidence": 0.7576059599717458}]}]}