{"title": [{"text": "A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings", "labels": [], "entities": [{"text": "Robust Topic Inference", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.6696155667304993}]}], "abstractContent": [{"text": "Uncovering thematic structures of SNS and blog posts is a crucial yet challenging task, because of the severe data spar-sity induced by the short length of texts and diverse use of vocabulary.", "labels": [], "entities": [{"text": "Uncovering thematic structures of SNS and blog posts", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7840685993432999}]}, {"text": "This hinders effective topic inference of traditional LDA because it infers topics based on document-level co-occurrence of words.", "labels": [], "entities": [{"text": "topic inference", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7304454296827316}]}, {"text": "To robustly infer topics in such contexts, we propose a latent concept topic model (LCTM).", "labels": [], "entities": []}, {"text": "Unlike LDA, LCTM reveals topics via co-occurrence of latent concepts, which we introduce as latent variables to capture conceptual similarity of words.", "labels": [], "entities": []}, {"text": "More specifically, LCTM models each topic as a distribution over the latent concepts , where each latent concept is a localized Gaussian distribution over the word embedding space.", "labels": [], "entities": []}, {"text": "Since the number of unique concepts in a corpus is often much smaller than the number of unique words, LCTM is less susceptible to the data spar-sity.", "labels": [], "entities": []}, {"text": "Experiments on the 20Newsgroups show the effectiveness of LCTM in dealing with short texts as well as the capability of the model in handling held-out documents with a high degree of OOV words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models such as Latent Dirichlet allocation (LDA) (, are widely used to uncover hidden topics within a text corpus.", "labels": [], "entities": [{"text": "Latent Dirichlet allocation (LDA)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.5770777215560278}]}, {"text": "LDA models each document as a mixture of topics where each topic is a distribution over words.", "labels": [], "entities": []}, {"text": "In essence, LDA reveals latent topics in a corpus by implicitly capturing document-level word cooccurrence patterns ().", "labels": [], "entities": []}, {"text": "In recent years, Social Networking Services and blogs have become increasingly prevalent due to the explosive growth of the Internet.", "labels": [], "entities": []}, {"text": "Uncovering the themantic structures of these posts is crucial for tasks like market review, trend estimation and soon.", "labels": [], "entities": [{"text": "market review", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.7466256022453308}, {"text": "trend estimation", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7322387993335724}, {"text": "soon", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9569821357727051}]}, {"text": "However, compared to more conventional documents, such as news articles and academic papers, analyzing the thematic content of blog posts can be challenging, because of their typically short length and the use of diverse vocabulary by various authors.", "labels": [], "entities": [{"text": "analyzing the thematic content of blog posts", "start_pos": 93, "end_pos": 137, "type": "TASK", "confidence": 0.7928645185061863}]}, {"text": "These factors can substantially decrease the chance of topically related words co-occurring in the same post, which in turn hinders effective topic inference in conventional topic models.", "labels": [], "entities": [{"text": "topic inference", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.6932318359613419}]}, {"text": "Additionally, sometimes small corpus size can further exacerbate topic inference, since word co-occurrence statistics becomes more sparse as the number of documents decreases.", "labels": [], "entities": [{"text": "topic inference", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7635747790336609}]}, {"text": "Recently, word embedding models, such as word2vec () and GloVe) have gained much attention with their ability to form clusters of conceptually similar words in the embedding space.", "labels": [], "entities": []}, {"text": "Inspired by this, we propose a latent concept topic model (LCTM) that infers topics based on documentlevel co-occurrence of references to the same concept.", "labels": [], "entities": []}, {"text": "More specifically, we introduce anew latent variable, termed a latent concept to capture conceptual similarity of words, and redefine each topic as a distribution over the latent concepts.", "labels": [], "entities": []}, {"text": "Each latent concept is then modeled as a localized Gaussian distribution over the embedding space.", "labels": [], "entities": []}, {"text": "This is illustrated in, where we denote the centers of the Gaussian distributions as concept vectors.", "labels": [], "entities": []}, {"text": "We see that each concept vector captures a representative concept of surrounding words, and the Gaussian distributions model the small variation between the latent concepts and the actual use of words.", "labels": [], "entities": []}, {"text": "Since the number of unique concepts that are referenced in a corpus is often much smaller than the number of unique words, we expect topically-related latent concepts to co-occur many times, even in short texts with diverse usage of words.", "labels": [], "entities": []}, {"text": "This in turn promotes topic inference in LCTM.", "labels": [], "entities": [{"text": "topic inference", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7756803631782532}]}, {"text": "LCTM further has the advantage of using continuous word embedding.", "labels": [], "entities": [{"text": "LCTM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7663816213607788}]}, {"text": "Traditional LDA assumes a fixed vocabulary of word types.", "labels": [], "entities": []}, {"text": "This modeling assumption prevents LDA from handling out of vocabulary (OOV) words in held-out documents.", "labels": [], "entities": []}, {"text": "On the other hands, since our topic model operates on the continuous vector space, it can naturally handle OOV words once their vector representation is provided.", "labels": [], "entities": []}, {"text": "The main contributions of our paper are as follows: We propose LCTM that infers topics via document-level co-occurrence patterns of latent concepts, and derive a collapsed Gibbs sampler for approximate inference.", "labels": [], "entities": []}, {"text": "We show that LCTM can accurately represent short texts by outperforming conventional topic models in a clustering task.", "labels": [], "entities": []}, {"text": "By means of a classification task, we furthermore demonstrate that LCTM achieves superior performance to other state-of-the-art topic models in handling documents with a high degree of OOV words.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: related work is summarized in Section 2, while LCTM and its inference algorithm are presented in Section 3.", "labels": [], "entities": []}, {"text": "Experiments on the 20News-groups are presented in Section 4, and a conclusion is presented in Section 5.", "labels": [], "entities": [{"text": "20News-groups", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9531505107879639}]}], "datasetContent": [{"text": "In this section, we study the empirical performance of LCTM on short texts.", "labels": [], "entities": []}, {"text": "We used the 20Newsgroups corpus, which consists of discussion posts about various news subjects authored by diverse readers.", "labels": [], "entities": [{"text": "20Newsgroups corpus", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.8747511804103851}]}, {"text": "Each document in the corpus is tagged with one of twenty newsgroups.", "labels": [], "entities": []}, {"text": "Only posts with less than 50 words are extracted for training datasets.", "labels": [], "entities": []}, {"text": "For external word embeddings, we used 50-dimensional GloVe 1 that were pre-trained on Wikipedia.", "labels": [], "entities": []}, {"text": "The datasets are summarized in Table 1.", "labels": [], "entities": []}, {"text": "See appendix A for the detail of the dataset preprocessing.", "labels": [], "entities": []}, {"text": "We compare the performance of the LCTM to the following six baselines: \u2022 LFLDA ( \u2022 LFDMM (, an extension of Dirichlet Multinomial Mixtures that incorporates word embeddings information.", "labels": [], "entities": []}, {"text": "\u2022 nI-cLDA, non-interactive constrained Latent Dirichlet Allocatoin, a variant of ITM (, where constraints are inferred by applying k-means to external word embeddings.", "labels": [], "entities": []}, {"text": "Each resulting word cluster is then regarded as a constraint.", "labels": [], "entities": []}, {"text": "See appendix B for the detail of the model.", "labels": [], "entities": [{"text": "detail", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9807084202766418}]}, {"text": "\u2022 GLDA (), Gaussian LDA.", "labels": [], "entities": [{"text": "GLDA", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.7418177127838135}]}, {"text": "\u2022 BTM (), Biterm Topic Model.", "labels": [], "entities": [{"text": "BTM", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.6169349551200867}]}, {"text": "In all the models, we set the number of topics to be 20.", "labels": [], "entities": []}, {"text": "For LCTM (resp. nI-ITM), we set the number of latent concepts (resp. constraints) to be 1000.", "labels": [], "entities": []}, {"text": "See appendix C for the detail of hyperparameter settings.", "labels": [], "entities": []}, {"text": "We preprocessed the 20Newsgroups as follows: We downloaded bag-of-words representation of the corpus available online 3 . Stop words 4 and words that were not covered in the GloVe were both removed.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 174, "end_pos": 179, "type": "DATASET", "confidence": 0.9481194019317627}]}, {"text": "After the preprocessing, we extracted short texts containing less than 50 words for training datasets.", "labels": [], "entities": []}, {"text": "We created three training datasets with varying numbers of documents, and one held-out dataset.", "labels": [], "entities": []}, {"text": "Each dataset was balanced in terms of the proportion of documents belonging to each newsgroup.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of datasets.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8035858869552612}]}, {"text": " Table 2: Proportions of OOV words and classifi- cation accuracy in the held-out documents.", "labels": [], "entities": [{"text": "Proportions", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9475476145744324}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.8311612606048584}]}]}