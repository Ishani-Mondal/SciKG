{"title": [{"text": "Specifying and Annotating Reduced Argument Span Via QA-SRL", "labels": [], "entities": []}], "abstractContent": [{"text": "Prominent semantic annotations take an inclusive approach to argument span annotation , marking arguments as full constituency subtrees.", "labels": [], "entities": []}, {"text": "Some works, however , showed that identifying a reduced argument span can be beneficial for various semantic tasks.", "labels": [], "entities": []}, {"text": "While certain practical methods do extract reduced argument spans, such as in Open-IE , these solutions are often ad-hoc and system-dependent, with no commonly accepted standards.", "labels": [], "entities": []}, {"text": "In this paper we propose a generic argument reduction criterion, along with an annotation procedure, and show that it can be consistently and intuitively annotated using the recent QA-SRL paradigm.", "labels": [], "entities": [{"text": "argument reduction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7113212943077087}]}], "introductionContent": [{"text": "Representations of predicate-argument structure need to determine the span of predicates and their corresponding arguments.", "labels": [], "entities": []}, {"text": "Surprisingly, there are no accepted NLP-standards which specify what the \"right\" span of an argument should be.", "labels": [], "entities": []}, {"text": "Semantic representations typically take an inclusive (or maximal) approach: PropBank annotation (), for example, marks arguments as full constituency subtrees.", "labels": [], "entities": []}, {"text": "From an application perspective, this maximal approach ensures that all arguments are indeed embedded within the annotated span, yet it is often not trivial how to accurately recover them.", "labels": [], "entities": []}, {"text": "In contrast to this maximal-span approach, Open-IE systems () put emphasis on extracting readable standalone propositions, typically producing shorter arguments (see examples in Section 2.1).", "labels": [], "entities": []}, {"text": "Several recent works have exploited this property, using Open-IE extractions as an intermediate representation within a larger framework.", "labels": [], "entities": []}, {"text": "built an Open-IE system which focuses on shorter argument spans.", "labels": [], "entities": []}, {"text": "They hypothesize that \"shorter arguments more likely to be useful for downstream applications\", and demonstrate this by using their system to extract facts about predefined entities in a state-ofthe-art Knowledge Base Population system.", "labels": [], "entities": []}, {"text": "Further, compared the performance of several off-the-shelf parsers in different semantic tasks.", "labels": [], "entities": []}, {"text": "Most relevant to this work is the comparison between Open-IE and SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.5956132411956787}]}, {"text": "Specifically, they suggest that SRL's longer arguments introduce noise which hurts performance for downstream tasks.", "labels": [], "entities": [{"text": "SRL", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9753736853599548}]}, {"text": "This is sustained empirically by showing that extractions from Open-IE4 1 significantly outperform ClearNLP's SRL) in textual similarity, analogies, and reading comprehension tasks.", "labels": [], "entities": [{"text": "ClearNLP's SRL", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.8434847195943197}]}, {"text": "While Open-IE extractors do provide a reduction of argument span, they lack consistency and principled rigor -there is no clear definition for the desired argument span, which is defined defacto by the different implementations.", "labels": [], "entities": [{"text": "consistency", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9794628024101257}]}, {"text": "This lack of a common system-independent definition, let alone an annotation methodology, hinders the creation of gold standard argument-span annotation.", "labels": [], "entities": []}, {"text": "In this work we propose a concrete argument span reduction criterion and an accompanying annotation procedure, based on the recent QA-SRL paradigm.", "labels": [], "entities": [{"text": "argument span reduction", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6530754268169403}]}, {"text": "We show that this criterion can be consistently annotated with high agreement, and that it is intuitive enough to be obtained through crowd-sourcing.", "labels": [], "entities": [{"text": "agreement", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9542104601860046}]}, {"text": "As future work, we intend to apply the reduction criterion to other types of predicates (e.g., nomi-nal and adjectival predication).", "labels": [], "entities": []}, {"text": "Subsequently, we would like to create a comprehensive annotated resource, as a benchmark for the detection of reduced argument spans.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the compilation and analysis of a small-scale expert annotation corpus.", "labels": [], "entities": []}, {"text": "Creating such corpus serves 3 goals: (1) It allows us to test the applicability of the argument reducing procedure, (2) By comparing it with Propbank we can examine how often, and in which cases, we reduce arguments (Section 4.1), and (3) We can assess the plausibility of crowd-sourcing argument span annotation (Sections 4.2 and 4.3).", "labels": [], "entities": [{"text": "argument reducing", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7378738671541214}, {"text": "Propbank", "start_pos": 141, "end_pos": 149, "type": "DATASET", "confidence": 0.9295580387115479}]}, {"text": "In order to achieve these goals, we sample 100 predicates of the Propbank corpus, which covered 260 arguments.", "labels": [], "entities": [{"text": "Propbank corpus", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.9744105339050293}]}, {"text": "To allow comparisons, we sample predicates which were annotated by QA-SRL and whose arguments were aligned by) with a matching Propbank argument.", "labels": [], "entities": []}, {"text": "Two expert annotators used the QA-SRL's interface to re-answer the original QA-SRL annotated questions with minimally-scoped arguments, according to the procedure described in Section 3.", "labels": [], "entities": []}, {"text": "Prior to annotating the expert dataset, the annotators discussed the process and resolved conflicts on a separate development set of 20 predicates.", "labels": [], "entities": []}, {"text": "Annotator agreement From an argument perspective, the annotators fully agreed on the span of 94.6% of the arguments.", "labels": [], "entities": []}, {"text": "Looking into the word token level, we found that fora given PropBank argument a = (w 1 , ..., w n ), the respective reduced arguments always constitute a subset of a.", "labels": [], "entities": []}, {"text": "This allows us to look at the annotation process as a list of n mapping decisions -for each w i , an annotator decides whether he (1) Maps it to one or more of the argu-ments of M (a), or (2) Deletes it.", "labels": [], "entities": []}, {"text": "The complete annotation required each annotator to make 985 such mappings decisions.", "labels": [], "entities": []}, {"text": "Word level agreement between the annotators was calculated as the percent of the decisions on which they agreed, and found to be 97.1%.", "labels": [], "entities": []}, {"text": "Overall, the annotators achieved a high level of agreement, suggesting that the reduction criterion can be consistently applied by trained annotators.", "labels": [], "entities": [{"text": "agreement", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9962722063064575}]}, {"text": "An analysis of the few disagreements revealed that the deviations between the annotators stem from semantic ambiguities, where two legitimate readings of the sentence led to different span annotations.", "labels": [], "entities": []}, {"text": "Finally, we compose the expert annotation dataset from 247 arguments on which both annotators fully agreed.", "labels": [], "entities": []}], "tableCaptions": []}