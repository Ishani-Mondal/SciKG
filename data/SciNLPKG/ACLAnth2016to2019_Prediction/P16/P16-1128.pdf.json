{"title": [], "abstractContent": [{"text": "Word embeddings-distributed representations of words-in deep learning are beneficial for many tasks in NLP.", "labels": [], "entities": [{"text": "Word embeddings-distributed representations of words-in deep learning", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.697669872215816}]}, {"text": "However , different embedding sets vary greatly in quality and characteristics of the captured information.", "labels": [], "entities": []}, {"text": "Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings.", "labels": [], "entities": []}, {"text": "Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings compared to individual embedding sets.", "labels": [], "entities": [{"text": "word similarity and analogy tasks", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.8154780149459839}, {"text": "part-of-speech tagging", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7266862392425537}]}, {"text": "One advantage of metaembeddings is the increased vocabulary coverage.", "labels": [], "entities": []}, {"text": "We release our metaembeddings publicly at http:// cistern.cis.lmu.de/meta-emb.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, deep neural network (NN) models have achieved remarkable results in NLP.", "labels": [], "entities": []}, {"text": "One reason for these results are word embeddings, compact distributed word representations learned in an unsupervised manner from large corpora ().", "labels": [], "entities": []}, {"text": "Some prior work has studied differences in performance of different embedding sets.", "labels": [], "entities": []}, {"text": "For example, show that the embedding sets HLBL), SENNA,) and Huang () have great variance in quality and characteristics of the semantics captured.", "labels": [], "entities": []}, {"text": "show that embeddings learned by NN machine translation models can outperform three representative monolingual embedding sets:), GloVe () and CW.", "labels": [], "entities": [{"text": "NN machine translation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.5514730910460154}, {"text": "GloVe", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9162721633911133}]}, {"text": "find that Brown clustering, SENNA, CW, Huang and word2vec yield significant gains for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.8867773115634918}]}, {"text": "Moreover, using these representations together achieved the best results, suggesting their complementarity.", "labels": [], "entities": []}, {"text": "These prior studies motivate us to explore an ensemble approach.", "labels": [], "entities": []}, {"text": "Each embedding set is trained by a different NN on a different corpus, hence can be treated as a distinct description of words.", "labels": [], "entities": []}, {"text": "We want to leverage this diversity to learn better-performing word embeddings.", "labels": [], "entities": []}, {"text": "Our expectation is that the ensemble contains more information than each component embedding set.", "labels": [], "entities": []}, {"text": "The ensemble approach has two benefits.", "labels": [], "entities": []}, {"text": "First, enhancement of the representations: metaembeddings perform better than the individual embedding sets.", "labels": [], "entities": []}, {"text": "Second, coverage: metaembeddings cover more words than the individual embedding sets.", "labels": [], "entities": [{"text": "coverage", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9668413996696472}]}, {"text": "The first three ensemble methods we introduce are CONC, SVD and 1TON and they directly only have the benefit of enhancement.", "labels": [], "entities": [{"text": "1TON", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.7547237873077393}]}, {"text": "They learn metaembeddings on the overlapping vocabulary of the embedding sets.", "labels": [], "entities": []}, {"text": "CONC concatenates the vectors of a word from the different embedding sets.", "labels": [], "entities": []}, {"text": "SVD performs dimension reduction on this concatenation.", "labels": [], "entities": [{"text": "SVD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7838948965072632}]}, {"text": "1TON assumes that a metaembedding for the word exists, e.g., it can be a randomly initialized vector in the beginning, and uses this metaembedding to predict representations of the word in the individual embedding sets by projections -the resulting fine-tuned metaembedding is expected to contain knowledge from all individual embedding sets.", "labels": [], "entities": []}, {"text": "To also address the objective of increased coverage of the vocabulary, we introduce 1TON + , a modification of 1TON that learns metaembeddings for all words in the vocabulary union in one step.", "labels": [], "entities": []}, {"text": "Let an out-of-vocabulary (OOV) word w of embedding set ES be a word that is not covered by ES (i.e., ES does not contain an embedding for w).", "labels": [], "entities": []}, {"text": "1 1TON + first randomly initializes the embeddings for OOVs and the metaembeddings, then uses a prediction setup similar to 1TON to update metaembeddings as well as OOV embeddings.", "labels": [], "entities": []}, {"text": "Thus, 1TON + simultaneously achieves two goals: learning metaembeddings and extending the vocabulary (for both metaembeddings and invidual embedding sets).", "labels": [], "entities": []}, {"text": "An alternative method that increases coverage is MUTUALLEARNING.", "labels": [], "entities": [{"text": "coverage", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9626026749610901}, {"text": "MUTUALLEARNING", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.6643468737602234}]}, {"text": "MUTUALLEARNING learns the embedding fora word that is an OOV in embedding set from its embeddings in other embedding sets.", "labels": [], "entities": [{"text": "MUTUALLEARNING", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.5933361053466797}]}, {"text": "We will use MUTUALLEARNING to increase coverage for CONC, SVD and 1TON, so that these three methods (when used together with MUTUALLEARNING) have the advantages of both performance enhancement and increased coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9776795506477356}, {"text": "CONC", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.7454198598861694}, {"text": "coverage", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9712916612625122}]}, {"text": "In summary, metaembeddings have two benefits compared to individual embedding sets: enhancement of performance and improved coverage of the vocabulary.", "labels": [], "entities": []}, {"text": "Below, we demonstrate this experimentally for three tasks: word similarity, word analogy and POS tagging.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7780144214630127}, {"text": "word analogy", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.798488974571228}, {"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.8414156436920166}]}, {"text": "If we simply view metaembeddings as away of coming up with better embeddings, then the alternative is to develop a single embedding learning algorithm that produces better embeddings.", "labels": [], "entities": []}, {"text": "Some improvements proposed before have the disadvantage of increasing the training time of embedding learning substantially; e.g., the NNLM presented in () is an order of magnitude less efficient than an algorithm like word2vec and, more generally, replacing a linear objective function with a nonlinear objective function increases training time.", "labels": [], "entities": []}, {"text": "Similarly, fine-tuning the hyperparameters of the embedding learning algorithm is complex and time consuming.", "labels": [], "entities": []}, {"text": "In terms of coverage, one might argue that we can retrain an existing algorithm like word2vec on a bigger corpus.", "labels": [], "entities": []}, {"text": "However, that needs much longer training time than our simple ensemble approaches which achieve coverage as well as enhancement with less effort.", "labels": [], "entities": [{"text": "coverage", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9255082607269287}]}, {"text": "In many cases, it is not possible to retrain using a different algorithm because the corpus is not publicly available.", "labels": [], "entities": []}, {"text": "But even if these obstacles could be overcome, it is unlikely that there ever will be a single \"best\" embedding learning algorithm.", "labels": [], "entities": []}, {"text": "So the current situation of multiple embedding sets with different properties being available is likely to persist for the forseeable future.", "labels": [], "entities": []}, {"text": "Metaembedding learning is a simple and efficient way of taking advantage of this diversity.", "labels": [], "entities": []}, {"text": "As we will show below they combine several complementary embedding sets and the resulting metaembeddings are stronger than each individual set.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we use five released embedding sets.", "labels": [], "entities": []}, {"text": "gives a summary of the five embedding sets.", "labels": [], "entities": []}, {"text": "The intersection of the five vocabularies has size 35,965, the union has size 2,788,636.", "labels": [], "entities": []}, {"text": "We train NNs by back-propagation with AdaGrad () and mini-batches.: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning (accuracy).", "labels": [], "entities": [{"text": "analogical reasoning", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.7792395353317261}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9937146306037903}]}, {"text": "The number of OOVs is given in parentheses for each result.", "labels": [], "entities": [{"text": "number of OOVs", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.8364946444829305}]}, {"text": "\"ind-full/ind-overlap\": individual embedding sets with respective full/overlapping vocabulary; \"ensemble\": ensemble results using all five embedding sets; \"discard\": one of the five embedding sets is removed.", "labels": [], "entities": []}, {"text": "If a result is better than all methods in \"ind-overlap\", then it is bolded.", "labels": [], "entities": []}, {"text": "Significant improvement over the best baseline in \"ind-overlap\" is underlined (online toolkit from http://vassarstats.net/index.html for Spearman correlation metric, test of equal proportions for accuracy, p < .05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.998971700668335}]}], "tableCaptions": [{"text": " Table 1: Embedding Sets (Dim: dimensionality of word embeddings).", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameters. bs: batch size; lr:  learning rate; l 2 : L2 weight.", "labels": [], "entities": []}, {"text": " Table 3: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning  (accuracy). The number of OOVs is given in parentheses for each result. \"ind-full/ind-overlap\": indi- vidual embedding sets with respective full/overlapping vocabulary; \"ensemble\": ensemble results using  all five embedding sets; \"discard\": one of the five embedding sets is removed. If a result is better  than all methods in \"ind-overlap\", then it is bolded. Significant improvement over the best baseline  in \"ind-overlap\" is underlined (online toolkit from http://vassarstats.net/index.html for  Spearman correlation metric, test of equal proportions for accuracy, p < .05).", "labels": [], "entities": [{"text": "analogical reasoning", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7523336112499237}, {"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9983736276626587}, {"text": "OOVs", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9692990779876709}, {"text": "accuracy", "start_pos": 663, "end_pos": 671, "type": "METRIC", "confidence": 0.9976668357849121}]}, {"text": " Table 4: Comparison of effectiveness of four methods for learning OOV embeddings. RND: random  initialization. AVG: average of embeddings of known words. ml: MUTUALLEARNING. RW(21) means  there are still 21 OOVs for the vocabulary union.", "labels": [], "entities": [{"text": "RND", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9466153383255005}, {"text": "AVG", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9928061366081238}, {"text": "MUTUALLEARNING", "start_pos": 159, "end_pos": 173, "type": "METRIC", "confidence": 0.946975588798523}, {"text": "RW", "start_pos": 175, "end_pos": 177, "type": "METRIC", "confidence": 0.9202710390090942}]}, {"text": " Table 5: POS tagging results on six target domains. \"baselines\" lists representative systems for this task,  including FLORS. \"+indiv / +meta\": FLORS with individual embedding set / metaembeddings. Bold  means higher than \"baselines\" and \"+indiv\".", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7844105064868927}, {"text": "FLORS", "start_pos": 120, "end_pos": 125, "type": "METRIC", "confidence": 0.8617910146713257}]}]}