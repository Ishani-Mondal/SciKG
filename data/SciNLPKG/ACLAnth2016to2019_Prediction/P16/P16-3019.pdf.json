{"title": [{"text": "An Efficient Cross-lingual Model for Sentence Classification Using Convolutional Neural Network", "labels": [], "entities": [{"text": "Sentence Classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.9475842416286469}]}], "abstractContent": [{"text": "In this paper, we propose a cross-lingual convolutional neural network (CNN) model that is based on word and phrase embeddings learned from unlabeled data in two languages and dependency grammar.", "labels": [], "entities": []}, {"text": "Compared to traditional machine translation (MT) based methods for cross lingual sentence modeling, our model is much simpler and does not need parallel corpora or language specific features.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.8648744344711303}, {"text": "cross lingual sentence modeling", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6231973096728325}]}, {"text": "We only use a bilingual dictionary and dependency parser.", "labels": [], "entities": []}, {"text": "This makes our model particularly appealing for resource poor languages.", "labels": [], "entities": []}, {"text": "We evaluate our model using English and Chinese data on several sentence classification tasks.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.7194975316524506}]}, {"text": "We show that our model achieves a comparable and even better performance than the traditional MT-based method.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 94, "end_pos": 102, "type": "TASK", "confidence": 0.9681711792945862}]}], "introductionContent": [{"text": "With the rapid growth of global Internet, huge amounts of information are created in different languages.", "labels": [], "entities": []}, {"text": "It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations.", "labels": [], "entities": []}, {"text": "Traditionally, cross-lingual systems rely highly on machine translation (MT) systems).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.8422996759414673}]}, {"text": "They translate data in one language into the other, and then apply monolingual models.", "labels": [], "entities": []}, {"text": "One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.960682213306427}]}, {"text": "Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.972251832485199}]}, {"text": "Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL).", "labels": [], "entities": [{"text": "structural correspondence learning (SCL)", "start_pos": 107, "end_pos": 147, "type": "TASK", "confidence": 0.6623183637857437}]}, {"text": "They showed some promising results on document level classification tasks.", "labels": [], "entities": [{"text": "document level classification tasks", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.7792962491512299}]}, {"text": "However, their methods require carefully designed language specific features and find the \"pivot features\" across languages, which can be very expensive and inefficient.", "labels": [], "entities": []}, {"text": "To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN).", "labels": [], "entities": []}, {"text": "Sentence modeling using CNN has shown its great potential in recent years (.", "labels": [], "entities": [{"text": "Sentence modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9363533854484558}]}, {"text": "One of the advantages is that CNN requires much less expertise knowledge than traditional feature based models.", "labels": [], "entities": [{"text": "CNN", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8827482461929321}]}, {"text": "The only input of the model, word embeddings, can be learned automatically from large unlabeled text data.", "labels": [], "entities": []}, {"text": "There are roughly two main differences between different languages, lexicon and grammar.", "labels": [], "entities": []}, {"text": "Lexicon can be seen as a set of symbols with each symbol representing certain meanings.", "labels": [], "entities": []}, {"text": "A bilingual dictionary easily enables us to map from one symbol set to another.", "labels": [], "entities": []}, {"text": "As for grammar, it decides the organization of lexical symbols, i.e., word order.", "labels": [], "entities": []}, {"text": "Different languages organize their words in different manners (see for an example).", "labels": [], "entities": []}, {"text": "To reduce grammar difference, we propose to use dependency grammar as an intermediate grammar.", "labels": [], "entities": []}, {"text": "As shown in, dependency grammar can yield a similar dependency tree between two sentences in different languages.", "labels": [], "entities": [{"text": "dependency grammar", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.8776650726795197}]}, {"text": "To bridge two different languages from aspects of both lexicon and grammar, our CNN-based cross-lingual model consists of two components, bilingual word embedding learning and CNN incorporating dependency information.", "labels": [], "entities": []}, {"text": "We propose a method to learn bilingual word embeddings as the input of CNN, using only a bilingual dictionary and unlabeled corpus.", "labels": [], "entities": []}, {"text": "We then adopt a dependency-based CNN (DCNN) ( to incorporate dependency tree information.", "labels": [], "entities": []}, {"text": "We also design lexical features and phrase-based bilingual embeddings to improve our cross-lingual sentence model.", "labels": [], "entities": []}, {"text": "We evaluate our model on English and Chinese data.", "labels": [], "entities": []}, {"text": "We train a cross-lingual model on English data and then test it on Chinese data.", "labels": [], "entities": []}, {"text": "Our experiments show that compared to the MT based crosslingual model, our model achieves a comparable and even better performance on several sentence classification tasks including question classification, opinion analysis and sentence level event detection.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9208861589431763}, {"text": "sentence classification", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.7332514822483063}, {"text": "question classification", "start_pos": 182, "end_pos": 205, "type": "TASK", "confidence": 0.8210164904594421}, {"text": "opinion analysis", "start_pos": 207, "end_pos": 223, "type": "TASK", "confidence": 0.7420865595340729}, {"text": "sentence level event detection", "start_pos": 228, "end_pos": 258, "type": "TASK", "confidence": 0.6803465262055397}]}], "datasetContent": [{"text": "To evaluate our model, we select four sentence classification tasks including question classification, sentiment classification on movie review, sentiment classification on product review and sentence level event detection.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7349119186401367}, {"text": "question classification", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.806056797504425}, {"text": "sentiment classification on movie review", "start_pos": 103, "end_pos": 143, "type": "TASK", "confidence": 0.8468475818634034}, {"text": "sentiment classification on product review", "start_pos": 145, "end_pos": 187, "type": "TASK", "confidence": 0.868532907962799}, {"text": "sentence level event detection", "start_pos": 192, "end_pos": 222, "type": "TASK", "confidence": 0.636018231511116}]}, {"text": "For each task, we either use existing data or collect our own.", "labels": [], "entities": []}, {"text": "It is difficult to find cross-lingual data with identical annotation schema for all the tasks.", "labels": [], "entities": []}, {"text": "We thus collect English and Chinese corpora from tasks with similar annotation schema and take the overlapping part.", "labels": [], "entities": []}, {"text": "For all the tasks, we train our model on English data, and test on Chinese data.", "labels": [], "entities": []}, {"text": "To tune our model, we split Chinese dataset into validation and test sets.", "labels": [], "entities": [{"text": "Chinese dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.8656638562679291}]}, {"text": "Question classification (QC) aims to determine the category of a given question sentence.", "labels": [], "entities": [{"text": "Question classification (QC)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8708653330802918}]}, {"text": "For English, we use the TREC 1 dataset.", "labels": [], "entities": [{"text": "TREC 1 dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.7817604343096415}]}, {"text": "For Chinese, we use a QA corpus from HIT-IRLab 2 . We kept the six overlapped question types for both English and Chinese corpora.", "labels": [], "entities": []}, {"text": "The final corpus includes 4,313 English questions and 4,031 Chinese questions (859 for testing, 859 for validation and 2,313 for training 3 ).", "labels": [], "entities": []}, {"text": "Sentiment classification on movie review (SC-M) aims to classify apiece of given movie review into positive or negative.", "labels": [], "entities": [{"text": "Sentiment classification on movie review (SC-M)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8921028524637222}]}, {"text": "For English, we use IMDB polarity movie reviews from () (5,331 positive and 5,331 negative).", "labels": [], "entities": []}, {"text": "For Chinese, we use the short Chinese movie reviews from Douban 4 . Like IMDB, users from Douban leave their comments along with a score for the movie.", "labels": [], "entities": [{"text": "Chinese movie reviews from Douban 4", "start_pos": 30, "end_pos": 65, "type": "DATASET", "confidence": 0.6390835692485174}]}, {"text": "We collected 250 one star reviews (lowest score), and 250 five star reviews (highest score).", "labels": [], "entities": []}, {"text": "We randomly split the 500 reviews into 200 for validation and 300 for testing.", "labels": [], "entities": []}, {"text": "Sentiment classification on product review (SC-P) aims to classify apiece of given product review into positive or negative.", "labels": [], "entities": [{"text": "Sentiment classification on product review (SC-P)", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.9058051630854607}]}, {"text": "Their Chinese dataset contains mostly short reviews.", "labels": [], "entities": [{"text": "Chinese dataset", "start_pos": 6, "end_pos": 21, "type": "DATASET", "confidence": 0.9060049951076508}]}, {"text": "However, their English Amazon product reviews are generally longer, containing several sentences.", "labels": [], "entities": [{"text": "English Amazon product reviews", "start_pos": 15, "end_pos": 45, "type": "DATASET", "confidence": 0.8327405452728271}]}, {"text": "Although our model is designed to take a single sentence as input, CNN can actually handle any input length.", "labels": [], "entities": []}, {"text": "We remove reviews that are longer than 100 words and treat the remaining review as a single sentence.", "labels": [], "entities": []}, {"text": "For dependency parsing, we combine the root of each sentence and make it a global dependency tree.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8927217125892639}]}, {"text": "In the end, we got 3,134 English product reviews (1,707 positive, 1,427 negative); 1000 (549 positive, 451 negative) and 314 (163 positive, 151 negative) Chinese ones for validation and testing respectively.", "labels": [], "entities": []}, {"text": "Sentence level event detection (ED) aims to determine if a sentence contains an event.", "labels": [], "entities": [{"text": "Sentence level event detection (ED)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8179490310805184}]}, {"text": "ACE 2005 corpus 5 is ideal for cross-lingual tasks, be-cause it contains annotated data for different languages with the same definition of events.", "labels": [], "entities": [{"text": "ACE 2005 corpus 5", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9807854145765305}]}, {"text": "Sentence is the smallest unit that contains a set of complete event information, i.e., triggers and corresponding arguments.", "labels": [], "entities": []}, {"text": "To build the sentence level corpus, we first split document into sentences.", "labels": [], "entities": []}, {"text": "For each sentence, if an event occurs (event triggers and arguments exist), we label the sentence as positive.", "labels": [], "entities": []}, {"text": "Otherwise, we label it as negative.", "labels": [], "entities": []}, {"text": "In the end we have 11,090 English sentences (3,688 positive, 7,402 negative).", "labels": [], "entities": []}, {"text": "From the Chinese data we randomly selected 500 Chinese sentences (157 positive, 343 negative) for test, and 500 (138 positive, 362 negative) for validation.", "labels": [], "entities": []}, {"text": "The remaining 5,039 ones (1767 positive, 3772 negative) are kept as training set.", "labels": [], "entities": []}, {"text": "Because this is a detection task, we report F-score for it.", "labels": [], "entities": [{"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9821949601173401}]}, {"text": "We compare our bilingual word embedding based strategy to MT-based approach on the above four cross-lingual sentence classification tasks.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 58, "end_pos": 66, "type": "TASK", "confidence": 0.9429113268852234}, {"text": "cross-lingual sentence classification tasks", "start_pos": 94, "end_pos": 137, "type": "TASK", "confidence": 0.700544036924839}]}, {"text": "Besides, we also evaluate the effectiveness of incorporating dependency information into CNN for sentence modeling.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.7503959238529205}]}, {"text": "For the traditional MT-based cross-lingual method, we use the state-of-the-art statistical MT system Moses 6 . Language model is trained on Chinese gigawords corpus with SRILM 8 . The parallel corpora used are from LDC 9 . We first translate English data into Chinese, and then apply the model trained on the translated dataset to the Chinese test data.", "labels": [], "entities": [{"text": "MT-based", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.9810035228729248}, {"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9328330159187317}, {"text": "Chinese test data", "start_pos": 335, "end_pos": 352, "type": "DATASET", "confidence": 0.71794726451238}]}, {"text": "For sentence classification, we use both basic gradient descent (SGD) learning method.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8451353907585144}]}, {"text": "We apply random dropout () on the last fully connected layer for regularization.", "labels": [], "entities": []}, {"text": "We use ADADELTA algorithm to automatically control the learning rate and progress.", "labels": [], "entities": [{"text": "ADADELTA", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9815355539321899}]}, {"text": "The batch size for SGD and feature maps are tuned on the validation set for each task and fixed across different configurations.", "labels": [], "entities": [{"text": "SGD", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.939696192741394}]}, {"text": "We preprocess all our corpora with Stanford CoreNLP (), including word segmentation, sentence segmentation and dependency parsing.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.923293262720108}, {"text": "word segmentation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7405018657445908}, {"text": "sentence segmentation", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7384829223155975}, {"text": "dependency parsing", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7887978553771973}]}, {"text": "shows the results of different systems.", "labels": [], "entities": []}, {"text": "When using the MT based methods, the basic CNN achieves better results than DCNN.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9604500532150269}]}, {"text": "One possible reason is that the translation system produces errors, which may affect the performance of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.8102548718452454}]}, {"text": "For our method using bilingual word embeddings, basic CNN encodes only lexicon mapping information, and is not good at capturing grammar patterns.", "labels": [], "entities": []}, {"text": "Therefore, it is natural this system has the lowest result.", "labels": [], "entities": []}, {"text": "DCNN performs better than CNN, because it is able to capture additional grammar patterns across two languages by incorporating dependency information.", "labels": [], "entities": [{"text": "DCNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8558552861213684}]}, {"text": "Adding lexical features (DCNN+Lex) further improves performance.", "labels": [], "entities": []}, {"text": "Given the fact that dependency parser is not perfect and dependency grammar between languages is not exactly the same, the grammar patterns that DCNN learned are not always reliable.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.8142432868480682}]}, {"text": "The lexical feature here acts as an additional evidence to make the model more robust.", "labels": [], "entities": []}, {"text": "DCNN+Lex+Phrase yields the best performance.", "labels": [], "entities": [{"text": "Phrase", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.8107694387435913}]}, {"text": "The bilingual lexicon dictionary we use contains 54,168 Chinese words, and of them have phrase-based translations.", "labels": [], "entities": []}, {"text": "Therefore, phrase-based bilingual word embeddings can represent sentences more accurately, and thus yield better results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of different systems. +Lex: lexi- cal features are used; +Phrase: phrase-based bilin- gual word embeddings and grammar are used.", "labels": [], "entities": []}]}