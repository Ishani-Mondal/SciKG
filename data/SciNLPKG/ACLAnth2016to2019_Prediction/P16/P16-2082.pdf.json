{"title": [{"text": "Incorporating Relational Knowledge into Word Representations using Subspace Regularization", "labels": [], "entities": [{"text": "Incorporating Relational Knowledge into Word Representations", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7001664141813914}, {"text": "Subspace Regularization", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.6462206989526749}]}], "abstractContent": [{"text": "Incorporating lexical knowledge from semantic resources (e.g., WordNet) has been shown to improve the quality of distributed word representations.", "labels": [], "entities": []}, {"text": "This knowledge often comes in the form of rela-tional triplets (x, r, y) where words x and y are connected by a relation type r.", "labels": [], "entities": []}, {"text": "Existing methods either ignore the relation types, essentially treating the word pairs as generic related words, or employ rather restrictive assumptions to model the rela-tional knowledge.", "labels": [], "entities": []}, {"text": "We propose a novel approach to model relational knowledge based on low-rank subspace regulariza-tion, and conduct experiments on standard tasks to evaluate its effectiveness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed word representations, also known as word embeddings, are low-dimensional vector representations for words that capture semantic aspects ().", "labels": [], "entities": []}, {"text": "The algorithms for learning the word embeddings rely on distributional hypothesis) that words occurring in similar contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "Word embeddings have been shown to capture interesting linguistic regularities by simple vector arithmetic (e.g., v(king)-v(man)+v(woman)\u2248 v(queen)) ().", "labels": [], "entities": []}, {"text": "They have also been used to derive downstream features for various NLP tasks, such as named entity recognition, chunking, dependency parsing, sentiment analysis, paraphrase detection and machine translation.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.6371921201546987}, {"text": "dependency parsing", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7647998929023743}, {"text": "sentiment analysis", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.896530419588089}, {"text": "paraphrase detection", "start_pos": 162, "end_pos": 182, "type": "TASK", "confidence": 0.8701856136322021}, {"text": "machine translation", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7978854477405548}]}, {"text": "Their promise as semantic word representations has led to increasing research efforts on improving their quality.", "labels": [], "entities": []}, {"text": "To this end, researchers have attempted to incorporate lexical knowledge into word embeddings by using additional regularization or loss terms in the learning objective.", "labels": [], "entities": []}, {"text": "This lexical knowledge is often available in the form of triplets {(w i , r, w j )}, where the words w i and w j are connected by relation type r.", "labels": [], "entities": []}, {"text": "These methods can be broadly classified into two categories.", "labels": [], "entities": []}, {"text": "First family of methods use a (over-)generalized notion of similarity between words and ignore the type of relations, essentially treating the two words as generic similar words (.", "labels": [], "entities": []}, {"text": "This places an implicit restriction on the types of relations that can be used with these methods.", "labels": [], "entities": []}, {"text": "Second family of methods model each relation type by a distinct operator.", "labels": [], "entities": []}, {"text": "assumed a distinct relation vector r for every relation and minimize the distance between the translated first word and the second word, i.e., d(w i + r, w j ) for every triplet (w i , r, w j ).", "labels": [], "entities": []}, {"text": "proposed a neural tensor network which uses a distinct tensor operator for every relation.", "labels": [], "entities": []}, {"text": "These methods were used to learn entity and relation embeddings from a large collection of relation triplets for the task of knowledge base completion.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6341846187909445}]}, {"text": "Since these methods did not use any co-occurrence information from a text corpus, all entities were required to appear at least once in the training data, ruling out generalization to unseen entities . More recently, combined the training objective of SKIP-GRAM () with the training objective of) to incorporate lexical knowledge into word embeddings.", "labels": [], "entities": []}, {"text": "combine the training objective of) with that of neural language model) using alternating direction method of multipliers).", "labels": [], "entities": []}, {"text": "Constant translation model () (referred as CTM from now on), although an important step in modeling relational knowledge, makes a rather restrictive assumption requiring all triplets (w i , r, w j ) pertaining to a relation type r to satisfy w i + r \u2248 w j , \u2200(i, j).", "labels": [], "entities": []}, {"text": "This restriction can be severe when learning from a large text corpus since vector representation of a word also needs to respect a huge set of co-occurrence instances with other words.", "labels": [], "entities": []}, {"text": "CTM is also not suitable for (i) modeling symmetric relations (e.g., synonyms, antonyms), and (ii) modeling transitive relations (e.g., synonyms, hypernyms).", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel formulation for modeling the relational knowledge which addresses these issues by relaxing the constant translation assumption and modeling each relation by a low-rank subspace, i.e., all the word pairs pertaining to a relation are assumed to lie in a low-rank subspace.", "labels": [], "entities": []}, {"text": "We demonstrate effectiveness of the learned word representations on the tasks of knowledge-base completion and word analogy.", "labels": [], "entities": [{"text": "knowledge-base completion", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.7073181867599487}, {"text": "word analogy", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.787757933139801}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: WordRep data: Accuracy on knowledge- base completion", "labels": [], "entities": [{"text": "WordRep data", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.935387134552002}, {"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9974150657653809}]}, {"text": " Table 2: Google analogy data: Accuracy on word  analogy task", "labels": [], "entities": [{"text": "Google analogy data", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.6974108517169952}, {"text": "Accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9946827292442322}, {"text": "word  analogy", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7488783299922943}]}]}