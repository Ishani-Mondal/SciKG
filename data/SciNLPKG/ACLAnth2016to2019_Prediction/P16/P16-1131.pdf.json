{"title": [{"text": "Probabilistic Graph-based Dependency Parsing with Convolutional Neural Network", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.6327150017023087}]}], "abstractContent": [{"text": "This paper presents neural probabilistic parsing models which explore up to third-order graph-based parsing with maximum likelihood training criteria.", "labels": [], "entities": [{"text": "neural probabilistic parsing", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6280095974604288}]}, {"text": "Two neural network extensions are exploited for performance improvement.", "labels": [], "entities": []}, {"text": "Firstly, a convo-lutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured.", "labels": [], "entities": []}, {"text": "Secondly, a linear layer is added to integrate different order neu-ral models and trained with perceptron method.", "labels": [], "entities": []}, {"text": "The proposed parsers are evaluated on English and Chinese Penn Tree-banks and obtain competitive accuracies.", "labels": [], "entities": [{"text": "English and Chinese Penn Tree-banks", "start_pos": 38, "end_pos": 73, "type": "DATASET", "confidence": 0.8110726118087769}]}], "introductionContent": [{"text": "Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.971465528011322}]}, {"text": "In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 3, "end_pos": 38, "type": "TASK", "confidence": 0.6018673876921335}]}, {"text": "While for graphbased parsing, on which we focus in this work, also show the effectiveness of neural methods.", "labels": [], "entities": [{"text": "graphbased parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.651268407702446}]}, {"text": "The graph-based parser generally consists of two components: one is the parsing algorithm for inference or searching the most likely parse tree, the other is the parameter estimation approach for the machine learning models.", "labels": [], "entities": []}, {"text": "For the former, classical dynamic programming algorithms are usually adopted, while for the latter, there are various solutions.", "labels": [], "entities": []}, {"text": "Like some previous neural methods (, to tackle the structure prediction problems, utilize a max-margin training criterion, which does not include probabilistic explanations.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.742141842842102}]}, {"text": "Re-visiting the traditional probabilistic criteria in log-linear models, this work utilizes maximum likelihood for neural network training.", "labels": [], "entities": []}, {"text": "adopt this method for constituency parsing, which scores the anchored rules with neural models and formalizes the probabilities with tree-structured random fields.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9020594954490662}]}, {"text": "Motivated by this work, we utilize the probabilistic treatment for dependency parsing: scoring the edges or high-order sub-trees with a neural model and calculating the gradients according to probabilistic criteria.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8379097878932953}]}, {"text": "Although scores are computed by a neural network, the existing dynamic programming algorithms for gradient calculation remain the same as those in log-linear models.", "labels": [], "entities": []}, {"text": "Graph-based methods search globally through the whole space for trees and get the highestscored one, however, the scores for the sub-trees are usually locally decided, considering only surrounding words within a limited-sized window.", "labels": [], "entities": []}, {"text": "Convolutional neural network (CNN) provides a natural way to model a whole sentence.", "labels": [], "entities": []}, {"text": "By introducing a distance-aware convolutional layer, sentence-level representation can be exploited for parsing.", "labels": [], "entities": []}, {"text": "We will especially verify the effectiveness of such representation incorporated with window-based representation.", "labels": [], "entities": []}, {"text": "Graph-based parsing has a natural extension through raising its order and higher-order parsers usually perform better.", "labels": [], "entities": [{"text": "Graph-based parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6188196837902069}]}, {"text": "In previous work on highorder graph-parsing, the scores of high-order subtrees usually include the lower-order parts in their high-order factorizations.", "labels": [], "entities": []}, {"text": "In traditional linear models, combining scores can be implemented by including low-order features.", "labels": [], "entities": []}, {"text": "However, for neural models, this is not that straightforward because of nonlinearity.", "labels": [], "entities": []}, {"text": "A straightforward strategy is simply adding up all the scores, which in fact works well; another way is stacking a linear layer on the top of the representation from various already-trained neural parsing models of different orders.", "labels": [], "entities": []}, {"text": "This paper presents neural probabilistic models for graph-based projective dependency parsing, and explores up to third-order models.", "labels": [], "entities": [{"text": "projective dependency parsing", "start_pos": 64, "end_pos": 93, "type": "TASK", "confidence": 0.6965353091557821}]}, {"text": "Here are the three highlights of the proposed methods: \u2022 Probabilistic criteria for neural network training.", "labels": [], "entities": [{"text": "neural network training", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.6795234481493632}]}, {"text": "(Section 2.2) \u2022 Sentence-level representation learned from a convolutional layer.", "labels": [], "entities": [{"text": "Sentence-level representation", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.892841637134552}]}, {"text": "(Section 3.2) \u2022 Ensemble models with a stacked linear output layer.", "labels": [], "entities": []}, {"text": "(Section 3.3) Our main contribution is exploring sub-tree scoring models which combine local features with a window-based neural network and global features from a distance-aware convolutional neural network.", "labels": [], "entities": []}, {"text": "A free distribution of our implementation is publicly available 1 . The remainder of the paper is organized as follows: Section 2 explains the probabilistic model for graph-based parsing, Section 3 describes our neural network models, Section 4 presents our experiments and Section 5 discusses related work, we summarize this paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed parsers are evaluated on English Penn Treebank (PTB) and Chinese Penn Treebank (CTB).", "labels": [], "entities": [{"text": "English Penn Treebank (PTB)", "start_pos": 38, "end_pos": 65, "type": "DATASET", "confidence": 0.9339767197767893}, {"text": "Chinese Penn Treebank (CTB)", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.9232612550258636}]}, {"text": "Unlabeled attachment scores (UAS), labeled attachment scores (LAS) and unlabeled complete matches (CM) are the metrics.", "labels": [], "entities": [{"text": "Unlabeled attachment scores (UAS)", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.7436575144529343}, {"text": "labeled attachment scores (LAS)", "start_pos": 35, "end_pos": 66, "type": "METRIC", "confidence": 0.822455421090126}, {"text": "unlabeled complete matches (CM)", "start_pos": 71, "end_pos": 102, "type": "METRIC", "confidence": 0.7096761365731558}]}, {"text": "Punctuations 2 are ignored as in previous work.", "labels": [], "entities": [{"text": "Punctuations", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.962264358997345}]}, {"text": "For English, we follow the splitting convention for PTB3: sections 2-21 for training, 22 for developing and 23 for test.", "labels": [], "entities": [{"text": "PTB3", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8772855401039124}]}, {"text": "We prepare three datasets of PTB, using different conversion tools: (1) Penn2Malt and the head rules of, noted as PTB-Y&M; (2) dependency converter in Stanford parser v3.3.0 with Stanford Basic Dependencies), noted as PTB-SD; (3) LTH Constituentto-Dependency Conversion Tool 4, noted as PTB-LTH.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9740145802497864}, {"text": "PTB-Y&M", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9325254956881205}, {"text": "PTB-SD", "start_pos": 218, "end_pos": 224, "type": "DATASET", "confidence": 0.9459077715873718}, {"text": "PTB-LTH", "start_pos": 287, "end_pos": 294, "type": "DATASET", "confidence": 0.9554390907287598}]}, {"text": "We use Stanford POS tagger () to get predicted POS tags for development and test sets, and the accuracies for their tags are 97.2% and 97.4%, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9855446219444275}]}, {"text": "For Chinese, we adopt the splitting convention for CTB5 described in.", "labels": [], "entities": []}, {"text": "The dependencies (noted as CTB), are converted with the Penn2Malt converter.", "labels": [], "entities": [{"text": "Penn2Malt converter", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.9676461815834045}]}, {"text": "Gold segmentation and POS tags are used as in previous work.", "labels": [], "entities": [{"text": "Gold segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6697405874729156}]}], "tableCaptions": [{"text": " Table 1: Effects of the components, on PTB-SD  development set.", "labels": [], "entities": [{"text": "PTB-SD  development set", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.9453303019205729}]}, {"text": " Table 2: Comparisons of results on the test sets.", "labels": [], "entities": []}]}