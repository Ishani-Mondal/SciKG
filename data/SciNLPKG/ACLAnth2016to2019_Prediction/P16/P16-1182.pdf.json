{"title": [{"text": "MUTT: Metric Unit TesTing for Language Generation Tasks", "labels": [], "entities": [{"text": "MUTT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.630864679813385}, {"text": "Language Generation Tasks", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7814822693665823}]}], "abstractContent": [{"text": "Precise evaluation metrics are important for assessing progress in high-level language generation tasks such as machine translation or image captioning.", "labels": [], "entities": [{"text": "machine translation or image captioning", "start_pos": 112, "end_pos": 151, "type": "TASK", "confidence": 0.6831257462501525}]}, {"text": "Historically , these metrics have been evaluated using correlation with human judgment.", "labels": [], "entities": []}, {"text": "However, human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness.", "labels": [], "entities": []}, {"text": "In this paper, we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations (e.g. active to passive voice) have on the automatic metric scores.", "labels": [], "entities": [{"text": "metric evaluation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.9599389433860779}]}, {"text": "These sentence \"corruptions\" serve as unit tests for precisely measuring the strengths and weaknesses of a given metric.", "labels": [], "entities": []}, {"text": "We find that not only are human annotations heavily inconsistent in this study, but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics (e.g. comparing passive and active sentences) better than a simple correlation with human judgment can.", "labels": [], "entities": []}], "introductionContent": [{"text": "The success of high-level language generation tasks such as machine translation (MT), paraphrasing and image/video captioning depends on the existence of reliable and precise automatic evaluation metrics.: A few select entries from the SICK dataset.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8369542896747589}, {"text": "image/video captioning", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.6087499409914017}, {"text": "SICK dataset", "start_pos": 236, "end_pos": 248, "type": "DATASET", "confidence": 0.8225861489772797}]}, {"text": "All of these entries follow the same \"Negated Subject\" transformation between sentence 1 and sentence 2, yet humans annotated them with an inconsistently wide range of scores (from 1 to 5).", "labels": [], "entities": []}, {"text": "Regardless of whether the gold labels for this particular transformation should score this high or low, they should score be scored consistently.", "labels": [], "entities": []}, {"text": "Efforts have been made to create standard metrics () to help advance the state-of-the-art.", "labels": [], "entities": []}, {"text": "However, most such popular metrics, despite their wide use, have serious deficiencies.", "labels": [], "entities": []}, {"text": "Many rely on ngram matching and assume that annotators generate all reasonable reference sentences, which is infeasible for many tasks.", "labels": [], "entities": [{"text": "ngram matching", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.6607852280139923}]}, {"text": "Furthermore, metrics designed for one task, e.g., MT, can be a poor fit for other tasks, e.g., video captioning.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.901127815246582}, {"text": "video captioning", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7057050317525864}]}, {"text": "To design better metrics, we need a principled approach to evaluating their performance.", "labels": [], "entities": []}, {"text": "Historically, MT metrics have been evaluated by how well they correlate with human annotations).", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9911596179008484}]}, {"text": "However, as we demonstrate in Sec.", "labels": [], "entities": []}, {"text": "5, human judgment can result in inconsistent scoring.", "labels": [], "entities": []}, {"text": "This presents a serious problem for determining whether a metric is \"good\" based on correlation with inconsistent human scores.", "labels": [], "entities": []}, {"text": "When \"gold\" target data is unreliable, even good metrics can appear to be inaccurate.", "labels": [], "entities": []}, {"text": "Furthermore, correlation of system output with human-derived scores typically provides an overall score but fails to isolate specific errors that metrics tend to miss.", "labels": [], "entities": []}, {"text": "This makes it difficult to discover system-specific weaknesses to improve their performance.", "labels": [], "entities": []}, {"text": "For instance, an ngram-based metric might effectively detect non-fluent, syntactic errors, but could also be fooled by legitimate paraphrases whose ngrams simply did not appear in the training set.", "labels": [], "entities": []}, {"text": "Although there has been some recent work on paraphrasing that provided detailed error analysis of system outputs), more often than not such investigations are seen as above-and-beyond when assessing metrics.", "labels": [], "entities": []}, {"text": "The goal of this paper is to propose a process for consistent and informative automated analysis of evaluation metrics.", "labels": [], "entities": []}, {"text": "This method is demonstrably more consistent and interpretable than correlation with human annotations.", "labels": [], "entities": []}, {"text": "In addition, we extend the SICK dataset to include un-scored fluency-focused sentence comparisons and we propose a toy metric for evaluation.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.7658931016921997}]}, {"text": "The rest of the paper is as follows: Section 2 introduces the corruption-based metric unit testing process, Section 3 lists the existing metrics we use in our experiments as well as the toy metric we propose, Section 4 describes the SICK dataset we used for our experiments, Section 5 motivates the need for corruption-based evaluation instead of correlation with human judgment, Section 6 describes the experimental procedure for analyzing the metric unit tests, Section 7 analyzes the results of our experiments, and in Section 8 we offer concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our Metric Unit TesTing experiments, we wanted to measure the fraction of times that a given metric is able to appropriately handle a particular corruption type.", "labels": [], "entities": []}, {"text": "Each (original,corruption) pair is considered atrial, which the metric either gets corrector   incorrect.", "labels": [], "entities": []}, {"text": "We report the percent of successful trials for each metric in, and 6.", "labels": [], "entities": []}, {"text": "Experiments were run using 5, 10, and 20 reference sentences to understand which metrics are able perform well without much data and also which metrics are able to effectively use more data to improve.", "labels": [], "entities": []}, {"text": "An accuracy of 75% would indicate that the metric is able to assign appropriate scores 3 out of 4 times.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9995879530906677}]}, {"text": "For Meaning-altering and Fleuncy-disrupting corruptions, the corrupted sentence will be truly different from the original and reference sentences.", "labels": [], "entities": [{"text": "Fleuncy-disrupting", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.9039387106895447}]}, {"text": "A trial would be successful when the score of the original s orig is rated higher than the score of the corruption s corr : s orig > s corr Alternatively, Meaning-preserving transformations create a \"corruption\" sentence which is just as correct as the original.", "labels": [], "entities": []}, {"text": "To reflect this, we consider atrial to be successful when the score of the corruption s corr is within 15% of the score of the original s orig : where is a small constant (10 \u22129 ) to prevent division by zero.", "labels": [], "entities": []}, {"text": "We refer to this alternative trial formulation as the Difference formula.", "labels": [], "entities": []}, {"text": "Our code is made available at https://github.", "labels": [], "entities": []}, {"text": "com/text-machine-lab/MUTT: Results for the Active-to-Passive corruption (using Difference formula scores).", "labels": [], "entities": [{"text": "MUTT", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8160377144813538}]}], "tableCaptions": [{"text": " Table 3: Pairwise correlation between the predictions of", "labels": [], "entities": []}, {"text": " Table 4: Meaning-altering corruptions. These % accuracies represent the number of times that a given  metric was able to correctly score the original sentence higher than the corrupted sentence. Numbers refer- enced in the prose analysis are highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 5: Meaning-preserving corruptions. These % accuracies represent the number of times that a given  metric was able to correctly score the semantically-equaivalent \"corrupted\" sentence within 15% of the  original sentence. Numbers referenced in the prose analysis are highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 6: Fluency-disrupting corruptions. These % accuracies represent the number of times that a given  metric was able to correctly score the original sentence higher than the corrupted sentence. Numbers refer- enced in the prose analysis are highlighted in bold.", "labels": [], "entities": []}]}