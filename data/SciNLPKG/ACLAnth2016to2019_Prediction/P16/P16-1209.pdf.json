{"title": [{"text": "Recurrent neural network models for disease name recognition using domain invariant features", "labels": [], "entities": [{"text": "Recurrent neural network", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8808378974596659}, {"text": "disease name recognition", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7684029340744019}]}], "abstractContent": [{"text": "Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems.", "labels": [], "entities": [{"text": "disease name recognition", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.6749758025010427}]}, {"text": "Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence.", "labels": [], "entities": []}, {"text": "In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task.", "labels": [], "entities": [{"text": "disease name recognition task", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.7829789519309998}]}, {"text": "In particular , we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories.", "labels": [], "entities": [{"text": "disease name recognition", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.65914253393809}]}, {"text": "We also utilize convolution neu-ral network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model.", "labels": [], "entities": []}, {"text": "We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset.", "labels": [], "entities": [{"text": "NCBI disease dataset", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.9020240704218546}]}, {"text": "Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering.", "labels": [], "entities": [{"text": "disease mention recognition task", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.727807529270649}]}, {"text": "Further the proposed models obtained improved performance on the classification task of disease names.", "labels": [], "entities": [{"text": "classification task of disease names", "start_pos": 65, "end_pos": 101, "type": "TASK", "confidence": 0.856558096408844}]}], "introductionContent": [{"text": "Automatic recognition of disease names in biomedical and clinical texts is of utmost importance for development of more sophisticated NLP systems such as information extraction, question answering, text summarization and soon ().", "labels": [], "entities": [{"text": "Automatic recognition of disease names in biomedical and clinical texts", "start_pos": 0, "end_pos": 71, "type": "TASK", "confidence": 0.8051322132349015}, {"text": "information extraction", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.823819488286972}, {"text": "question answering", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.911384642124176}, {"text": "text summarization", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.7335047125816345}]}, {"text": "Complicate and inconsistent terminologies, ambiguities caused by use of abbreviations and acronyms, new disease names, multiple names (possibly of varying number of words) for the same disease, complicated syntactic structure referring to multiple related names or entities are some of the major reasons for making automatic identification of the task difficult and challenging (.", "labels": [], "entities": []}, {"text": "State-ofthe-art disease name recognition systems) depends on user defined features which in turn try to capture context keeping in mind above mentioned challenges.", "labels": [], "entities": [{"text": "name recognition", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8423052430152893}]}, {"text": "Feature engineering not only requires linguistic as well as domain insight but also is time consuming and is corpus dependent.", "labels": [], "entities": [{"text": "Feature engineering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8672957420349121}]}, {"text": "Recently window based neural network approach of) got lot of attention in different sequence tagging tasks in NLP.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7004021108150482}]}, {"text": "It gave state-of-art results in many sequence labeling problems without using many hand designed or manually engineered features.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7709538042545319}]}, {"text": "One major drawback of this approach is its inability to capture features from outside window.", "labels": [], "entities": []}, {"text": "Consider a sentence \"Given that the skin of these adult mice also exhibits signs of de novo hair-follicle morphogenesis, we wondered whether human pilomatricomas might originate from hair matrix cells and whether they might possess beta-catenin-stabilizing mutations\" (taken verbatim from PMID: 10192393), words such as signs and originate appearing both sides of the word \"pilomatricomas\", play important role in deciding it is a disease.", "labels": [], "entities": [{"text": "PMID", "start_pos": 289, "end_pos": 293, "type": "METRIC", "confidence": 0.958254873752594}]}, {"text": "Any model relying on features defined based on words occurring within a fixed window of neighboring words will fail to capture information of influential words occurring outside this window.", "labels": [], "entities": []}, {"text": "Our motivation can be summarized in the following question: can we identify disease name and categorize them without relying on feature en-gineering, domain-knowledge or task specific resources?", "labels": [], "entities": []}, {"text": "In other words, we can say this work is motivated towards mitigating the two issues: first, feature engineering relying on linguistic and domain-specific knowledge; and second, bring flexibility in capturing influential words affecting model decisions irrespective of their occurrence anywhere within the sentence.", "labels": [], "entities": []}, {"text": "For the first, we used character-based embedding (likely to capture orthographic and morphological features) as well as word embedding (likely to capture lexicosemantic features) as features of the neural network models.", "labels": [], "entities": []}, {"text": "For the second issue, we explore various recurrent neural network (RNN) architectures for their ability to capture long distance contexts.", "labels": [], "entities": []}, {"text": "We experiment with bidirectional RNN (Bi-RNN), bidirectional long short term memory network (Bi-LSTM) and bidirectional gated recurrent unit (Bi-GRU).", "labels": [], "entities": []}, {"text": "In each of these models we used sentence level log likelihood approach at the top layer of the neural architecture.", "labels": [], "entities": []}, {"text": "The main contributions of the work can be summarized as follows", "labels": [], "entities": []}], "datasetContent": [{"text": "We used NCBI dataset (Do\u02d8 gan and Lu, 2012), the most comprehensive publicly available dataset annotated with disease mentions, in this work.", "labels": [], "entities": [{"text": "NCBI dataset (Do\u02d8 gan and Lu, 2012)", "start_pos": 8, "end_pos": 43, "type": "DATASET", "confidence": 0.7864716621962461}]}, {"text": "NCBI dataset has been manually annotated by a group of medical practitioners for identifying diseases and their types in biomedical articles.", "labels": [], "entities": [{"text": "NCBI dataset", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9756567776203156}]}, {"text": "All disease mentions were categorized into four different categories, namely, specific disease, disease class, composite disease and modifier.", "labels": [], "entities": []}, {"text": "A word is annotated as specific disease, if it indicates a particular disease.", "labels": [], "entities": []}, {"text": "Disease class category indicates a word describing a family of many specific diseases, such as autoimmune disorder.", "labels": [], "entities": []}, {"text": "A string signifying two or more different disease mentions is annotated with composite mention.", "labels": [], "entities": []}, {"text": "Modifier category indicates disease mention has been used as modifiers for other concepts.", "labels": [], "entities": []}, {"text": "This dataset is a extension of the AZDC dataset () which was annotated with disease mentions only and not with their categories.", "labels": [], "entities": [{"text": "AZDC dataset", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8953429162502289}]}, {"text": "Statistics of the dataset is mentioned in the: Performance of various models using 25 dimensional CE features, A:Disease name recognition, B: Disease classification task disease types are flattened into a single category and, the B: disease class recognition, where we need to decide exact categories of disease mentions.", "labels": [], "entities": [{"text": "Disease name recognition", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.6553625464439392}, {"text": "Disease classification task disease types", "start_pos": 142, "end_pos": 183, "type": "TASK", "confidence": 0.8265858173370362}, {"text": "disease class recognition", "start_pos": 233, "end_pos": 258, "type": "TASK", "confidence": 0.6403002639611562}]}, {"text": "It is noteworthy to mention that the Task B is more challenging as it requires model to capture semantic contexts to put disease mentions into appropriate categories.", "labels": [], "entities": []}, {"text": "Next we investigated the results obtained by the various models using only 50 dim word embedding features.", "labels": [], "entities": []}, {"text": "The first part of table 4 shows the results obtained by different RNNs and the window based neural network (NN).", "labels": [], "entities": []}, {"text": "In this case RNN models are giving better results than the NN model for both the tasks.", "labels": [], "entities": []}, {"text": "In particular performance of Bi-LSTM models are best than others in both the tasks.", "labels": [], "entities": []}, {"text": "We observe that for the task A, RNN models obtained 1.2% to 3% improvement in F1-score than the baseline NN performance.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995001554489136}]}, {"text": "Similarly 2.55% to 4% improvement in F1-score are observed for the task B, with Bi-LSTM model obtaining more than 4% improvement.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.999466598033905}, {"text": "Bi-LSTM", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9348088502883911}]}, {"text": "In second part of this table we compare the results obtained by various models using the features set obtained by combining the two feature sets.", "labels": [], "entities": []}, {"text": "If we look at performance of individual model using three different set of features, model using only word embedding features seems to give consistently best performance.", "labels": [], "entities": []}, {"text": "Among all models, Bi-LSTM using word embedding features obtained best F1-scores of 79.13% and 63.16% for the tasks A and B respectively.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9989155530929565}]}], "tableCaptions": [{"text": " Table 2: Dataset statistics. spe. dis. : specific  disease and comp. men.: composite mention", "labels": [], "entities": [{"text": "Dataset statistics. spe. dis.", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.8838052153587341}]}, {"text": " Table 3: Performance of various models using 25 dimensional CE features, A:Disease name recognition,  B: Disease classification task", "labels": [], "entities": [{"text": "Disease name recognition", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.6756369471549988}, {"text": "Disease classification", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7626869082450867}]}, {"text": " Table 4: Performance of various models using 50 dimensional WE features. A:Disease name recognition,  B: Disease classification task", "labels": [], "entities": [{"text": "Disease name recognition", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.734348992506663}, {"text": "Disease classification", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.7692190408706665}]}, {"text": " Table 5: Performance of different models with  50 dim embedded vectors in Task A validation  set when word vectors are not getting updated  while training", "labels": [], "entities": []}, {"text": " Table 7: Comparisons of our best model results and state-of-art results. SM-SVM :Soft Margin Support  Vector Machine", "labels": [], "entities": []}]}