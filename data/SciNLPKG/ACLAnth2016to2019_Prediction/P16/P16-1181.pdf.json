{"title": [{"text": "How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents", "labels": [], "entities": [{"text": "Joint Sentence Boundary Detection", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.6187319383025169}]}], "abstractContent": [{"text": "We cast sentence boundary detection and syntactic parsing as a joint problem, so an entire text document forms a training instance for transition-based dependency parsing.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.6938204367955526}, {"text": "syntactic parsing", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7311694920063019}, {"text": "transition-based dependency parsing", "start_pos": 135, "end_pos": 170, "type": "TASK", "confidence": 0.62998299797376}]}, {"text": "When trained with an early update or max-violation strategy for inexact search, we observe that only a tiny part of these very long training instances is ever exploited.", "labels": [], "entities": []}, {"text": "We demonstrate this effect by extending the ArcStandard transition system with swap for the joint prediction task.", "labels": [], "entities": []}, {"text": "When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation.", "labels": [], "entities": []}, {"text": "A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.7016106247901917}]}], "introductionContent": [{"text": "Although punctuation mostly provides reliable cues for segmenting longer texts into sentence units, human readers are able to exploit their understanding of the syntactic and semantic structure to (re-)segment input in the absence of such cues.", "labels": [], "entities": []}, {"text": "When working with carefully copy-edited text documents, sentence boundary detection can be viewed as a minor preprocessing task in Natural Language Processing, solvable with very high accuracy.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7608811855316162}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9867503046989441}]}, {"text": "However, when dealing with the output of automatic speech recognition or \"noisier\" texts such as blogs and emails, non-trivial sentence segmentation issues do occur., for example, show how much impact fully automatic preprocessing can have on parsing quality for well-edited and less-edited text.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7407296001911163}, {"text": "sentence segmentation", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.7428668737411499}]}, {"text": "Two possible strategies to approach this problem are (i) to exploit other cues for sentence boundaries, such as prosodic phrasing and intonation in speech (e.g.,) or formatting cues in text documents (, and (ii) to emulate the human ability to exploit syntactic competence for segmentation.", "labels": [], "entities": [{"text": "formatting cues in text documents", "start_pos": 166, "end_pos": 199, "type": "TASK", "confidence": 0.831330406665802}]}, {"text": "We focus hereon the latter, which has received little attention, and propose to cast sentence boundary detection and syntactic (dependency) parsing as a joint problem, such that segmentations that would give rise to suboptimal syntactic structures can be discarded early on.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.6407626867294312}, {"text": "syntactic (dependency) parsing", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.6354725360870361}]}, {"text": "A joint model for parsing and sentence boundary detection by definition operates on documents rather than single sentences, as is the standard case for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9785319566726685}, {"text": "sentence boundary detection", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6469482084115347}]}, {"text": "The task is illustrated in, which shows the beginning of a document in the Switchboard corpus, a collection of transcribed telephone dialogues.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.8724015951156616}]}, {"text": "The parser must predict the syntactic structure of the three sentences as well as the start points of each sentence.", "labels": [], "entities": []}, {"text": "The simple fact that documents are considerably longer than sentences, often by orders of magnitude, creates some interesting challenges fora joint system.", "labels": [], "entities": []}, {"text": "First of all, the decoder needs to handle long inputs efficiently.", "labels": [], "entities": []}, {"text": "This problem is easily solved by using transition-based decoders, which excel in this kind of setting due to their incremental approach and their low theoretical complexity.", "labels": [], "entities": []}, {"text": "Specifically, we use a transition-based decoder that extends the Swap transition system of in order to introduce sentence boundaries during the parsing process.", "labels": [], "entities": []}, {"text": "The parser performs inexact search for the optimal structure by you said you have four cats i have four cats how old are they . .", "labels": [], "entities": []}, {"text": "The beginning of a sample document from the Switchboard corpus.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.8349387645721436}]}, {"text": "Tokens that start a sentence are underlined.", "labels": [], "entities": []}, {"text": "The task is to predict syntactic structure and sentence boundaries jointly.", "labels": [], "entities": []}, {"text": "maintaining abeam of several candidate derivations throughout the parsing process.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.9134774804115295}]}, {"text": "We will show in this paper that, besides efficient decoding, a second, equally significant challenge lies in the way such a parser is trained.", "labels": [], "entities": []}, {"text": "Normally, beam-search transition-based parsers are trained with structured perceptrons using either early update) or max-violation updates).", "labels": [], "entities": []}, {"text": "Yet our analysis demonstrates that neither of these update strategies is appropriate for training on very long input sequences as they discard a large portion of the training data.", "labels": [], "entities": []}, {"text": "2 A significant part of the training data is therefore never used to train the model.", "labels": [], "entities": []}, {"text": "As a remedy to this problem, we instead use an adaptation of the update strategy in.", "labels": [], "entities": []}, {"text": "They apply early update in a coreference resolution system and observe that the task is inherently so difficult that the correct item practically never stays in the beam.", "labels": [], "entities": [{"text": "early update", "start_pos": 11, "end_pos": 23, "type": "METRIC", "confidence": 0.915439635515213}, {"text": "coreference resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9072661697864532}]}, {"text": "So early updates are unable to exploit the full instances during training.", "labels": [], "entities": []}, {"text": "They propose to apply the updates iteratively on the same document until the full document has been observed.", "labels": [], "entities": []}, {"text": "In our case, i.e. when parsing entire documents, the problem is similar in that early updates do not reach the point where the learning algorithm exploits the full training data within reasonable time.", "labels": [], "entities": [{"text": "parsing entire documents", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8825031121571859}]}, {"text": "Training instead with the iterative update strategy gives us significantly better models in substantially less training time.", "labels": [], "entities": []}, {"text": "The second contribution in this paper is to demonstrate empirically that syntactic information can makeup to a large extent for missing or unreliable cues from punctuation.", "labels": [], "entities": []}, {"text": "The joint system implements this hypothesis and allows us to test the influence of syntactic information on the pre-diction of sentence boundaries as compared to a pipeline baseline where both tasks are performed independently of each other.", "labels": [], "entities": []}, {"text": "For our analysis, we use the Wall Street Journal as the standard benchmark set and as a representative for copyedited text.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9629290501276652}]}, {"text": "We also use the Switchboard corpus of transcribed dialogues as a representative for data where punctuation cannot give clues to a sentence boundary predictor (other types of data that may show this property to varying degrees are web content data, e.g. forum posts or chat protocols, or (especially historical) manuscripts).", "labels": [], "entities": [{"text": "Switchboard corpus of transcribed dialogues", "start_pos": 16, "end_pos": 59, "type": "DATASET", "confidence": 0.8879042506217957}]}, {"text": "While the Switchboard corpus gives us a realistic scenario fora setting with unreliable punctuation, the syntactic complexity of telephone conversations is rather low compared to the Wall Street Journal.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.8304933905601501}, {"text": "Wall Street Journal", "start_pos": 183, "end_pos": 202, "type": "DATASET", "confidence": 0.9251541892687479}]}, {"text": "Therefore, as a controlled experiment for assessing how far syntactic competence alone can take us if we stop trusting punctuation and capitalization entirely, we perform joint sentence boundary detection/parsing on a lower-cased, nopunctuation version of the Wall Street Journal.", "labels": [], "entities": [{"text": "sentence boundary detection/parsing", "start_pos": 177, "end_pos": 212, "type": "TASK", "confidence": 0.7794692754745484}, {"text": "nopunctuation version of the Wall Street Journal", "start_pos": 231, "end_pos": 279, "type": "DATASET", "confidence": 0.7942676544189453}]}, {"text": "In this setting, where the parser must rely on syntactic information alone to predict sentence boundaries, syntactic information makes a difference of 10 percentage point absolute for the sentence boundary detection task, and two points for labeled parsing accuracy.", "labels": [], "entities": [{"text": "sentence boundary detection task", "start_pos": 188, "end_pos": 220, "type": "TASK", "confidence": 0.701854296028614}, {"text": "labeled parsing", "start_pos": 241, "end_pos": 256, "type": "TASK", "confidence": 0.5601615309715271}, {"text": "accuracy", "start_pos": 257, "end_pos": 265, "type": "METRIC", "confidence": 0.8071590662002563}]}], "datasetContent": [{"text": "We experiment with two parts of the English Penn Treebank ().", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.9275406996409098}]}, {"text": "We use the Wall Street Journal (WSJ) as an example of copy-edited newspaper-quality texts with proper punctuation and capitalized sentences.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.956346720457077}]}, {"text": "We also use the Switchboard portion which consists of (transcribed) telephone conversations between strangers.", "labels": [], "entities": []}, {"text": "Following previous work on Switchboard we lowercase all text and remove punctuation and disfluency markups.", "labels": [], "entities": []}, {"text": "We use sections 2-21 of the WSJ for training, 24 as development set and 23 as test set.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.505083441734314}]}, {"text": "We convert both data sets to Stanford dependencies with the Stanford dependency converter (de).", "labels": [], "entities": []}, {"text": "We predict part-of-speech tags with the CRF tagger MARMOT and annotate the training sets via 10-fold jackknifing.", "labels": [], "entities": [{"text": "MARMOT", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.4104378819465637}]}, {"text": "Depending on the experimental sce-nario we use MARMOT in two different settingsstandard sentence-level where we train and apply it on sentences, and document-level where a whole document is fed to the tagger, implicitly treating it as a single very long sentence.", "labels": [], "entities": []}, {"text": "We work with two well-established sentence boundary detection baselines.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6842998464902242}]}, {"text": "Following) we use the tokenizer from the Stanford CoreNLP) and the sentence boundary detector from OpenNLP 5 which has been shown to achieve state-of-the-art results on WSJ.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.8968266248703003}, {"text": "OpenNLP 5", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.8952097296714783}, {"text": "WSJ", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.877621591091156}]}, {"text": "We evaluate the performance of sentence boundary detection on the token level using F-measure (F 1 ).", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6827523410320282}, {"text": "F-measure (F 1", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.7375940233469009}]}, {"text": "Typical sentence boundary detectors such as CORENLP or OPENNLP focus on punctuation marks and are therefore inapplicable to data like Switchboard that does not originally include punctuation.", "labels": [], "entities": []}, {"text": "In such cases CRF taggers are commonly selected as baselines, e.g. for punctuation prediction experiments ().", "labels": [], "entities": [{"text": "CRF taggers", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.8972248733043671}, {"text": "punctuation prediction experiments", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.8434722026189169}]}, {"text": "We therefore introduce a third baseline using MARMOT.", "labels": [], "entities": [{"text": "MARMOT", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.45646849274635315}]}, {"text": "For this, we augment the POS tags with information to indicate if a token starts anew sentence or not.", "labels": [], "entities": []}, {"text": "We prepare the training data accordingly and train the document-level sequence labeler on them.", "labels": [], "entities": []}, {"text": "shows the accuracies of all baseline systems on the development sets.", "labels": [], "entities": []}, {"text": "For WSJ all three algorithms achieve similar results which shows that MARMOT is a competitive baseline.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5084457993507385}, {"text": "MARMOT", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.7272982597351074}]}, {"text": "As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ.", "labels": [], "entities": [{"text": "predicting sentence boundaries", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.8952069481213888}, {"text": "Switchboard dataset", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.8927989602088928}, {"text": "WSJ", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.9464459419250488}]}, {"text": "Our parser implements the labeled version of the transition system described in Section 2 with a default beam size of 20.", "labels": [], "entities": []}, {"text": "We use the oracle by  to create transition sequences for each sentence of a document, and then concatenate them with SB transitions that occur as early as possible (cf. Section 2).", "labels": [], "entities": []}, {"text": "The feature set is based on previous work ( and was developed fora sentence-based parser for the WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9466143846511841}]}, {"text": "We made initial experiments trying to introduce new features aimed at capturing sentence boundaries such as trying to model verb subcategorization or sentence length, however none of these proved useful compared to the baseline feature set.", "labels": [], "entities": []}, {"text": "Following the line of work by Bohnet et al., we use the passiveaggressive algorithm) instead of the vanilla perceptron, parameter averaging, and a hash function to map features (Bohnet, 2010).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results (F 1 ) for baselines for sentence  boundary detection on dev sets.", "labels": [], "entities": [{"text": "sentence  boundary detection", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6898758808771769}]}, {"text": " Table 3: Sentence boundary detection results (F 1 )  on test sets.", "labels": [], "entities": [{"text": "Sentence boundary detection", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.874833345413208}, {"text": "F 1 )", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9518250425656637}]}, {"text": " Table 4: Parsing results (LAS) on test sets for dif- ferent sentence boundaries.", "labels": [], "entities": [{"text": "LAS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.7689141631126404}]}]}