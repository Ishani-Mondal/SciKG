{"title": [], "abstractContent": [{"text": "In this paper we explore the correlation between the sound of words and their meaning , by testing if the polarity ('good guy' or 'bad guy') of a character's role in a work of fiction can be predicted by the name of the character in the absence of any other context.", "labels": [], "entities": []}, {"text": "Our approach is based on phonological and other features proposed in prior theoretical studies of fictional names.", "labels": [], "entities": []}, {"text": "These features are used to construct a predictive model over a manually annotated corpus of characters from motion pictures.", "labels": [], "entities": []}, {"text": "By experimenting with different mixtures of features, we identify phonological features as being the most discriminative by comparison to social and other types of features, and we delve into a discussion of specific phonological and phonotactic indicators of a character's role's polarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Could it be possible for fictional characters' names such as 'Dr. No' and 'Hannibal Lecter' to be attributed to positive characters whereas names such as 'Jane Eyre' and 'Mary Poppins' to negative ones?", "labels": [], "entities": []}, {"text": "Could someone guess who is the hero and who is the competitor based only on the name of the character and what would be the factors that contribute to such intuition?", "labels": [], "entities": []}, {"text": "Literary theory suggests that it should be possible, because fictional character names function as expressions of experience, ethos, teleology, values, culture, ideology, and attitudes of the character.", "labels": [], "entities": []}, {"text": "However, work in literary theory, psychology, linguistics and philosophy has studied fictional names by analysing individual works or small clusters of closely related works, such as those of a particular author.", "labels": [], "entities": [{"text": "literary theory", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7256883084774017}]}, {"text": "By contrast, we apply tools from computational linguistics at a larger scale aiming to identify more general patterns that are not tied to any specific creator's idiosyncrasies and preferences; in the hope that extracting such patterns can provide valuable insights about how the sound of names and, more generally, words correlates with their meaning.", "labels": [], "entities": []}, {"text": "At the core of our approach is the idea that the names of fictional characters follow (possibly subconsciously) a perception of what a positive or a negative name ought to sound like that is shared between the creator and the audience.", "labels": [], "entities": []}, {"text": "Naturally the personal preferences or experiences of the creator might add noise, but fictional characters' names will at least not suffer (or suffer less) from the systematic cultural bias bound to exist in real persons' names.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first present the relevant background, including both theoretical work and computational work relevant to peoples' names (Section 2).", "labels": [], "entities": []}, {"text": "Based on this theoretical work, we then proceed to formulate a set of features that can be computationally extracted from names, and which we hypothesise to be discriminative enough to allow for the construction of a model that accurately predicts whether a character plays a positive or negative role in a work of fiction (Section 3).", "labels": [], "entities": []}, {"text": "In order to test this hypothesis, we constructed a corpus of characters from popular English-language motion pictures.", "labels": [], "entities": []}, {"text": "After describing corpus construction and presenting results (Section 4), we proceed to discuss these results (Section 5) and conclude (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental design consisted of an iterated approach performing experiments with different sets of features.", "labels": [], "entities": []}, {"text": "This process was driven by a preliminary chi-squared analysis in order to exploit feature significance.", "labels": [], "entities": []}, {"text": "The algorithms that are used for the experiments are Naive Bayes and J48   (Salzberg, 1994) decision trees.", "labels": [], "entities": [{"text": "J48   (Salzberg, 1994) decision trees", "start_pos": 69, "end_pos": 106, "type": "DATASET", "confidence": 0.6719619706273079}]}, {"text": "Each experiment is done using a 10-fold cross validation on the available data, using a confidence factor of 0.25 for post-pruning.", "labels": [], "entities": []}, {"text": "For all the experiments we used the Weka toolkit ().", "labels": [], "entities": [{"text": "Weka toolkit", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9705266952514648}]}, {"text": "Due to the imbalance of our dataset in favor of positive classes (see), we sub-sampled the dataset maintaining the initial genre distribution.", "labels": [], "entities": []}, {"text": "We also applied principal component analysis (PCA) in order to guarantee the independence of the classification features, as required by the Naive Bayes algorithm.", "labels": [], "entities": [{"text": "principal component analysis (PCA)", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.6802781869967779}]}, {"text": "To explore the behavior of the algorithms to the change of trained data we generated the learning curves shown in.", "labels": [], "entities": []}, {"text": "In both cases the learning curves are well-behaved since the error rate grows monotonically as the training set shrinks.", "labels": [], "entities": []}, {"text": "However, the precision, recall, and Fscores achieved by J48 are significantly better that those of Naive Bayes.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9998478889465332}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9998396635055542}, {"text": "Fscores", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9998332262039185}, {"text": "J48", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.842394232749939}]}, {"text": "This preliminary experiment led us to use J48 for the main experiment, where we try different features in order to understand which are the most discriminative ones.", "labels": [], "entities": []}, {"text": "These results are collected in and discussed immediately below.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: List of features", "labels": [], "entities": []}, {"text": " Table 2: Number of annotations per genre before  and after resampling", "labels": [], "entities": [{"text": "resampling", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.6421087980270386}]}, {"text": " Table 4: Comparison of Naive Bayes and J48", "labels": [], "entities": [{"text": "J48", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8053759336471558}]}, {"text": " Table 5: Performance of J48 for different feature settings", "labels": [], "entities": [{"text": "J48", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9092052578926086}]}]}