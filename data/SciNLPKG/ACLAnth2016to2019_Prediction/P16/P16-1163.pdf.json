{"title": [{"text": "Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network", "labels": [], "entities": [{"text": "Implicit Discourse Relation Detection", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9301163852214813}]}], "abstractContent": [{"text": "Word pairs, which are one of the most easily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations held between text segments.", "labels": [], "entities": []}, {"text": "However, because of the data sparsity problem, the performance achieved by using word pair features is limited.", "labels": [], "entities": []}, {"text": "In this paper, in order to overcome the data sparsity problem, we propose the use of word embeddings to replace the original words.", "labels": [], "entities": []}, {"text": "Moreover, we adopt a gated relevance network to capture the semantic interaction between word pairs, and then aggregate those semantic interactions using a pooling layer to select the most informative interactions.", "labels": [], "entities": []}, {"text": "Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations.", "labels": [], "entities": [{"text": "Penn Discourse Tree Bank", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.9686800241470337}]}], "introductionContent": [{"text": "Ina well-written document, no unit of the text is completely isolated, discourse relations describe how two units (e.g. clauses, sentences, and larger multi-clause groupings) of discourse are logically connected.", "labels": [], "entities": []}, {"text": "Many downstream NLP applications such as opinion mining, summarization, and event detection, can benefit from those relations.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.87787064909935}, {"text": "summarization", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9887459874153137}, {"text": "event detection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.8101789653301239}]}, {"text": "The task of automatically identify discourse relation is relatively simple when explicit connectives such as however and because are given ().", "labels": [], "entities": []}, {"text": "However, the identification becomes much more challenging when such connectives are missing.", "labels": [], "entities": [{"text": "identification", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9619406461715698}]}, {"text": "In fact, such implicit discourse relations outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser ().", "labels": [], "entities": []}, {"text": "Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task).", "labels": [], "entities": []}, {"text": "Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation.", "labels": [], "entities": []}, {"text": "However, because of the data sparsity problem and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (), the classifiers based on word pairs in the previous studies did notwork well.", "labels": [], "entities": []}, {"text": "Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs.", "labels": [], "entities": []}, {"text": "Consider the following sentence pair with a casual relation as an example: S1: Psyllium's not a good crop.", "labels": [], "entities": []}, {"text": "S2: You get a rain at the wrong time and the crop is ruined.", "labels": [], "entities": []}, {"text": "Intuitively, (good, wrong) and (good, ruined), seem to be the most informative word pairs, and it is likely that they will trigger a contrast relation.", "labels": [], "entities": []}, {"text": "Therefore, we can see that another main disadvantage of using word pairs is the lack of contextual information, and using n-gram pairs will again suffer from data sparsity problem.", "labels": [], "entities": []}, {"text": "Recently, the distributed word representations ( have shown an advantage when dealing with data sparsity problem, and many deep learning based models are generating substantial interests in text semantic matching and have achieved some significant progresses (Hu  et al., 2014;).", "labels": [], "entities": [{"text": "text semantic matching", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.7317826350529989}]}, {"text": "Inspired by their work, we in this paper propose the use of word embeddings to replace the original words in the text segments to fight against the data sparsity problem.", "labels": [], "entities": []}, {"text": "Further more, in order to preserve the contextual information around the word embeddings, we encode the text segment to its positional representation via a recurrent neural network, specifically, we use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997).", "labels": [], "entities": []}, {"text": "Then, to overcome the semantic gap, we propose the use of a gated relevance network to capture the semantic interaction between those positional representations.", "labels": [], "entities": []}, {"text": "Finally, all the interactions generated by the relevance network are fed to a max pooling layer to get the strongest interactions.", "labels": [], "entities": []}, {"text": "We then aggregate them to predict the discourse relation through a multi-layer perceptron (MLP).", "labels": [], "entities": []}, {"text": "Our model is trained end to end by BackPropagation and Adagrad.", "labels": [], "entities": []}, {"text": "The main contribution of this paper can be summarized as follows: \u2022 We use word embeddings to replace the original words in the text segments to overcome data sparsity problem.", "labels": [], "entities": []}, {"text": "In order to preserve the contextual information, we further encode the text segment to its positional representation through a recurrent neural network.", "labels": [], "entities": []}, {"text": "\u2022 To deal with the semantic gap problem, we adopt a gated relevance network to capture the semantic interaction between the intermediate representations of the text segments.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results on PDTB ( show that the proposed method can achieve better performance in recognizing discourse level relations in all of the relations than the previous methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset we used in this work is Penn Discourse.", "labels": [], "entities": [{"text": "Penn Discourse", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9797984063625336}]}, {"text": "The negative samples were chosen randomly from training sections 2-20.", "labels": [], "entities": []}, {"text": "In this part, we will mainly introduce the experiment settings, including baselines and parameter setting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The unbalanced sample distribution of  PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8572241067886353}]}, {"text": " Table 1. The negative samples were chosen ran- domly from training sections 2-20.", "labels": [], "entities": []}, {"text": " Table 3: The performances of different approaches on the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8667677640914917}]}, {"text": " Table 4: Comparison of our model with different  parameter settings to the gated relevance network.  Cmp denotes the comparison relation, Ctg denotes  the contingency relation, Exp denotes the expan- sion relation and Tmp denotes the temporal rela- tion.", "labels": [], "entities": []}]}