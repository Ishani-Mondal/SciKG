{"title": [], "abstractContent": [{"text": "A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontol-ogy.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.6903765052556992}]}, {"text": "In this paper, we propose a sentence rewriting based semantic parsing method, which can effectively resolve the mismatch problem by rewriting a sentence into anew form which has the same structure with its target logical form.", "labels": [], "entities": [{"text": "sentence rewriting based semantic parsing", "start_pos": 28, "end_pos": 69, "type": "TASK", "confidence": 0.7351125776767731}]}, {"text": "Specifically , we propose two sentence-rewriting methods for two common types of mis-match: a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch.", "labels": [], "entities": []}, {"text": "We evaluate our sentence rewriting based semantic parser on the benchmark semantic parsing dataset-WEBQUESTIONS.", "labels": [], "entities": [{"text": "sentence rewriting based semantic parser", "start_pos": 16, "end_pos": 56, "type": "TASK", "confidence": 0.7345840930938721}]}, {"text": "Experimental results show that our system outperforms the base system with a 3.4% gain in F1, and generates logical forms more accurately and parses sentences more robustly.", "labels": [], "entities": [{"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9990319013595581}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8180661797523499}]}, {"text": "shows an example of semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.8006267845630646}]}, {"text": "Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering) and information extraction (.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8161896169185638}, {"text": "natural language understanding", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6526140868663788}, {"text": "question answering", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.9160886406898499}, {"text": "information extraction", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.8895138800144196}]}, {"text": "Semantic parsing, however, is a challenging", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7946920394897461}]}], "datasetContent": [{"text": "In this section, we assess our method and compare it with other methods.", "labels": [], "entities": []}, {"text": "Dataset: We evaluate all systems on the benchmark WEBQUESTIONS dataset, which contains 5,810 question-answer pairs.", "labels": [], "entities": [{"text": "WEBQUESTIONS dataset", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.8361031413078308}]}, {"text": "All questions are collected by crawling the Google Suggest API, and their answers are obtained using Amazon Mechanical Turk.", "labels": [], "entities": []}, {"text": "This dataset covers several popular topics and its questions are commonly asked on the web.", "labels": [], "entities": []}, {"text": "According to, 85% of questions can be answered by predicting a single binary relation.", "labels": [], "entities": []}, {"text": "In our experiments, we use the standard train-test split (Berant et al., 2013), i.e., 3,778 questions (65%) for training and 2,032 questions (35%) for testing, and divide the training set into 3 random 80%-20% splits for development.", "labels": [], "entities": []}, {"text": "Furthermore, to verify the effectiveness of our method on solving the vocabulary mismatch problem, we manually select 50 mismatch test examples from the WEBQUESTIONS dataset, where all sentences have different structure with their target logical forms, e.g., \"Who is keyshia cole dad?\" and \"What countries have german as the official language?\".", "labels": [], "entities": [{"text": "WEBQUESTIONS dataset", "start_pos": 153, "end_pos": 173, "type": "DATASET", "confidence": 0.9451456367969513}]}, {"text": "System Settings: In our experiments, we use the Freebase Search API for entity lookup.", "labels": [], "entities": [{"text": "Freebase Search API", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.9063745935757955}]}, {"text": "We load Freebase using Virtuoso, and execute logical forms by converting them to SPARQL and querying using Virtuoso.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9424847364425659}]}, {"text": "We learn the parameters of our system by making three passes over the training dataset, with the beam size K = 200, the dictionary rewriting size K D = 100, and the template rewriting size K T = 100.", "labels": [], "entities": []}, {"text": "Baselines: We compare our method with several traditional systems, including semantic parsing based systems, information extraction based systems, machine translation based systems (), embedding based systems (, and QA based system.", "labels": [], "entities": [{"text": "semantic parsing based", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.792634646097819}, {"text": "information extraction based", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.7766532500584921}]}, {"text": "Evaluation: Following previous work, we evaluate different systems using the fraction of correctly answered questions.", "labels": [], "entities": []}, {"text": "Because golden answers may have multiple values, we use the average F1 score as the main evaluation metric.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9820970594882965}]}, {"text": "provides the performance of all base-lines and our method.", "labels": [], "entities": []}, {"text": "We can see that:  1.", "labels": [], "entities": []}, {"text": "Our method achieved competitive performance: Our system outperforms all baselines and get the best F1-measure of 53.1 on WE-BQUESTIONS dataset.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9994286894798279}, {"text": "WE-BQUESTIONS dataset", "start_pos": 121, "end_pos": 142, "type": "DATASET", "confidence": 0.9036004841327667}]}, {"text": "2. Sentence rewriting is a promising technique for semantic parsing: By employing sentence rewriting, our system gains a 3.4% F1 improvement over the base system we used (Berant and Liang, 2015).", "labels": [], "entities": [{"text": "Sentence rewriting", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8714581727981567}, {"text": "semantic parsing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.8159700930118561}, {"text": "F1", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9991487264633179}]}, {"text": "3. Compared to all baselines, our system gets the highest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9996416568756104}]}, {"text": "This result indicates that our parser can generate more-accurate logical forms by sentence rewriting.", "labels": [], "entities": []}, {"text": "Our system also achieves the second highest recall, which is a competitive performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9997146725654602}]}, {"text": "Interestingly, both the two systems with the highest recall (  2015) rely on extra-techniques such as entity linking and relation matching.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9991856217384338}, {"text": "entity linking", "start_pos": 102, "end_pos": 116, "type": "TASK", "confidence": 0.7481847107410431}, {"text": "relation matching", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.8384418189525604}]}, {"text": "The effectiveness on mismatch problem.", "labels": [], "entities": []}, {"text": "To analyze the commonness of mismatch problem in semantic parsing, we randomly sample 500 questions from the training data and do manually analysis, we found that 12.2% out of the sampled questions have mismatch problems: 3.8% out of them have 1-N mismatch problem and 8.4% out of them have N-1 mismatch problem.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7858959436416626}]}, {"text": "To verify the effectiveness of our method on solving the mismatch problem, we conduct experiments on the 50 mismatch test examples and shows the performance.", "labels": [], "entities": []}, {"text": "We can see that our system can effectively resolve the mismatch between natural language and target ontology: compared to the base system, our system achieves a significant 54.5% F1 im-provement.", "labels": [], "entities": [{"text": "F1", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.9986931681632996}]}], "tableCaptions": [{"text": " Table 8: The results of our system and recently  published systems. The results of other systems  are from either original papers or the standard  evaluation web.", "labels": [], "entities": []}, {"text": " Table 10: The results of the base system and our  systems on the 2032 test questions.", "labels": [], "entities": []}]}