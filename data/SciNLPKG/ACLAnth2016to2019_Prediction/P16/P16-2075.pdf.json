{"title": [{"text": "Machine Translation Evaluation Meets Community Question Answering", "labels": [], "entities": [{"text": "Machine Translation Evaluation Meets Community Question Answering", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.790236600807735}]}], "abstractContent": [{"text": "We explore the applicability of machine translation evaluation (MTE) methods to a very different problem: answer ranking in community Question Answering.", "labels": [], "entities": [{"text": "machine translation evaluation (MTE)", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.8589359670877457}, {"text": "answer ranking in community Question Answering", "start_pos": 106, "end_pos": 152, "type": "TASK", "confidence": 0.6570365279912949}]}, {"text": "In particular , we adopt a pairwise neural network (NN) architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings, and which efficiently models complex non-linear interactions.", "labels": [], "entities": []}, {"text": "The evaluation results show state-of-the-art performance, with sizeable contribution from both the MTE features and from the pairwise NN architecture.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We experiment with the data from SemEval-2016 Task 3.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7510687907536825}]}, {"text": "The task offers a higher quality training dataset TRAIN-PART1, which includes 1,412 questions and 14,110 answers, and a lower-quality TRAIN-PART2 with 382 questions and 3,790 answers.", "labels": [], "entities": [{"text": "TRAIN-PART1", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9769342541694641}, {"text": "TRAIN-PART2", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.8886469006538391}]}, {"text": "We train our model on TRAIN-PART1 with hidden layers of size 3 for 100 epochs with minibatches of size 30, regularization of 0.005, and a decay of 0.0001, using stochastic gradient descent with adagrad (Duchi et al., 2011); we use Theano () for learning.", "labels": [], "entities": [{"text": "regularization", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.9894578456878662}]}, {"text": "We normalize the input feature values to the interval using minmax, and we initialize the network weights by sampling from a uniform distribution as in.", "labels": [], "entities": []}, {"text": "We train the model using all pairs of good vs. bad comments, in both orders, ignoring ties.: Main results on the ranking task.", "labels": [], "entities": [{"text": "Main", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9586772322654724}]}, {"text": "At test time, we get the full ranking by scoring all possible pairs, and we accumulate the scores at the comment level.", "labels": [], "entities": []}, {"text": "We evaluate the model on TRAIN-PART2 after each epoch, and ultimately we keep the model that achieves the highest accuracy; 5 in case of a tie, we prefer the parameters from an earlier epoch.", "labels": [], "entities": [{"text": "TRAIN-PART2", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.717101514339447}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9986080527305603}]}, {"text": "We selected the above parameter values on the DEV dataset (244 questions and 2,440 answers) using the full model, and we used them for all experiments below, where we evaluate on the official TEST dataset (329 questions and 3,270 answers).", "labels": [], "entities": [{"text": "DEV dataset", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9890798330307007}, {"text": "TEST dataset", "start_pos": 192, "end_pos": 204, "type": "DATASET", "confidence": 0.8974762856960297}]}, {"text": "We report mean average precision (MAP), which is the official evaluation measure, and also average recall (AvgRec) and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9314510424931844}, {"text": "average recall (AvgRec)", "start_pos": 91, "end_pos": 114, "type": "METRIC", "confidence": 0.8643229365348816}, {"text": "mean reciprocal rank (MRR)", "start_pos": 119, "end_pos": 145, "type": "METRIC", "confidence": 0.9054733117421468}]}, {"text": "shows the evaluation results for three configurations of our MTE-based cQA system.", "labels": [], "entities": [{"text": "MTE-based cQA", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.6105726957321167}]}, {"text": "We can see that the vanilla MTE system (MTE vanilla ), which only uses features from our original MTE model, i.e., it does not have any task-specific features (TASK FEATURES and QL VECTORS), performs surprisingly well despite the differences in the MTE and cQA tasks.", "labels": [], "entities": [{"text": "TASK", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9839112758636475}, {"text": "FEATURES", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.5179194808006287}]}, {"text": "It outperforms a random baseline (Baseline rand ) and a chronological baseline that assumes that early comments are better than later ones (Baseline time ) by large margins: by about 11 and 17 MAP points absolute, respectively.", "labels": [], "entities": [{"text": "MAP", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9929304122924805}]}, {"text": "For the other two measures the results are similar.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Main results on the ranking task.", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9585662484169006}]}, {"text": " Table 2: Results of the ablation study.", "labels": [], "entities": []}, {"text": " Table 3: Comparative results with the best  SemEval-2016 Task 3, subtask A systems.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.751221756140391}]}]}