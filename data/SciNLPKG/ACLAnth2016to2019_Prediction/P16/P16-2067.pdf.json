{"title": [{"text": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "labels": [], "entities": [{"text": "Multilingual Part-of-Speech Tagging", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6153064469496409}, {"text": "Auxiliary Loss", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.7094887793064117}]}], "abstractContent": [{"text": "Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence mod-eling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise.", "labels": [], "entities": []}, {"text": "We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.7732058167457581}]}, {"text": "We compare bi-LSTMs to traditional POS taggers across languages and data sizes.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.776488721370697}]}, {"text": "We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.5294587761163712}]}, {"text": "The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages.", "labels": [], "entities": []}, {"text": "Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, bidirectional long short-term memory networks (bi-LSTM) (; Hochreiter and Schmidhuber, 1997) have been used for language modelling (, POS tagging (, transition-based dependency parsing (, fine-grained sentiment analysis (, syntactic chunking (, and semantic role labeling (.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7817354202270508}, {"text": "POS tagging", "start_pos": 144, "end_pos": 155, "type": "TASK", "confidence": 0.8672247529029846}, {"text": "transition-based dependency parsing", "start_pos": 159, "end_pos": 194, "type": "TASK", "confidence": 0.6205454170703888}, {"text": "fine-grained sentiment analysis", "start_pos": 198, "end_pos": 229, "type": "TASK", "confidence": 0.6450496713320414}, {"text": "syntactic chunking", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.8009979724884033}, {"text": "semantic role labeling", "start_pos": 259, "end_pos": 281, "type": "TASK", "confidence": 0.6544903914133707}]}, {"text": "LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients.", "labels": [], "entities": []}, {"text": "Bidirectional LSTMs make a backward and forward pass through the sequence before passing onto the next layer.", "labels": [], "entities": []}, {"text": "For further details, see.", "labels": [], "entities": []}, {"text": "We consider using bi-LSTMs for POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.8813375234603882}]}, {"text": "Previous work on using deep learning-based methods for POS tagging has focused either on a single language) or a small set of languages ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9182848632335663}]}, {"text": "Instead we evaluate our models across 22 languages.", "labels": [], "entities": []}, {"text": "In addition, we compare performance with representations at different levels of granularity (words, characters, and bytes).", "labels": [], "entities": []}, {"text": "These levels of representation were previously introduced in different efforts), but a comparative evaluation was missing.", "labels": [], "entities": []}, {"text": "Moreover, deep networks are often said to require large volumes of training data.", "labels": [], "entities": []}, {"text": "We investigate to what extent bi-LSTMs are more sensitive to the amount of training data and label noise than standard POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.6406590044498444}]}, {"text": "Finally, we introduce a novel model, a bi-LSTM trained with auxiliary loss.", "labels": [], "entities": []}, {"text": "The model jointly predicts the POS and the log frequency of the next word.", "labels": [], "entities": [{"text": "POS", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9965950846672058}]}, {"text": "The intuition behind this model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words.", "labels": [], "entities": []}, {"text": "We indeed observe performance gains on rare and out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "These performance gains transfer into general improvements for morphologically rich languages.", "labels": [], "entities": []}, {"text": "Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel bi-LSTM model with auxiliary loss (LOGFREQ).", "labels": [], "entities": [{"text": "auxiliary loss (LOGFREQ)", "start_pos": 255, "end_pos": 279, "type": "METRIC", "confidence": 0.775307047367096}]}], "datasetContent": [{"text": "All bi-LSTM models were implemented in CNN/pycnn, 1 a flexible neural network library.", "labels": [], "entities": []}, {"text": "For all models we use the same hyperparameters, which were set on English dev, i.e., SGD training with cross-entropy loss, no mini-batches, 20 epochs, default learning rate (0.1), 128 dimensions for word embeddings, 100 for character and byte embeddings, 100 hidden states and Gaussian noise with \u03c3=0.2.", "labels": [], "entities": [{"text": "default learning rate", "start_pos": 151, "end_pos": 172, "type": "METRIC", "confidence": 0.7940737207730612}]}, {"text": "As training is stochastic in nature, we use a fixed seed throughout.", "labels": [], "entities": []}, {"text": "Embeddings are not initialized with pre-trained embeddings, except when reported otherwise.", "labels": [], "entities": []}, {"text": "In that case we use offthe-shelf polyglot embeddings (Al-).", "labels": [], "entities": []}, {"text": "No further unlabeled data is considered in this paper.", "labels": [], "entities": []}, {"text": "The code is released at: https: //github.com/bplank/bilstm-aux Taggers We want to compare POS taggers under varying conditions.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.6233346164226532}]}, {"text": "We hence use three different types of taggers: our implementation of a bi-LSTM; TNT)-a second order HMM with suffix trie handling for OOVs.", "labels": [], "entities": []}, {"text": "We use TNT as it was among the best performing taggers evaluated in.", "labels": [], "entities": [{"text": "TNT", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.9582555294036865}]}, {"text": "We complement the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation () based on crfsuite.", "labels": [], "entities": []}, {"text": "For the multilingual experiments, we use the data from the Universal Dependencies project v1.2 (Nivre et al., 2015) (17 POS) with the canonical data splits.", "labels": [], "entities": []}, {"text": "For languages with token segmentation ambiguity we use the provided gold segmentation.", "labels": [], "entities": [{"text": "token segmentation", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.68154177069664}]}, {"text": "If there is more than one treebank per language, we use the treebank that has the canonical language name (e.g., Finnish instead of Finnish-FTB).", "labels": [], "entities": []}, {"text": "We consider all languages that have at least 60k tokens and are distributed with word forms, resulting in 22 languages.", "labels": [], "entities": []}, {"text": "We also report accuracies on WSJ (45 POS) using the standard splits).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9929661750793457}, {"text": "WSJ", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.5780199766159058}]}, {"text": "The overview of languages is provided in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Tagging accuracies on UD 1.2 test sets.  w: words, c: characters, b: bytes. Bold/ \u2020: best  accuracy/representation; +POLYGLOT: using pre-trained embeddings. FREQBIN: our multi-task model.  OOV ACC: accuracies on OOVs. BTS: best results in Gillick et al. (2016) (not strictly comparable).", "labels": [], "entities": [{"text": "UD 1.2 test sets", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.8428081721067429}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9981129169464111}, {"text": "POLYGLOT", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9962766766548157}, {"text": "FREQBIN", "start_pos": 167, "end_pos": 174, "type": "METRIC", "confidence": 0.9537113904953003}, {"text": "BTS", "start_pos": 228, "end_pos": 231, "type": "METRIC", "confidence": 0.7398287057876587}]}]}