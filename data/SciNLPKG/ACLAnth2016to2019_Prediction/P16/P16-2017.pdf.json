{"title": [{"text": "Semantic classifications for detection of verb metaphors", "labels": [], "entities": [{"text": "Semantic classifications", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8245063126087189}, {"text": "detection of verb metaphors", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7282193154096603}]}], "abstractContent": [{"text": "We investigate the effectiveness of semantic generalizations/classifications for capturing the regularities of the behavior of verbs in terms of their metaphoric-ity.", "labels": [], "entities": [{"text": "semantic generalizations/classifications", "start_pos": 36, "end_pos": 76, "type": "TASK", "confidence": 0.7644734680652618}]}, {"text": "Starting from orthographic word unigrams, we experiment with various ways of defining semantic classes for verbs (grammatical, resource-based, dis-tributional) and measure the effectiveness of these classes for classifying all verbs in a running text as metaphor or non metaphor.", "labels": [], "entities": []}], "introductionContent": [{"text": "According to the Conceptual Metaphor theory, metaphoricity is a property of concepts in a particular context of use, not of specific words.", "labels": [], "entities": []}, {"text": "The notion of a concept is a fluid one, however.", "labels": [], "entities": []}, {"text": "While write and wrote would likely constitute instances of the same concept according to any definition, it is less clear whether eat and gobble would.", "labels": [], "entities": []}, {"text": "Furthermore, the Conceptual Metaphor theory typically operates with whole semantic domains that certainly generalize beyond narrowly-conceived concepts; thus, save and waste share a very general semantic feature of applying to finite resources -it is this meaning element that accounts for the observation that they tend to be used metaphorically in similar contexts.", "labels": [], "entities": []}, {"text": "In this paper, we investigate which kinds of generalizations are the most effective for capturing regularities of metaphor usage.", "labels": [], "entities": [{"text": "capturing regularities of metaphor usage", "start_pos": 88, "end_pos": 128, "type": "TASK", "confidence": 0.7456248521804809}]}], "datasetContent": [{"text": "We use the VU Amsterdam Metaphor Corpus (.", "labels": [], "entities": [{"text": "VU Amsterdam Metaphor Corpus", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.9079352468252182}]}, {"text": "The corpus contains annotations of all tokens in running text as metaphor or non metaphor, according to a protocol similar to MIP.", "labels": [], "entities": []}, {"text": "The data come from the BNC, across 4 genres: news (N), academic writing (A), fiction (F), and conversation (C).", "labels": [], "entities": [{"text": "BNC", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8630495667457581}]}, {"text": "We address each genre separately.", "labels": [], "entities": []}, {"text": "We consider all verbs apart from have, be, and do.", "labels": [], "entities": []}, {"text": "We use the same training and testing partitions as Beigman.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of feature sets. All features are  binary features indicating class membership.", "labels": [], "entities": []}, {"text": " Table 2: Summary of the data. #T = # of texts; #I  = # of instances; %M = percentage of metaphors.", "labels": [], "entities": []}, {"text": " Table 3: Performance (F1) of each of the feature  sets, xval on training data. U = unigram baseline.", "labels": [], "entities": [{"text": "F1)", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9054518043994904}]}, {"text": " Table 4: Benchmark performance, F1 score.", "labels": [], "entities": [{"text": "Benchmark", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7597948312759399}, {"text": "F1 score", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9508354663848877}]}]}