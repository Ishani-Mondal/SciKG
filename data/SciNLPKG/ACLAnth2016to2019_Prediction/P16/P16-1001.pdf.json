{"title": [{"text": "Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing", "labels": [], "entities": [{"text": "Noise reduction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7812553942203522}, {"text": "Abstract Meaning Representation parsing", "start_pos": 67, "end_pos": 106, "type": "TASK", "confidence": 0.8561682999134064}]}], "abstractContent": [{"text": "Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenomena , resolve anaphora, and identify word senses to eliminate ambiguous interpretations.", "labels": [], "entities": [{"text": "Semantic parsers map natural language statements into meaning representations", "start_pos": 0, "end_pos": 77, "type": "TASK", "confidence": 0.8412128686904907}]}, {"text": "Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a dependency parse, utilizes a graph to represent relationships between concepts (Ba-narescu et al., 2013).", "labels": [], "entities": [{"text": "Abstract meaning representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7848493754863739}]}, {"text": "As with dependency parsing, transition-based approaches area common approach to this problem.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8422717452049255}]}, {"text": "However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding.", "labels": [], "entities": []}, {"text": "Imitation learning algorithms have been shown to help these systems recover from such errors.", "labels": [], "entities": [{"text": "Imitation learning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8962615728378296}]}, {"text": "To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions: noise reduction and targeted exploration.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9793093204498291}, {"text": "noise reduction", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.7496548295021057}]}, {"text": "The former mitigates the noise in the feature representation, a result of the complexity of the task.", "labels": [], "entities": []}, {"text": "The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space.", "labels": [], "entities": []}, {"text": "We achieve state-of-the art results, and improve upon standard transition-based parsing by 4.7 F 1 points.", "labels": [], "entities": []}], "introductionContent": [{"text": "Meaning representation languages and systems have been devised for specific domains, such as ATIS for air-travel bookings ( and database queries (  is that it is domain-independent and useful in a variety of applications ().", "labels": [], "entities": [{"text": "Meaning representation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7951004803180695}, {"text": "ATIS", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.7127426862716675}]}, {"text": "The first AMR parser by used graph-based inference to find a highestscoring maximum spanning connected acyclic graph.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9002327620983124}]}, {"text": "Later work by was inspired by the similarity between the dependency parse of a sentence and its semantic AMR graph ().", "labels": [], "entities": [{"text": "dependency parse of a sentence", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.8163178324699402}]}, {"text": "start from the dependency parse and learn a transition-based parser that converts it incrementally into an AMR graph using greedy decoding.", "labels": [], "entities": []}, {"text": "An advantage of this approach is that the initial stage of dependency parsing is well-studied and trained using larger corpora than that for which AMR annotations exist.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8487297594547272}]}, {"text": "Greedy decoding, where the parser builds the parse while maintaining only the best hypothesis at each step, has a well-documented disadvantage: error propagation (.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.7282760739326477}]}, {"text": "When the parser encounters states during parsing that are unlike those found during training, it is more likely to make mistakes, leading to states which are increasingly more foreign and causing errors to accumulate.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main dataset used is the newswire (proxy) section of LDC2014T12 ().", "labels": [], "entities": [{"text": "newswire (proxy) section of LDC2014T12", "start_pos": 29, "end_pos": 67, "type": "DATASET", "confidence": 0.682868595634188}]}, {"text": "The data from years 1995-2006 form the training data, with 2007 as the validation set and 2008 as the test set.", "labels": [], "entities": []}, {"text": "The data split is the same as that used by and.", "labels": [], "entities": []}, {"text": "We first assess the impact of noise reduction using the alpha bound, and report these experiments without Rollouts (i.e. using DAGGER) to isolate the effect of noise reduction.", "labels": [], "entities": [{"text": "noise reduction", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7215819954872131}, {"text": "DAGGER", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.7838809490203857}, {"text": "noise reduction", "start_pos": 160, "end_pos": 175, "type": "TASK", "confidence": 0.7430135905742645}]}, {"text": "summarises results using exact imitation and DAGGER with the \u03b1-bound set to discard a training instance after one misclassification.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9962208867073059}]}, {"text": "This is the most extreme setting, and the one that gave best results.", "labels": [], "entities": []}, {"text": "We try AROW (, PassiveAggressive (PA)), and perceptron) classifiers, with averaging in all cases.", "labels": [], "entities": [{"text": "AROW", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9707815647125244}]}, {"text": "We see a benefit from the \u03b1-bound for exact imitation only with AROW, which is more noise-sensitive than PA or the simple perceptron.", "labels": [], "entities": [{"text": "exact imitation", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.650168240070343}, {"text": "AROW", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9324045777320862}, {"text": "PA", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9274260997772217}]}, {"text": "With DAGGER there is a benefit for all classifiers.", "labels": [], "entities": []}, {"text": "In all cases the \u03b1-bound and DAGGER are synergistic; without the \u03b1-bound imitation learning works less well, if at all.", "labels": [], "entities": [{"text": "DAGGER", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.998755693435669}]}, {"text": "\u03b1=1 was the optimal setting, with lesser benefit observed for larger values.", "labels": [], "entities": []}, {"text": "We now turn our attention to targeted exploration and focused costing, for which we use V-DAGGER as explained in section 4.", "labels": [], "entities": [{"text": "V-DAGGER", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.7722982168197632}]}, {"text": "For all V-  DAGGER experiments we use AROW with regularisation parameter C=1000, and \u03b4=0.3.", "labels": [], "entities": [{"text": "AROW", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9817292094230652}, {"text": "regularisation parameter C", "start_pos": 48, "end_pos": 74, "type": "METRIC", "confidence": 0.8314125537872314}]}, {"text": "shows results by iteration of reducing the number of RollOuts explored.", "labels": [], "entities": []}, {"text": "Only the expert action, plus actions that score close to the bestscoring action (defined by the threshold) are used for RollOuts.", "labels": [], "entities": [{"text": "RollOuts", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.8986032605171204}]}, {"text": "Using the action cost information from RollOuts does surpass simple DAGGER, and unsurprisingly more exploration is better.", "labels": [], "entities": [{"text": "RollOuts", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9432591199874878}, {"text": "DAGGER", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9117097854614258}]}, {"text": "shows the same data, but by total computational time spent 2 . This adjusts the picture, as small amounts of exploration give a faster benefit, albeit not always reaching the same peak performance.", "labels": [], "entities": []}, {"text": "As a baseline, three iterations of V-DAGGER without targeted exploration (threshold = \u221e) takes 9600 minutes on the same hardware to give an F-Score of 0.652 on the validation set.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.9986899495124817}]}, {"text": "shows the improvement using focused costing.", "labels": [], "entities": []}, {"text": "The 'n/m' setting sets b, the number of initial actions taken by\u02c6\u03c0by\u02c6 by\u02c6\u03c0 in a RollOut ton, and then increases this by mat each iteration.", "labels": [], "entities": []}, {"text": "We gain an increase of 2.9 points from 0.682 to 0.711.", "labels": [], "entities": []}, {"text": "In all the settings tried, focused costing improves the results, and requires progressive removal of the expert to achieve the best score.", "labels": [], "entities": []}, {"text": "We use the classifier from the Focused Costing 5/5 run to achieve an F-Score on the held-out test set of 0.70, equal to the best published result so far ().", "labels": [], "entities": [{"text": "F-Score", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9991625547409058}]}, {"text": "Our gain of 4.7 points from imitation learning over standard transition-based parsing is orthogonal to that of using exact imitation with additional trained analysers; they experience again of 2 points from using a Charniak parser) trained on the full OntoNotes corpus instead of the Stanford parser used here and in, and a further gain of 2 points from a semantic role labeller.", "labels": [], "entities": []}, {"text": "Using DAGGER with this system we obtained an F-Score of 0.60 in the Semeval 2016 task on AMR parsing, one standard deviation above the mean of all entries.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9993313550949097}, {"text": "AMR parsing", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.8253718316555023}]}, {"text": "( Finally we test on all components of the LDC2014T12 corpus as shown in, which include both newswire and weblog data, as well as the freely available AMRs for The Little Prince, (lpp) 3 . For each we use exact imitation, DAG-GER, and V-DAGGER on the train/validation/splits specified in the corpus.", "labels": [], "entities": [{"text": "LDC2014T12 corpus", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.9640867710113525}, {"text": "AMRs for The Little Prince", "start_pos": 151, "end_pos": 177, "type": "DATASET", "confidence": 0.6335940659046173}, {"text": "imitation", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.946931004524231}, {"text": "DAG-GER", "start_pos": 222, "end_pos": 229, "type": "METRIC", "confidence": 0.9070398211479187}]}, {"text": "In all cases, imitation learning without RollOuts (DAGGER) improves on exact imitation, and incorporating RollOuts (V-DAGGER) provides an additional benefit.", "labels": [], "entities": [{"text": "imitation learning", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.9216224253177643}]}, {"text": "use SEARN on the same datasets, but with a very different transition system.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9129209518432617}]}, {"text": "We show their results for comparison.", "labels": [], "entities": []}, {"text": "Our expert achieves a Smatch F-Score of 0.94 on the training data.", "labels": [], "entities": [{"text": "Smatch", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9772591590881348}, {"text": "F-Score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.8518156409263611}]}, {"text": "This explains why DAG-GER, which assumes a good expert, is effective.", "labels": [], "entities": [{"text": "DAG-GER", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.5579847097396851}]}, {"text": "Introducing RollOuts provides additional theoretical benefits from a non-decomposable loss function that can take into account longer-term impacts of an action.", "labels": [], "entities": []}, {"text": "This provides much more information than the 0/1 binary action cost in DAGGER, and we can use Na\u00a8\u0131veNa\u00a8\u0131ve Smatch as an approximation to our actual objective function during training.", "labels": [], "entities": []}, {"text": "This informational benefit comes at the cost of increased noise and computational expense, which we control with targeted exploration and focused costing.", "labels": [], "entities": []}, {"text": "We gain 2.7 points in F-Score, at the cost of 80-100x more computation.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9894402623176575}]}, {"text": "In problems with a less good expert, the gain from exploration could be much greater.", "labels": [], "entities": []}, {"text": "Similarly, if designing an expert fora task is time-consuming, then it maybe a better investment to rely on exploration with a poor expert to achieve the same result.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: DAGGER with \u03b1-bound. All figures are F-Scores on the validation set. 5 iterations of classifier training take place  after each DAgger iteration. A decay rate (\u03b4) for \u03c0  *  of 0.3 was used.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9870827198028564}, {"text": "decay rate (\u03b4)", "start_pos": 158, "end_pos": 172, "type": "METRIC", "confidence": 0.7372889935970306}]}, {"text": " Table 4: Comparison of previous work on the AMR task. R, P and F are Recall, Precision and F-Score.", "labels": [], "entities": [{"text": "AMR task", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.8239774107933044}, {"text": "Recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9822821617126465}, {"text": "Precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9431357979774475}, {"text": "F-Score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9757106304168701}]}]}