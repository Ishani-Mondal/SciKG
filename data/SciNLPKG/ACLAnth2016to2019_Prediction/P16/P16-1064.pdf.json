{"title": [{"text": "Detecting Common Discussion Topics Across Culture From News Reader Comments", "labels": [], "entities": [{"text": "Detecting Common Discussion Topics Across Culture From News Reader Comments", "start_pos": 0, "end_pos": 75, "type": "TASK", "confidence": 0.8845083713531494}]}], "abstractContent": [{"text": "News reader comments found in many on-line news websites are typically massive in amount.", "labels": [], "entities": []}, {"text": "We investigate the task of Cultural-common Topic Detection (CTD), which is aimed at discovering common discussion topics from newsreader comments written in different languages.", "labels": [], "entities": [{"text": "Cultural-common Topic Detection (CTD)", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.8036558628082275}]}, {"text": "We propose anew probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages.", "labels": [], "entities": []}, {"text": "We also develop a partially collapsed Gibbs sampler which effectively incorporates the term translation relationship into the detection of cultural-common topics for model parameter learning.", "labels": [], "entities": [{"text": "model parameter learning", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.626688947280248}]}, {"text": "Experimental results show improvements over the state-of-the-art model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Nowadays the rapid development of information and communication technology enables more and more people around the world to engage in the movement of globalization.", "labels": [], "entities": []}, {"text": "One effect of globalization is to facilitate greater connections between people bringing cultures closer than before.", "labels": [], "entities": []}, {"text": "This also contributes to the convergence of some elements of different cultures.", "labels": [], "entities": []}, {"text": "For example, there is a growing tendency of people watching the same movie, listening to the same music, and reading the news about the same event.", "labels": [], "entities": []}, {"text": "This kind of cultural homogenization brings the emergence of commonality of some aspects of different cultures worldwide.", "labels": [], "entities": []}, {"text": "It would be beneficial to identify such common aspects among cultures.", "labels": [], "entities": []}, {"text": "For example, it can provide some insights for global market and international business).", "labels": [], "entities": []}, {"text": "Many news websites from different regions in the world report significant events which are of interests to people from different continents.", "labels": [], "entities": []}, {"text": "These websites also allow readers around the world to give their comments in their own languages.", "labels": [], "entities": []}, {"text": "The volume of comments is often enormous especially for popular events.", "labels": [], "entities": []}, {"text": "Ina news website, readers from a particular culture background tend to write comments in their own preferred languages.", "labels": [], "entities": []}, {"text": "For some important or global events, we observe that readers from different cultures, via different languages, express common discussion topics.", "labels": [], "entities": []}, {"text": "For instance, on March 8 2014, Malaysia Airlines Flight MH370, carrying 227 passengers and 12 crew members, disappeared.", "labels": [], "entities": [{"text": "Malaysia Airlines Flight MH370", "start_pos": 31, "end_pos": 61, "type": "DATASET", "confidence": 0.8503957986831665}]}, {"text": "Upon the happening of this event, many news articles around the world reported it and many readers from different continents commented on this event.", "labels": [], "entities": []}, {"text": "Through analyzing the reader comments manually, we observe that both English-speaking and Chinese-speaking readers expressed in their corresponding languages their desire for praying for the MH370 flight.", "labels": [], "entities": []}, {"text": "This is an example of a cultural-common discussion topic.", "labels": [], "entities": []}, {"text": "Identifying such cultural-common topics automatically can facilitate better understanding and organization of the common concerns or interests of readers with different language background.", "labels": [], "entities": []}, {"text": "Such technology can be deployed for developing various applications.", "labels": [], "entities": []}, {"text": "One application is to build a reader comment digest system that can organize comments by cultural-common discussion topics and rank the topics by popularity.", "labels": [], "entities": []}, {"text": "This provides a functionality of analyzing the common focus of readers from different cultures on a particular event.", "labels": [], "entities": []}, {"text": "An example of such application is shown in.", "labels": [], "entities": []}, {"text": "Under each event, reader comments are grouped by cultural-common topics.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the task of Cultural-common Topic Detection (CTD) on multilingual newsreader comments.", "labels": [], "entities": [{"text": "Cultural-common Topic Detection (CTD)", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.7879852652549744}]}, {"text": "Reader comments about a global event, written in different languages, from different news websites around the world exist in massive amount.", "labels": [], "entities": []}, {"text": "The main goal of this task is to discover cultural-common discussion topics from raw multilingual newsreader comments fora news event.", "labels": [], "entities": []}, {"text": "One challenge is that the discussion topics are unknown.", "labels": [], "entities": []}, {"text": "Another challenge is related to the language gap issue.", "labels": [], "entities": []}, {"text": "Precisely, the words of reader comments in different languages are composed of different terms in their corresponding languages.", "labels": [], "entities": []}, {"text": "Such language gap issue poses a great deal of challenge for identifying cultural-common discussion topics in multilingual news comments settings.", "labels": [], "entities": []}, {"text": "One recent work done by is to organize newsreader comments around entities and aspects discussed by readers.", "labels": [], "entities": []}, {"text": "Such organization of reader comments cannot handle the identification of common discussion topics.", "labels": [], "entities": [{"text": "identification of common discussion topics", "start_pos": 55, "end_pos": 97, "type": "TASK", "confidence": 0.8498452425003051}]}, {"text": "On the other hand, the Muto model proposed by BoydGraber and can extract common topics from multilingual documents.", "labels": [], "entities": [{"text": "BoydGraber", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.9569596648216248}]}, {"text": "This model merely outputs cross-lingual topics of matching word pairs.", "labels": [], "entities": []}, {"text": "One example of such kind of topic contains key terms of word pairs such as \"plane: ocean: . .", "labels": [], "entities": []}, {"text": "\". The assumption of one-toone mapping of words has some drawbacks.", "labels": [], "entities": []}, {"text": "One drawback is that the correspondence of identified common topics is restricted to the vocabulary level.", "labels": [], "entities": []}, {"text": "Another drawback is that the one-toone mapping of words cannot fit the original word occurrences well.", "labels": [], "entities": []}, {"text": "For example, the English term \"plane\" appears in the English documents frequently while the Chinese translation \"\" appears less.", "labels": [], "entities": []}, {"text": "It is not reasonable that \"plane\" and \" \" share the same probability mass in common topics.", "labels": [], "entities": []}, {"text": "Another closely related existing work is the PCLSA model proposed by . PCLSA employs a mixture of English words and Chinese words to represent common topics.", "labels": [], "entities": []}, {"text": "It incorporates bilingual constraints into the Probabilistic Latent Semantic Analysis (PLSA) model and assumes that word pairs in the dictionary share similar probability in a common topic.", "labels": [], "entities": [{"text": "Probabilistic Latent Semantic Analysis (PLSA)", "start_pos": 47, "end_pos": 92, "type": "TASK", "confidence": 0.6922001200062888}]}, {"text": "However, similar to one-to-one mapping of words, such bilingual constraints cannot handle well the original word co-occurrence in each language resulting in a degradation of the coherence and interpretability of common topics.", "labels": [], "entities": []}, {"text": "We propose anew probabilistic graphical model which is able to detect cultural-common topics from multilingual newsreader comments in an unsupervised manner.", "labels": [], "entities": []}, {"text": "In principle, no labeled data is needed.", "labels": [], "entities": []}, {"text": "In this paper, we focus on dealing with two languages, namely, English and Chinese newsreader comments.", "labels": [], "entities": []}, {"text": "Different from prior works, we design a technique based on auxiliary distributions which incorporates word distributions from the other language and can capture the common semantics on the topic level.", "labels": [], "entities": []}, {"text": "We develop a partially collapsed Gibbs sampler which decouples the inference of topic distribution and word distribution.", "labels": [], "entities": []}, {"text": "We also incorporate the term translation relationship, derived from a bilingual dictionary, into the detection of cultural-common topics for model parameter learning.", "labels": [], "entities": []}, {"text": "We have prepared a data set by collecting English and Chinese reader comments from different regions reflecting different culture.", "labels": [], "entities": []}, {"text": "Our experimental results are encouraging showing improvements over the state-of-the-art model. and organized newsreader comments via identified entities or aspects.", "labels": [], "entities": []}, {"text": "Such kind of organization via entities or aspects cannot capture common topics discussed by readers.", "labels": [], "entities": []}, {"text": "Digesting merely based on entities fails to work in multilingual settings due to the fact that the common entities have distinct mentions in different languages.", "labels": [], "entities": [{"text": "Digesting merely based on entities", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8676812767982482}]}, {"text": "discovered common topics from comparable texts via a PLSA based mixture model.", "labels": [], "entities": []}, {"text": "proposed a MixedCollection Topic Model for finding common topics from different collections.", "labels": [], "entities": []}, {"text": "Despite the fact that the above models can find a kind of common topic, they only deal with a single language setting without considering the language gap.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each event, we partitioned the comments into a subset of 90% for the graphical model parameter estimation.", "labels": [], "entities": []}, {"text": "The remaining 10% is used as a holdout data for the evaluation of the CCP metric as discussed in Section 4.4.1.", "labels": [], "entities": [{"text": "CCP metric", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8228867650032043}]}, {"text": "We repeated the runs five times.", "labels": [], "entities": []}, {"text": "For each run, we randomly split the comments to obtain the holdout data.", "labels": [], "entities": []}, {"text": "As a result, we have five runs for our method as well as comparative methods.", "labels": [], "entities": []}, {"text": "We make use of the holdout data of one event, namely the event \"MH370 Flight Accident\", to estimate the number of topics K for all models and \u03bb in Eq.", "labels": [], "entities": [{"text": "MH370 Flight Accident", "start_pos": 64, "end_pos": 85, "type": "DATASET", "confidence": 0.7173113425572714}]}, {"text": "The setting of K is described in Section 4.4.3.", "labels": [], "entities": []}, {"text": "We set \u03bb = 0.5 after tuning.", "labels": [], "entities": []}, {"text": "For hyper-parameters, we set \u03b1 to 0.5 and \u03b2 to 0.01.", "labels": [], "entities": []}, {"text": "When performing our Gibbs algorithm, we set the maximum iteration number as 1000, and the burn-in sweeps as 100.", "labels": [], "entities": []}, {"text": "We conduct quantitative experiments to evaluate how well our MCTA model can discover culturalcommon topics.", "labels": [], "entities": []}, {"text": "We use two metrics to evaluate the topic quality.", "labels": [], "entities": []}, {"text": "The first metric is the \"cross-collection perplexity\" measure denoted as CCP which is similar to the one used in . The CCP of high quality cultural-common topics should be lower than those topics which are not shared by the English and Chinese reader comments.", "labels": [], "entities": []}, {"text": "The calculation of CCP consists of two steps: 1) For each k \u2208 K, we translate \u03d5 e k into Chinese word distribution T (\u03d5 e k ) and translate \u03d5 ck English word distribution T (\u03d5 ck ).", "labels": [], "entities": []}, {"text": "To translate \u03d5 e k and \u03d5 ck , we lookup the bilingual dictionary and conduct word-toword translation.", "labels": [], "entities": [{"text": "word-toword translation", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.6851296424865723}]}, {"text": "If one word has several translations, we distribute its probability mass equally to each English translation.", "labels": [], "entities": []}, {"text": "2) We use T (\u03d5 e k ) to fit the holdout Chinese comments C and T (\u03d5 ck ) to fit the holdout English comments E using Eq.", "labels": [], "entities": []}, {"text": "13 depicts the calculation of CCP.", "labels": [], "entities": [{"text": "calculation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9485501050949097}, {"text": "CCP", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8632650375366211}]}, {"text": "The lower the CCP value is, the better the performance is.", "labels": [], "entities": [{"text": "CCP", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.917064368724823}]}, {"text": "For each detected common topic, we wish to evaluate the degree of commonality.", "labels": [], "entities": []}, {"text": "We design another metric called \"topic commonality distance\" denoted by TCD.", "labels": [], "entities": [{"text": "TCD", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.7712982892990112}]}, {"text": "We first evaluate the KLdivergence between the English topic and translated Chinese topic.", "labels": [], "entities": [{"text": "KLdivergence", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.6002404093742371}]}, {"text": "We also evaluate the KLdivergence between the Chinese topic and translated English topic.", "labels": [], "entities": [{"text": "KLdivergence", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.4969441592693329}]}, {"text": "Then TCD is computed as the average sum of the two KL-divergences.", "labels": [], "entities": [{"text": "TCD", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9755209684371948}]}, {"text": "The lower the TCD measure is, the better the topic is.", "labels": [], "entities": [{"text": "TCD measure", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.983109712600708}]}, {"text": "The topic detected by PCLSA is a mixture of English and Chinese words.", "labels": [], "entities": []}, {"text": "We obtain English representation and Chinese representation of the topic by the conditional probabilities as given in Eq.", "labels": [], "entities": []}, {"text": "14.  The average CCP values of the three models are shown in.", "labels": [], "entities": [{"text": "CCP", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.8307821750640869}]}, {"text": "Our MCTA model achieves the best performance compared with PCLSA and LDA.", "labels": [], "entities": []}, {"text": "Both MCTA and PCLSA achieve a better CCP than LDA because they can bridge the language gap in the multilingual newsreader comments to some extent.", "labels": [], "entities": []}, {"text": "Compared with PCLSA, our MCTA model demonstrates a 4.2% improvement.", "labels": [], "entities": []}, {"text": "Our MCTA model provides a better characterization of the collections.", "labels": [], "entities": []}, {"text": "One reason is that our MCTA model learns the word distribution of cultural-common topics using an effective topic modeling with a prior Dirichlet distribution.", "labels": [], "entities": []}, {"text": "It is similar to the advantage of LDA over PLSA.", "labels": [], "entities": []}, {"text": "Moreover, the bilingual constraints in PCLSA cannot handle the original natural word co-occurrence well in each language.", "labels": [], "entities": []}, {"text": "In contrast, MCTA represents cultural-common topics as a mixture of the original topics and the translated topics, which capture the comment semantics more effectively.", "labels": [], "entities": []}, {"text": "The average TCD of three models are shown in: Topic quality evaluation as measured by TCD tified by MCTA have better topic commonality because our MCTA model can capture the common semantics between newsreader comments in different languages.", "labels": [], "entities": [{"text": "TCD", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9814775586128235}]}, {"text": "We also evaluate the coherence of topics generated by PCLSA and MCTA, which indicates the interpretability of topics.", "labels": [], "entities": [{"text": "MCTA", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.77447110414505}]}, {"text": "Following Newman et al., we use a pointwise mutual information (PMI) score to measure the topic coherence.", "labels": [], "entities": [{"text": "pointwise mutual information (PMI) score", "start_pos": 34, "end_pos": 74, "type": "METRIC", "confidence": 0.6617433215890612}]}, {"text": "We compute the average PMI score of top 20 topic word pairs using Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 66, "end_pos": 68, "type": "DATASET", "confidence": 0.8163849115371704}]}, {"text": "15. observed that it is important to use an external data set to evaluate PMI.", "labels": [], "entities": [{"text": "PMI", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9404295682907104}]}, {"text": "Therefore, we use a 20-word sliding window in Wikipedia to identify the co-occurrence of word pairs.", "labels": [], "entities": []}, {"text": "The experimental results are shown in.", "labels": [], "entities": []}, {"text": "We can see that our MCTA model generally improves the coherence of the learned topics compared with PCLSA.", "labels": [], "entities": []}, {"text": "The word-to-word bilingual constraints in PCLSA are not as effective.", "labels": [], "entities": []}, {"text": "On the other hand, our MTCA model incorporates the bilingual translations using auxiliary distributions which incorporate word distributions from the other language on the topic level and can capture common semantics of multilingual reader comments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. The English  reader comments are collected from Yahoo 1 and  the Chinese reader comments are collected from  Sina News 2 . We first remove news reader com- ments whose length is less than 5 words. We re- move the punctuations and the stop words. For En- glish comments, we also stem each word to its root", "labels": [], "entities": [{"text": "Sina News", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.883137434720993}]}, {"text": " Table 2: Topic quality evaluation as measured by  CCP", "labels": [], "entities": [{"text": "CCP", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.7119770050048828}]}, {"text": " Table 3: Topic quality evaluation as measured by  TCD", "labels": [], "entities": [{"text": "TCD", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8287361860275269}]}, {"text": " Table 4: Topic coherence evaluation", "labels": [], "entities": []}]}