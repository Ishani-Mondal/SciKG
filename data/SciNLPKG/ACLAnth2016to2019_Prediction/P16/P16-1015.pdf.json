{"title": [{"text": "Generalized Transition-based Dependency Parsing via Control Parameters", "labels": [], "entities": [{"text": "Generalized Transition-based Dependency Parsing", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.5402719005942345}]}], "abstractContent": [{"text": "In this paper, we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states.", "labels": [], "entities": []}, {"text": "This generalization provides a unified framework to describe and compare various transition-based parsing approaches from both a theoretical and empirical perspective.", "labels": [], "entities": []}, {"text": "This includes well-known transition systems, but also previously unstudied systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transition-based dependency parsing is perhaps the most successful parsing framework in use today.", "labels": [], "entities": [{"text": "Transition-based dependency parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6423654556274414}]}, {"text": "This is due to the fact that it can process sentences in linear time; is highly accurate (; and has elegant mechanisms for parsing non-projective sentences.", "labels": [], "entities": [{"text": "parsing non-projective sentences", "start_pos": 123, "end_pos": 155, "type": "TASK", "confidence": 0.8825231393178304}]}, {"text": "As a result, there have been numerous studies into different transition systems, each with varying properties and complexities.", "labels": [], "entities": []}, {"text": "While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems.", "labels": [], "entities": []}, {"text": "Such a framework would be beneficial for many reasons: It would provide a language from which we can theoretically compare known transition systems; it can give rise to new systems that could have favorable empirical properties; and an implementation of the generalization allows for comprehensive empirical studies.", "labels": [], "entities": []}, {"text": "In this work we provide such a generalized transition-based parsing framework.", "labels": [], "entities": []}, {"text": "Our framework can be cast as transition-based parsing as it contains both parser states as well as transitions between these states that construct dependency trees.", "labels": [], "entities": []}, {"text": "As in traditional transition-based parsing, the state maintains two data structures: a set of unprocessed tokens (normally called the buffer); and a set of operative tokens (often called the stack).", "labels": [], "entities": []}, {"text": "Key to our generalization is the notion of active tokens, which is the set of tokens in which new arcs can be created and/or removed from consideration.", "labels": [], "entities": []}, {"text": "A parser instantiation is defined by a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance.", "labels": [], "entities": []}, {"text": "We show that a number of different transition systems can be described via this framework.", "labels": [], "entities": []}, {"text": "Critically the two most common systems are covered -arc-eager and arc-standard.", "labels": [], "entities": []}, {"text": "But also Attardi's non-projective), Kuhlmann's hybrid system (), the directed acyclic graph (DAG) parser of, and likely others.", "labels": [], "entities": []}, {"text": "More interestingly, the easy-first framework of can be described as an arc-standard system with an unbounded active token capacity.", "labels": [], "entities": []}, {"text": "We present a number of experiments with an implementation of our generalized framework.", "labels": [], "entities": []}, {"text": "One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss some possibilities and provide experiments for these in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our implementation uses a linear model y i \u2208y w\u00b7 f(x, y 1 , . .", "labels": [], "entities": []}, {"text": ", y i ) to assign a score to a sequence y = y 1 , y 2 , . .", "labels": [], "entities": []}, {"text": "y m of parser transitions, given sentence x.", "labels": [], "entities": []}, {"text": "Model parameters are trained using the structured perceptron with \"early update\" () and features follow that of.", "labels": [], "entities": [{"text": "early update", "start_pos": 67, "end_pos": 79, "type": "METRIC", "confidence": 0.9539512395858765}]}, {"text": "For the arc-standard and arc-eager transition systems, we use the static oracle to derive a single gold sequence fora given sentence and its gold tree.", "labels": [], "entities": []}, {"text": "For systems where there is no such static oracle, for example the easy-first system, we use the method proposed by to select a gold sequence such that, for each update, the condition w \u00b7 f(x, \u02c6 y k ) < w \u00b7 f(x, y k ) always holds, which is required for perceptron convergence.", "labels": [], "entities": [{"text": "perceptron convergence", "start_pos": 253, "end_pos": 275, "type": "TASK", "confidence": 0.7839128971099854}]}, {"text": "Her\u00ea y k denotes the length k prefix of a correct sequence and y k denotes the highest scoring sequence in the beam.", "labels": [], "entities": []}, {"text": "We carryout the experiments on the Wall Street Journal using the standard splits for the training set (section 2-21), development set (section 22) and test set (section 23).", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 35, "end_pos": 54, "type": "DATASET", "confidence": 0.9217615524927775}]}, {"text": "We converted the constituency trees to Stanford dependencies version 3.3.0 (de).", "labels": [], "entities": []}, {"text": "We used a CRFbased Part-of-Speech tagger to generate 5-fold jack-knifed Part-of-Speech tag annotation of the training set and used predicted tags on the development and test set.", "labels": [], "entities": []}, {"text": "The tagger reaches accuracy scores similar to the Stanford tagger () with 97.44% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9996743202209473}]}, {"text": "The unlabeled and labeled accuracy scores exclude punctuation marks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9854469299316406}]}, {"text": "Obviously, there are many interesting instantiations for the generalized transition system.", "labels": [], "entities": []}, {"text": "In particular, it would be interesting to investigate parsing performance of systems with different active token size and arc-distance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.976658046245575}]}, {"text": "Before we investigate these system in the next subsections, we present the performance on standard systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: State-of-the-art comparison. denotes  our own re-implementation. The systems in the  first block on the top use neural networks.", "labels": [], "entities": []}]}