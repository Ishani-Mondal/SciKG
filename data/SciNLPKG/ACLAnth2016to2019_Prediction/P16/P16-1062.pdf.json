{"title": [{"text": "Effects of Creativity and Cluster Tightness on Short Text Clustering Performance", "labels": [], "entities": []}], "abstractContent": [{"text": "Properties of corpora, such as the diversity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short texts.", "labels": [], "entities": []}, {"text": "We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms.", "labels": [], "entities": []}, {"text": "We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for k-means clustering of a linguistically creative dataset, but do not help with less creative texts.", "labels": [], "entities": []}, {"text": "Yet the choice of similarity metric interacts with the choice of clustering method.", "labels": [], "entities": []}, {"text": "We find that graph-based clustering methods perform well on tightly clustered data but poorly on loosely clustered data.", "labels": [], "entities": []}, {"text": "Semantic similarity met-rics generate loosely clustered output even when applied to a tightly clustered dataset.", "labels": [], "entities": []}, {"text": "Thus, the best performing clustering systems could not use semantic metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpora of collective discourse-texts generated by multiple authors in response to the same stimulus-have varying properties depending on the stimulus and goals of the authors.", "labels": [], "entities": []}, {"text": "For instance, when multiple puzzle-composers write crossword puzzle clues for the same word, they will try to write creative, unique clues to make the puzzle interesting and challenging; clues for \"star\" could be \"Paparazzi's target\" or \"Sky light.\"", "labels": [], "entities": []}, {"text": "In contrast, people writing a descriptive caption fora photograph can adopt a less creative style.", "labels": [], "entities": []}, {"text": "Corpora may also differ on how similar texts within a particular class are to one another, compared to how similar they are to texts from other classes.", "labels": [], "entities": []}, {"text": "For example, entries in a cartoon captioning contest that all relate to the same cartoon may vary widely in subject, while crossword clues for the same word would likely be more tightly clustered.", "labels": [], "entities": [{"text": "cartoon captioning contest", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8369274934132894}]}, {"text": "This paper studies how such text properties affect the best method of clustering short texts.", "labels": [], "entities": [{"text": "clustering short texts", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.8973050713539124}]}, {"text": "Choosing how to cluster texts involves two major decisions: choosing a similarity metric to determine which texts are alike, and choosing a clustering method to group those texts.", "labels": [], "entities": []}, {"text": "We hypothesize that creativity may drive authors to express the same concept in a wide variety of ways, leading to data that can benefit from different similarity metrics than less creative texts.", "labels": [], "entities": []}, {"text": "At the same time, we hypothesize that tightly clustered datasets-datasets where each text is much more similar to texts in its cluster than to texts from other clusters-can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data.", "labels": [], "entities": []}, {"text": "This paper explores the interaction of these effects.", "labels": [], "entities": []}, {"text": "Recently, distributional semantics has been popular and successful for measuring text similarity.", "labels": [], "entities": []}, {"text": "Word embeddings represent similar words in similar locations in vector space: \"cat\" is closer to \"feline\" than to \"bird.\"", "labels": [], "entities": []}, {"text": "It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways.", "labels": [], "entities": []}, {"text": "And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity measures such as summed word embeddings and deep neural networks can have an advantage over more traditional similarity metrics, such as n-gram counts, n-gram tf-idf vectors, and dependency tree kernels, when applied to creative texts.", "labels": [], "entities": []}, {"text": "However, unlike inmost text similarity tasks, in clustering the choice of similarity metric interacts with both the choice of clustering method and the properties of the text.", "labels": [], "entities": []}, {"text": "Graph-based clustering techniques can be quite effective in clustering short texts), yet this paper will show that they are sensitive to how tightly clustered the data is.", "labels": [], "entities": [{"text": "clustering short texts", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8808326125144958}]}, {"text": "Moreover, the tightness of clusters in a dataset is a property of both the underlying data and the similarity metric.", "labels": [], "entities": []}, {"text": "We show that when the underlying data can be clustered tightly enough to use powerful graph-based clustering methods, using semantics-based similarity metrics actually creates a disadvantage compared to methods that rely on the surface form of the text, because semantic metrics reduce tightness.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes related work.", "labels": [], "entities": []}, {"text": "Section 3 describes four datasets of short texts.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the similarity metrics and clustering methods used in our experiments, as well as the evaluation measures.", "labels": [], "entities": []}, {"text": "Section 5 shows that semantics-based similarity metrics have some advantage when clustering short texts from the most creative dataset, but ultimately do not perform the best when graph-based clustering is an option.", "labels": [], "entities": []}, {"text": "In Section 6, we demonstrate the powerful effect that tightness of clusters has on the best combination of similarity metric and clustering method fora given dataset.", "labels": [], "entities": []}, {"text": "Finally, Section 7 draws conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Collective discourse () involves multiple writers generating texts in response to the same stimulus.", "labels": [], "entities": []}, {"text": "Ina corpus of texts relating to several stimuli, it maybe desirable to cluster according to which stimulus each text relates to-for instance, grouping all of the news headlines about the same event together.", "labels": [], "entities": []}, {"text": "Here, we consider texts triggered by several types of stimuli: photographs that need descriptive captions, cartoons that need humorous captions, and crossword answers that need original clues.", "labels": [], "entities": []}, {"text": "Each need shapes the properties of the texts.", "labels": [], "entities": []}, {"text": "The Pascal Captions dataset (hereinafter PAS) and the 8K ImageFlickr dataset) are sets of captions solicited from Mechanical Turkers for photographs from Flickr and from the Pat-tern Analysis, Statistical Modeling, and Computational Learning (PASCAL) Visual Object Classes Challenge (.", "labels": [], "entities": [{"text": "Pascal Captions dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.69928906361262}, {"text": "PAS", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.670640766620636}, {"text": "8K ImageFlickr dataset", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.7890140215555826}, {"text": "Statistical Modeling, and Computational Learning (PASCAL) Visual Object Classes Challenge", "start_pos": 193, "end_pos": 282, "type": "TASK", "confidence": 0.7122157766268804}]}, {"text": "PAS includes twenty categories of images (e.g., dogs, as in Example \"a man walking a small dog on a very wavy beach\" \"A person in a large black coats walks a white dog on the beach through rough waves.\"", "labels": [], "entities": []}, {"text": "\"Walking a dog on the edge of the ocean\" This task did not encourage creativity; instructions said to \"describe the image in one complete but simple sentence.\"", "labels": [], "entities": []}, {"text": "This could lead to sentences within a cluster being rather similar to each other.", "labels": [], "entities": []}, {"text": "However, because photographs may contain overlapping elements-for instance, a photograph in the \"bus\" category of PAS might also show cars, while a photograph in the \"cars\" category could also contain a bus-texts in one cluster can also be quite similar to texts from other clusters.", "labels": [], "entities": []}, {"text": "Thus, these datasets should not be very tightly clustered.", "labels": [], "entities": []}, {"text": "The New Yorker magazine has a weekly competition in which readers submit possible captions fora captionless cartoon (Example (2)) (.", "labels": [], "entities": [{"text": "The New Yorker magazine", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.7875511050224304}]}, {"text": "We use the cartoon each caption is associated with as its gold standard cluster.", "labels": [], "entities": []}, {"text": "The complete dataset includes over 1.9 million captions for 366 cartoons.", "labels": [], "entities": []}, {"text": "For this work, we use a total of 5000 captions from 20 randomly selected cartoons as the \"TOON\" dataset.", "labels": [], "entities": [{"text": "TOON\" dataset", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.7677587866783142}]}, {"text": "Since caption writers seek to standout from the crowd, we expect high creativity.", "labels": [], "entities": []}, {"text": "This may encourage a more varied vocabulary than the FLK and PAS captions that merely describe the image.", "labels": [], "entities": [{"text": "FLK", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.8064420223236084}]}, {"text": "We also expect wide variation in the meanings of captions for the same cartoon, due to the different joke senses submitted for each, leading to low intra-cluster similarity.", "labels": [], "entities": []}, {"text": "Moreover, some users may submit the same caption for more than one cartoon, so we can expect surprisingly high intercluster similarity despite the wide variation in cartoon prompt images.", "labels": [], "entities": []}, {"text": "We therefore do not expect TOON to be tightly clustered.", "labels": [], "entities": [{"text": "TOON", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.5951926112174988}]}, {"text": "A dataset of particularly creative texts is comprised of crossword clues.", "labels": [], "entities": []}, {"text": "We use the clues as texts and the answer words as their gold standard cluster; all of the clues in Example (3) belong to the \"toe\" cluster.", "labels": [], "entities": [{"text": "Example", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.8346676826477051}]}, {"text": "The complete crossword clues dataset includes 1.7M different clues corresponding to 174,638 unique answers.", "labels": [], "entities": []}, {"text": "The \"CLUE\" dataset includes 5000 clues corresponding to 20 unique answers selected by randomly choosing answers that have 250 or more unique clues, and then randomly choosing 250 of those clues for each answer.", "labels": [], "entities": [{"text": "CLUE\" dataset", "start_pos": 5, "end_pos": 18, "type": "DATASET", "confidence": 0.6725923120975494}]}, {"text": "Since words repeat, crossword authors must be creative to come up with clues that will not bore cruciverbalists.", "labels": [], "entities": []}, {"text": "CLUE should thus contain many alternative phrasings for essentially the same idea.", "labels": [], "entities": [{"text": "CLUE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9189909100532532}]}, {"text": "At the same time, there is likely to be relatively little overlap between clues for different answers, so CLUE should be tightly clustered.", "labels": [], "entities": []}, {"text": "Adjusted Rand Index We use the sklearn implementation of the Adjusted Rand Index (ARI) (: where RI is the Rand Index, T P is the number of true positives, TN is true negatives, and F P and F N are false positives and false negatives, respectively.", "labels": [], "entities": [{"text": "Adjusted Rand Index (ARI)", "start_pos": 61, "end_pos": 86, "type": "METRIC", "confidence": 0.752696360150973}]}, {"text": "The Rand Index ranges from 0 to 1.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9292470216751099}]}, {"text": "ARI adjusts the Rand Index for chance, so that the score ranges from -1 to 1.", "labels": [], "entities": [{"text": "ARI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8555412888526917}, {"text": "Rand Index", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9591233134269714}, {"text": "chance", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9801614880561829}]}, {"text": "Random labeling will achieve an ARI score close to 0; perfect labeling achieves an ARI of 1.", "labels": [], "entities": [{"text": "ARI score", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.979938417673111}, {"text": "ARI", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9983501434326172}]}, {"text": "Purity is a score in the range that indicates to what extent sentences in the same predicted cluster actually belong to the same cluster.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9918083548545837}]}, {"text": "Given \u2126 = {\u03c9 1 , \u03c9 2 , ..., \u03c9 K }, the predicted clusters, C = {c 1 , c 2 , ..., c J }, the true clusters, and N , the number of examples, purity is Normalized Mutual Information (NMI).", "labels": [], "entities": [{"text": "purity", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9919249415397644}]}, {"text": "We use the sklearn implementation of NMI: The numerator is the mutual information (MI) of predicted cluster labels \u2126 and true cluster labels C.", "labels": [], "entities": [{"text": "mutual information (MI)", "start_pos": 63, "end_pos": 86, "type": "METRIC", "confidence": 0.8193070530891419}]}, {"text": "MI describes how much knowing what the predicted clusters are increases knowledge about what the actual classes are.", "labels": [], "entities": [{"text": "MI", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6796178221702576}]}, {"text": "Using marginal entropy (H(x)), NMI normalizes MI so that it ranges from 0 to 1.", "labels": [], "entities": [{"text": "MI", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.935275673866272}]}, {"text": "If C and \u2126 are identical-that is, if the clusters are perfect-NMI will be 1.", "labels": [], "entities": []}, {"text": "We hypothesize that if a dataset uses a wide variety of words to express the same ideas, similarity metrics that rely on the surface form of the sentence will beat a disadvantage compared to similarity metrics based in distributional semantics.", "labels": [], "entities": []}, {"text": "Thus, word2vec, LSTM autoencoders, and skip-thoughts ought to perform better than the n-gram-based methods and dependency count method when applied to CLUE, but should enjoy no advantage when applied to PAS.", "labels": [], "entities": []}, {"text": "We begin by comparing the performance of all similarity metrics on PAS and CLUE, using kmeans for clustering.", "labels": [], "entities": [{"text": "CLUE", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8432067632675171}]}, {"text": "We then also examine their performance with MCL.", "labels": [], "entities": []}, {"text": "compares the performance of all similarity metrics on PAS and CLUE using k-means and MCL.", "labels": [], "entities": []}, {"text": "Using k-means on PAS, the unigram tf-idf similarity metric gives the strongest performance for purity and NMI and came in a close second for ARI.", "labels": [], "entities": [{"text": "unigram tf-idf similarity metric", "start_pos": 26, "end_pos": 58, "type": "METRIC", "confidence": 0.6174214854836464}, {"text": "purity", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9970863461494446}, {"text": "NMI", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9931694269180298}, {"text": "ARI", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.5585894584655762}]}, {"text": "LSTM slightly outperformed the other similarity metrics on ARI, but had middle-of-theroad results on the other evaluations.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5124054551124573}, {"text": "ARI", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8152132034301758}]}, {"text": "Overall, the semantics-based similarity metrics gave reasonable but not exceptional ARI and purity results, but were at the low end on NMI.", "labels": [], "entities": [{"text": "ARI", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9627178311347961}, {"text": "purity", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9590318202972412}, {"text": "NMI", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.9169924259185791}]}, {"text": "This is consistent with our hypothesis that when authors are not trying to express creativity by using a wider vocabulary, surface-based similarity metrics suffice.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Vocabulary properties of each dataset", "labels": [], "entities": []}, {"text": " Table 2: A comparison of all similarity metrics on PAS and CLUE datasets, clustered using k-means and  MCL. For all evaluations, higher scores are better.", "labels": [], "entities": [{"text": "CLUE datasets", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8464913964271545}]}, {"text": " Table 3: Modularity for all datasets", "labels": [], "entities": []}]}