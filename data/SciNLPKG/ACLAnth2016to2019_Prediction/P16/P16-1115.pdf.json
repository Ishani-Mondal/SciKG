{"title": [{"text": "Resolving References to Objects in Photographs using the Words-As-Classifiers Model", "labels": [], "entities": [{"text": "Resolving References to Objects in Photographs", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.9006357689698538}]}], "abstractContent": [{"text": "A common use of language is to refer to visually present objects.", "labels": [], "entities": []}, {"text": "Modelling it in computers requires modelling the link between language and perception.", "labels": [], "entities": []}, {"text": "The \"words as classifiers\" model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words.", "labels": [], "entities": []}, {"text": "It was recently shown to perform well in a game-playing scenario with a small number of object types.", "labels": [], "entities": []}, {"text": "We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available.", "labels": [], "entities": []}, {"text": "Using a pre-trained convolu-tional neural network to extract image region features, and augmenting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible .", "labels": [], "entities": [{"text": "reference resolution task", "start_pos": 221, "end_pos": 246, "type": "TASK", "confidence": 0.7971665263175964}]}], "introductionContent": [{"text": "A common use of language is to refer to objects in the shared environment of speaker and addressee.", "labels": [], "entities": []}, {"text": "Being able to simulate this is of particular importance for verbal human/robot interfaces (HRI), and the task has consequently received some attention in this field ().", "labels": [], "entities": [{"text": "verbal human/robot interfaces (HRI)", "start_pos": 60, "end_pos": 95, "type": "TASK", "confidence": 0.6433676555752754}]}, {"text": "Here, we study a somewhat simpler precursor task, namely that of resolution of reference to objects in static images (photographs), but use a larger set of object types than is usually done in HRI work (> 300, see below).", "labels": [], "entities": [{"text": "resolution of reference to objects in static images (photographs)", "start_pos": 65, "end_pos": 130, "type": "TASK", "confidence": 0.8000530058687384}]}, {"text": "More formally, the task is to retrieve, given a referring expression e and an image I, the region bb * of the image that is most likely to contain the referent of the expression.", "labels": [], "entities": []}, {"text": "As candidate regions, we use both manually annotated regions as well as automatically computed ones.", "labels": [], "entities": []}, {"text": "As our starting point, we use the \"words-asclassifiers\" model recently proposed by.", "labels": [], "entities": []}, {"text": "It has before only been tested in a small domain and with specially designed features; here, we apply it to real-world photographs and use learned representations from a convolutional neural network (.", "labels": [], "entities": []}, {"text": "We learn models for between 400 and 1,200 words, depending on the training data set.", "labels": [], "entities": []}, {"text": "As we show, the model performs competitive with the state of the art ( on the same data sets.", "labels": [], "entities": []}, {"text": "Our background interest in situated interaction makes it important for us that the approach we use is 'dialogue ready'; and it is, in the sense that it supports incremental processing (giving results while the incoming utterance is going on) and incremental learning (being able to improve performance from interactive feedback).", "labels": [], "entities": []}, {"text": "However, in this paper we focus purely on 'batch', noninteractive performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "The task in our experiments is the following: Given an image I together with bounding boxes of regions (bb 1 , . .", "labels": [], "entities": []}, {"text": ", bb n ) within it, and a referring expression e, predict which of these regions contains the referent of the expression.", "labels": [], "entities": []}, {"text": "By Corpus We start with training and testing models for all three corpora (REFERIT, REFCOCO, GREXP) separately.", "labels": [], "entities": [{"text": "REFERIT", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9354670643806458}, {"text": "REFCOCO", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9518424868583679}, {"text": "GREXP", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9760103821754456}]}, {"text": "But first, we establish some baselines.", "labels": [], "entities": []}, {"text": "The first is just randomly picking one of the candidate regions.", "labels": [], "entities": []}, {"text": "The second is a 1-rule classifier that picks the largest region.", "labels": [], "entities": []}, {"text": "The respective accuracies on the corpora are as follows: REFERIT 0.20/0.19; REFCOCO 0.16/0.23; GREXP 0.19/0.20.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9843385815620422}, {"text": "REFERIT", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.995364785194397}, {"text": "REFCOCO", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9790205359458923}, {"text": "GREXP", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9607970118522644}]}, {"text": "Training on the training sets of REFERIT, REF-COCO and GREX with the regime described above (min.", "labels": [], "entities": [{"text": "REFERIT", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9184241890907288}, {"text": "REF-COCO", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.7295745015144348}, {"text": "GREX", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9893472194671631}]}, {"text": "40 occurrences) gives us classifiers for 429, 503, and 682 words, respectively.", "labels": [], "entities": []}, {"text": "shows the evaluation on the respective test parts: accuracy (acc), mean reciprocal rank (mrr) and for how much of the expression, on average, a word classifier is present (arc).", "labels": [], "entities": [{"text": "accuracy (acc)", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.8202230930328369}, {"text": "mean reciprocal rank (mrr)", "start_pos": 67, "end_pos": 93, "type": "METRIC", "confidence": 0.9096430540084839}]}, {"text": "'>0' shows how much of the testcorpus is left if expressions are filtered out for which not even a single word is the model (which we evaluate by default as false), and accuracy for that reduced set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.999687671661377}]}, {"text": "The 'NR' rows give the same numbers for reduced test sets in which all relational expressions have been removed; '%tst' shows how much of a reduction that is relative to the full testset.", "labels": [], "entities": []}, {"text": "The rows with the citations give the best reported results from the literature.", "labels": [], "entities": []}, {"text": "As this shows, inmost cases we come close, but do not quite reach these results.", "labels": [], "entities": []}, {"text": "The distance is the biggest for GREXP with its much longer expressions.", "labels": [], "entities": [{"text": "GREXP", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.7169020771980286}]}, {"text": "As discussed above, not only are the descriptions longer on average in this corpus, the vocabulary size is also much higher.", "labels": [], "entities": []}, {"text": "Many of the descriptions contain action descriptions (\"the man smiling at the woman\"), which do not seem to be as helpful to our model.", "labels": [], "entities": []}, {"text": "Overall, the expressions in this corpus do appear to be more like 'mini-captions' describing the region rather than referring expressions that efficiently single it out among the set of distractors; our model tries to capture the latter.", "labels": [], "entities": []}, {"text": "Combining Corpora A nice effect of our setup is that we can freely mix the corpora for training, as image regions are represented in the same way regardless of source corpus, and we can combine occurrences of a word across corpora.", "labels": [], "entities": []}, {"text": "We tested combining the testsets of REFERIT and RE-FCOCO (RI+RC in the Table below), REFCOCO and GREXP (RC+GR), and all three (REFERIT, REF-COCO, and GREXP; RI+RC+GR), yielding models for 793, 933, 1215 words, respectively (with the same \"min. 40 occurrences\" criterion).", "labels": [], "entities": [{"text": "REFERIT", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.884231686592102}, {"text": "RE-FCOCO", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9503956437110901}, {"text": "GREXP", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9867773652076721}]}, {"text": "For all testsets, the results were at least stable compared to, for some they improved.", "labels": [], "entities": []}, {"text": "For reasons of space, we only show the improvements here.", "labels": [], "entities": []}, {"text": "Computed Region Proposals Here, we cannot expect the system to retrieve exactly the ground truth bounding box, since we cannot expect the set of automatically computed regions to contain it.", "labels": [], "entities": []}, {"text": "We follow in using intersection over union (IoU) as metric (the size of the intersective area between candidate and ground truth bounding box normalised by the size of the union) and taking an IoU \u2265 0.5 of the top candidate as a threshold for success (P@1).", "labels": [], "entities": []}, {"text": "As a more relaxed metric, we also count for the SAIAPR proposals (of which there are 100 per image) as success when at least one among the top 10 candidates exceeds this IoU threshold (R@10: Results on region proposals With the higher quality proposals provided for the MSCOCO data, and the shorter, more prototypical referring expressions from REFCOCO, we narrowly beat the reported results.", "labels": [], "entities": [{"text": "IoU", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.9840061068534851}, {"text": "R", "start_pos": 185, "end_pos": 186, "type": "METRIC", "confidence": 0.9529156684875488}, {"text": "MSCOCO data", "start_pos": 270, "end_pos": 281, "type": "DATASET", "confidence": 0.7258705496788025}]}, {"text": "(Again, note that we use a different split that ensures separation on the level of images between training and test.)", "labels": [], "entities": []}, {"text": "() performs relatively better on the region proposals (the gap is wider), on GREXP, we come relatively closer using these proposals.", "labels": [], "entities": [{"text": "GREXP", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8817964196205139}]}, {"text": "We can speculate that using automatically computed boxes of a lower selectivity (REFERIT) shifts the balance between needing to actually recognise the image and getting information from the shape and position of the box (our positional features; see Section 5).", "labels": [], "entities": [{"text": "REFERIT", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9982820749282837}]}, {"text": "Ablation Experiments To get an idea about what the classifiers actually pickup on, we trained variants given only the positional features (POS columns below in) and only object features (NOPOS columns).", "labels": [], "entities": []}, {"text": "We also applied a variant of the model with only the top 20 classifiers (in terms of number of positive training examples; TOP20).", "labels": [], "entities": [{"text": "TOP20", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.5683335065841675}]}, {"text": "We only show accuracy here, and repeat the relevant numbers from   This table shows an interesting pattern.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996604919433594}]}, {"text": "To a large extent, the object image features and the positional features seem to carry redundant information, with the latter on their own performing better than the former on their own.", "labels": [], "entities": []}, {"text": "The full model, however, still gains something from the combination of the feature types.", "labels": [], "entities": []}, {"text": "The top-20 classifiers (and consequently, top 20 most frequent words) alone reach decent performance (the numbers are shown for the full test set here; if reduced to only utterances whereat least one word is known, the numbers rise, but the reduction of the testset is much more severe than for the full models with much larger vocabulary).", "labels": [], "entities": []}, {"text": "Error Analysis shows the accuracy of the model split by length of the referring expression (top lines; lower lines show the proportion of expression of this length in the whole corpus).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9996799230575562}]}, {"text": "The pattern is similar for all corpora (but less pronounced for GREXP): shorter utterances fare better.", "labels": [], "entities": [{"text": "GREXP", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.4615389406681061}]}, {"text": "Manual inspection of the errors made by the system further corroborates the suspicion that composition as done here neglects too much of the internal structure of the expression.", "labels": [], "entities": []}, {"text": "An example from REFERIT where we get a wrong prediction is \"second person from left\".", "labels": [], "entities": [{"text": "REFERIT", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.6088005304336548}]}, {"text": "The model clearly does not have a notion of counting, and here it wrongly selects the leftmost person.", "labels": [], "entities": []}, {"text": "Ina similar vein, we gave results above fora testset where spatial relations where removed, but other forms of relation (e.g., \"child sitting on womans lap\") that weren't modelled still remain in the corpus.", "labels": [], "entities": []}, {"text": "We see as an advantage of the model that we can inspect words individually.", "labels": [], "entities": []}, {"text": "Given the performance of short utterances, we can conclude that the word/object classifiers themselves perform reasonably well.", "labels": [], "entities": []}, {"text": "This seems to be somewhat independent of the number of training examples they received.", "labels": [], "entities": []}, {"text": "shows, for REFERIT, # training instances (x-axis) vs. average accuracy on the validation set, for the whole vocabulary.", "labels": [], "entities": [{"text": "REFERIT", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9156215786933899}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9988380074501038}]}, {"text": "As this shows, the classifiers tend to get better with more training instances, but there are good ones even with very little training material.", "labels": [], "entities": []}, {"text": ") This is, to a degree, as expected: our assumption behind training classifiers for all ocurring words and not pre-filtering based on their part-of-speech or prior hypotheses about visual relevance was that words that can occur in all kinds of visual contexts will lead to classifiers whose contributions cancel out across all candidate objects in a scene.", "labels": [], "entities": []}, {"text": "However, the mean average precision of the classifiers for colour words is also relatively low at 0.6 (std 0.08), for positional words (\"left\", \"right\", \"center\", etc.) it is 0.54 (std 0.1).", "labels": [], "entities": [{"text": "mean average", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.8995719850063324}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.5052111148834229}]}, {"text": "This might suggest that the features we take from the CNN might indeed be more appropriate for tasks close to what they were originally trained on, namely category and not attribute prediction.", "labels": [], "entities": [{"text": "attribute prediction", "start_pos": 172, "end_pos": 192, "type": "TASK", "confidence": 0.7188209891319275}]}, {"text": "We will explore this in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results; separately by corpus. See text  for description of columns and rows.", "labels": [], "entities": []}, {"text": " Table 2: Results, combined corpora", "labels": [], "entities": []}, {"text": " Table 3: Results on region proposals", "labels": [], "entities": []}, {"text": " Table 4: Results with reduced models", "labels": [], "entities": []}]}