{"title": [{"text": "Incremental Parsing with Minimal Features Using Bi-Directional LSTM", "labels": [], "entities": [{"text": "Incremental Parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8883342146873474}]}], "abstractContent": [{"text": "Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition , and potentially omitting important global context.", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.9747826457023621}]}, {"text": "To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence.", "labels": [], "entities": []}, {"text": "This model achieves state-of-the-art results among greedy dependency parsers for English.", "labels": [], "entities": []}, {"text": "We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both En-glish and Chinese.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8555851578712463}]}], "introductionContent": [{"text": "Recently, neural network-based parsers have become popular, with the promise of reducing the burden of manual feature engineering.", "labels": [], "entities": []}, {"text": "For example, and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts () by vector embeddings of the atomic features.", "labels": [], "entities": []}, {"text": "However, this approach has two related limitations.", "labels": [], "entities": []}, {"text": "First, it still depends on a large number of carefully designed atomic features.", "labels": [], "entities": []}, {"text": "For example, and subsequent work such as use 48 atomic features from, including select thirdorder dependencies.", "labels": [], "entities": []}, {"text": "More importantly, this approach inevitably leaves out some nonlocal information which could be useful.", "labels": [], "entities": []}, {"text": "In particular, though such a model can exploit similarities between words and other embedded categories, and learn interactions among those atomic features, it cannot exploit any other details of the text.", "labels": [], "entities": []}, {"text": "We aim to reduce the need for manual induction of atomic features to the bare minimum, by using bi-directional recurrent neural networks to automatically learn context-sensitive representations for each word in the sentence.", "labels": [], "entities": []}, {"text": "This approach allows the model to learn arbitrary patterns from the entire sentence, effectively extending the generalization power of embedding individual words to longer sequences.", "labels": [], "entities": []}, {"text": "Since such a feature representation is less dependent on earlier parser decisions, it is also more resilient to local mistakes.", "labels": [], "entities": []}, {"text": "With just three positional features we can build a greedy shift-reduce dependency parser that is on par with the most accurate parser in the published literature for English Treebank.", "labels": [], "entities": [{"text": "English Treebank", "start_pos": 166, "end_pos": 182, "type": "DATASET", "confidence": 0.9817356169223785}]}, {"text": "This effort is similar in motivation to the stack-LSTM of, but uses a much simpler architecture.", "labels": [], "entities": []}, {"text": "We also extend this model to predict phrasestructure trees with a novel shift-promote-adjoin system tailored to greedy constituency parsing, and with just two more positional features (defining tree span) and nonterminal label embeddings we achieve the most accurate greedy constituency parser for both English and Chinese.", "labels": [], "entities": []}, {"text": "It is also common to stack multiple such LSTM layers, where the output of the forward and backward networks atone layer are concatenated to form the input to the next.", "labels": [], "entities": []}, {"text": "We found that parsing performance could be improved by using two bidirectional LSTM layers in this manner, and concatenating the output of both layers as the positional feature representation, which becomes the input to the fully-connected layer.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9796125888824463}]}, {"text": "This architec-  ture is shown in.", "labels": [], "entities": []}, {"text": "Intuitively, this represents the sentence position by the word in the context of the sentence up to that point and the sentence after that point in the first layer, as well as modeling the \"higher-order\" interactions between parts of the sentence in the second layer.", "labels": [], "entities": []}, {"text": "In Section 5 we report results using only one LSTM layer (\"Bi-LSTM\") as well as with two layers where output from each layer is used as part of the positional feature (\"2-Layer Bi-LSTM\").", "labels": [], "entities": []}], "datasetContent": [{"text": "We report both dependency and constituency parsing results on both English and Chinese.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.6845115274190903}]}, {"text": "All experiments were conducted with minimal hyperparameter tuning.", "labels": [], "entities": []}, {"text": "The settings used for the reported results are summarized in.", "labels": [], "entities": []}, {"text": "Networks parameters were updated using gradient backpropagation, including backpropagation through time for the recurrent components, using ADADELTA for learning rate scheduling.", "labels": [], "entities": [{"text": "ADADELTA", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9192129969596863}]}, {"text": "We also applied dropout () (with p = 0.5) to the output of each LSTM layer (separately for each connection in the case of the two-layer network).", "labels": [], "entities": []}, {"text": "We tested both types of parser on the Penn Treebank (PTB) and Penn Chinese Treebank (CTB-5), with the standard splits for each of training, development, and test sets.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.9567862153053284}, {"text": "Penn Chinese Treebank (CTB-5)", "start_pos": 62, "end_pos": 91, "type": "DATASET", "confidence": 0.9548966189225515}]}, {"text": "Automatically predicted part of speech tags with 10-way jackknifing were used as inputs for all tasks except for Chinese dependency parsing, where we used gold tags, following the traditions in literature.", "labels": [], "entities": [{"text": "Chinese dependency parsing", "start_pos": 113, "end_pos": 139, "type": "TASK", "confidence": 0.6391244331995646}]}, {"text": "shows results for English Penn Treebank using Stanford dependencies.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 18, "end_pos": 39, "type": "DATASET", "confidence": 0.9352338512738546}]}, {"text": "Despite the minimally designed feature representation, relatively few training iterations, and lack of precomputed embeddings, the parser performed on par with state-of-the-art incremental dependency parsers, and slightly outperformed the state-ofthe-art greedy parser.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Development and test set results for shift- reduce dependency parser on Penn Treebank using  only (s 1 , s 0 , q 0 ) positional features.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9951002895832062}]}, {"text": " Table 3: Ablation studies on PTB dev set (wsj  22). Forward and backward context, and part-of- speech input were all critical to strong performace.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9724869132041931}, {"text": "PTB dev set", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8075042565663656}]}, {"text": " Table 4: Development and test set results for shift- reduce dependency parser on Penn Chinese Tree- bank (CTB-5) using only (s 1 , s 0 , q 0 ) position fea- tures (trained and tested with gold POS tags).", "labels": [], "entities": [{"text": "Penn Chinese Tree- bank (CTB-5)", "start_pos": 82, "end_pos": 113, "type": "DATASET", "confidence": 0.9690834805369377}]}, {"text": " Table 5: Test F-scores for constituency parsing on  Penn Treebank and CTB-5. Dependency Constituency  Embeddings  Word (dims)  50  100  Tags (dims)  20  100  Nonterminals (dims)  - 100  Pretrained  No  No  Network details  LSTM units (each direction)  200  200  ReLU hidden units  200 / decision  1000  Training  Training epochs  10  10  Minibatch size (sentences)  10  10  Dropout (LSTM output only)  0.5  0.5  L2 penalty (all weights)  none  1 \u00d7 10 \u22128  ADADELTA \u03c1  0.99  0.99  ADADELTA  1 \u00d7 10 \u22127  1 \u00d7 10 \u22127", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8283936083316803}, {"text": "Penn Treebank", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9954725205898285}, {"text": "CTB-5", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.5516706109046936}, {"text": "ADADELTA \u03c1  0.99  0.99  ADADELTA  1", "start_pos": 456, "end_pos": 491, "type": "METRIC", "confidence": 0.8929163614908854}]}, {"text": " Table 6: Hyperparameters and training settings.", "labels": [], "entities": []}]}