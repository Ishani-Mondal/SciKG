{"title": [{"text": "Using Sentence-Level LSTM Language Models for Script Inference", "labels": [], "entities": []}], "abstractContent": [{"text": "There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents.", "labels": [], "entities": []}, {"text": "These systems operate on struc-tured verb-argument events produced by an NLP pipeline.", "labels": [], "entities": []}, {"text": "We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.", "labels": [], "entities": [{"text": "predicting missing events in documents", "start_pos": 193, "end_pos": 231, "type": "TASK", "confidence": 0.8476581692695617}]}], "introductionContent": [{"text": "Statistical scripts are probabilistic models of event sequences.", "labels": [], "entities": []}, {"text": "A learned script model is capable of processing a document and inferring events that are probable but not explicitly stated.", "labels": [], "entities": []}, {"text": "These models operate on automatically extracted structured events (for example, verbs with entity arguments), which are derived from standard NLP tools such as dependency parsers and coreference resolution engines.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 183, "end_pos": 205, "type": "TASK", "confidence": 0.8845202624797821}]}, {"text": "Recent work has demonstrated that standard sequence models applied to such extracted event sequences, e.g. discriminative language models and Long Short Term Memory (LSTM) recurrent neural nets (, are able to infer held-out events more accurately than previous approaches.", "labels": [], "entities": []}, {"text": "These results call into question the extent to which statistical event inference systems require linguistic preprocessing and syntactic structure.", "labels": [], "entities": []}, {"text": "In an attempt to shed light on this issue, we compare existing script models to LSTMs trained as sentencelevel language models which try to predict the sequence of words in the next sentence from a learned representation of the previous sentences using no linguistic preprocessing.", "labels": [], "entities": []}, {"text": "Some prior statistical script learning systems are focused on knowledge induction.", "labels": [], "entities": [{"text": "knowledge induction", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8165752291679382}]}, {"text": "These systems are primarily designed to induce collections of co-occurring event types involving the same entities, and their ability to infer held-out events is not their primary intended purpose, inter alia).", "labels": [], "entities": []}, {"text": "In the present work, we instead investigate the behavior of systems trained to directly optimize performance on the task of predicting subsequent events; in other words, we are investigating statistical models of events in discourse.", "labels": [], "entities": []}, {"text": "Much prior research on statistical script learning has also evaluated on inferring missing events from documents.", "labels": [], "entities": [{"text": "statistical script learning", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6028246879577637}]}, {"text": "However, the exact form that this task takes depends on the adopted definition of what constitutes an event: in previous work, events are defined in different ways, with differing degrees of structure.", "labels": [], "entities": []}, {"text": "We consider simply using raw text, which requires no explicit syntactic annotation, as our mediating representation, and evaluate how raw text models compare to models of more structured events.", "labels": [], "entities": []}, {"text": "introduced skip-thought vector models, in which an RNN is trained to encode a sentence within a document into a lowdimensional vector that supports predicting the neighboring sentences in the document.", "labels": [], "entities": []}, {"text": "Though the objective function used to train networks maximizes performance on the task of predicting sentences from their neighbors, do not evaluate directly on the ability of networks to predict text; they instead demonstrate that the intermediate low-dimensional vector embeddings are useful for other tasks.", "labels": [], "entities": [{"text": "predicting sentences from their neighbors", "start_pos": 90, "end_pos": 131, "type": "TASK", "confidence": 0.8489024400711059}]}, {"text": "We directly evaluate the text predictions produced by such sentence-level RNN encoder-decoder models, and measure their utility for the task of predicting subsequent events.", "labels": [], "entities": []}, {"text": "We find that, on the task of predicting the text of held-out sentences, the systems we train to operate on the level of raw text generally outperform the systems we train to predict text mediated by automatically extracted event structures.", "labels": [], "entities": [{"text": "predicting the text of held-out sentences", "start_pos": 29, "end_pos": 70, "type": "TASK", "confidence": 0.8464284241199493}]}, {"text": "On the other hand, if we run an NLP pipeline on the automatically generated text and extract structured events from these predictions, we achieve prediction performance roughly comparable to that of systems trained to predict events directly.", "labels": [], "entities": []}, {"text": "The difference between word-level and event-level models on the task of event prediction is marginal, indicating that the task of predicting the next event, particularly in an encoder-decoder setup, may not necessarily need to be mediated by explicit event structures.", "labels": [], "entities": [{"text": "event prediction", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7271382659673691}]}, {"text": "To our knowledge, this is the first effort to evaluate sentence-level RNN language models directly on the task of predicting document text.", "labels": [], "entities": [{"text": "predicting document text", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.8799382448196411}]}, {"text": "Our results show that such models are useful for predicting missing information in text; and the fact that they require no linguistic preprocessing makes them more applicable to languages where quality parsing and co-reference tools are not available.", "labels": [], "entities": [{"text": "predicting missing information in text", "start_pos": 49, "end_pos": 87, "type": "TASK", "confidence": 0.886095404624939}]}], "datasetContent": [{"text": "The evaluation of inference-focused statistical script systems is not straightforward.", "labels": [], "entities": []}, {"text": "introduced the Narrative Cloze evaluation, in which a single event is held out from a document and systems are judged by the ability to infer this held-out event given the remaining events.", "labels": [], "entities": []}, {"text": "This evaluation has been used by a number of published script systems.", "labels": [], "entities": []}, {"text": "This automated evaluation measures systems' ability to model and predict events as they co-occur in text.", "labels": [], "entities": []}, {"text": "The exact definition of the Narrative Cloze evaluation depends on the formulation of events used in a script system.", "labels": [], "entities": []}, {"text": "For example, Chambers and Jurafsky evaluate inference of verbs with noun information about multiple arguments.", "labels": [], "entities": []}, {"text": "In order to gather human judgments of inference quality, the latter also learn an encoder-decoder LSTM network for transforming verbs and noun arguments into English text to present to annotators for evaluation.", "labels": [], "entities": []}, {"text": "We evaluate instead on the task of directly inferring sequences of words.", "labels": [], "entities": []}, {"text": "That is, instead of defining the Narrative Cloze to be the evaluation of predictions of held-out events, we define the task to be the evaluation of predictions of held-out text; in this setup, predictions need not be mediated by noisy, automatically-extracted events.", "labels": [], "entities": []}, {"text": "To evaluate inferred text against gold standard text, we argue that the BLEU metric (, commonly used to evaluate Statistical Machine Translation systems, is a natural evaluation metric.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9831829369068146}, {"text": "Statistical Machine Translation", "start_pos": 113, "end_pos": 144, "type": "TASK", "confidence": 0.6758677462736765}]}, {"text": "It is an n-gram-level analog to the eventlevel Narrative Cloze evaluation: whereas the Narrative Cloze evaluates a system on its ability to reconstruct events as they occur in documents, BLEU evaluates a system on how well it reconstructs the n-grams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.9947224855422974}]}, {"text": "This evaluation takes some inspiration from the evaluation of neural encoder-decoder translation models (, which use similar architectures for the task of Machine Translation.", "labels": [], "entities": [{"text": "neural encoder-decoder translation", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.7369136412938436}, {"text": "Machine Translation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.8124596178531647}]}, {"text": "That is, the task we present can bethought of as \"translating\" a sentence into its successor.", "labels": [], "entities": []}, {"text": "While we do not claim that BLEU is necessarily the optimal way of evaluating text-level inferences, but we do claim that it is a natural ngram-level analog to the Narrative Cloze task on events.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.998288094997406}]}, {"text": "If a model infers text, we may also evaluate it on the task of inferring events by automatically extracting structured events from its output text (in the same way as events are extracted from natural text).", "labels": [], "entities": []}, {"text": "This allows us to compare directly to previous event-based models on the task they are optimized for, namely, predicting structured events.", "labels": [], "entities": [{"text": "predicting structured events", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.9085903366406759}]}, {"text": "We train a number of LSTM encoder-decoder networks which vary in their input and output.", "labels": [], "entities": []}, {"text": "Models are trained on English Language Wikipedia, with 1% of the documents held out as a validation set.", "labels": [], "entities": [{"text": "English Language Wikipedia", "start_pos": 22, "end_pos": 48, "type": "DATASET", "confidence": 0.8168665369351705}]}, {"text": "Our test set consists of 10,000 unseen sentences (from articles in neither the training nor validation set).", "labels": [], "entities": []}, {"text": "We train models with batch stochastic gradient descent with momentum, minimizing the cross-entropy error of output predictions.", "labels": [], "entities": []}, {"text": "All models are implemented in.", "labels": [], "entities": []}, {"text": "We use a vocabulary of the 50,000 most frequent tokens, replacing all other tokens with an out-of-vocabulary pseudo-token.", "labels": [], "entities": []}, {"text": "Learned word embeddings are 100-dimensional, and the latent LSTM vector is 500-dimensional.", "labels": [], "entities": []}, {"text": "To extract events from text, we use the Stanford Dependency Parser.", "labels": [], "entities": [{"text": "Stanford Dependency Parser", "start_pos": 40, "end_pos": 66, "type": "DATASET", "confidence": 0.9440710544586182}]}, {"text": "We use the Moses toolkit () to calculate BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9980123043060303}]}, {"text": "We evaluate the task of predicting held-out text with three metrics.", "labels": [], "entities": [{"text": "predicting held-out text", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.8941685557365417}]}, {"text": "The first metric is BLEU, which is standard BLEU (the geometric mean of modified 1-, 2-, 3-, and 4-gram precision against a gold standard, multiplied by a brevity penalty which penalizes short candidates).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9987510442733765}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9983910918235779}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9534245729446411}]}, {"text": "The second metric we present, BLEU-BP, is BLEU without the brevity Via the script multi-bleu.pl. penalty: in the task of predicting successor sentences, depending on predictions' end use, ontopic brevity is not necessarily undesirable.", "labels": [], "entities": [{"text": "BLEU-BP", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9980366826057434}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.998571515083313}, {"text": "predicting successor sentences", "start_pos": 121, "end_pos": 151, "type": "TASK", "confidence": 0.8660436272621155}]}, {"text": "Evaluations are over top system inferences (that is, decoding is done by taking the argmax).", "labels": [], "entities": [{"text": "argmax", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.974371612071991}]}, {"text": "Finally, we also present values for unigram precision (1G P), one of the components of BLEU.", "labels": [], "entities": [{"text": "unigram precision (1G P)", "start_pos": 36, "end_pos": 60, "type": "METRIC", "confidence": 0.7296249916156133}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9842585325241089}]}, {"text": "We also evaluate on the task of predicting heldout verb-argument events, either directly or via inferred text.", "labels": [], "entities": [{"text": "predicting heldout verb-argument events", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.86709363758564}]}, {"text": "We use two evaluation metrics for this task.", "labels": [], "entities": []}, {"text": "First, the Accuracy metric measures the percentage of a system's most confident guesses that are totally correct.", "labels": [], "entities": [{"text": "Accuracy metric", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.9760598838329315}]}, {"text": "That is, for each held-out event, a system makes its single most confident guess for that event, and we calculate the total percentage of such guesses which are totally correct.", "labels": [], "entities": []}, {"text": "Some authors (e.g.,) present results on the \"Recall at k\" metric, judging gold-standard recall against a list of top k event inferences; this metric is equivalent to \"Recall at 1.\"", "labels": [], "entities": [{"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9803047776222229}]}, {"text": "This is quite astringent metric, as an inference is only counted correct if the verb and all arguments are correct.", "labels": [], "entities": []}, {"text": "To relax this requirement, we also present results on what we call the Partial Credit metric, which is the percentage of held-out event components identical to the respective components in a system's top inference.", "labels": [], "entities": [{"text": "Partial Credit metric", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.5867876609166464}]}, {"text": "ply reproduces the input sentence as its own successor.", "labels": [], "entities": []}, {"text": "5 Below this are systems which make predictions from event information, with systems which make predictions from raw text underneath.", "labels": [], "entities": []}, {"text": "Transformations written X a Y are, recall, encoder-decoder LSTMs with attention.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.993364691734314}]}, {"text": "Note, first, that the text-level models outperform other models on BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.66875821352005}]}, {"text": "In particular, the two-step model e 1 e 2 t 2 (and comparable model with attention) which first predicts successor events and then, as a separate step, expands these events into text, performs quite poorly.", "labels": [], "entities": []}, {"text": "This is perhaps due to the fact that the translation from text to events is lossy, so reconstructing raw sentence tokens is not straightforward.", "labels": [], "entities": []}, {"text": "The BLEU-BP scores, which are BLEU without the brevity penalty, are noticeably higher in the text-level models than the raw BLEU scores.", "labels": [], "entities": [{"text": "BLEU-BP", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9979653358459473}, {"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9971535205841064}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9798188209533691}]}, {"text": "This is in part because these models seem to produce shorter sentences, as illustrated below in section 4.4.", "labels": [], "entities": []}, {"text": "The attention mechanism does not obviously benefit either text or event level prediction encoder-decoder models.", "labels": [], "entities": []}, {"text": "This could be because there is not an obvious alignment structure between contiguous spans of raw text (or events) in natural documents.", "labels": [], "entities": []}, {"text": "These results provide evidence that, if the Narrative Cloze task is defined to evaluate prediction of held-out text from a document, then sentencelevel RNN language models provide superior performance to RNN models operating at the event level.", "labels": [], "entities": []}, {"text": "In other words, linguistic pre-processing does not obviously benefit encoder-decoder models trained to predict succeeding text.", "labels": [], "entities": []}, {"text": "gives results on the task of predicting the next verb with its nominal arguments; that is, whereas gave results on a text analog to the Narrative Cloze evaluation (BLEU),   results on the verb-with-arguments prediction version.", "labels": [], "entities": [{"text": "predicting the next verb with its nominal arguments", "start_pos": 29, "end_pos": 80, "type": "TASK", "confidence": 0.8127822205424309}, {"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.8688869476318359}]}, {"text": "In the t 1 t 2 e 2 [0] system (and the comparable system with attention), events are extracted from automatically generated text by parsing output text and applying the same event extractor to this parse used to extract events from raw text.", "labels": [], "entities": []}, {"text": "The row labeled Most common in gives performance for the baseline system which always guesses the most common event in the training set.", "labels": [], "entities": []}, {"text": "The LSTM models trained to directly predict events are roughly comparable to systems which operate on raw text, performing slightly worse on accuracy and slightly better when taking partial credit into account.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9991518259048462}]}, {"text": "As with the previous comparisons with BLEU, the attention mechanism does not provide an obvious improvement when decoding inferences, perhaps, again, because the event inference problem lacks a clear alignment structure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9740240573883057}]}, {"text": "These systems infer their most probable guesses of e 2 [0], the first event in the succeeding sentence.", "labels": [], "entities": []}, {"text": "In order fora system prediction to be counted as correct, it must have the correct strings for grammatical head words of all components of the correct event.", "labels": [], "entities": []}, {"text": "Note also that we judge only against a system's single most confident prediction (as opposed to some prior work () which takes the top k predictions-the numbers presented here are therefore noticeably lower).", "labels": [], "entities": []}, {"text": "We do this mainly for computational reasons: namely, abeam search over a full sentence's text would be quite computationally expensive.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Successor text predictions evaluated with", "labels": [], "entities": []}, {"text": " Table 2: Next event prediction accuracy (numbers  are percentages: maximum value is 100).", "labels": [], "entities": [{"text": "event prediction", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.6486792266368866}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9691038131713867}]}, {"text": " Table 3: Varying the amount of context in text- level models. \"Num Prev Sents\" is the number of  previous sentences supplied during encoding.", "labels": [], "entities": [{"text": "Num Prev Sents\"", "start_pos": 64, "end_pos": 79, "type": "METRIC", "confidence": 0.7612713277339935}]}]}