{"title": [], "abstractContent": [{"text": "Neural Machine Translation (MT) has reached state-of-the-art results.", "labels": [], "entities": [{"text": "Neural Machine Translation (MT)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.869343101978302}]}, {"text": "However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.655331015586853}]}, {"text": "In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.8147165179252625}]}, {"text": "The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network.", "labels": [], "entities": []}, {"text": "The proposed MT scheme provides improved results even when the source language is not morphologically rich.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9824284911155701}]}, {"text": "Improvements up to 3 BLEU points are obtained in the German-English WMT task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9991642236709595}, {"text": "WMT task", "start_pos": 68, "end_pos": 76, "type": "TASK", "confidence": 0.591352641582489}]}], "introductionContent": [{"text": "Machine Translation (MT) is the set of algorithms that aim at transforming a source language into a target language.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8780479073524475}]}, {"text": "For the last 20 years, one of the most popular approaches has been statistical phrase-based MT, which uses a combination of features to maximise the probability of the target sentence given the source sentence (.", "labels": [], "entities": [{"text": "statistical phrase-based MT", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.456774244705836}]}, {"text": "Just recently, the neural MT approach has appeared and obtained state-of-the-art results.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.8348884582519531}]}, {"text": "Among its different strengths neural MT does not need to pre-design feature functions beforehand; optimizes the entire system at once because it provides a fully trainable model; uses word embeddings () so that words (or minimal units) are not independent anymore; and is easily extendable to multimodal sources of information (.", "labels": [], "entities": [{"text": "neural MT", "start_pos": 30, "end_pos": 39, "type": "TASK", "confidence": 0.5644891262054443}]}, {"text": "As for weaknesses, neural MT has a strong limitation in vocabulary due to its architecture and it is difficult and computationally expensive to tune all parameters in the deep learning structure.", "labels": [], "entities": [{"text": "neural MT", "start_pos": 19, "end_pos": 28, "type": "TASK", "confidence": 0.5694647133350372}]}, {"text": "In this paper, we use the neural MT baseline system from (, which follows an encoder-decoder architecture with attention, and introduce elements from the characterbased neural language model (.", "labels": [], "entities": []}, {"text": "The translation unit continues to be the word, and we continue using word embeddings related to each word as an input vector to the bidirectional recurrent neural network (attention-based mechanism).", "labels": [], "entities": []}, {"text": "The difference is that now the embeddings of each word are no longer an independent vector, but are computed from the characters of the corresponding word.", "labels": [], "entities": []}, {"text": "The system architecture has changed in that we are using a convolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder.", "labels": [], "entities": []}, {"text": "This is a significant difference from previous work) which uses the neural MT architecture from () without modification to deal with subword units (but not including unigram characters).", "labels": [], "entities": []}, {"text": "Subword-based representations have already been explored in Natural Language Processing (NLP), e.g. for POS tagging), name entity recognition (Santos and aes, 2015), parsing (, normalization) or learning word representations).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.8594754934310913}, {"text": "name entity recognition", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.7149767478307089}, {"text": "parsing", "start_pos": 166, "end_pos": 173, "type": "TASK", "confidence": 0.9692590832710266}]}, {"text": "These previous works show different advantages of using character-level information.", "labels": [], "entities": []}, {"text": "In our case, with the new character-based neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications), especially when dealing with morphologically rich languages.", "labels": [], "entities": []}, {"text": "When using the character-based source word embeddings in MT, there ceases to be unknown words in the source input, while the size of the target vocabulary remains unchanged.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.8515346050262451}]}, {"text": "Although the target vocabulary continues with the same limitation as in the standard neural MT system, the fact that there are no unknown words in the source helps to reduce the number of unknowns in the target.", "labels": [], "entities": []}, {"text": "Moreover, the remaining unknown target words can now be more successfully replaced with the corresponding source-aligned words.", "labels": [], "entities": []}, {"text": "As a consequence, we obtain a significant improvement in terms of translation quality (up to 3 BLEU points).", "labels": [], "entities": [{"text": "translation", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.929415225982666}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9991751313209534}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly explains the architecture of the neural MT that we are using as a baseline system.", "labels": [], "entities": []}, {"text": "Section 3 describes the changes introduced in the baseline architecture in order to use characterbased embeddings instead of the standard lookupbased word representations.", "labels": [], "entities": []}, {"text": "Section 4 reports the experimental framework and the results obtained in the German-English WMT task.", "labels": [], "entities": [{"text": "WMT task", "start_pos": 92, "end_pos": 100, "type": "TASK", "confidence": 0.5667144954204559}]}, {"text": "Finally, section 5 concludes with the contributions of the paper and further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports the data used, its preprocessing, baseline details and results with the enhanced character-based neural MT system.", "labels": [], "entities": [{"text": "character-based neural MT", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.4598015646139781}]}], "tableCaptions": [{"text": " Table 3: De-En BLEU results.", "labels": [], "entities": [{"text": "De-En", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.6409753561019897}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9701793193817139}]}]}