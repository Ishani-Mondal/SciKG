{"title": [{"text": "QA-It: Classifying Non-Referential It for Question Answer Pairs", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces anew corpus, QA-It, for the classification of non-referential it.", "labels": [], "entities": []}, {"text": "Our dataset is unique in a sense that it is annotated on question answer pairs collected from multiple genres, useful for developing advanced QA systems.", "labels": [], "entities": []}, {"text": "Our annotation scheme makes clear distinctions between 4 types of it, providing guidelines for many erroneous cases.", "labels": [], "entities": []}, {"text": "Several statistical models are built for the classification of it, showing encouraging results.", "labels": [], "entities": [{"text": "classification", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.9737966060638428}]}, {"text": "To the best of our knowledge, this is the first time that such a corpus is created for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8920178413391113}]}], "introductionContent": [{"text": "One important factor in processing document-level text is to resolve coreference resolution; one of the least developed tasks left in natural language processing.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.83836430311203}]}, {"text": "Coreference resolution can be processed in two steps, mention detection and antecedent resolution.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9156383275985718}, {"text": "mention detection", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6890947818756104}, {"text": "antecedent resolution", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7374634146690369}]}, {"text": "For mention detection, the classification of the pronoun it as either referential or non-referential is of critical importance because the identification of non-referential instances of it is essential to remove from the total list of possible mentions.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8312109410762787}]}, {"text": "Although previous work has demonstrated a lot of promise for classifying all instances of it (, it is still a difficult task, especially when performed on social networks data containing grammatical errors, ambiguity, and colloquial language.", "labels": [], "entities": []}, {"text": "In specific, we found that the incorrect classification of non-referential it was one of the major reasons for the failure of a question answering system handling social networks data.", "labels": [], "entities": [{"text": "question answering system handling social networks", "start_pos": 128, "end_pos": 178, "type": "TASK", "confidence": 0.8250142931938171}]}, {"text": "In this paper, we first introduce our new corpus, QA-It, sampled from the Yahoo!", "labels": [], "entities": []}, {"text": "Answers corpus and manually annotated with 4 categories of it, referential-nominal, referential-others, non-referential, and errors.", "labels": [], "entities": []}, {"text": "We also present statistical models for the classification of these four categories, each showing incremental improvements from one another.", "labels": [], "entities": []}, {"text": "The manual annotation of this corpus is challenging because the rhetoric used in this dataset is often ambiguous; consequently, the automatic classification becomes undoubtedly more challenging.", "labels": [], "entities": []}, {"text": "Our best model shows an accuracy of \u224878%, which is lower than some of the results achieved by previous work, but expected because our dataset is much harder to comprehend even for humans, showing an inter-annotation agreement of \u224865%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9997046589851379}]}, {"text": "However, we believe that this corpus provides an initiative to development a better coreference resolution system for the setting of question answering.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.8871033191680908}, {"text": "setting of question answering", "start_pos": 122, "end_pos": 151, "type": "TASK", "confidence": 0.707169696688652}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distributions of our corpus. Doc/Sen/Tok: number of documents/sentences/tokens. C 1..4 : number  of it-instances in categories described in Sections 4.1, 4.2, 4.3, and 4.4.", "labels": [], "entities": []}, {"text": " Table 2: Distributions of our data for each docu- ment size.", "labels": [], "entities": []}, {"text": " Table 3: Accuracies achieved by each model (in %). ACC: overall accuracy, C 1..4 : F1 scores for 4  categories in Section 4. The highest accuracies are highlighted in bold.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9992421865463257}, {"text": "ACC", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9964568018913269}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9861378073692322}, {"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9994603991508484}]}, {"text": " Table 4: Distributions of our data splits.", "labels": [], "entities": []}]}