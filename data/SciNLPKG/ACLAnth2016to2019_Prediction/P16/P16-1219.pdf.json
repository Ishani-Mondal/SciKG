{"title": [{"text": "TransG : A Generative Model for Knowledge Graph Embedding", "labels": [], "entities": [{"text": "Knowledge Graph Embedding", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.7386751770973206}]}], "abstractContent": [{"text": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become anew, hot topic in artificial intelligence.", "labels": [], "entities": []}, {"text": "This paper proposes a novel gen-erative model (TransG) to address the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples.", "labels": [], "entities": []}, {"text": "The new model can discover latent semantics fora relation and leverage a mixture of relation-specific component vectors to embed a fact triple.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first generative model for knowledge graph embedding, and at the first time, the issue of multiple relation semantics is formally discussed.", "labels": [], "entities": [{"text": "multiple relation semantics", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.6146512031555176}]}, {"text": "Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract or real-world knowledge is always a major topic in Artificial Intelligence.", "labels": [], "entities": [{"text": "Abstract or real-world knowledge", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7681355029344559}]}, {"text": "Knowledge bases such as Wordnet and Freebase () have been shown very useful to AI tasks including question answering, knowledge inference, and soon.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9749005436897278}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.8708903193473816}, {"text": "knowledge inference", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7576533555984497}]}, {"text": "However, traditional knowledge bases are symbolic and logic, thus numerical machine learning methods cannot be leveraged to support the computation over the knowledge bases.", "labels": [], "entities": []}, {"text": "To this end, knowledge graph embedding has been proposed to project entities and relations into continuous vector spaces.", "labels": [], "entities": []}, {"text": "Among various embedding models, there is a line * Correspondence author of translation-based models such as TransE), TransH (), TransR (, and other related models () ().", "labels": [], "entities": []}, {"text": "A dot denotes a triple and its position is decided by the difference vector between tail and head entity (t \u2212 h).", "labels": [], "entities": []}, {"text": "Since TransE adopts the principle oft \u2212 h \u2248 r, there is supposed to be only one cluster whose centre is the relation vector r.", "labels": [], "entities": []}, {"text": "However, results show that there exist multiple clusters, which justifies our multiple relation semantics assumption.", "labels": [], "entities": []}, {"text": "A fact of knowledge base can usually be represented by a triple (h, r, t) where h, r, t indicate ahead entity, a relation, and a tail entity, respectively.", "labels": [], "entities": []}, {"text": "All translation-based models almost follow the same principle hr + r \u2248 tr where hr , r, tr in-dicate the embedding vectors of triple (h, r, t), with the head and tail entity vector projected with respect to the relation space.", "labels": [], "entities": []}, {"text": "In spite of the success of these models, none of the previous models has formally discussed the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples.", "labels": [], "entities": []}, {"text": "As can be seen from, visualization results on embedding vectors obtained from TransE ( show that, there are different clusters fora specific relation, and different clusters indicate different latent semantics.", "labels": [], "entities": []}, {"text": "For example, the relation HasPart has at least two latent semantics: composition-related as HasPart, Leg) and location-related as (Atlantics, HasPart, NewYorkBay).", "labels": [], "entities": [{"text": "NewYorkBay", "start_pos": 151, "end_pos": 161, "type": "DATASET", "confidence": 0.8329452276229858}]}, {"text": "As one more example, in Freebase, (Jon Snow, birthplace, Winter Fall) and (George R.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.944719672203064}]}, {"text": "R. Martin, birthplace, U.S.) are mapped to schema /fictional universe/fictional character/place of birth and /people/person/place of birth respectively, indicating that birthplace has different meanings.", "labels": [], "entities": []}, {"text": "This phenomenon is quite common in knowledge bases for two reasons: artificial simplification and nature of knowledge.", "labels": [], "entities": []}, {"text": "On one hand, knowledge base curators could not involve too many similar relations, so abstracting multiple similar relations into one specific relation is a common trick.", "labels": [], "entities": []}, {"text": "On the other hand, both language and knowledge representations often involve ambiguous information.", "labels": [], "entities": []}, {"text": "The ambiguity of knowledge means a semantic mixture.", "labels": [], "entities": []}, {"text": "For example, when we mention \"Expert\", we may refer to scientist, businessman or writer, so the concept \"Expert\" maybe ambiguous in a specific situation, or generally a semantic mixture of these cases.", "labels": [], "entities": []}, {"text": "However, since previous translation-based models adopt hr + r \u2248 tr , they assign only one translation vector for one relation, and these models are notable to deal with the issue of multiple relation semantics.", "labels": [], "entities": []}, {"text": "To illustrate more clearly, as showed in, there is only one unique representation for relation HasPart in traditional models, thus the models made more errors when embedding the triples of the relation.", "labels": [], "entities": []}, {"text": "Instead, in our proposed model, we leverage a Bayesian non-parametric infinite mixture model to handle multiple relation semantics by generating multiple translation components fora relation.", "labels": [], "entities": []}, {"text": "Thus, different semantics are characterized by different components in our embedding model.", "labels": [], "entities": []}, {"text": "For example, we can distinguish the two clusters HasPart.1 or HasPart.2, where the relation semantics are automatically clustered to represent the meaning of associated entity pairs.", "labels": [], "entities": []}, {"text": "To summarize, our contributions are as follows: \u2022 We propose anew issue in knowledge graph embedding, multiple relation semantics that a relation in knowledge graph may have different meanings revealed by the associated entity pairs, which has never been studied previously.", "labels": [], "entities": []}, {"text": "\u2022 To address the above issue, we propose a novel Bayesian non-parametric infinite mixture embedding model, TransG.", "labels": [], "entities": []}, {"text": "The model can automatically discover semantic clusters of a relation, and leverage a mixture of multiple relation components for translating an entity pair.", "labels": [], "entities": []}, {"text": "Moreover, we present new insights from the generative perspective.", "labels": [], "entities": [{"text": "generative", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.9650021195411682}]}, {"text": "\u2022 Extensive experiments show that our proposed model obtains substantial improvements against the state-of-the-art baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are conducted on four public benchmark datasets that are the subsets of Wordnet and Freebase, respectively.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.9860799908638}, {"text": "Freebase", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8383947610855103}]}, {"text": "The statistics of these datasets are listed in Tab.1.", "labels": [], "entities": [{"text": "Tab.1", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9639120101928711}]}, {"text": "Experiments are conducted on two tasks : Link Prediction and Triple Classification.", "labels": [], "entities": [{"text": "Link Prediction", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6937421262264252}, {"text": "Triple Classification", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.859070360660553}]}, {"text": "To further demonstrate how the proposed model approaches multiple relation semantics, we present semantic component analysis at the end of this section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of datasets", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results on link prediction  Datasets  WN18  FB15K", "labels": [], "entities": [{"text": "link prediction  Datasets  WN18  FB15K", "start_pos": 32, "end_pos": 70, "type": "DATASET", "confidence": 0.8054259538650512}]}, {"text": " Table 3: Evaluation results on FB15K by mapping properties of relations(%)  Tasks  Predicting Head(HITS@10) Predicting Tail(HITS@10)  Relation Category  1-1 1-N N-1  N-N  1-1 1-N N-1  N-N  Unstructured (Bordes et al., 2011) 34.5 2.5  6.1  6.6  34.3 4.2  1.9  6.6  SE(Bordes et al., 2011)  35.6 62.6 17.2  37.5  34.9 14.6 68.3  41.3  SME(bilinear) (Bordes et al., 2012) 30.9 69.6 19.9  38.6  28.2 13.1 76.0  41.8  TransE (Bordes et al., 2013)  43.7 65.7 18.2  47.2  43.7 19.7 66.7  50.0  TransH (Wang et al., 2014)  66.8 87.6 28.7  64.5  65.5 39.8 83.3  67.2  TransR (Lin et al., 2015b)  78.8 89.2 34.1  69.2  79.2 37.4 90.4  72.1  CTransR (Lin et al., 2015b)  81.5 89.0 34.7  71.2  80.8 38.6 90.1  73.8  PTransE (Lin et al., 2015a)  90.1 92.0 58.7  86.1  90.1 70.7 87.5  88.7  KG2E (He et al., 2015)  92.3 93.7 66.0  69.6  92.6 67.9 94.4  73.4  TransG (this paper)  93.0 96.0 62.5  86.8  92.8 68.1 94.5  88.8", "labels": [], "entities": [{"text": "FB15K", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9103490710258484}, {"text": "SE", "start_pos": 265, "end_pos": 267, "type": "METRIC", "confidence": 0.9633893966674805}, {"text": "PTransE", "start_pos": 705, "end_pos": 712, "type": "METRIC", "confidence": 0.6435574293136597}]}, {"text": " Table 5: Triple classification: accuracy(%) for dif- ferent embedding methods.  Methods WN11 FB13 AVG.  LFM  73.8  84.3  79.0  NTN  70.4  87.1  78.8  TransE  75.9  81.5  78.7  TransH  78.8  83.3  81.1  TransR  85.9  82.5  84.2  CTransR  85.7  N/A  N/A  KG2E  85.4  85.3  85.4  TransG  87.4  87.3  87.4", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9996041655540466}, {"text": "WN11 FB13 AVG.  LFM  73.8  84.3  79.0  NTN  70.4  87.1  78.8  TransE  75.9  81.5  78.7  TransH  78.8  83.3  81.1  TransR  85.9  82.5  84.2  CTransR  85.7  N/A  N/A  KG2E  85.4  85.3  85.4  TransG  87.4  87.3  87.4", "start_pos": 89, "end_pos": 302, "type": "DATASET", "confidence": 0.900391137599945}]}]}