{"title": [{"text": "\"The red one!\": On learning to refer to things based on discriminative properties", "labels": [], "entities": []}], "abstractContent": [{"text": "As a first step towards agents learning to communicate about their visual environment , we propose a system that, given visual representations of a referent (CAT) and a context (SOFA), identifies their dis-criminative attributes, i.e., properties that distinguish them (has_tail).", "labels": [], "entities": []}, {"text": "Moreover , although supervision is only provided in terms of discriminativeness of attributes for pairs, the model learns to assign plausible attributes to specific objects (SOFA-has_cushion).", "labels": [], "entities": []}, {"text": "Finally, we present a preliminary experiment confirming the referential success of the predicted discriminative attributes.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has recently been renewed interest in developing systems capable of genuine language understanding (.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7061754316091537}]}, {"text": "In this perspective, it is important to think of an appropriate general framework for teaching language to machines.", "labels": [], "entities": []}, {"text": "Since we use language primarily for communication, a reasonable approach is to develop systems within a genuine communicative setup.", "labels": [], "entities": []}, {"text": "Out long-term goal is thus to develop communities of computational agents that learn how to use language efficiently in order to achieve communicative success (.", "labels": [], "entities": []}, {"text": "Within this general picture, one fundamental aspect of meaning where communication is indeed crucial is the act of reference, the ability to successfully talk to others about things in the external world.", "labels": [], "entities": []}, {"text": "A specific instantiation of reference studied in this paper is that of referring expression generation (Dale and is_round is_metal is_green made_of_wood Figure 1: Discriminative attributes predicted by our model.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6162441372871399}]}, {"text": "Can you identify the intended referent?", "labels": [], "entities": []}, {"text": "See Section 6 for more information.", "labels": [], "entities": []}, {"text": "A necessary condition for achieving successful reference is that referring expressions (REs) accurately distinguish the intended referent from any other object in the context.", "labels": [], "entities": []}, {"text": "Along these lines, we present here a model that, given an intended referent and a context object, predicts the attributes that discriminate between the two.", "labels": [], "entities": []}, {"text": "Some examples of the behaviour of the model are presented in.", "labels": [], "entities": []}, {"text": "Importantly, and distinguishing our work from earlier literature on generating REs): (i) the input objects are represented by natural images, so that the agent must learn to extract relevant attributes from realistic data; and (ii) no direct supervision on the attributes of a single object is provided: the training signal concerns their discriminativeness for object pairs (that is, during learning, the agent might be told that has_tail is discriminative for CAT, SOFA, but not that it is an attribute of cats).", "labels": [], "entities": []}, {"text": "We use this \"pragmatic\" signal since it could later be replaced by a measure of success in actual communication between two agents (e.g., whether a second agent was able to pick the correct referent given a RE).", "labels": [], "entities": [{"text": "RE", "start_pos": 207, "end_pos": 209, "type": "METRIC", "confidence": 0.9113265872001648}]}], "datasetContent": [{"text": "We generated the Discriminative Attribute Dataset, consisting of pairs of (intended) referents and contexts, with respect to which the referents should be identified by their distinctive attributes., which contains per-concept (as opposed to perimage) attributes for 500 concrete concepts (CAT, SOFA, MILK) spanning across different categories (MAMMALS, FURNITURE), annotated with 636 general attributes.", "labels": [], "entities": [{"text": "FURNITURE", "start_pos": 354, "end_pos": 363, "type": "METRIC", "confidence": 0.9596975445747375}]}, {"text": "We disregarded ambiguous concepts (e.g., bat), thus reducing our working set of concepts C to 462 and the number of attributes V to 573, as we eliminated any attribute that did not occur with concepts in C.", "labels": [], "entities": []}, {"text": "We extracted on average 100 images annotated with each of these concepts from ImageNet (.", "labels": [], "entities": [{"text": "ImageNet", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9494460821151733}]}, {"text": "Finally, each image i of concept c was associated to a visual instance vector, by feeding the image to the), as implemented in the MatConvNet toolkit, and extracting the second-to-last fully-connected (fc) layer as its 4096-dimensional visual representation v c i . We split the target concepts into training, validation and test sets, containing 80%, 10% and 10% of the concepts in each category, respectively.", "labels": [], "entities": [{"text": "MatConvNet toolkit", "start_pos": 131, "end_pos": 149, "type": "DATASET", "confidence": 0.9329540133476257}]}, {"text": "This ensures that (i) the intersection between train and test concepts is empty, thus allowing us to test the generalization of the model across different objects, but (ii) there are instances of all categories in each set, so that it is possible to generalize across training and testing objects.", "labels": [], "entities": []}, {"text": "Finally we build all possible combinations of concepts in the training split to form pairs of referents and contexts c r , cc and obtain their (binary) attribute vectors p cr and p cc from ViSA, resulting in 70K training pairs.", "labels": [], "entities": [{"text": "ViSA", "start_pos": 189, "end_pos": 193, "type": "DATASET", "confidence": 0.9105461835861206}]}, {"text": "From the latter, we derive, for each pair, a concept-level \"discriminativeness\" vector by computing the symmetric difference d cr,cc = (p cr \u2212 p cc ) \u222a (p cc \u2212 p cr ).", "labels": [], "entities": []}, {"text": "The latter will contain 1s for discriminative attributes, 0s elsewhere.", "labels": [], "entities": []}, {"text": "On average, each pair is associated with 20 discriminative attributes.", "labels": [], "entities": []}, {"text": "The final training data are triples of the form c r , cc , d cr,cc (the model never observes the attribute vectors of specific concepts), to be associated with visual instances of the two concepts.", "labels": [], "entities": []}, {"text": "Note that ViSA contain concept-level attributes, but images contain specific instances of concepts for which a general attribute might not hold.", "labels": [], "entities": []}, {"text": "This introduces a small amount of noise.", "labels": [], "entities": []}, {"text": "For example, is_green would in general be a discriminative attribute for apples and cats, but it is not for the second sample in.", "labels": [], "entities": []}, {"text": "Using datasets with perimage attribute annotations would solve this issue.", "labels": [], "entities": []}, {"text": "However, those currently available only cover specific classes of concepts (e.g., only clothes, or animals, or scenes, etc.).", "labels": [], "entities": []}, {"text": "Thus, taken separately, they are not general enough for our purposes, and we cannot merge them, since their concepts live in different attribute spaces.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Predicting discriminative features", "labels": [], "entities": [{"text": "Predicting discriminative", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7720700204372406}]}, {"text": " Table 3: Predicting concept attributes", "labels": [], "entities": [{"text": "Predicting concept", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7644290030002594}]}]}