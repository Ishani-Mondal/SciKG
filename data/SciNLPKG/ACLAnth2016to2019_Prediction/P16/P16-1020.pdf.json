{"title": [{"text": "Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings", "labels": [], "entities": [{"text": "Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings", "start_pos": 0, "end_pos": 80, "type": "TASK", "confidence": 0.8072562482621934}]}], "abstractContent": [{"text": "We present a novel method for jointly learning compositional and non-compositional phrase embeddings by adaptively weighting both types of em-beddings using a compositionality scoring function.", "labels": [], "entities": []}, {"text": "The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings.", "labels": [], "entities": []}, {"text": "In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art.", "labels": [], "entities": []}, {"text": "Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task.", "labels": [], "entities": [{"text": "transitive verb disambiguation task", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.7755259871482849}]}, {"text": "We also show that a simple ensemble technique further improves the results for both tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing words and phrases in a vector space has proven effective in a variety of language processing tasks).", "labels": [], "entities": []}, {"text": "In most of the previous work, phrase embeddings are computed from word embeddings by using various kinds of composition functions.", "labels": [], "entities": []}, {"text": "Such composed embeddings are called compositional embeddings.", "labels": [], "entities": []}, {"text": "An alternative way of computing phrase embeddings is to treat phrases as single units and assigning a unique embedding to each candidate phrase (.", "labels": [], "entities": []}, {"text": "Such embeddings are called noncompositional embeddings.", "labels": [], "entities": []}, {"text": "Relying solely on non-compositional embeddings has the obvious problem of data sparsity (i.e. rare or unknown phrase problems).", "labels": [], "entities": []}, {"text": "At the same time, however, using compositional embeddings is not always the best option since some phrases are inherently non-compositional.", "labels": [], "entities": []}, {"text": "For example, the phrase \"bear fruits\" means \"to yield results\" 1 but it is hard to infer its meaning by composing the meanings of \"bear\" and \"fruit\".", "labels": [], "entities": []}, {"text": "Treating all phrases as compositional also has a negative effect in learning the composition function because the words in those idiomatic phrases are not just uninformative but can serve as noisy samples in the training.", "labels": [], "entities": []}, {"text": "These problems have motivated us to adaptively combine both types of embeddings.", "labels": [], "entities": []}, {"text": "Most of the existing methods for learning phrase embeddings can be divided into two approaches.", "labels": [], "entities": []}, {"text": "One approach is to learn compositional embeddings by regarding all phrases as compositional ().", "labels": [], "entities": []}, {"text": "The other approach is to learn both types of embeddings separately and use the better ones).", "labels": [], "entities": []}, {"text": "show that non-compositional embeddings are better suited fora phrase similarity task, whereas report the opposite results on other tasks.", "labels": [], "entities": [{"text": "phrase similarity task", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7711948454380035}]}, {"text": "These results suggest that we should not stick to either of the two types of embeddings unconditionally and could learn better phrase embeddings by considering the compositionality levels of the individual phrases in a more flexible fashion.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method that jointly learns compositional and non-compositional embeddings by adaptively weighting both types of phrase embeddings using a compositionality scoring function.", "labels": [], "entities": []}, {"text": "The scoring function is used to quantify the level of compositionality of each phrase: The overview of our method and examples of the compositionality scores.", "labels": [], "entities": []}, {"text": "Given a phrase p, our method first computes the compositionality score \u03b1(p) (Eq.), and then computes the phrase embedding v(p) using the compositional and non-compositional embeddings, c(p) and n(p), respectively (Eq.). and learned in conjunction with the target task for learning phrase embeddings.", "labels": [], "entities": []}, {"text": "In experiments, we apply our method to the task of learning transitive verb phrase embeddings and demonstrate that it allows us to achieve state-of-the-art performance on standard datasets for compositionality detection and verb disambiguation.", "labels": [], "entities": [{"text": "compositionality detection", "start_pos": 193, "end_pos": 219, "type": "TASK", "confidence": 0.8597046434879303}, {"text": "verb disambiguation", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.7188397794961929}]}], "datasetContent": [{"text": "Detection Function  Dataset Next, we evaluated the learned embeddings on the transitive verb disambiguation dataset GS'11 8 provided by.", "labels": [], "entities": []}, {"text": "GS'11 consists of 200 pairs of transitive verbs and each verb pair takes the same subject and object.", "labels": [], "entities": [{"text": "GS'11", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8571832776069641}]}, {"text": "For example, the transitive verb \"run\" is known as a polysemous word and this task requires one to identify the meanings of \"run\" and \"operate\" as similar to each other when taking \"people\" as their subject and \"company\" as their object.", "labels": [], "entities": []}, {"text": "In the same setting, however, the meanings of \"run\" and \"move\" are not similar to each other.", "labels": [], "entities": []}, {"text": "Each pair has multiple human ratings indicating how similar the phrases of the pair are.", "labels": [], "entities": []}, {"text": "Evaluation metric The evaluation was performed by calculating Spearman's rank correlation scores between the human ratings and the cosine similarity scores of v(SV O) in Eq..", "labels": [], "entities": [{"text": "cosine similarity scores of v(SV O)", "start_pos": 131, "end_pos": 166, "type": "METRIC", "confidence": 0.6749393741289774}]}, {"text": "Following the previous studies, we used the goldstandard ratings in two ways: averaging the human ratings for each SVO tuple (GS'11a) and treating each human rating separately (GS'11b).", "labels": [], "entities": [{"text": "GS'11a", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.6489522457122803}, {"text": "GS'11b", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.4910595417022705}]}, {"text": "Ensemble technique We used the same ensemble technique described in Section 5.1.", "labels": [], "entities": []}, {"text": "In this task we produced two ensemble results: Ensemble A and Ensemble B. The former used the averaged cosine similarity from the results of the BNC and Wikipedia data, and the latter further incorporated the result of the BNC-Wikipedia data.", "labels": [], "entities": [{"text": "BNC and Wikipedia data", "start_pos": 145, "end_pos": 167, "type": "DATASET", "confidence": 0.7615734338760376}, {"text": "BNC-Wikipedia data", "start_pos": 223, "end_pos": 241, "type": "DATASET", "confidence": 0.9536782205104828}]}, {"text": "Baselines We compared our adaptive joint learning method with two baseline methods.", "labels": [], "entities": []}, {"text": "One is the method in and it is equivalent to fixing \u03b1(V O) to 1 in our method.", "labels": [], "entities": []}, {"text": "The other is fixing \u03b1(V O) to 0.5 in our method, which serves as a baseline to evaluate how effective the proposed adaptive weighting method is. shows our results and the state of the art, and our method outperforms almost all of the previous methods in both datasets.", "labels": [], "entities": []}, {"text": "Again, the ensemble technique further improves the results, and overall, Ensemble B yields the best results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Compositionality detection task.", "labels": [], "entities": [{"text": "Compositionality detection", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.9702408313751221}]}, {"text": " Table 2: Examples of the compositionality scores.", "labels": [], "entities": []}, {"text": " Table 3: The 10 highest and lowest average com- positionality scores with the corresponding verbs  on the BNC data.", "labels": [], "entities": [{"text": "BNC data", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.9727774560451508}]}, {"text": " Table 5: Examples of the closest neighbors in the learned embedding space. All of the results were  obtained by using the Wikipedia data, and the values of \u03b1(V O) are the same as those in Table 2.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.9374536275863647}]}, {"text": " Table 4: Transitive verb disambiguation task. The  results for \u03b1(V O) = 1 are reported in Hashimoto  and Tsuruoka (2015).", "labels": [], "entities": [{"text": "Transitive verb disambiguation task", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.7352867275476456}, {"text": "\u03b1(V O) = 1", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.8733740789549691}]}]}