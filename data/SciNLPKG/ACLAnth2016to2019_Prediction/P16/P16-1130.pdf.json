{"title": [{"text": "A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Continuous Space Rule Selection", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.579221099615097}, {"text": "Syntax-based Statistical Machine Translation", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.7235459610819817}]}], "abstractContent": [{"text": "One of the major challenges for statistical machine translation (SMT) is to choose the appropriate translation rules based on the sentence context.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.8089910596609116}]}, {"text": "This paper proposes a continuous space rule selection (CSRS) model for syntax-based SMT to perform this context-dependent rule selection.", "labels": [], "entities": [{"text": "continuous space rule selection (CSRS)", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.7306856683322361}, {"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.8893413543701172}, {"text": "context-dependent rule selection", "start_pos": 104, "end_pos": 136, "type": "TASK", "confidence": 0.6357576549053192}]}, {"text": "In contrast to existing maximum en-tropy based rule selection (MERS) models , which use discrete representations of words as features, the CSRS model is learned by a feed-forward neural network and uses real-valued vector representations of words, allowing for better generalization.", "labels": [], "entities": []}, {"text": "In addition, we propose a method to train the rule selection models only on minimal rules, which are more frequent and have richer training data compared to non-minimal rules.", "labels": [], "entities": []}, {"text": "We tested our model on different translation tasks and the CSRS model outperformed a base-line without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.902166485786438}, {"text": "BLEU score", "start_pos": 177, "end_pos": 187, "type": "METRIC", "confidence": 0.9789901673793793}]}], "introductionContent": [{"text": "In syntax-based statistical machine translation (SMT), especially tree-to-string () and forest-to-string () SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.8108150859673818}]}, {"text": "A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals.", "labels": [], "entities": []}, {"text": "One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in.", "labels": [], "entities": []}, {"text": "Selecting correct rules during decoding is a major challenge for SMT in general, and syntax-based models are no exception.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9939219951629639}]}, {"text": "There have been several methods proposed to resolve this ambiguity.", "labels": [], "entities": []}, {"text": "The most simple method, used in the first models of tree-to-string translation (), estimated the probability of a translation rule by relative frequencies.", "labels": [], "entities": [{"text": "tree-to-string translation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.7921755611896515}]}, {"text": "For example, in, the rule that occurs more times in the training data will have a higher score.", "labels": [], "entities": []}, {"text": "Later,  proposed a maximum entropy based rule selection (MERS, Section 2) model for syntax-based SMT, which used contextual information for rule selection, such as words surrounding a rule and words covered by nonterminals in a rule.", "labels": [], "entities": [{"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.8586246371269226}]}, {"text": "For example, to choose the correct rule from the two rules in for decoding a particular input sentence, if the source phrase covered by \"x1\" is \"a thief\" and this child phrase has been seen in the training data, then the MERS model can use this information to determine that the first rule should be applied.", "labels": [], "entities": []}, {"text": "However, if the source phrase covered by \"x1\" is a slightly different phrase, such as \"a gunman\", it will be hard for the MERS model to select the correct rule, because it treats \"thief\" and \"gunman\" as two different and unrelated words.", "labels": [], "entities": []}, {"text": "In this paper, we propose a continuous space rule selection (CSRS, Section 3) model, which is learned by a feed-forward neural network and replaces the discrete representations of words used in the MERS model with real-valued vector representations of words for better generalization.", "labels": [], "entities": [{"text": "continuous space rule selection", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.6392246335744858}]}, {"text": "For example, the CSRS model can use the similarity of word representations for \"gunman\" and \"thief\" to infer that \"a gunman\" is more similar with \"a thief\" than \"a cold\".", "labels": [], "entities": []}, {"text": "In addition, we propose anew method, applicable to both the MERS and CSRS models, to train rule selection models only on minimal rules.", "labels": [], "entities": [{"text": "MERS", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8445204496383667}]}, {"text": "These minimal rules are more frequent and have richer training data compared to non-minimal rules, making it possible to further relieve the data sparsity problem.", "labels": [], "entities": []}, {"text": "In experiments (Section 4), we validate the proposed CSRS model and the minimal rule training method on English-to-German, Englishto-French, English-to-Chinese and English-toJapanese translation tasks.", "labels": [], "entities": [{"text": "English-toJapanese translation tasks", "start_pos": 164, "end_pos": 200, "type": "TASK", "confidence": 0.7051801681518555}]}, {"text": "2 Tree-to-String SMT and MERS 2.1 Tree-to-String SMT In tree-to-string SMT (), a parse tree for the source sentence F is transformed into a target sentence E using translation rules R.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8186209797859192}, {"text": "MERS", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9308789372444153}, {"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7907277941703796}, {"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8236177563667297}]}, {"text": "Each tree-based translation ruler \u2208 R translates a source subtree\u02dctsubtree\u02dc subtree\u02dct into a target string\u02dcestring\u02dc string\u02dce, which can contain both terminals and nonterminals.", "labels": [], "entities": []}, {"text": "During decoding, the translation system examines different derivations for each source sentence and outputs the one with the highest probability, For a translation E of a source sentence F with derivation R, the translation probability is calculated as follows, Here, h k are features used in the translation system and \u03bb k are feature weights.", "labels": [], "entities": []}, {"text": "Features used in's model contain a language model and simple features based on relative frequencies, which do not consider context information.", "labels": [], "entities": []}, {"text": "One of the most important features used in this model is based on the log conditional probability of the target string given the input source subtree log Pr\u02dce|\u02dct Pr\u02dc Pr\u02dce|Pr\u02dce|\u02dc Pr\u02dce|\u02dct . This allows the model to determine which target strings are more likely to be used in translation.", "labels": [], "entities": []}, {"text": "However, as the correct translation of the rules may depend on context that is not directly included in the rule, this simple contextindependent estimate is inherently inaccurate.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Translation results. The bold numbers  stand for the best systems.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9455298185348511}]}, {"text": " Table 6: Minimal rules contained in R 2 and R 3 .  Shadows (R 2b , R 3a and R 3b ) stand for ambiguous  rules.", "labels": [], "entities": []}, {"text": " Table 7: Scores of minimal rules.", "labels": [], "entities": []}]}