{"title": [{"text": "The Creation and Analysis of a Website Privacy Policy Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "Website privacy policies are often ignored by Internet users, because these documents tend to belong and difficult to understand.", "labels": [], "entities": []}, {"text": "However, the significance of privacy policies greatly exceeds the attention paid to them: these documents are binding legal agreements between website operators and their users, and their opaqueness is a challenge not only to Internet users but also to policy regulators.", "labels": [], "entities": []}, {"text": "One proposed alternative to the status quo is to automate or semi-automate the extraction of salient details from privacy policy text, using a combination of crowdsourcing, natural language processing, and machine learning.", "labels": [], "entities": [{"text": "extraction of salient details from privacy policy text", "start_pos": 79, "end_pos": 133, "type": "TASK", "confidence": 0.7961139753460884}]}, {"text": "However, there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies.", "labels": [], "entities": []}, {"text": "To remedy this problem, we introduce a corpus of 115 privacy policies (267K words) with manual annotations for 23K fine-grained data practices.", "labels": [], "entities": []}, {"text": "We describe the process of using skilled annotators and a purpose-built annotation tool to produce the data.", "labels": [], "entities": []}, {"text": "We provide findings based on a census of the annotations and show results toward automating the annotation procedure.", "labels": [], "entities": []}, {"text": "Finally , we describe challenges and opportunities for the research community to use this corpus to advance research in both privacy and language technologies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Privacy policies written in natural language area nearly pervasive feature of websites and mobile applications.", "labels": [], "entities": []}, {"text": "The \"notice and choice\" legal regimes of many countries require that website operators post a notice of how they gather and process users' information.", "labels": [], "entities": []}, {"text": "In theory, users then choose whether to accept those practices or to abstain from using the website or service.", "labels": [], "entities": []}, {"text": "In practice, however, the average Internet user struggles to understand the contents of privacy policies) and generally does not read them; President's Concil of Advisors on.", "labels": [], "entities": [{"text": "President's Concil of Advisors", "start_pos": 140, "end_pos": 170, "type": "DATASET", "confidence": 0.9248879194259644}]}, {"text": "This disconnect between Internet users and the data practices that affect them has led to the assessment that the notice and choice model is ineffective in the status quo.", "labels": [], "entities": []}, {"text": "Thus, an opening exists for language technologies to help \"bridge the gap\" between privacy policies in their current form and representations that serve the needs of Internet users.", "labels": [], "entities": []}, {"text": "Such abridge would also serve unmet needs of policy regulators, who do not have the means to assess privacy policies in large numbers.", "labels": [], "entities": []}, {"text": "Legal text is a familiar domain for natural language processing, and the legal community has demonstrated some reciprocal interest.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.7573702136675516}]}, {"text": "However, the scale of the problem and its significance-i.e., to virtually any Internet user, as well as to website operators and policy regulators-distinguishes it and provides immense motivation (.", "labels": [], "entities": []}, {"text": "To this end, we introduce a corpus of 115 website privacy policies annotated with detailed information about the data practices that they describe.", "labels": [], "entities": []}, {"text": "This information consists of 23K data practices, 128K practice attributes, and 103K annotated text spans, all produced by skilled anno-tators.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first large-scale effort to annotate privacy policies at such a fine level of detail.", "labels": [], "entities": []}, {"text": "It exceeds prior efforts to annotate sentence-level fragments of policy text, answer simple overarching questions about privacy policy contents (, or analyze the readability of privacy policies (.", "labels": [], "entities": []}, {"text": "We further present analysis that demonstrates the richness of the corpus and the feasibility of partly automating the annotation of privacy policies.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We discuss related work and contextualize the corpus we have created in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the creation of the corpus, including the collection of a diverse set of policies and the creation of a privacy policy annotation tool.", "labels": [], "entities": []}, {"text": "Section 4 presents analysis that illustrates the diversity and complexity of the corpus, and Section 5 shows results on the prediction of policy structure.", "labels": [], "entities": [{"text": "prediction of policy structure", "start_pos": 124, "end_pos": 154, "type": "TASK", "confidence": 0.8213080614805222}]}, {"text": "Finally, in Section 6 we describe some promising avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our dataset consisted of 3,792 segments from 115 privacy policies.", "labels": [], "entities": []}, {"text": "We represented the text of each segment as a dense vector using Paragraph2Vec () and the GENSIM toolkit.", "labels": [], "entities": [{"text": "GENSIM toolkit", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.9276710152626038}]}, {"text": "This approach exploited semantic similarities between words in the vocabulary of privacy policies, acknowledging that the vocabulary in this domain is specialized but not completely standardized.", "labels": [], "entities": []}, {"text": "We assigned each policy segment a binary vector of categoryspecific labels, with each element in the vector corresponding to the presence or absence of a data practice category in the segment.", "labels": [], "entities": []}, {"text": "We considered a vector with twelve elements, with nine of them coming from existing practice categories (all except Other).", "labels": [], "entities": []}, {"text": "The remaining three came from elevating three attributes of Other to category status: Introductory/Generic, Practice Not Covered, and Privacy Contact Information.", "labels": [], "entities": []}, {"text": "We created gold standard data for this problem using a simplified consolidation approach: if two or more annotators agreed that a category is present in a segment, then we labeled that segment with the category.", "labels": [], "entities": []}, {"text": "To predict the category labels of privacy policy segments, we tried three approaches.", "labels": [], "entities": []}, {"text": "Two were logistic regression and SVM models, for which we treated this as a multi-class classification problem.", "labels": [], "entities": []}, {"text": "Since 2 12 unique category vectors exist, we trimmed the label space to only those that occur in the training set.", "labels": [], "entities": []}, {"text": "The third was a sequence labeling approach inspired by prior work to apply hidden Markov models (HMMs) to privacy policy text ).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.6527718007564545}]}, {"text": "Our work differs from this prior work by using labels from an annotation scheme constructed by privacy experts rather than topics developed from an unsupervised method.", "labels": [], "entities": []}, {"text": "Additionally, in our formulation, each hidden state corresponds to one of the unique binary vectors that represent classes of category combinations in the training data.", "labels": [], "entities": []}, {"text": "The HMM's transition probabilities capture the tendency of privacy policy authors to organize topics (i.e., practice categories in our annotation scheme) in similar sequences.", "labels": [], "entities": []}, {"text": "Since each segment is represented by a unique real-valued vector from Paragraph2Vec, it was not possible to directly obtain an emission probability distribution from the training data.", "labels": [], "entities": [{"text": "Paragraph2Vec", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.941461443901062}]}, {"text": "Therefore, we ran the K-Means++ algorithm using the scikit-learn toolkit) on the segment vector representations and assigned each segment to a cluster.", "labels": [], "entities": []}, {"text": "The emission probability distribution then captured the tendencies of a given class and generated the segment that is represented as a cluster.", "labels": [], "entities": []}, {"text": "These two distributions are estimated empirically from the training data, and we used Viterbi decoding to obtain the best labeling sequence during the prediction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Totalized statistics on the corpus.", "labels": [], "entities": []}, {"text": " Table 2: By-category descriptive statistics for the data practices in the corpus. These statistics are cal- culated prior to consolidating multiple annotators' work. Means and medians are calculated across the  population of policies in the corpus. Coverage and Kappa are calculated in terms of by-segment contents.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 263, "end_pos": 268, "type": "METRIC", "confidence": 0.9655471444129944}]}, {"text": " Table 3: Precision/Recall/F1 for the three models. The three starred categories resulted from the decom- position of the original Other category, which is excluded here. Categories are ordered in this table in  descending order by frequency in the dataset.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9899320602416992}, {"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.6575113534927368}, {"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.8251110911369324}]}]}