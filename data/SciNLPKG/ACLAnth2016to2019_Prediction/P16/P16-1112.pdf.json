{"title": [{"text": "Compositional Sequence Labeling Models for Error Detection in Learner Writing", "labels": [], "entities": [{"text": "Compositional Sequence Labeling", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.744879404703776}, {"text": "Error Detection", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.6719350218772888}, {"text": "Learner Writing", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.6903395503759384}]}], "abstractContent": [{"text": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing.", "labels": [], "entities": [{"text": "error detection in learner writing", "start_pos": 92, "end_pos": 126, "type": "TASK", "confidence": 0.6754009008407593}]}, {"text": "We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs.", "labels": [], "entities": [{"text": "error detection", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.6854572743177414}]}, {"text": "Experiments on the CoNLL-14 shared task dataset show the model is able to outper-form other participants on detecting errors in learner writing.", "labels": [], "entities": [{"text": "CoNLL-14 shared task dataset", "start_pos": 19, "end_pos": 47, "type": "DATASET", "confidence": 0.8499758839607239}]}, {"text": "Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated systems for detecting errors in learner writing are valuable tools for second language learning and assessment.", "labels": [], "entities": [{"text": "detecting errors in learner writing", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.7711238145828248}]}, {"text": "Most work in recent years has focussed on error correction, with error detection performance measured as a byproduct of the correction output ().", "labels": [], "entities": [{"text": "error correction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.711391344666481}, {"text": "error detection", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7498475611209869}]}, {"text": "However, this assumes that systems are able to propose a correction for every detected error, and accurate systems for correction might not be optimal for detection.", "labels": [], "entities": []}, {"text": "While closed-class errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks ().", "labels": [], "entities": []}, {"text": "Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold-standard corrections.", "labels": [], "entities": [{"text": "error correction", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.693028137087822}]}, {"text": "Therefore, we treat error detection in learner writing as an independent task and propose a system for labeling each token as being corrector incorrect in context.", "labels": [], "entities": [{"text": "error detection in learner writing", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.7621387243270874}]}, {"text": "Common approaches to similar sequence labeling tasks involve learning weights or probabilities for context n-grams of varying sizes, or relying on previously extracted high-confidence context patterns.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7375435928503672}]}, {"text": "Both of these methods can suffer from data sparsity, as they treat words as independent units and miss out on potentially related patterns.", "labels": [], "entities": []}, {"text": "In addition, they need to specify a fixed context size and are therefore often limited to using a small window near the target.", "labels": [], "entities": []}, {"text": "Neural network models aim to address these weaknesses and have achieved success in various NLP tasks such as language modeling () and speech recognition (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7688924372196198}, {"text": "speech recognition", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.8117254972457886}]}, {"text": "Recent developments in machine translation have also shown that text of varying length can be represented as a fixed-size vector using convolutional networks) or recurrent neural networks (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7679016590118408}]}, {"text": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing.", "labels": [], "entities": [{"text": "error detection in learner writing", "start_pos": 92, "end_pos": 126, "type": "TASK", "confidence": 0.6754009127616882}]}, {"text": "We perform a systematic comparison of alternative compositional structures for constructing informative context representations.", "labels": [], "entities": []}, {"text": "Based on the findings, we propose a novel framework for performing error detection in learner writing, which achieves state-of-the-art results on two datasets of errorannotated learner essays.", "labels": [], "entities": [{"text": "error detection", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7301333695650101}]}, {"text": "The sequence labeling model creates a single variable-size network over the whole sentence, conditions each label on all the words, and predicts all labels together.", "labels": [], "entities": []}, {"text": "The effects of different datasets on the overall performance are investigated by incrementally providing additional training data to the model.", "labels": [], "entities": []}, {"text": "Finally, we integrate the error detection framework with a publicly deployed self-assessment system, leading 1181 to performance comparable to human annotators.", "labels": [], "entities": [{"text": "error detection", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7038298100233078}]}], "datasetContent": [{"text": "We evaluate the alternative network structures on the publicly released First Certificate in English dataset (FCE-public,).", "labels": [], "entities": [{"text": "First Certificate in English dataset (FCE-public", "start_pos": 72, "end_pos": 120, "type": "DATASET", "confidence": 0.8719392589160374}]}, {"text": "The dataset contains short texts, written by learners of English as an additional language in response to exam prompts eliciting freetext answers and assessing mastery of the upperintermediate proficiency level.", "labels": [], "entities": []}, {"text": "The texts have been manually error-annotated using a taxonomy of 77 error types.", "labels": [], "entities": []}, {"text": "We use the released test set for evaluation, containing 2,720 sentences, leaving 30,953 sentences for training.", "labels": [], "entities": [{"text": "released test set", "start_pos": 11, "end_pos": 28, "type": "DATASET", "confidence": 0.7702575524648031}]}, {"text": "We further separate 2,222 sentences from the training set for development and hyper-parameter tuning.", "labels": [], "entities": [{"text": "hyper-parameter tuning", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7275414168834686}]}, {"text": "The dataset contains manually annotated error spans of various types of errors, together with their suggested corrections.", "labels": [], "entities": []}, {"text": "We convert this to a tokenlevel error detection task by labeling each token inside the error span as being incorrect.", "labels": [], "entities": [{"text": "tokenlevel error detection task", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.7017653658986092}]}, {"text": "In order to capture errors involving missing words, the error label is assigned to the token immediately after the incorrect gap -this is motivated by the intuition that while this token is correct when considered in isolation, it is incorrect in the current context, as another token should have preceeded it.", "labels": [], "entities": []}, {"text": "As the main evaluation measure for error detection we use F 0.5 , which was also the measure adopted in the CoNLL-14 shared task on error correction ().", "labels": [], "entities": [{"text": "error detection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7446814477443695}, {"text": "F 0.5", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9750203788280487}, {"text": "CoNLL-14 shared task on error correction", "start_pos": 108, "end_pos": 148, "type": "TASK", "confidence": 0.685134693980217}]}, {"text": "It combines both precision and recall, while assigning twice as much weight to precision, since accurate feedback is often more important than coverage in error detection applications Briscoe, 2015), require the system to propose a correction and are therefore not directly applicable on the task of error detection.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9993481040000916}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9985981583595276}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9983792304992676}, {"text": "error detection", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.6976722776889801}, {"text": "error detection", "start_pos": 300, "end_pos": 315, "type": "TASK", "confidence": 0.7211258858442307}]}, {"text": "During the experiments, the input text was lowercased and all tokens that occurred less than twice in the training data were represented as a single unk token.", "labels": [], "entities": []}, {"text": "Word embeddings were set to size 300 and initialised using the publicly released pretrained Word2Vec vectors ().", "labels": [], "entities": [{"text": "Word2Vec vectors", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9022097587585449}]}, {"text": "The convolutional networks use window size 3 on either side of the target token and produce a 300-dimensional context-dependent vector.", "labels": [], "entities": []}, {"text": "The recurrent networks use hidden layers of size 200 in either direction.", "labels": [], "entities": []}, {"text": "We also added an extra hidden layer of size 50 between each of the composition functions and the output layer -this allows the network to learn a separate non-linear transformation and reduces the dimensionality of the compositional vectors.", "labels": [], "entities": []}, {"text": "The parameters were optimised using gradient descent with initial learning rate 0.001, the ADAM algorithm ( for dynamically adapting the learning rate, and batch size of 64 sentences.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.8936769962310791}]}, {"text": "F 0.5 on the development set was evaluated at each epoch, and the best model was used for final evaluations.", "labels": [], "entities": []}, {"text": "contains results for experiments comparing different composition architectures on the task of error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 94, "end_pos": 109, "type": "TASK", "confidence": 0.6983633041381836}]}, {"text": "The CRF has the lowest F 0.5 score compared to any of the neural models.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9905003706614176}]}, {"text": "It memorises frequent error sequences with high precision, but does not generalise sufficiently, resulting in low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9972270131111145}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9988180994987488}]}, {"text": "The ability to condition on the previous label also does not provide much help on this task -there are only two possible labels and the errors are relatively sparse.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the CRF and alternative neural network structures on the public FCE dataset for  token-level error detection in learner writing.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9665181040763855}, {"text": "token-level error detection", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.673513283332189}]}, {"text": " Table 2: Results on the public FCE test set when  incrementally providing more training data to the  error detection model.", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.7946801582972208}, {"text": "error detection", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.679037407040596}]}, {"text": " Table 3: Error detection results on the two official annotations for the CoNLL-14 shared task test dataset.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9657317996025085}, {"text": "CoNLL-14 shared task test dataset", "start_pos": 74, "end_pos": 107, "type": "DATASET", "confidence": 0.8399710893630982}]}, {"text": " Table 3. We first eval- uated each of the human annotators with respect to  the other, in order to estimate the upper bound on  this task. The average F 0.5 of roughly 50% shows  that the task is difficult and even human experts  have a rather low agreement. It has been shown", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 152, "end_pos": 157, "type": "METRIC", "confidence": 0.9794972538948059}]}]}