{"title": [], "abstractContent": [{"text": "Automatically generating a natural language description of an image is a fundamental problem in artificial intelligence.", "labels": [], "entities": [{"text": "Automatically generating a natural language description of an image", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.7688233123885261}]}, {"text": "This task involves both computer vision and natural language processing and is called \"image caption generation.\"", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.8465722401936849}]}, {"text": "Research on image caption generation has typically focused on taking in an image and generating a caption in English as existing image caption corpora are mostly in English.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8596480886141459}]}, {"text": "The lack of corpora in languages other than English is an issue, especially for morphologically rich languages such as Japanese.", "labels": [], "entities": []}, {"text": "There is thus a need for corpora sufficiently large for image caption-ing in other languages.", "labels": [], "entities": [{"text": "image caption-ing", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7079683840274811}]}, {"text": "We have developed a Japanese version of the MS COCO caption dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese version of the dataset to generate a caption in Japanese.", "labels": [], "entities": [{"text": "MS COCO caption dataset", "start_pos": 44, "end_pos": 67, "type": "DATASET", "confidence": 0.8931784778833389}]}, {"text": "As the Japanese portion of the corpus is small, our model was designed to transfer the knowledge representation obtained from the English portion into the Japanese portion.", "labels": [], "entities": []}, {"text": "Experiments showed that the resulting bilingual comparable corpus has better performance than a monolingual corpus, indicating that image understanding using a resource-rich language benefits a resource-poor language.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7968541979789734}]}], "introductionContent": [{"text": "Automatically generating image captions by describing the content of an image using natural language sentences is a challenging task.", "labels": [], "entities": [{"text": "generating image captions", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.6479763289292654}]}, {"text": "It is especially challenging for languages other than En- * * Both authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "glish due to the sparsity of annotated resources in the target language.", "labels": [], "entities": []}, {"text": "A promising solution to this problem is to create a comparable corpus.", "labels": [], "entities": []}, {"text": "To support the image caption generation task in Japanese, we have annotated images taken from the MS COCO caption dataset) with Japanese captions.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.861484944820404}, {"text": "MS COCO caption dataset", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.9084672331809998}]}, {"text": "We call our corpus the \"YJ Captions 26k Dataset.\"", "labels": [], "entities": [{"text": "YJ Captions 26k Dataset", "start_pos": 24, "end_pos": 47, "type": "DATASET", "confidence": 0.8703653514385223}]}, {"text": "While the size of our dataset is comparatively large with 131,740 captions, it greatly trails the 1,026,459 captions in the MS COCO dataset.", "labels": [], "entities": [{"text": "MS COCO dataset", "start_pos": 124, "end_pos": 139, "type": "DATASET", "confidence": 0.9366465012232462}]}, {"text": "We were thus motivated to transfer the resources in English (source language) to Japanese and thereby improve image caption generation in Japanese (target language).", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.8315963943799337}]}, {"text": "In natural language processing, a task involving transferring information across languages is known as a cross-lingual natural language task, and well known tasks include cross-lingual sentiment analysis), cross-lingual named entity recognition, cross-lingual dependency parsing (, and cross-lingual information retrieval (.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6666383743286133}, {"text": "cross-lingual sentiment analysis", "start_pos": 171, "end_pos": 203, "type": "TASK", "confidence": 0.6723756988843282}, {"text": "cross-lingual named entity recognition", "start_pos": 206, "end_pos": 244, "type": "TASK", "confidence": 0.6200367286801338}, {"text": "cross-lingual dependency parsing", "start_pos": 246, "end_pos": 278, "type": "TASK", "confidence": 0.6974590222040812}, {"text": "cross-lingual information retrieval", "start_pos": 286, "end_pos": 321, "type": "TASK", "confidence": 0.6194779177506765}]}, {"text": "Existing work in the cross-lingual setting is usually formulated as follows.", "labels": [], "entities": []}, {"text": "First, to overcome the language barrier, create a connection between the source and target languages, generally by using a dictionary or parallel corpus.", "labels": [], "entities": []}, {"text": "Second, develop an appropriate knowledge transfer approach to leverage the annotated data from the source language for use in training a model in the target language, usually supervised or semi-supervised.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.768503725528717}]}, {"text": "These two steps typically amount to automatically generating and expanding the pseudo-training data for the target language by exploiting the knowledge obtained from the source language.", "labels": [], "entities": []}, {"text": "We propose a very simple approach to crosslingual image caption generation: exploit the English corpus to improve the performance of image caption generation in another language.", "labels": [], "entities": [{"text": "crosslingual image caption generation", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.7480187192559242}, {"text": "image caption generation", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.8014270961284637}]}, {"text": "In this ap-proach, no resources besides the images found in the corpus are used to connect the languages, and we consider our dataset to be a comparable corpus.", "labels": [], "entities": []}, {"text": "Paired texts in a comparable corpus describe the same topic, in this case an image, but unlike a parallel corpus, the texts are not exact translations of each other.", "labels": [], "entities": []}, {"text": "This unrestrictive setting enables the model to be used to create image caption resources in other languages.", "labels": [], "entities": []}, {"text": "Moreover, this model scales better than creating a parallel corpus with exact translations of the descriptions.", "labels": [], "entities": []}, {"text": "Our transfer model is very simple.", "labels": [], "entities": []}, {"text": "We start with a neural image caption model ( and pretrain it using the English portion of the corpus.", "labels": [], "entities": [{"text": "image caption", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7814687192440033}]}, {"text": "We then remove all of the trained neural network layers except for one crucial layer, the one closest to the vision system.", "labels": [], "entities": []}, {"text": "Next we attach an untrained Japanese generation model and train it using the Japanese portion of the corpus.", "labels": [], "entities": []}, {"text": "This results in improved generation in Japanese compared to using only the Japanese portion of the corpus.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first paper to address the problem of cross-lingual image caption generation.", "labels": [], "entities": [{"text": "cross-lingual image caption generation", "start_pos": 80, "end_pos": 118, "type": "TASK", "confidence": 0.770608477294445}]}, {"text": "First, we have created and plan to release the first ever significantly large corpus for image caption generation for the Japanese language, forming a comparable corpus with existing English datasets.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.8065448005994161}]}, {"text": "Second, we have created a very simple model based on neural image caption generation for Japanese that can exploit the English portion of the dataset.", "labels": [], "entities": [{"text": "neural image caption generation", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.7824112921953201}]}, {"text": "Again, we are the first to report results in cross-lingual image caption generation, and our surprisingly simple method improves the evaluation metrics significantly.", "labels": [], "entities": [{"text": "cross-lingual image caption generation", "start_pos": 45, "end_pos": 83, "type": "TASK", "confidence": 0.7420120313763618}]}, {"text": "This method is well suited as a baseline for future work on cross-lingual image caption generation.", "labels": [], "entities": [{"text": "cross-lingual image caption generation", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.7449480146169662}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we describe related work in image caption generation and list the corpora currently available for caption generation.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8515031536420187}, {"text": "caption generation", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.9154900014400482}]}, {"text": "Then in Section 3 we present the statistics for our corpus and explain how we obtained them.", "labels": [], "entities": []}, {"text": "We then explain our model in Section 4 and present the results of our experimental evaluation in Section 5.", "labels": [], "entities": []}, {"text": "We discuss the results in Section 6, and conclude in Section 7 with a summary of the key points.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because our caption dataset is annotated for only 26,500 images of the MS COCO training set, we reorganized the dataset split for our experiments.", "labels": [], "entities": [{"text": "MS COCO training set", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9288635700941086}]}, {"text": "Training and validation set images of the MS COCO dataset were mixed and split into four blocks, and these blocks were assigned to training, validation, and testing as shown in   We used six standard metrics for evaluating the quality of the generated Japanese sentences: BLEU-1, BLEU-2, BLEU-3, BLEU-4 (), ROUGE-L, and CIDEr-D ().", "labels": [], "entities": [{"text": "MS COCO dataset", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9106294512748718}, {"text": "BLEU-1", "start_pos": 272, "end_pos": 278, "type": "METRIC", "confidence": 0.9965217113494873}, {"text": "BLEU-2", "start_pos": 280, "end_pos": 286, "type": "METRIC", "confidence": 0.9836711883544922}, {"text": "BLEU-3", "start_pos": 288, "end_pos": 294, "type": "METRIC", "confidence": 0.9885560870170593}, {"text": "BLEU-4", "start_pos": 296, "end_pos": 302, "type": "METRIC", "confidence": 0.9918339252471924}, {"text": "ROUGE-L", "start_pos": 307, "end_pos": 314, "type": "METRIC", "confidence": 0.9964962601661682}]}, {"text": "We used the COCO caption evaluation tool () to compute the metrics.", "labels": [], "entities": [{"text": "COCO caption evaluation", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.8276310761769613}]}, {"text": "BLEU () was originally designed for automatic machine translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9740042686462402}, {"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7495611608028412}]}, {"text": "By counting n-gram co-occurrences, it rates the quality of a translated sentence given several reference sentences.", "labels": [], "entities": []}, {"text": "To apply BLEU, we considered that generating image captions is the same as translating images into sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9966084957122803}]}, {"text": "ROUGE) is an evaluation metric designed by adapting BLEU to evaluate automatic text summarization algorithms.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9271151423454285}, {"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9626411199569702}, {"text": "text summarization", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.6566021293401718}]}, {"text": "ROUGE is based on the longest common subsequences instead of n-grams.: Evaluation Metrics ated for test set images.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9428420662879944}]}, {"text": "Our proposed model is labeled \"transfer.\"", "labels": [], "entities": []}, {"text": "As you can see, it outperformed the other two models for every metric.", "labels": [], "entities": []}, {"text": "In particular, the CIDEr-D score was about 4% higher than that for the monolingual baseline.", "labels": [], "entities": [{"text": "CIDEr-D score", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.8256259262561798}]}, {"text": "The performance of a model trained using the English and Japanese corpora alternately is shown on the line label \"alternate.\"", "labels": [], "entities": [{"text": "alternate", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9507439732551575}]}, {"text": "Surprisingly, this model had lower performance than the baseline model.", "labels": [], "entities": []}, {"text": "In, we plot the learning curves represented by the CIDEr score for the Japanese captions generated for the validation set images.", "labels": [], "entities": [{"text": "CIDEr score", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.5780326724052429}]}, {"text": "Transfer learning from English to Japanese converged faster than learning from the Japanese dataset or learning by training from both languages alternately.", "labels": [], "entities": []}, {"text": "shows the relationship between the CIDEr score and the Japanese dataset size (number of images).", "labels": [], "entities": [{"text": "Japanese dataset size", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.8140663305918375}]}, {"text": "The models pretrained using English captions (blue line) outperformed the ones trained using only Japanese captions for all training dataset sizes.", "labels": [], "entities": []}, {"text": "As can be seen by comparing the case of 4,000 images with that of 20,000 images, the improvement due to cross-lingual transfer was larger when the Japanese dataset was smaller.", "labels": [], "entities": [{"text": "Japanese dataset", "start_pos": 147, "end_pos": 163, "type": "DATASET", "confidence": 0.7923706769943237}]}, {"text": "These results show that pretraining the model with all available English captions is roughly equivalent to training the model with captions for 10,000 additional images in Japanese.", "labels": [], "entities": []}, {"text": "This, in our case, nearly halves the cost of building the corpus.", "labels": [], "entities": []}, {"text": "Examples of machine-generated captions along with the crowd-written ground truth captions (English translations) are shown in.", "labels": [], "entities": [{"text": "crowd-written ground truth captions", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.6733215898275375}]}], "tableCaptions": []}