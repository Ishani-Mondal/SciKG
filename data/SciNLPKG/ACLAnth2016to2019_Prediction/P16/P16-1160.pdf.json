{"title": [{"text": "A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.7448137402534485}]}], "abstractContent": [{"text": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7383323013782501}]}, {"text": "In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation?", "labels": [], "entities": [{"text": "neural machine translation generate a character sequence", "start_pos": 50, "end_pos": 106, "type": "TASK", "confidence": 0.8046789595058986}]}, {"text": "To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs-En-Cs, En-De, En-Ru and En-Fi-using the parallel corpora from WMT'15.", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 216, "end_pos": 222, "type": "DATASET", "confidence": 0.961371898651123}]}, {"text": "Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs.", "labels": [], "entities": []}, {"text": "Furthermore , the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.", "labels": [], "entities": []}], "introductionContent": [{"text": "The existing machine translation systems have relied almost exclusively on word-level modelling with explicit segmentation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7282724231481552}]}, {"text": "This is mainly due to the issue of data sparsity which becomes much more severe, especially for n-grams, when a sentence is represented as a sequence of characters rather than words, as the length of the sequence grows significantly.", "labels": [], "entities": []}, {"text": "In addition to data sparsity, we often have a priori belief that a word, or its segmented-out lexeme, is a basic unit of meaning, making it natural to approach translation as mapping from a sequence of source-language words to a sequence of target-language words.", "labels": [], "entities": []}, {"text": "This has continued with the more recently proposed paradigm of neural machine translation, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary ().", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7042691111564636}]}, {"text": "Therefore, in this paper, we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit word segmentation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.701375683148702}]}, {"text": "To answer this question, we focus on representing the target side as a character sequence.", "labels": [], "entities": []}, {"text": "We evaluate neural machine translation models with a character-level decoder on four language pairs from WMT'15 to make our evaluation as convincing as possible.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.6872991323471069}, {"text": "WMT'15", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9576393961906433}]}, {"text": "We represent the source side as a sequence of subwords extracted using byte-pair encoding from , and vary the target side to be either a sequence of subwords or characters.", "labels": [], "entities": []}, {"text": "On the target side, we further design a novel recurrent neural network (RNN), called biscale recurrent network, that better handles multiple timescales in a sequence, and test it in addition to a naive, stacked recurrent neural network.", "labels": [], "entities": []}, {"text": "On all of the four language pairs-En-Cs, En-De, En-Ru and En-Fi-, the models with a characterlevel decoder outperformed the ones with a subword-level decoder.", "labels": [], "entities": []}, {"text": "We observed a similar trend with the ensemble of each of these configurations, outperforming both the previous best neural and non-neural translation systems on EnCs, En-De and En-Fi, while achieving a comparable result on En-Ru.", "labels": [], "entities": []}, {"text": "We find these results to be a strong evidence that neural machine translation can indeed learn to translate at the character-level and that in fact, it benefits from doing so.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.6831009387969971}]}], "datasetContent": [{"text": "For evaluation, we represent a source sentence as a sequence of subword symbols extracted by bytepair encoding (BPE, ) and a target sentence either as a sequence of BPE-based symbols or as a sequence of characters.", "labels": [], "entities": []}, {"text": "Corpora and Preprocessing We use all available parallel corpora for four language pairs from WMT'15: En-Cs, En-De, En-Ru and En-Fi.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9755802750587463}, {"text": "WMT'15", "start_pos": 93, "end_pos": 99, "type": "DATASET", "confidence": 0.9403908252716064}]}, {"text": "They consist of 12.1M, 4.5M, 2.3M and 2M sentence pairs, respectively.", "labels": [], "entities": []}, {"text": "We tokenize each corpus using a tokenization script included in Moses.", "labels": [], "entities": []}, {"text": "We only use the sentence pairs, when the source side is up to 50 subword symbols long and the target side is either up to 100 subword symbols or 500 characters.", "labels": [], "entities": []}, {"text": "We do not use any monolingual corpus.", "labels": [], "entities": []}, {"text": "For all the pairs other than En-Fi, we use newstest-2013 as a development set, and newstest-2014 (Test 1 ) and newstest-2015 (Test 2 ) as test sets.", "labels": [], "entities": []}, {"text": "For En-Fi, we use newsdev-2015 and newstest-2015 as development and test sets, respectively.", "labels": [], "entities": []}], "tableCaptions": []}