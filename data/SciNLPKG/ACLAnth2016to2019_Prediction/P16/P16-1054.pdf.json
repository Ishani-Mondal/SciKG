{"title": [{"text": "Towards more variation in text generation: Developing and evaluating variation models for choice of referential form", "labels": [], "entities": [{"text": "text generation", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7540944516658783}]}], "abstractContent": [{"text": "In this study, we introduce a non-deterministic method for referring expression generation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.8255080779393514}]}, {"text": "We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network.", "labels": [], "entities": []}, {"text": "Both are evaluated using the VaREG corpus.", "labels": [], "entities": [{"text": "VaREG corpus", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9385934174060822}]}, {"text": "Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model.", "labels": [], "entities": [{"text": "GREC-2.0 corpus", "start_pos": 89, "end_pos": 104, "type": "DATASET", "confidence": 0.9608581066131592}]}], "introductionContent": [{"text": "Automatic text generation is the process of converting non-linguistic data into coherent and comprehensible text).", "labels": [], "entities": [{"text": "Automatic text generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6241209109624227}]}, {"text": "In recent years, interest in text generation has substantially increased, due to the emergence of new applications such as \"robot-journalism\".", "labels": [], "entities": [{"text": "text generation", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.8494252264499664}]}, {"text": "Even though computers these days are perfectly capable of automatically producing text, the results are arguably often rather rigid, always producing the same kind and style of text, which makes them somewhat \"boring\" to read, especially when reading multiple texts in succession.", "labels": [], "entities": []}, {"text": "Human-written texts, by contrast, do not suffer from this problem, presumably because human authors have an innate tendency to produce variation in their use of words and constructions.", "labels": [], "entities": []}, {"text": "Indeed, psycholinguistic research has shown that when speakers produce referring expressions in comparable contexts, they non-deterministically vary both the form and the contents of their references (.", "labels": [], "entities": []}, {"text": "In this paper, we present and evaluate models of referring expression generation that mimic this human non-determinacy and show that this enables us to generate varied references in texts, which, in terms of coherence and comprehensibility, did not yield significant differences from human-produced references according to human judges.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.680252214272817}]}, {"text": "In particular, in this study we focus on the choice of referential form, which is the first decision to be made by referring expression generation models) and which determines whether a reference takes the form of a proper name, a pronoun, a definite description, etc.", "labels": [], "entities": []}, {"text": "Several such models have been proposed.", "labels": [], "entities": []}, {"text": "However, all of these are fully deterministic, always choosing the same referential form in the same context.", "labels": [], "entities": []}, {"text": "The fact that these models are generally based on text corpora which have only one gold standard form per reference (the one produced by the original author) does not help either.", "labels": [], "entities": []}, {"text": "When the corpus contains, say, a description at some point in the text, this does not mean that, for example, a proper name could not occur in that position as well.", "labels": [], "entities": []}, {"text": "Generally, we just don't know.", "labels": [], "entities": []}, {"text": "To counter this problem, a recent corpus, called VaREG, was developed in which 20 different writers were asked to produce references fora particular topic in a variety of texts, giving rise to a distribution over forms per reference.", "labels": [], "entities": [{"text": "VaREG", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8161941170692444}]}, {"text": "This gives us the possibility to distinguish situations where there is more or less agreement between writers in their choices of referential form.", "labels": [], "entities": []}, {"text": "But it also enables anew paradigm for choosing referential forms, where instead of predicting the most likely referential form, we can in fact predict the frequency in which a reference assumes a specific form, allowing us to turn the choice of referential form into a non-deterministic probabilistic model.", "labels": [], "entities": []}, {"text": "In this study, we introduce two different models that take the individual variation into account for the choice of referential form, one based on Naive Bayes and one on Recurrent Neural Networks.", "labels": [], "entities": []}, {"text": "Both are evaluated using the VaREG corpus.", "labels": [], "entities": [{"text": "VaREG corpus", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9385934174060822}]}, {"text": "Furthermore, we use the best performing model to generate referential forms in texts from the GREC-2.0 corpus, based on the roulette-wheel generation process, and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model.", "labels": [], "entities": [{"text": "GREC-2.0 corpus", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9647121429443359}]}], "datasetContent": [{"text": "For each reference slot encountered in the VaREG corpus, we evaluated how well a model takes the individual variation into account in the choice of referential form by comparing its predicted distribution of referential forms (\u02c6 y) with the real distribution (y).", "labels": [], "entities": [{"text": "VaREG corpus", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.9138137698173523}]}, {"text": "We performed this comparison through two experiments.", "labels": [], "entities": []}, {"text": "In the first, the models were trained and tested with VaREG corpus.", "labels": [], "entities": [{"text": "VaREG corpus", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.7621759474277496}]}, {"text": "In the second, we aimed to check to what extent the referring expressions from the GREC-2.0 corpus are similar inform to the referring expressions from VaREG corpus by training the models with the first corpus and testing with the second.", "labels": [], "entities": [{"text": "GREC-2.0 corpus", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.9463786482810974}, {"text": "VaREG corpus", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.9275788366794586}]}, {"text": "We evaluated three versions of each text.", "labels": [], "entities": []}, {"text": "The Original is the original text in the corpus, including the original referring expressions selected by the author.", "labels": [], "entities": []}, {"text": "We compare this with a Random variant, which does include variation of referential forms, but selects them in a fully random way.", "labels": [], "entities": []}, {"text": "Finally, in the third, Generated version, all references are generated according to the method outlined at Section 6.1.", "labels": [], "entities": []}, {"text": "depicts an example of text in the three versions.", "labels": [], "entities": []}, {"text": "In total, we make 3 versions of 9 pseudorandomly selected texts (5 covering animate topics and 4 inanimate ones, varying in length) from the GREC-2.0 corpus, yielding 27 texts in total.", "labels": [], "entities": [{"text": "GREC-2.0 corpus", "start_pos": 141, "end_pos": 156, "type": "DATASET", "confidence": 0.9571217894554138}]}, {"text": "These were distributed over 3 lists, such that each list contains one variant of each text, and there is an equal number of texts from the 3 conditions (Original, Random, Generated).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. We chose them based on an  ad-hoc analysis, where we searched for an optimal  combination to obtain the best predictions.", "labels": [], "entities": []}, {"text": " Table 3: Average Jensen-Shannon divergence and  Spearman's correlation coefficient of the models  in Experiment 1.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient", "start_pos": 49, "end_pos": 83, "type": "METRIC", "confidence": 0.9128674417734146}]}, {"text": " Table 4: Average Jensen-Shannon divergence and  Spearman's correlation coefficient of the models  in Experiment 2.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient", "start_pos": 49, "end_pos": 83, "type": "METRIC", "confidence": 0.9027532637119293}]}]}