{"title": [{"text": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.9218177199363708}]}], "abstractContent": [{"text": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.660186231136322}]}, {"text": "In this paper, we introduce a novel neutral network architecture that benefits from both word-and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF.", "labels": [], "entities": []}, {"text": "Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.6011334359645844}]}, {"text": "We evaluate our system on two data sets for two sequence labeling tasks-Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER).", "labels": [], "entities": [{"text": "sequence labeling tasks-Penn Treebank WSJ corpus", "start_pos": 48, "end_pos": 96, "type": "TASK", "confidence": 0.6512401700019836}, {"text": "part-of-speech (POS) tagging", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.5851055383682251}, {"text": "CoNLL 2003 corpus", "start_pos": 134, "end_pos": 151, "type": "DATASET", "confidence": 0.8853230873743693}, {"text": "named entity recognition (NER)", "start_pos": 156, "end_pos": 186, "type": "TASK", "confidence": 0.7305758247772852}]}, {"text": "We obtain state-of-the-art performance on both datasets-97.55% accuracy for POS tagging and 91.21% F1 for NER.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9995019435882568}, {"text": "POS tagging", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.7726956903934479}, {"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.99953293800354}, {"text": "NER", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.7363715767860413}]}], "introductionContent": [{"text": "Linguistic sequence labeling, such as part-ofspeech (POS) tagging and named entity recognition (NER), is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community.", "labels": [], "entities": [{"text": "Linguistic sequence labeling", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6459399163722992}, {"text": "part-ofspeech (POS) tagging", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6413515925407409}, {"text": "named entity recognition (NER)", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.7869162758191427}, {"text": "deep language understanding", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.6165478229522705}]}, {"text": "Natural language processing (NLP) systems, like syntactic parsing ( and entity coreference resolution, are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7297367105881373}, {"text": "syntactic parsing", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.710181251168251}, {"text": "entity coreference resolution", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.7658083041508993}, {"text": "POS tagging", "start_pos": 187, "end_pos": 198, "type": "TASK", "confidence": 0.694327712059021}]}, {"text": "Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF), which rely heavily on hand-crafted features and taskspecific resources.", "labels": [], "entities": []}, {"text": "For example, English POS taggers benefit from carefully designed word spelling features; orthographic features and external resources such as gazetteers are widely used in NER.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.64034104347229}, {"text": "word spelling", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7173409461975098}]}, {"text": "However, such task-specific knowledge is costly to develop, making sequence labeling models difficult to adapt to new tasks or new domains.", "labels": [], "entities": []}, {"text": "In the past few years, non-linear neural networks with as input distributed word representations, also known as word embeddings, have been broadly applied to NLP problems with great success.", "labels": [], "entities": []}, {"text": "proposed a simple but effective feed-forward neutral network that independently classifies labels for each word by using contexts within a window with fixed size.", "labels": [], "entities": []}, {"text": "Recently, recurrent neural networks (RNN), together with its variants such as long-short term memory (LSTM)) and gated recurrent unit (GRU) (), have shown great success in modeling sequential data.", "labels": [], "entities": []}, {"text": "Several RNN-based neural network models have been proposed to solve sequence labeling tasks like speech recognition (, POS tagging (  and NER (, achieving competitive performance against traditional models.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7402929216623306}, {"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.749684602022171}]}, {"text": "However, even systems that have utilized distributed representations as inputs have used these to augment, rather than replace, hand-crafted features (e.g. word spelling and capitalization patterns).", "labels": [], "entities": [{"text": "word spelling", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.6802906692028046}]}, {"text": "Their performance drops rapidly when the models solely depend on neural embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we propose a neural network architecture for sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.6817459464073181}]}, {"text": "It is a truly endto-end model requiring no task-specific resources, feature engineering, or data pre-processing beyond pre-trained word embeddings on unlabeled corpora.", "labels": [], "entities": []}, {"text": "Thus, our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.7278857032457987}]}, {"text": "We first use convolutional neural networks (CNNs) ( to encode character-level information of a word into its character-level representation.", "labels": [], "entities": []}, {"text": "Then we combine character-and word-level representations and feed them into bi-directional LSTM (BLSTM) to model context information of each word.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.8234302401542664}]}, {"text": "On top of BLSTM, we use a sequential CRF to jointly decode labels for the whole sentence.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.5071502923965454}]}, {"text": "We evaluate our model on two linguistic sequence labeling tasks -POS tagging on Penn Treebank WSJ (, and NER on English data from the CoNLL 2003 shared task.", "labels": [], "entities": [{"text": "linguistic sequence labeling", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.6301237940788269}, {"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7545392513275146}, {"text": "Penn Treebank WSJ", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.977763036886851}, {"text": "NER on English data from the CoNLL 2003 shared task", "start_pos": 105, "end_pos": 156, "type": "DATASET", "confidence": 0.7085326969623565}]}, {"text": "Our end-to-end model outperforms previous stateof-the-art systems, obtaining 97.55% accuracy for POS tagging and 91.21% F1 for NER.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9993348717689514}, {"text": "POS tagging", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.7628098726272583}, {"text": "F1", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9997618794441223}]}, {"text": "The contributions of this work are (i) proposing a novel neural network architecture for linguistic sequence labeling.", "labels": [], "entities": [{"text": "linguistic sequence labeling", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.6522368888060252}]}, {"text": "(ii) giving empirical evaluations of this model on benchmark data sets for two classic NLP tasks.", "labels": [], "entities": []}, {"text": "(iii) achieving state-of-the-art performance with this truly end-to-end system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Hyper-parameters for all experiments.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9621365666389465}]}, {"text": " Table 2: Corpora statistics. SENT and TOKEN  refer to the number of sentences and tokens in each  data set.", "labels": [], "entities": [{"text": "SENT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9979467988014221}, {"text": "TOKEN", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9927800297737122}]}, {"text": " Table 3: Performance of our model on both the development and test sets of the two tasks, together with  three baseline systems.", "labels": [], "entities": []}, {"text": " Table 4: POS tagging accuracy of our model on  test data from WSJ proportion of PTB, together  with top-performance systems. The neural net- work based models are marked with  \u2021.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7478060722351074}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9696765542030334}, {"text": "WSJ", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.7351213693618774}, {"text": "PTB", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.5603228807449341}]}, {"text": " Table 6: Results with different choices of word  embeddings on the two tasks (accuracy for POS  tagging and F1 for NER).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994719624519348}, {"text": "POS  tagging", "start_pos": 92, "end_pos": 104, "type": "TASK", "confidence": 0.7615843713283539}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.996478259563446}]}, {"text": " Table 7: Results with and without dropout on two  tasks (accuracy for POS tagging and F1 for NER).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9995983242988586}, {"text": "POS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.772370845079422}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9979934692382812}, {"text": "NER", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.8103203177452087}]}, {"text": " Table 8: Statistics of the partition on each corpus.  It lists the number of tokens of each subset for POS  tagging and the number of entities for NER.", "labels": [], "entities": [{"text": "POS  tagging", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.7891063988208771}]}, {"text": " Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9994650483131409}, {"text": "F1", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9944197535514832}]}]}