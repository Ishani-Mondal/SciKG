{"title": [{"text": "Which Coreference Evaluation Metric Do You Trust? A Proposal fora Link-based Entity Aware Metric", "labels": [], "entities": []}], "abstractContent": [{"text": "Interpretability and discriminative power are the two most basic requirements for an evaluation metric.", "labels": [], "entities": [{"text": "Interpretability", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8295790553092957}]}, {"text": "In this paper, we report the mention identification effect in the B 3 , CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly.", "labels": [], "entities": [{"text": "mention identification", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7495372593402863}, {"text": "CEAF", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.7442817091941833}, {"text": "BLANC coreference evaluation metrics", "start_pos": 82, "end_pos": 118, "type": "DATASET", "confidence": 0.7795037180185318}]}, {"text": "The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric.", "labels": [], "entities": [{"text": "MUC", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.6382494568824768}]}, {"text": "It is a known fact that none of the current metrics are reliable.", "labels": [], "entities": []}, {"text": "The common practice for ranking coreference resolvers is to use the average of three different metrics.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.8739796578884125}]}, {"text": "However, one cannot expect to obtain a reliable score by averaging three unreliable metrics.", "labels": [], "entities": []}, {"text": "We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics.", "labels": [], "entities": [{"text": "LEA", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9480774402618408}]}, {"text": "LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.", "labels": [], "entities": [{"text": "LEA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9078919887542725}, {"text": "CoNLL scorer", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.8666906654834747}]}], "introductionContent": [{"text": "There exists a variety of models (e.g. pairwise, entity-based, and ranking) and feature sets (e.g. string match, lexical, syntactic, and semantic) to be used in coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.9605347812175751}]}, {"text": "There is no known formal way to prove which coreference model is superior to the others and which set of features is more beneficial/less useful in coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.9581882655620575}]}, {"text": "The only way to compare different models, features or implementations of coreference resolvers is to compare the values of the existing coreference resolution evaluation metrics.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.9157523214817047}, {"text": "coreference resolution evaluation", "start_pos": 136, "end_pos": 169, "type": "TASK", "confidence": 0.8756094574928284}]}, {"text": "By comparing the evaluation scores, we determine which system performs best, which model suits coreference resolution better, and which feature set is useful for improving the recall or precision of a coreference resolver.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.9066879749298096}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.998736560344696}, {"text": "precision", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.9438300132751465}, {"text": "coreference resolver", "start_pos": 201, "end_pos": 221, "type": "TASK", "confidence": 0.8458058536052704}]}, {"text": "Therefore, evaluation metrics play an important role in the advancement of the underlying technology.", "labels": [], "entities": []}, {"text": "It is imperative for the evaluation metrics to be reliable.", "labels": [], "entities": []}, {"text": "However, it is not a trivial task to score output entities with various kinds of coreference errors.", "labels": [], "entities": []}, {"text": "Several evaluation metrics have been introduced for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.9843330681324005}]}, {"text": "Metrics that are being used widely are MUC (, B 3 (Bagga and Baldwin, 1998), CEAF (, and BLANC ().", "labels": [], "entities": [{"text": "MUC", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.46361151337623596}, {"text": "B 3", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9467776417732239}, {"text": "CEAF", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.6678146123886108}, {"text": "BLANC", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9922427535057068}]}, {"text": "There are known flaws for each of these metrics.", "labels": [], "entities": []}, {"text": "Besides, the agreement between all these metrics is relatively low, and it is not clear which metric is the most reliable.", "labels": [], "entities": [{"text": "agreement", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9875138401985168}]}, {"text": "The CoNLL-2011/2012 shared tasks) ranked participating systems using an average of three metrics, i.e. MUC, B 3 , and CEAF, following a proposal by.", "labels": [], "entities": [{"text": "CoNLL-2011/2012 shared tasks", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.8797560214996338}, {"text": "MUC", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.7914774417877197}, {"text": "B 3", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.8828150033950806}, {"text": "CEAF", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.665830671787262}]}, {"text": "Averaging three unreliable scores does not result in a reliable one.", "labels": [], "entities": []}, {"text": "Besides, when an average score is used for comparisons, it is not possible to analyse recall and precision to determine which output is more precise and which one covers more coreference information.", "labels": [], "entities": [{"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9962592124938965}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9909033179283142}]}, {"text": "This is indeed a requirement for coreference resolvers to be used in end-tasks.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.917506605386734}]}, {"text": "Therefore, averaging individual metrics is nothing but a compromise.", "labels": [], "entities": []}, {"text": "As mentioned by, interpretability and discriminative power are two basic requirements fora reasonable evaluation metric.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.9710590243339539}]}, {"text": "In regard to the interpretability requirement a high score should indicate that the vast majority of coreference relations and entities are detected correctly.", "labels": [], "entities": []}, {"text": "Similarly, a system that resolves none of the coreference relations or entities should get a zero score.: Counterintuitive values of B 3 , CEAF and BLANC recall and precision.", "labels": [], "entities": [{"text": "Counterintuitive", "start_pos": 106, "end_pos": 122, "type": "METRIC", "confidence": 0.9467129111289978}, {"text": "B 3", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9840394556522369}, {"text": "CEAF", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.8830577731132507}, {"text": "BLANC", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.9954406023025513}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9224628806114197}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.998902440071106}]}, {"text": "An evaluation metric should also be discriminative.", "labels": [], "entities": []}, {"text": "It should be able to discriminate between good and bad coreference decisions.", "labels": [], "entities": []}, {"text": "In this paper, we report on a drawback for B 3 , CEAF, and BLANC which violates the interpretability requirement.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.7245745658874512}, {"text": "BLANC", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.8789804577827454}, {"text": "interpretability", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.9653088450431824}]}, {"text": "We also show that this flaw invalidates the recall/precision analysis of coreference outputs based on these three metrics.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9981694221496582}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.8003762364387512}]}, {"text": "We then review the current evaluation metrics with their known flaws to explain why we cannot trust them and need anew reliable one.", "labels": [], "entities": []}, {"text": "Finally, we propose LEA, a Link-based Entity Aware evaluation metric that is designed to overcome problems of the existing metrics.", "labels": [], "entities": [{"text": "LEA", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9773969650268555}]}, {"text": "We have begun the process of integrating the LEA metric in the official CoNLL scorer 1 so as to continue the progress made in recent years to produce replicable evaluation metrics.", "labels": [], "entities": [{"text": "LEA", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8165752291679382}, {"text": "official CoNLL scorer 1", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.7697722762823105}]}, {"text": "In order to use the LEA metric, there is no additional requirement than that of the CoNLL scorer v8.01 2 .", "labels": [], "entities": [{"text": "LEA metric", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.8472079932689667}, {"text": "CoNLL scorer v8.01 2", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.9053822904825211}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Counterintuitive values of B 3 , CEAF and BLANC recall and precision.", "labels": [], "entities": [{"text": "Counterintuitive", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.906219482421875}, {"text": "B 3", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.957406610250473}, {"text": "CEAF", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.6081593632698059}, {"text": "BLANC", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9800297617912292}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.8998160362243652}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9990845918655396}]}, {"text": " Table 3: F 1 scores for Table 2's response entities.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9895169138908386}]}, {"text": " Table 4: Results on the CoNLL 2012 test set.", "labels": [], "entities": [{"text": "CoNLL 2012 test set", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9585544466972351}]}, {"text": " Table 5: The results of the CoNLL 2012 shared task.", "labels": [], "entities": [{"text": "CoNLL 2012 shared task", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.7688980996608734}]}]}