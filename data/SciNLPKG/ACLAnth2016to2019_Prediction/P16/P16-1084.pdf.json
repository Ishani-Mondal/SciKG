{"title": [{"text": "How Well Do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation", "labels": [], "entities": [{"text": "Computers Solve Math Word Problems", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.7102651238441468}, {"text": "Large-Scale Dataset Construction and Evaluation", "start_pos": 48, "end_pos": 95, "type": "TASK", "confidence": 0.6933608293533325}]}], "abstractContent": [{"text": "Recently a few systems for automatically solving math word problems have reported promising results.", "labels": [], "entities": [{"text": "automatically solving math word problems", "start_pos": 27, "end_pos": 67, "type": "TASK", "confidence": 0.7412055850028991}]}, {"text": "However, the datasets used for evaluation have limitations in both scale and diversity.", "labels": [], "entities": []}, {"text": "In this paper, we build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types.", "labels": [], "entities": []}, {"text": "Problems in the dataset are semi-automatically obtained from community question-answering (CQA) web pages.", "labels": [], "entities": []}, {"text": "A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users, which significantly reduces human annotation cost.", "labels": [], "entities": []}, {"text": "Experiments conducted on the new dataset lead to interesting and surprising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s.", "labels": [], "entities": [{"text": "automatically solving math word problems", "start_pos": 31, "end_pos": 71, "type": "TASK", "confidence": 0.7215995192527771}]}, {"text": "As early proposals seldom report empirical evaluation results, it is unclear how well they perform.", "labels": [], "entities": []}, {"text": "Recently, promising results have been reported on both statistical learning approaches ( and semantic parsing methods ().", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.7393876314163208}]}, {"text": "However, we observe two limitations on the datasets used by these previous works.", "labels": [], "entities": []}, {"text": "First, the datasets are small.", "labels": [], "entities": []}, {"text": "The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems.", "labels": [], "entities": []}, {"text": "The Dolphin1878 * Work done while this author was an intern at Microsoft Research.", "labels": [], "entities": [{"text": "Dolphin1878", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7091651558876038}]}, {"text": "dataset, the largest collection among them, contains 1878 problems.", "labels": [], "entities": []}, {"text": "Second, the diversity of problems in the datasets is low.", "labels": [], "entities": []}, {"text": "The Alg514 collection contains linear algebra problems of 28 types (determined by 28 unique equation systems), with each problem type corresponding to at least 6 problems.", "labels": [], "entities": [{"text": "Alg514 collection", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9571395814418793}]}, {"text": "Although the Dolphin1878 collection has over 1,000 problem types, only number word problems (i.e., math word problems about the operations and relationship of numbers) are contained in the collection.", "labels": [], "entities": [{"text": "Dolphin1878 collection", "start_pos": 13, "end_pos": 35, "type": "DATASET", "confidence": 0.9738486707210541}]}, {"text": "Due to the above two limitations, observations and conclusions based on existing datasets may not be representative.", "labels": [], "entities": []}, {"text": "Therefore it is hard to give a convincing answer to the following question: How well do state-of-the-art computer algorithms perform in solving math word problems?", "labels": [], "entities": []}, {"text": "To answer this question, we need to re-evaluate state-of-the-art approaches on a larger and more diverse data set.", "labels": [], "entities": []}, {"text": "It is not hard to collect a large set of problems from the web.", "labels": [], "entities": []}, {"text": "The real challenge comes from attaching annotations to the problems.", "labels": [], "entities": []}, {"text": "Important annotation types include equation systems (required by most statistical learning methods for model training) and gold answers (for testing algorithm performance).", "labels": [], "entities": []}, {"text": "Manually adding equation systems and gold answers is extremely time-consuming . In this paper, we build a large-scale and diverse dataset called Dolphin18K 2 , which contains over 18,000 annotated math word problems.", "labels": [], "entities": [{"text": "Dolphin18K 2", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.8846008777618408}]}, {"text": "It is constructed by semi-automatically extracting problems, equation systems and answers from community question-answering (CQA) web pages.", "labels": [], "entities": []}, {"text": "The source data we leverage are the (question, answer text) pairs in the math category of Yahoo!", "labels": [], "entities": []}, {"text": "An-1 According to our experience, the speed is about 10-15 problems per hour fora person with good math skills.", "labels": [], "entities": [{"text": "speed", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.98895663022995}]}, {"text": "Available from http://research.microsoft.com/enus/projects/dolphin/.", "labels": [], "entities": []}, {"text": "swers . Please note that the answer text provided by CQA users cannot be used directly in evaluation as gold answers, because answer numbers and other numbers are often mixed together in answer text (refer to of Section 3).", "labels": [], "entities": [{"text": "CQA", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9413226246833801}]}, {"text": "We train a ranking SVM model to identify (structured) problem answers from unstructured answer text.", "labels": [], "entities": []}, {"text": "We then conduct experiments to test the performance of some recent math problem solving systems on the dataset.", "labels": [], "entities": [{"text": "math problem solving", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.6573989590009054}]}, {"text": "We make the following main observations, 1.", "labels": [], "entities": []}, {"text": "All systems evaluated on the Dolphin18K dataset perform much worse than on their original small and less diverse datasets.", "labels": [], "entities": [{"text": "Dolphin18K dataset", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.9869092702865601}]}, {"text": "2. On the large dataset, a simple similaritybased method performs as well as more sophisticated statistical learning approaches.", "labels": [], "entities": []}, {"text": "3. System performance improves sub-linearly as more training data is used.", "labels": [], "entities": []}, {"text": "This suggests that we need to develop algorithms which can utilize data more effectively.", "labels": [], "entities": []}, {"text": "Our experiments indicate that the problem of automatic math word problem solving is still far from being solved.", "labels": [], "entities": [{"text": "automatic math word problem solving", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.6349235713481903}]}, {"text": "Good results obtained on small datasets may not be good indicators of high performance on larger and diverse datasets.", "labels": [], "entities": []}, {"text": "For current methods, simply adding more training data is not an effective way to improve performance.", "labels": [], "entities": []}, {"text": "New methodologies are required for this topic.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to construct a large and diverse problem collection of elementary mathematics (i.e., math topics frequently taught at the primary or secondary school levels).", "labels": [], "entities": []}, {"text": "We build our dataset by automatically extracting problems and their annotations from the mathematics category of the Yahoo!", "labels": [], "entities": []}, {"text": "A math problem post on Yahoo!", "labels": [], "entities": [{"text": "math problem post", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.8466944098472595}]}, {"text": "Answers consists of the raw problem text and one or multiple pieces of answer text provided by its answerers (refer to).", "labels": [], "entities": []}, {"text": "Please note that posts cannot be used directly as our dataset entries.", "labels": [], "entities": []}, {"text": "For example, for training statistical models, we have to extract equation systems from the unstructured text of user answers.", "labels": [], "entities": []}, {"text": "We also need to extract numbers (56,000 and 21,000 in from the raw answer text as gold answers.", "labels": [], "entities": []}, {"text": "We perform the following actions to the posts, \u2022 Removing the posts that do not contain a math problem of our scope (Section 3.1) \u2022 Cleaning problem text (Section 3.1) \u2022 Extracting gold answers (Section 3.2) \u2022 Extracting equation systems (Section 3.3) In Section 3.4, we report some statistics of our dataset and compare them with previous ones.", "labels": [], "entities": []}, {"text": "Below area list of previous benchmark datasets for math word problem solving.", "labels": [], "entities": [{"text": "math word problem solving", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7614566087722778}]}, {"text": "The following equation system corresponds to the above template, shows some statistical information of our dataset and previous ones.", "labels": [], "entities": []}, {"text": "It can be seen that our dataset has a much larger scale (about 10 times the size of the Dolphin1878 collection and more than 17 times larger than the others) and higher diversity (in terms of both problem types and the number of templates contained).", "labels": [], "entities": [{"text": "Dolphin1878 collection", "start_pos": 88, "end_pos": 110, "type": "DATASET", "confidence": 0.9749537408351898}]}, {"text": "We split our dataset into a development set and an evaluation set.", "labels": [], "entities": []}, {"text": "The development set is used for algorithm design and debugging, while the evaluation set is for training and testing.", "labels": [], "entities": [{"text": "algorithm design", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7881242334842682}]}, {"text": "Any problem in the evaluation set should be invisible to the people who design an automatic math problem solving system.", "labels": [], "entities": [{"text": "math problem solving", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.6824103991190592}]}, {"text": "Statistics on our dataset are shown in, where dev and eval represent the development set and the evaluation set respectively.", "labels": [], "entities": []}, {"text": "Most problems are assigned with both equation systems and gold answers.", "labels": [], "entities": []}, {"text": "Some of them are annotated with answers only, either because annotators feel it is hard to do so, or because our equation extraction algorithm returns empty results.", "labels": [], "entities": [{"text": "equation extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7280910164117813}]}, {"text": "As most previous systems only handle linear equation systems, we summarize, in, the distribution of linear problems in the evaluation set by template size.", "labels": [], "entities": []}, {"text": "In the table, the size of a template is defined as the number of problems corresponding to this template.", "labels": [], "entities": []}, {"text": "Between the two numbers in each cell, the first one is the number of problems,   We report the performance of several state-of-theart systems on our new dataset.", "labels": [], "entities": []}, {"text": "KAZB: A template-based statistical learning method introduced in . It maps a problem to one equation template defined in the training set by reasoning across problem sentences.", "labels": [], "entities": [{"text": "KAZB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7763993144035339}]}, {"text": "KAZB reports an accuracy of 68.7% on the Alg514 dataset.", "labels": [], "entities": [{"text": "KAZB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9210284352302551}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9996116757392883}, {"text": "Alg514 dataset", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.9826700389385223}]}, {"text": "SIM is a simple similarity-based method implemented by us.", "labels": [], "entities": []}, {"text": "To solve a problem, it calculates the lexical similarity between the problem and each problem in the training set.", "labels": [], "entities": []}, {"text": "Then the equation system of the most similar problem is ap- plied to the new problem.", "labels": [], "entities": []}, {"text": "Ina little more details, a test problem PT is solved in two steps: template selection, and template slot filling.", "labels": [], "entities": [{"text": "template selection", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.65727399289608}, {"text": "template slot filling", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.6055731773376465}]}, {"text": "In the first step, each problem is modeled as a vector of word TF-IDF scores.", "labels": [], "entities": []}, {"text": "The similarity between two problems is calculated by the weighted Jaccard similarity between their corresponding vectors.", "labels": [], "entities": []}, {"text": "We choose, from the training data, problem P 1 that has the maximal similarity with PT and use the equation template T of P 1 as the template of problem PT . In the second step, the numbers appearing in problem PT are mapped to the number slots of template T (which has been identified in the first step).", "labels": [], "entities": []}, {"text": "The mapping is implemented by selecting one problem P 2 from all the training problems corresponding to template T so that it has the minimum word-level edit-distance to PT . Then the number mapping of P 2 is borrowed as the number mapping of PT . For example, for the following test problem, An overnight mail service charges $3.60 for the first six ounces and $0.45 for each additional ounce or fraction of an ounce.", "labels": [], "entities": [{"text": "PT", "start_pos": 170, "end_pos": 172, "type": "METRIC", "confidence": 0.6482815742492676}]}, {"text": "Find the number of ounces in a package that cost $7.65 to deliver.", "labels": [], "entities": []}, {"text": "Assuming that a problem P 1 has maximum Jaccard similarity with the above problem and its corresponding equation template is as follows, this template will be identified in the first step, Assume that P 2 has the minimum edit-distance to PT among all the training problems corresponding to template T . Suppose the numbers in P 2 are (by their order in the problem text), 3.5, 5, 0.5, 6.5 Also suppose P 2 is annotated with the following equation system, 3.5 + 0.5 * (x \u2212 5) = 6.5 Then we will choose P 2 and borrow its number mapping.", "labels": [], "entities": [{"text": "PT", "start_pos": 238, "end_pos": 240, "type": "METRIC", "confidence": 0.9769476652145386}]}, {"text": "So the mapping from numbers in the above test problem to template slots will be, 3.60/n 1 ; 6/n 3 ; 0.45/n 2 ; 7.65/n 4 In implementing SIM, we do not use any POS tagging or syntactic parsing features for similarity calculation.", "labels": [], "entities": [{"text": "POS tagging or syntactic parsing", "start_pos": 159, "end_pos": 191, "type": "TASK", "confidence": 0.6896021664142609}]}, {"text": "This method gets an accuracy of 71.2% on Alg514 and 49.0% on SingleEQ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9997052550315857}, {"text": "Alg514", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.8853585124015808}, {"text": "SingleEQ", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9450064897537231}]}, {"text": "Systems not included for evaluation: Although the system of achieves very high performance on number word problems, we do not include it in our evaluation because it is unknown how to extend it to other problem types.", "labels": [], "entities": []}, {"text": "The system of is not included in our evaluation because it only handles homogeneous addition/subtraction problems.", "labels": [], "entities": []}, {"text": "The systems of and are also not included because so far they only supports problems with one single linear equation.", "labels": [], "entities": []}, {"text": "shows the accuracy of various systems on different subsets of our dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994304776191711}]}, {"text": "In the table, Manual.Linear contains all the manually annotated problems with linear equation systems.", "labels": [], "entities": []}, {"text": "It contains 2,675 problems and 876 templates (as shown in).", "labels": [], "entities": []}, {"text": "Auto.LinearT6 (containing 4,826 problems) is the set of all the automatically annotated problems with a template size larger than or equal to 6.", "labels": [], "entities": []}, {"text": "Similarly, LinearT2 means the subset of problems with template size \u2265 2.", "labels": [], "entities": []}, {"text": "For each system on each subset, experiments are conducted using 5-fold cross-validation with 80% problems randomly selected as training data and the remaining 20% for test.", "labels": [], "entities": []}, {"text": "In the table, \"-\" means that the system does not complete running on the dataset in three days.", "labels": [], "entities": []}, {"text": "Since KAZB and ZDC only handle linear equation systems, they are not applicable to the datasets n/a n/a 16.7% Alg514 68.7% 79.7% 71.2%: Overall evaluation results containing nonlinear problems.", "labels": [], "entities": [{"text": "KAZB", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.88409423828125}, {"text": "Alg514", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.7832744121551514}]}, {"text": "An \"n/a\" is filled in the corresponding cell in this case.", "labels": [], "entities": []}, {"text": "The results show that all three systems (KAZB, ZDC, and SIM) have extremely low performance on our new datasets.", "labels": [], "entities": [{"text": "KAZB", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.742378830909729}]}, {"text": "Surprisingly, no system achieves an accuracy rate of over 25%.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9811799824237823}]}, {"text": "Such results indicate that automatic math word problem solving is still a very challenging task.", "labels": [], "entities": [{"text": "automatic math word problem solving", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.682686311006546}]}, {"text": "Another surprising observation is that KAZB and ZDC do not perform better than SIM, a simple similarity-based method which runs much faster than the two statistical learning systems.", "labels": [], "entities": []}, {"text": "By comparing the results obtained from the manual version of the datasets with their corresponding auto version (for example, Manuall.Linear vs. Auto.Linear), we can see larger accuracy scores on the auto versions . This demonstrates the usefulness of the automatically annotated data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9984661340713501}]}, {"text": "Considering the huge cost of manually assigning equation systems and gold answers, automatic annotation has good potential in constructing larger datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of different datasets", "labels": [], "entities": []}, {"text": " Table 3: Annotation statistics for our dataset", "labels": [], "entities": []}, {"text": " Table 4: Problem distribution by template size (for  linear problems only)", "labels": [], "entities": []}, {"text": " Table 5: Overall evaluation results", "labels": [], "entities": []}, {"text": " Table 6: The case of fewer number of templates", "labels": [], "entities": []}, {"text": " Table 7: System performance with different training data size (setting: fixed-test-set)", "labels": [], "entities": []}, {"text": " Table 8: System performance with different training data size (setting: increasing-test-set)", "labels": [], "entities": []}]}