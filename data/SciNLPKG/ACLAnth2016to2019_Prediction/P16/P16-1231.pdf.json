{"title": [], "abstractContent": [{"text": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.7124497294425964}, {"text": "dependency parsing", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7972716987133026}, {"text": "sentence compression", "start_pos": 151, "end_pos": 171, "type": "TASK", "confidence": 0.7421830892562866}]}, {"text": "Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models.", "labels": [], "entities": []}, {"text": "We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network approaches have taken the field of natural language processing (NLP) by storm.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.8184588750203451}]}, {"text": "In particular, variants of long short-term memory (LSTM) networks (Hochreiter and) have produced impressive results on some of the classic NLP tasks such as part-ofspeech tagging ( , syntactic parsing () and semantic role labeling (.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.7742979228496552}, {"text": "syntactic parsing", "start_pos": 183, "end_pos": 200, "type": "TASK", "confidence": 0.8340761661529541}, {"text": "semantic role labeling", "start_pos": 208, "end_pos": 230, "type": "TASK", "confidence": 0.7097365856170654}]}, {"text": "One might speculate that it is the recurrent nature of these models that enables these results.", "labels": [], "entities": []}, {"text": "In this work we demonstrate that simple feedforward networks without any recurrence can achieve comparable or better accuracies than LSTMs, as long as they are globally normalized.", "labels": [], "entities": []}, {"text": "Our model, described in detail in Section 2, uses a transition system) and feature embeddings as introduced by.", "labels": [], "entities": []}, {"text": "We do not use any recurrence, but perform beam search for maintaining multiple hy- * On leave from Columbia University.", "labels": [], "entities": [{"text": "beam search", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8241293132305145}]}, {"text": "potheses and introduce global normalization with a conditional random field (CRF) objective) to overcome the label bias problem that locally normalized models suffer from.", "labels": [], "entities": []}, {"text": "Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (.", "labels": [], "entities": []}, {"text": "We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss.", "labels": [], "entities": []}, {"text": "In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models.", "labels": [], "entities": []}, {"text": "Lookahead features can partially mitigate this discrepancy, but cannot fully compensate for it-a point to which we return later.", "labels": [], "entities": []}, {"text": "To empirically demonstrate the effectiveness of global normalization, we evaluate our model on part-of-speech tagging, syntactic dependency parsing and sentence compression (Section 4).", "labels": [], "entities": [{"text": "global normalization", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7897664606571198}, {"text": "part-of-speech tagging", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.7173615843057632}, {"text": "syntactic dependency parsing", "start_pos": 119, "end_pos": 147, "type": "TASK", "confidence": 0.6474530398845673}, {"text": "sentence compression", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.735923707485199}]}, {"text": "Our model achieves state-of-the-art accuracy on all of these tasks, matching or outperforming LSTMs while being significantly faster.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9990880489349365}]}, {"text": "In particular for dependency parsing on the Wall Street Journal we achieve the best-ever published unlabeled attachment score of 94.61%.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.9313822388648987}, {"text": "Wall Street Journal", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.9569936593373617}, {"text": "attachment score", "start_pos": 109, "end_pos": 125, "type": "METRIC", "confidence": 0.9101709425449371}]}, {"text": "As discussed in more detail in Section 5, we also outperform previous structured training approaches used for neural network transition-based parsing.", "labels": [], "entities": [{"text": "neural network transition-based parsing", "start_pos": 110, "end_pos": 149, "type": "TASK", "confidence": 0.6177155822515488}]}, {"text": "Our ablation experiments show that we outperform and because we do global backpropagation training of all model parameters, while they fix the neural network parameters when training the global part of their model.", "labels": [], "entities": []}, {"text": "We also outperform despite using a smaller beam.", "labels": [], "entities": []}, {"text": "To shed additional light on the label bias problem in practice, we provide a sentence compression example where the local model completely fails.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.751090407371521}]}, {"text": "We then demonstrate that a globally normalized parsing model without any lookahead features is almost as accurate as our best model, while a locally normalized model loses more than 10% absolute inaccuracy because it cannot effectively incorporate evidence as it becomes available.", "labels": [], "entities": [{"text": "globally normalized parsing", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.4844052791595459}]}, {"text": "Finally, we provide an open-source implementation of our method, called SyntaxNet, 1 which we have integrated into the popular TensorFlow 2 framework.", "labels": [], "entities": []}, {"text": "We also provide a pre-trained, state-of-the art English dependency parser called \"Parsey McParseface,\" which we tuned fora balance of speed, simplicity, and accuracy.", "labels": [], "entities": [{"text": "Parsey McParseface", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.78192538022995}, {"text": "speed", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.9929371476173401}, {"text": "simplicity", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.9936331510543823}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9979669451713562}]}], "datasetContent": [{"text": "To demonstrate the flexibility and modeling power of our approach, we provide experimental results on a diverse set of structured prediction tasks.", "labels": [], "entities": []}, {"text": "We apply our approach to POS tagging, syntactic dependency parsing, and sentence compression.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.889503687620163}, {"text": "syntactic dependency parsing", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6752487818400065}, {"text": "sentence compression", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.7627592384815216}]}, {"text": "While directly optimizing the global model defined by Eq.", "labels": [], "entities": []}, {"text": "(5) works well, we found that training the model in two steps achieves the same precision much faster: we first pretrain the network using the local objective given in Eq.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9992818236351013}]}, {"text": "(4), and then perform additional training steps using the global objective given in Eq.", "labels": [], "entities": []}, {"text": "We pretrain all layers except the softmax layer in this way.", "labels": [], "entities": []}, {"text": "We purposefully abstain from complicated hand engineering of input features, which might improve performance further.", "labels": [], "entities": []}, {"text": "We use the training recipe from Weiss et al.", "labels": [], "entities": []}, {"text": "(2015) for each training stage of our model.", "labels": [], "entities": []}, {"text": "Specifically, we use averaged stochastic gradient descent with momentum, and we tune the learning rate, learning rate schedule, momentum, and early stopping time using a separate held-out corpus for each task.", "labels": [], "entities": [{"text": "early stopping time", "start_pos": 142, "end_pos": 161, "type": "METRIC", "confidence": 0.9470715522766113}]}, {"text": "We tune again with a different set of hyperparameters for training with the global objective.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, \"Parsey McParseface.\"", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.7047453224658966}, {"text": "English WSJ and Treebank Union", "start_pos": 48, "end_pos": 78, "type": "DATASET", "confidence": 0.89790860414505}, {"text": "CoNLL'09", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.9504848718643188}, {"text": "Parsey McParseface", "start_pos": 169, "end_pos": 187, "type": "DATASET", "confidence": 0.8775207102298737}]}, {"text": " Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.5539094309012095}, {"text": "WSJ corpus", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.9743342101573944}, {"text": "UAS", "start_pos": 196, "end_pos": 199, "type": "METRIC", "confidence": 0.9892293810844421}, {"text": "LAS", "start_pos": 211, "end_pos": 214, "type": "METRIC", "confidence": 0.9986783862113953}]}, {"text": " Table 3: Final CoNLL '09 dependency parsing test set results.", "labels": [], "entities": [{"text": "CoNLL '09 dependency parsing", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.8040852844715118}]}, {"text": " Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.9345798492431641}, {"text": "News data", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.8924187421798706}, {"text": "News training corpus", "start_pos": 148, "end_pos": 168, "type": "DATASET", "confidence": 0.8741727272669474}]}, {"text": " Table 5: WSJ dev set scores for successively deeper levels  of backpropagation. The full parameter set corresponds to  backpropagation all the way to the embeddings. Wi: hidden  layer i weights.", "labels": [], "entities": []}, {"text": " Table 6: Example sentence compressions where the label bias of the locally normalized model leads to a breakdown during  beam search. The probability of each compression under the local (pL) and global (pG) models shows that only the global  model can properly represent zero probability for the empty compression.", "labels": [], "entities": [{"text": "sentence compressions", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7246200144290924}]}]}