{"title": [{"text": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Policy Optimisation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7198992669582367}]}], "abstractContent": [{"text": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning.", "labels": [], "entities": []}, {"text": "In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect.", "labels": [], "entities": []}, {"text": "This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line.", "labels": [], "entities": []}, {"text": "In practice neither of these apply for most real world applications.", "labels": [], "entities": []}, {"text": "Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model.", "labels": [], "entities": []}, {"text": "This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.", "labels": [], "entities": [{"text": "dialogue policy learning", "start_pos": 155, "end_pos": 179, "type": "TASK", "confidence": 0.7234712441762289}]}], "introductionContent": [{"text": "Spoken Dialogue Systems (SDS) allow humancomputer interaction using natural speech.", "labels": [], "entities": []}, {"text": "They can be broadly divided into two categories: chatoriented systems which aim to converse with users and provide reasonable contextually relevant responses (, and task-oriented systems designed to assist users to achieve specific goals (e.g. find hotels, movies or bus schedules) ().", "labels": [], "entities": []}, {"text": "The latter are typically designed according to a structured ontology (or a database schema), which defines the domain: An example of a task-oriented dialogue with a pre-defined task and the evaluation results. that the system can talk about.", "labels": [], "entities": []}, {"text": "Teaching a system how to respond appropriately in a task-oriented SDS is non-trivial.", "labels": [], "entities": []}, {"text": "This dialogue management task is often formulated as a manually defined dialogue flow that directly determines the quality of interaction.", "labels": [], "entities": [{"text": "dialogue management task", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.8451316157976786}]}, {"text": "More recently, dialogue management has been formulated as a reinforcement learning (RL) problem which can be automatically optimised (.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.9168063998222351}, {"text": "reinforcement learning (RL)", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.697248113155365}]}, {"text": "In this framework, the system learns by atrial and error process governed by a potentially delayed learning objective defined by a reward function.", "labels": [], "entities": []}, {"text": "A typical approach to defining the reward function in a task-oriented dialogue system is to apply a small per-turn penalty to encourage short dialogues and to give a large positive reward at the end of each successful interaction. is an example of a dialogue task which is typically set for users who are being paid to converse with the system.", "labels": [], "entities": []}, {"text": "When users are primed with a specific task to complete, dialogue success can be determined from subjective user ratings (Subj), or an objective measure (Obj) based on whether or not the pre-specified task was completed (.", "labels": [], "entities": [{"text": "objective measure (Obj)", "start_pos": 134, "end_pos": 157, "type": "METRIC", "confidence": 0.76616952419281}]}, {"text": "However, prior knowledge of the user's goal is not normally available in real situations, making the objective reward estimation approach impractical.", "labels": [], "entities": []}, {"text": "Furthermore, objective ratings are inflexible and often fail as can be seen from, if the user does not strictly follow the task.", "labels": [], "entities": []}, {"text": "This results in a mismatch between the Obj and Subj ratings.", "labels": [], "entities": [{"text": "Obj", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.5658903121948242}]}, {"text": "However, relying on subjective ratings alone is also problematic since crowd-sourced subjects frequently give inaccurate responses and real users are often unwilling to extend the interaction in order to give feedback, resulting in unstable learning ().", "labels": [], "entities": []}, {"text": "In order to filter out incorrect user feedback, Ga\u0161i\u00b4 used only dialogues for which Obj = Subj.", "labels": [], "entities": [{"text": "Obj", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9384486675262451}]}, {"text": "Nonetheless, this is inefficient and not feasible anyway inmost real-world tasks where the user's goal is generally unknown and difficult to infer.", "labels": [], "entities": []}, {"text": "In light of the above, proposed learning a neural network-based Obj estimator from off-line simulated dialogue data.", "labels": [], "entities": []}, {"text": "This removes the need for the Obj check during online policy learning and the resulting policy is as effective as one trained with dialogues using the Obj = Subj check.", "labels": [], "entities": []}, {"text": "However, a user simulator will only provide a rough approximation of real user statistics and developing a user simulator is a costly process ().", "labels": [], "entities": []}, {"text": "To deal with the above issues, this paper describes an on-line active learning method in which users are asked to provide feedback on whether the dialogue was successful or not.", "labels": [], "entities": []}, {"text": "However, active learning is used to limit requests for feedback to only those cases where the feedback would be useful, and also a noise model is introduced to compensate for cases where the user feedback is inaccurate.", "labels": [], "entities": []}, {"text": "A Gaussian process classification (GPC) model is utilised to robustly model the uncertainty presented by the noisy user feedback.", "labels": [], "entities": [{"text": "Gaussian process classification (GPC)", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.7802185912926992}]}, {"text": "Since GPC operates on a fixed-length observation space and dialogues are of variable-length, a recurrent neural network (RNN)-based embedding function is used to provide fixed-length dialogue representations.", "labels": [], "entities": []}, {"text": "In essence, the proposed method learns a dialogue policy and a reward estimator on-line from scratch, and is directly applicable to real-world applications.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "The next section gives an overview of related work.", "labels": [], "entities": []}, {"text": "The proposed framework is then described in \u00a73.", "labels": [], "entities": []}, {"text": "This consists of the policy learning algorithm, the creation of the dialogue embedding function and the active reward model trained from real user ratings.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9112800657749176}]}, {"text": "In \u00a74, the proposed approach is evaluated in the context of an application providing restaurant information in Cambridge, UK.", "labels": [], "entities": []}, {"text": "We first give an in-depth analysis of the dialogue embedding space.", "labels": [], "entities": []}, {"text": "The results of the active reward model when it is trained together with a dialogue policy on-line with real users are then presented.", "labels": [], "entities": []}, {"text": "Finally, our conclusions are presented in \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "The target application is a live telephone-based spoken dialogue system providing restaurant information for the Cambridge (UK) area.", "labels": [], "entities": [{"text": "Cambridge (UK) area", "start_pos": 113, "end_pos": 132, "type": "DATASET", "confidence": 0.9308441519737244}]}, {"text": "The domain consists of approximately 150 venues each having 6 slots (attributes) of which 3 can be used by the system to constrain the search (food-type, area and price-range) and the remaining 3 are informable properties (phone-number, address and postcode) available once a required database entity has been found.", "labels": [], "entities": []}, {"text": "The shared core components of the SDS common to all experiments comprise a HMM-based recogniser, a confusion network (CNet) semantic input decoder, the BUDS belief state tracker) that factorises the dialogue state using a dynamic Bayesian network, and a template based natural language generator to map system semantic actions into natural language responses to the user.", "labels": [], "entities": [{"text": "BUDS belief state tracker", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.5615886002779007}]}, {"text": "All policies were trained using the GP-SARSA algorithm and the summary action space of the RL policy contains 20 actions.", "labels": [], "entities": []}, {"text": "The reward given to each dialogue was set to 20 \u00d7 1 success \u2212 N , where N is the dialogue turn number and 1 is the indicator function for dialogue success, which is determined by different methods as described in the following section.", "labels": [], "entities": []}, {"text": "These rewards constitute the reinforcement signal used for policy learning.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.8667394518852234}]}, {"text": "In order to compare performance, the averaged results obtained between 400-500 training dialogues are shown in the first section of along with one standard error.", "labels": [], "entities": []}, {"text": "For the 400-500 interval, the Subj, off-line RNN and on-line GP systems achieved comparable results without statistical differences.", "labels": [], "entities": [{"text": "Subj", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9438444375991821}, {"text": "RNN", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8864723443984985}]}, {"text": "The results of continuing training on the Subj and on-line GP systems from 500 to 850 training dialogues are also shown.", "labels": [], "entities": [{"text": "Subj", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9124525785446167}]}, {"text": "As can be seen, the on-line GP system was significantly better presumably because it is more robust to erroneous user feedback compared to the Subj system.", "labels": [], "entities": []}, {"text": "The above results verify the effectiveness of the proposed reward model for policy learning.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.8055968880653381}]}, {"text": "Here we investigate further the accuracy of the model in predicting the subjective success rate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9992749094963074}]}, {"text": "An evaluation of the on-line GP reward model between 1 and 850 training dialogues is presented in.", "labels": [], "entities": [{"text": "GP reward", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.6120049357414246}]}, {"text": "Since three reward models were learnt each with 850 dialogues, there were a total of 2550 training dialogues.", "labels": [], "entities": []}, {"text": "Of these, the models queried the user for feedback a total of 454 times, leaving 2096 dialogues for which learning relied on the reward model's prediction.", "labels": [], "entities": []}, {"text": "The results shown in the table are thus the average over 2096 dialogues.: Subjective evaluation of the Obj=Subj, off-line RNN, Subj and on-line GP system during different stages of on-line policy learning.", "labels": [], "entities": []}, {"text": "Subjective: user binary rating on dialogue success.", "labels": [], "entities": []}, {"text": "Statistical significance was calculated using a twotailed Students t-test with p-value of 0.05.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.725315123796463}]}], "tableCaptions": [{"text": " Table 2: Statistical evaluation of the prediction of  the on-line GP systems with respect to Subj rating.", "labels": [], "entities": [{"text": "Subj rating", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.5641111731529236}]}]}