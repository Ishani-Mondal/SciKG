{"title": [{"text": "Reference Bias in Monolingual Machine Translation Evaluation", "labels": [], "entities": [{"text": "Monolingual Machine Translation", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.7086433470249176}]}], "abstractContent": [{"text": "In the translation industry, human translations are assessed by comparison with the source texts.", "labels": [], "entities": []}, {"text": "In the Machine Translation (MT) research community, however, it is a common practice to perform quality assessment using a reference translation instead of the source text.", "labels": [], "entities": [{"text": "Machine Translation (MT) research", "start_pos": 7, "end_pos": 40, "type": "TASK", "confidence": 0.8525913655757904}]}, {"text": "In this paper we show that this practice has a serious issue-annotators are strongly biased by the reference translation provided, and this can have a negative impact on the assessment of MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 188, "end_pos": 190, "type": "TASK", "confidence": 0.9951639175415039}]}], "introductionContent": [{"text": "Equivalence to the source text is the defining characteristic of translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9693570137023926}]}, {"text": "One of the fundamental aspects of translation quality is, therefore, its semantic adequacy, which reflects to what extent the meaning of the original text is preserved in the translation.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.9129429161548615}]}, {"text": "In the field of Machine Translation (MT), on the other hand, it has recently become common practice to perform quality assessment using a human reference translation instead of the source text.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.8739658474922181}]}, {"text": "Reference-based evaluation is an attractive practical solution since it does not require bilingual speakers.", "labels": [], "entities": [{"text": "Reference-based evaluation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7640175521373749}]}, {"text": "However, we believe this approach has a strong conceptual flaw: the assumption that the task of translation has a single correct solution.", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.9457681775093079}]}, {"text": "In reality, except for very short sentences or very specific technical domains, the same source sentence maybe correctly translated in many different ways.", "labels": [], "entities": []}, {"text": "Depending on abroad textual and real-world context, the translation can differ from the source text at any linguistic level -lexical, syntactic, semantic or even discourse -and still be considered perfectly correct.", "labels": [], "entities": []}, {"text": "Therefore, using a single translation as a proxy for the original text maybe unreliable.", "labels": [], "entities": []}, {"text": "In the monolingual, reference-based evaluation scenario, human judges are expected to recognize acceptable variations between translation options and assign a high score to a good MT, even if it happens to be different from a particular human reference provided.", "labels": [], "entities": [{"text": "MT", "start_pos": 180, "end_pos": 182, "type": "TASK", "confidence": 0.891235888004303}]}, {"text": "In this paper we argue that, contrary to this expectation, annotators are strongly biased by the reference.", "labels": [], "entities": []}, {"text": "They inadvertently favor machine translations (MTs) that make similar choices to the ones present in the reference translation.", "labels": [], "entities": [{"text": "machine translations (MTs)", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7477989435195923}]}, {"text": "To test this hypothesis, we perform an experiment where the same set of MT outputs is manually assessed using different reference translations and analyze the discrepancies between the resulting quality scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9645766615867615}]}, {"text": "The results confirm that annotators are indeed heavily influenced by the particular human translation that was used for evaluation.", "labels": [], "entities": []}, {"text": "We discuss the implications of this finding on the reliability of current practices in manual quality assessment.", "labels": [], "entities": [{"text": "manual quality assessment", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.6255688369274139}]}, {"text": "Our general recommendation is that, in order to avoid reference bias, the assessment should be performed by comparing the MT output to the original text, rather than to a reference.", "labels": [], "entities": [{"text": "MT", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.9303097724914551}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we present related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our experimental settings.", "labels": [], "entities": []}, {"text": "In Section 4 we focus on the effect of reference bias on MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9579954743385315}]}, {"text": "In Section 5 we examine the impact of the fatigue factor on the results of our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "MT data with multiple references is rare.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9712536334991455}]}, {"text": "We used MTC-P4 Chinese-English dataset, produced by Linguistic Data Consortium (LDC2006T04).", "labels": [], "entities": [{"text": "MTC-P4 Chinese-English dataset", "start_pos": 8, "end_pos": 38, "type": "DATASET", "confidence": 0.9027337034543356}, {"text": "Linguistic Data Consortium (LDC2006T04)", "start_pos": 52, "end_pos": 91, "type": "DATASET", "confidence": 0.8563459267218908}]}, {"text": "The dataset contains 919 source sentences from news domain, 4 reference translations and MT outputs generated by 10 translation systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9391071200370789}]}, {"text": "Human translations were produced by four teams of professional translators and included editor's proofreading.", "labels": [], "entities": []}, {"text": "All teams used the same translation guidelines, which emphasize faithfulness to the source sentence as one of the main requirements.", "labels": [], "entities": []}, {"text": "We note that even in such a scenario, human translations differ from each other.", "labels": [], "entities": []}, {"text": "We measured the average similarity between the four references in the dataset using the Meteor evaluation metric.", "labels": [], "entities": [{"text": "Meteor evaluation metric", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.8751378655433655}]}, {"text": "Meteor scores range between 0 and 1 and reflect the proportion of similar words occurring in similar order.", "labels": [], "entities": []}, {"text": "This metric is normally used to compare the MT output with a human reference, but it can also be applied to measure similarity between any two translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9564893841743469}]}, {"text": "We computed Meteor for all possible combinations between the four available references and took the average score.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.9033649563789368}]}, {"text": "Even though Meteor covers certain amount of acceptable linguistic variation by allowing for synonym and paraphrase matching, the resulting score is only 0.33, which shows that, not surprisingly, human translations vary substantially.", "labels": [], "entities": [{"text": "synonym and paraphrase matching", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.5988649278879166}]}, {"text": "To make the annotation process feasible given the resources available, we selected a subset of 100 source sentences for the experiment.", "labels": [], "entities": []}, {"text": "To ensure variable levels of similarity between the MT and each of the references, we computed sentencelevel Meteor scores for the MT outputs using each of the references and selected the sentences with the highest standard deviation between the scores.", "labels": [], "entities": [{"text": "similarity", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9615865349769592}, {"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.5075082778930664}, {"text": "MT outputs", "start_pos": 131, "end_pos": 141, "type": "TASK", "confidence": 0.8574479818344116}]}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement for different- references (Diff. ref.), same-reference (Same ref.)  and source-based evaluation (Source)", "labels": [], "entities": []}, {"text": " Table 2: Average human scores for the groups of  annotators using different references and BLEU  scores calculated with the corresponding refer- ences. Human scores range from 1 to 5, while  BLEU scores range from 0 to 1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9989270567893982}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9984114170074463}]}]}