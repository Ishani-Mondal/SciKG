{"title": [{"text": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "labels": [], "entities": [{"text": "Sequence-to-Sequence Learning", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.8911544382572174}]}], "abstractContent": [{"text": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence.", "labels": [], "entities": []}, {"text": "A similar phenomenon is observable inhuman language communication.", "labels": [], "entities": [{"text": "observable inhuman language communication", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.6369237974286079}]}, {"text": "For example, humans tend to repeat entity names or even long phrases in conversation.", "labels": [], "entities": []}, {"text": "The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation.", "labels": [], "entities": [{"text": "copying", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9769385457038879}]}, {"text": "In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose anew model called COPYNET with encoder-decoder structure.", "labels": [], "entities": [{"text": "COPYNET", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.7845988869667053}]}, {"text": "COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence.", "labels": [], "entities": [{"text": "COPYNET", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.914495050907135}, {"text": "word generation", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7557790279388428}]}, {"text": "Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of COPYNET.", "labels": [], "entities": []}, {"text": "For example, COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks.", "labels": [], "entities": [{"text": "COPYNET", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.7774197459220886}, {"text": "text summarization tasks", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.7985137303670248}]}], "introductionContent": [{"text": "Recently, neural network-based sequence-tosequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation ( ), Syntactic Parsing (), Text Summarization ( and Dialogue Systems ( ).", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.7972068190574646}, {"text": "Syntactic Parsing", "start_pos": 208, "end_pos": 225, "type": "TASK", "confidence": 0.8884173035621643}, {"text": "Text Summarization", "start_pos": 230, "end_pos": 248, "type": "TASK", "confidence": 0.8643469512462616}]}, {"text": "Seq2Seq is essentially an encoder-decoder model, in which the encoder first transform the input sequence to a certain representation which can then transform the representation into the output sequence.", "labels": [], "entities": []}, {"text": "Adding the attention mechanism) to Seq2Seq, first proposed for automatic alignment in machine translation, has led to significant improvement on the performance of various tasks.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.6968282163143158}]}, {"text": "Different from the canonical encoderdecoder architecture, the attention-based Seq2Seq model revisits the input sequence in its raw form (array of word representations) and dynamically fetches the relevant piece of information based mostly on the feedback from the generation of the output sequence.", "labels": [], "entities": []}, {"text": "In this paper, we explore another mechanism important to the human language communication, called the \"copying mechanism\".", "labels": [], "entities": [{"text": "human language communication", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.6596009234587351}]}, {"text": "Basically, it refers to the mechanism that locates a certain segment of the input sentence and puts the segment into the output sequence.", "labels": [], "entities": []}, {"text": "For example, in the following two dialogue turns we observe different patterns in which some subsequences (colored blue) in the response (R) are copied from the input utterance (I): I: Hello Jack, my name is Chandralekha.", "labels": [], "entities": []}, {"text": "R: Nice to meet you, Chandralekha.", "labels": [], "entities": []}, {"text": "I: This new guy doesn't perform exactly as we expected.", "labels": [], "entities": []}, {"text": "R: What do you mean by \"doesn't perform exactly as we expected\"?", "labels": [], "entities": []}, {"text": "Both the canonical encoder-decoder and its variants with attention mechanism rely heavily on the representation of \"meaning\", which might not be sufficiently inaccurate in cases in which the system needs to refer to sub-sequences of input like entity names or dates.", "labels": [], "entities": []}, {"text": "In contrast, the copying mechanism is closer to the rote memorization in language processing of human being, deserving a different modeling strategy in neural network-based models.", "labels": [], "entities": [{"text": "copying", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9746878147125244}]}, {"text": "We argue that it will benefit many Seq2Seq tasks to have an elegant unified model that can accommodate both understanding and rote memorization.", "labels": [], "entities": []}, {"text": "Towards this goal, we propose COPYNET, which is not only capable of the regular generation of words but also the operation of copying appropriate segments of the input sequence.", "labels": [], "entities": []}, {"text": "Despite the seemingly \"hard\" operation of copying, COPYNET can be trained in an end-toend fashion.", "labels": [], "entities": [{"text": "copying", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9851757287979126}, {"text": "COPYNET", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.6310614943504333}]}, {"text": "Our empirical study on both synthetic datasets and real world datasets demonstrates the efficacy of COPYNET.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report our empirical study of COPYNET on the following three tasks with different characteristics 1.", "labels": [], "entities": [{"text": "COPYNET", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.5955937504768372}]}, {"text": "A synthetic dataset on with simple patterns; 2.", "labels": [], "entities": []}, {"text": "A real-world task on text summarization; 3.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7963749468326569}]}, {"text": "A dataset for simple single-turn dialogues.", "labels": [], "entities": []}, {"text": "Dataset: We first randomly generate transformation rules with 5\u223c20 symbols and variables x & y, e.g. with {a b c d e f g h m} being regular symbols from a vocabulary of size 1,000.", "labels": [], "entities": []}, {"text": "As shown in the table below, each rule can further produce a number of instances by replacing the variables with randomly generated subsequences (1\u223c15 symbols) from the same vocabulary.", "labels": [], "entities": []}, {"text": "We create five types of rules, including \"x \u2192 \u2205\".", "labels": [], "entities": []}, {"text": "The task is to learn to do the Seq2Seq transformation from the training instances.", "labels": [], "entities": [{"text": "Seq2Seq transformation", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.6616874933242798}]}, {"text": "This dataset is designed to study the behavior of COPYNET on handling simple and rigid patterns.", "labels": [], "entities": []}, {"text": "Since the strings to repeat are random, they can also be viewed as some extreme cases of rote memorization.", "labels": [], "entities": []}, {"text": "Rule-type Examples (e.g. x = i h k, y = j c) Experimental Setting: We select 200 artificial rules from the dataset, and for each rule 200 instances are generated, which will be split into training (50%) and testing (50%).", "labels": [], "entities": []}, {"text": "We compare the accuracy of COPYNET and the RNN EncoderDecoder with (i.e. RNNsearch) or without attention (denoted as Enc-Dec).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995251893997192}, {"text": "COPYNET", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.885720431804657}, {"text": "RNN EncoderDecoder", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.9003876447677612}]}, {"text": "For a fair comparison, we use bi-directional GRU for encoder and another GRU for decoder for all Seq2Seq models, with hidden layer size = 300 and word embedding dimension = 150.", "labels": [], "entities": [{"text": "GRU", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9362236857414246}]}, {"text": "We use bin size = 10 in beam search for testing.", "labels": [], "entities": []}, {"text": "The prediction is considered  correct only when the generated sequence is exactly the same as the given one.", "labels": [], "entities": []}, {"text": "It is clear from that COPYNET significantly outperforms the other two on all rule-types except \"x \u2192 \u2205\", indicating that COPYNET can effectively learn the patterns with variables and accurately replicate rather long subsequence of symbols at the proper places.This is hard to Enc-Dec due to the difficulty of representing along sequence with very high fidelity.", "labels": [], "entities": []}, {"text": "This difficulty can be alleviated with the attention mechanism.", "labels": [], "entities": []}, {"text": "However attention alone seems inadequate for handling the case where strict replication is needed.", "labels": [], "entities": []}, {"text": "A closer look (see for example) reveals that the decoder is dominated by copy-mode when moving into the subsequence to replicate, and switch to generate-mode after leaving this area, showing COPYNET can achieve a rather precise coordination of the two modes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The test accuracy (%) on synthetic data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9633350372314453}]}, {"text": " Table 2: Some statistics of the LCSTS dataset.", "labels": [], "entities": [{"text": "LCSTS dataset", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9524047374725342}]}, {"text": " Table 3: Testing performance of LCSTS, where  \"RNN\" is canonical Enc-Dec, and \"RNN context\"  its attentive variant.", "labels": [], "entities": []}, {"text": " Table 4: The decoding accuracy on the two testing  sets. Decoding is admitted success only when the  answer is found exactly in the Top-K outputs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9975162744522095}]}]}