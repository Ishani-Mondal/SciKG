{"title": [{"text": "Learning Concept Taxonomies from Multi-modal Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We study the problem of automatically building hypernym taxonomies from tex-tual and visual data.", "labels": [], "entities": []}, {"text": "Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.8793684840202332}]}, {"text": "Instead, we propose a probabilistic model for taxonomy induction by jointly leverag-ing text and images.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9255259335041046}]}, {"text": "To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words.", "labels": [], "entities": []}, {"text": "The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch fora collection of unseen conceptual label items with associated images.", "labels": [], "entities": []}, {"text": "We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.", "labels": [], "entities": [{"text": "WordNet hierarchies", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9622602462768555}]}], "introductionContent": [{"text": "Human knowledge is naturally organized as semantic hierarchies.", "labels": [], "entities": []}, {"text": "For example, in WordNet, specific concepts are categorized and assigned to more general ones, leading to a semantic hierarchical structure (a.k.a taxonomy).", "labels": [], "entities": []}, {"text": "A variety of NLP tasks, such as question answering (, document clustering () and text generation can benefit from the conceptual relationship present in these hierarchies.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8746830523014069}, {"text": "document clustering", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7585939168930054}, {"text": "text generation", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7551360726356506}]}, {"text": "Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9428355693817139}]}, {"text": "Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision.", "labels": [], "entities": [{"text": "automatic induction of taxonomies", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.6463364437222481}]}, {"text": "On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text;).", "labels": [], "entities": []}, {"text": "These works generally ignore the rich visual data which encode important perceptual semantics () and have proven to be complementary to linguistic information and helpful for many tasks).", "labels": [], "entities": []}, {"text": "On the other hand, researchers have built visual hierarchies by utilizing only visual features (.", "labels": [], "entities": []}, {"text": "The resulting hierarchies are limited in interpretability and usability for knowledge transfer.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7974018156528473}]}, {"text": "Hence, we propose to combine both visual and textual knowledge to automatically build taxonomies.", "labels": [], "entities": []}, {"text": "We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g. ImageNet () or retrieved from the web using search engines, as illustrated in Such a scenario is realistic and can be extended to a variety of tasks; for example, in knowledge base construction ), text and image collections are readily available but label relations among categories are to be uncovered.", "labels": [], "entities": [{"text": "knowledge base construction", "start_pos": 346, "end_pos": 373, "type": "TASK", "confidence": 0.664588729540507}]}, {"text": "In largescale object recognition, automatically learning relations between labels can be quite useful).", "labels": [], "entities": [{"text": "largescale object recognition", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6227108041445414}]}, {"text": "Both textual and visual information provide important cues for taxonomy induction.lustrates this via an example.", "labels": [], "entities": [{"text": "taxonomy induction.lustrates", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.9275902509689331}]}, {"text": "The parent category seafish and its two child categories shark and ray are closely related as: there is a hypernym-hyponym (is-a) relation between the words \"seafish\" and \"shark\"/\"ray\" through text descriptions like \"...seafish, such as shark and ray...\", \"...shark and ray area group of seafish...\"; (2) images of the close neighbors, e.g., shark and ray are usually visually similar and images of the child, e.g. shark/ray are similar to a subset of images of seafish.", "labels": [], "entities": []}, {"text": "To effectively capture these patterns, in contrast to previous works that rely on various hand-crafted features), we extract features by leveraging the distributed representations that embed images) and words (  as compact vectors, based on which the semantic closeness is directly measured in vector space.", "labels": [], "entities": []}, {"text": "Further, we develop a probabilistic framework that integrates the rich multi-modal features to induce \"is-a\" relations between categories, encouraging local semantic consistency that each category should be visually and textually close to its parent and siblings.", "labels": [], "entities": []}, {"text": "In summary, this paper has the following contributions: (1) We propose a novel probabilistic Bayesian model (Section 3) for taxonomy induction by jointly leveraging textual and visual data.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.9053748250007629}]}, {"text": "The model is discriminatively trained and can be directly applied to build a taxonomy from scratch fora collection of semantic labels.", "labels": [], "entities": []}, {"text": "(2) We design novel features (Section 4) based on generalpurpose distributed representations of text and images to capture both textual and visual relations between labels.", "labels": [], "entities": []}, {"text": "(3) We evaluate our model and features on the ImageNet hierarchies with two different taxonomy induction tasks (Section 5).", "labels": [], "entities": [{"text": "ImageNet hierarchies", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.9104371666908264}]}, {"text": "We achieve superior performance on both tasks and improve the F 1 score by 2x in the taxonomy construction task, compared to previous approaches.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9914299249649048}, {"text": "taxonomy construction task", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.8727657397588094}]}, {"text": "Extensive comparisons demonstrate the effectiveness of integrating visual features with language features for taxonomy induction.", "labels": [], "entities": [{"text": "taxonomy induction", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8970197439193726}]}, {"text": "We also provide qualitative analysis on our features, the learned model, and the taxonomies induced to provide further insights (Section 5.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "We first disclose our implementation details in section 5.1 and the supplementary material for bet-ter reproducibility.", "labels": [], "entities": []}, {"text": "We then compare our model with previous state-of-the-art methods () with two taxonomy induction tasks.", "labels": [], "entities": []}, {"text": "Finally, we provide analysis on the weights and taxonomies induced.", "labels": [], "entities": []}, {"text": "We evaluate our model on three subtrees sampled from the ImageNet taxonomy.", "labels": [], "entities": [{"text": "ImageNet taxonomy", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9635564982891083}]}, {"text": "To collect the subtrees, we start from a given root (e.g. consumer goods) and traverse the full taxonomy using BFS, and collect all descendant nodes within a depth h (number of nodes in the longest path).", "labels": [], "entities": [{"text": "BFS", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.7473913431167603}]}, {"text": "We vary h: Statistics of our evaluation set.", "labels": [], "entities": []}, {"text": "The bottom 4 rows give the number of nodes within each height h \u2208 {4, 5, 6, 7}.", "labels": [], "entities": []}, {"text": "The scale of the threes range from small to large, and there is no overlapping among them.", "labels": [], "entities": []}, {"text": "to get a series of subtrees with increasing heights h \u2208 {4, 5, 6, 7} and various scales (maximally 1326 nodes) in different domains.", "labels": [], "entities": []}, {"text": "The statistics of the evaluation sets are provided in.", "labels": [], "entities": []}, {"text": "To avoid ambiguity, all nodes used in ILSVRC 2012 are removed as the CNN feature extractor is trained on them.", "labels": [], "entities": [{"text": "ILSVRC 2012", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.933130145072937}, {"text": "CNN feature extractor", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.6579447189966837}]}, {"text": "We design two different tasks to evaluate our model.", "labels": [], "entities": []}, {"text": "(1) In the hierarchy completion task, we randomly remove some nodes from a tree and use the remaining hierarchy for training.", "labels": [], "entities": [{"text": "hierarchy completion task", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7730072736740112}]}, {"text": "In the test phase, we infer the parent of each removed node and compare it with groundtruth.", "labels": [], "entities": []}, {"text": "This task is designed to figure out whether our model can successfully induce hierarchical relations after learning from within-domain parent-child pairs.", "labels": [], "entities": []}, {"text": "(2) Different from the previous one, the hierarchy construction task is designed to test the generalization ability of our model, i.e. whether our model can learn statistical patterns from one hierarchy and transfer the knowledge to build a taxonomy for another collection of out-of-domain labels.", "labels": [], "entities": [{"text": "hierarchy construction", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7532932758331299}]}, {"text": "Specifically, we select two trees as the training set to learn w.", "labels": [], "entities": []}, {"text": "In the test phase, the model is required to build the full taxonomy from scratch for the third tree.", "labels": [], "entities": []}, {"text": "We use Ancestor F 1 as our evaluation metric ().", "labels": [], "entities": [{"text": "Ancestor F 1", "start_pos": 7, "end_pos": 19, "type": "METRIC", "confidence": 0.8850228389104208}]}, {"text": "Specifically, we measure F 1 = 2P R/(P + R) values of predicted \"is-a\" relations where the precision (P) and recall (R) are: We compare our method to two previously state-of-the-art models by and, which are closest to ours.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 91, "end_pos": 104, "type": "METRIC", "confidence": 0.9225619286298752}, {"text": "recall (R)", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9316304326057434}]}, {"text": "The ancestor-F 1 scores are reported.", "labels": [], "entities": [{"text": "ancestor-F 1 scores", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9320824344952902}]}], "tableCaptions": [{"text": " Table 1: Statistics of our evaluation set. The bot- tom 4 rows give the number of nodes within each  height h \u2208 {4, 5, 6, 7}. The scale of the threes  range from small to large, and there is no overlap- ping among them.", "labels": [], "entities": [{"text": "overlap- ping", "start_pos": 195, "end_pos": 208, "type": "METRIC", "confidence": 0.9536338249842325}]}, {"text": " Table 2: Comparisons among different variants of  our model, Fu et al. (2014) and Bansal et al.", "labels": [], "entities": []}, {"text": " Table 3: The performance when different combi- nations of visual features are enabled.", "labels": [], "entities": []}]}