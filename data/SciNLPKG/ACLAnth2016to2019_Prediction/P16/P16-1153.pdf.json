{"title": [{"text": "Deep Reinforcement Learning with a Natural Language Action Space", "labels": [], "entities": [{"text": "Deep Reinforcement Learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7419650157292684}]}], "abstractContent": [{"text": "This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.", "labels": [], "entities": []}, {"text": "Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning.", "labels": [], "entities": []}, {"text": "We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures.", "labels": [], "entities": []}, {"text": "Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.", "labels": [], "entities": []}], "introductionContent": [{"text": "This work is concerned with learning strategies for sequential decision-making tasks, where a system takes actions at a particular state with the goal of maximizing a long-term reward.", "labels": [], "entities": []}, {"text": "More specifically, we consider tasks where both the states and the actions are characterized by natural language, such as in human-computer dialog systems, tutoring systems, or text-based games.", "labels": [], "entities": []}, {"text": "Ina text-based game, for example, the player (or system, in this case) is given a text string that describes the current state of the game and several text strings that describe possible actions one could take.", "labels": [], "entities": []}, {"text": "After selecting one of the actions, the environment state is updated and revealed in anew textual description.", "labels": [], "entities": []}, {"text": "A reward is given either at each transition or in the end.", "labels": [], "entities": []}, {"text": "The objective is to understand, at each step, the state text and all the action texts to pick the most relevant action, navigating through the sequence of texts so as to obtain the highest longterm reward.", "labels": [], "entities": []}, {"text": "Here the notion of relevance is based on the joint state/action impact on the reward: an action text string is said to be \"more relevant\" (to a state text string) than the other action texts if taking that action would lead to a higher longterm reward.", "labels": [], "entities": []}, {"text": "Because a player's action changes the environment, reinforcement learning) is appropriate for modeling longterm dependency in text games.", "labels": [], "entities": []}, {"text": "There is a large body of work on reinforcement learning.", "labels": [], "entities": []}, {"text": "Of most interest here are approaches leveraging neural networks because of their success in handling a large state space.", "labels": [], "entities": []}, {"text": "Early work -TD-gammon -used a neural network to approximate the state value function.", "labels": [], "entities": []}, {"text": "Recently, inspired by advances in deep learning (), significant progress has been made by combining deep learning with reinforcement learning.", "labels": [], "entities": []}, {"text": "Building on the approach of Q-learning (, the \"Deep Q-Network\" (DQN) was developed and applied to Atari games () and shown to achieve human level performance by applying convolutional neural networks to the raw image pixels.", "labels": [], "entities": []}, {"text": "applied a Long Short-Term Memory network to characterize the state space in a DQN framework for learning control policies for parserbased text games.", "labels": [], "entities": []}, {"text": "More recently, have also proposed a goal-driven web navigation task for language based sequential decision making study.", "labels": [], "entities": [{"text": "web navigation task", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7744572162628174}, {"text": "language based sequential decision making", "start_pos": 72, "end_pos": 113, "type": "TASK", "confidence": 0.5495467722415924}]}, {"text": "Another stream of work focuses on continuous control with deep reinforcement learning (, where an actor-critic algorithm operates over a known continuous action space.", "labels": [], "entities": []}, {"text": "Inspired by these successes and recent work using neural networks to learn phrase-or sentence-level embeddings, we propose a novel deep architecture for text understanding, which we calla deep reinforcement relevance network (DRRN).", "labels": [], "entities": [{"text": "text understanding", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.7812390625476837}]}, {"text": "The DRRN uses separate deep neural networks to map state and action text strings into embedding vectors, from which \"relevance\" is measured numerically by a general interaction function, such as their inner product.", "labels": [], "entities": [{"text": "DRRN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8973579406738281}]}, {"text": "The output of this interaction function defines the value of the Q-function for the current state-action pair, which characterizes the optimal long-term reward for pairing these two text strings.", "labels": [], "entities": []}, {"text": "The Q-function approximation is learned in an end-to-end manner by Q-learning.", "labels": [], "entities": []}, {"text": "The DRRN differs from prior work in that earlier studies mostly considered action spaces that are bounded and known.", "labels": [], "entities": []}, {"text": "For actions described by natural language text strings, the action space is inherently discrete and potentially unbounded due to the exponential complexity of language with respect to sentence length.", "labels": [], "entities": []}, {"text": "A distinguishing aspect of the DRRN architecture -compared to simple DQN extensions -is that two different types of meaning representations are learned, reflecting the tendency for state texts to describe scenes and action texts to describe potential actions from the user.", "labels": [], "entities": []}, {"text": "We show that the DRRN learns a continuous space representation of actions that successfully generalize to paraphrased descriptions of actions unseen in training.", "labels": [], "entities": [{"text": "DRRN", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.857573390007019}]}], "datasetContent": [{"text": "We apply DRRNs with both 1 and 2 hidden layer structures.", "labels": [], "entities": []}, {"text": "In most experiments, we use dotproduct as the interaction function and set the hidden dimension to be the same for each hidden layer.", "labels": [], "entities": []}, {"text": "We use DRRNs with 20, 50 and 100-dimension hidden layer(s) and build learning curves during experience-replay training.", "labels": [], "entities": []}, {"text": "The learning rate is constant: \u03b7 t = 0.001.", "labels": [], "entities": []}, {"text": "In testing, as in training, we apply softmax selection.", "labels": [], "entities": []}, {"text": "We record average final rewards as performance of the model.", "labels": [], "entities": []}, {"text": "The DRRN is compared to multiple baselines: a linear model, two max-action DQNs (MA DQN) (L = 1 or 2 hidden layers), and two per-action DQNs (PA DQN) (again, L = 1, 2).", "labels": [], "entities": []}, {"text": "All baselines use the same Q-learning framework with different function approximators to predict Q(s t , at ) given the current state and actions.", "labels": [], "entities": []}, {"text": "For the linear and MA DQN baselines, the input is the textbased state and action descriptions, each as a bag of words, with the number of outputs equal to the maximum number of actions.", "labels": [], "entities": [{"text": "MA DQN baselines", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.7158623536427816}]}, {"text": "When there are fewer actions than the maximum, the highest scoring available action is used.", "labels": [], "entities": []}, {"text": "The PA DQN baseline  takes each pair of state-action texts as input, and generates a corresponding Q-value.", "labels": [], "entities": [{"text": "PA DQN baseline", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.7667583425839742}]}, {"text": "We use softmax selection, which is widely applied in practice, to trade-off exploration vs. exploitation.", "labels": [], "entities": []}, {"text": "Specifically, for each experiencereplay, we first generate 200 episodes of data (about 3K tuples in \"Saving John\" and 16K tuples in \"Machine of Death\") using the softmax selection rule in (2), where we set \u03b1 = 0.2 for the first game and \u03b1 = 1.0 for the second game.", "labels": [], "entities": [{"text": "Saving John\"", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.8275939424832662}]}, {"text": "The \u03b1 is picked according to an estimation of range of the optimal Q-values.", "labels": [], "entities": []}, {"text": "We then shuffle the generated data tuples (s t , at , rt , s t+1 ) update the model as described in Section 2.4.", "labels": [], "entities": []}, {"text": "The model is trained with multiple epochs for all configurations, and is evaluated after each experience-replay.", "labels": [], "entities": []}, {"text": "The discount factor \u03b3 is set to 0.9.", "labels": [], "entities": [{"text": "discount factor \u03b3", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.948127826054891}]}, {"text": "For DRRN and all baselines, network weights are initialized with small random values.", "labels": [], "entities": [{"text": "DRRN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8444841504096985}]}, {"text": "To prevent algorithms from \"remembering\" state-action ordering and make choices based on action wording, each time the algorithm/player reads text from the simulator, we randomly shuffle the list of actions.", "labels": [], "entities": [{"text": "remembering\" state-action ordering", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.6814264506101608}]}, {"text": "3 This will encourage the algorithms to make decisions based on the understanding of the texts that describe the states and actions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The final average rewards and standard  deviations on \"Saving John\".", "labels": [], "entities": [{"text": "standard", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9712201356887817}, {"text": "Saving John\"", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.7042672832806905}]}, {"text": " Table 3: The final average rewards and standard  deviations on \"Machine of Death\".", "labels": [], "entities": []}, {"text": " Table 5: The final average rewards and stan- dard deviations on paraphrased game \"Machine of  Death\".", "labels": [], "entities": []}]}