{"title": [{"text": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method", "labels": [], "entities": [{"text": "Improving Hypernymy Detection", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8337247371673584}]}], "abstractContent": [{"text": "Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches.", "labels": [], "entities": [{"text": "Detecting hypernymy relations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8843926787376404}]}, {"text": "Distributional methods, whose supervised variants are the current best performers, and path-based methods , which received less research attention.", "labels": [], "entities": []}, {"text": "We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network , that achieves results comparable to distributional methods.", "labels": [], "entities": []}, {"text": "We then extend the approach to integrate both path-based and distributional signals, significantly improving upon the state-of-the-art on this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hypernymy is an important lexical-semantic relation for NLP tasks.", "labels": [], "entities": []}, {"text": "For instance, knowing that Tom Cruise is an actor can help a question answering system answer the question \"which actors are involved in Scientology?\".", "labels": [], "entities": [{"text": "question answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7744636535644531}]}, {"text": "While semantic taxonomies, like WordNet, define hypernymy relations between word types, they are limited in scope and domain.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.934344470500946}]}, {"text": "Therefore, automated methods have been developed to determine, fora given term-pair (x, y), whether y is an hypernym of x, based on their occurrences in a large corpus.", "labels": [], "entities": []}, {"text": "For a couple of decades, this task has been addressed by two types of approaches: distributional, and path-based.", "labels": [], "entities": []}, {"text": "In distributional methods, the decision whether y is a hypernym of x is based on the distributional representations of these terms.", "labels": [], "entities": []}, {"text": "Lately, with the popularity of word embeddings, most focus has shifted towards supervised distributional methods, in which each (x, y) term-pair is represented using some combination of the terms' embedding vectors.", "labels": [], "entities": []}, {"text": "In contrast to distributional methods, in which the decision is based on the separate contexts of x and y, path-based methods base the decision on the lexico-syntactic paths connecting the joint occurrences of x and yin a corpus.", "labels": [], "entities": []}, {"text": "identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features.", "labels": [], "entities": []}, {"text": "Using individual paths as features results in a huge, sparse feature space.", "labels": [], "entities": []}, {"text": "While some paths are rare, they often consist of certain unimportant components.", "labels": [], "entities": []}, {"text": "For instance, \"Spelt is a species of wheat\" and \"Fantasy is a genre of fiction\" yield two different paths: X be species of Y and X be genre of Y, while both indicating that X is-a Y.", "labels": [], "entities": []}, {"text": "A possible solution is to generalize paths by replacing words along the path with their part-of-speech tags or with wild cards, as done in the PATTY system (.", "labels": [], "entities": []}, {"text": "Overall, the state-of-the-art path-based methods perform worse than the distributional ones.", "labels": [], "entities": []}, {"text": "This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods.", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9957667589187622}]}, {"text": "While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms.", "labels": [], "entities": []}, {"text": "Though these approaches seem complementary, there has been rather little work on integrating them (.", "labels": [], "entities": []}, {"text": "In this paper, we present HypeNET, an integrated path-based and distributional method for hypernymy detection.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8661040365695953}]}, {"text": "Inspired by recent progress in relation classification, we use along shortterm memory (LSTM) network) to encode dependency paths.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.9382825195789337}]}, {"text": "In order to create enough training data for our network, we followed previous methodology of constructing a dataset based on knowledge resources.", "labels": [], "entities": []}, {"text": "We first show that our path-based approach, on its own, substantially improves performance over prior path-based methods, yielding performance comparable to state-of-the-art distributional methods.", "labels": [], "entities": []}, {"text": "Our analysis suggests that the neural path representation enables better generalizations.", "labels": [], "entities": []}, {"text": "While coarse-grained generalizations, such as replacing a word by its POS tag, capture mostly syntactic similarities between paths, HypeNET captures also semantic similarities.", "labels": [], "entities": []}, {"text": "We then show that we can easily integrate distributional signals in the network.", "labels": [], "entities": []}, {"text": "The integration results confirm that the distributional and pathbased signals indeed provide complementary information, with the combined model yielding an improvement of up to 14 F 1 points over each individual model.", "labels": [], "entities": [{"text": "F 1", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.935647189617157}]}], "datasetContent": [{"text": "As our primary dataset, we perform standard random splitting, with 70% train, 25% test and 5% validation sets.", "labels": [], "entities": [{"text": "random splitting", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6974422335624695}]}, {"text": "As pointed out by   to perform \"lexical memorization\", i.e., instead of learning a relation between the two terms, they mostly learn an independent property of a single term in the pair: whether it is a \"prototypical hypernym\" or not.", "labels": [], "entities": []}, {"text": "For instance, if the training set contains term-pairs such as (dog, animal), (cat, animal), and (cow, animal), all annotated as positive examples, the algorithm may learn that animal is a prototypical hypernym, classifying any new (x, animal) pair as positive, regardless of the relation between x and animal.", "labels": [], "entities": []}, {"text": "suggested to split the train and test sets such that each will contain a distinct vocabulary (\"lexical split\"), in order to prevent the model from overfitting by lexical memorization.", "labels": [], "entities": []}, {"text": "To investigate such behaviors, we present results also fora lexical split of our dataset.", "labels": [], "entities": []}, {"text": "In this case, we split the train, test and validation sets such that each contains a distinct vocabulary.", "labels": [], "entities": []}, {"text": "We note that this differs from , who split only the train and the test sets, and dedicated a subset of the train for validation.", "labels": [], "entities": []}, {"text": "We chose to deviate from  because we noticed that when the validation set contains terms from the train set, the model is rewarded for lexical memorization when tuning the hyper-parameters, consequently yielding suboptimal performance on the lexically-distinct test set.", "labels": [], "entities": []}, {"text": "When each set has a distinct vocabulary, the hyper-parameters are tuned to avoid lexical memorization and are likely to perform better on the test set.", "labels": [], "entities": []}, {"text": "We tried to keep roughly the same 70/25/5 ratio in our lexical split.", "labels": [], "entities": []}, {"text": "The sizes of the two datasets are shown in.", "labels": [], "entities": []}, {"text": "Indeed, training a model on a lexically split dataset may result in a more general model, that can better handle pairs consisting of two unseen terms during inference.", "labels": [], "entities": []}, {"text": "However, we argue that in the common applied scenario, the inference involves an unseen pair (x, y), in which x and/or y have already been observed separately.", "labels": [], "entities": []}, {"text": "Models trained on a random split may introduce the model with a term's \"prior probability\" of being a hypernym or a hyponym, and this information can be exploited beneficially at inference time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The number of instances in each dataset.", "labels": [], "entities": []}, {"text": " Table 4: Performance scores of our method compared to the path-based baselines and the state-of-the-art distributional methods  for hypernymy detection, on both variations of the dataset -with lexical and random split to train / test / validation.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.8158973455429077}]}, {"text": " Table 5: Examples of indicative paths learned by each method, with corresponding true positive term-pairs from the random  split test set. Hypernyms are marked red and hyponyms are marked blue.", "labels": [], "entities": []}, {"text": " Table 6: Distribution of relations holding between each pair  of terms in the resources among false positive pairs.", "labels": [], "entities": []}, {"text": " Table 7: (Overlapping) categories of false negative pairs:", "labels": [], "entities": []}]}