{"title": [{"text": "Vocabulary Manipulation for Neural Machine Translation", "labels": [], "entities": [{"text": "Vocabulary Manipulation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7811832129955292}, {"text": "Neural Machine Translation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7514564990997314}]}], "abstractContent": [{"text": "In order to capture rich language phenomena , neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.7233270605405172}]}, {"text": "In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small subset of the full output vocabulary.", "labels": [], "entities": []}, {"text": "For each sentence or batch, we only predict the target words in its sentence-level or batch-level vocabulary.", "labels": [], "entities": []}, {"text": "Thus, we reduce both the computing time and the memory usage.", "labels": [], "entities": []}, {"text": "Our method simply takes into account the translation options of each word or phrase in the source sentence , and picks a very small target vocabulary for each sentence based on a word-to-word translation model or a bilingual phrase library learned from a traditional machine translation model.", "labels": [], "entities": []}, {"text": "Experimental results on the large-scale English-to-French task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9984678626060486}, {"text": "large vocabulary neural machine translation", "start_pos": 149, "end_pos": 192, "type": "TASK", "confidence": 0.7184036135673523}]}], "introductionContent": [{"text": "Neural machine translation (NMT) ( ) has gained popularity in recent two years.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8170322378476461}]}, {"text": "But it can only handle a small vocabulary size due to the computational complexity.", "labels": [], "entities": []}, {"text": "In order to capture rich language phenomena and have a better word coverage, neural machine translation models have to use a large vocabulary.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7397527893384298}]}, {"text": "alleviated the large vocabulary issue by proposing an approach that partitions the training corpus and defines a subset of the full target vocabulary for each partition.", "labels": [], "entities": []}, {"text": "Thus, they only use a subset vocabulary for each partition in the training procedure without increasing computational complexity.", "labels": [], "entities": []}, {"text": "However, there are still some drawbacks of's method.", "labels": [], "entities": []}, {"text": "First, the importance sampling is simply based on the sequence of training sentences, which is not linguistically motivated, thus, translation ambiguity may not be captured in the training.", "labels": [], "entities": []}, {"text": "Second, the target vocabulary for each training batch is fixed in the whole training procedure.", "labels": [], "entities": []}, {"text": "Third, the target vocabulary size for each batch during training still needs to be as large as 30k, so the computing time is still high.", "labels": [], "entities": []}, {"text": "In this paper, we alleviate the above issues by introducing a sentence-level vocabulary, which is very small compared with the full target vocabulary.", "labels": [], "entities": []}, {"text": "In order to capture the translation ambiguity, we generate those sentence-level vocabularies by utilizing word-to-word and phrase-tophrase translation models which are learned from a traditional phrase-based machine translation system (SMT).", "labels": [], "entities": [{"text": "phrase-tophrase translation", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.6872600615024567}, {"text": "phrase-based machine translation system (SMT)", "start_pos": 195, "end_pos": 240, "type": "TASK", "confidence": 0.7451187542506627}]}, {"text": "Another motivation of this work is to combine the merits of both traditional SMT and NMT, since training an NMT system usually takes several weeks, while the word alignment and rule extraction for SMT are much faster (can be done in one day).", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9867643117904663}, {"text": "word alignment", "start_pos": 158, "end_pos": 172, "type": "TASK", "confidence": 0.7467950880527496}, {"text": "rule extraction", "start_pos": 177, "end_pos": 192, "type": "TASK", "confidence": 0.7319603264331818}, {"text": "SMT", "start_pos": 197, "end_pos": 200, "type": "TASK", "confidence": 0.9766021370887756}]}, {"text": "Thus, for each training sentence, we build a separate target vocabulary which is the union of following three parts: \u2022 target vocabularies of word and phrase translations that can be applied to the current sentence.", "labels": [], "entities": []}, {"text": "(to capture the translation ambiguity) \u2022 top 2k most frequent target words.", "labels": [], "entities": []}, {"text": "(to cover the unaligned target words) \u2022 target words in the reference of the current sentence.", "labels": [], "entities": []}, {"text": "(to make the reference reachable) As we use mini-batch in the training procedure, we merge the target vocabularies of all the sentences in each batch, and update only those related parameters for each batch.", "labels": [], "entities": []}, {"text": "In addition, we also shuffle the training sentences at the beginning of each epoch, so the target vocabulary fora specific sentence varies in each epoch.", "labels": [], "entities": []}, {"text": "In the beam search for the development or test set, we apply the similar procedure for each source sentence, except the third bullet (as we do not have the reference) and mini-batch parts.", "labels": [], "entities": []}, {"text": "Experimental results on large-scale English-to-French task (Section 5) show that our method achieves significant improvements over the large vocabulary neural machine translation system.", "labels": [], "entities": [{"text": "large vocabulary neural machine translation", "start_pos": 135, "end_pos": 178, "type": "TASK", "confidence": 0.648915046453476}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The average reference coverage ratios (in word-level) on the training and development sets. We  use fixed top 10 candidates for each phrase when generating V P  x , and top 2k most common words for  V T", "labels": [], "entities": []}, {"text": " Table 2: Average vocabulary size for each sen- tence or mini-batch (80 sentences). The full vo- cabulary is 500k, all other words are UNKs.", "labels": [], "entities": [{"text": "UNKs", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.8308095335960388}]}, {"text": " Table 3: Given a trained NMT model, we decode the development set with various top n most common  target words. For En-Fr task, the results suggest that we can reduce the n to 50 without losing much in  terms of BLEU score. The average size of V o is reduced to as small as 202, which is significant lower  than 2067 (the default setting we use in our training).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 213, "end_pos": 223, "type": "METRIC", "confidence": 0.984113872051239}]}, {"text": " Table 4: Single system results on En-Fr task.", "labels": [], "entities": [{"text": "En-Fr task", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.6193159222602844}]}]}