{"title": [{"text": "Multi-Modal Representations for Improved Bilingual Lexicon Learning", "labels": [], "entities": [{"text": "Improved Bilingual Lexicon", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6823398570219675}]}], "abstractContent": [{"text": "Recent work has revealed the potential of using visual representations for bilingual lexicon learning (BLL).", "labels": [], "entities": [{"text": "bilingual lexicon learning (BLL)", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.7504009107748667}]}, {"text": "Such image-based BLL methods, however, still fall short of linguistic approaches.", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple yet effective multi-modal approach that learns bilingual semantic representations that fuse linguistic and visual input.", "labels": [], "entities": []}, {"text": "These new bilingual multi-modal embeddings display significant performance gains in the BLL task for three language pairs on two benchmark-ing test sets, outperforming linguistic-only BLL models using three different types of state-of-the-art bilingual word embed-dings, as well as visual-only BLL models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicon learning (BLL) is the task of finding words that share a common meaning across different languages.", "labels": [], "entities": [{"text": "Bilingual lexicon learning (BLL)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8427022894223531}]}, {"text": "It plays an important role in a variety of fundamental tasks in IR and NLP, e.g. cross-lingual information retrieval and statistical machine translation.", "labels": [], "entities": [{"text": "IR", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9890185594558716}, {"text": "cross-lingual information retrieval", "start_pos": 81, "end_pos": 116, "type": "TASK", "confidence": 0.6754876375198364}, {"text": "statistical machine translation", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.7270513276259104}]}, {"text": "The majority of current BLL models aim to learn lexicons from comparable data.", "labels": [], "entities": []}, {"text": "These approaches work by (1) mapping language pairs to a shared crosslingual vector space (SCLVS) such that words are close when they have similar meanings; and (2) extracting close lexical items from the induced SCLVS.", "labels": [], "entities": []}, {"text": "Bilingual word embedding (BWE) induced models currently hold the state-of-the-art on BLL (.", "labels": [], "entities": [{"text": "Bilingual word embedding (BWE) induced", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6900592488901955}, {"text": "BLL", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.6170504689216614}]}, {"text": "Although methods for learning SCLVSs are predominantly text-based, this space need not be linguistic in nature: Bergsma and van and used labeled images from the Web to learn bilingual lexicons based on visual features, with features derived from deep convolutional neural networks (CNNs) leading to the best results ().", "labels": [], "entities": []}, {"text": "However, vision-based BLL does not yet perform at the same level as state-of-the-art linguistic models.", "labels": [], "entities": []}, {"text": "Here, we unify the strengths of both approaches into one single multi-modal vision-language SCLVS.", "labels": [], "entities": []}, {"text": "It has been found in multi-modal semantics that linguistic and visual representations are often complementary in terms of the information they encode (.", "labels": [], "entities": []}, {"text": "This is the first work to test the effectiveness of the multi-modal approach in a BLL setting.", "labels": [], "entities": []}, {"text": "Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations.", "labels": [], "entities": []}, {"text": "These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (, as well as over the uni-modal visual representations from.", "labels": [], "entities": [{"text": "BLL test sets", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.77263143658638}]}, {"text": "We also propose a weighting technique based on image dispersion ( ) that governs the influence of visual information in fused representations, and show that this technique leads to robust multi-modal models which do not require fine tuning of the fusion parameter.", "labels": [], "entities": []}], "datasetContent": [{"text": "Task: Bilingual Lexicon Learning Given a source language word w s , the task is to find a target language word wt closest tow sin the SCLVS, and the resulting pair (w s , wt ) is a bilingual lexicon entry.", "labels": [], "entities": [{"text": "Bilingual Lexicon Learning", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.791384239991506}]}, {"text": "Performance is measured using the BLL   LinguaTools . The 100K most frequent words were retained for all models.", "labels": [], "entities": [{"text": "BLL   LinguaTools", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.81706902384758}]}, {"text": "We followed related work () for learning the mapping W in M-EMB: starting from the BNC word frequency list, the 6, 318 most frequent EN words were translated to the three other languages using Google Translate.", "labels": [], "entities": [{"text": "BNC word frequency list", "start_pos": 83, "end_pos": 106, "type": "DATASET", "confidence": 0.7430969029664993}]}, {"text": "The lists were subsequently cleaned, removing all pairs that contain IT/ES/NL words occurring in the test sets and least frequent pairs, to build the final 3\u00d75K training pairs.", "labels": [], "entities": []}, {"text": "We trained two monolingual SGNS models, using SGD with a global learning rate of 0.025.", "labels": [], "entities": []}, {"text": "For G-EMB, as in the original work (, the bilingual signal for the cross-lingual regularization was provided in the first 500K sentences from Europarl.v7).", "labels": [], "entities": [{"text": "Europarl.v7", "start_pos": 142, "end_pos": 153, "type": "DATASET", "confidence": 0.8571832776069641}]}, {"text": "We used SGD with a global learning rate 0.15.", "labels": [], "entities": []}, {"text": "For V-EMB, monolingual SGNS was trained on pseudo-bilingual documents using SGD with a global learning rate 0.025.", "labels": [], "entities": []}, {"text": "All BWEs were trained with d = 300.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.877940833568573}]}, {"text": "7 Other parameters are: 15 epochs, 15 negatives, subsampling rate 1e \u2212 4.", "labels": [], "entities": []}, {"text": "We report results with two \u03b1 standard values: 0.5 and 0.7 (more weight assigned to the linguistic part).", "labels": [], "entities": []}, {"text": "summarizes Acc 1 scores, focusing on interesting comparisons across different dimen-6 http://linguatools.org/tools/corpora/ 7 Similar trends were observed with all models and d = 64, 500.", "labels": [], "entities": [{"text": "Acc 1", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9830525517463684}]}, {"text": "We also vary the window size from 4 to 16 in steps of 4, and always report the best scoring linguistic embeddings.", "labels": [], "entities": []}, {"text": "sions 8 . There is a marked difference in performance on BERGSMA500 and VULIC1000: visual-only BLL models on VULIC1000 perform two times worse than linguistic-only BLL models.", "labels": [], "entities": [{"text": "BERGSMA500", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.5553510189056396}, {"text": "VULIC1000", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9580499529838562}, {"text": "VULIC1000", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.9648006558418274}]}, {"text": "This is easily explained by the increased abstractness of test words in VULIC1000 in comparison to BERGSMA500 9 , which highlights the need fora multi-modal approach.", "labels": [], "entities": [{"text": "VULIC1000", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9311626553535461}, {"text": "BERGSMA500", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.8425824642181396}]}], "tableCaptions": [{"text": " Table 1: Summary of the Acc 1 scores on BERGSMA500 (regular font) and VULIC1000 (italic) across all  BLL runs. M/G/V-EMB denotes the BWE linguistic model. Other settings are in the form Y-Z-0.W: (1)  Y denotes the visual metric, (2) Z denotes the fusion model: E is for Early-Fusion, L is for Late-Fusion,  and (3) 0.W denotes the \u03b1 value. Highest scores per column are in bold.", "labels": [], "entities": [{"text": "Acc 1", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9691847264766693}, {"text": "BERGSMA500", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.6043384671211243}, {"text": "VULIC1000", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.7289494872093201}]}]}