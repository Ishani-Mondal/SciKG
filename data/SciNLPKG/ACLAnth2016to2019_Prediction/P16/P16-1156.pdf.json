{"title": [{"text": "Morphological Smoothing and Extrapolation of Word Embeddings", "labels": [], "entities": [{"text": "Morphological Smoothing and Extrapolation of Word Embeddings", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7606790832110814}]}], "abstractContent": [{"text": "Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context.", "labels": [], "entities": []}, {"text": "For instance , each count noun in Czech has 12 forms (where English uses only singular and plural).", "labels": [], "entities": []}, {"text": "Even in large corpora, we are unlikely to observe all inflections of a given lemma.", "labels": [], "entities": []}, {"text": "This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information.", "labels": [], "entities": []}, {"text": "We solve this problem by exploiting existing morphological resources that can enumerate a word's component morphemes.", "labels": [], "entities": []}, {"text": "We present a latent-variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words.", "labels": [], "entities": []}, {"text": "The latent variables represent embeddings of morphemes, which combine to create em-beddings of words.", "labels": [], "entities": []}, {"text": "Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy , and word similarity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8982577323913574}]}], "introductionContent": [{"text": "Representations of words as high-dimensional real vectors have been shown to benefit a wide variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Because of this demonstrated utility, many aspects of vector representations have been explored recently in the literature.", "labels": [], "entities": [{"text": "vector representations", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7233998626470566}]}, {"text": "One of the most interesting discoveries is that these representations capture meaningful morpho-syntactic and semantic properties through very simple linear relations: in a semantic vector space, we observe that That this equation approximately holds across many morphologically related 4-tuples indicates that the learned embeddings capture a feature of English morphology-adding the past tense feature roughly corresponds to adding a certain vector.", "labels": [], "entities": []}, {"text": "Moreover, manipulating this equation yields what we will call the vector offset method () for approximating other vectors.", "labels": [], "entities": []}, {"text": "For instance, if we only know the vectors for the Spanish words comieron (ate), comemos (eat) and bebieron (drank), we can produce an approximation of the vector for bebemos (drink), as shown in.", "labels": [], "entities": []}, {"text": "Many languages exhibit much richer morphology than English.", "labels": [], "entities": []}, {"text": "While English nouns commonly take two forms -singular and pluralCzech nouns take 12 and Turkish nouns takeover 30.", "labels": [], "entities": []}, {"text": "This increase in word forms per lemma creates considerable data sparsity.", "labels": [], "entities": []}, {"text": "Fortunately, for many languages there exist large morphological lexicons, or better yet, morphological tools that can analyze any word form-meaning that we have analyses (usually accurate) for forms that were unobserved or rare in our training corpus.", "labels": [], "entities": []}, {"text": "Our proposed method runs as a fast postprocessor (taking under a minute to process 100-dimensional embeddings of a million observed word types) on the output of any existing tool that constructs word embeddings, such as WORD2VEC.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 220, "end_pos": 228, "type": "DATASET", "confidence": 0.9054083228111267}]}], "datasetContent": [{"text": "We perform three experiments to test the ability of our model to improve on WORD2VEC.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9245262742042542}]}, {"text": "To reiterate, our approach does not generate or analyze a word's spelling.", "labels": [], "entities": []}, {"text": "Rather, it uses an existing morphological analysis of a word's spelling (constructed manually or by a rule-based or statistical system) as a resource to improve its embedding.", "labels": [], "entities": []}, {"text": "In our first experiment, we attempt to identify a corpus word that expresses a given set of morphological attributes.", "labels": [], "entities": []}, {"text": "In our second experiment, we attempt to use a word's embedding to predict the words that appear in its context, i.e., the skip-gram objective of.", "labels": [], "entities": []}, {"text": "Our third example attempts to use word embeddings to predict human similarity judgments.", "labels": [], "entities": [{"text": "predict human similarity judgments", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.635504923760891}]}, {"text": "We experiment on 5 languages: Czech, English, German, Spanish and Turkish.", "labels": [], "entities": []}, {"text": "For each language, our corpus data consists of the full Wikipedia text.", "labels": [], "entities": []}, {"text": "in Appendix A reports the number of types and tokens and their ratio.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.915944516658783}]}, {"text": "The lexicons we use are characterized in: MorfFlex CZ for Czech, CELEX for English and German ( and lexicons for Spanish and Turkish that were scraped from Wiktionary by.", "labels": [], "entities": [{"text": "MorfFlex CZ", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.8502988815307617}, {"text": "CELEX", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9530215263366699}]}, {"text": "Given a finite training corpus C and a lexicon L,    types i \u2208 C, using the GENSIM implementation of the WORD2VEC hierarchical softmax skip-gram model), with a context size of 5.", "labels": [], "entities": []}, {"text": "We set the dimension n to 100 for all experiments.", "labels": [], "entities": [{"text": "dimension", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9631819725036621}]}, {"text": "We then apply our GGM to generate smoothed embeddings w i for all word types i \u2208 C \u2229 L.", "labels": [], "entities": []}, {"text": "(Recall that the noun and verb sense of bats are separate types in L, even if conflated in C, and get separate embeddings.)", "labels": [], "entities": []}, {"text": "How do we handle other word types?", "labels": [], "entities": [{"text": "handle other word types", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7481585592031479}]}, {"text": "For an out-of-vocabulary (OOV) test word i \u2208 C, we will extrapolate w i \u2190 k\u2208M i m k on demand, as the GGM predicts, provided i \u2208 L.", "labels": [], "entities": []}, {"text": "If any of these morphemes m k were themselves never seen in C, we back off to the mode of the prior to take m k = 0. 7 Our experiments also encounter out-of-lexicon (OOL) test words i \u2208 L, for which we have no morphological analysis; here we take w i = vi (unsmoothed) if i \u2208 C and w i = 0 otherwise.", "labels": [], "entities": []}, {"text": "Our first set of experiments uses embeddings for word selection.", "labels": [], "entities": [{"text": "word selection", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.811419278383255}]}, {"text": "Our prediction task is to identify the unique word i \u2208 C that expresses the An additional important hyperparameter is the number of epochs.", "labels": [], "entities": []}, {"text": "The default value in the GENSIM package is 5, which is suitable for larger corpora.", "labels": [], "entities": [{"text": "GENSIM package", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9110289216041565}]}, {"text": "We use this value for Experiments 1 and 3.", "labels": [], "entities": []}, {"text": "Experiment 2 involves training on smaller corpora and we found it necessary to set the number of epochs to 10.", "labels": [], "entities": []}, {"text": "One could in principle learn \"backoff morphemes.\"", "labels": [], "entities": []}, {"text": "For instance, if borogoves is analyzed as, we might want m Lemma=OOV NOUN = 0 to represent novel nouns.", "labels": [], "entities": [{"text": "Lemma", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9386981129646301}, {"text": "OOV", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.7401113510131836}, {"text": "NOUN", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.5797989964485168}]}, {"text": "morphological attributes M i . To do this, we predict a target embedding x, and choose the most similar unsmoothed word by cosine distance, \u02c6 \u0131 = argmax j\u2208C v j \u00b7 x.", "labels": [], "entities": []}, {"text": "We are scored correct if\u02c6\u0131if\u02c6if\u02c6\u0131 = i.", "labels": [], "entities": []}, {"text": "Our experimental design ensures that i \u2208 L, since if it were, we could trivially find i simply by consulting L.", "labels": [], "entities": []}, {"text": "The task is to identify missing lexical entries, by exploiting the distributional properties in C.", "labels": [], "entities": []}, {"text": "8 Given the input bundle M i , our method predicts the embedding x = k\u2208M i m k , and so looks fora word j \u2208 C whose unsmoothed embedding v j \u2248 x.", "labels": [], "entities": []}, {"text": "The GGM's role here is to predict that the bundle M i will be realized by something like x.", "labels": [], "entities": []}, {"text": "The baseline method is the analogy method of equation.", "labels": [], "entities": []}, {"text": "This predicts the embedding x via the vector-offset formula v a + (v b \u2212 v c ), where a, b, c \u2208 C \u2229 L are three other words sharing i's coarse part of speech such that M i can be expressed as M a + (M b \u2212 M c ).", "labels": [], "entities": []}, {"text": "9 Specifically, the baseline chooses a, b, c uniformly at random from all possibilities.", "labels": [], "entities": []}, {"text": "(This is not too inefficient: given a, at most one choice of (b, c) is possible.)", "labels": [], "entities": []}, {"text": "Note that the baseline extrapolates from the unsmoothed embeddings of 3 other words, whereas the GGM considers all words in C \u2229 L that share i's morphemes.: Test results for Experiment 1.", "labels": [], "entities": [{"text": "GGM", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.8246190547943115}]}, {"text": "The rows indicate the inflection of the test word i to be predicted (superscript P indicates plural, superscript S singular).", "labels": [], "entities": []}, {"text": "The columns indicate the prediction method.", "labels": [], "entities": [{"text": "prediction", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.949264407157898}]}, {"text": "Each number is an average over 10 training-test splits.", "labels": [], "entities": []}, {"text": "Improvements marked with a are statistically significant (p < 0.05) under a paired permutation test over these 10 runs.", "labels": [], "entities": []}, {"text": "Experimental Setup: A lexical resource consists of pairs (word form i, analysis M i ).", "labels": [], "entities": []}, {"text": "For each language, we take a random 80% of these pairs to serve as the training lexicon L that is seen by the GGM.", "labels": [], "entities": [{"text": "GGM", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.9466859698295593}]}, {"text": "The remaining pairs are used to construct our prediction problems (given M i , predict i), with a random 10% each as dev and test examples.", "labels": [], "entities": []}, {"text": "We compare our method against the baseline method on ten such random training-test splits.", "labels": [], "entities": []}, {"text": "We are releasing all splits for future research.", "labels": [], "entities": []}, {"text": "For some dev and test examples, the baseline method has no choice of the triple a, b, c.", "labels": [], "entities": []}, {"text": "Rather than score these examples as incorrect, our baseline results do not consider them at all (which inflates performance).", "labels": [], "entities": []}, {"text": "For each remaining example, to reduce variance, the baseline method reports the average performance on up to 100 a, b, c triples sampled uniformly without replacement.", "labels": [], "entities": [{"text": "variance", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9718801975250244}]}, {"text": "The automatically created analogy problems (a, b, c \u2192 i) solved by the baseline are similar to those of.", "labels": [], "entities": []}, {"text": "However, most previous analogy evaluation sets evaluate only on 4-tuples of frequent words, to escape the need for smoothing, while ours also include infrequent words.", "labels": [], "entities": []}, {"text": "Previous evaluation sets also tend to be translations of the original English datasets-leaving them impoverished as they therefore only test morpho-syntactic properties found in English.", "labels": [], "entities": []}, {"text": "E.g., the German analogy problems of do not explore the four cases and two numbers in the German adjectival system.", "labels": [], "entities": []}, {"text": "Thus our baseline analogy results are useful as a more comprehensive study of the vector offset method for randomly sampled words.", "labels": [], "entities": []}, {"text": "Results: Overall results for 5 languages are shown in.", "labels": [], "entities": []}, {"text": "Additional rows breakdown performance by the inflection of the target word i.", "labels": [], "entities": []}, {"text": "(The inflections shown are the ones for which the baseline method is most accurate.)", "labels": [], "entities": []}, {"text": "For almost all target inflections, GGM is significantly better than the analogy baseline.", "labels": [], "entities": [{"text": "GGM", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9946231842041016}]}, {"text": "An extreme case is the vocative plural in Czech, for which GGM predicts vectors better by more than 70%.", "labels": [], "entities": []}, {"text": "In other cases, the margin is slimmer; but GGM loses only on predicting the Spanish feminine singular participle.", "labels": [], "entities": [{"text": "margin", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9912522435188293}, {"text": "GGM", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.35598140954971313}]}, {"text": "For Czech, German, English and Spanish the results are clear-GGM yields better predictions.", "labels": [], "entities": []}, {"text": "This is not surprising as our method incorporates information from multiple morphologically related forms.", "labels": [], "entities": []}, {"text": "More detailed results for two languages are given in.", "labels": [], "entities": []}, {"text": "Here, each row constrains the source word a to have a certain inflectional tag; again we average over up to 100 analogies, now chosen under this constraint, and again we discard a test example i from the test set if no such analogy exists.", "labels": [], "entities": []}, {"text": "The GGM row considers all test examples.", "labels": [], "entities": [{"text": "GGM row", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.6221320629119873}]}, {"text": "Past work on morphosyntactic analogies has generally constrained a to be the unmarked (lemma) form.", "labels": [], "entities": []}, {"text": "However, we observe that it is easier to predict one word form from another starting from a form that is \"closer\" in morphological space.", "labels": [], "entities": []}, {"text": "For instance, it is easier to predict Czech forms inflected in the genitive plural from forms in nominative plural, rather than the nominative singular.", "labels": [], "entities": []}, {"text": "Likewise, it is easier to predict a singular form from another singular form rather than from a plural form.", "labels": [], "entities": []}, {"text": "It also is easier to predict partially syncretic forms, i.e., two inflected forms that share the same orthographic string; e.g., in Czech the nominative plural and the accusative plural are identical for inanimate nouns.", "labels": [], "entities": []}, {"text": "We now evaluate the smoothed and extrapolated representations w i . Fundamentally, we want to know if our approach improves the embeddings of the entire vocabulary, as if we had seen more evidence.", "labels": [], "entities": []}, {"text": "But we cannot simply compare our smoothed vectors to \"gold\" vectors trained on much more data, since two different runs of WORD2VEC will produce incomparable embedding schemes.", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.8749775290489197}]}, {"text": "We must ask whether our embeddings improve results on a downstream task.", "labels": [], "entities": []}, {"text": "To avoid choosing a downstream task with a narrow application, we evaluate our embedding using the WORD2VEC skip-gram objective on held-out data-as one would evaluate a language model.", "labels": [], "entities": [{"text": "WORD2VEC skip-gram objective", "start_pos": 99, "end_pos": 127, "type": "METRIC", "confidence": 0.7372150619824728}]}, {"text": "If we believe that a better score on the WORD2VEC objective indicates generally more useful embeddings-which indeed we do as we optimize for it-then improving this score indicates that our smoothed vectors are superior.", "labels": [], "entities": [{"text": "WORD2VEC objective", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.8084431886672974}]}, {"text": "Concretely, the objective is where T sis the s th sentence in the test corpus, t indexes its tokens, and j indexes tokens near t.", "labels": [], "entities": []}, {"text": "The probability model p word2vec is defined in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 47, "end_pos": 49, "type": "DATASET", "confidence": 0.9238948822021484}]}, {"text": "It relies on an embedding of the word form T st . 10 Our baseline approach In the hierarchical softmax version, it also relies on a separate embedding fora variable-length bit-string encoding of the context word Tsj.", "labels": [], "entities": []}, {"text": "Unfortunately, we do not currently know of away to smooth these bit-string encodings (also found by WORD2VEC).", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.9170503616333008}]}, {"text": "However, it might be possible to directly incorporate morphology into the construction of the vocabulary tree that defines the bit-strings.", "labels": [], "entities": []}, {"text": "simply uses WORD2VEC's embeddings (or 0 for OOV words T st \u2208 C).", "labels": [], "entities": []}, {"text": "Our GGM approach substitutes \"better\" embeddings when T st appears in the lexicon L (if T st is ambiguous, we use the mean w i vector from all i \u2208 L with spelling T st ).", "labels": [], "entities": []}, {"text": "Note that (8) is itself a kind of task of predicting words in context, resembling language modeling or a \"cloze\" task.", "labels": [], "entities": [{"text": "predicting words in context", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.8627500832080841}, {"text": "language modeling", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7351194620132446}]}, {"text": "Also, Taddy (2015) showed how to use this objective for document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7931019961833954}]}, {"text": "Experimental Setup: We evaluate GGM on the same 5 languages, but now holdout part of the corpus instead of part of the lexicon.", "labels": [], "entities": []}, {"text": "We take the training corpus C to be the initial portion of Wikipedia of size 10 5 , 10 6 , 10 7 or 10 8 . (We skip the 10 8 case for the smaller datasets: Czech and Turkish).", "labels": [], "entities": []}, {"text": "The 10 7 tokens after that are the dev corpus; the next 10 7 tokens are the test corpus.", "labels": [], "entities": []}, {"text": "Results: We report results on three languages in and all languages in Appendix B. Smoothing from vi tow i helps a lot, reducing perplexity by up to 48% (Czech) with 10 5 training tokens and up to 10% (Spanish) even with 10 8 training tokens.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.763722836971283}]}, {"text": "This roughly halves the perplexity, which in the case of 10 5 training tokens, is equivalent to 8\u00d7 more training data.", "labels": [], "entities": []}, {"text": "This is a clear win for lower-resource languages.", "labels": [], "entities": []}, {"text": "We get larger gains from smoothing the rarer predicting words, but even words with frequency \u2265 10 \u22124 benefit.", "labels": [], "entities": []}, {"text": "(The exception is Turkish, where the large gains are confined to rare predicting words.)", "labels": [], "entities": []}, {"text": "See Appendix B for more analysis.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8690831065177917}]}, {"text": "As a third and final experiment, we consider word similarity using the WS-353 data set (), translated into Spanish (Hassan and Mihalcea, 2009) and German.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7679693400859833}, {"text": "WS-353 data set", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9814092318216959}]}, {"text": "The datasets are composed of 353 pairs of words.", "labels": [], "entities": []}, {"text": "Multiple native speakers were then asked to give an integral value between 1 and 10 indicating the similarity of that pair, and those values were then averaged.", "labels": [], "entities": [{"text": "similarity", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9690377712249756}]}, {"text": "In each case, we train the GGM on the whole Wikipedia corpus for the language.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.9301066100597382}]}, {"text": "Since in each language every word in the WS-353 set is in fact a lemma, we use the latent embedding our GGM learns in the experiment.", "labels": [], "entities": [{"text": "WS-353 set", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.9407006204128265}]}, {"text": "In Spanish, for example, we use the learned latent morpheme embedding for the lemma BEBER (recall this takes information from every element in the paradigm, e.g., bebemos and beben), rather than the embedding for the infinitival form beber.", "labels": [], "entities": [{"text": "BEBER", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9852625131607056}]}, {"text": "In highly inflected languages we expect this to improve performance, because to get the embedding of a lemma, it leverages the distributional signal from all inflected forms of that lemma, not just a single one.", "labels": [], "entities": []}, {"text": "Note that unlike previous retrofitting approaches, we do not introduce new semantic information into the model, but rather simply allow the model to better exploit the distributional properties already in the text, by considering words with related lemmata together.", "labels": [], "entities": []}, {"text": "In essence, our approach embeds a lemma as the average of all words containing that lemma, after \"correcting\" those forms by subtracting off their other morphemes (e.g., inflectional affixes).", "labels": [], "entities": []}, {"text": "Results: As is standard in the literature, we report Spearman's correlation cofficient \u03c1 between the averaged human scores and the cosine distance between the embeddings.", "labels": [], "entities": [{"text": "Spearman's correlation cofficient \u03c1", "start_pos": 53, "end_pos": 88, "type": "METRIC", "confidence": 0.7797191798686981}]}, {"text": "We additionally report the average num-ber of forms per lemma.", "labels": [], "entities": []}, {"text": "We find that we improve performance on the Spanish and German datasets over the original skip-gram vectors, but only equal the performance on English.", "labels": [], "entities": []}, {"text": "This is not surprising as German and Spanish have roughly 3 and 4 times more forms per lemma than English.", "labels": [], "entities": []}, {"text": "We speculate that cross-linguistically the GGM will improve word similarity scores more for languages with richer morphology.", "labels": [], "entities": [{"text": "GGM", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.7353066205978394}]}], "tableCaptions": [{"text": " Table 2: The two tables show how the Gaussian graphical model (GGM) compares to various analogies on Czech nouns (left)  and Spanish verbs (right).The numbers in each cell represent the accuracy (larger is better). The columns represent the inflection  of the word i to be predicted. Our GGM model is the top row. The other rows subdivide the baseline analogy results according  to the inflection of source word a. Abbreviations: in the Czech noun table (left), the first word indicates the case and the second  the number, e.g., Dat Sg = Dative Singular. In the Spanish verb table (right), the first word is the person and number and the  second the tense, e.g., 3pp Pt = 3rd-person plural past.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9994251728057861}]}, {"text": " Table 3: Test results for Experiment 1. The rows indicate the inflection of the test word i to be predicted (superscript P indicates  plural, superscript S singular). The columns indicate the prediction method. Each number is an average over 10 training-test  splits. Improvements marked with a are statistically significant (p < 0.05) under a paired permutation test over these 10 runs.", "labels": [], "entities": []}, {"text": " Table 4: Word similarity results (correlations) using the WS- 353 dataset in the three languages, in which it is available.  Since all the words in WS-353 are lemmata, we report the  average inflected form to lemma ratio for forms appearing in  the datasets.", "labels": [], "entities": [{"text": "WS- 353 dataset", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9525670409202576}, {"text": "WS-353", "start_pos": 149, "end_pos": 155, "type": "DATASET", "confidence": 0.9459180235862732}]}]}