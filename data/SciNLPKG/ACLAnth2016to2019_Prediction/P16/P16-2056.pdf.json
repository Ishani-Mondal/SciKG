{"title": [{"text": "Bootstrapped Text-level Named Entity Recognition for Literature", "labels": [], "entities": [{"text": "Bootstrapped Text-level Named Entity Recognition", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.44304372668266295}]}], "abstractContent": [{"text": "We present a named entity recognition (NER) system for tagging fiction: LitNER.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.8088117937246958}, {"text": "tagging fiction", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.919465184211731}]}, {"text": "Relative to more traditional approaches , LitNER has two important properties: (1) it makes no use of hand-tagged data or gazetteers, instead it boot-straps a model from term clusters; and (2) it leverages multiple instances of the same name in a text.", "labels": [], "entities": []}, {"text": "Our experiments show it to substantially outperform off-the-shelf supervised NER systems.", "labels": [], "entities": [{"text": "NER", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9368700385093689}]}], "introductionContent": [{"text": "Much of the work on applying NLP to the analysis of literature has focused on literary figures/characters in the text, e.g. in the context of social network analysis ( or analysis of characterization ().", "labels": [], "entities": [{"text": "social network analysis", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.7149236003557841}, {"text": "analysis of characterization", "start_pos": 171, "end_pos": 199, "type": "TASK", "confidence": 0.8174630403518677}]}, {"text": "Named entity recognition (NER) of person names is generally the first step in identifying characters; locations are also a prevalent NE type, and can be useful when tracking different plot threads), or trends in the settings of fiction.", "labels": [], "entities": [{"text": "Named entity recognition (NER) of person names", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8137715425756242}]}, {"text": "There are not, to our knowledge, any NER systems that are specifically targeted at literature, and most related work has used Stanford CoreNLP as an off-the-shelf solution (.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 126, "end_pos": 142, "type": "DATASET", "confidence": 0.8865050673484802}]}, {"text": "In this paper, we show that it is possible to take advantage of the properties of fiction texts, in particular the repetition of names, to build a high-performing 3-class NER system which distinguishes people and locations from other capitalized words and phrases.", "labels": [], "entities": []}, {"text": "Notably, we do this without any hand-labelled data whatsoever, bootstrapping a text-level context classifier from a low-dimensional Brown clustering of the Project Gutenberg corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our interest is in a general NER system for literature.", "labels": [], "entities": [{"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9745896458625793}]}, {"text": "Though there area few novels which have been tagged for characters, we wanted to test our system relative to a much wider range of fiction.", "labels": [], "entities": []}, {"text": "To this end, we randomly sampled texts, sentences, and then names within those sentences from our name-segmented Project Gutenberg corpus to produce a set of 1000 examples.", "labels": [], "entities": []}, {"text": "These were tagged by a single annotator, an English native speaker with a PhD in English Literature.", "labels": [], "entities": []}, {"text": "The annotator was presented with the sentence and the pre-segmented name of interest, and asked (via written instructions) to categorize the indicated name into PERSON, LOCATION, OTHER, UNCERTAIN due to ambiguity, or segmentation error.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9902521371841431}, {"text": "LOCATION", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.8441675901412964}, {"text": "OTHER", "start_pos": 179, "end_pos": 184, "type": "METRIC", "confidence": 0.9370302557945251}, {"text": "UNCERTAIN", "start_pos": 186, "end_pos": 195, "type": "METRIC", "confidence": 0.8119089007377625}]}, {"text": "We ran a separate two-annotator agreement study over 200 examples which yielded a Cohen's Kappa of 0.84, suggesting high enough reliability that a single annotator was sufficient.", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.590570459763209}]}, {"text": "The class  For the main evaluation, we excluded both UNCER-TAIN examples and segmentation errors, but had our annotator provide correct segmentation for the 15 segmentation errors and carried out a separate comparison on these.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 77, "end_pos": 89, "type": "TASK", "confidence": 0.9338607788085938}]}, {"text": "We compare our system to a selection of publicly available, off-the-shelf NER systems: OpenNLP, 4 LingPipe, and Stanford CoreNLP (), as well as the initial Brown clustering.", "labels": [], "entities": [{"text": "OpenNLP", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9582983255386353}, {"text": "Stanford CoreNLP", "start_pos": 112, "end_pos": 128, "type": "DATASET", "confidence": 0.8429286181926727}]}, {"text": "OpenNLP allowed us to classify only PERSON and LOCATION, but for Stanford CoreNLP and LingPipe we used the existing 3-entity systems, with the ORGANI-ZATION tag collapsed into OTHER (as it was in our guidelines; instances of ORGANIZATION are rare in literature).", "labels": [], "entities": [{"text": "LingPipe", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9231927990913391}]}, {"text": "Since the exact segmentation guidelines likely varied across these systems-in particular, we found that Stanford CoreNLP often left off the title in names such as Mr. Smithand we didn't want to focus on these issues, we did not require exact matches of our name segmentation; instead, we consider the entire name as PERSON or LOCATION if any of the tokens were tagged as such (names with both tags were considered OTHER).", "labels": [], "entities": [{"text": "Mr. Smithand", "start_pos": 163, "end_pos": 175, "type": "DATASET", "confidence": 0.8814816176891327}, {"text": "name segmentation", "start_pos": 257, "end_pos": 274, "type": "TASK", "confidence": 0.7466492056846619}, {"text": "PERSON", "start_pos": 316, "end_pos": 322, "type": "METRIC", "confidence": 0.970569372177124}, {"text": "LOCATION", "start_pos": 326, "end_pos": 334, "type": "METRIC", "confidence": 0.7012864947319031}, {"text": "OTHER", "start_pos": 414, "end_pos": 419, "type": "METRIC", "confidence": 0.9323263168334961}]}, {"text": "For our system (LitNER), we test aversion where only the immediate sentence context is used (\"sentence\"), and versions based on text context (\"text\") with or without our phrase improvement (\"\u00b1phrase\").", "labels": [], "entities": []}, {"text": "We evaluate using two standard metrics: accuracy (\"Acc\"), and macroaveraged F-score (\"F M \").", "labels": [], "entities": [{"text": "accuracy (\"Acc\")", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.7985915020108223}, {"text": "macroaveraged F-score (\"F M \")", "start_pos": 62, "end_pos": 92, "type": "METRIC", "confidence": 0.8820558190345764}]}], "tableCaptions": [{"text": " Table 2: Performance of NER systems", "labels": [], "entities": [{"text": "NER", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9386932849884033}]}]}