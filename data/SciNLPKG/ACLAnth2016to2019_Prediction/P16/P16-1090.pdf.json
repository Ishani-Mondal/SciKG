{"title": [{"text": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings", "labels": [], "entities": [{"text": "Learning Semantic Mappings", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.6616816918055216}]}], "abstractContent": [{"text": "Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be cor-rect?", "labels": [], "entities": []}, {"text": "We answer the question in the affirmative provided our model family is well-specified.", "labels": [], "entities": []}, {"text": "Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output.", "labels": [], "entities": []}, {"text": "We operational-ize this principle for semantic parsing, the task of mapping utterances to logical forms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7360580563545227}]}, {"text": "We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models.", "labels": [], "entities": []}, {"text": "We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9995490908622742}]}, {"text": "Empirically , we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.9399784505367279}]}], "introductionContent": [{"text": "If a user asks a system \"How many painkillers should I take?\", it is better for the system to say \"don't know\" rather than making a costly incorrect prediction.", "labels": [], "entities": []}, {"text": "When the system is learned from data, uncertainty pervades, and we must manage this uncertainty properly to achieve our precision requirement.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9991483688354492}]}, {"text": "It is particularly challenging since training inputs might not be representative of test inputs due to limited data, covariate shift), or adversarial filtering (.", "labels": [], "entities": []}, {"text": "In this unforgiving setting, can we still train a system that is guaranteed to either abstain or to make the correct prediction?", "labels": [], "entities": []}, {"text": "Our present work is motivated by the goal of  : Given a set of training examples, we compute C, the set of all mappings consistent with the training examples.", "labels": [], "entities": []}, {"text": "On an input x, if all mappings in C unanimously predict the same output, we return that output; else we return \"don't know\".", "labels": [], "entities": []}, {"text": "building reliable question answering systems and natural language interfaces.", "labels": [], "entities": [{"text": "question answering", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.8036410510540009}]}, {"text": "Our goal is to learn a semantic mapping from examples of utterancelogical form pairs ().", "labels": [], "entities": []}, {"text": "More generally, we assume the input x is a bag (multiset) of source atoms (e.g., words {area, of, Ohio}), and the output y is a bag of target atoms (e.g., predicates {area, OH}).", "labels": [], "entities": []}, {"text": "We consider learning mappings M that decompose according to the multiset sum: M (x) = s\u2208x M (s) (e.g., M ({Ohio}) = {OH}, M ({area,of,Ohio}) = {area,OH}).", "labels": [], "entities": []}, {"text": "The main challenge is that an individual training example (x, y) does not tell us which source atoms map to which target atoms.", "labels": [], "entities": []}, {"text": "1 How can a system be 100% sure about something if it has seen only a small number of possibly non-representative examples?", "labels": [], "entities": []}, {"text": "Our approach is based on what we call the unanimity principle (Section 2.1).", "labels": [], "entities": []}, {"text": "Let M be a model family that contains the true mapping from inputs to outputs.", "labels": [], "entities": []}, {"text": "Let C be the subset of mappings that are consistent A semantic parser further requires modeling the context dependence of words and the logical form structure joining the predicates.", "labels": [], "entities": []}, {"text": "Our framework handles these cases with a different choice of source and target atoms (see Section 4.2). with the training data.", "labels": [], "entities": []}, {"text": "If all mappings M \u2208 C unanimously predict the same output on a test input, then we return that output; else we return \"don't know\" (see).", "labels": [], "entities": []}, {"text": "The unanimity principle provides robustness to the particular input distribution, so that we can tolerate even adversaries (, provided the training outputs are still mostly correct.", "labels": [], "entities": []}, {"text": "To operationalize the unanimity principle, we need to be able to efficiently reason about the predictions of all consistent mappings C.", "labels": [], "entities": []}, {"text": "To this end, we represent a mapping as a matrix M , where M st is number of times target atom t (e.g., OH) shows up for each occurrence of the source atom s (e.g., Ohio) in the input.", "labels": [], "entities": []}, {"text": "We show that unanimous prediction can be performed by solving two integer linear programs.", "labels": [], "entities": [{"text": "unanimous prediction", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.912815272808075}]}, {"text": "With a linear programming relaxation (Section 3), we further show that checking unanimity over C can be done very efficiently without any optimization but rather by checking the predictions of just two random mappings, while still guaranteeing 100% precision with probability 1 (Section 3.2).", "labels": [], "entities": [{"text": "precision", "start_pos": 249, "end_pos": 258, "type": "METRIC", "confidence": 0.9983726143836975}]}, {"text": "We further relax the linear program to a linear system, which gives us a geometric view of the unanimity: We predict on anew input if it can be expressed as a \"linear combination\" of the training inputs.", "labels": [], "entities": []}, {"text": "As an example, suppose we are given training data consisting of (CI) cities in Iowa, (CO) cities in Ohio, and (AI) area of Iowa).", "labels": [], "entities": []}, {"text": "We can compute (AO) area of Ohio by analogy: (AO) = (CO) -(CI) + (AI).", "labels": [], "entities": [{"text": "Ohio", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.836869478225708}, {"text": "AO", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9583209753036499}, {"text": "CO) -(CI) + (AI)", "start_pos": 53, "end_pos": 69, "type": "METRIC", "confidence": 0.808098726802402}]}, {"text": "Other reasoning patterns fallout from more complex linear combinations.", "labels": [], "entities": []}, {"text": "We can handle noisy data (Section 3.4) by asking for unanimity over additional slack variables.", "labels": [], "entities": []}, {"text": "We also show how the linear algebraic formulation enables other extensions such as learning from denotations (Section 5.1), active learning (Section 5.2), and paraphrasing (Section 5.3).", "labels": [], "entities": []}, {"text": "We validate our methods in Section 4.", "labels": [], "entities": []}, {"text": "On artificial data generated from an adversarial distribution with noise, we show that unanimous prediction obtains 100% precision, whereas point estimates fail.", "labels": [], "entities": [{"text": "unanimous prediction", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.6736585199832916}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9994864463806152}]}, {"text": "On GeoQuery (, a standard semantic parsing dataset, where our model assumptions are violated, we still obtain 100% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.999136745929718}]}, {"text": "We were able to reach 70% recall on recovering predicates and 59% on full logical forms.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9996775388717651}]}, {"text": "source atoms target atoms {area, of, Iowa} {area, IA} {cities, in, Ohio} {city, OH} {cities, in, Iowa} {city, IA}: Given the training examples in the top table, there are exactly four mappings consistent with these training examples.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Two different choices of target atoms: (A) shows predicates and (B) shows predicates conjoined  with their argument position. (A) is sufficient for simply recovering the predicates, whereas (B) allows  for logical form reconstruction.", "labels": [], "entities": [{"text": "logical form reconstruction", "start_pos": 216, "end_pos": 243, "type": "TASK", "confidence": 0.62143142024676}]}]}