{"title": [], "abstractContent": [{"text": "We introduce the Treebank of Learner En-glish (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL).", "labels": [], "entities": []}, {"text": "The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus.", "labels": [], "entities": [{"text": "Cambridge First Certificate in English (FCE) corpus", "start_pos": 110, "end_pos": 161, "type": "DATASET", "confidence": 0.8756039076381259}]}, {"text": "The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence.", "labels": [], "entities": [{"text": "FCE", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.928059458732605}]}, {"text": "Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungram-matical English.", "labels": [], "entities": []}, {"text": "Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8366866111755371}, {"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.6951784789562225}, {"text": "TLE dataset", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.8197433948516846}, {"text": "parsing", "start_pos": 136, "end_pos": 143, "type": "TASK", "confidence": 0.9573354721069336}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8385818600654602}]}, {"text": "We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language 1 .", "labels": [], "entities": [{"text": "second language acquisition", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.6533035635948181}, {"text": "automatic processing of ungrammatical language 1", "start_pos": 132, "end_pos": 180, "type": "TASK", "confidence": 0.6365029960870743}]}], "introductionContent": [{"text": "The majority of the English text available worldwide is generated by non-native speakers.", "labels": [], "entities": []}, {"text": "Such texts introduce a variety of challenges, most notably grammatical errors, and are of paramount importance for the scientific study of language acquisition as well as for NLP.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 139, "end_pos": 159, "type": "TASK", "confidence": 0.7276720404624939}]}, {"text": "Despite the ubiquity of non-native English, there is currently no publicly available syntactic treebank for English as a Second Language (ESL).", "labels": [], "entities": []}, {"text": "To address this shortcoming, we present the Treebank of Learner English (TLE), a first of its kind resource for non-native English, containing 5,124 sentences manually annotated with POS tags and dependency trees.", "labels": [], "entities": []}, {"text": "The TLE sentences are drawn from the FCE dataset, and authored by English learners from 10 different native language backgrounds.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9886281192302704}]}, {"text": "The treebank uses the Universal Dependencies (UD) formalism, which provides a unified annotation framework across different languages and is geared towards multilingual NLP (.", "labels": [], "entities": []}, {"text": "This characteristic allows our treebank to support computational analysis of ESL using not only English based but also multilingual approaches which seek to relate ESL phenomena to native language syntax.", "labels": [], "entities": [{"text": "computational analysis of ESL", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.6364905759692192}]}, {"text": "While the annotation inventory and guidelines are defined by the English UD formalism, we build on previous work in learner language analysis ( to formulate an additional set of annotation conventions aiming at a uniform treatment of ungrammatical learner language.", "labels": [], "entities": [{"text": "English UD formalism", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.7993783553441366}, {"text": "learner language analysis", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.5994450847307841}]}, {"text": "Our annotation scheme uses a two-layer analysis, whereby a distinct syntactic annotation is provided for the original and the corrected version of each sentence.", "labels": [], "entities": []}, {"text": "This approach is enabled by a pre-existing error annotation of the FCE which is used to generate an error corrected variant of the dataset.", "labels": [], "entities": [{"text": "FCE", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8506649732589722}]}, {"text": "Our inter-annotator agreement results provide evidence for the ability of the annotation scheme to support consistent annotation of ungrammatical structures.", "labels": [], "entities": []}, {"text": "Finally, a corpus that is annotated with both grammatical errors and syntactic dependencies paves the way for empirical investigation of the relation between grammaticality and syntax.", "labels": [], "entities": []}, {"text": "Understanding this relation is vital for improving tagging and parsing performance on learner language, syntax based grammatical error correction, and many other fundamental challenges in NLP.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.5864808559417725}, {"text": "syntax based grammatical error correction", "start_pos": 104, "end_pos": 145, "type": "TASK", "confidence": 0.5660228133201599}]}, {"text": "In this work, we take the first step in this direction by benchmarking tagging and parsing accuracy on our dataset under different training regimes, and obtaining several estimates for the impact of grammatical errors on these tasks.", "labels": [], "entities": [{"text": "benchmarking tagging", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.6036919057369232}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.949805498123169}]}, {"text": "To summarize, this paper presents three contributions.", "labels": [], "entities": []}, {"text": "First, we introduce the first large scale syntactic treebank for ESL, manually annotated with POS tags and universal dependencies.", "labels": [], "entities": []}, {"text": "Second, we describe a linguistically motivated annotation scheme for ungrammatical learner English and provide empirical support for its consistency via inter-annotator agreement analysis.", "labels": [], "entities": []}, {"text": "Third, we benchmark a state of the art parser on our dataset and estimate the influence of grammatical errors on the accuracy of automatic POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.999129593372345}, {"text": "POS tagging", "start_pos": 139, "end_pos": 150, "type": "TASK", "confidence": 0.851040780544281}, {"text": "dependency parsing", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7212328910827637}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We start by presenting an overview of the treebank in section 2.", "labels": [], "entities": []}, {"text": "In sections 3 and 4 we provide background information on the annotation project, and review the main annotation stages leading to the current form of the dataset.", "labels": [], "entities": []}, {"text": "The ESL annotation guidelines are summarized in section 5.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement analysis is presented in section 6, followed by parsing experiments in section 7.", "labels": [], "entities": [{"text": "Inter-annotator agreement analysis", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6556588709354401}]}, {"text": "Finally, we review related work in section 8 and present the conclusion in section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "The TLE enables studying parsing for learner language and exploring relationships between grammatical errors and parsing performance.", "labels": [], "entities": [{"text": "TLE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5770062208175659}]}, {"text": "Here, we present parsing benchmarks on our dataset, and provide several estimates for the extent to which grammatical errors degrade the quality of automatic POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 158, "end_pos": 169, "type": "TASK", "confidence": 0.8322315812110901}, {"text": "dependency parsing", "start_pos": 174, "end_pos": 192, "type": "TASK", "confidence": 0.7430649101734161}]}, {"text": "Our first experiment measures tagging and parsing accuracy on the TLE and approximates the global impact of grammatical errors on automatic annotation via performance comparison between the original and error corrected sentence versions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.6526870727539062}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9180322885513306}]}, {"text": "In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser), state of the art tools for statistical POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "Turbo tagger", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.6366048455238342}, {"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.7970383763313293}, {"text": "dependency parsing", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.8548944294452667}]}, {"text": "presents tagging and parsing results on a test set of 500 TLE sentences (9,591 original tokens, 9,700 corrected tokens).", "labels": [], "entities": []}, {"text": "Results are provided for three different training regimes.", "labels": [], "entities": []}, {"text": "The first regime uses the training portion of version 1.3 of the EWT, the UD English treebank, containing 12,543 sentences (204,586 tokens).", "labels": [], "entities": [{"text": "EWT", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9750726819038391}, {"text": "UD English treebank", "start_pos": 74, "end_pos": 93, "type": "DATASET", "confidence": 0.9414777358373007}]}, {"text": "The second training mode uses 4,124 training sentences (78,541 original tokens, 79,581 corrected tokens) from the TLE corpus.", "labels": [], "entities": [{"text": "TLE corpus", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.8057182133197784}]}, {"text": "In the third setup we combine these two training corpora.", "labels": [], "entities": []}, {"text": "The remaining 500 TLE sentences (9,549 original tokens, 9,695 corrected tokens) are allocated to a development set, not used in this experiment.", "labels": [], "entities": []}, {"text": "Parsing of the test sentences was performed on predicted POS tags.", "labels": [], "entities": []}, {"text": "The EWT training regime, which uses out of domain texts written in standard English, provides the lowest performance on all the evaluation met-: Tagging and parsing results on a test set of 500 sentences from the TLE corpus.", "labels": [], "entities": [{"text": "EWT training regime", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8185532093048096}, {"text": "Tagging", "start_pos": 145, "end_pos": 152, "type": "TASK", "confidence": 0.9778433442115784}, {"text": "TLE corpus", "start_pos": 213, "end_pos": 223, "type": "DATASET", "confidence": 0.8367894887924194}]}, {"text": "EWT is the English UD treebank.", "labels": [], "entities": [{"text": "EWT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9226023554801941}, {"text": "English UD treebank", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.8714871207873026}]}, {"text": "TLE orig are original sentences from the TLE.", "labels": [], "entities": []}, {"text": "TLE corr are the corresponding error corrected sentences. rics.", "labels": [], "entities": [{"text": "TLE corr", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9364680349826813}]}, {"text": "An additional factor which negatively affects performance in this regime are systematic differences in the EWT annotation of possessive pronouns, expletives and names compared to the UD guidelines, which are utilized in the TLE.", "labels": [], "entities": [{"text": "EWT", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.8155236840248108}]}, {"text": "In particular, the EWT annotates possessive pronoun UPOS as PRON rather than DET, which leads the UPOS results in this setup to be lower than the PTB POS results.", "labels": [], "entities": [{"text": "EWT", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.745201051235199}, {"text": "PRON", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.8283007740974426}, {"text": "DET", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9875378012657166}]}, {"text": "Improved results are obtained using the TLE training data, which, despite its smaller size, is closer in genre and syntactic characteristics to the TLE test set.", "labels": [], "entities": [{"text": "TLE training data", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.7237869699796041}, {"text": "TLE test set", "start_pos": 148, "end_pos": 160, "type": "DATASET", "confidence": 0.8889729976654053}]}, {"text": "The strongest PTB POS tagging and parsing results are obtained by combining the EWT with the TLE training data, yielding 95.77 POS accuracy and a UAS of 90.3 on the original version of the TLE test set.", "labels": [], "entities": [{"text": "PTB POS tagging", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.693514088789622}, {"text": "EWT", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.7134888768196106}, {"text": "TLE training data", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.6642107963562012}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.8652130961418152}, {"text": "UAS", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9994372725486755}, {"text": "TLE test set", "start_pos": 189, "end_pos": 201, "type": "DATASET", "confidence": 0.797137439250946}]}, {"text": "The dual annotation of sentences in their original and error corrected forms enables estimating the impact of grammatical errors on tagging and parsing by examining the performance gaps between the two sentence versions.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.5895211497942606}]}, {"text": "Averaged across the three training conditions, the POS tagging accuracy on the original sentences is lower than the accuracy on the sentence corrections by 1.0 UPOS and 0.61 POS.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.6978220045566559}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.5546062588691711}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9983649849891663}, {"text": "UPOS", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9807928204536438}]}, {"text": "Parsing performance degrades by 1.9 UAS, 1.59 LA and 2.21 LAS.", "labels": [], "entities": [{"text": "UAS", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9908084273338318}, {"text": "LA", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9960236549377441}, {"text": "LAS", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9921270608901978}]}, {"text": "To further elucidate the influence of grammatical errors on parsing quality, table 4 compares performance on tokens in the original sentences appearing inside grammatical error tags to those appearing outside such tags.", "labels": [], "entities": []}, {"text": "Although grammatical errors may lead to tagging and parsing errors with respect to any element in the sentence, we expect erroneous tokens to be more challenging to analyze compared to grammatical tokens.", "labels": [], "entities": []}, {"text": "This comparison indeed reveals a substantial difference between the two types of tokens, with an average gap of: Tagging and parsing results on the original version of the TLE test set for tokens marked with grammatical errors (Ungrammatical) and tokens not marked for errors (Grammatical).", "labels": [], "entities": [{"text": "Tagging", "start_pos": 113, "end_pos": 120, "type": "TASK", "confidence": 0.9790610671043396}, {"text": "TLE test set", "start_pos": 172, "end_pos": 184, "type": "DATASET", "confidence": 0.7875769237677256}]}, {"text": "the global measurements in the first experiment, this analysis, which focuses on the local impact of remove/replace errors, suggests a stronger effect of grammatical errors on the dependency labels than on the dependency structure.", "labels": [], "entities": []}, {"text": "Finally, we measure tagging and parsing performance relative to the fraction of sentence tokens marked with grammatical errors.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.8804901838302612}]}, {"text": "Similarly to the previous experiment, this analysis focuses on remove/replace rather than insert errors.", "labels": [], "entities": []}, {"text": "Points connected by continuous lines denote performance on the original TLE sentences.", "labels": [], "entities": []}, {"text": "Points connected by dashed lines denote performance on the corresponding error corrected sentences.", "labels": [], "entities": []}, {"text": "The number of sentences whose errors fall within each percentage range appears in parenthesis.", "labels": [], "entities": []}, {"text": "presents the average sentential performance as a function of the percentage of tokens in the original sentence marked with grammati-cal errors.", "labels": [], "entities": []}, {"text": "In this experiment, we train the parser on the EWT training set and test on the entire TLE corpus.", "labels": [], "entities": [{"text": "EWT training set", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9451172351837158}, {"text": "TLE corpus", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.7318070083856583}]}, {"text": "Performance curves are presented for POS, UAS and LAS on the original and error corrected versions of the annotations.", "labels": [], "entities": [{"text": "POS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.6479164361953735}, {"text": "UAS", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9062490463256836}, {"text": "LAS", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9854368567466736}, {"text": "error", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9623861312866211}]}, {"text": "We observe that while the performance on the corrected sentences is close to constant, original sentence performance is decreasing as the percentage of the erroneous tokens in the sentence grows.", "labels": [], "entities": []}, {"text": "Overall, our results suggest a negative, albeit limited effect of grammatical errors on parsing.", "labels": [], "entities": []}, {"text": "This outcome contrasts a study by which reported a larger performance gap of 7.6 UAS and 8.8 LAS between sentences with and without grammatical errors.", "labels": [], "entities": [{"text": "UAS", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9439783692359924}, {"text": "LAS", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9710021615028381}]}, {"text": "We believe that our analysis provides a more accurate estimate of this impact, as it controls for both sentence content and sentence length.", "labels": [], "entities": []}, {"text": "The latter factor is crucial, since it correlates positively with the number of grammatical errors in the sentence, and negatively with parsing accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9366089701652527}]}], "tableCaptions": [{"text": " Table 1: Statistics of the TLE. Standard deviations  are denoted in parenthesis.", "labels": [], "entities": [{"text": "TLE", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.5723676085472107}]}, {"text": " Table 2: Inter-annotator agreement on the entire  TLE corpus. Agreement is measured as the frac- tion of tokens that remain unchanged after an edit- ing round. The four evaluation columns corre- spond to universal POS tags, PTB POS tags, un- labeled attachment, and dependency labels. Co- hen's Kappa scores", "labels": [], "entities": [{"text": "TLE corpus", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.7908965051174164}, {"text": "Agreement", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9773385524749756}, {"text": "Co- hen's Kappa scores", "start_pos": 286, "end_pos": 308, "type": "METRIC", "confidence": 0.6379048675298691}]}, {"text": " Table 3: Tagging and parsing results on a test set of  500 sentences from the TLE corpus. EWT is the  English UD treebank. TLE orig are original sen- tences from the TLE. TLE corr are the correspond- ing error corrected sentences.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9705918431282043}, {"text": "TLE corpus", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.7708270251750946}, {"text": "EWT", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.7877098917961121}, {"text": "English UD treebank", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.81440802415212}]}, {"text": " Table 4: Tagging and parsing results on the origi- nal version of the TLE test set for tokens marked  with grammatical errors (Ungrammatical) and to- kens not marked for errors (Grammatical).", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9811722636222839}, {"text": "TLE test set", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.7964719533920288}]}]}