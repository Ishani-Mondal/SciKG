{"title": [{"text": "Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning", "labels": [], "entities": [{"text": "Improving Named Entity Recognition", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8669333457946777}, {"text": "Word Segmentation Representation Learning", "start_pos": 65, "end_pos": 106, "type": "TASK", "confidence": 0.7960638776421547}]}], "abstractContent": [{"text": "Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6854336063067118}, {"text": "information extraction", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.740861177444458}]}, {"text": "For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7091722041368484}]}, {"text": "While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system.", "labels": [], "entities": [{"text": "NER", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9609131813049316}]}, {"text": "New state-of-the-art word seg-mentation systems use neural models to learn representations for predicting word boundaries.", "labels": [], "entities": [{"text": "predicting word boundaries", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.8213263352711996}]}, {"text": "We show that these same representations , jointly trained with an NER system, yield significant improvements in NER for Chinese social media.", "labels": [], "entities": [{"text": "NER", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.914841890335083}]}, {"text": "In our experiments , jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results.", "labels": [], "entities": [{"text": "NER", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9755748510360718}, {"text": "word segmentation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7659159600734711}]}], "introductionContent": [{"text": "Entity mention detection, and more specifically named entity recognition (NER), has become a popular task for social media analysis (;).", "labels": [], "entities": [{"text": "Entity mention detection", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8033654888470968}, {"text": "named entity recognition (NER)", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.7712593227624893}, {"text": "social media analysis", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.698583741982778}]}, {"text": "Many downstream applications that use social media, such as relation extraction () and entity linking (), rely on first identifying mentions of entities.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.8131517767906189}, {"text": "entity linking", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7542309761047363}]}, {"text": "Not surprisingly, accuracy of NER systems in social media trails state-of-the-art systems for news text and other formal domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9986043572425842}]}, {"text": "While this gap is shrinking in English (, it remains large in other languages, such as Chinese (.", "labels": [], "entities": []}, {"text": "One reason for this gap is the lack of robust up-stream NLP systems that provide useful features for NER, such as part-of-speech tagging or chunking.", "labels": [], "entities": [{"text": "NER", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.921919047832489}, {"text": "part-of-speech tagging", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.728952944278717}]}, {"text": "annotated Twitter data for these systems to improve a Twitter NER tagger, however, these systems do not exist for social media inmost languages.", "labels": [], "entities": []}, {"text": "Another approach has been that of and, who relied on training unsupervised lexical embeddings in place of these upstream systems and achieved state-of-the-art results for English and Chinese social media, respectively.", "labels": [], "entities": []}, {"text": "The same approach was also found helpful for NER in the news domain In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step for many tasks (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7350846230983734}]}, {"text": "showed the value of word segmentation to Chinese NER in social media by using character positional embeddings, which encoded word segmentation information.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7232845723628998}, {"text": "word segmentation information", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.7645948032538096}]}, {"text": "In this paper, we investigate better ways to incorporate word boundary information into an NER system for Chinese social media.", "labels": [], "entities": []}, {"text": "We combine the state-of-the-art Chinese word segmentation system () with the best Chinese social media NER model (.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6892535239458084}]}, {"text": "Since both systems used learned representations, we propose an integrated model that allows for joint training learned representations, providing more information to the NER system about hidden representations learned from word segmentation, as compared to features based on segmentation output.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 223, "end_pos": 240, "type": "TASK", "confidence": 0.6597498506307602}]}, {"text": "Our integrated model achieves nearly Input For CWS C (1) \u2026\u2026 C (n-1) C (n): The joint model for Chinese word segmentation and NER.", "labels": [], "entities": [{"text": "Input", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9524672627449036}, {"text": "Chinese word segmentation", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.5871520837148031}, {"text": "NER", "start_pos": 125, "end_pos": 128, "type": "TASK", "confidence": 0.9056469202041626}]}, {"text": "The left hand side is an LSTM module for word segmentation, and the right hand side is a traditional feature-based CRF model for NER.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.778215080499649}]}, {"text": "Note that the linear chain CRF for NER has both access to the feature extractor specifically for NER and the representations produced by the LSTM module for word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7468113005161285}]}, {"text": "The CRF in this version is a log-bilinear CRF, where it treats the embeddings and hidden vectors inputs as variables and modifies them according to the objective function.", "labels": [], "entities": []}, {"text": "As a result, it enables propagating the gradients back into the LSTM to adjust the parameters.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7183889746665955}]}, {"text": "Therefore, the word segmentation and NER training share all the parameters of the LSTM module.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7894355952739716}, {"text": "NER training", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.8841530382633209}]}, {"text": "This facilitates the joint training.", "labels": [], "entities": []}, {"text": "a 5% absolute improvement over the previous best results on both NER and nominal mentions for Chinese social media.", "labels": [], "entities": [{"text": "NER", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.5250305533409119}]}], "datasetContent": [{"text": "We use the same training, development and test splits as for word segmentation and for NER.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7586193382740021}, {"text": "NER", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8054579496383667}]}, {"text": "Word Segmentation The segmentation data is taken from the SIGHAN 2005 shared task.", "labels": [], "entities": [{"text": "Word Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7077915966510773}, {"text": "SIGHAN 2005 shared task", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.8046073615550995}]}, {"text": "We used the PKU portion, which includes 43,963 word sentences as training and 4,278 sentences as test.", "labels": [], "entities": [{"text": "PKU portion", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8775568902492523}]}, {"text": "We did not apply any special preprocessing.", "labels": [], "entities": []}, {"text": "NER This dataset contains 1,890 Sina Weibo messages annotated with four entity types (person, organization, location and geo-political entity), including named and nominal mentions.", "labels": [], "entities": [{"text": "NER This dataset contains 1,890 Sina Weibo messages", "start_pos": 0, "end_pos": 51, "type": "DATASET", "confidence": 0.873463362455368}]}, {"text": "We note that the word segmentation dataset is significantly larger than the NER data, which motivates our subsampling during training ( \u00a73).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7094607502222061}, {"text": "NER data", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.8478778898715973}]}, {"text": "shows results for NER in terms of precision, recall and F1 for named (left) and nominal (right) mentions on both dev and test sets.", "labels": [], "entities": [{"text": "NER", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.741636335849762}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9995542168617249}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9995554089546204}, {"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9997482895851135}]}, {"text": "The hyper-parameters are tuned on dev data and then applied on test.", "labels": [], "entities": []}, {"text": "We now explain the results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NER results for named and nominal mentions on dev and test data.", "labels": [], "entities": [{"text": "NER", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6550695896148682}]}]}