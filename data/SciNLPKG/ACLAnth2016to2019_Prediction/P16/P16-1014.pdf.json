{"title": [], "abstractContent": [{"text": "The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems , including traditional count-based and deep learning models.", "labels": [], "entities": []}, {"text": "We propose a novel way to deal with the rare and unseen words for the neural network models using attention.", "labels": [], "entities": []}, {"text": "Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary.", "labels": [], "entities": []}, {"text": "At each timestep, the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context.", "labels": [], "entities": []}, {"text": "We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known.", "labels": [], "entities": []}, {"text": "Using our proposed model, we observe improvements on two tasks, neural machine translation on the Europarl En-glish to French parallel corpora and text summarization on the Gigaword dataset.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.622243344783783}, {"text": "Europarl En-glish to French parallel corpora", "start_pos": 98, "end_pos": 142, "type": "DATASET", "confidence": 0.9125641783078512}, {"text": "text summarization", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7305676639080048}, {"text": "Gigaword dataset", "start_pos": 173, "end_pos": 189, "type": "DATASET", "confidence": 0.9735476672649384}]}], "introductionContent": [{"text": "Words are the basic input/output units inmost of the NLP systems, and thus the ability to cover a large number of words is a key to building a robust NLP system.", "labels": [], "entities": []}, {"text": "However, considering that (i) the number of all words in a language including named entities is very large and that (ii) language itself is an evolving system (people create new words), this can be a challenging problem.", "labels": [], "entities": []}, {"text": "A common approach followed by the recent neural network based NLP systems is to use a softmax output layer where each of the output dimension corresponds to a word in a predefined word-shortlist.", "labels": [], "entities": []}, {"text": "Because computing high dimensional softmax is computationally expensive, in practice the shortlist is limited to have only top-K most frequent words in the training corpus.", "labels": [], "entities": []}, {"text": "All other words are then replaced by a special word, called the unknown word (UNK).", "labels": [], "entities": []}, {"text": "The shortlist approach has two fundamental problems.", "labels": [], "entities": []}, {"text": "The first problem, which is known as the rare word problem, is that some of the words in the shortlist occur less frequently in the training set and thus are difficult to learn a good representation, resulting in poor performance.", "labels": [], "entities": []}, {"text": "Second, it is obvious that we can lose some important information by mapping different words to a single dummy token UNK.", "labels": [], "entities": []}, {"text": "Even if we have a very large shortlist including all unique words in the training set, it does not necessarily improve the test performance, because there still exists a chance to see an unknown word attest time.", "labels": [], "entities": []}, {"text": "This is known as the unknown word problem.", "labels": [], "entities": []}, {"text": "In addition, increasing the shortlist size mostly leads to increasing rare words due to Zipf's Law.", "labels": [], "entities": []}, {"text": "These two problems are particularly critical in language understanding tasks such as factoid question answering () where the words that we are interested in are often named entities which are usually unknown or rare words.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7613952457904816}, {"text": "factoid question answering", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.8438254594802856}]}, {"text": "Ina similar situation, where we have a limited information on how to call an object of interest, it seems that humans (and also some primates) have an efficient behavioral mechanism of drawing attention to the object: pointing (Matthews et al., 2012).", "labels": [], "entities": []}, {"text": "Pointing makes it possible to deliver information and to associate context to a particular object without knowing how to call it.", "labels": [], "entities": []}, {"text": "In particular, human infants use pointing as a fundamental communication tool.", "labels": [], "entities": []}, {"text": "In this paper, inspired by the pointing behavior of humans and recent advances in the atten-tion mechanism () and the pointer networks ( , we propose a novel method to deal with the rare or unknown word problem.", "labels": [], "entities": []}, {"text": "The basic idea is that we can see many NLP problems as a task of predicting target text given context text, where some of the target words appear in the context as well.", "labels": [], "entities": []}, {"text": "We observe that in this case we can make the model learn to point a word in the context and copy it to the target text, as well as when to point.", "labels": [], "entities": []}, {"text": "For example, in machine translation, we can seethe source sentence as the context, and the target sentence as what we need to predict.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7335751950740814}]}, {"text": "In, we show an example depiction of how words can be copied from source to target in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7321799695491791}]}, {"text": "Although the source and target languages are different, many of the words such as named entities are usually represented by the same characters in both languages, making it possible to copy.", "labels": [], "entities": []}, {"text": "Similarly, in text summarization, it is natural to use some words in the original text in the summarized text as well.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.5949280560016632}]}, {"text": "Specifically, to predict a target word at each timestep, our model first determines the source of the word generation, that is, whether to take one from a predefined shortlist or to copy one from the context.", "labels": [], "entities": []}, {"text": "For the former, we apply the typical softmax operation, and for the latter, we use the attention mechanism to obtain the pointing softmax probability over the context words and pick the one of high probability.", "labels": [], "entities": []}, {"text": "The model learns this decision so as to use the pointing only when the context includes a word that can be copied to the target.", "labels": [], "entities": []}, {"text": "This way, our model can predict even the words which are not in the shortlist, as long as it appears in the context.", "labels": [], "entities": []}, {"text": "Although some of the words still need to be labeled as UNK, i.e., if it is neither in the shortlist nor in the context, in experiments we show that this learning when and whereto point improves the performance in machine translation and text summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.788929283618927}, {"text": "text summarization", "start_pos": 237, "end_pos": 255, "type": "TASK", "confidence": 0.7189099490642548}]}, {"text": "Guillaume and Cesar have a blue car in Lausanne.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide our main experimental results with the pointer softmax on machine translation and summarization tasks.", "labels": [], "entities": [{"text": "machine translation and summarization tasks", "start_pos": 86, "end_pos": 129, "type": "TASK", "confidence": 0.7580552697181702}]}, {"text": "In our experiments, we have used the same baseline model and just replaced the softmax layer with pointer softmax layer at the language model.", "labels": [], "entities": []}, {"text": "We use the Adadelta (Zeiler, 2012) learning rule for the training of NMT models.", "labels": [], "entities": [{"text": "Adadelta (Zeiler, 2012) learning rule", "start_pos": 11, "end_pos": 48, "type": "DATASET", "confidence": 0.6516444720327854}]}, {"text": "The code for pointer softmax model is available at https://github.com/ caglar/pointer_softmax.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Gigaword Corpus when point- ers are used for UNKs in the training data, using  Rouge-F1 as the evaluation metric.", "labels": [], "entities": [{"text": "Gigaword Corpus", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.6641654968261719}, {"text": "UNKs", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.8138981461524963}]}, {"text": " Table 2: Results on anonymized Gigaword Corpus  when pointers are used for entities, using Rouge- F1 as the evaluation metric.  Rouge-1 Rouge-2 Rouge-L  NMT + lvt  34.89  16.78  32.37  NMT + lvt + PS 35.11  16.76  32.55", "labels": [], "entities": []}, {"text": " Table 3: Results on Gigaword Corpus for model- ing UNK's with pointers in terms of recall.", "labels": [], "entities": [{"text": "Gigaword Corpus", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.501094326376915}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.998017430305481}]}]}