{"title": [{"text": "Strategies for Training Large Vocabulary Neural Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Training neural network language models overlarge vocabularies is computa-tionally costly compared to count-based models such as Kneser-Ney.", "labels": [], "entities": []}, {"text": "We present a systematic comparison of neural strategies to represent and train large vocabularies , including softmax, hierarchical soft-max, target sampling, noise contrastive estimation and self normalization.", "labels": [], "entities": [{"text": "noise contrastive estimation", "start_pos": 159, "end_pos": 187, "type": "TASK", "confidence": 0.7186658481756846}]}, {"text": "We extend self normalization to be a proper esti-mator of likelihood and introduce an efficient variant of softmax.", "labels": [], "entities": []}, {"text": "We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complemen-tarity to Kneser-Ney.", "labels": [], "entities": [{"text": "speed/accuracy trade-off", "start_pos": 94, "end_pos": 118, "type": "METRIC", "confidence": 0.8145038336515427}, {"text": "complemen-tarity", "start_pos": 123, "end_pos": 139, "type": "METRIC", "confidence": 0.9660449624061584}]}], "introductionContent": [{"text": "Neural network language models () have gained popularity for tasks such as automatic speech recognition () and statistical machine translation ().", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.656993160645167}, {"text": "statistical machine translation", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.7541459401448568}]}, {"text": "Similar models are also developed for translation (, summarization ( and language generation (.", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9887911081314087}, {"text": "summarization", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9883667826652527}, {"text": "language generation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.723713681101799}]}, {"text": "Language models assign a probability to a word given a context of preceding, and possibly subsequent, words.", "labels": [], "entities": []}, {"text": "The model architecture determines how the context is represented and there are several choices including recurrent neural networks (, or log-bilinear models.", "labels": [], "entities": []}, {"text": "This paper does not focus on architecture or context representation but rather on how to efficiently deal with large output vocabularies, a problem common to all approaches to neural language modeling and related tasks (machine translation, language generation).", "labels": [], "entities": [{"text": "context representation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7166898250579834}, {"text": "machine translation", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.717866986989975}, {"text": "language generation", "start_pos": 241, "end_pos": 260, "type": "TASK", "confidence": 0.7000410407781601}]}, {"text": "We therefore experiment with a classical feed-forward neural network model similar to.", "labels": [], "entities": []}, {"text": "Practical training speed for these models quickly decreases as the vocabulary grows.", "labels": [], "entities": []}, {"text": "This is due to three combined factors: (i) model evaluation and gradient computation become more time consuming, mainly due to the need of computing normalized probabilities over a large vocabulary; (ii) large vocabularies require more training data in order to observe enough instances of infrequent words which increases training times; (iii) a larger training set often allows for larger models which requires more training iterations.", "labels": [], "entities": [{"text": "model evaluation", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7846718728542328}]}, {"text": "This paper provides an overview of popular strategies to model large vocabularies for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7264629304409027}]}, {"text": "This includes the classical softmax overall output classes, hierarchical softmax which introduces latent variables, or clusters, to simplify normalization, target sampling which only considers a random subset of classes for normalization, noise contrastive estimation which discriminates between genuine data points and samples from a noise distribution, and infrequent normalization, also referred as self-normalization, which computes the partition function at an infrequent rate.", "labels": [], "entities": [{"text": "noise contrastive estimation", "start_pos": 239, "end_pos": 267, "type": "TASK", "confidence": 0.7223387857278188}]}, {"text": "We also extend self-normalization to be a proper estimator of likelihood.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce differentiated softmax, a novel variation of softmax which assigns more parameters, or capacity, to frequent words and which we show to be faster and more accurate than softmax ( \u00a72).", "labels": [], "entities": []}, {"text": "Our comparison assumes a reasonable budget of one week for training models on a high end GPU (Nvidia K40).", "labels": [], "entities": [{"text": "Nvidia K40)", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9543500741322836}]}, {"text": "We evaluate on three benchmarks differing in the amount of training data and vocabulary size, that is Penn Treebank, Gigaword and the Billion Word benchmark ( \u00a73).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9910764694213867}]}, {"text": "Our results show that conclusions drawn from small datasets do not always generalize to larger settings.", "labels": [], "entities": []}, {"text": "For instance, hierarchical softmax is less accurate than softmax on the small vocabulary Penn Treebank task but performs best on the very large vocabulary Billion Word benchmark.", "labels": [], "entities": [{"text": "Penn Treebank task", "start_pos": 89, "end_pos": 107, "type": "DATASET", "confidence": 0.8398556510607401}]}, {"text": "This is because hierarchical softmax is the fastest method for training and can perform more training updates in the same period of time.", "labels": [], "entities": []}, {"text": "Furthermore, our re-sults with differentiated softmax demonstrate that assigning capacity where it has the most impact allows to train better models in our time budget ( \u00a74).", "labels": [], "entities": []}, {"text": "Our analysis also shows clearly that traditional Kneser-Ney models are competitive on rare words, contrary to the common belief that neural models are better on infrequent words ( \u00a75).", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets We run experiments over three news datasets of different sizes: Penn Treebank (PTB), WMT11-lm (billionW) and English Gigaword, version 5 (gigaword is a trade-off between being able to do a comprehensive exploration of the various settings for each method and good accuracy.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.9688526153564453}, {"text": "WMT11-lm", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.8190300464630127}, {"text": "English Gigaword", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.8175001740455627}, {"text": "accuracy", "start_pos": 273, "end_pos": 281, "type": "METRIC", "confidence": 0.9980777502059937}]}, {"text": "The chosen training times are not long enough to observe over-fitting, i.e. validation performance is still improving -albeit very slowly -at the end of the training session.", "labels": [], "entities": []}, {"text": "As a general observation, even on the small PTB where 24 hours is rather long, we always found better results using the full training time, possibly increasing the dropout rate.", "labels": [], "entities": [{"text": "PTB", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9719238877296448}]}, {"text": "A concern maybe that a fixing the training time favors models with better implementations.", "labels": [], "entities": []}, {"text": "However, all models are very similar and their core computations are always matrix/matrix products.", "labels": [], "entities": []}, {"text": "Training differs mostly in the size and frequency of large matrix/matrix products.", "labels": [], "entities": []}, {"text": "Matrix products rely on CuBLAS 3 , using torch 4 . For the matrix sizes involved (> 500\u00d71, 000), the time complexity of matrix product is linear in each dimension, both on CPU (Intel MKL 5 ) and GPU (CuBLAS), with a 10X speedup for GPU (Nvidia K40) compared to CPU (Intel Xeon E5-2680).", "labels": [], "entities": []}, {"text": "Therefore, the speed trade-off applies to both CPU and GPU hardware, albeit with a different timescale.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Test perplexity of individual models and  interpolation with Kneser-Ney.", "labels": [], "entities": []}, {"text": " Table 3: Training and test speed on billionW in to- kens per second for generation of the next word.  Most techniques are identical to softmax at test  time. HSM can be faster for rescoring.", "labels": [], "entities": []}, {"text": " Table 5: Test entropy on gigaword over subsets of the frequency ranked vocabulary; rank 1 is the most  frequent word.", "labels": [], "entities": []}]}