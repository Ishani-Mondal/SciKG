{"title": [{"text": "Improved Representation Learning for Question Answer Matching", "labels": [], "entities": [{"text": "Improved Representation Learning", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9167670408884684}, {"text": "Question Answer Matching", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8686550458272299}]}], "abstractContent": [{"text": "Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers.", "labels": [], "entities": [{"text": "Passage-level question answer matching", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7149749398231506}]}, {"text": "In this work, we propose a series of deep learning models to address passage answer selection.", "labels": [], "entities": [{"text": "passage answer selection", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.8324641982714335}]}, {"text": "To match passage answers to questions accommodating their complex semantic relations, unlike most previous work that utilizes a single deep learning structure, we develop hybrid models that process the text using both convolutional and recurrent neu-ral networks, combining the merits on extracting linguistic information from both structures.", "labels": [], "entities": []}, {"text": "Additionally, we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question, which is imperative for better modeling long answer sequences.", "labels": [], "entities": []}, {"text": "The results on two public benchmark datasets, InsuranceQA and TREC-QA, show that our proposed models outperform a variety of strong baselines.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.821646511554718}, {"text": "TREC-QA", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9522867202758789}]}], "introductionContent": [{"text": "Passage-level answer selection is one of the essential components in typical question answering (QA) systems.", "labels": [], "entities": [{"text": "Passage-level answer selection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7988855044047037}, {"text": "question answering (QA)", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.8444256067276001}]}, {"text": "It can be defined as follows: Given a question and a pool of candidate passages, select the passages that contain the correct answer.", "labels": [], "entities": []}, {"text": "The performance of the passage selection task is not only crucial to non-factoid QA systems, where a question is expected to be answered with a sequence of descriptive text (e.g. the question in Table 1), but also very important to factoid QA systems, where the answer passage selection step is Question: Does Medicare cover my spouse?", "labels": [], "entities": [{"text": "passage selection task", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9129018783569336}, {"text": "answer passage selection", "start_pos": 262, "end_pos": 286, "type": "TASK", "confidence": 0.6394311388333639}]}, {"text": "Ground-truth answer: If your spouse has worked and paid Medicare taxes for the entire required 40 quarters, or is eligible for Medicare by virtue of being disabled or some other reason, your spouse can receive his/her own medicare benefits.", "labels": [], "entities": []}, {"text": "If your spouse has not met those qualifications, if you have met them, and if your spouse is age 65, he/she can receive Medicare based on your eligibility.", "labels": [], "entities": [{"text": "Medicare", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.5325499176979065}]}, {"text": "Another candidate answer: If you were married to a Medicare eligible spouse for at least 10 years, you may qualify for Medicare.", "labels": [], "entities": [{"text": "Medicare", "start_pos": 119, "end_pos": 127, "type": "DATASET", "confidence": 0.716865599155426}]}, {"text": "If you are widowed, and have not remarried, and you were married to your spouse at least 9 months before your spouse's death, you maybe eligible for Medicare benefits under a spouse provision.: An example of a question with the ground-truth answer and a negative answer extracted from the InsuranceQA dataset.", "labels": [], "entities": [{"text": "InsuranceQA dataset", "start_pos": 289, "end_pos": 308, "type": "DATASET", "confidence": 0.9928784072399139}]}, {"text": "also known as passage scoring.", "labels": [], "entities": [{"text": "passage scoring", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.98052978515625}]}, {"text": "In factoid QA, if the sentences selected by the passage scorer module do not contain the answer, it will definitely lead to an incorrect response from the QA system.", "labels": [], "entities": [{"text": "factoid QA", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.6230193674564362}]}, {"text": "One central challenge of this task lies in the complex and versatile semantic relations observed between questions and passage answers.", "labels": [], "entities": []}, {"text": "For example, while the task of supporting passage selection for factoid QA maybe largely cast as a textual entailment problem, what makes an answer better than another in the real world for non-factoid QA often depends on many factors.", "labels": [], "entities": [{"text": "passage selection", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8249224126338959}, {"text": "factoid QA", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.5793651938438416}]}, {"text": "Specifically, different from many other pairmatching NLP tasks, the linguistic similarities between questions and answers mayor may not be indicative for our task.", "labels": [], "entities": []}, {"text": "This is because, depending on what the question is looking for, a good answer may come in different forms: sometimes a correct answer completes the question precisely with the missing information, and in other scenarios, good answers need to elaborate part of the question to rationalize it, and soon.", "labels": [], "entities": []}, {"text": "For instance, the question in only contains five words, while the best answer uses 60 words for elaboration.", "labels": [], "entities": []}, {"text": "On the other hand, the best answers from a pool can also be noisy and include extraneous information irrelevant to the question.", "labels": [], "entities": []}, {"text": "Additionally, while a good answer must relate to the question, they often do not share common lexical units.", "labels": [], "entities": []}, {"text": "For instance, in the example question, \"cover\" is not directly mentioned in the answer.", "labels": [], "entities": [{"text": "cover", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.916521430015564}]}, {"text": "This issue may confuse simple word-matching systems.", "labels": [], "entities": []}, {"text": "These challenges consequently make handcrafting features much less desirable compared to deep learning based methods.", "labels": [], "entities": []}, {"text": "Furthermore, they also require our systems to learn how to distinguish useful pieces from irrelevant ones, and further, to focus more on the former.", "labels": [], "entities": []}, {"text": "Finally, the system should be capable of capturing the nuances between the best answer and an acceptable one.", "labels": [], "entities": []}, {"text": "For example, the second answer in is suitable fora questioner, whose spouse is Medicare eligible, asking about his/her own coverage, while the example question is more likely asked by a person, who is Medicare eligible, asking about his/her spouse' coverage.", "labels": [], "entities": []}, {"text": "Clearly, the first answer is more appropriate for the question, although the second one implicitly answers it.", "labels": [], "entities": []}, {"text": "A good system should reflect this preference.", "labels": [], "entities": []}, {"text": "While this task is usually approached as a pairwise-ranking problem, the best strategy to capture the association between the questions and answers is still an open problem.", "labels": [], "entities": []}, {"text": "Established approaches normally suffer from two weaknesses at this point.", "labels": [], "entities": []}, {"text": "First, prior work, such as, resort to either convolutional neural network (CNN) or recurrent neural network (RNN) respectively.", "labels": [], "entities": []}, {"text": "However, each structure describes only one semantic perspective of the text.", "labels": [], "entities": []}, {"text": "CNN emphasizes the local interaction within n-gram, while RNN is designed to capture long range information and forget unimportant local information.", "labels": [], "entities": [{"text": "CNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8003137707710266}]}, {"text": "How to combine the merits from both has not been sufficiently explored.", "labels": [], "entities": []}, {"text": "Secondly, previous approaches are usually based on independently generated question and answer embeddings; the quality of such representations, however, usually degrades as the answer sequences grow longer.", "labels": [], "entities": []}, {"text": "In this work, we propose a series of deep learning models in order to address such weaknesses.", "labels": [], "entities": []}, {"text": "We start with the basic discriminative framework for answer selection.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.9386849701404572}]}, {"text": "We first propose two independent models, Convolutional-pooling LSTM and Convolution-based LSTM, which are designed to benefit from both of the two popular deep learning structures to distinguish better between useful and irrelevant pieces presented in questions and answers.", "labels": [], "entities": []}, {"text": "Next, by breaking the independence assumption of the question and answer embedding, we introduce an effective attention mechanism to generate answer representations according to the question, such that the embeddings do not overlook informative parts of the answers.", "labels": [], "entities": []}, {"text": "We report experimental results for two answer selection datasets: (1) InsuranceQA 1 , a recently released large-scale nonfactoid QA dataset from the insurance domain, and (2) TREC-QA 2 , which was created by based on Text REtrieval Conference (TREC) QA track data.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC) QA track data", "start_pos": 217, "end_pos": 263, "type": "DATASET", "confidence": 0.5441223680973053}]}, {"text": "The contribution of this paper is hence threefold: 1) We propose hybrid neural networks, which learn better representations for both questions and answers by combining merits of both RNN and CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 191, "end_pos": 194, "type": "DATASET", "confidence": 0.7845633029937744}]}, {"text": "2) We prove the effectiveness of attention on the answer selection task, which has not been sufficiently explored in prior work.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.9199134111404419}]}, {"text": "3) We achieve the state-of-the-art results on both TREC-QA and InsuranceQA datasets.", "labels": [], "entities": [{"text": "TREC-QA", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.5250345468521118}, {"text": "InsuranceQA datasets", "start_pos": 63, "end_pos": 83, "type": "DATASET", "confidence": 0.9529080092906952}]}, {"text": "The rest of the paper is organized as follows: Section 2 describes the related work for answer selection; Section 3 provides the details of the proposed models; Experimental settings and results are discussed in Section 4 and 5; Finally, we draw conclusions in Section 6.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.9519335627555847}]}], "datasetContent": [{"text": "The first dataset we use to evaluate the proposed approaches is the InsuranceQA, which has been recently proposed by.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.7520473003387451}]}, {"text": "We use the first version of this dataset.", "labels": [], "entities": []}, {"text": "This dataset contains question and answer pairs from the insurance domain and is already divided into a training set, a validation set, and two test sets.", "labels": [], "entities": []}, {"text": "We do not see any obvious categorical differentiation between two tests' questions.", "labels": [], "entities": []}, {"text": "We list the numbers of questions and answers of the dataset in.", "labels": [], "entities": []}, {"text": "We refer the reader to, for more details regarding the InsuranceQA data.", "labels": [], "entities": [{"text": "InsuranceQA data", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9831403493881226}]}, {"text": "In this dataset, a question may have multiple correct answers, and normally the questions are much shorter than answers.", "labels": [], "entities": []}, {"text": "The average length of questions in tokens is 7, while the average length of answers is 94.", "labels": [], "entities": []}, {"text": "Such difference posts additional challenges for the answer selection task.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.9550011356671652}]}, {"text": "This corpus contains 24981 unique answers in total.", "labels": [], "entities": []}, {"text": "For the development and test sets, the InsuranceQA also includes an answer pool of 500 candidate answers for each question.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8893582820892334}]}, {"text": "These answer pools were constructed by including the correct answer(s) and randomly selected candidates from the complete set of unique answers.", "labels": [], "entities": []}, {"text": "The top-1 accuracy of the answer selection is reported.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9938801527023315}]}, {"text": "In this section we detail our experimental setup and results using the TREC-QA dataset.", "labels": [], "entities": [{"text": "TREC-QA dataset", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.911367803812027}]}], "tableCaptions": [{"text": " Table 2: Numbers of Qs and As in InsuranceQA.", "labels": [], "entities": [{"text": "Numbers of Qs and As", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8074700236320496}, {"text": "InsuranceQA", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9514800906181335}]}, {"text": " Table 3: The experimental results of InsuranceQA.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.8281376361846924}]}]}