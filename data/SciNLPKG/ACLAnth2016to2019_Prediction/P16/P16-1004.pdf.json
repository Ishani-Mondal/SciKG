{"title": [{"text": "Language to Logical Form with Neural Attention", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic parsing aims at mapping natural language to machine interpretable meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8411253690719604}]}, {"text": "Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain-or representation-specific.", "labels": [], "entities": []}, {"text": "In this paper we present a general method based on an attention-enhanced encoder-decoder model.", "labels": [], "entities": []}, {"text": "We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors.", "labels": [], "entities": []}, {"text": "Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8287670314311981}]}, {"text": "There has recently been a surge of interest in developing machine learning methods for semantic parsing (see the references in Section 2), due in part to the existence of corpora containing utterances annotated with formal meaning representations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7404519021511078}]}, {"text": "shows an example of a question (left handside) and its annotated logical form (right handside), taken from JOBS (), a well-known semantic parsing benchmark.", "labels": [], "entities": [{"text": "JOBS", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.8785624504089355}, {"text": "semantic parsing", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.7383439838886261}]}, {"text": "In order to predict the correct logical form fora given utterance, most previous systems rely on predefined templates and manually designed features, which often render the parsing model domain-or representation-specific.", "labels": [], "entities": []}, {"text": "In this work, we aim to use a simple yet effective method to bridge the gap between natural language and logical form with minimal domain knowledge.", "labels": [], "entities": []}, {"text": "Encoder-decoder architectures based on recurrent neural networks have been successfully applied to a variety of NLP tasks ranging from syntactic parsing ( , to machine translation (), and image description generation).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.6807376146316528}, {"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.7595326602458954}, {"text": "image description generation", "start_pos": 188, "end_pos": 216, "type": "TASK", "confidence": 0.821533719698588}]}, {"text": "As shown in, we adapt the general encoder-decoder paradigm to the semantic parsing task.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7708040177822113}]}, {"text": "Our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units.", "labels": [], "entities": []}, {"text": "We present two model variants, the first one treats semantic parsing as a vanilla sequence transduction task, whereas our second model is equipped with a hierarchical tree decoder which explicitly captures the compositional structure of logical forms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7347682565450668}]}, {"text": "We also introduce an attention mechanism () allowing the model to learn soft alignments between natural language and logical forms and present an argument identification step to handle rare mentions of entities and numbers.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 146, "end_pos": 169, "type": "TASK", "confidence": 0.6963286995887756}]}, {"text": "Evaluation results demonstrate that compared to previous methods our model achieves similar or better performance across datasets and meaning representations, despite using no hand-engineered domain-or representation-specific features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our method against multiple previous systems on four datasets.", "labels": [], "entities": []}, {"text": "We describe these datasets below, and present our experimental settings and results.", "labels": [], "entities": []}, {"text": "Finally, we conduct model analysis in order to understand what the model learns.", "labels": [], "entities": []}, {"text": "The code is available at https://github.", "labels": [], "entities": []}, {"text": "com/donglixp/lang2logic.", "labels": [], "entities": []}, {"text": "Our model was trained on the following datasets, covering different domains and using different meaning representations.", "labels": [], "entities": []}, {"text": "Examples for each domain are shown in.", "labels": [], "entities": []}, {"text": "GEO This is a standard semantic parsing benchmark which contains 880 queries to a database of U.S. geography.", "labels": [], "entities": [{"text": "GEO This", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.832650363445282}, {"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7420423030853271}]}, {"text": "GEO has 880 instances split into a training set of 680 training examples and 200 test examples.", "labels": [], "entities": [{"text": "GEO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9539649486541748}]}, {"text": "We used the same meaning representation based on lambda-calculus as Turn on heater when temperature drops below 58 degree TRIGGER: Weather -Current temperature drops below -((Temperature (58)) (Degrees in (f))) ACTION: WeMo Insight Switch -Turn on -((Which switch?", "labels": [], "entities": [{"text": "TRIGGER", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9697853922843933}, {"text": "ACTION", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.7770612835884094}]}, {"text": "(\"\"))): Examples of natural language descriptions and their meaning representations from four datasets.", "labels": [], "entities": []}, {"text": "The average length of input and output sequences is shown in the second column.", "labels": [], "entities": []}, {"text": "recipes from the IFTTT website . Recipes are simple programs with exactly one trigger and one action which users specify on the site.", "labels": [], "entities": [{"text": "IFTTT website", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9398646354675293}]}, {"text": "Whenever the conditions of the trigger are satisfied, the action is performed.", "labels": [], "entities": []}, {"text": "Actions typically revolve around home security (e.g., \"turn on my lights when I arrive home\"), automation (e.g., \"text me if the door opens\"), well-being (e.g., \"remind me to drink water if I've been at a bar for more than two hours\"), and soon.", "labels": [], "entities": [{"text": "automation", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9769561886787415}]}, {"text": "Triggers and actions are selected from different channels (160 in total) representing various types of services, devices (e.g., Android), and knowledge sources (such as ESPN or Gmail).", "labels": [], "entities": [{"text": "ESPN", "start_pos": 169, "end_pos": 173, "type": "DATASET", "confidence": 0.8880281448364258}]}, {"text": "In the dataset, there are 552 trigger functions from 128 channels, and 229 action functions from 99 channels.", "labels": [], "entities": []}, {"text": "We used original split which contains 77, 495 training, 5, 171 development, and 4, 294 test examples.", "labels": [], "entities": []}, {"text": "The IFTTT programs are represented as abstract syntax trees and are paired with natural language descriptions provided by users (see).", "labels": [], "entities": []}, {"text": "Here, numbers and URLs are identified.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of natural language descriptions and their meaning representations from four datasets.  The average length of input and output sequences is shown in the second column.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results on JOBS.", "labels": [], "entities": [{"text": "JOBS", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9047468304634094}]}, {"text": " Table 3: Evaluation results on GEO. 10-fold cross- validation is used for the systems shown in the top  half of the table. The standard split of ZC05 is  used for all other systems.", "labels": [], "entities": [{"text": "GEO", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9392274022102356}, {"text": "ZC05", "start_pos": 146, "end_pos": 150, "type": "DATASET", "confidence": 0.9346434473991394}]}, {"text": " Table 4: Evaluation results on ATIS.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8087489604949951}]}, {"text": " Table 5: Evaluation results on IFTTT.", "labels": [], "entities": [{"text": "IFTTT", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.6723865866661072}]}]}