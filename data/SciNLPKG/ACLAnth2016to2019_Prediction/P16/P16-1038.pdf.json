{"title": [{"text": "Grapheme-to-Phoneme Models for (Almost) Any Language", "labels": [], "entities": []}], "abstractContent": [{"text": "Grapheme-to-phoneme (g2p) models are rarely available in low-resource languages, as the creation of training and evaluation data is expensive and time-consuming.", "labels": [], "entities": []}, {"text": "We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages.", "labels": [], "entities": []}, {"text": "We then develop phoneme and language distance metrics based on phono-logical and linguistic knowledge; applying those, we adapt g2p models for high-resource languages to create models for related low-resource languages.", "labels": [], "entities": []}, {"text": "We provide results for models for 229 adapted languages .", "labels": [], "entities": []}], "introductionContent": [{"text": "Grapheme-to-phoneme (g2p) models convert words into pronunciations, and are ubiquitous in speech-and text-processing systems.", "labels": [], "entities": []}, {"text": "Due to the diversity of scripts, phoneme inventories, phonotactic constraints, and spelling conventions among the world's languages, they are typically languagespecific.", "labels": [], "entities": []}, {"text": "Thus, while most statistical g2p learning methods are language-agnostic, they are trained on language-specific data-namely, a pronunciation dictionary consisting of word-pronunciation pairs, as in.", "labels": [], "entities": []}, {"text": "Building such a dictionary fora new language is both time-consuming and expensive, because it requires expertise in both the language and a notation system like the International Phonetic Alphabet, applied to thousands of word-pronunciation pairs.", "labels": [], "entities": []}, {"text": "Unsurprisingly, resources have been allocated only to the most heavily-researched languages.", "labels": [], "entities": []}, {"text": "GlobalPhone, one of the most extensive multilingual text and speech databases, has pronunciation dictionaries in only 20 languages ( . We have been unable to obtain this dataset.", "labels": [], "entities": [{"text": "GlobalPhone", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9709116816520691}]}, {"text": "\u0261 \u026a f t\u02b0 \u0261 \u026a ft \u0263 \u026a ft class k\u02b0 l ae s kl a\u02d0 s kl \u0251 s send s e\u031e n dz \u025b n t s \u025b n t: Example pronunciations of English words using English, German, and Dutch g2p models.", "labels": [], "entities": []}, {"text": "For most of the world's more than 7,100 languages (, no data exists and the many technologies enabled by g2p models are inaccessible.", "labels": [], "entities": []}, {"text": "Intuitively, however, pronouncing an unknown language should not necessarily require large amounts of language-specific knowledge or data.", "labels": [], "entities": []}, {"text": "A native German or Dutch speaker, with no knowledge of English, can approximate the pronunciations of an English word, albeit with slightly different phonemes.", "labels": [], "entities": []}, {"text": "demonstrates that German and Dutch g2p models can do the same.", "labels": [], "entities": []}, {"text": "Motivated by this, we create and evaluate g2p models for low-resource languages by adapting existing g2p models for high-resource languages using linguistic and phonological information.", "labels": [], "entities": []}, {"text": "To facilitate our experiments, we create several notable data resources, including a multilingual pronunciation dictionary with entries for more than 500 languages.", "labels": [], "entities": []}, {"text": "The contributions of this work are: \u2022 Using data scraped from Wiktionary, we clean and normalize pronunciation dictionaries for 531 languages.", "labels": [], "entities": [{"text": "normalize pronunciation dictionaries", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.8553785880406698}]}, {"text": "To our knowledge, this is the most comprehensive multilingual pronunciation dictionary available.", "labels": [], "entities": []}, {"text": "\u2022 We synthesize several named entities corpora to create a multilingual corpus covering 384 languages.", "labels": [], "entities": []}, {"text": "\u2022 We develop a language-independent distance metric between IPA phonemes.", "labels": [], "entities": []}, {"text": "\u2022 We extend previous metrics for languagelanguage distance with additional information and metrics.", "labels": [], "entities": [{"text": "languagelanguage distance", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6983350962400436}]}, {"text": "\u2022 We create two sets of g2p models for \"high resource\" languages: 97 simple rule-based models extracted from Wikipedia's \"IPA Help\" pages, and 85 data-driven models built from Wiktionary data.", "labels": [], "entities": [{"text": "Wikipedia's \"IPA Help\" pages", "start_pos": 109, "end_pos": 137, "type": "DATASET", "confidence": 0.740043933902468}]}, {"text": "\u2022 We develop methods for adapting these g2p models to related languages, and describe results for 229 adapted models.", "labels": [], "entities": []}, {"text": "\u2022 We release all data and models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The next two sections describe our high-resource and adapted g2p models.", "labels": [], "entities": []}, {"text": "To evaluate these models, we compute the following metrics: \u2022 % of words skipped: This shows the coverage of the g2p model.", "labels": [], "entities": []}, {"text": "Some g2p models do not coverall character sequences.", "labels": [], "entities": []}, {"text": "All other metrics are computed over non-skipped words.", "labels": [], "entities": []}, {"text": "\u2022 word error rate (WER): The percent of incorrect 1-best pronunciations.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 2, "end_pos": 23, "type": "METRIC", "confidence": 0.8850442369778951}]}, {"text": "\u2022 word error rate 100-best (WER 100): The percent of 100-best lists without the correct pronunciation.", "labels": [], "entities": [{"text": "word error rate 100-best (WER 100)", "start_pos": 2, "end_pos": 36, "type": "METRIC", "confidence": 0.8621922507882118}]}, {"text": "\u2022 phoneme error rate (PER): The percent of errors per phoneme.", "labels": [], "entities": [{"text": "error rate (PER)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9360573768615723}]}, {"text": "A PER of 15.0 indicates that, on average, a linguist would have to edit 15 out of 100 phonemes of the output.", "labels": [], "entities": [{"text": "PER", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9992795586585999}]}, {"text": "We then average these metrics across all languages (weighting each language equally).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Results for high-resource models. The top portion of the table shows results for all models; the  bottom shows results only for languages with both IPA Help and Wiktionary models.", "labels": [], "entities": []}, {"text": " Table 7: WER scores for Bengali, Tagalog,  Turkish, and German models. Unioned models  with IPA Help rules tend to perform better than  Wiktionary-only models, but not consistently.", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.984775722026825}]}, {"text": " Table 8: Results for adapted g2p models. Final adapted results (using the 85 languages covered by Wik- tionary and unioned high-resource models, as well as rescripting) cover 229 languages.", "labels": [], "entities": []}]}