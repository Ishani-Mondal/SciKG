{"title": [{"text": "Phrase Table Pruning via Submodular Function Maximization", "labels": [], "entities": [{"text": "Submodular Function Maximization", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6986958384513855}]}], "abstractContent": [{"text": "Phrase table pruning is the act of removing phrase pairs from a phrase table to make it smaller, ideally removing the least useful phrases first.", "labels": [], "entities": [{"text": "Phrase table pruning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7884419957796732}]}, {"text": "We propose a phrase table pruning method that formulates the task as a submodular function maximization problem, and solves it by using a greedy heuristic algorithm.", "labels": [], "entities": []}, {"text": "The proposed method can scale with input size and long phrases, and experiments show that it achieves higher BLEU scores than state-of-the-art pruning methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9993021488189697}]}], "introductionContent": [{"text": "A phrase table, a key component of phrase-based statistical machine translation (PBMT) systems, consists of a set of phrase pairs.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PBMT)", "start_pos": 35, "end_pos": 86, "type": "TASK", "confidence": 0.7301806466920036}]}, {"text": "A phrase pair is a pair of source and target language phrases, and is used as the atomic translation unit.", "labels": [], "entities": [{"text": "atomic translation", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7455754280090332}]}, {"text": "Today's PBMT systems have to store and process large phrase tables that contain more than 100M phrase pairs, and their sheer size prevents PBMT systems for running in resource-limited environments such as mobile phones.", "labels": [], "entities": []}, {"text": "Even if a computer has enough resources, the large phrase tables increase turnaround time and prevent the rapid development of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9931568503379822}]}, {"text": "Phrase table pruning is the technique of removing ineffective phrase pairs from a phrase table to make it smaller while minimizing the performance degradation.", "labels": [], "entities": [{"text": "Phrase table pruning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7951476573944092}]}, {"text": "Existing phrase table pruning methods use different metrics to rank the phrase pairs contained in the table, and then remove lowranked pairs.", "labels": [], "entities": []}, {"text": "Metrics used in previous work are frequency, conditional probability, and Fisher's exact test score.", "labels": [], "entities": [{"text": "frequency", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9951593279838562}, {"text": "exact test score", "start_pos": 83, "end_pos": 99, "type": "METRIC", "confidence": 0.8898123701413473}]}, {"text": "evaluated many phrase table pruning methods, and concluded that entropy-based pruning method () offers the best performance.", "labels": [], "entities": []}, {"text": "The entropy-based pruning method uses entropy to measure the redundancy of a phrase pair, where we say a phrase pair is redundant if it can be replaced by other phrase pairs.", "labels": [], "entities": []}, {"text": "The entropy-based pruning method runs in time linear to the number of phrase-pairs.", "labels": [], "entities": []}, {"text": "Unfortunately, its running time is also exponential to the length of phrases contained in the phrase pairs, since it contains the problem of finding an optimal phrase alignment, which is known to be NP-hard.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.6940287798643112}]}, {"text": "Therefore, the method can be impractical if the phrase pairs consist of longer phrases.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel phrase table pruning method that formulates and solves the phrase table pruning problem as a submodular function maximization problem.", "labels": [], "entities": []}, {"text": "A submodular function is a kind of set function that satisfies the submodularity property.", "labels": [], "entities": []}, {"text": "Generally, the submodular function maximization problem is NP-hard, however, it is known that (1 \u2212 1/e) optimal solutions can be obtained by using a simple greedy algorithm.", "labels": [], "entities": [{"text": "submodular function maximization", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.7178337574005127}]}, {"text": "Since a greedy algorithm scales with large inputs, our method can be applicable to large phrase tables.", "labels": [], "entities": []}, {"text": "One key factor of the proposed method is its carefully designed objective function that evaluates the quality of a given phrase table.", "labels": [], "entities": []}, {"text": "In this paper, we use a simple monotone submodular function that evaluates the quality of a given phrase table by its coverage of a training corpus.", "labels": [], "entities": []}, {"text": "Our method is simple, parameter free, and does not cause exponential explosion of the computation time with longer phrases.", "labels": [], "entities": []}, {"text": "We conduct experiments with two different language pairs, and show that the proposed method shows higher BLEU scores than state-of-the-art pruning methods.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9993401169776917}]}], "datasetContent": [], "tableCaptions": []}