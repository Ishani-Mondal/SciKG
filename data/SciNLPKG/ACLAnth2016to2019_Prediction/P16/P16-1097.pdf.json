{"title": [{"text": "Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora", "labels": [], "entities": [{"text": "Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora", "start_pos": 0, "end_pos": 83, "type": "TASK", "confidence": 0.5946231335401535}]}], "abstractContent": [{"text": "We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora.", "labels": [], "entities": []}, {"text": "The basic idea is to encourage two asym-metric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments.", "labels": [], "entities": [{"text": "identifying latent phrase and word alignments", "start_pos": 140, "end_pos": 185, "type": "TASK", "confidence": 0.5950044393539429}]}, {"text": "The agreement is defined at both word and phrase levels.", "labels": [], "entities": []}, {"text": "We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently.", "labels": [], "entities": []}, {"text": "Experiments on the Chinese-English dataset show that agreement-based learning significantly improves both alignment and translation performance.", "labels": [], "entities": [{"text": "Chinese-English dataset", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.6791637390851974}]}], "introductionContent": [{"text": "Parallel corpora, which are large collections of parallel texts, serve as an important resource for inducing translation correspondences, either at the level of words ( or phrases.", "labels": [], "entities": [{"text": "translation correspondences", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.8274407088756561}]}, {"text": "However, the availability of large-scale, wide-coverage corpora still remains a challenge even in the era of big data: parallel corpora are usually only existent for resourcerich languages and restricted to limited domains such as government documents and news articles.", "labels": [], "entities": []}, {"text": "Therefore, intensive attention has been drawn to exploiting non-parallel corpora for acquiring translation correspondences.", "labels": [], "entities": [{"text": "acquiring translation correspondences", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.7377524971961975}]}, {"text": "Most previous efforts have concentrated on learning parallel lexicons from non-parallel corpora, including parallel sentence and lexicon extraction via bootstrapping (), inducing parallel lexicons via canonical correlation analysis (Haghighi * Corresponding author: Yang Liu.", "labels": [], "entities": [{"text": "parallel sentence and lexicon extraction", "start_pos": 107, "end_pos": 147, "type": "TASK", "confidence": 0.6018888592720032}]}, {"text": "et, training IBM models on monolingual corpora as decipherment (), and deriving parallel lexicons from bilingual word embeddings.", "labels": [], "entities": []}, {"text": "Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora.", "labels": [], "entities": []}, {"text": "present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon.", "labels": [], "entities": []}, {"text": "continue this line of research to further introduce an iterative approach to joint learning of parallel lexicons and phrases.", "labels": [], "entities": [{"text": "joint learning of parallel lexicons and phrases", "start_pos": 77, "end_pos": 124, "type": "TASK", "confidence": 0.7127574809959957}]}, {"text": "They introduce a corpus-level latentvariable translation model in a non-parallel scenario and develop a training algorithm that alternates between (1) using a parallel lexicon to extract parallel phrases from non-parallel corpora and (2) using the extracted parallel phrases to enlarge the parallel lexicon.", "labels": [], "entities": [{"text": "corpus-level latentvariable translation", "start_pos": 17, "end_pos": 56, "type": "TASK", "confidence": 0.6893238027890524}]}, {"text": "They show that starting from a small seed lexicon, their approach is capable of learning both new words and phrases gradually overtime.", "labels": [], "entities": []}, {"text": "However, due to the structural divergence between natural languages as well as the presence of noisy data, only using asymmetric translation models might be insufficient to accurately identify parallel lexicons and phrases from non-parallel corpora.", "labels": [], "entities": []}, {"text": "report that the accuracy on Chinese-English dataset is only around 40% after running for 70 iterations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.999697208404541}, {"text": "Chinese-English dataset", "start_pos": 28, "end_pos": 51, "type": "DATASET", "confidence": 0.7639912664890289}]}, {"text": "In addition, their approach seems prone to be affected by noisy data in non-parallel corpora as the accuracy drops significantly with the increase of noise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9990763664245605}]}, {"text": "Since asymmetric word alignment and phrase alignment models are usually complementary, it is natural to combine them to make more accurate predictions.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.715094119310379}, {"text": "phrase alignment", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7543793022632599}]}, {"text": "In this work, we propose to in-troduce agreement-based learning ( into extracting parallel lexicons and phrases from non-parallel corpora.", "labels": [], "entities": []}, {"text": "Based on the latent-variable model proposed by, we propose two kinds of loss functions to take into account the agreement between both phrase alignment and word alignment in two directions.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.7280605733394623}, {"text": "word alignment", "start_pos": 156, "end_pos": 170, "type": "TASK", "confidence": 0.6703278720378876}]}, {"text": "As the inference is intractable, we resort to a Viterbi EM algorithm to train the two models efficiently.", "labels": [], "entities": []}, {"text": "Experiments on the Chinese-English dataset show that agreementbased learning is more robust to noisy data and leads to substantial improvements in phrase alignment and machine translation evaluations.", "labels": [], "entities": [{"text": "Chinese-English dataset", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.6860319226980209}, {"text": "phrase alignment", "start_pos": 147, "end_pos": 163, "type": "TASK", "confidence": 0.829501211643219}, {"text": "machine translation evaluations", "start_pos": 168, "end_pos": 199, "type": "TASK", "confidence": 0.8105622331301371}]}], "datasetContent": [{"text": "In this section, we evaluate our approach in two tasks: phrase alignment (Section 4.1) and machine translation (Section 4.2).", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.835837185382843}, {"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7985938489437103}]}, {"text": "Given two monolingual corpora E and F , we suppose there exists aground truth parallel corpus G and denote an extracted parallel corpus as D.", "labels": [], "entities": []}, {"text": "The quality of an extracted parallel corpus can be measured by F1 = 2|D \u2229 G|/(|D| + |G|).", "labels": [], "entities": [{"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9985935091972351}]}, {"text": "Following and, we evaluate our approach on domain adaptation for machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7228154242038727}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7646850943565369}]}, {"text": "The data set consists of two in-domain nonparallel corpora and an out-domain parallel corpus.", "labels": [], "entities": []}, {"text": "The in-domain non-parallel corpora consists of 2.65M Chinese phrases and 3.67M English phrases extracted from LDC news articles.", "labels": [], "entities": [{"text": "LDC news articles", "start_pos": 110, "end_pos": 127, "type": "DATASET", "confidence": 0.9109439253807068}]}, {"text": "We use a small out-domain parallel corpus extracted from financial news of FTChina which contains 10K phrase pairs.", "labels": [], "entities": [{"text": "out-domain parallel corpus extracted from financial news of FTChina", "start_pos": 15, "end_pos": 82, "type": "DATASET", "confidence": 0.7084560460514493}]}, {"text": "The task is to extract a parallel corpus from in-domain non-parallel corpora starting from a small out-domain parallel corpus.", "labels": [], "entities": []}, {"text": "We use the state-of-the-art translation system Moses ( and evaluate the performance on Chinese-English NIST datasets.", "labels": [], "entities": [{"text": "NIST datasets", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.8840824365615845}]}, {"text": "The development set is NIST 2006 and the test set is NIST 2005.", "labels": [], "entities": [{"text": "NIST 2006", "start_pos": 23, "end_pos": 32, "type": "DATASET", "confidence": 0.9734197854995728}, {"text": "NIST 2005", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9850524663925171}]}, {"text": "The evaluation metric is caseinsensitive BLEU4 ().", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.989051103591919}]}, {"text": "We use the SRILM toolkit) to train a 4-gram English language model on a monolingual corpus with 399M English words.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8215209543704987}]}, {"text": "At iteration 0, only the out-domain corpus is used and the BLEU score is 5.61.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9810076057910919}]}, {"text": "All methods iteratively extract parallel phrases from non-parallel corpora and enlarge the extracted parallel corpus.", "labels": [], "entities": []}, {"text": "We find that agreementbased learning achieves much higher BLEU scores while obtains a smaller parallel corpus as compared with independent learning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9989795088768005}]}, {"text": "One possible reason is that the agreement-based learning rules out most unlikely phrase pairs by encouraging consensus between two models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of seed lexicon size in terms of F1  on the development set.", "labels": [], "entities": [{"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.998801589012146}]}, {"text": " Table 2: Effect of noise in terms of F1 on the de- velopment set.", "labels": [], "entities": [{"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9990046620368958}, {"text": "de- velopment set", "start_pos": 48, "end_pos": 65, "type": "METRIC", "confidence": 0.6641995906829834}]}, {"text": " Table 3: Example learned parallel lexicons and  phrases. New words that are not included in the  seed lexicon are highlighted in italic.", "labels": [], "entities": []}, {"text": " Table 4: Results on domain adaptation for machine translation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7788436412811279}, {"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7846940457820892}]}]}