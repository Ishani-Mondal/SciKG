{"title": [{"text": "Sentiment Domain Adaptation with Multiple Sources", "labels": [], "entities": [{"text": "Sentiment Domain Adaptation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9285438656806946}]}], "abstractContent": [{"text": "Domain adaptation is an important research topic in sentiment analysis area.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.798113614320755}, {"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9683997333049774}]}, {"text": "Existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew domain adaptation approach which can exploit sentiment knowledge from multiple source domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7646479606628418}]}, {"text": "We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains using multi-task learning.", "labels": [], "entities": []}, {"text": "Then we transfer them to target domain with the help of words' sentiment polarity relations extracted from the un-labeled target domain data.", "labels": [], "entities": []}, {"text": "The similarities between target domain and different source domains are also incorporated into the adaptation process.", "labels": [], "entities": []}, {"text": "Experimental results on benchmark dataset show the effectiveness of our approach in improving cross-domain sentiment classification performance .", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 94, "end_pos": 131, "type": "TASK", "confidence": 0.7719060182571411}]}], "introductionContent": [{"text": "Sentiment classification is a hot research topic in natural language processing field, and has many applications in both academic and industrial areas (.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9394692480564117}]}, {"text": "Sentiment classification is widely known as a domain-dependent task).", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9480476379394531}]}, {"text": "The sentiment classifier trained in one domain may not perform well in another domain.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8472262620925903}]}, {"text": "This is because sentiment expressions used in different domains are usually different.", "labels": [], "entities": []}, {"text": "For example, \"boring\" * Corresponding author. and \"lengthy\" are frequently used to express negative sentiments in Book domain.", "labels": [], "entities": []}, {"text": "However, they rarely appear in Electronics domain).", "labels": [], "entities": [{"text": "Electronics domain", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9765602648258209}]}, {"text": "Thus a sentiment classifier trained in Electronics domain cannot accurately predict their sentiments in Book domain.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8228349685668945}, {"text": "Book domain", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.9212988018989563}]}, {"text": "In addition, the same word may convey different sentiments in different domains.", "labels": [], "entities": []}, {"text": "For example, in Electronics domain \"easy\" is usually used in positive reviews, e.g., \"this digital camera is easy to use.\"", "labels": [], "entities": [{"text": "Electronics domain", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.9256661236286163}]}, {"text": "However, it is frequently used as a negative word in Movie domain.", "labels": [], "entities": [{"text": "Movie domain", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.919102817773819}]}, {"text": "For instance, \"the ending of this movie is easy to guess.\"", "labels": [], "entities": []}, {"text": "Thus, the sentiment classifier trained in one domain usually cannot be applied to another domain directly.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8175515234470367}]}, {"text": "In order to tackle this problem, sentiment domain adaptation has been widely studied.", "labels": [], "entities": [{"text": "sentiment domain adaptation", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.8613096674283346}]}, {"text": "For example, proposed to compute the correspondence among features from different domains using their associations with pivot features based on structural correspondence learning (SCL).", "labels": [], "entities": [{"text": "structural correspondence learning (SCL)", "start_pos": 144, "end_pos": 184, "type": "TASK", "confidence": 0.6596201757589976}]}, {"text": "proposed a spectral feature alignment (SFA) algorithm to align the domain-specific words from different domains in order to reduce the gap between source and target domains.", "labels": [], "entities": [{"text": "spectral feature alignment (SFA)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8251584966977438}]}, {"text": "However, all of these methods transfer sentiment information from only one source domain.", "labels": [], "entities": []}, {"text": "When the source and target domains have significant difference in feature distributions, the adaptation performance will heavily decline.", "labels": [], "entities": []}, {"text": "In some cases, the performance of sentiment domain adaptation is even worse than that without adaptation, which is usually known as negative transfer . In this paper we propose anew domain adaptation approach for cross-domain sentiment classification.", "labels": [], "entities": [{"text": "sentiment domain adaptation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7090702255566915}, {"text": "cross-domain sentiment classification", "start_pos": 213, "end_pos": 250, "type": "TASK", "confidence": 0.7954965233802795}]}, {"text": "Our approach can exploit the sentiment information in multiple source domains to reduce the risk of negative transfer effectively.", "labels": [], "entities": []}, {"text": "Our approach consists of two steps, i.e., training and adaptation.", "labels": [], "entities": []}, {"text": "At the training stage, we extract two kinds of sentiment models, i.e., the global model and the domain-specific models, from the data of multiple source domains using multi-task learning.", "labels": [], "entities": []}, {"text": "The global sentiment model can capture the common sentiment knowledge shared by various domains, and has better generalization performance than the sentiment model trained in a single source domain.", "labels": [], "entities": []}, {"text": "The domain-specific sentiment model can capture the specific sentiment knowledge in each source domain.", "labels": [], "entities": []}, {"text": "At the adaptation stage, we transfer both kinds of sentiment knowledge to target domain with the help of the words' sentiment graph of target domain.", "labels": [], "entities": []}, {"text": "The sentiment graph contains words' domain-specific sentiment polarity relations extracted from the syntactic parsing results of the unlabeled data in target domain.", "labels": [], "entities": []}, {"text": "Since sentiment transfer between similar domains is more effective than dissimilar domains, we incorporate the similarities between target domain and different source domains into the adaptation process.", "labels": [], "entities": [{"text": "sentiment transfer", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.9373587965965271}]}, {"text": "In order to estimate the similarity between two domains, we propose a novel domain similarity measure based on their sentiment graphs.", "labels": [], "entities": []}, {"text": "Extensive experiments were conducted on the benchmark Amazon product review dataset.", "labels": [], "entities": [{"text": "Amazon product review dataset", "start_pos": 54, "end_pos": 83, "type": "DATASET", "confidence": 0.9001573771238327}]}, {"text": "The experimental results show that our approach can improve the performance of cross-domain sentiment classification effectively.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 79, "end_pos": 116, "type": "TASK", "confidence": 0.756426473458608}]}], "datasetContent": [{"text": "The dataset used in our experiments is the famous Amazon product review dataset collected by.", "labels": [], "entities": [{"text": "Amazon product review dataset collected", "start_pos": 50, "end_pos": 89, "type": "DATASET", "confidence": 0.9406540155410766}]}, {"text": "It is widely used as a benchmark dataset for cross-domain sentiment classification.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.8443940877914429}]}, {"text": "Four domains, i.e., Book, DVD, Electronics and Kitchen, are included in this dataset.", "labels": [], "entities": [{"text": "Book", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.9450190663337708}]}, {"text": "Each domain contains 1,000 positive and 1,000 negative reviews.", "labels": [], "entities": []}, {"text": "Besides, a large number of unlabeled reviews are provided.", "labels": [], "entities": []}, {"text": "The detailed statistics of this dataset are shown in.", "labels": [], "entities": []}, {"text": "In our experiments, each domain was selected in turn as target domain, and remaining domains as source domains.", "labels": [], "entities": []}, {"text": "In each experiment, we randomly selected N labeled samples from the), unigrams and bigrams were used as features.", "labels": [], "entities": []}, {"text": "The sentiment polarity relations of bigrams were extracted by expanding the polarity relations between unigrams using modifying relations.", "labels": [], "entities": []}, {"text": "For example, from the review \"this phone is very beautiful and not expensive,\" we extract not only sentiment polarity relation between \"beautiful\" and \"expensive\", but also polarity relation between \"beautiful\" and \"not expensive\" (coherent sentiment), and that between \"very beautiful\" and \"expensive\" (opposite sentiment), since \"very\" and \"not\" are used to modify \"beautiful\" and \"expensive\" respectively.", "labels": [], "entities": []}, {"text": "Classification accuracy was selected as the evaluation metric.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8316939473152161}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9521913528442383}]}, {"text": "We manually set \u03b2 in Eq.", "labels": [], "entities": [{"text": "\u03b2", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9494891166687012}]}, {"text": "(5) to 0.01.", "labels": [], "entities": []}, {"text": "The values of \u03b1, \u03bb 1 , and \u03bb 2 in Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 34, "end_pos": 36, "type": "DATASET", "confidence": 0.922359824180603}]}, {"text": "(4) were selected using cross validation.", "labels": [], "entities": []}, {"text": "The optimization problems in Eq.", "labels": [], "entities": []}, {"text": "(4) and Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.779607355594635}]}, {"text": "(5) were solved using alternating direction method of multipliers (ADM-M)).", "labels": [], "entities": []}, {"text": "Each experiment was repeated 10 times independently and average results were reported.", "labels": [], "entities": []}, {"text": "In this section we conducted experiments to evaluate the performance of our approach by comparing it with a series of baseline methods.", "labels": [], "entities": []}, {"text": "The methods to be compared are: 1) SCL, domain adaptation based on structural correspondence learning (; 2) SFA, domain adaptation based on spectral feature alignment (Pan et al., 2010); 3) SCL-com and SFA-com, adapting SCL and SFA to multiple source domain scenario by first training a cross-domain sentiment classifier in each source domain and then combining their classification results using majority voting; 4) SST, cross-domain sentiment classification by using multiple source domains to construct a sentiment sensitive thesaurus for feature expansion (Bollegala et al., 2011); 5) IDDIWP, multiple-domain sentiment analysis by identifying domain dependent/independent word polarity (Yoshida et al., 2011); 6) DWHC, DAM and CP-MDA, three general-purpose multiple source domain adaptation methods proposed in, and (Chattopadhyay et al., 2011) respectively; 7) SDAMS-LS, SDAMS-SVM, and SDAMS-Log, our proposed sentiment domain adaptation approaches with square loss, hinge loss, and logistic loss respectively; 8) AllTraining, all the domains were involved in the training phase of our approach and there is no adaptation phase.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7051912695169449}, {"text": "cross-domain sentiment classification", "start_pos": 422, "end_pos": 459, "type": "TASK", "confidence": 0.7274883687496185}]}, {"text": "This method is introduced to provide an upper bound for the performance of our approach.", "labels": [], "entities": []}, {"text": "The experimental results of these methods are summarized in.", "labels": [], "entities": []}, {"text": "From we can see that our approach achieves the best performance among all the methods compared here.", "labels": [], "entities": []}, {"text": "SCL and SFA are famous cross-domain sentiment classification methods.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.7377482652664185}]}, {"text": "In these methods, the sentiment knowledge is transferred from one source domain to target domain.", "labels": [], "entities": []}, {"text": "According to, our approach performs significantly better than them.", "labels": [], "entities": []}, {"text": "This result indicates that the sentiment knowledge extracted from one source domain may contain heavy domain-specific bias and maybe inappropriate for the target domain.", "labels": [], "entities": []}, {"text": "Our approach can tackle this problem by extracting the global sentiment model from multiple source domains.", "labels": [], "entities": []}, {"text": "This global model can capture the general sentiment knowledge shared by various domains and has better generalization performance.", "labels": [], "entities": []}, {"text": "It can reduce the risk of negative transfer effectively.", "labels": [], "entities": []}, {"text": "Our approach also outperforms SCLcom and SFA-com.", "labels": [], "entities": [{"text": "SFA-com", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.940178394317627}]}, {"text": "In SCL-com and SFA-com, the sentiment information in different source domains is combined at the classification stage, while in our approach it is combined at the learning stage.", "labels": [], "entities": [{"text": "SFA-com", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.8207237720489502}]}, {"text": "The superior performance of our approach compared with SCL-com and SFA-com shows that our approach is a more appropriate way to exploit the sentiment knowledge in different source domains.", "labels": [], "entities": [{"text": "SFA-com", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.8554483652114868}]}, {"text": "SST and IDDIWP also utilize data from multiple source domains as our approach.", "labels": [], "entities": []}, {"text": "But our approach can still outperform them.", "labels": [], "entities": []}, {"text": "This is because in these methods, the similarities between target domain and different source domains are not considered.", "labels": [], "entities": []}, {"text": "Since different domains usually have different sentiment relatedness, our approach can exploit the sentiment information in multiple source domains more accurately by incorporating the similarities between target domain and each source domain into the adaptation process.", "labels": [], "entities": []}, {"text": "Our approach also outperforms the state-of-theart general-purpose multiple source domain adaptation methods, such as DWHC, DAM, and CP-MDA.", "labels": [], "entities": [{"text": "DWHC", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.9242188930511475}]}, {"text": "This is because our approach can exploit more sentiment-related characteristics and knowledge for sentiment domain adaptation, such as the general sentiment knowledge shared by various domains, the sentiment graph based domain similarities, and the word-level sentiment polarity relations.", "labels": [], "entities": [{"text": "sentiment domain adaptation", "start_pos": 98, "end_pos": 125, "type": "TASK", "confidence": 0.7070892850557963}]}, {"text": "Thus, our approach is more suitable for sentiment domain adaptation than these generalpurpose multiple source domain adaptation methods.", "labels": [], "entities": [{"text": "sentiment domain adaptation", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.8056823015213013}]}, {"text": "Another observation from is that the performance of our approach is quite close to the upper bound, i.e., All-Training, especially in Electronics and Kitchen domains.", "labels": [], "entities": []}, {"text": "This result validates the effectiveness of our approach in sentiment domain adaptation.", "labels": [], "entities": [{"text": "sentiment domain adaptation", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.9065684874852499}]}], "tableCaptions": [{"text": " Table 1: The statistics of the dataset.", "labels": [], "entities": []}, {"text": " Table 2: The performance of different methods.", "labels": [], "entities": []}]}