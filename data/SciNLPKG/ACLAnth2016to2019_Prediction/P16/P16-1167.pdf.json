{"title": [{"text": "Learning Prototypical Event Structure from Photo Albums", "labels": [], "entities": [{"text": "Learning Prototypical Event Structure from Photo Albums", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.6763549830232348}]}], "abstractContent": [{"text": "Activities and events in our lives are structural , be it a vacation, a camping trip, or a wedding.", "labels": [], "entities": []}, {"text": "While individual details vary, there are characteristic patterns that are specific to each of these scenarios.", "labels": [], "entities": []}, {"text": "For example , a wedding typically consists of a sequence of events such as walking down the aisle, exchanging vows, and dancing.", "labels": [], "entities": []}, {"text": "In this paper, we present a data-driven approach to learning event knowledge from a large collection of photo albums.", "labels": [], "entities": []}, {"text": "We formulate the task as constrained optimization to induce the prototypical temporal structure of an event, integrating both visual and textual cues.", "labels": [], "entities": []}, {"text": "Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many common scenarios in our lives, such as a wedding or a camping trip, show characteristic structural patterns.", "labels": [], "entities": []}, {"text": "As illustrated in, these patterns can be sequential, such as in a wedding, where exchanging vows generally happens before cutting the cake.", "labels": [], "entities": []}, {"text": "In other scenarios, there maybe a set of composing events, but no prominent temporal relations.", "labels": [], "entities": []}, {"text": "A camping trip, for example, might include events such as hiking, which can happen either before or after setting up a tent.", "labels": [], "entities": []}, {"text": "This observation on the prototypical patterns in everyday scenarios goes back to early artificial intelligence research.", "labels": [], "entities": []}, {"text": "Scripts (, an early formalism, were developed to encode the necessary background knowledge to support an inference engine for commonsense reasoning in limited domains.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 126, "end_pos": 147, "type": "TASK", "confidence": 0.8373159468173981}]}, {"text": "However, early ap-: We collect photo albums of common scenarios (e.g., weddings) and cluster their images and captions to learn the hierarchical events that makeup these scenarios.", "labels": [], "entities": []}, {"text": "We use constrained optimization to decode the temporal order of these events, and we extract the prototypical descriptions that define them.", "labels": [], "entities": []}, {"text": "proaches based on hand-coded symbolic representations proved to be brittle and difficult to scale.", "labels": [], "entities": []}, {"text": "An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or commonsense knowledge bottom-up from large-scale data.", "labels": [], "entities": [{"text": "statistical knowledge induction", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.6952069799105326}]}, {"text": "While most prior work is based on text (), recent work begins exploring the use of images as well (.", "labels": [], "entities": []}, {"text": "In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions.", "labels": [], "entities": [{"text": "learning knowledge about common life scenarios (e.g., weddings, camping trips)", "start_pos": 46, "end_pos": 124, "type": "TASK", "confidence": 0.6416935473680496}]}, {"text": "The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see for more details).", "labels": [], "entities": []}, {"text": "We cast unsupervised learning of event structure as a sequential multimodal clustering prob-lem, which requires solving two subproblems concurrently: identifying the boundaries of events and assigning identities to each of these events.", "labels": [], "entities": []}, {"text": "We formulate this process as constrained optimization, where constraints encode the temporal event patterns that are induced directly from the data.", "labels": [], "entities": []}, {"text": "The outcome is a statistically induced prototypical structure of events characterized by their visual and textual representations.", "labels": [], "entities": []}, {"text": "We evaluate the quality and utility of the learned knowledge in three tasks: temporal event ordering, segmentation prediction, and multimodal summarization.", "labels": [], "entities": [{"text": "temporal event ordering", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.5891638497511545}, {"text": "segmentation prediction", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.9718230962753296}]}, {"text": "Our experimental results show the performance of our model in predicting the order of photos in albums, partitioning photo albums into event sequences, and summarizing albums.", "labels": [], "entities": [{"text": "summarizing albums", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.903680682182312}]}], "datasetContent": [{"text": "For this study, we have compiled anew corpus of multimodal photo albums across 12 distinct scenarios.", "labels": [], "entities": []}, {"text": "It comprises of 34,818 albums containing 1.5 million pairs of online photographs and their textual descriptions.", "labels": [], "entities": []}, {"text": "shows the list of scenarios and the corresponding data statistics.", "labels": [], "entities": []}, {"text": "We choose six scenarios (the top half of) that we expect have an inherent temporal event structure that can be learned and six that we expect do not (the bottom half of).", "labels": [], "entities": []}, {"text": "The dataset is collected using the Flickr API 1,2 . We use the scenario names and variations of them (e.g., Paris Trip, Paris Vacation) as queries for images.", "labels": [], "entities": [{"text": "Flickr API 1,2", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.8983338276545206}, {"text": "Paris Trip", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.9109034538269043}]}, {"text": "We then form albums from these images by grouping images by user, sorting them by timestamp, and extracting groups that are within a contained time frame (e.g., 24 hours fora wedding, 5 days fora trip).", "labels": [], "entities": []}, {"text": "For all images, we extract the first sentences of the corresponding textual descriptions as captions and also store their timestamps.", "labels": [], "entities": []}, {"text": "This data is publicly available at https://www.cs.washington.edu/ projects/nlp/protoevents.", "labels": [], "entities": []}, {"text": "For scenarios with more than 1000 albums, we use 100 albums for each of the development and test sets and use the rest for training.", "labels": [], "entities": []}, {"text": "For scenarios with less than 1000 albums, we use 50 albums for each of the development and test sets, and the rest for training.", "labels": [], "entities": []}, {"text": "We optimize our objective function using integer linear programming () with the Gurobi solver.", "labels": [], "entities": []}, {"text": "For computational efficiency, temporally close sets of consecutive photos are treated as one unit during the optimization.", "labels": [], "entities": []}, {"text": "We use these units to reduce the number of variables and constraints in the model from a function of the number of photos to a function of the number of units.", "labels": [], "entities": []}, {"text": "We form these units heuristically by merging images agglomeratively when their timestamps are within a certain range of the closest image in a unit.", "labels": [], "entities": []}, {"text": "When merging photos, the textual affinity of each unit fora particular event is the maximum affinity for that event among photos in that unit.", "labels": [], "entities": []}, {"text": "The visual affinity of each unit is the average of all affinities for that event among photos in that unit.", "labels": [], "entities": []}, {"text": "The textual and visual similarities of consecutive units are defined in terms of the similarities between the two photos at the units' boundary.", "labels": [], "entities": []}, {"text": "Temporal information for events not aligned well with a particular unit should not influence the objective, so we include temporal scores only for unit-event pairs which have both textual and visual event assignment scores greater than 0.05.", "labels": [], "entities": []}, {"text": "We tune the hyperparameters using grid search on the development set.", "labels": [], "entities": []}, {"text": "In models where the corresponding objective components are included, we set \u03b3 ce = 1, \u03b3 ve = 1, \u03b3 cs = .5, \u03b3 vs = .15, \u03b3 lp = 1, and \u03b3 gp = 4 Q (where Q is the  We evaluate the performance of our model on three tasks.", "labels": [], "entities": []}, {"text": "The first task evaluates the effect of learned temporal knowledge in predicting the correct order of photos in an unseen album (Section 6.1).", "labels": [], "entities": []}, {"text": "The second task evaluates the model's ability to segment albums into logical groupings (Section 6.2).", "labels": [], "entities": []}, {"text": "The third task evaluates the quality of prototypical captions and their use in photo album summarization (Section 6.3).", "labels": [], "entities": [{"text": "photo album summarization", "start_pos": 79, "end_pos": 104, "type": "TASK", "confidence": 0.5269875228404999}]}, {"text": "We evaluate the summaries produced by each method with a human evaluation using Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.8779374857743582}]}, {"text": "We use albums from the test set that contain more than 40 photos for the wedding scenario.", "labels": [], "entities": []}, {"text": "For each album, at random, we present two summaries generated by two algorithms.", "labels": [], "entities": []}, {"text": "AMT workers are instructed to choose the better summary considering both the images and the captions.", "labels": [], "entities": [{"text": "AMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7229172587394714}]}, {"text": "For each comparison of two summaries for an album, we aggregate answers from three workers by majority voting.", "labels": [], "entities": []}, {"text": "We set b = 7.", "labels": [], "entities": []}, {"text": "The number of assigned events in an album, h, varies by album.", "labels": [], "entities": []}, {"text": "As seen in, the summary from the full model is preferred 57.7% of the time compared to the KTH baseline.", "labels": [], "entities": [{"text": "KTH baseline", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8082196414470673}]}, {"text": "The summaries generated using the full model perform slightly better than the summaries from k-MEANS.", "labels": [], "entities": []}, {"text": "We attribute: Example summaries from the wedding, Paris trip, and baby birth scenarios.", "labels": [], "entities": []}, {"text": "In cases where the album had less events than b, the additionally chosen photos are outlined in red.", "labels": [], "entities": []}, {"text": "These photos do not have their caption replaced by a prototypical captions and merely fill out the summary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Temporal ordering pairwise photo results. The metric reported is accuracy, the percentage of time the correct photo", "labels": [], "entities": [{"text": "Temporal ordering pairwise photo", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8871695697307587}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9995766282081604}]}, {"text": " Table 4: Segmentation results for our full model. F1 scores", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9992650151252747}]}, {"text": " Table 5: Ablation study of objective function components", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9718685150146484}]}, {"text": " Table 6: Summarization results. The selection rates indi-", "labels": [], "entities": [{"text": "Summarization", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.7728902101516724}]}, {"text": " Table 7: Captioning results. We evaluate the caption qual-", "labels": [], "entities": [{"text": "Captioning", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9712659120559692}]}]}