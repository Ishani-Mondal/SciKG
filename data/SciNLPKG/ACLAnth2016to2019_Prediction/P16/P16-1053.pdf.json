{"title": [{"text": "A Sentence Interaction Network for Modeling Dependence between Sentences", "labels": [], "entities": [{"text": "Modeling Dependence between Sentences", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.9231880754232407}]}], "abstractContent": [{"text": "Modeling interactions between two sentences is crucial fora number of natural language processing tasks including Answer Selection, Dialogue Act Analysis , etc.", "labels": [], "entities": [{"text": "Modeling interactions between two sentences", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8747339129447937}, {"text": "Answer Selection", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.9477293491363525}, {"text": "Dialogue Act Analysis", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.7412873307863871}]}, {"text": "While deep learning methods like Recurrent Neural Network or Convo-lutional Neural Network have been proved to be powerful for sentence modeling, prior studies paid less attention on interactions between sentences.", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7629168331623077}]}, {"text": "In this work, we propose a Sentence Interaction Network (SIN) for modeling the complex interactions between two sentences.", "labels": [], "entities": []}, {"text": "By introducing \"interaction states\" for word and phrase pairs, SIN is powerful and flexible in capturing sentence interactions for different tasks.", "labels": [], "entities": []}, {"text": "We obtain significant improvements on Answer Selection and Dialogue Act Analysis without any feature engineering .", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.9511576890945435}, {"text": "Dialogue Act Analysis", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.8502195278803507}]}], "introductionContent": [{"text": "There exist complex interactions between sentences in many natural language processing (NLP) tasks such as Answer Selection (, Dialogue Act Analysis, etc.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 107, "end_pos": 123, "type": "TASK", "confidence": 0.929429829120636}, {"text": "Dialogue Act Analysis", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.6875242789586385}]}, {"text": "For instance, given a question and two candidate answers below, though they are all talking about cats, only the first Q What do cats look like?", "labels": [], "entities": []}, {"text": "A1 Cats have large eyes and furry bodies.", "labels": [], "entities": []}, {"text": "A2 Cats like to play with boxes and bags.", "labels": [], "entities": []}, {"text": "answer correctly answers the question about cats' appearance.", "labels": [], "entities": []}, {"text": "It is important to appropriately model the relation between two sentences in such cases.", "labels": [], "entities": []}, {"text": "* Correspondence author For sentence pair modeling, some methods first project the two sentences to fix-sized vectors separately without considering the interactions between them, and then fed the sentence vectors to other classifiers as features fora specific task).", "labels": [], "entities": [{"text": "sentence pair modeling", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7657499810059866}]}, {"text": "Such methods suffer from being unable to encode context information during sentence embedding.", "labels": [], "entities": []}, {"text": "A more reasonable way to capture sentence interactions is to introduce some mechanisms to utilize information from both sentences at the same time.", "labels": [], "entities": []}, {"text": "Some methods attempt to introduce an attention matrix which contains similarity scores between words and phrases to approach sentence interactions.", "labels": [], "entities": []}, {"text": "While the meaning of words and phrases may drift from contexts to contexts, simple similarity scores maybe too weak to capture the complex interactions, and a more powerful interaction mechanism is needed.", "labels": [], "entities": []}, {"text": "In this work, we propose a Sentence Interaction Network (SIN) focusing on modeling sentence interactions.", "labels": [], "entities": []}, {"text": "The main idea behind this model is that each word in one sentence may potentially influence every word in another sentence in some degree (the word \"influence\" here may refer to \"answer\" or \"match\" in different tasks).", "labels": [], "entities": []}, {"text": "So, we introduce a mechanism that allows information to flow from every word (or phrase) in one sentence to every word (or phrase) in another sentence.", "labels": [], "entities": []}, {"text": "These \"information flows\" are real-valued vectors describing how words and phrases interact with each other, for example, a word (or phrase) in one sentence can modify the meaning of a word (or phrase) in another sentence through such \"information flows\".", "labels": [], "entities": []}, {"text": "Specifically, given two sentences s 1 and s 2 , for every word x tin s 1 , we introduce a \"candidate interaction state\" for every word x \u03c4 in s 2 . This state is regarded as the \"influence\" of x \u03c4 to x t , and is actually the \"information flow\" from x \u03c4 to x t mentioned above.", "labels": [], "entities": []}, {"text": "By summing overall the \"candidate interaction states\", we generate an \"interaction state\" for x t , which represents the influence of the whole sentence s 2 to word x t . When feeding the \"interaction state\" and the word embedding together into Recurrent Neural Network (with Long Short-Time Memory unit in our model), we obtain a sentence vector with context information encoded.", "labels": [], "entities": []}, {"text": "We also add a convolution layer on the word embeddings so that interactions between phrases can also be modeled.", "labels": [], "entities": []}, {"text": "SIN is powerful and flexible for modeling sentence interactions in different tasks.", "labels": [], "entities": []}, {"text": "First, the \"interaction state\" is a vector, compared with a single similarity score, it is able to encode more information for word or phrase interactions.", "labels": [], "entities": []}, {"text": "Second, the interaction mechanism in SIN can be adapted to different functions for different tasks during training, such as \"word meaning adjustment\" for Dialogue Act Analysis or \"Answering\" for Answer Selection.", "labels": [], "entities": [{"text": "word meaning adjustment", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.5684003134568533}, {"text": "Dialogue Act Analysis", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7072599728902181}, {"text": "Answering\"", "start_pos": 180, "end_pos": 190, "type": "TASK", "confidence": 0.901140421628952}, {"text": "Answer Selection", "start_pos": 195, "end_pos": 211, "type": "TASK", "confidence": 0.8830198049545288}]}, {"text": "Our main contributions are as follows: \u2022 We propose a Sentence Interaction Network (SIN) which utilizes anew mechanism to model sentence interactions.", "labels": [], "entities": []}, {"text": "\u2022 We add convolution layers to SIN, which improves the ability to model interactions between phrases.", "labels": [], "entities": []}, {"text": "\u2022 We obtain significant improvements on Answer Selection and Dialogue Act Analysis without any handcrafted features.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9528660178184509}, {"text": "Dialogue Act Analysis", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.848447342713674}]}, {"text": "The rest of the paper is structured as follows: We survey related work in Section 2, introduce our method in Section 3, present the experiments in Section 4, and summarize our work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we test our model on two tasks: Answer Selection and Dialogue Act Analysis.", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.9555574059486389}, {"text": "Dialogue Act Analysis", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7607756853103638}]}, {"text": "Both tasks require to model interactions between sentences.", "labels": [], "entities": []}, {"text": "We also conduct auxiliary experiments for analyzing the interaction mechanism in our SIN model.", "labels": [], "entities": []}, {"text": "The WikiQA  correct answers from the development and test set.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.8613367676734924}]}, {"text": "Some statistics are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of WikiQA (Q=Question,  A=Answer)", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.896626889705658}, {"text": "A=Answer", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.870217521985372}]}, {"text": " Table 2: Results on answer selection 4 .", "labels": [], "entities": [{"text": "answer selection", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.9690245091915131}]}, {"text": " Table 3: Dialogue act labels", "labels": [], "entities": []}, {"text": " Table 3.  The same data split as in", "labels": [], "entities": []}]}