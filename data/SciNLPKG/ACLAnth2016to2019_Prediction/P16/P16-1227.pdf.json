{"title": [{"text": "Multimodal Pivots for Image Caption Translation", "labels": [], "entities": [{"text": "Image Caption Translation", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.8671626249949137}]}], "abstractContent": [{"text": "We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space.", "labels": [], "entities": [{"text": "statistical machine translation of image descriptions", "start_pos": 34, "end_pos": 87, "type": "TASK", "confidence": 0.7641003926595052}]}, {"text": "The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7351130247116089}, {"text": "crosslingual reranking of translation outputs", "start_pos": 164, "end_pos": 209, "type": "TASK", "confidence": 0.7932135581970214}]}, {"text": "Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolin-gually captioned images, and on state-of-the-art convolutional neural networks to compute image similarities.", "labels": [], "entities": []}, {"text": "Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9933306574821472}]}], "introductionContent": [{"text": "Multimodal data consisting of images and natural language descriptions (henceforth called captions) are an abundant source of information that has led to a recent surge in research integrating language and vision.", "labels": [], "entities": []}, {"text": "Recently, the aspect of multilinguality has been added to multimodal language processing in a shared task at the WMT16 conference.", "labels": [], "entities": [{"text": "multimodal language processing", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.6208567221959432}, {"text": "WMT16 conference", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.8093718588352203}]}, {"text": "There is clearly also a practical demand for multilingual image captions, e.g., automatic translation of descriptions of art works would allow access to digitized art catalogues across language barriers and is thus of social and cultural interest; multilingual product descriptions are of high commercial interest since they would allow to widen e-commerce transactions automatically to international markets.", "labels": [], "entities": [{"text": "multilingual image captions", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.7272776365280151}, {"text": "automatic translation of descriptions of art works", "start_pos": 80, "end_pos": 130, "type": "TASK", "confidence": 0.8268617221287319}]}, {"text": "However, while datasets of images and monolingual captions already include millions of tuples, the largest multilingual datasets of images and captions known to the authors contain 20,000 () or 30,000 2 triples of images with German and English descriptions.", "labels": [], "entities": []}, {"text": "In this paper, we want to address the problem of multilingual captioning from the perspective of statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 97, "end_pos": 134, "type": "TASK", "confidence": 0.8012613952159882}]}, {"text": "In contrast to prior work on generating captions directly from images (,,, inter alia), our goal is to integrate visual information into an SMT pipeline.", "labels": [], "entities": [{"text": "SMT pipeline", "start_pos": 140, "end_pos": 152, "type": "TASK", "confidence": 0.9287805557250977}]}, {"text": "Visual context provides orthogonal information that is free of the ambiguities of natural language, therefore it serves to disambiguate and to guide the translation process by grounding the translation of a source caption in the accompanying image.", "labels": [], "entities": []}, {"text": "Since datasets consisting of source language captions, images, and target language captions are not available in large quantities, we would instead like to utilize large datasets of images and target-side monolingual captions to improve SMT models trained on modest amounts of parallel captions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 237, "end_pos": 240, "type": "TASK", "confidence": 0.9942553639411926}]}, {"text": "Let the task of caption translation be defined as follows: For production of a target caption e i of an image i, a system may use as input an image caption for image i in the source language f i , as well as the image i itself.", "labels": [], "entities": [{"text": "caption translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.9771633744239807}]}, {"text": "The system may safely assume that f i is relevant to i, i.e., the identification of relevant captions for i () is not itself part of the task of caption translation.", "labels": [], "entities": [{"text": "caption translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8457964360713959}]}, {"text": "In contrast to the inference problem of finding\u00eaing\u02c6ing\u00ea = argmax e p(e|f ) in text-based SMT, multimodal caption translation allows to take into consideration i as well as f i in finding\u00eafinding\u02c6finding\u00ea i : In this paper, we approach caption translation by a general crosslingual reranking framework where fora given pair of source caption and image, monolingual captions in the target language are used to rerank the output of the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.8779111504554749}, {"text": "multimodal caption translation", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.7046506007512411}, {"text": "caption translation", "start_pos": 236, "end_pos": 255, "type": "TASK", "confidence": 0.916276603937149}, {"text": "SMT", "start_pos": 434, "end_pos": 437, "type": "TASK", "confidence": 0.9801958203315735}]}, {"text": "We present two approaches to retrieve target language captions for reranking by pivoting on images that are similar to the input image.", "labels": [], "entities": [{"text": "retrieve target language captions", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.680686704814434}]}, {"text": "One approach calculates image similarity based deep convolutional neural network (CNN) representations.", "labels": [], "entities": []}, {"text": "Another approach calculates similarity in visual space by comparing manually annotated object categories.", "labels": [], "entities": []}, {"text": "We compare the multimodal pivot approaches to reranking approaches that are based on text only, and to standard SMT baselines trained on parallel data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9765169620513916}]}, {"text": "Compared to a strong baseline trained on 29,000 parallel caption data, we find improvements of over 1 BLEU point for reranking based on visual pivots.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9992604851722717}]}, {"text": "Notably, our reranking approach does not rely on large amounts of in-domain parallel data which are not available in practical scenarios such as e-commerce localization.", "labels": [], "entities": []}, {"text": "However, in such scenarios, monolingual product descriptions are naturally given in large amounts, thus our work is a promising pilot study towards real-world caption translation.", "labels": [], "entities": [{"text": "real-world caption translation", "start_pos": 148, "end_pos": 178, "type": "TASK", "confidence": 0.6508968969186147}]}], "datasetContent": [{"text": "The in-domain baseline and TSR-CNN differed in their output in 169 out of 500 cases on the test set.", "labels": [], "entities": [{"text": "TSR-CNN", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.5888795256614685}]}, {"text": "These 169 cases were presented to a human judge alongside the German source captions in a double-blinded pairwise preference ranking experiment.", "labels": [], "entities": [{"text": "German source captions", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.8331105907758077}]}, {"text": "The order of presentation was randomized for the two systems.", "labels": [], "entities": []}, {"text": "The judge was asked to rank fluency and accuracy of the translations independently.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9982777833938599}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "Overall, there was a clear preference for the output of TSR-CNN.", "labels": [], "entities": [{"text": "TSR-CNN", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8787490725517273}]}, {"text": "shows example translations produced by both cdec baselines, TSR-TXT, TSR-CNN, and TSR-HCA, together with source caption, image, and reference translation.", "labels": [], "entities": [{"text": "TSR-TXT", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.8138267397880554}, {"text": "TSR-CNN", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.7835140228271484}, {"text": "TSR-HCA", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.8226737976074219}]}, {"text": "The visual information induced by target side captions of pivot images allows a disambiguation of translation alternatives such as \"skirt\" versus \"rock (music)\" for the German \"Rock\", \"pole\" versus \"mast\" for the German \"Masten\", and is able to repair mistranslations such as \"foot\" instead of \"mouth\" for the German \"Maul\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of images and sentences in  MS COCO image and caption data used in exper- iments.", "labels": [], "entities": [{"text": "MS COCO image and caption data", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.890026698509852}]}, {"text": " Table 2: Parallel and monolingual data used  for training machine translation systems. Sen- tence counts are given for raw data without pre- processing. O/I: both out-of-domain and in- domain system, I: in-domain system only.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6779065281152725}, {"text": "Sen- tence counts", "start_pos": 88, "end_pos": 105, "type": "METRIC", "confidence": 0.924725204706192}]}, {"text": " Table 4: Metric scores for all systems and their  significance levels as reported by Multeval. p o - values are relative to the cdec out-of-domain  baseline, p d -values are relative to the cdec in- domain baseline, p t -values are relative to TSR- TXT and p c -values are relative to TSR-CNN. Best  results are reported in bold face. 15", "labels": [], "entities": [{"text": "Multeval", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8784311413764954}, {"text": "TSR- TXT", "start_pos": 245, "end_pos": 253, "type": "DATASET", "confidence": 0.7937968969345093}, {"text": "TSR-CNN", "start_pos": 286, "end_pos": 293, "type": "DATASET", "confidence": 0.9583477973937988}]}]}