{"title": [{"text": "Scalable Semi-Supervised Query Classification Using Matrix Sketching", "labels": [], "entities": [{"text": "Scalable Semi-Supervised Query Classification", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.6232418045401573}]}], "abstractContent": [{"text": "The enormous scale of unlabeled text available today necessitates scalable schemes for representation learning in natural language processing.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.8978520035743713}]}, {"text": "For instance, in this paper we are interested in classifying the intent of a user query.", "labels": [], "entities": [{"text": "classifying the intent of a user query", "start_pos": 49, "end_pos": 87, "type": "TASK", "confidence": 0.7556291563170296}]}, {"text": "While our labeled data is quite limited, we have access to virtually an unlimited amount of unlabeled queries, which could be used to induce useful representations: for instance by principal component analysis (PCA).", "labels": [], "entities": [{"text": "principal component analysis (PCA)", "start_pos": 181, "end_pos": 215, "type": "TASK", "confidence": 0.7436630129814148}]}, {"text": "However, it is prohibitive to even store the data in memory due to its sheer size, let alone apply conventional batch algorithms.", "labels": [], "entities": []}, {"text": "In this work, we apply the recently proposed matrix sketching algorithm to entirely obviate the problem with scalability (Liberty, 2013).", "labels": [], "entities": [{"text": "matrix sketching", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7866996228694916}, {"text": "Liberty, 2013)", "start_pos": 122, "end_pos": 136, "type": "DATASET", "confidence": 0.9424683153629303}]}, {"text": "This algorithm approximates the data within a specified memory bound while preserving the covariance structure necessary for PCA.", "labels": [], "entities": []}, {"text": "Using matrix sketching, we significantly improve the user intent classification accuracy by leveraging large amounts of unlabeled queries.", "labels": [], "entities": [{"text": "user intent classification", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.5361534357070923}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9474601149559021}]}], "introductionContent": [{"text": "The large amount of high quality unlabeled data available today provides an opportunity to improve performance in tasks with limited supervision through a semi-supervised framework: learn useful representations from the unlabeled data and use them to augment supervised models.", "labels": [], "entities": []}, {"text": "Unfortunately, conventional exact methods are no longer feasible on such data due to scalability issues.", "labels": [], "entities": []}, {"text": "Even algorithms that are considered relatively scalable (e.g., the Lanczos algorithm) for computing eigenvalue decomposition of large sparse matrices) fall apart in this scenario, since the data cannot be stored in the memory of a single machine.", "labels": [], "entities": []}, {"text": "Consequently, approximate methods are needed.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in improving the performance for sentence classification task by leveraging unlabeled data.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.8620455066363016}]}, {"text": "For this task, supervision is precious but the amount of unlabeled sentences is essentially unlimited.", "labels": [], "entities": []}, {"text": "We aim to learn sentence representations from as many unlabeled queries as possible via principal component analysis (PCA): specifically, learn a projection matrix for embedding a bag-of-words vector into a lowdimensional dense feature vector.", "labels": [], "entities": [{"text": "principal component analysis (PCA)", "start_pos": 88, "end_pos": 122, "type": "TASK", "confidence": 0.7350817620754242}]}, {"text": "However, it is not clear how we can compute an effective PCA when we are unable to even store the data in the memory.", "labels": [], "entities": []}, {"text": "Recently, Liberty (2013) proposed a scheme, called matrix sketching, for approximating a matrix while preserving its covariance structure.", "labels": [], "entities": [{"text": "matrix sketching", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7518478333950043}]}, {"text": "This algorithm, given a memory budget, deterministically processes a stream of data points while never exceeding the memory bound.", "labels": [], "entities": []}, {"text": "It does so by occasionally computing singular value decomposition (SVD) on a small matrix.", "labels": [], "entities": [{"text": "singular value decomposition (SVD)", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.7014519224564234}]}, {"text": "Importantly, the algorithm has a theoretical guarantee on the accuracy of the approximated matrix in terms of its covariance structure, which is the key quantity in PCA calculation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9993075132369995}, {"text": "PCA calculation", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.9031330943107605}]}, {"text": "We propose to combine the matrix sketching algorithm with random hashing to completely remove limitations on data sizes.", "labels": [], "entities": [{"text": "matrix sketching", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7535870969295502}]}, {"text": "In experiments, we significantly improve the intent classification accuracy by learning sentence representations from huge amounts of unlabeled sentences, outperforming a strong baseline based on word embeddings trained on 840 billion tokens ().", "labels": [], "entities": [{"text": "intent classification", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.6932942867279053}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9200149774551392}]}], "datasetContent": [{"text": "To test our proposed method, we conduct intent classification experiments) across a suite of 22 domains shown in.", "labels": [], "entities": [{"text": "intent classification", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.6890423148870468}]}, {"text": "An intent is defined as the type of content the user is seeking.", "labels": [], "entities": []}, {"text": "This task is part of the spoken language understanding problem ().", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.657690554857254}]}, {"text": "The amount of training data we used ranges from 12k to 120k (in number of queries) across different domains, the test data was from 2k to 20k.", "labels": [], "entities": []}, {"text": "The number of intents ranges from 5 to 39 per domains.", "labels": [], "entities": []}, {"text": "To learn a PCA projection matrix from the unlabeled data, we collected around 17 billion unlabeled queries from search logs, which give the original data matrix whose columns are bag-of-n-grams vector (up to trigrams) and has dimensions approximately 17 billions by 41 billions, more specifically, X \u2208 R 17,032,086,719\u00d740,986,835,008 We use a much smaller sketching matrix Y \u2208 R 1,000,000\u00d71,000,000 to approximate X.", "labels": [], "entities": []}, {"text": "Note that column size is hashing size.", "labels": [], "entities": []}, {"text": "We parallelized the sketching computation over 1,000 machines; we will call the number of machines parallelized over \"batch\".", "labels": [], "entities": []}, {"text": "In all our experiments, we train a linear multi-class SVM).", "labels": [], "entities": []}, {"text": "shows the performance of intent classification across domains.", "labels": [], "entities": [{"text": "intent classification", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7304284870624542}]}, {"text": "For the baseline, SVM without embedding (w/o Embed) achieved 91.99% accuracy, which is already very competitive.", "labels": [], "entities": [{"text": "SVM", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9144884347915649}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9994547963142395}]}, {"text": "However, the models with word embedding trained on w/o Embed 6B-50d 840B-300d 6 billion tokens (6B-50d) and 840 billion tokens (840B-300d) () achieved 92.89% and 93.00%, respectively.", "labels": [], "entities": []}, {"text": "50d and 300d denote size of embedding dimension.", "labels": [], "entities": []}, {"text": "To use word embeddings as a sentence representation, we simply use averaged word vectors over a sentence, normalized and conjoined with the original representation as in.", "labels": [], "entities": []}, {"text": "Surprisingly, when we use sentence representation (SENT) induced from the sketching method with our data set, we can boost the performance up to 93.49%, corresponding to a 18.78% decrease in error relative to a SVM without representation.", "labels": [], "entities": [{"text": "error", "start_pos": 191, "end_pos": 196, "type": "METRIC", "confidence": 0.9801368713378906}]}, {"text": "Also, we see that the extended sentence representation (SENT+) can get additional gains.", "labels": [], "entities": [{"text": "SENT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.7785991430282593}]}], "tableCaptions": [{"text": " Table 1: Performance comparison between different embeddings style.", "labels": [], "entities": []}, {"text": " Table 2: Performance for selected domains as the number of unlabeled data increases.", "labels": [], "entities": []}]}