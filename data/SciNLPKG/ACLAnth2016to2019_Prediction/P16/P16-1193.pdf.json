{"title": [{"text": "A Vector Space for Distributional Semantics for Entailment *", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional semantics creates vector-space representations that capture many forms of semantic similarity, but their relation to semantic entailment has been less clear.", "labels": [], "entities": []}, {"text": "We propose a vector-space model which provides a formal foundation fora distributional semantics of entailment.", "labels": [], "entities": []}, {"text": "Using a mean-field approximation, we develop approximate inference procedures and entailment operators over vectors of probabilities of features being known (ver-sus unknown).", "labels": [], "entities": []}, {"text": "We use this framework to reinterpret an existing distributional-semantic model (Word2Vec) as approximating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations.", "labels": [], "entities": []}, {"text": "In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previous results.", "labels": [], "entities": [{"text": "hyponymy detection", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8793783187866211}]}], "introductionContent": [{"text": "Modelling entailment is a fundamental issue in computational semantics.", "labels": [], "entities": [{"text": "Modelling entailment", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9304251074790955}]}, {"text": "It is also important for many applications, for example to produce abstract summaries or to answer questions from text, where we need to ensure that the input text entails the output text.", "labels": [], "entities": []}, {"text": "There has been a lot of interest in modelling entailment in a vector-space, but most of this work takes an empirical, often ad-hoc, approach to this problem, and achieving good results has been difficult (.", "labels": [], "entities": []}, {"text": "In this work, we propose anew framework for modelling entailment in a vector-space, and illustrate its effective- * This work was partially supported by French ANR grant CIFRE N 1324/2014.: Pattern of logical entailment between nothing known (unk), two different features f and g known, and the complement off (\u00acf ) known.", "labels": [], "entities": [{"text": "French ANR grant CIFRE N 1324/2014.", "start_pos": 153, "end_pos": 188, "type": "DATASET", "confidence": 0.8324096575379372}]}, {"text": "ness with a distributional-semantic model of hyponymy detection.", "labels": [], "entities": [{"text": "hyponymy detection", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7950232625007629}]}, {"text": "Unlike previous vector-space models of entailment, the proposed framework explicitly models what information is unknown.", "labels": [], "entities": []}, {"text": "This is a crucial property, because entailment reflects what information is and is not known; a representation y entails a representation x if and only if everything that is known given x is also known given y.", "labels": [], "entities": []}, {"text": "Thus, we model entailment in a vector space where each dimension represents something we might know.", "labels": [], "entities": []}, {"text": "As illustrated in, knowing that a feature f is true always entails knowing that same feature, but never entails knowing that a different feature g is true.", "labels": [], "entities": []}, {"text": "Also, knowing that a feature is true always entails not knowing anything (unk), since strictly less information is still entailment, but the reverse is never true.", "labels": [], "entities": []}, {"text": "Table 1 also illustrates that knowing that a feature f is false (\u00acf ) patterns exactly the same way as knowing that an unrelated feature g is true.", "labels": [], "entities": []}, {"text": "This illustrates that the relevant dichotomy for entailment is known versus unknown, and not true versus false.", "labels": [], "entities": []}, {"text": "Previous vector-space models have been very successful at modelling semantic similarity, in particular using distributional semantic models (e.g. ().", "labels": [], "entities": []}, {"text": "Distributional semantics uses the distributions of words in contexts to induce vectorspace embeddings of words, which have been shown to be useful fora wide variety of tasks.", "labels": [], "entities": [{"text": "Distributional semantics", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7645106613636017}]}, {"text": "Two words are predicted to be similar if the dot product between their vectors is high.", "labels": [], "entities": []}, {"text": "But the dot product is an anti-symmetric operator, which makes it more natural to interpret these vectors as representing whether features are true or false, whereas the dichotomy known versus unknown is asymmetric.", "labels": [], "entities": []}, {"text": "We surmise that this is why distributional semantic models have had difficulty modelling lexical entailment ().", "labels": [], "entities": []}, {"text": "To develop a vector-space model of whether features are known or unknown, we start with discrete binary vectors, where 1 means known and 0 means unknown.", "labels": [], "entities": []}, {"text": "Entailment between these discrete binary vectors can be calculated by independently checking each dimension.", "labels": [], "entities": []}, {"text": "But as soon as we try to do calculations with distributions over these vectors, we need to deal with the case where the features are not independent.", "labels": [], "entities": []}, {"text": "For example, if feature f has a 50% chance of being true and a 50% chance of being false, we can't assume that there is a 25% chance that both f and \u00acf are known.", "labels": [], "entities": []}, {"text": "This simple case of mutual exclusion is just one example of a wide range of constraints between features which we need to handle in semantic models.", "labels": [], "entities": []}, {"text": "These constraints mean that the different dimensions of our vector space are not independent, and therefore exact models are not factorised.", "labels": [], "entities": []}, {"text": "Because the models are not factorised, exact calculations of entailment and exact inference of vectors are intractable.", "labels": [], "entities": []}, {"text": "Mean-field approximations area popular approach to efficient inference for intractable models.", "labels": [], "entities": [{"text": "Mean-field approximations", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7966864109039307}]}, {"text": "Ina mean-field approximation, distributions over binary vectors are represented using a single probability for each dimension.", "labels": [], "entities": []}, {"text": "These vectors of real values are the basis of our proposed vector space for entailment.", "labels": [], "entities": []}, {"text": "In this work, we propose a vector-space model which provides a formal foundation fora distributional semantics of entailment.", "labels": [], "entities": []}, {"text": "This framework is derived from a mean-field approximation to entailment between binary vectors, and includes operators for measuring entailment between vectors, and procedures for inferring vectors in an entailment graph.", "labels": [], "entities": []}, {"text": "We validate this framework by using it to reinterpret existing) word embedding vectors as approximating an entailment-based model of the distribution of words in contexts.", "labels": [], "entities": []}, {"text": "This reinterpretation allows us to use existing word embeddings as an unsupervised model of lexical entailment, successfully predicting hyponymy relations using the proposed entailment operators in both unsupervised and semi-supervised experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate whether the proposed framework is an effective model of entailment in vector spaces, we apply the interpretations from Section 3 to publicly available word embeddings and use them to predict the hyponymy relations in a benchmark dataset.", "labels": [], "entities": []}, {"text": "This framework predicts that the more accurate interpretations of Word2Vec result in more accurate unsupervised models of hyponymy.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9408789873123169}]}, {"text": "We evaluate on detecting hyponymy relations between words because hyponymy is the canonical type of lexical entailment; most of the semantic features of a hypernym (e.g. \"animal\") must be included in the semantic features of the hyponym (e.g. \"dog\").", "labels": [], "entities": []}, {"text": "We evaluate in both a fully unsupervised setup and a semi-supervised setup.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pattern of logical entailment between  nothing known (unk), two different features f and  g known, and the complement of f (\u00acf ) known.", "labels": [], "entities": []}, {"text": " Table 3: Accuracies on the BLESS data from  Weeds et al. (2014), for hyponymy detection (50%  Acc) and hyponymy direction classification (Dir  Acc), in the unsupervised (upper box) and semi- supervised (lower box) experiments. For unsuper- vised accuracies, * marks a significant difference  with the previous row.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9981918931007385}, {"text": "BLESS data from  Weeds et al. (2014)", "start_pos": 28, "end_pos": 64, "type": "DATASET", "confidence": 0.9341494679450989}, {"text": "hyponymy detection", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.6992265284061432}, {"text": "Acc", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9819462895393372}, {"text": "hyponymy direction classification", "start_pos": 104, "end_pos": 137, "type": "TASK", "confidence": 0.701050877571106}, {"text": "Dir  Acc)", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9370899597803751}]}]}