{"title": [], "abstractContent": [{"text": "Character-based and word-based methods are two main types of statistical models for Chinese word segmentation, the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model, with the advantages that word-level features can be easily utilized.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.6041555901368459}]}, {"text": "Neural models have been exploited for character-based Chi-nese word segmentation, giving high accuracies by making use of external character embeddings, yet requiring less feature engineering.", "labels": [], "entities": [{"text": "character-based Chi-nese word segmentation", "start_pos": 38, "end_pos": 80, "type": "TASK", "confidence": 0.5769167989492416}, {"text": "accuracies", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9660907983779907}]}, {"text": "In this paper, we study a neu-ral model for word-based Chinese word segmentation, by replacing the manually-designed discrete features with neural features in a word-based segmentation framework.", "labels": [], "entities": [{"text": "word-based Chinese word segmentation", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.5609002485871315}]}, {"text": "Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature , and a further combination of discrete and neural features gives top accuracies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical word segmentation methods can be categorized character-based) and word-based approaches.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7222289144992828}]}, {"text": "The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7252575010061264}, {"text": "sequence labeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6857312321662903}]}, {"text": "The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features.", "labels": [], "entities": []}, {"text": "An influential character-based word segmentation model () uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a word, respectively, employing conditional random field (CRF) to model the correspondence between the input character sequence and output label sequence.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.71629399061203}]}, {"text": "For each character, features are extracted from a five-character context window and a twolabel history window.", "labels": [], "entities": []}, {"text": "Subsequent work explores different label sets (), feature sets (Shi and and semi-supervised learning (, reporting state-of-the-art accuracies.", "labels": [], "entities": []}, {"text": "Recently, neural network models have been investigated for the character tagging approach.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9323086738586426}]}, {"text": "The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks.", "labels": [], "entities": []}, {"text": "In particular, convolution neural network 1 (, tensor neural network (), recursive neural network) and longshort-term-memory (LSTM) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models.", "labels": [], "entities": [{"text": "neural network segmentation", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.7028981248537699}]}, {"text": "Since it is challenging to integrate word features to the CRF inference framework of the existing step action buffer(\u00b7 \u00b7 \u00b7 w\u22121w0) queue(c0c1 Figure 2: Segmentation process of \"\u4e2d \u56fd (Chinese) \u5916 \u4f01 (foreign company) \u4e1a \u52a1 (business) \u53d1\u5c55 (develop) \u8fc5\u901f (quickly)\".", "labels": [], "entities": []}, {"text": "character-based methods, we take inspiration from word-based discrete segmentation instead.", "labels": [], "entities": [{"text": "word-based discrete segmentation", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.6418749690055847}]}, {"text": "In particular, we follow, using the transition-based framework to decode a sentence from left-to-right incrementally, scoring partially segmented results using both character-level and word-level features.", "labels": [], "entities": []}, {"text": "Beam-search is applied to reduce error propagation and large-margin training with early-update () is used for learning from inexact search.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.6625028401613235}, {"text": "early-update", "start_pos": 82, "end_pos": 94, "type": "METRIC", "confidence": 0.9640926718711853}]}, {"text": "We replace the discrete word and character features of with word and character embeddings, respectively, and change their linear model into a deep neural network.", "labels": [], "entities": []}, {"text": "Following and, we use convolution neural networks to achieve local feature combination and LSTM to learn global sentence-level features, respectively.", "labels": [], "entities": []}, {"text": "The resulting model is a word-based neural segmenter that can leverage rich embedding features.", "labels": [], "entities": [{"text": "word-based neural segmenter", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6123048365116119}]}, {"text": "Its correlation with existing work on Chinese segmentation is shown in.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.7150377929210663}]}, {"text": "Results on standard benchmark datasets show the effectiveness of word embedding features for neural segmentation.", "labels": [], "entities": [{"text": "neural segmentation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7395497858524323}]}, {"text": "Our method achieves stateof-the-art results without any preprocess based on external knowledge such as Chinese idioms of and.", "labels": [], "entities": []}, {"text": "We release our code under GPL for research reference.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 5: Main results on CTB60 test dataset.", "labels": [], "entities": [{"text": "CTB60 test dataset", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.9800549348195394}]}, {"text": " Table 6: Main results on PKU and MSR test  datasets.", "labels": [], "entities": [{"text": "PKU and MSR test  datasets", "start_pos": 26, "end_pos": 52, "type": "DATASET", "confidence": 0.7506704688072204}]}]}