{"title": [{"text": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations", "labels": [], "entities": [{"text": "Sentence Representations", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8245333135128021}]}], "abstractContent": [{"text": "We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of high-quality sentence embeddings.", "labels": [], "entities": [{"text": "Siamese Continuous Bag of Words (Siamese CBOW)", "start_pos": 15, "end_pos": 61, "type": "TASK", "confidence": 0.6320916579829322}]}, {"text": "Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings.", "labels": [], "entities": [{"text": "Averaging the embeddings of words in a sentence", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7891903072595596}]}, {"text": "However, word em-beddings trained with the methods currently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.7530205547809601}]}, {"text": "Siamese CBOW handles this problem by training word em-beddings directly for the purpose of being averaged.", "labels": [], "entities": []}, {"text": "The underlying neural network learns word embeddings by predicting , from a sentence representation, its surrounding sentences.", "labels": [], "entities": []}, {"text": "We show the ro-bustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings have proven to be beneficial in a variety of tasks in NLP such as machine translation (, parsing), semantic search (, and tracking the meaning of words and concepts overtime ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7842620313167572}, {"text": "parsing)", "start_pos": 105, "end_pos": 113, "type": "TASK", "confidence": 0.8870531618595123}, {"text": "semantic search", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.8015378415584564}, {"text": "tracking the meaning of words and concepts overtime", "start_pos": 138, "end_pos": 189, "type": "TASK", "confidence": 0.8346250280737877}]}, {"text": "It is not evident, however, how word embeddings should be combined to represent larger pieces of text, like sentences, paragraphs or documents.", "labels": [], "entities": []}, {"text": "Surprisingly, simply averaging word embeddings of all words in a text has proven to be a strong baseline or feature across a multitude of tasks.", "labels": [], "entities": []}, {"text": "Word embeddings, however, are not optimized specifically for representing sentences.", "labels": [], "entities": []}, {"text": "In this paper we present a model for obtaining word embeddings that are tailored specifically for the task of averaging them.", "labels": [], "entities": []}, {"text": "We do this by directly including a comparison of sentence embeddings-the averaged embeddings of the words they contain-in the cost function of our network.", "labels": [], "entities": []}, {"text": "Word embeddings are typically trained in a fast and scalable way from unlabeled training data.", "labels": [], "entities": []}, {"text": "As the training data is unlabeled, word embeddings are usually not task-specific.", "labels": [], "entities": []}, {"text": "Rather, word embeddings trained on a large training corpus, like the ones from) are employed across different tasks).", "labels": [], "entities": []}, {"text": "These two qualities-(i) being trainable from large quantities of unlabeled data in a reasonable amount of time, and (ii) robust performance across different tasks-are highly desirable and allow word embeddings to be used in many large-scale applications.", "labels": [], "entities": []}, {"text": "In this work we aim to optimize word embeddings for sentence representations in the same manner.", "labels": [], "entities": []}, {"text": "We want to produce general purpose sentence embeddings that should score robustly across multiple test sets, and we want to leverage large amounts of unlabeled training material.", "labels": [], "entities": []}, {"text": "In the word2vec algorithm, Mikolov et al.", "labels": [], "entities": []}, {"text": "(2013a) construe a supervised training criterion for obtaining word embeddings from unsupervised data, by predicting, for every word, its surrounding words.", "labels": [], "entities": []}, {"text": "We apply this strategy at the sentence level, where we aim to predict a sentence from its adjacent sentences (.", "labels": [], "entities": []}, {"text": "This allows us to use unlabeled training data, which is easy to obtain; the only restriction is that documents need to be split into sentences and that the order between sentences is preserved.", "labels": [], "entities": []}, {"text": "The main research question we address is whether directly optimizing word embeddings for the task of being averaged to produce sentence embeddings leads to word embeddings that are better suited for this task than word2vec does.", "labels": [], "entities": []}, {"text": "Therefore, we test the embeddings in an unsupervised learning scenario.", "labels": [], "entities": []}, {"text": "We use 20 evaluation sets that stem from a wide variety of sources (newswire, video descriptions, dictionary descriptions, microblog posts", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the efficacy of our siamese network for producing sentence embeddings we use multiple  test sets.", "labels": [], "entities": []}, {"text": "We use Siamese CBOW to learn word embeddings from an unlabeled corpus.", "labels": [], "entities": []}, {"text": "For every sentence pair in the test sets, we compute two sentence representations by averaging the word embeddings of each sentence.", "labels": [], "entities": []}, {"text": "Words that are missing from the vocabulary and, hence, have no word embedding, are omitted.", "labels": [], "entities": []}, {"text": "The cosine similarity between the two sentence vectors is produced as a final semantic similarity score.", "labels": [], "entities": []}, {"text": "As we want a clean way to directly evaluate the embeddings on multiple sets we train our model and the models we compare with on exactly the same training data.", "labels": [], "entities": []}, {"text": "We do not compute extra features, perform extra preprocessing steps or incorporate the embeddings in supervised training schemes.", "labels": [], "entities": []}, {"text": "Additional steps like these are very likely to improve evaluation scores, but they would obscure our main evaluation purpose in this paper, which is to directly test the embeddings.", "labels": [], "entities": []}, {"text": "We use 20 SemEval datasets from the SemEval semantic textual similarity task in), which consist of sentence pairs from a wide array of sources (e.g., newswire, tweets, video descriptions) that have been manually annotated by multiple human assessors on a 5 point scale (1: semantically unrelated, 5: semantically similar).", "labels": [], "entities": [{"text": "SemEval semantic textual similarity task", "start_pos": 36, "end_pos": 76, "type": "TASK", "confidence": 0.8060067296028137}]}, {"text": "In the ground truth, the final similarity score for every sentence pair is the mean of the annotator judgements, and as such can be a floating point number like 2.685.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.8835133612155914}]}, {"text": "The evaluation metric used by SemEval, and hence by us, is Pearson's r.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.6138766606648763}]}, {"text": "As Spearman's r is often reported as well, we do so too.", "labels": [], "entities": [{"text": "Spearman's r", "start_pos": 3, "end_pos": 15, "type": "METRIC", "confidence": 0.6668555537859598}]}, {"text": "Statistical significance To see whether Siamese CBOW yields significantly different scores for the same input sentence pairs from word2vec CBOW-the method it is theoretically most similar to-we compute Wilcoxon signed-rank test statistics between all runs on all evaluation sets.", "labels": [], "entities": []}, {"text": "Runs are considered statistically significantly different for p-values < 0.0001.", "labels": [], "entities": []}, {"text": "In, the results of Siamese CBOW on 20 SemEval datasets are displayed, together with the results of the baseline systems.", "labels": [], "entities": [{"text": "Siamese CBOW on 20 SemEval datasets", "start_pos": 19, "end_pos": 54, "type": "DATASET", "confidence": 0.686720202366511}]}, {"text": "As we can see from the table, Siamese CBOW outperforms the baselines in the majority of cases.", "labels": [], "entities": [{"text": "Siamese CBOW", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.5707436949014664}]}, {"text": "The very low scores of skip-thought on MSRpar appear to be a glitch, which we will ignore.", "labels": [], "entities": [{"text": "MSRpar", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.8549550771713257}]}, {"text": "It is interesting to see that for the set with the highest average sentence length (2013 SMT, with 24.7 words per sentence on average) Siamese CBOW is very close to skip-thought, the best performing baseline.", "labels": [], "entities": [{"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.43841949105262756}, {"text": "Siamese CBOW", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.5594424903392792}]}, {"text": "In terms of lexical term overlap, unsurprisingly, all methods have trouble with the sets with little overlap answers-forums, which both have 7% lexical overlap).", "labels": [], "entities": []}, {"text": "It is interesting to see, however, that for the next two sets (2015 belief and 2012 MSRpar, 11% and 14% overlap respectively) Siamese CBOW manages to get the best performance.", "labels": [], "entities": [{"text": "2012 MSRpar", "start_pos": 79, "end_pos": 90, "type": "DATASET", "confidence": 0.46685564517974854}, {"text": "Siamese CBOW", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.7011168897151947}]}, {"text": "The highest performance on all sets is 0.7315 Pearson's r of Siamese CBOW on the 2014 tweet-news set.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.8554112911224365}, {"text": "Siamese CBOW on the 2014 tweet-news set", "start_pos": 61, "end_pos": 100, "type": "DATASET", "confidence": 0.8603623168809074}]}, {"text": "This figure is not very far from the best performing SemEval run that year which has 0.792 Pearson's r.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.6134010255336761}]}, {"text": "This is remarkable as Siamese CBOW is completely unsupervised, while the NTNU system which scored best on this set () was optimized using multiple training sets.", "labels": [], "entities": [{"text": "Siamese CBOW", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.6574710011482239}, {"text": "NTNU", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9127856492996216}]}, {"text": "In recent work, present FastSent, a model similar to ours (see \u00a75 fora more elaborate discussion); results are not reported for all evaluation sets we use, and hence, we compare the results of FastSent and Siamese CBOW separately, in.", "labels": [], "entities": []}, {"text": "FastSent and Siamese CBOW each outperform the other on half of the evaluation sets, which clearly suggests that the differences between the two methods are complementary.", "labels": [], "entities": [{"text": "FastSent", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8625270128250122}]}], "tableCaptions": [{"text": " Table 3: Time spent per method on all 20 SemEval  datasets, 17,608 sentence pairs, and the average  time spent on a single sentence pair (time in sec- onds unless indicated otherwise).  20 sets  1 pair", "labels": [], "entities": [{"text": "SemEval  datasets", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.7953953742980957}]}]}