{"title": [{"text": "Active Learning for Dependency Parsing with Partial Annotation", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.612159252166748}]}], "abstractContent": [{"text": "Different from traditional active learning based on sentence-wise full annotation (FA), this paper proposes active learning with dependency-wise partial annotation (PA) as a finer-grained unit for dependency parsing.", "labels": [], "entities": [{"text": "dependency-wise partial annotation (PA)", "start_pos": 129, "end_pos": 168, "type": "METRIC", "confidence": 0.618651752670606}, {"text": "dependency parsing", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.7936853468418121}]}, {"text": "At each iteration, we select a few most uncertain words from an unlabeled data pool, manually annotate their syntactic heads, and add the partial trees into labeled data for parser retraining.", "labels": [], "entities": [{"text": "parser retraining", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.9018556475639343}]}, {"text": "Compared with sentence-wise FA, dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence.", "labels": [], "entities": []}, {"text": "Our work makes the following contributions.", "labels": [], "entities": []}, {"text": "First, we are the first to apply a probabilistic model to active learning for dependency parsing, which can 1) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics, and 2) directly learn parameters from PA based on a forest-based training objective.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8227298557758331}]}, {"text": "Second, we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English.", "labels": [], "entities": []}, {"text": "Finally, we conduct human annotation experiments to compare FA and PA on real annotation time and quality.", "labels": [], "entities": [{"text": "FA", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.996992826461792}, {"text": "PA", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9898832440376282}]}], "introductionContent": [{"text": "During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks ().", "labels": [], "entities": [{"text": "supervised dependency parsing", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.578318456808726}]}, {"text": "However, the * Correspondence author.", "labels": [], "entities": []}, {"text": "$ 0 I 1 saw 2 Sarah 3 with 4 a 5 telescope 6 Figure 1: A partially annotated sentence, where only the heads of \"saw\" and \"with\" are decided.", "labels": [], "entities": []}, {"text": "upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques.", "labels": [], "entities": []}, {"text": "Meanwhile, previous research on out-of-domain dependency parsing gains little success (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.6743690818548203}]}, {"text": "A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre.", "labels": [], "entities": [{"text": "open-domain parsing", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.6819021999835968}]}, {"text": "Recently, several small-scale treebanks on web texts have been built for study and evaluation).", "labels": [], "entities": []}, {"text": "Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models.", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.6557613253593445}]}, {"text": "Traditionally, AL utilizes full annotation (FA) for parsing (, where a whole syntactic tree is annotated fora given sentence at a time.", "labels": [], "entities": [{"text": "full annotation (FA", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.520738773047924}, {"text": "parsing", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.9648739099502563}]}, {"text": "However, as commented by, the annotation process is complex, slow, and prone to mistakes when FA is required.", "labels": [], "entities": [{"text": "FA", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9961196184158325}]}, {"text": "Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models.", "labels": [], "entities": []}, {"text": "Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing.", "labels": [], "entities": [{"text": "AL", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9495666027069092}, {"text": "partial annotation (PA)", "start_pos": 64, "end_pos": 87, "type": "METRIC", "confidence": 0.7763591170310974}, {"text": "dependency parsing", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8110353946685791}]}, {"text": "They find that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate.", "labels": [], "entities": []}, {"text": "Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives.", "labels": [], "entities": []}, {"text": "(1) This is the first work that applies a stateof-the-art probabilistic parsing model to AL for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8236218392848969}]}, {"text": "The CRF-based dependency parser on the one hand allows us to use probabilities of trees or marginal probabilities of single dependencies for uncertainty measurement, and on the other hand can directly learn parameters from partially annotated trees.", "labels": [], "entities": [{"text": "CRF-based dependency parser", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.5401624937852224}]}, {"text": "Using probabilistic models maybe ubiquitous in AL for relatively simpler tasks like classification and sequence labeling, but is definitely novel for dependency parsing which is dominated by linear models with perceptron-like training.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.6404535174369812}, {"text": "dependency parsing", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.8468407392501831}]}, {"text": "(2) Based on the CRF-based parser, we make systematic comparison among several uncertainty metrics for both FA and PA.", "labels": [], "entities": [{"text": "FA", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.992360532283783}, {"text": "PA", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.8243432641029358}]}, {"text": "Simulation experiments show that compared with using FA, AL with PA can greatly reduce annotation effort in terms of dependency number by 62.2% on Chinese and by 74.2% on English.", "labels": [], "entities": [{"text": "FA", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9218000173568726}]}, {"text": "(3) We build a visualized annotation platform and conduct human annotation experiments to compare FA and PA on real annotation time and quality, where we obtain several interesting observations and conclusions.", "labels": [], "entities": [{"text": "FA", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9841390252113342}, {"text": "PA", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.8758379817008972}]}, {"text": "All codes, along with the data from human annotation experiments, are released at http: //hlt.suda.edu.cn/ \u02dc zhli for future research study.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Chinese Penn Treebank 5.1 (CTB ) for Chinese and Penn Treebank (PTB ) for English.", "labels": [], "entities": [{"text": "Penn Treebank 5.1 (CTB )", "start_pos": 15, "end_pos": 39, "type": "DATASET", "confidence": 0.9229146440823873}, {"text": "Penn Treebank (PTB )", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.9586296796798706}]}, {"text": "For both datasets, we follow the standard data split, and convert original bracketed structures into dependency structures using Penn2Malt with its default head-finding rules.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 129, "end_pos": 138, "type": "DATASET", "confidence": 0.9811439514160156}]}, {"text": "To be more realistic, we use automatic part-of-speech (POS) tags produced by a state-of-the-art CRF-based tagger (94.1% on CTB -test, and 97.2% on PTB -test, nfold jackknifing on training data), since POS tags encode much syntactic annotation.", "labels": [], "entities": [{"text": "PTB -test", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.7107873558998108}]}, {"text": "Because AL experiments need to train many parsing models, we throw out all training sentences longer than 50 to speedup our experiments.", "labels": [], "entities": []}, {"text": "Following previous practice on AL with PA (, we adopt the following AL settings for both Chinese and English . The first 500 training sentences are used as the seed labeled data L.", "labels": [], "entities": []}, {"text": "In the case of FA, K = 500 new sentences are selected and annotated at each iteration.", "labels": [], "entities": [{"text": "FA", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9677063822746277}]}, {"text": "In the case of single dependency-wise PA, we select and annotate M = 10, 000 dependencies, which roughly correspond to 500 sentences considering that the averaged sentence length is about 22.3 in CTB -train and 23.2 in PTB -train.", "labels": [], "entities": [{"text": "CTB -train", "start_pos": 196, "end_pos": 206, "type": "DATASET", "confidence": 0.9001809159914652}, {"text": "PTB -train", "start_pos": 219, "end_pos": 229, "type": "DATASET", "confidence": 0.9379556179046631}]}, {"text": "In the case of batch dependency-wise PA, we set K = 500, and r = 20% for Chinese and r = 10% for English, considering that the parser trained on all data achieves about 80% and 90% accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 181, "end_pos": 191, "type": "METRIC", "confidence": 0.9618865847587585}]}, {"text": "We measure parsing performance using the standard unlabeled attachment score (UAS) including punctuation marks.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 50, "end_pos": 82, "type": "METRIC", "confidence": 0.7908454686403275}]}, {"text": "Please note that we always treat punctuation marks as ordinary words when selecting annotation tasks and calculating UAS, in order to make fair comparison between FA and PA.", "labels": [], "entities": [{"text": "UAS", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.4183969497680664}, {"text": "FA", "start_pos": 163, "end_pos": 165, "type": "METRIC", "confidence": 0.9962064027786255}, {"text": "PA", "start_pos": 170, "end_pos": 172, "type": "METRIC", "confidence": 0.7965046763420105}]}, {"text": "4.1 FA vs. Single Dependency-wise PA First, we make comparison on the performance of AL with FA and with single dependency-wise PA.", "labels": [], "entities": [{"text": "FA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9885908961296082}, {"text": "FA", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9782575964927673}]}, {"text": "Results on Chinese are shown in.", "labels": [], "entities": [{"text": "Chinese", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8931031227111816}]}, {"text": "Following previous work, we use the number of annotated dependencies (x-axis) as the annotation cost in order to fairly compare FA and PA.", "labels": [], "entities": [{"text": "FA", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.9966018199920654}, {"text": "PA", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.989793062210083}]}, {"text": "We use FA with random selection as a baseline.", "labels": [], "entities": [{"text": "FA", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9964703321456909}]}, {"text": "We also draw the accuracy of the CRF-based parser trained on all training data, which can be regarded as the upper bound.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9997268319129944}]}, {"text": "For FA, the curve of the normalized tree score intertwines with that of random selection.", "labels": [], "entities": [{"text": "FA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.6115219593048096}]}, {"text": "Meanwhile, the performance of normalized tree probability is very close to that of averaged marginal probability, and both are clearly superior to the baseline with random selection.", "labels": [], "entities": []}, {"text": "For PA, the difference among the three uncertainty metrics is small.", "labels": [], "entities": [{"text": "PA", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9303951263427734}]}, {"text": "The marginal probability gap clearly outperforms the other two metrics before 50, 000 annotated dependencies, and remains   very competitive at all other points.", "labels": [], "entities": []}, {"text": "The marginal probability max achieves best peak UAS, and even outperforms the parser trained on all data, which can be explained by small disturbance during complex model training.", "labels": [], "entities": [{"text": "UAS", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9802541136741638}]}, {"text": "The marginal probability entropy, although being the most complex metric among the three, seems inferior all the time.", "labels": [], "entities": []}, {"text": "It is clear that using PA can greatly reduce annotation effort compared with using FA in terms of annotated dependencies.", "labels": [], "entities": [{"text": "FA", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.8699693083763123}]}, {"text": "Results on English are shown in.", "labels": [], "entities": [{"text": "English", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9472187161445618}]}, {"text": "The overall findings are similar to those in, except that the distinction among different methods is more clear.", "labels": [], "entities": []}, {"text": "For FA, normalized tree score is consistently better than the random baseline.", "labels": [], "entities": [{"text": "FA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9770068526268005}, {"text": "normalized tree score", "start_pos": 8, "end_pos": 29, "type": "METRIC", "confidence": 0.7108610272407532}]}, {"text": "Normalized tree probability always outperforms normalized tree score.", "labels": [], "entities": []}, {"text": "Averaged marginal probability performs best, except being slightly inferior to normalized tree probability in earlier stages.", "labels": [], "entities": []}, {"text": "For PA, it is consistent that marginal probability gap is better than marginal probability max, and marginal probability entropy is the worst.", "labels": [], "entities": [{"text": "PA", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9753918647766113}]}, {"text": "In summary, based on the results on the de-   908,154 = 10% on English, to reach the same performance with parsers trained on all data.", "labels": [], "entities": []}, {"text": "Moreover, the PA methods converges much faster than the FA ones, since for the same x-axis number, much more sentences (with partial trees) are used as training data for AL with PA than FA.", "labels": [], "entities": []}, {"text": "So far, we measure annotation effort in terms of the number of annotated dependencies and assume that it takes the same amount of time to annotate different words, which is obviously unrealistic.", "labels": [], "entities": []}, {"text": "To understand whether active learning based on PA can really reduce annotation time over based on FA in practice, we build a web browser based annotation system, 10 and conduct human annotation experiments on Chinese.", "labels": [], "entities": [{"text": "FA", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9954828023910522}]}, {"text": "In this part, we use CTB 7.0 which is a newer and larger version and covers more genres, and adopt the newly proposed Stanford dependencies () which are more understandable for annotators.", "labels": [], "entities": [{"text": "CTB 7.0", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.910213053226471}]}, {"text": "Since manual syntactic annotation is very difficult and time-consuming, we only keep sentences with length in order to better measure annotation time by focusing on sentences of reasonable length, which leave us 12, 912 training sentences under the official data split.", "labels": [], "entities": []}, {"text": "Then, we use a random half of training sentences to train a CRF-based parser, and select 20% most uncertain words with marginal probability gap for each sentence of the left half.", "labels": [], "entities": []}, {"text": "We employ 6 postgraduate students as our annotators who are at different levels of familiarity in syntactic annotation.", "labels": [], "entities": []}, {"text": "Before annotation, the annotators are trained for about two hours by introducing the basic concepts, guidelines, and illustrating examples.", "labels": [], "entities": []}, {"text": "Then, they are asked to practice on the annotation system for about another two hours.", "labels": [], "entities": []}, {"text": "Finally, all annotators are required to  formally annotate the same 100 sentences.", "labels": [], "entities": []}, {"text": "The system is programed that each sentence has 3 FA submissions and 3 PA submissions.", "labels": [], "entities": [{"text": "FA", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9966501593589783}, {"text": "PA", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9974192380905151}]}, {"text": "During formal annotation, the annotators are not allowed to discuss with each other or lookup any guideline or documents, which may incur unnecessary inaccuracy in timing.", "labels": [], "entities": []}, {"text": "Instead, the annotators can only decide the syntactic structures based on the basic knowledge of dependency grammar and one's understanding of the sentence structure.", "labels": [], "entities": []}, {"text": "The annotation process lasts for about 5 hours.", "labels": [], "entities": []}, {"text": "On average, each annotator completes 50 sentences with FA (763 dependencies) and 50 sentences with PA (178 dependencies).", "labels": [], "entities": [{"text": "FA", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9991154074668884}, {"text": "PA", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9950055480003357}]}, {"text": "lists the results in descending order of an annotator's experience in syntactic annotation.", "labels": [], "entities": []}, {"text": "The first two columns compare the time needed for annotating a dependency in seconds.", "labels": [], "entities": []}, {"text": "On average, annotating a dependency in PA takes about twice as much time as in FA, which is reasonable considering the words to be annotated in PA maybe more difficult for annotators while the annotation of some tasks in FA maybe very trivial and easy.", "labels": [], "entities": []}, {"text": "Combined with the results in, we may infer that to achieve 77.3% accuracy on CTB -test, AL with FA requires 149, 051 \u00d7 6.7 = 998, 641.7 seconds of annotation, whereas AL with batch dependency-wise PA needs 56, 389 \u00d7 13.6 = 766, 890.4 seconds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9991599321365356}, {"text": "CTB -test", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.6493327716986338}, {"text": "FA", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9982220530509949}]}, {"text": "Thus, we may roughly say that AL with PA can reduce annotation time over FA by 998,641.7 = 23.2%.", "labels": [], "entities": [{"text": "annotation time", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.9305086433887482}, {"text": "FA", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9745299816131592}]}, {"text": "We also report annotation accuracy according to the gold-standard Stanford dependencies converted from bracketed structures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9752650856971741}]}, {"text": "Overall, the accuracy of FA is 70.36 \u2212 59.06 = 11.30% higher An anonymous reviewer commented that the direct comparison between an annotator's performance on PA and FA based on accuracy maybe misleading since the FA and PA sentences for one annotator are mutually exclusive. than that of PA, which should be due to the trivial tasks in FA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996160268783569}, {"text": "FA", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9806705713272095}, {"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9966933727264404}]}, {"text": "To be more fair, we compare the accuracies of FA and PA on the same 20% selected difficult words, and find that annotators exhibit different responses to the switch.", "labels": [], "entities": [{"text": "FA", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.980981707572937}, {"text": "PA", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9894005656242371}]}, {"text": "Annotator #4 achieve 12.58% higher accuracy when under PA than under FA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9992977380752563}, {"text": "PA", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9119217991828918}, {"text": "FA", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.9920748472213745}]}, {"text": "The reason maybe that under PA, annotators can be more focused and therefore perform better on the few selected tasks.", "labels": [], "entities": []}, {"text": "In contrast, some annotators may perform better under FA.", "labels": [], "entities": [{"text": "FA", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9924452900886536}]}, {"text": "For example, annotation accuracy of annotator #2 increases by 10.04% when switching from PA to FA, which maybe due to that FA allows annotators to spend more time on the same sentence and gain help from annotating easier tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9528233408927917}, {"text": "FA", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9611126780509949}]}, {"text": "Overall, we find that the accuracy of PA is 59.06 \u2212 57.28 = 1.78% higher than that of FA, indicating that PA actually can improve annotation quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9997630715370178}, {"text": "PA", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9898515939712524}, {"text": "FA", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9982841610908508}]}], "tableCaptions": [{"text": " Table 2: Results on test data.", "labels": [], "entities": []}, {"text": " Table 3: Statistics of human annotation.", "labels": [], "entities": []}]}