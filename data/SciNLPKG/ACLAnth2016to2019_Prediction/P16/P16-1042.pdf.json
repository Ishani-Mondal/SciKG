{"title": [{"text": "Combining Natural Logic and Shallow Reasoning for Question Answering", "labels": [], "entities": [{"text": "Combining Natural Logic", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7579669157663981}, {"text": "Question Answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7598809003829956}]}], "abstractContent": [{"text": "Broad domain question answering is often difficult in the absence of structured knowledge bases, and can benefit from shallow lexical methods (broad coverage) and logical reasoning (high precision).", "labels": [], "entities": [{"text": "Broad domain question answering", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.744743600487709}, {"text": "precision", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9926504492759705}]}, {"text": "We propose an approach for incorporating both of these signals in a unified framework based on natural logic.", "labels": [], "entities": []}, {"text": "We extend the breadth of inferences afforded by natural logic to include relational entailment (e.g., buy \u2192 own) and meronymy (e.g., a person born in a city is born the city's country).", "labels": [], "entities": []}, {"text": "Furthermore, we train an evaluation function-akin to gameplaying-to evaluate the expected truth of candidate premises on the fly.", "labels": [], "entities": []}, {"text": "We evaluate our approach on answering multiple choice science questions, achieving strong results on the dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering is an important task in NLP, and becomes both more important and more difficult when the answers are not supported by handcurated knowledge bases.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9247870445251465}]}, {"text": "In these cases, viewing question answering as textual entailment over a very large premise set can offer a means of generalizing reliably to open domain questions.", "labels": [], "entities": [{"text": "question answering as textual entailment", "start_pos": 24, "end_pos": 64, "type": "TASK", "confidence": 0.8210018157958985}]}, {"text": "A natural approach to textual entailment is to treat it as a logical entailment problem.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7311033010482788}]}, {"text": "However, this high-precision approach is not feasible in cases where a formal proof is difficult or impossible.", "labels": [], "entities": []}, {"text": "For example, consider the following hypothesis (H) and its supporting premise (P) for the question Which part of a plant produces the seeds?: P: Ovaries are the female part of the flower, which produces eggs that are needed for making seeds.", "labels": [], "entities": []}, {"text": "H: A flower produces the seeds.", "labels": [], "entities": []}, {"text": "This requires a relatively large amount of inference: the most natural atomic fact in the sentence is that ovaries produce eggs.", "labels": [], "entities": []}, {"text": "These inferences are feasible in a limited domain, but become difficult the more open-domain reasoning they require.", "labels": [], "entities": []}, {"text": "In contrast, even a simple lexical overlap classifier could correctly predict the entailment.", "labels": [], "entities": []}, {"text": "In fact, such a bag-of-words entailment model has been shown to be surprisingly effective on the Recognizing Textual Entailment (RTE) challenges).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.655392105380694}]}, {"text": "On the other hand, such methods are also notorious for ignoring even trivial cases of nonentailment that are easy for natural logic, e.g., recognizing negation in the example below: P: Eating candy for dinner is an example of a poor health habit.", "labels": [], "entities": []}, {"text": "H: Eating candy is an example of a good health habit.", "labels": [], "entities": []}, {"text": "We present an approach to leverage the benefits of both methods.", "labels": [], "entities": []}, {"text": "Natural logic -a proof theory over the syntax of natural language -offers a framework for logical inference which is already familiar to lexical methods.", "labels": [], "entities": []}, {"text": "As an inference system searches fora valid premise, the candidates it explores can be evaluated on their similarity to a premise by a conventional lexical classifier.", "labels": [], "entities": []}, {"text": "We therefore extend a natural logic inference engine in two key ways: first, we handle relational entailment and meronymy, increasing the total number of inferences that can be made.", "labels": [], "entities": []}, {"text": "We further implement an evaluation function which quickly provides an estimate for how likely a candidate premise is to be supported by the knowledge base, without running the full search.", "labels": [], "entities": []}, {"text": "This can then more easily match a known premise despite still not matching exactly.", "labels": [], "entities": []}, {"text": "We present the following contributions: (1) we extend the classes of inferences NaturalLI can perform on real-world sentences by incorporating relational entailment and meronymy, and by operat-ing over dependency trees; (2) we augment NaturalLI with an evaluation function to provide an estimate of entailment for any query; and (3) we run our system over the Aristo science questions corpus, achieving the strong results.", "labels": [], "entities": [{"text": "Aristo science questions corpus", "start_pos": 360, "end_pos": 391, "type": "DATASET", "confidence": 0.6671591401100159}]}], "datasetContent": [{"text": "There are many cases -particularly as the length of the premise and the hypothesis grow -where despite our improvements NaturalLI will fail to find any supporting premises; for example: P: Food serves mainly for growth, energy and body repair, maintenance and protection.", "labels": [], "entities": [{"text": "NaturalLI", "start_pos": 120, "end_pos": 129, "type": "DATASET", "confidence": 0.8657020926475525}, {"text": "body repair", "start_pos": 231, "end_pos": 242, "type": "TASK", "confidence": 0.7284671068191528}]}, {"text": "H: Animals get energy for growth and repair from food.", "labels": [], "entities": []}, {"text": "In addition to requiring reasoning with multiple implicit premises (a concomitant weak point of natural logic), a correct interpretation of the sentence requires fairly nontrivial nonlocal reasoning: Food serves mainly for x \u2192 Animals get x from food.", "labels": [], "entities": []}, {"text": "Nonetheless, there enough lexical clues in the sentence that even a simple entailment classifier would get the example correct.", "labels": [], "entities": []}, {"text": "We build such a classifier and adapt it as an evaluation function inside NaturalLI in case no premises are found during search.", "labels": [], "entities": []}, {"text": "A version of the classifier constructed in Section 4.1, but over keywords rather than keyphrases can be incorporated directly into NaturalLI's search to give a score for each candidate premise Heat energy is being transferred when a stove is used to boil water in a pan.", "labels": [], "entities": []}, {"text": "When you heat water on a stove, thermal energy is transferred.: An illustration of an alignment between a premise and a hypothesis.", "labels": [], "entities": []}, {"text": "Keyphrases can be multiple words (e.g., heat energy), and can be approximately matched (e.g., to thermal energy).", "labels": [], "entities": []}, {"text": "In the premise, used, boil and pan are unaligned.", "labels": [], "entities": [{"text": "pan", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9529765844345093}]}, {"text": "Note that heat water is incorrectly tagged as a compound noun. visited.", "labels": [], "entities": []}, {"text": "This can bethought of as analogous to the evaluation function in game-playing search -even though an agent cannot play a game of Chess to completion, at some depth it can apply an evaluation function to its leaf states.", "labels": [], "entities": []}, {"text": "Using keywords rather than keyphrases is in general a hindrance to the fuzzy alignments the system can produce.", "labels": [], "entities": []}, {"text": "Importantly though, this allows the feature values to be computed incrementally as the search progresses, based on the score of the parent state and the mutation or deletion being performed.", "labels": [], "entities": []}, {"text": "For instance, if we are deleting a word which was previously aligned perfectly to the premise, we would subtract the weight fora perfect and imperfect alignment, and add the weight for an unaligned premise keyphrase.", "labels": [], "entities": []}, {"text": "This has the same effect as applying the trained classifier to the new state, and uses the same weights learned for this classifier, but requires substantially less computation.", "labels": [], "entities": []}, {"text": "In addition to finding entailments from candidate premises, our system also allows us to encode a notion of likely negation.", "labels": [], "entities": []}, {"text": "We can consider the following two statements na\u00a8\u0131velyna\u00a8\u0131vely sharing every keyword.", "labels": [], "entities": []}, {"text": "Each token marked with its polarity: However, we note that all of the keyword pairs are in opposite polarity contexts.", "labels": [], "entities": []}, {"text": "We can therefore define a pair of keywords as matching in NaturalLI if the following two conditions hold: (1) their lemmatized surface forms match exactly, and (2) they have the same polarity in the sentence.", "labels": [], "entities": []}, {"text": "The second constraint encodes a good approximation for negation.", "labels": [], "entities": [{"text": "negation", "start_pos": 55, "end_pos": 63, "type": "TASK", "confidence": 0.9661580324172974}]}, {"text": "To illustrate, consider the polarity signatures of common operators:  We evaluate our entailment system on the Regents Science Exam portion of the Aristo dataset.", "labels": [], "entities": [{"text": "Regents Science Exam portion", "start_pos": 111, "end_pos": 139, "type": "DATASET", "confidence": 0.9201850146055222}, {"text": "Aristo dataset", "start_pos": 147, "end_pos": 161, "type": "DATASET", "confidence": 0.842033863067627}]}, {"text": "The dataset consists of a collection of multiple-choice science questions from the New York Regents 4 th Grade Science Exams.", "labels": [], "entities": [{"text": "New York Regents 4 th Grade Science Exams", "start_pos": 83, "end_pos": 124, "type": "DATASET", "confidence": 0.6505678556859493}]}, {"text": "Each multiple choice option is translated to a candidate hypotheses.", "labels": [], "entities": []}, {"text": "A large corpus is given as a knowledge base; the task is to find support in this knowledge base for the hypothesis.", "labels": [], "entities": []}, {"text": "Our system is in many ways well-suited to the dataset.", "labels": [], "entities": []}, {"text": "Although certainly many of the facts require complex reasoning (see Section 6.4), the majority can be answered from a single premise.", "labels": [], "entities": []}, {"text": "Unlike FraCaS or the RTE challenges, however, the task does not have explicit premises to run inference from, but rather must infer the truth of the hypothesis from a large collection of supporting text.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.9475299715995789}]}, {"text": "We present results on the Aristo dataset in, alongside prior work and strong baselines.", "labels": [], "entities": [{"text": "Aristo dataset", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.9505782723426819}]}, {"text": "In all cases, NaturalLI is run with the evaluation function enabled; the limited size of the text corpus and the complexity of the questions would cause the basic NaturalLI system to perform poorly.", "labels": [], "entities": []}, {"text": "The test set for this corpus consists of only 68 examples, and therefore both perceived large differences in model scores and the apparent best system should be interpreted cautiously.", "labels": [], "entities": []}, {"text": "NaturalLI consistently achieves the best training accuracy, and is more stable between configurations on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9901004433631897}]}, {"text": "For instance, it maybe consistently discarding lexically similar but actually contradictory premises that often confuse some subset of the baselines.", "labels": [], "entities": []}, {"text": "KNOWBOT is the dialog system presented in  variants of the system: held-out is the system's performance when it is not allowed to use the dialog collected from humans for the example it is answering; oracle is the full system.", "labels": [], "entities": [{"text": "KNOWBOT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6978098750114441}]}, {"text": "Note that the oracle variant is a human-in-the-loop system.", "labels": [], "entities": []}, {"text": "We additionally present three baselines.", "labels": [], "entities": []}, {"text": "The first simply uses Solr's IR confidence to rank entailment (Solr Only in).", "labels": [], "entities": [{"text": "IR confidence", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.841215968132019}]}, {"text": "The max IR score of any premise given a hypothesis is taken as the score for that hypothesis.", "labels": [], "entities": [{"text": "max IR score", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.8267271518707275}]}, {"text": "Furthermore, we report results for the entailment classifier defined in Section 4.1 (Classifier), optionally including the Solr score as a feature.", "labels": [], "entities": []}, {"text": "We also report performance of the evaluation function in NaturalLI applied directly to the premise and hypothesis, without any inference (Evaluation Function).", "labels": [], "entities": []}, {"text": "Last, we evaluate NaturalLI with the improvements presented in this paper (NaturalLI in Table 1).", "labels": [], "entities": []}, {"text": "We additionally tune weights on our training set fora simple model combination with (1) Solr (with weight 6:1 for NaturalLI) and (2) the standalone classifier (with weight 24:1 for NaturalLI).", "labels": [], "entities": [{"text": "Solr", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9409024119377136}]}, {"text": "Empirically, both parameters were observed to be fairly robust.", "labels": [], "entities": []}, {"text": "To  an associated 500 example training set (and 249 example development set).", "labels": [], "entities": []}, {"text": "These are substantially more difficult as they contain afar larger number of questions that require an understanding of a more complex process.", "labels": [], "entities": []}, {"text": "Nonetheless, the trend illustrated in holds for this larger set, as shown in.", "labels": [], "entities": []}, {"text": "Note that with a web-scale corpus, accuracy of an IR-based system can be pushed up to 51.4%; a PMI-based solver, in turn, achieves an accuracy of 54.8% -admittedly higher than our best system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999430239200592}, {"text": "PMI-based solver", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.5460430830717087}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.998599112033844}]}, {"text": "An interesting avenue of future work would be to run NaturalLI over such a large web-scale corpus, and to incorporate PMI-based statistics into the evaluation function.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on the Aristo science questions  dataset. All NaturalLI runs include the evalua- tion function. Results are reported using only the  Barron's study guide or SCITEXT as the support- ing KNOWBOT is the dialog system presented in  Hixon et. al (2015). The held-out version uses ad- ditional facts from other question's dialogs; the or- acle version made use of human input on the ques- tion it was answering. The test set did not exist at  the time KNOWBOT was published.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9531756043434143}, {"text": "Aristo science questions  dataset", "start_pos": 26, "end_pos": 59, "type": "DATASET", "confidence": 0.7191549241542816}, {"text": "Barron's study guide", "start_pos": 152, "end_pos": 172, "type": "DATASET", "confidence": 0.9784034639596939}, {"text": "Hixon et. al (2015)", "start_pos": 247, "end_pos": 266, "type": "DATASET", "confidence": 0.8722895085811615}]}, {"text": " Table 2: Results of our baselines and NaturalLI on  a larger dataset of 250 examples. All NaturalLI  runs include the evaluation function.", "labels": [], "entities": []}]}