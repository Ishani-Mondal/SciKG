{"title": [{"text": "MUSEEC: A Multilingual Text Summarization Tool", "labels": [], "entities": [{"text": "Multilingual Text Summarization Tool", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6739576905965805}]}], "abstractContent": [{"text": "The MUSEEC (MUltilingual SEntence Extraction and Compression) summariza-tion tool implements several extractive summarization techniques-at the level of complete and compressed sentences-that can be applied, with some minor adaptations , to documents in multiple languages.", "labels": [], "entities": [{"text": "MUSEEC (MUltilingual SEntence Extraction and Compression) summariza-tion", "start_pos": 4, "end_pos": 76, "type": "TASK", "confidence": 0.7344025572141012}]}, {"text": "The current version of MUSEEC provides the following summarization methods: (1) MUSE-a supervised summa-rizer, based on a genetic algorithm (GA), that ranks document sentences and extracts top-ranking sentences into a summary , (2) POLY-an unsupervised sum-marizer, based on linear programming (LP), that selects the best extract of document sentences, and (3) WECOM-an un-supervised extension of POLY that compiles a document summary from compressed sentences.", "labels": [], "entities": []}, {"text": "In this paper, we provide an overview of MUSEEC methods and its architecture in general.", "labels": [], "entities": [{"text": "MUSEEC", "start_pos": 41, "end_pos": 47, "type": "TASK", "confidence": 0.730232834815979}]}], "introductionContent": [{"text": "High quality summaries can significantly reduce the information overload of many professionals in a variety of fields.", "labels": [], "entities": []}, {"text": "Moreover, the publication of information on the Internet in an ever-increasing variety of languages dictates the importance of developing multi-lingual summarization tools that can be readily applied to documents in multiple languages.", "labels": [], "entities": []}, {"text": "There is a distinction between extractive summarization that is aimed at the selection of a subset of the most relevant fragments -mostly complete sentences -from a source text, and abstractive summarization that generates a summary as a reformulated synopsis expressing the main idea of the input documents.", "labels": [], "entities": []}, {"text": "Unlike the abstractive summarization methods, which require natural language processing operations, language-independent summarizers work in an extractive manner, usually via ranking fragments of a summarized text by a relevance score and selecting the top-ranked fragments (e.g., sentences) into a summary.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.5398435294628143}]}, {"text": "Because sentence scoring methods, like MUSE (MUltilingual Sentence Extractor), use a greedy approach, they cannot necessarily find the best extract out of all possible combinations of sentences.", "labels": [], "entities": [{"text": "sentence scoring", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7429023683071136}, {"text": "MUltilingual Sentence Extractor)", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.668713666498661}]}, {"text": "Another approach, based on the maximum coverage principle, tries to find the best subset of extracted sentences.", "labels": [], "entities": []}, {"text": "This problem is known as NPhard (), but an approximate solution can be found by the POLY algorithm) in polynomial time.", "labels": [], "entities": [{"text": "POLY", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9349951148033142}]}, {"text": "Given the tight length constraints, extractive systems that select entire sentences are quite limited in the quality of summaries they can produce.", "labels": [], "entities": []}, {"text": "Compressive summarization seeks to overcome this limitation by compiling summaries from compressed sentences that are composed of strictly relevant information).", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.769858181476593}]}, {"text": "WE-COM (Weighted COMpression) summarization approach () combines methods for term weighting and sentence compression into a weighted compression model.", "labels": [], "entities": [{"text": "WE-COM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6065715551376343}, {"text": "Weighted COMpression) summarization", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.44967663288116455}, {"text": "term weighting", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.6938643455505371}, {"text": "sentence compression", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.6968584209680557}]}, {"text": "WECOM extends POLY by utilizing the choice of POLY's objective functions for the term-weighting model.", "labels": [], "entities": []}, {"text": "In this paper, we present MUSEEC, a multilingual text summarization platform, which currently implements three single-document summarization algorithms: MUSE), POLY algorithm (, and WECOM ().", "labels": [], "entities": [{"text": "text summarization", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.6837867945432663}, {"text": "POLY algorithm", "start_pos": 160, "end_pos": 174, "type": "METRIC", "confidence": 0.9464497864246368}]}], "datasetContent": [{"text": "WECOM was evaluated in) on three different datasets) using automated and human experiments.", "labels": [], "entities": [{"text": "WECOM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.803023636341095}]}, {"text": "Both automated and human scores have shown that compression significantly improves the quality of generated summaries.", "labels": [], "entities": []}, {"text": "Table 5 contains results for POLY and WECOM summarizers on the DUC 2002 dataset.", "labels": [], "entities": [{"text": "POLY", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9336338639259338}, {"text": "WECOM", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.8277055621147156}, {"text": "DUC 2002 dataset", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9815155863761902}]}, {"text": "Statistical testing (using a paired T-test) showed that there is a significant improvement in ROUGE-1 recall between ILP concept-based extraction method of and WECOM with weights generated by Gillick and Favre's method.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9833528995513916}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.8361924886703491}, {"text": "ILP concept-based extraction", "start_pos": 117, "end_pos": 145, "type": "TASK", "confidence": 0.6479355990886688}, {"text": "WECOM", "start_pos": 160, "end_pos": 165, "type": "DATASET", "confidence": 0.5324703454971313}]}, {"text": "Another significant improvement is between ILP extraction method of and WECOM with weights generated by McDonald's method.", "labels": [], "entities": [{"text": "ILP extraction", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.836889773607254}, {"text": "WECOM", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.6247706413269043}]}, {"text": "Practical running times for MUSE (summarization) and POLY are tens of milliseconds per a text document of a few thousand words.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.5914971232414246}, {"text": "summarization)", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7626904845237732}, {"text": "POLY", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9469732046127319}]}, {"text": "WECOM running time is strictly dependent on the running time of dependency parsing performed by Stanford CoreNLP package, which takes 2 \u2212 3 seconds per sentence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7550396621227264}, {"text": "Stanford CoreNLP package", "start_pos": 96, "end_pos": 120, "type": "DATASET", "confidence": 0.9026072025299072}]}, {"text": "Given pre-saved pre-processing results, WECOM takes tens of milliseconds per document as well.", "labels": [], "entities": [{"text": "WECOM", "start_pos": 40, "end_pos": 45, "type": "TASK", "confidence": 0.4359840154647827}]}], "tableCaptions": [{"text": " Table 2: MSS task. English.", "labels": [], "entities": [{"text": "MSS task", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.6567533612251282}]}, {"text": " Table 3: MSS task. Hebrew.", "labels": [], "entities": [{"text": "MSS task. Hebrew.", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.8660900712013244}]}, {"text": " Table 4: MSS task. Arabic.", "labels": [], "entities": [{"text": "MSS task", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.6803337335586548}]}, {"text": " Table 5: ROUGE-1 and -2 scores. DUC 2002.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9927821159362793}, {"text": "DUC 2002.", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.953055759270986}]}]}