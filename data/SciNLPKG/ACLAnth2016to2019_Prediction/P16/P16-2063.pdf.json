{"title": [], "abstractContent": [{"text": "This paper studies the effect of limited precision data representation and computation on word embeddings.", "labels": [], "entities": []}, {"text": "We present a systematic evaluation of word embeddings with limited memory and discuss methods that directly train the limited precision representation with limited memory.", "labels": [], "entities": []}, {"text": "Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks.", "labels": [], "entities": [{"text": "word/phrase similarity and dependency parsing tasks", "start_pos": 131, "end_pos": 182, "type": "TASK", "confidence": 0.6943731009960175}]}], "introductionContent": [{"text": "There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of NLP tasks.", "labels": [], "entities": []}, {"text": "Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, NLP applications.", "labels": [], "entities": []}, {"text": "However, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory.", "labels": [], "entities": []}, {"text": "For example, for 1 million words, loading 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system.", "labels": [], "entities": []}, {"text": "Considering applications that make use of billions of tokens and multiple languages, size issues impose significant limitations on the practical use of word embeddings.", "labels": [], "entities": []}, {"text": "This paper presents the question of whether it is possible to significantly reduce the memory needs for the use and training of word embeddings.", "labels": [], "entities": []}, {"text": "Specifically, we ask \"what is the impact of representing each dimension of a dense representation with significantly fewer bits than the standard 64 bits?\"", "labels": [], "entities": []}, {"text": "Moreover, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used.", "labels": [], "entities": []}, {"text": "The results we present are quite surprising.", "labels": [], "entities": []}, {"text": "We show that it is possible to reduce the memory consumption by an order of magnitude both when word embeddings are being used and in training.", "labels": [], "entities": []}, {"text": "In the first case, as we show, simply truncating the resulting representations after training and using a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits.", "labels": [], "entities": []}, {"text": "Moreover, we provide two ways to train existing algorithms () when the memory is limited during training and show that, here, too, an order of magnitude saving in memory is possible without degrading performance.", "labels": [], "entities": []}, {"text": "We conduct comprehensive experiments on existing word and phrase similarity and relatedness datasets as well as on dependency parsing, to evaluate these results.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.800077497959137}]}, {"text": "Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space required by a factor of 16.", "labels": [], "entities": []}, {"text": "The truncated word embeddings are available from the papers web page at https://cogcomp.cs.illinois.", "labels": [], "entities": []}, {"text": "edu/page/publication_view/790.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings.", "labels": [], "entities": []}, {"text": "We train the word embedding algorithms, word2vec), based on the Oct. 2013 Wikipedia dump.", "labels": [], "entities": [{"text": "Wikipedia dump", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.8089707791805267}]}, {"text": "We first compare levels of truncation of word2vec embeddings, and then evaluate the stochastic rounding and the auxiliary vectors based methods for training word2vec vectors.", "labels": [], "entities": []}, {"text": "We use multiple test datasets as follows.", "labels": [], "entities": []}, {"text": "Word similarity datasets have been widely used to evaluate word embedding results.", "labels": [], "entities": []}, {"text": "We use the datasets summarized by): wordsim-353, wordsim-sim, wordsim-rel, MC-30, RG-65, MTurk-287, MTurk-771, MEN 3000, YP-130, Rare-Word, Verb-143, and SimLex-999.", "labels": [], "entities": [{"text": "MTurk-287", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.8803872466087341}, {"text": "MTurk-771", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.8691691756248474}]}, {"text": "We compute the similarities between pairs of words and check the Spearman's rank correlation coefficient ( between the computer and the human labeled ranks.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 65, "end_pos": 104, "type": "METRIC", "confidence": 0.595375120639801}]}, {"text": "We use the paraphrase (bigram) datasets used in (, ppdb all, bigrams vn, bigrams nn, and bigrams jnn, to test whether the truncation affects phrase level embedding.", "labels": [], "entities": []}, {"text": "Our phrase level embedding is based on the average of the words inside each phrase.", "labels": [], "entities": []}, {"text": "Note that it is also easy to incorporate our truncation methods into existing phrase embedding algorithms.", "labels": [], "entities": []}, {"text": "We follow () in using cosine similarity to evaluate the correlation between the computed similarity and annotated similarity between paraphrases.", "labels": [], "entities": []}, {"text": "We also incorporate word embedding results into a downstream task, dependency parsing, to evaluate whether the truncated embedding results are still good features compared to the original features.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7961232662200928}]}, {"text": "We follow the setup of () in a monolingual setting 3 . We train the parser with 5,000 iterations using different truncation settings for word2vec embedding.", "labels": [], "entities": []}, {"text": "The data used to train and evaluate the parser is the English data in the CoNLL-X shared task).", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 74, "end_pos": 93, "type": "DATASET", "confidence": 0.8054423133532206}]}, {"text": "We follow () in using the labeled attachment score (LAS) to evaluate the different parsing results.", "labels": [], "entities": [{"text": "labeled attachment score (LAS)", "start_pos": 26, "end_pos": 56, "type": "METRIC", "confidence": 0.8437320093313853}]}, {"text": "Here we only show the word embedding results for 200 dimensions, since empirically we found 25-dimension results were not as stable as 200 dimensions.", "labels": [], "entities": []}, {"text": "The results shown in for dependency parsing are consistent with word similarity and paraphrasing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.861903190612793}]}, {"text": "First, we see that binarization for CBOW and skipgram is again better than the truncation approach.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7091990113258362}]}, {"text": "Second, for truncation results, more bits leads to better results.", "labels": [], "entities": []}, {"text": "With 8-bits, we can again obtain results similar to those obtained from the original word2vec embedding.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The detailed average results for word similarity and paraphrases of Fig. 1.", "labels": [], "entities": [{"text": "Fig. 1", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.9256719052791595}]}, {"text": " Table 2: Comparing the training CBOW  models: We set the average value of the original  word2vec embeddings to be 1, and the values in  the table are relative to the original embeddings  baselines. \"avg. (w.)\" represents the average  values of all word similarity datasets. \"avg. (b.)\"  represents the average values of all bigram phrase  similarity datasets. \"Stoch. (16 b.)\" represents  the method using stochastic rounding applied  to 16-bit precision. \"Trunc. (8 b.)\" represents  the method using truncation with 8-bit auxiliary  update vectors applied to 8-bit precision.", "labels": [], "entities": [{"text": "Trunc", "start_pos": 458, "end_pos": 463, "type": "METRIC", "confidence": 0.9603821635246277}]}, {"text": " Table 3: Evaluation results for dependency  parsing (in LAS).  Bits  CBOW Skipgram  Original 88.58% 88.15%  Binary 89.25% 88.41%  4-bits  87.56% 86.46%  6-bits  88.62% 87.98%  8-bits  88.63% 88.16%", "labels": [], "entities": [{"text": "dependency  parsing", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8437728881835938}, {"text": "CBOW Skipgram  Original 88.58", "start_pos": 70, "end_pos": 99, "type": "DATASET", "confidence": 0.7825040072202682}]}]}