{"title": [{"text": "Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.5855003496011099}]}], "abstractContent": [{"text": "Recently, many neural network models have been applied to Chinese word seg-mentation.", "labels": [], "entities": []}, {"text": "However, such models focus more on collecting local information while long distance dependencies are not well learned.", "labels": [], "entities": []}, {"text": "To integrate local features with long distance dependencies, we propose a dependency-based gated recursive neural network.", "labels": [], "entities": []}, {"text": "Local features are first collected by bi-directional long short term memory network, then combined and refined to long distance dependencies via gated re-cursive neural network.", "labels": [], "entities": []}, {"text": "Experimental results show that our model is a competitive model for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6229525506496429}]}], "introductionContent": [{"text": "Word segmentation is an important pre-process step in Chinese language processing.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6965684592723846}, {"text": "Chinese language processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6514752904574076}]}, {"text": "Most widely used approaches treat Chinese word segmentation (CWS) task as a sequence labeling problem in which each character in the input sequence is assigned with a tag.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS) task", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.8059627626623426}]}, {"text": "Many previous approaches have been effectively applied to CWS problem.", "labels": [], "entities": [{"text": "CWS problem", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.9519473314285278}]}, {"text": "However, these approaches incorporated many handcrafted features, thus restricting the generalization ability of these models.", "labels": [], "entities": []}, {"text": "Neural network models have the advantage of minimizing the effort in feature engineering.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7001002281904221}]}, {"text": "(2011) developed a general neural network architecture for sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.773581067721049}]}, {"text": "Following this work, neural network approaches have been well studied and widely applied to CWS task with good results (.", "labels": [], "entities": [{"text": "CWS task", "start_pos": 92, "end_pos": 100, "type": "TASK", "confidence": 0.9101390838623047}]}, {"text": "The character \"\u9762\" is labeled as \"E\" (end of word) in the top sentence while labeled as \"B\" (begin of word) in the bottom one even though \"\u9762\" has the same adjacent characters, \"\u5730\" and \"\u79ef\".", "labels": [], "entities": []}, {"text": "However, these models focus more on collecting local features while long distance dependencies are not well learned.", "labels": [], "entities": []}, {"text": "In fact, relying on the information of adjacent words is not enough for CWS task.", "labels": [], "entities": [{"text": "CWS", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9625166654586792}]}, {"text": "An example is shown in.", "labels": [], "entities": []}, {"text": "The character \"\u9762\" has different tags in two sentences, even with the same adjacent characters, \" \u5730\" and \" \u79ef\".", "labels": [], "entities": []}, {"text": "Only long distance dependencies can help the model recognize tag correctly in this example.", "labels": [], "entities": []}, {"text": "Thus, long distance information is an important factor for CWS task.", "labels": [], "entities": [{"text": "CWS task", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.8838241994380951}]}, {"text": "The main limitation of chain structure for sequence labeling is that long distance dependencies decay inevitably.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.5654800683259964}]}, {"text": "Though forget gate mechanism is added, it is difficult for bi-directional long short term memory network (Bi-LSTM), a kind of chain structure, to avoid this problem.", "labels": [], "entities": []}, {"text": "In general, tree structure works better than chain structure to model long term information.", "labels": [], "entities": []}, {"text": "Therefore, we use gated recursive neural network (GRNN) which is a kind of tree structure to capture long distance dependencies.", "labels": [], "entities": []}, {"text": "Motivated by the fact, we propose the dependency-based gated recursive neural network (DGRNN) to integrate local features with long dis-tance dependencies.", "labels": [], "entities": []}, {"text": "shows the structure of DGRNN.", "labels": [], "entities": [{"text": "DGRNN", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.7312166690826416}]}, {"text": "First of all, local features are collected by Bi-LSTM.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.909351646900177}]}, {"text": "Secondly, GRNN recursively combines and refines local features to capture long distance dependencies.", "labels": [], "entities": []}, {"text": "Finally, with the help of local features and long distance dependencies, our model generates the probability of the tag of word.", "labels": [], "entities": []}, {"text": "The main contributions of the paper are as follows: \u2022 We present the dependency-based gated recursive neural network to combine local features with long distance dependencies.", "labels": [], "entities": []}, {"text": "\u2022 To verify the effectiveness of the proposed approach, we conduct experiments on three widely used datasets.", "labels": [], "entities": []}, {"text": "Our proposed model achieves the best performance compared with other state-of-the-art approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first compare our model with baseline methods, Bi-LSTM and GRNN on three datasets.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.8193784356117249}, {"text": "GRNN", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.6101571917533875}]}, {"text": "The results evaluated by F-score (F 1 score) are reported in.", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9984799027442932}, {"text": "F 1 score)", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9677449762821198}]}, {"text": "First, the output of Bi-LSTM is concatenated to a vector.", "labels": [], "entities": []}, {"text": "Second, softmax layer takes the vector as input and generates each tag probability.: Comparisons for DGRNN and state-ofthe-art non-neural network approaches on F-score.", "labels": [], "entities": []}, {"text": "The structure of GRNN is recursive.", "labels": [], "entities": []}, {"text": "GRNN combines adjacent word vectors to the more abstract representation in bottom-up way.", "labels": [], "entities": [{"text": "GRNN", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7738866806030273}]}, {"text": "Furthermore, we conduct experiments with amplification gate on three development datasets.", "labels": [], "entities": []}, {"text": "shows that amplification gate significantly increases F-score on three datasets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9987444877624512}]}, {"text": "Amplification even achieves 0.9% improvement on CTB6 dataset.", "labels": [], "entities": [{"text": "CTB6 dataset", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9767269790172577}]}, {"text": "It is demonstrated that amplification gate is an effective mechanism.", "labels": [], "entities": []}, {"text": "We compare our proposed model with previous neural approaches on PKU, MSRA and CT-B6 test datasets.", "labels": [], "entities": [{"text": "PKU", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8530672788619995}, {"text": "MSRA and CT-B6 test datasets", "start_pos": 70, "end_pos": 98, "type": "DATASET", "confidence": 0.7227208197116852}]}, {"text": "Experimental results are reported in  We also compare DGRNN with other state-ofthe-art non-neural networks, as shown in. implements the work of Sun and Xu (2011) on CTB6 dataset and achieves 95.7% F-score.", "labels": [], "entities": [{"text": "CTB6 dataset", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9586165547370911}, {"text": "F-score", "start_pos": 197, "end_pos": 204, "type": "METRIC", "confidence": 0.9992890357971191}]}, {"text": "We achieve the best result on P-KU dataset only with unigram embeddings.", "labels": [], "entities": [{"text": "P-KU dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.7916502058506012}]}, {"text": "The experimental results show that our model is a competitive model for Chinese word segmentation.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.6123987932999929}]}], "tableCaptions": [{"text": " Table 1: Comparisons for DGRNN and other neu- ral approaches based on traditional unigram em- beddings.", "labels": [], "entities": []}, {"text": " Table 2: Comparisons for DGRNN and state-of- the-art non-neural network approaches on F-score.", "labels": [], "entities": []}]}