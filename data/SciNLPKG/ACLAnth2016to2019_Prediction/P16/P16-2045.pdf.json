{"title": [{"text": "Phrase-Level Combination of SMT and TM Using Constrained Word Lattice", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8723675012588501}]}], "abstractContent": [{"text": "Constrained translation has improved statistical machine translation (SMT) by combining it with translation memory (TM) at sentence-level.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.7665746112664541}]}, {"text": "In this paper, we propose using a constrained word lattice, which encodes input phrases and TM constraints together, to combine SMT and TM at phrase-level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9685744047164917}]}, {"text": "Experiments on English-Chinese and English-French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination.", "labels": [], "entities": [{"text": "sentence-level constrained translation", "start_pos": 142, "end_pos": 180, "type": "TASK", "confidence": 0.6393210291862488}]}], "introductionContent": [{"text": "The combination of statistical machine translation (SMT) and translation memory (TM) has proven to be beneficial in improving translation quality and has drawn attention from many researchers).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.7855889250834783}]}, {"text": "Among various combination approaches, constrained translation () is a simple one and can be readily adopted.", "labels": [], "entities": [{"text": "constrained translation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.4969262182712555}]}, {"text": "Given an input sentence, constrained translation retrieves similar TM instances and uses matched segments to constrain the translation space of the input by generating a constrained input.", "labels": [], "entities": []}, {"text": "Then an SMT engine is used to search fora complete translation of the constrained input.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9861652851104736}]}, {"text": "Despite its effectiveness in improving SMT, previous constrained translation works at the sentence-level, which means that matched segments in a TM instance are either all adopted or all abandoned regardless of their individual quality (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9967359900474548}]}, {"text": "In this paper, we propose a phrase-level constrained translation approach which uses a constrained word lattice to encode the input and constraints from the TM together and allows a decoder to directly optimize the selection of constraints towards translation quality (Section 2).", "labels": [], "entities": [{"text": "phrase-level constrained translation", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.6486547191937765}]}, {"text": "We conduct experiments (Section 3) on English-Chinese (EN-ZH) and English-French (EN-FR) TM data.", "labels": [], "entities": []}, {"text": "Results show that our method is significantly better than previous combination approaches, including sentence-level constrained methods and a recent phrase-level combination method.", "labels": [], "entities": []}, {"text": "Specifically, it improves the BLEU () score by up to +5.5% on EN-ZH and +2.4% on EN-FR over a phrase-based baseline ( and decreases the TER () error by up to -4.3%/-2.2%, respectively.", "labels": [], "entities": [{"text": "BLEU () score", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9376694758733114}, {"text": "TER () error", "start_pos": 136, "end_pos": 148, "type": "METRIC", "confidence": 0.954206109046936}]}], "datasetContent": [{"text": "In our experiments, a baseline system PB is built with the phrase-based model in Moses (.", "labels": [], "entities": []}, {"text": "We compare our approach with three other combination methods.", "labels": [], "entities": []}, {"text": "ADD combines PB with addition (Ma et al., 2011), while SUB combines PB with subtraction (.", "labels": [], "entities": [{"text": "ADD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7227351665496826}]}, {"text": "WANG combines SMT and TM at phraselevel during decoding ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9136692881584167}]}, {"text": "For each phrase pair applied to translate an input phrase, WANG finds its corresponding phrase pairs in a TM instance and then extracts features which are directly added to the loglinear framework) as sparse features.", "labels": [], "entities": []}, {"text": "We build three systems based on our approach: CWL add only uses constraints from addition; CWL sub only uses constraints from subtraction; CWL both uses constraints from both.", "labels": [], "entities": []}, {"text": "shows a summary of our datasets.", "labels": [], "entities": []}, {"text": "The EN-ZH dataset is a translation memory from Symantec.", "labels": [], "entities": [{"text": "EN-ZH dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9045508503913879}]}, {"text": "Our EN-FR dataset is from the publicly available JRC-Acquis corpus.", "labels": [], "entities": [{"text": "EN-FR dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7634596824645996}, {"text": "JRC-Acquis corpus", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9714077413082123}]}, {"text": "Word alignment is performed by GIZA++ () with heuristic function grow-diag-final-and.: Experimental results of comparing our approach (CWL x ) with previous work.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7243604212999344}]}, {"text": "All scores reported are an average of 3 runs.", "labels": [], "entities": []}, {"text": "Scores with * are significantly better than that of the baseline PB at p < 0.01.", "labels": [], "entities": []}, {"text": "Bold scores are significantly better than that of all previous work at p < 0.01.", "labels": [], "entities": [{"text": "Bold", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9725004434585571}]}, {"text": "We use SRILM) to train a 5-gram language model on the target side of our training data with modified Kneser-Ney discounting.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.8348339200019836}]}, {"text": "Batch MIRA) is used to tune weights.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.8174030780792236}]}, {"text": "Caseinsensitive BLEU and TER are used to evaluate translation results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9967498779296875}, {"text": "TER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9983042478561401}, {"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9478709697723389}]}, {"text": "shows experimental results on EN-ZH and EN-FR.", "labels": [], "entities": [{"text": "EN-ZH", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.8686583042144775}, {"text": "EN-FR", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9191445708274841}]}, {"text": "We find that our method (CWL x ) significantly improves the baseline system PB on EN-ZH by up to +5.5% BLEU score and by +2.4% BLEU score on EN-FR.", "labels": [], "entities": [{"text": "PB", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9742079973220825}, {"text": "EN-ZH", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.9381629824638367}, {"text": "BLEU score", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9789425730705261}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9993038177490234}, {"text": "EN-FR", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.9657072424888611}]}, {"text": "In terms of TER, our system significantly decreases the error by up to -4.3%/-2.2% on EN-ZH and EN-FR, respectively.", "labels": [], "entities": [{"text": "TER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9993550181388855}, {"text": "error", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9884400367736816}]}, {"text": "Although, compared to the baseline PB, ADD and SUB work well on EN-ZH, they reduce the translation quality on EN-FR.", "labels": [], "entities": [{"text": "ADD", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9431027770042419}, {"text": "EN-ZH", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.94389808177948}, {"text": "EN-FR", "start_pos": 110, "end_pos": 115, "type": "DATASET", "confidence": 0.9402546882629395}]}, {"text": "By contrast, their phrase-level countparts (CWL add and CWL sub ) bring consistent improvements over the baseline on both language pairs.", "labels": [], "entities": []}, {"text": "This suggests that a combination approach based on constrained word lattices is more effective and robust than sentencelevel constrained translation.", "labels": [], "entities": [{"text": "sentencelevel constrained translation", "start_pos": 111, "end_pos": 148, "type": "TASK", "confidence": 0.6433417201042175}]}, {"text": "Compared to system WANG, our method produces significantly better translations as well.", "labels": [], "entities": [{"text": "translations", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.9571971893310547}]}, {"text": "In addition, our approach is simpler and easier to adopt than WANG.", "labels": [], "entities": [{"text": "WANG", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.4761931002140045}]}], "tableCaptions": [{"text": " Table 1: Summary of English-Chinese (EN-ZH)  and English-French (EN-FR) datasets", "labels": [], "entities": [{"text": "EN-FR) datasets", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.5966221292813619}]}, {"text": " Table 2: Experimental results of comparing our  approach (CWL x ) with previous work. All scores  reported are an average of 3 runs. Scores with  *   are significantly better than that of the baseline PB  at p < 0.01. Bold scores are significantly better  than that of all previous work at p < 0.01.", "labels": [], "entities": []}, {"text": " Table 3: Composition of test subsets based  on fuzzy match scores on English-Chinese and  English-French data.", "labels": [], "entities": []}]}