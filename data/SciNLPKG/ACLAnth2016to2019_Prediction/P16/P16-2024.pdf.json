{"title": [{"text": "Simple PPDB: A Paraphrase Database for Simplification", "labels": [], "entities": [{"text": "Simplification", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.9368564486503601}]}], "abstractContent": [{"text": "We release the Simple Paraphrase Database, a subset of of the Paraphrase Database (PPDB) adapted for the task of text simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7688842117786407}]}, {"text": "We train a supervised model to associate simplification scores with each phrase pair, producing rankings competitive with state-of-the-art lexical simplification models.", "labels": [], "entities": []}, {"text": "Our new simplification database contains 4.5 million paraphrase rules, making it the largest available resource for lexical simplification.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To evaluate Simple PPDB, we apply it in a setting intended to emulate the way it is likely to be used in practice.", "labels": [], "entities": [{"text": "Simple PPDB", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.5477559119462967}]}, {"text": "We use the Newsela Simplification Dataset (), a corpus of manually simplified news articles.", "labels": [], "entities": [{"text": "Newsela Simplification Dataset", "start_pos": 11, "end_pos": 41, "type": "DATASET", "confidence": 0.7848727504412333}]}, {"text": "This corpus is currently the cleanest available simplification dataset and is likely to be used to train and/or evaluate the simplification systems that we envision benefitting most from Simple PPDB.", "labels": [], "entities": []}, {"text": "We draw a sample of 100 unique word types (\"targets\") from the corpus for which Simple PPDB has at least one candidate simplification.", "labels": [], "entities": []}, {"text": "For each target, we take Simple PPDB's full list of simplification rules which are of high quality according to the PPDB 2.0 paraphrase score and which match the syntactic category of the target.", "labels": [], "entities": [{"text": "Simple PPDB", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.8042995631694794}]}, {"text": "On average, Simple PPDB proposes 8.8 such candidate simplifications per target.", "labels": [], "entities": [{"text": "Simple PPDB", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.7627665996551514}]}, {"text": "Our baselines include three existing methods for generating lists of candidates that were proposed in prior work.", "labels": [], "entities": []}, {"text": "The methods we test for generating lists of candidate paraphrases fora given target are: the WordNetGenerator, which pulls synonyms from WordNet (), the KauchakGenerator, which generates candidates based on automatic alignments between Simple Wikipedia and normal, and the GlavasGenerator, which generates candidates from nearby phrases in vector space (Glava\u0161 and\u0160tajner and\u02c7and\u0160tajner, 2015) (we use the pre-trained Word2Vec VSM ().", "labels": [], "entities": [{"text": "WordNetGenerator", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.9342466592788696}, {"text": "WordNet", "start_pos": 137, "end_pos": 144, "type": "DATASET", "confidence": 0.953332781791687}, {"text": "Word2Vec VSM", "start_pos": 418, "end_pos": 430, "type": "DATASET", "confidence": 0.8885433673858643}]}, {"text": "For each generated list, we follow's supervised SVM Rank approach to rank the candidates for simplicity.", "labels": [], "entities": []}, {"text": "We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus () and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences).", "labels": [], "entities": [{"text": "Google NGrams corpus", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.9416229724884033}, {"text": "Simple Wikipedia corpus", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.889623761177063}]}, {"text": "We omit the language modeling features since our evaluation does not consider the context in which the substitution is to be applied.", "labels": [], "entities": []}, {"text": "All of these methods (the three generation methods and the ranker) are implemented as part of the LEXenstein toolkit (.", "labels": [], "entities": []}, {"text": "We use the LEXenstein implementations for the results reported here, using off-the-shelf configurations and treating each method as a black box.", "labels": [], "entities": []}, {"text": "We use each of the generate-and-rank methods to produce a ranked list of simplification candidates for each of the 100 targets drawn from the Newsela corpus.", "labels": [], "entities": [{"text": "Newsela corpus", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.9766305685043335}]}, {"text": "When a generation method fails to produce any candidates fora given target, we simply ignore that target for that particular method.", "labels": [], "entities": []}, {"text": "This is to avoid: Examples of top-ranked simplifications proposed by Simple PPDB for several input words.", "labels": [], "entities": []}, {"text": "Often, the best simplification fora single word is a multiword phrase, or vice-versa.", "labels": [], "entities": []}, {"text": "These many-to-one mappings are overlooked when systems use only length or frequency as a proxy for simplicity.", "labels": [], "entities": []}, {"text": "an unfair advantage, since, by construction, PPDB will have full coverage of our list of 100 targets.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.942961573600769}, {"text": "coverage", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9793114066123962}]}, {"text": "In the end, the GlavasGenerator is evaluated over 95, the WordNetGenerator over 82, and the KauchakGenerator over 48.", "labels": [], "entities": [{"text": "WordNetGenerator", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.9522391557693481}]}, {"text": "The results in do not change significantly if we restrict all systems to the 48 targets which the KauchakGenerator is capable of handling.", "labels": [], "entities": [{"text": "KauchakGenerator", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.9146618247032166}]}, {"text": "Since the GlavasGenerator is capable of producing an arbitrary number of candidates for each target, we limit the length of each of its candidate lists to be equal to the number of candidates produced by Simple PPDB for that same target.", "labels": [], "entities": []}, {"text": "For each of the proposed rules from all four systems, we collect human judgements on Amazon Mechanical Turk, using the same annotation interface as before.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.9590449730555216}]}, {"text": "That is, we ask 7 workers to view each pair and indicate which of the two phrases is simpler, or to indicate that there is no difference.", "labels": [], "entities": []}, {"text": "We take the majority label to be the true label for each rule.", "labels": [], "entities": []}, {"text": "Workers show moderate agreement on the 3-way task (\u03ba = 0.4 \u00b1 0.03), with 14% of pairs receiving unanimous agreement and 37% receiving the same label from 6 out of 7 annotators.", "labels": [], "entities": []}, {"text": "We note that the \u03ba metric is likely a lower bound, as it punishes low agreement on pairs for which there is little difference in complexity, and thus the \"correct\" answer is not clear (e.g. for the pair matter, subject, 3 annotators say that matter is simpler, 2 say that subject is simpler, and 2 say there is no difference).", "labels": [], "entities": []}, {"text": "Results.: Precision of relative simplification rankings of three existing lexical simplification methods compared to the Simple PPDB resource in terms of Average Precision and P@1 (both range from 0 to 1 and higher is better).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9787079691886902}, {"text": "Average Precision", "start_pos": 154, "end_pos": 171, "type": "METRIC", "confidence": 0.8801566362380981}, {"text": "P@1", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.9523676435152689}]}, {"text": "All of the existing methods were evaluated using the implementations as provided in the LEXenstein toolkit.", "labels": [], "entities": [{"text": "LEXenstein toolkit", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.8613133132457733}]}, {"text": "likely due to a combination of the additional features applied in Simple PPDB's model (e.g. word embeddings) and the difference in training data (Simple PPDB's model was trained on 11K paraphrase pairs with trinary labels, while the model was trained on 500 words, each with a ranked list of paraphrases).", "labels": [], "entities": []}, {"text": "In addition, Simple PPDB offers the largest coverage (: Overall coverage of three existing lexical simplification methods compared to the Simple PPDB resource.", "labels": [], "entities": []}, {"text": "Glavas is marked as \u221e since it generates candidates based on nearness in vector space, and in theory could generate as many words/phrases as are in the vocabulary of the vector space.", "labels": [], "entities": [{"text": "Glavas", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9182180762290955}]}, {"text": "each target-for the 100 targets drawn from the Newsela corpus, PPDB provided an average of 8.8 candidates per target.", "labels": [], "entities": [{"text": "Newsela corpus", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9853163957595825}, {"text": "PPDB", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.867992103099823}]}, {"text": "The next best generator, the WordNet-based system, produces only 6.7 candidates per target on average, and has a total vocabulary of only 155K words.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9474945664405823}]}], "tableCaptions": [{"text": " Table 2: Accuracy on 10-fold cross-validation, and precision  for identifying simplifying rules. Folds are constructed so  that train and test vocabularies are disjoint.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9923275113105774}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9995278120040894}]}, {"text": " Table 3: Examples of top-ranked simplifications proposed by Simple PPDB for several input words. Often, the best simplifi- cation for a single word is a multiword phrase, or vice-versa. These many-to-one mappings are overlooked when systems use  only length or frequency as a proxy for simplicity.", "labels": [], "entities": []}, {"text": " Table 6: Overall coverage of three existing lexical simplifica- tion methods compared to the Simple PPDB resource. Glavas  is marked as \u221e since it generates candidates based on near- ness in vector space, and in theory could generate as many  words/phrases as are in the vocabulary of the vector space.", "labels": [], "entities": []}]}