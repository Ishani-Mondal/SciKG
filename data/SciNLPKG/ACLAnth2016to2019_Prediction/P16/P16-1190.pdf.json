{"title": [{"text": "Jointly Learning to Embed and Predict with Multiple Languages", "labels": [], "entities": [{"text": "Learning to Embed and Predict with Multiple Languages", "start_pos": 8, "end_pos": 61, "type": "TASK", "confidence": 0.6807003282010555}]}], "abstractContent": [{"text": "We propose a joint formulation for learning task-specific cross-lingual word em-beddings, along with classifiers for that task.", "labels": [], "entities": []}, {"text": "Unlike prior work, which first learns the embeddings from parallel data and then plugs them in a supervised learning problem, our approach is one-shot: a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss.", "labels": [], "entities": []}, {"text": "We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension, a limitation which does not exist for other co-regularizers (such as the 1-distance).", "labels": [], "entities": []}, {"text": "Despite its simplicity, our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German, with training times below 1 minute.", "labels": [], "entities": [{"text": "RCV1/RCV2 dataset", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.8780696094036102}]}, {"text": "On the TED Corpus, we obtain the highest reported scores on 10 out of 11 languages.", "labels": [], "entities": [{"text": "TED Corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9380217790603638}]}], "introductionContent": [{"text": "Distributed representations of text (embeddings) have been the target of much research in natural language processing.", "labels": [], "entities": [{"text": "Distributed representations of text (embeddings)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7925991075379508}, {"text": "natural language processing", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.6480133136113485}]}, {"text": "Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt fora wide variety of tasks, such as language modeling (, sentence tagging (), sentiment analysis), parsing), and machine translation (.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 163, "end_pos": 180, "type": "TASK", "confidence": 0.7221042364835739}, {"text": "sentence tagging", "start_pos": 184, "end_pos": 200, "type": "TASK", "confidence": 0.721075251698494}, {"text": "sentiment analysis", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.8214816749095917}, {"text": "parsing", "start_pos": 226, "end_pos": 233, "type": "TASK", "confidence": 0.9712050557136536}, {"text": "machine translation", "start_pos": 240, "end_pos": 259, "type": "TASK", "confidence": 0.8141928315162659}]}, {"text": "At the same time, there has been a consistent progress in devising \"universal\" multilingual models via cross-lingual transfer techniques of various kinds.", "labels": [], "entities": []}, {"text": "This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages.", "labels": [], "entities": []}, {"text": "Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space.", "labels": [], "entities": []}, {"text": "A string of work started with on learning bilingual embeddings for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7772536277770996}]}, {"text": "proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space.", "labels": [], "entities": []}, {"text": "A bilingual auto-encoder was proposed by, while applied canonical correlation analysis to parallel data to improve monolingual embeddings.", "labels": [], "entities": []}, {"text": "Other works optimize a sum of monolingual and cross-lingual terms (, or introduce bilingual variants of skip-gram (.", "labels": [], "entities": []}, {"text": "Recently,  extended the non-compositional paragraph vectors of to a bilingual setting, achieving anew state of the art at the cost of more expensive (and non-deterministic) prediction.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative joint formulation that learns embeddings suited to a particular task, together with the corresponding classifier for that task.", "labels": [], "entities": []}, {"text": "We do this by minimizing a combination of a supervised loss function and a multilingual regularization term.", "labels": [], "entities": []}, {"text": "Our approach leads to a convex optimization problem and makes abridge between classical co-regularization approaches for semi-supervised learning) and modern representation learning.", "labels": [], "entities": []}, {"text": "In addition, we show that Euclidean co-regularizers have serious limitations to learn rich embeddings, when the number of task labels is small.", "labels": [], "entities": []}, {"text": "We establish this by proving that the resulting embedding matrices have their rank upper bounded by the number of labels.", "labels": [], "entities": []}, {"text": "This limitation does not exist for other regularizers (convex or not), such as the 1 -distance and noise-contrastive distances.", "labels": [], "entities": []}, {"text": "Our experiments in the RCV1/RCV2 dataset yield state-of-the-art accuracy (92.7%) with this simple convex formulation, when transferring from English to German, without the need of negative sampling, extra monolingual data, or nonadditive representations.", "labels": [], "entities": [{"text": "RCV1/RCV2 dataset", "start_pos": 23, "end_pos": 40, "type": "DATASET", "confidence": 0.8155173808336258}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9993477463722229}]}, {"text": "For the reverse direction, our best number (79.3%), while far behind the recent para_doc approach ( , is on par with current compositional methods.", "labels": [], "entities": []}, {"text": "On the TED corpus, we obtained general purpose multilingual embeddings for 11 target languages, by considering the (auxiliary) task of reconstructing pre-trained English word vectors.", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8296771943569183}]}, {"text": "The resulting embeddings led to cross-lingual multi-label classifiers that achieved the highest reported scores on 10 out of these 11 languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus.", "labels": [], "entities": [{"text": "cross-lingual classification", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7726496160030365}, {"text": "Reuters RCV1/RCV2 dataset", "start_pos": 81, "end_pos": 106, "type": "DATASET", "confidence": 0.9540749311447143}, {"text": "multi-label classification", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.7304002046585083}, {"text": "TED Corpus", "start_pos": 186, "end_pos": 196, "type": "DATASET", "confidence": 0.9795152544975281}]}], "tableCaptions": [{"text": " Table 2: Cross-lingual experiments on the TED Corpus using English as a source language. Reported  are the micro-averaged F 1 scores for a machine translation baseline and the two strongest systems of", "labels": [], "entities": [{"text": "TED Corpus", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.8964314758777618}, {"text": "F 1 scores", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9355453848838806}]}, {"text": " Table 3: Monolingual experiments on the TED Corpus. Shown are the micro-averaged F 1 scores for a  bag-of-words baseline, a system trained on Polyglot embeddings, the two strongest systems of", "labels": [], "entities": [{"text": "TED Corpus", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.8645358383655548}, {"text": "F 1 scores", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9222701986630758}]}]}