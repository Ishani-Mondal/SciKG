{"title": [{"text": "Synthesizing Compound Words for Machine Translation", "labels": [], "entities": [{"text": "Synthesizing Compound Words", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.879264771938324}, {"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.68805031478405}]}], "abstractContent": [{"text": "Most machine translation systems construct translations from a closed vocabulary of target word forms, posing problems for translating into languages that have productive compounding processes.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7275974452495575}]}, {"text": "We present a simple and effective approach that deals with this problem in two phases.", "labels": [], "entities": []}, {"text": "First, we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language.", "labels": [], "entities": []}, {"text": "Then, for each identified span, we generate a pool of possible compounds which are added to the translation model as \"synthetic\" phrase translations.", "labels": [], "entities": []}, {"text": "Experiments reveal that (i) we can effectively predict what spans can be compounded; (ii) our compound generation model produces good compounds; and (iii) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks.", "labels": [], "entities": []}, {"text": "We additionally introduce KomposEval, anew multi-reference dataset of English phrases and their translations into German compounds .", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation systems make a closedvocabulary assumption: with the exception of basic rules for copying unknown word types from the input to the output, they can produce words in the target language only from a fixed, finite vocabulary.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7946237623691559}]}, {"text": "While this is always a na\u00efve assumption given the long-tailed distributions that characterize natural language, it is particularly challenging in languages such as German and Finnish that have productive compounding processes.", "labels": [], "entities": []}, {"text": "In such languages, expressing compositions of basic concepts can require an unbounded number of words.", "labels": [], "entities": []}, {"text": "For example, English multiword phrases like market for bananas, market for pears, and market for plums are expressed in German with single compound words (respectively, as Bananenmarkt, Birnenmarkt, and Pflaumenmarkt).", "labels": [], "entities": []}, {"text": "Second, while they are individually rare, compound words are, on the whole, frequent in native texts (.", "labels": [], "entities": []}, {"text": "Third, compounds are crucial for translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.949203222990036}]}, {"text": "Not only does generating them make the output seem more natural, but they are contentrich.", "labels": [], "entities": []}, {"text": "Since each compound has, by definition, at least two stems, they are intuitively (at least) doubly important for translation adequacy.", "labels": [], "entities": []}, {"text": "Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling.", "labels": [], "entities": [{"text": "compounding", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9774045944213867}]}, {"text": "In this paper we introduce a two-stage method ( \u00a72) to dynamically generate novel compound word forms given a source language input text and incorporate these as \"synthetic rules\" in a standard phrase-based translation system (.", "labels": [], "entities": []}, {"text": "First, a binary classifier examines each source-language sentence and labels each span therein with whether that span could become a compound word when translated into the target language.", "labels": [], "entities": []}, {"text": "Second, we transduce the identified phrase into the target language using a word-to-character translation model.", "labels": [], "entities": [{"text": "word-to-character translation", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.7167592346668243}]}, {"text": "This system makes a closed vocabulary assumption, albeit at the character (rather than word) level-thereby enabling new word forms to be generated.", "labels": [], "entities": []}, {"text": "Training data for these models is extracted from automatically aligned and compound split parallel corpora ( \u00a73).", "labels": [], "entities": []}, {"text": "We evaluate our approach on both intrinsic and extrinsic metrics.", "labels": [], "entities": []}, {"text": "Since German compounds are relatively rare, their impact on the standard MT evaluation metrics (e.g., BLEU) is minimal, as we show with an oracle experiment, and we find that our synthetic phrase approach obtains only modest improvements in overall translation quality.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.8835570216178894}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9986988306045532}]}, {"text": "To better assess its merits, we commissioned anew test set, which we dub KomposEval (from the German word fora compound word, Komposita), consisting of a set of 1090 English phrases and their translations as German compound words by a professional English-German translator.", "labels": [], "entities": []}, {"text": "The translator was instructed to produce as many compoundword translations as were reasonable ( \u00a74).", "labels": [], "entities": []}, {"text": "This dataset permits us to evaluate our compound generation component directly, and we show that (i) without mechanisms for generating compound words, MT systems cannot produce the long tail of compounds; and (ii) our method is an effective method for creating correct compounds.", "labels": [], "entities": [{"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.9729491472244263}]}], "datasetContent": [{"text": "Before considering the problem of integrating our compound model with a full machine translation system, we perform an intrinsic evaluation of each of the two steps of our pipeline.: Decomposition of a target compound into possible analyses, given a source phrase and a morpheme-level translation table.", "labels": [], "entities": []}, {"text": "This process allows us to learn the \"phonetic glue\" that can go in between morphemes, as well as the inflections that can attach to the end of a compound word.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our classifier, by measuring its precision and recall on the two held out test sets described in \u00a72.1 taken from two language pairs: English-German and EnglishFinnish.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9996601343154907}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9996126294136047}]}, {"text": "Furthermore, we show results both with down-sampling (balanced data set) and without down-sampling (unbalanced data set).", "labels": [], "entities": []}, {"text": "Our classifier can freely trade off precision and recall by generalizing its requirement to call an example positive from p(compound | span) > 0.5 to p(compound | span) > \u03c4 , for \u03c4 \u2208 (0, 1), allowing us to report full precision-recall curves.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9987286925315857}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9984908103942871}]}, {"text": "We find that our best results for the unbalanced cases come at \u03c4 = 0.24 for German and \u03c4 = 0.29 for Finnish, with F-scores of 20.1% and 67.8%, respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9989936947822571}]}, {"text": "In the balanced case, we achieve 67.1% and 97.0% F-scores with \u03c4 = 0.04 and \u03c4 = 0.57 on German and Finnish respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9969114661216736}]}, {"text": "To evaluate our compound generator, we fed it the source side of our newly created KomposEval corpus and had it output a 100-best list of hypotheses translations for each English phrase.", "labels": [], "entities": [{"text": "KomposEval corpus", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9067355394363403}]}, {"text": "From this we are able to compute many intrinsic quality metrics.", "labels": [], "entities": []}, {"text": "We report the following metrics: \u2022 Mean reciprocal rank (MRR); which is one divided by the average overall segments of the position that the reference translation appears in our k-best list.", "labels": [], "entities": [{"text": "Mean reciprocal rank (MRR)", "start_pos": 35, "end_pos": 61, "type": "METRIC", "confidence": 0.9497845868269602}]}, {"text": "\u2022 Character error rate (CER), or the average number of character-level edits that are required to turn our 1-best hypothesis into the nearest of the reference translations.", "labels": [], "entities": [{"text": "Character error rate (CER)", "start_pos": 2, "end_pos": 28, "type": "METRIC", "confidence": 0.8773288528124491}]}, {"text": "\u2022 Precision at 1, 5, and 10, which indicate what percentage of the time a reference translation can be found in the top 1, 5, or 10 hypotheses of our k-best list, respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9986785054206848}]}, {"text": "These results can be found in.", "labels": [], "entities": []}, {"text": "We compare to a na\u00efve baseline that is just a standard English-German phrase-based translation system with no special handling of compound word forms.", "labels": [], "entities": []}, {"text": "We immediately see that the baseline system is simply unable to generate most of the compound words in the test set, resulting in extraordinarily low metric scores across the board.", "labels": [], "entities": []}, {"text": "Its one saving grace is its tolerable CER score, which shows that the system is capable of generating the correct morphemes, but is failing to correctly ad- 0% <0.01% Our model 0.7004 2.506 61.38% 81.47% 84.31%: Mean reciprocal rank, character error rate, and precision at K statistics of our baseline MT system and our compound generator.", "labels": [], "entities": [{"text": "CER score", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9828603863716125}, {"text": "precision", "start_pos": 260, "end_pos": 269, "type": "METRIC", "confidence": 0.9995965361595154}, {"text": "MT", "start_pos": 302, "end_pos": 304, "type": "TASK", "confidence": 0.913413941860199}]}, {"text": "join them and add the phonological glue required to produce a well-formed compound word.", "labels": [], "entities": []}, {"text": "Our system, on the other hand, is capable of reaching at least one of the five references for every single sentence in the test set, and has a reference translation in the top 5 hypotheses in its k-best list over 80% of the time.", "labels": [], "entities": []}, {"text": "Qualitatively, the compounds generated by our model are remarkably good, and very understandable.", "labels": [], "entities": []}, {"text": "Major error classes include incorrect word sense, non-compositional phrases, and special non-concatenative effects at word boundaries.", "labels": [], "entities": []}, {"text": "An example of each of these errors, along with some examples of good compound generation can be found in.", "labels": [], "entities": []}, {"text": "Finally, we use our compound generator as part of a larger machine translation pipeline.", "labels": [], "entities": [{"text": "machine translation pipeline", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.7665857076644897}]}, {"text": "We run our compound span classifier on each of our translation system's tune and test sets, and extract our generator's top ten hypotheses for each of the postively identified spans.", "labels": [], "entities": []}, {"text": "These English phrases are then added to a synthetic phrase table, along with their German compound translations, and two features: the compound generator's score, and an indicator feature simply showing that the rule represents a synthetic compound.", "labels": [], "entities": []}, {"text": "shows some example rules of this form.", "labels": [], "entities": []}, {"text": "The weights of these features are learned, along with the standard translation system weights, by the MIRA algorithm as part of the MT training procedure.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.5841898322105408}, {"text": "MT training", "start_pos": 132, "end_pos": 143, "type": "TASK", "confidence": 0.8873909711837769}]}, {"text": "The underlying translation system is a standard Hiero () system using the cdec () decoder, trained on all constrained-track WMT English-German data as of the 2014 translation task.", "labels": [], "entities": [{"text": "WMT English-German data", "start_pos": 124, "end_pos": 147, "type": "DATASET", "confidence": 0.7545870542526245}]}, {"text": "Tokenization was done with cdec's tokenize-anything script.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9050471782684326}]}, {"text": "The first character of each sentence was down cased if the unigram probability of the downcased version of the first word was higher than that of the original casing.", "labels": [], "entities": []}, {"text": "Word alignment was performed using cdec's fast_align tool,: Improvements in English-German translation quality using our method of compound generation on.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7205317914485931}]}, {"text": "* indicates the set used for tuning the MT system. and symmetrized using the grow-diag heuristic.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.950619637966156}]}, {"text": "Training is done using cdec's implementation of the MIRA algorithm.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.4667263925075531}]}, {"text": "Evaluation was done using MultEval).", "labels": [], "entities": [{"text": "MultEval", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.525202214717865}]}, {"text": "A 4-gram language model was estimated using KenLM's lmplz tool (.", "labels": [], "entities": []}, {"text": "In addition to running our full end-to-end pipeline, we run an oracle experiment wherein we run the same pre-processing pipeline (compound splitting, bidirectionally aligning, intersecting, and de-splitting) on each test set to identify which spans do, in fact, turn into compounds, as well as their ideal translations.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.7117443382740021}]}, {"text": "We then add grammar rules that allow precisely these source spans to translate into these oracle translations.", "labels": [], "entities": []}, {"text": "This allows us to get an upper bound on the impact compound generation could have on translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.954168975353241}]}, {"text": "The results, summarized in, show that adding these extra compounds has little effect on metric scores compared to our baseline system.", "labels": [], "entities": [{"text": "metric scores", "start_pos": 88, "end_pos": 101, "type": "METRIC", "confidence": 0.949707567691803}]}, {"text": "Nevertheless, we believe that the qualitative improvements of our methods are more significant than the automatic metrics would indicate.", "labels": [], "entities": []}, {"text": "Our method targets a very specific problem that pertains only to dense content-bearing target words that humans find very important.", "labels": [], "entities": []}, {"text": "Moreover, BLEU is unable to reasonably evaluate improvements in these long tail phenomena, as it only captures exact lexical matches, and because we are purposely generating fewer target words than a standard translation system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9878084659576416}]}], "tableCaptions": [{"text": " Table 6: Improvements in English-German trans- lation quality using our method of compound gen- eration on", "labels": [], "entities": [{"text": "trans- lation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.6281307836373647}]}, {"text": " Table 5: Example synthetic rules dynamically added to our system to translate the phrase \"market for  bananas\" into a German compound word. Note that we correctly generate both the nominative form  (with no suffix) and the genitive forms (with the -s and -es suffixes).", "labels": [], "entities": []}, {"text": " Table 7: Improvements in English-Finnish trans- lation quality using our method of compound gen- eration on WMT 2014 tuning, devtest, and test  sets. * indicates the set used for tuning the MT  system.", "labels": [], "entities": [{"text": "WMT 2014 tuning", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.8959176739056905}, {"text": "MT", "start_pos": 191, "end_pos": 193, "type": "TASK", "confidence": 0.8994406461715698}]}]}