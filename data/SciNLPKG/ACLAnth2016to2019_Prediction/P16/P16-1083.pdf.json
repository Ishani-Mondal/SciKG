{"title": [{"text": "Normalized Log-Linear Interpolation of Backoff Language Models is Efficient", "labels": [], "entities": []}], "abstractContent": [{"text": "We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model, contradicting Hsu (2007).", "labels": [], "entities": []}, {"text": "While prior work reported that log-linear interpolation yields lower per-plexity than linear interpolation, normalizing at query time was impractical.", "labels": [], "entities": []}, {"text": "We normalize the model offline in advance, which is efficient due to a recurrence relationship between the normalizing factors.", "labels": [], "entities": []}, {"text": "To tune interpolation weights, we apply Newton's method to this convex problem and show that the derivatives can be computed efficiently in a batch process.", "labels": [], "entities": []}, {"text": "These findings are combined in new open-source interpolation tool, which is distributed with KenLM.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.892310380935669}]}, {"text": "With 21 out-of-domain corpora, log-linear interpolation yields 72.58 per-plexity on TED talks, compared to 75.91 for linear interpolation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Log-linearly interpolated backoff language models yielded better perplexity than linearly interpolated models, but experiments and adoption were limited due the impractically high cost of querying.", "labels": [], "entities": []}, {"text": "This cost is due to normalizing to form a probability distribution by brute-force summing over the entire vocabulary for each query.", "labels": [], "entities": []}, {"text": "Instead, we prove that the log-linearly interpolated model can be normalized offline in advance and exactly expressed as an ordinary backoff language model.", "labels": [], "entities": []}, {"text": "This contradicts, who claimed that log-linearly interpolated models \"cannot be efficiently represented as a backoff n-gram model.\"", "labels": [], "entities": []}, {"text": "We show that offline normalization is efficient due to a recurrence relationship between the normalizing factors ().", "labels": [], "entities": []}, {"text": "This forms the basis for our opensource implementation, which is part of KenLM: https://kheafield.com/code/kenlm/.", "labels": [], "entities": []}, {"text": "Linear interpolation, combines several language models pi into a single model p L p L (w n | w n\u22121 where \u03bb i are weights and w n 1 are words.", "labels": [], "entities": [{"text": "Linear interpolation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7349076569080353}]}, {"text": "Because each component model pi is a probability distribution and the non-negative weights \u03bb i sum to 1, the interpolated model p L is also a probability distribution.", "labels": [], "entities": []}, {"text": "This presumes that the models have the same vocabulary, an issue we discuss in \u00a73.1.", "labels": [], "entities": []}, {"text": "A log-linearly interpolated model p LL uses the weights \u03bb i as powers.", "labels": [], "entities": []}, {"text": "The weights \u03bb i are unconstrained real numbers, allowing parameters to soften or sharpen distributions.", "labels": [], "entities": []}, {"text": "Negative weights can be used to divide a mixed-domain model by an out-of-domain model.", "labels": [], "entities": []}, {"text": "To form a probability distribution, the product is normalized The sum is taken overall words x in the combined vocabulary of the underlying models, which can number in the millions or even billions.", "labels": [], "entities": []}, {"text": "Computing Z efficiently is a key contribution in this work.", "labels": [], "entities": []}, {"text": "Our proofs assume the component models pi are backoff language models that memorize probability for seen n-grams and charge a 876 backoff penalty bi for unseen n-grams.", "labels": [], "entities": []}, {"text": "pi (w n | w n\u22121 1 ) = pi (w n | w n\u22121 1 ) if w n 1 is seen pi (w n | w n\u22121 2 )b i (w n\u22121 1 ) o.w.", "labels": [], "entities": []}, {"text": "While linearly or log-linearly interpolated models can be queried online by querying the component models, doing so costs RAM to store duplicated n-grams and CPU time to perform lookups.", "labels": [], "entities": []}, {"text": "Log-linear interpolation is particularly slow due to normalizing over the entire vocabulary.", "labels": [], "entities": [{"text": "Log-linear interpolation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8455115258693695}]}, {"text": "Instead, it is preferable to combine the models offline into a single backoff model containing the union of n-grams.", "labels": [], "entities": []}, {"text": "Doing so is impossible for linear interpolation ( \u00a73.2); SRILM) and MITLM () implement an approximation.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.7238030433654785}, {"text": "MITLM", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.7040854692459106}]}, {"text": "In contrast, we prove that offline log-linear interpolation requires no such approximation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments for perplexity, query speed, memory consumption, and effectiveness in a machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.6920350939035416}]}, {"text": "Individual language models were trained on English corpora from the WMT 2016 news translation shared task (.", "labels": [], "entities": [{"text": "WMT 2016 news translation shared task", "start_pos": 68, "end_pos": 105, "type": "TASK", "confidence": 0.7786231736342112}]}, {"text": "This includes the seven newswires (afp, apw, cna, ltw, nyt, wpb, xin) from English Gigaword Fifth Edition; the 2007-2015 news crawls; 4 News discussion; News commmentary v11; English from Europarl v8 (; the English side of the French-English parallel corpus (; and the English side of SETIMES2 (.", "labels": [], "entities": [{"text": "English Gigaword Fifth Edition", "start_pos": 75, "end_pos": 105, "type": "DATASET", "confidence": 0.8986405581235886}, {"text": "Europarl", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.8006353974342346}]}, {"text": "We additionally built one language model trained on the concatenation of all of the above corpora.", "labels": [], "entities": []}, {"text": "All corpora were preprocessed using the standard Moses () scripts to perform normalization, tokenization, and truecasing.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 92, "end_pos": 104, "type": "TASK", "confidence": 0.9527274370193481}]}, {"text": "To prevent SRILM from running out of RAM, we excluded the large monolingual CommonCrawl data, but included English from the parallel CommonCrawl data.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 11, "end_pos": 16, "type": "TASK", "confidence": 0.8386470079421997}, {"text": "CommonCrawl data", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9488336145877838}, {"text": "CommonCrawl data", "start_pos": 133, "end_pos": 149, "type": "DATASET", "confidence": 0.9514144062995911}]}, {"text": "All language models are 5-gram backoff language models trained with modified Kneser-Ney smoothing) using lmplz ().", "labels": [], "entities": []}, {"text": "Also to prevent SRILM from running out of RAM, we pruned singleton trigrams and above.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 16, "end_pos": 21, "type": "TASK", "confidence": 0.5339574217796326}, {"text": "RAM", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9930018782615662}]}, {"text": "For linear interpolation, we tuned weights using IRSTLM.", "labels": [], "entities": [{"text": "IRSTLM", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.4836913049221039}]}, {"text": "To workaround SRILM's limitation often models, we interpolated the first ten then carried the combined model and added nine more component models, repeating this last step as necessary.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 14, "end_pos": 19, "type": "TASK", "confidence": 0.5975533127784729}]}, {"text": "Weights were normalized within batches to achieve the correct final weighting.", "labels": [], "entities": []}, {"text": "This simply extends the way SRILM internally carries a combined model and adds one model at a time.", "labels": [], "entities": []}, {"text": "We experiment with two domains: TED talks, which is out of domain, and news, which is indomain for some corpora.", "labels": [], "entities": [{"text": "TED talks", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.7330030500888824}]}, {"text": "For TED, we tuned on the IWSLT 2010 English dev set and test on the 2010 test set.", "labels": [], "entities": [{"text": "TED", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8795268535614014}, {"text": "IWSLT 2010 English dev set", "start_pos": 25, "end_pos": 51, "type": "DATASET", "confidence": 0.9450527906417847}, {"text": "2010 test set", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9225283066431681}]}, {"text": "For news, we tuned on the English side of the WMT 2015 Russian-English evaluation set and test on the WMT 2014 RussianEnglish evaluation set.", "labels": [], "entities": [{"text": "WMT 2015 Russian-English evaluation set", "start_pos": 46, "end_pos": 85, "type": "DATASET", "confidence": 0.9482045531272888}, {"text": "WMT 2014 RussianEnglish evaluation set", "start_pos": 102, "end_pos": 140, "type": "DATASET", "confidence": 0.9456660389900208}]}, {"text": "To measure generalization, we also evaluated news on models tuned for TED and vice-versa.", "labels": [], "entities": [{"text": "generalization", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9716033935546875}]}, {"text": "Results are shown in   Log-linear interpolation performs better on TED (72.58 perplexity versus 75.91 for offline linear interpolation).", "labels": [], "entities": [{"text": "TED", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9029391407966614}]}, {"text": "However, it performs worse on news.", "labels": [], "entities": []}, {"text": "In future work, we plan to investigate whether log-linear wins when all corpora are outof-domain since it favors agreement by all models.", "labels": [], "entities": []}, {"text": "compares the speed and memory performance of the competing methods.", "labels": [], "entities": [{"text": "speed", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9818098545074463}]}, {"text": "While the log-linear tuning is much slower, its compilation is faster compared to the offline linear model's long run time.", "labels": [], "entities": [{"text": "compilation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.8482217788696289}]}, {"text": "Since the model formats are the same for the concatenation and log-linear, they share the fastest query speeds.", "labels": [], "entities": []}, {"text": "Query speed was measured using) faster probing data structure.", "labels": [], "entities": []}, {"text": "We trained a statistical phrase-based machine translation system for Romanian-English on the Romanian-English parallel corpora released as part of the 2016 WMT news translation shared task.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 13, "end_pos": 57, "type": "TASK", "confidence": 0.5984326228499413}, {"text": "WMT news translation shared task", "start_pos": 156, "end_pos": 188, "type": "TASK", "confidence": 0.779682993888855}]}, {"text": "We trained three variants of this MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9846839308738708}]}, {"text": "The first used a single language model trained on the concatenation of the 21 individual LM training corpora.", "labels": [], "entities": []}, {"text": "The second used 22 language models, with each LM presented to Moses as a separate feature.", "labels": [], "entities": []}, {"text": "The third used a single language model which is an interpolation of all 22 models.", "labels": [], "entities": []}, {"text": "This variant was run with offline linear, online linear, and log-linear interpolation.", "labels": [], "entities": []}, {"text": "All MT system variants were optimized using IWSLT 2011 Romanian-English TED test as the development set, and were evaluated using the IWSLT 2012 Romanian-English TED test set.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9664124846458435}, {"text": "IWSLT 2011 Romanian-English TED test", "start_pos": 44, "end_pos": 80, "type": "DATASET", "confidence": 0.9055395722389221}, {"text": "IWSLT 2012 Romanian-English TED test set", "start_pos": 134, "end_pos": 174, "type": "DATASET", "confidence": 0.946192224820455}]}, {"text": "As shown in, no significant difference in MT quality as measured by BLEU was observed; the best BLEU score came from separate features at 18.40 while log-linear scored 18.15.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9890033602714539}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9929306507110596}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9987258315086365}]}, {"text": "We leave: Machine translation performance comparison in an end-to-end system.", "labels": [], "entities": [{"text": "Machine translation performance comparison", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.8052223920822144}]}, {"text": "jointly tuned normalized log-linear interpolation to future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Linearly interpolating two models p 1  and p 2 with equal weight yields an unnormalized  model p L . If gaps are filled with zeros instead, the  model is normalized.", "labels": [], "entities": []}, {"text": " Table 5: Test set perplexities. In the middle ta- ble, weights are optimized for TED and include  a model trained on all concatenated text. In the  bottom table, weights are optimized for news and  exclude the model trained on all concatenated text.", "labels": [], "entities": []}, {"text": " Table 6: Speed and memory consumption of LM combination methods. Interpolated models include the  concatenated model. Tuning and compiling times are in minutes, memory consumption in gigabytes,  and query time in microseconds per query (on 1G of held-out Common Crawl monolingual data).", "labels": [], "entities": [{"text": "Common Crawl monolingual data", "start_pos": 256, "end_pos": 285, "type": "DATASET", "confidence": 0.7243146449327469}]}, {"text": " Table 7: Machine translation performance com- parison in an end-to-end system.", "labels": [], "entities": [{"text": "Machine translation performance com- parison", "start_pos": 10, "end_pos": 54, "type": "TASK", "confidence": 0.661212295293808}]}]}