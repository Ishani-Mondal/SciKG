{"title": [{"text": "One for All: Towards Language Independent Named Entity Linking", "labels": [], "entities": [{"text": "Language Independent Named Entity Linking", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.6266765892505646}]}], "abstractContent": [{"text": "Entity linking (EL) is the task of dis-ambiguating mentions in text by associating them with entries in a predefined database of mentions (persons, organizations , etc).", "labels": [], "entities": [{"text": "Entity linking (EL) is the task of dis-ambiguating mentions in text by associating them with entries in a predefined database of mentions (persons, organizations , etc)", "start_pos": 0, "end_pos": 168, "type": "Description", "confidence": 0.7757296456444648}]}, {"text": "Most previous EL research has focused mainly on one language, English, with less attention being paid to other languages , such as Spanish or Chinese.", "labels": [], "entities": [{"text": "EL", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9622418284416199}]}, {"text": "In this paper, we introduce LIEL, a Language Independent Entity Linking system, which provides an EL framework which, once trained on one language, works remarkably well on a number of different languages without change.", "labels": [], "entities": [{"text": "LIEL", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9280387163162231}, {"text": "Language Independent Entity Linking", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.6087802350521088}]}, {"text": "LIEL makes a joint global prediction over the entire document, employing a discriminative re-ranking framework with many domain and language-independent feature functions.", "labels": [], "entities": [{"text": "LIEL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7746094465255737}]}, {"text": "Experiments on numerous benchmark datasets, show that the proposed system , once trained on one language, En-glish, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competitor system), demonstrating the viability of the approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "We live in a golden age of information, where we have access to vast amount of data in various forms: text, video and audio.", "labels": [], "entities": []}, {"text": "Being able to analyze this data automatically, usually involves filling a relational database, which, in turn, requires the processing system to be able to identify actors across documents by assigning unique identifiers to them.", "labels": [], "entities": []}, {"text": "Entity Linking (EL) is the task of mapping specific textual mentions of entities in a text document to an entry in a large catalog of entities, often called a knowledge base or KB, and is one of the major tasks in the Knowledge-Base Population track at the Text Analysis Conference (TAC) ( ).", "labels": [], "entities": [{"text": "Entity Linking (EL) is the task of mapping specific textual mentions of entities in a text document to an entry in a large catalog of entities, often called a knowledge base or KB", "start_pos": 0, "end_pos": 179, "type": "Description", "confidence": 0.7564264891876115}, {"text": "Text Analysis Conference (TAC)", "start_pos": 257, "end_pos": 287, "type": "TASK", "confidence": 0.7808564205964407}]}, {"text": "The task also involves grouping together (clustering) N IL entities which do not have any target referents in the KB.", "labels": [], "entities": []}, {"text": "Previous work, pioneered by (, have used Wikipedia as this target catalog of entities because of its wide coverage and its frequent updates made by the community.", "labels": [], "entities": []}, {"text": "As with many NLP approaches, most of the previous EL research have focused on English, mainly because it has many NLP resources available, it is the most prevalent language on the web, and the fact that the English Wikipedia is the largest among all the Wikipedia datasets.", "labels": [], "entities": [{"text": "Wikipedia datasets", "start_pos": 254, "end_pos": 272, "type": "DATASET", "confidence": 0.8528801500797272}]}, {"text": "However, there are plenty of web documents in other languages, such as Spanish (, and Chinese (), with a large number of speakers, and there is a need to be able to develop EL systems for these languages (and others!) quickly and inexpensively.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the hypothesis that we can train an EL model that is entirely unlexicalized, by only allowing features that compute similarity between the text in the input document and the text/information in the KB.", "labels": [], "entities": []}, {"text": "For this purpose, we propose a novel approach to entity linking, which we call Language Independent Entity Linking (henceforth LIEL).", "labels": [], "entities": [{"text": "entity linking", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7557409107685089}, {"text": "Language Independent Entity Linking (henceforth LIEL)", "start_pos": 79, "end_pos": 132, "type": "TASK", "confidence": 0.663882177323103}]}, {"text": "We test this hypothesis by applying the English-trained system on Spanish and Chinese datasets, with great success.", "labels": [], "entities": []}, {"text": "This paper has three novel contributions: 1) extending a powerful inference algorithm for global entity linking, built using similarity measures, corpus statistics, along with knowledge base statis-tics, 2) integrates many language-agnostic and domain independent features in an exponential framework, and 3) provide empirical evidence on a large variety of popular benchmark datasets that the resulting model outperforms or matches the best published results, and, most importantly, the trained model transfers well across languages, outperforming the state-of-the-art (SOTA) in Spanish and matching it in Chinese.", "labels": [], "entities": [{"text": "global entity linking", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.6394180357456207}]}, {"text": "We organize the paper as follows: the next section motivates the problem and discusses the language-independent model along with the features.", "labels": [], "entities": []}, {"text": "Section 3 describes our experiments and comparison with the state-of-the-art.", "labels": [], "entities": []}, {"text": "Section 4 illustrates the related previous work and Section 5 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate LIEL's capability by testing against several state-of-the-art EL systems on English, then apply the English-trained system to Spanish and Chinese EL tasks to test its language transcendability.", "labels": [], "entities": []}, {"text": "English: al., 2014) 9 , which contain data from diverse genre like discussion forum, blogs and news.", "labels": [], "entities": []}, {"text": "provides key statistics on these datasets.", "labels": [], "entities": []}, {"text": "In the TAC 10 evaluation setting, EL systems are given as input a document and a query mention with its offsets in the input document.", "labels": [], "entities": [{"text": "TAC 10 evaluation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.5487259527047476}]}, {"text": "As the output, systems need to predict the KB id of the input query mention if it exists in the KB or N IL if it does not.", "labels": [], "entities": []}, {"text": "Further, they need to cluster the mentions which contain the same N IL ids across queries.", "labels": [], "entities": []}, {"text": "The training dataset, WikiTrain, consists of 10,000 random Wikipedia pages, where all of the phrases that link to other Wikipedia articles are treated as mentions, and the target Wikipedia page is the label.", "labels": [], "entities": [{"text": "WikiTrain", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.9094398617744446}]}, {"text": "The dataset was made available by  We follow standard measures used in the literature for the entity linking task.", "labels": [], "entities": [{"text": "entity linking task", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8296411434809366}]}, {"text": "To evaluate EL accuracy on ACE and MSNBC, we report on a Bag-of-Titles (BOT) F1 evaluation as introduced by).", "labels": [], "entities": [{"text": "EL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9669236540794373}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9240604639053345}, {"text": "ACE and MSNBC", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.7719610730806986}, {"text": "Bag-of-Titles (BOT) F1 evaluation", "start_pos": 57, "end_pos": 90, "type": "METRIC", "confidence": 0.6596366316080093}]}, {"text": "In BOT-F1, we compare the set of Wikipedia titles output fora document with the gold set of titles for that document (ignoring duplicates), and compute standard precision, recall, and F1 measures.", "labels": [], "entities": [{"text": "BOT-F1", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.5648404359817505}, {"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9987425208091736}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.998313307762146}, {"text": "F1", "start_pos": 184, "end_pos": 186, "type": "METRIC", "confidence": 0.9995562434196472}]}, {"text": "On the TAC dataset, we use standard metrics B 3 + variant of precision, recall and F1.", "labels": [], "entities": [{"text": "TAC dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.8770822286605835}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996706247329712}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9995484948158264}, {"text": "F1", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9993852376937866}]}, {"text": "On these datasets, the B 3 + F 1 metric includes the clustering score for the N IL entities, and hence systems that only perform binary N IL prediction would be heavily penalized 11 .  Note that LIEL was trained only on the English Wikitrain dataset (Section 3.1), and then applied, unchanged, to all the evaluation datasets across languages and domains described in Section 3.1.", "labels": [], "entities": [{"text": "B 3 + F 1 metric", "start_pos": 23, "end_pos": 39, "type": "METRIC", "confidence": 0.9440165460109711}, {"text": "LIEL", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.8756908774375916}, {"text": "English Wikitrain dataset", "start_pos": 224, "end_pos": 249, "type": "DATASET", "confidence": 0.8230770826339722}]}, {"text": "Hence, it is the same instance of the model for all languages.", "labels": [], "entities": []}, {"text": "As we will observe, this one system consistently outperforms the state of the art, even though it is using exactly the same trained model across the datasets.", "labels": [], "entities": []}, {"text": "We consider this to be the take-away message of this paper.", "labels": [], "entities": []}, {"text": "Chinese: shows the results of LIEL's performance on the Chinese benchmark dataset compared to the state-of-the-art.", "labels": [], "entities": [{"text": "Chinese benchmark dataset", "start_pos": 56, "end_pos": 81, "type": "DATASET", "confidence": 0.8945585687955221}]}, {"text": "Systems 7 and 8 obtains almost similar scores.", "labels": [], "entities": []}, {"text": "We observe that LIEL is tied with System 1 and achieves competitive performance compared to Systems 7 and 8 (note that LIEL has a confidence interval of [0.597, 0.632]) which requires labeled Chinese TAC data to be trained on and the same model does notwork for other languages.", "labels": [], "entities": []}, {"text": "Emphasizing again: LIEL is trained only once, on English, and tested on Chinese unchanged.", "labels": [], "entities": [{"text": "LIEL", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9970197081565857}]}], "tableCaptions": [{"text": " Table 1: Data statistics: number of mention  queries, % of mention queries that have their  referents present in the Wikipedia/KB, and %  of mention queries that have no referents in  Wikipedia/KB as per our datasets. En=English,  Es=Spanish and Zh=Chinese for the evaluation  data for TAC for the years 2013 and 2014.", "labels": [], "entities": [{"text": "Wikipedia/KB", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9093349973360697}, {"text": "Wikipedia/KB", "start_pos": 185, "end_pos": 197, "type": "DATASET", "confidence": 0.8906209667523702}, {"text": "TAC", "start_pos": 287, "end_pos": 290, "type": "DATASET", "confidence": 0.7746442556381226}]}]}