{"title": [], "abstractContent": [{"text": "We present anew, structured approach to text simplification using conditional random fields over top-down traversals of dependency graphs that jointly predicts possible compressions and paraphrases.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7891001105308533}]}, {"text": "Our model reaches readability scores comparable to word-based compression approaches across a range of metrics and human judgements while maintaining more of the important information.", "labels": [], "entities": [{"text": "word-based compression", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7206911593675613}]}], "introductionContent": [{"text": "Sentence-level text simplification is the problem of automatically modifying sentences so that they become easier to read, while maintaining most of the relevant information in them.", "labels": [], "entities": [{"text": "Sentence-level text simplification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7930134733517965}]}, {"text": "This can benefit applications as pre-processing for machine translation and assisting technologies for readers with reduced literacy.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7892627120018005}]}, {"text": "Sentence-level text simplification ignores sentence splitting and reordering, and typically focuses on compression (deletion of words) and paraphrasing or lexical substitution.", "labels": [], "entities": [{"text": "Sentence-level text simplification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8135660886764526}, {"text": "sentence splitting", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7369929850101471}]}, {"text": "We include paraphrasing and lexical substitution here, while previous work in sentence simplification has often focused exclusively on deletion.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7571028470993042}, {"text": "sentence simplification", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7057988196611404}]}, {"text": "Approaches that address compression and paraphrasing (or more tasks) integrally include (.", "labels": [], "entities": []}, {"text": "Simplification beyond deletion is motivated by observation that abstractive sentence summaries written by humans often \"include paraphrases or synonyms and use alternative syntactic constructions ('gave John the book' versus 'gave the book to John').\"", "labels": [], "entities": [{"text": "abstractive sentence summaries written by humans", "start_pos": 64, "end_pos": 112, "type": "TASK", "confidence": 0.7993892927964529}]}, {"text": "Such lexical or syntactic alternations may contribute strongly to the readability of a sentence if they replace difficult words with shorter or more familiar ones, in particular for low-literacy readers (.", "labels": [], "entities": []}, {"text": "Our joint approach to deletion and paraphrasing works against the limitation that abstractive simplifications \"are not capable of being generated by.] most sentence compression algorithms\".", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.7398125529289246}]}, {"text": "Furthermore, a central concern in text simplification is to ensure the grammaticality of the output, especially with low-proficiency readers as the target audience.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7814715802669525}]}, {"text": "Our approach to this problem is to remove or paraphrase entire syntactic units in the original sentence, thus avoiding to remove phrase heads without removing their arguments or modifiers.", "labels": [], "entities": []}, {"text": "Like, we rely on dependency structures rather than constituent structures, which promises more robust syntactic analysis and allows us to operate on discontinuous syntactic units.", "labels": [], "entities": []}, {"text": "Contributions We present a sentence simplification model which is, to the best of our knowledge, the first model that uses structured prediction over dependency trees and models compression and paraphrasing jointly.", "labels": [], "entities": [{"text": "sentence simplification", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6957959681749344}]}, {"text": "Our model uses Viterbi decoding rather than scoring of all candidates and outputs probabilities reflecting model confidence.", "labels": [], "entities": []}], "datasetContent": [{"text": "Baselines In the following experiments, we compare our approach to state-of-the-art approaches to sentence compression and joint compression/paraphrasing.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7932380437850952}, {"text": "joint compression/paraphrasing", "start_pos": 123, "end_pos": 153, "type": "TASK", "confidence": 0.7952069789171219}]}, {"text": "For the first of these two categories, we consider the LSTM system described in as well as the results reported therein for the MIRA system).", "labels": [], "entities": [{"text": "MIRA system", "start_pos": 128, "end_pos": 139, "type": "DATASET", "confidence": 0.6919127404689789}]}, {"text": "As a joint approach, we consider Reluctant Trimmer (RT), a simplification system that employs synchronous dependency grammars ().", "labels": [], "entities": [{"text": "Reluctant Trimmer (RT)", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.6695630669593811}]}, {"text": "Since the LSTM system requires great amounts of training data, which were not available to us, we cannot reproduce its out-: Performance on joint deletion and paraphrasing detection for our tree labeling system (evaluating both on entire subtrees and token level) as well as for the RT baseline (tokens only).", "labels": [], "entities": [{"text": "paraphrasing detection", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.7108880430459976}]}, {"text": "Note that RT is trained on the (Simple) English Wikipedia, not on the Google compressions, and therefore the results may not be directly comparable.", "labels": [], "entities": [{"text": "RT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8121908903121948}, {"text": "Simple) English Wikipedia", "start_pos": 32, "end_pos": 57, "type": "DATASET", "confidence": 0.596709705889225}]}, {"text": "put and therefore limit our comparison of human rankings to the eleven output examples provided in the paper.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on joint deletion and para- phrasing detection for our tree labeling system  (evaluating both on entire subtrees and token level)  as well as for the RT baseline (tokens only).", "labels": [], "entities": [{"text": "para- phrasing detection", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.7204470336437225}]}, {"text": " Table 2: Compression ratios and automatic read- ability scores for the Google compression data set,  compared to the system output. Readability is in- dicated by a high Flesh Reading Ease score and a  low Dale-Chall score. * indicates differences com- pared to the original sentences that are significant  at p < 10 \u22123 .", "labels": [], "entities": [{"text": "Compression", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9656258225440979}, {"text": "automatic read- ability", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.9085440933704376}, {"text": "Google compression data set", "start_pos": 72, "end_pos": 99, "type": "DATASET", "confidence": 0.9108498245477676}, {"text": "Flesh Reading Ease score", "start_pos": 170, "end_pos": 194, "type": "METRIC", "confidence": 0.9502421915531158}]}, {"text": " Table 3: Mean readability and informativeness rat- ings for the first 200 sentences in the Google data  (upper) and for the 11 sample sentences listed in  Filippova et al. (2015) (lower).", "labels": [], "entities": [{"text": "informativeness rat- ings", "start_pos": 31, "end_pos": 56, "type": "METRIC", "confidence": 0.7860851436853409}, {"text": "Google data", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9436196386814117}]}]}