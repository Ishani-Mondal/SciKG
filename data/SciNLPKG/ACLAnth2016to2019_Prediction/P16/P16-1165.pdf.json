{"title": [{"text": "Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models", "labels": [], "entities": [{"text": "Speech Act Modeling of Written Asynchronous Conversations", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.7045081981590816}]}], "abstractContent": [{"text": "This paper addresses the problem of speech act recognition in written asyn-chronous conversations (e.g., fora, emails).", "labels": [], "entities": [{"text": "speech act recognition in written asyn-chronous conversations (e.g., fora, emails)", "start_pos": 36, "end_pos": 118, "type": "TASK", "confidence": 0.7503231167793274}]}, {"text": "We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences.", "labels": [], "entities": []}, {"text": "Our models use sentence representations encoded by along short term memory (LSTM) recurrent neural model.", "labels": [], "entities": []}, {"text": "Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTMs provide better task-specific representations, and (ii) the global joint model improves over local models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Asynchronous conversations, where participants communicate with each other at different times (e.g., fora, emails), have become very common for discussing events, issues, queries and life experiences.", "labels": [], "entities": []}, {"text": "In doing so, participants interact with each other in complex ways, performing certain communicative acts like asking questions, requesting information or suggesting something.", "labels": [], "entities": []}, {"text": "These are called speech acts.", "labels": [], "entities": []}, {"text": "For example, consider the excerpt of a forum conversation from our corpus in.", "labels": [], "entities": []}, {"text": "The participant who posted the first comment C 1 , describes his situation by the first two sentences and then asks a question in the third sentence.", "labels": [], "entities": []}, {"text": "Other participants respond to the query by suggesting something or asking for clarification.", "labels": [], "entities": []}, {"text": "In this process, the participants get into a conversation by taking turns, each of which consists of one or more speech acts.", "labels": [], "entities": []}, {"text": "The two-part structures across posts like 'question-answer' and 'request-grant' are called adjacency pairs.", "labels": [], "entities": []}, {"text": "Identification of speech acts is an important step towards deep conversation analysis in these media (), and has been shown to be useful in many downstream applications including summarization ( and question answering.", "labels": [], "entities": [{"text": "Identification of speech acts", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8841310143470764}, {"text": "deep conversation analysis", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.6563325822353363}, {"text": "summarization", "start_pos": 179, "end_pos": 192, "type": "TASK", "confidence": 0.9942125678062439}, {"text": "question answering", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.9168545603752136}]}, {"text": "Previous attempts to automatic (sentence-level) speech act recognition in asynchronous conversation () suffer from at least one of the two major flaws.", "labels": [], "entities": [{"text": "sentence-level) speech act recognition", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.6135032176971436}]}, {"text": "Firstly, they use bag-of-word (BOW) representation (e.g., unigram, bigram) to encode lexical information in a sentence.", "labels": [], "entities": []}, {"text": "However, consider the suggestion sentences in the example.", "labels": [], "entities": []}, {"text": "Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to identify the right speech act.", "labels": [], "entities": []}, {"text": "Furthermore, BOW representation could be quite sparse and may not generalize well when used in classification models.", "labels": [], "entities": [{"text": "BOW", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9422869682312012}]}, {"text": "Secondly, existing approaches mostly disregard conversational dependencies between sentences.", "labels": [], "entities": []}, {"text": "For instance, consider the example again, where we tag the sentences with the human annotations ('Human') and with the predictions of a local ('Local') classifier that considers word order for sentence representation but classifies each sentence separately.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 193, "end_pos": 216, "type": "TASK", "confidence": 0.7565096616744995}]}, {"text": "Prediction errors are underlined and highlighted in red.", "labels": [], "entities": [{"text": "Prediction errors", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.9501917660236359}]}, {"text": "Notice the first and second sentences of comment 4, which are tagged mistakenly as statement and response, respectively, by our best local classifier.", "labels": [], "entities": []}, {"text": "We hypothesize that some of the errors made by the local classifier could be corrected by employing a global joint model that performs a collective classification taking into account the conversational dependencies between sentences (e.g., adjacency relations).", "labels": [], "entities": []}, {"text": "However, unlike synchronous conversations (e.g., phone, meeting), modeling conversational dependencies between sentences in asynchronous conversation is challenging, especially in those where explicit thread structure (reply-to relations) is missing, which is also our case.", "labels": [], "entities": []}, {"text": "The conversational flow often lacks sequential dependencies in its temporal order.", "labels": [], "entities": []}, {"text": "For example, if we arrange the sentences as they arrive in the conversation, it becomes hard to capture any dependency between the act types because the two components of the adjacency pairs can be far apart in the sequence.", "labels": [], "entities": []}, {"text": "This leaves us with one open research question: how to model the dependencies between sentences in a single comment and between sentences across different comments?", "labels": [], "entities": []}, {"text": "In this paper, we attempt to address this question by designing and experimenting with conditional structured models over arbitrary graph structure of the conversation.", "labels": [], "entities": []}, {"text": "More concretely, we make the following contributions.", "labels": [], "entities": []}, {"text": "Firstly, we propose to use Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) hidden layer to perform composition of phrases and to represent sentences using distributed condensed vectors (i.e., embeddings).", "labels": [], "entities": []}, {"text": "We experiment with both unidirectional and bidirectional RNNs.", "labels": [], "entities": []}, {"text": "Secondly, we propose conditional structured models in the form of pairwise Conditional Random Field (Murphy, 2012) over arbitrary conversational structures.", "labels": [], "entities": []}, {"text": "We experiment with different variations of this model to capture different types of interactions between sentences inside the comments and across the comments.", "labels": [], "entities": []}, {"text": "These models use the LSTM encoded vectors as feature vectors for performing the classification task jointly.", "labels": [], "entities": []}, {"text": "As a secondary contribution, we also present and release a forum dataset annotated with a standard speech act tagset.", "labels": [], "entities": []}, {"text": "We train our models on different settings using synchronous and asynchronous corpora, and evaluate on two forum datasets.", "labels": [], "entities": []}, {"text": "Our main findings are: (i) LSTM RNNs provide better representation than BOW; (ii) Bidirectional LSTMs, which encode a sentence using two vectors provide better representation than the unidirectional ones; and (iii) Global joint models improve over local models given that it considers the right graph structure.", "labels": [], "entities": []}, {"text": "The source code and the new dataset are available at http://alt.qcri.", "labels": [], "entities": []}, {"text": "org/tools/speech-act/", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental settings, results and analysis.", "labels": [], "entities": []}, {"text": "We evaluate our models on the two forum corpora QC3 and TA.", "labels": [], "entities": [{"text": "QC3", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.787466287612915}, {"text": "TA", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.615199089050293}]}, {"text": "For performance comparison, we use both accuracy and macro-averaged F 1 score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9997603297233582}, {"text": "F 1 score", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9399148027102152}]}, {"text": "Accuracy gives the overall performance of a classifier but could be biased to most populated ones.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9717455506324768}]}, {"text": "Macro-averaged F 1 weights equally every class and is not influenced by class imbalance.", "labels": [], "entities": []}, {"text": "Statistical significance tests are done using an approximate randomization test based on the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9986761212348938}]}, {"text": "Because of the noise and informal nature of conversational texts, we performed a series of preprocessing steps.", "labels": [], "entities": []}, {"text": "We normalize all characters to their lower-cased forms, truncate elongations to two characters, spell out every digit and URL.", "labels": [], "entities": []}, {"text": "We further tokenized the texts using the CMU TweetNLP tool ().", "labels": [], "entities": [{"text": "CMU TweetNLP", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8257632553577423}]}, {"text": "In the following, we first demonstrate the effectiveness of LSTM RNNs for learning representations of sentences automatically to identify their speech acts.", "labels": [], "entities": []}, {"text": "Then in subsection 4.2, we show the usefulness of pairwise CRFs for capturing conversational dependencies in speech act recognition.", "labels": [], "entities": [{"text": "speech act recognition", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.6686956087748209}]}], "tableCaptions": [{"text": " Table 2: Statistics about TA and BC3 corpora.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of speech acts in our corpora.", "labels": [], "entities": [{"text": "Distribution of speech acts", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8796200305223465}]}, {"text": " Table 4: Corpus statistics for QC3.", "labels": [], "entities": [{"text": "QC3", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8929007649421692}]}, {"text": " Table 7: Results on CAT dataset.", "labels": [], "entities": [{"text": "CAT dataset", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.8463501632213593}]}, {"text": " Table 8: Setting for CON dataset. The numbers in- side parentheses indicate the number of sentences.", "labels": [], "entities": [{"text": "CON dataset", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.7309094667434692}]}]}