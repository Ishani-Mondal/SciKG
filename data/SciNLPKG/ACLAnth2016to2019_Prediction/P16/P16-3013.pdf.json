{"title": [{"text": "Improving Dependency Parsing Using Sentence Clause Charts", "labels": [], "entities": [{"text": "Improving Dependency Parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8653021256128947}]}], "abstractContent": [{"text": "We propose a method for improving the dependency parsing of complex sentences.", "labels": [], "entities": [{"text": "dependency parsing of complex sentences", "start_pos": 38, "end_pos": 77, "type": "TASK", "confidence": 0.8303065776824952}]}, {"text": "This method assumes segmentation of input sentences into clauses and does not require to retrain a parser of one's choice.", "labels": [], "entities": []}, {"text": "We represent a sentence clause structure using clause charts that provide a layer of embedding for each clause in the sentence.", "labels": [], "entities": []}, {"text": "Then we formulate a parsing strategy as a two-stage process where (i) coordinated and subordinated clauses of the sentence are parsed separately with respect to the sentence clause chart and (ii) their dependency trees become subtrees of the final tree of the sentence.", "labels": [], "entities": []}, {"text": "The object language is Czech and the parser used is a maximum spanning tree parser trained on the Prague Dependency Treebank.", "labels": [], "entities": [{"text": "Prague Dependency Treebank", "start_pos": 98, "end_pos": 124, "type": "DATASET", "confidence": 0.9626610080401102}]}, {"text": "We have achieved an average 0.97% improvement in the un-labeled attachment score.", "labels": [], "entities": []}, {"text": "Although the method has been designed for the dependency parsing of Czech, it is useful for other parsing techniques and languages.", "labels": [], "entities": [{"text": "dependency parsing of Czech", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.8643028289079666}]}], "introductionContent": [{"text": "Syntactic parsing is an integral part of a complex text processing pipeline whose quality impacts the overall performance of the text processing system.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8858921527862549}]}, {"text": "For illustration, we share our experience with a system focused on extracting semantic relations from unstructured texts.", "labels": [], "entities": [{"text": "extracting semantic relations from unstructured texts", "start_pos": 67, "end_pos": 120, "type": "TASK", "confidence": 0.7742446263631185}]}, {"text": "Namely, we have developed the RExtractor system, 1 which processes texts by linguistically-aware tools and extracts entities and relations using queries over dependency trees.", "labels": [], "entities": []}, {"text": "The language used for testing RExtractor was Czech and the legal domain was chosen to be explored in detail (.", "labels": [], "entities": []}, {"text": "We evaluated RExtractor on the Czech Legal Text Treebank (CLTT) enriched with manually annotated entities and their relations (.", "labels": [], "entities": [{"text": "Czech Legal Text Treebank (CLTT)", "start_pos": 31, "end_pos": 63, "type": "DATASET", "confidence": 0.9522885254451207}]}, {"text": "Because of the lack of any Czech gold legal-domain treebank, we used a parser trained on newspaper texts to parse CLTT.", "labels": [], "entities": [{"text": "Czech gold legal-domain treebank", "start_pos": 27, "end_pos": 59, "type": "DATASET", "confidence": 0.8313501477241516}, {"text": "parse CLTT", "start_pos": 108, "end_pos": 118, "type": "TASK", "confidence": 0.6811683475971222}]}, {"text": "The RExtractor system achieved precision of 80.6% and recall of 63.2% and we identified three sources of errors: (i) incorrect dependency tree (59.7%), (ii) missing or incorrectly formulated extraction query (38.3%), (iii) missing or incorrectly recognized entity (2.1%).", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.999570906162262}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9997708201408386}]}, {"text": "One can see that the errors are caused mainly by the insufficient quality of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7702915668487549}]}, {"text": "The main reason why it happens is that newspaper texts differ from legal texts in several language phenomena influenced by the high frequency of very long sentences in legal texts.", "labels": [], "entities": []}, {"text": "provides evidence of difficulty with dependency parsing long sentences -as the sentence length increases, the unlabeled attachment score decreases.", "labels": [], "entities": [{"text": "dependency parsing long sentences", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.8995643705129623}, {"text": "unlabeled attachment score", "start_pos": 110, "end_pos": 136, "type": "METRIC", "confidence": 0.7300985256830851}]}, {"text": "The numbers are provided for two Czech dependency treebanks, namely the Prague Dependency Treebank with the development and evaluation test subsets 2 (PDT, dtest, etest, resp.) and the Czech Academic Corpus (CAC) 3 , see and, resp.", "labels": [], "entities": [{"text": "Prague Dependency Treebank", "start_pos": 72, "end_pos": 98, "type": "DATASET", "confidence": 0.9385812481244405}, {"text": "Czech Academic Corpus (CAC) 3", "start_pos": 185, "end_pos": 214, "type": "DATASET", "confidence": 0.9185012578964233}]}, {"text": "This paper describes our method how to use information about a sentence clause structure in full-scale dependency parsing.", "labels": [], "entities": [{"text": "full-scale dependency parsing", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.5854346454143524}]}, {"text": "Section 2 lists a number of previous approaches to improve dependency parsing including selected works on parsing Czech.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.843047022819519}, {"text": "parsing", "start_pos": 106, "end_pos": 113, "type": "TASK", "confidence": 0.9724618792533875}]}, {"text": "The data and tools used in our experiments are summarized in Section 3.", "labels": [], "entities": []}, {"text": "We represent sentence clause structures using clause charts defined and quantitatively studied in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we propose an original strategy to parse pairs of coordinated and subordinated clauses and apply it on the data.", "labels": [], "entities": [{"text": "parse pairs of coordinated and subordinated clauses", "start_pos": 49, "end_pos": 100, "type": "TASK", "confidence": 0.7554594618933541}]}, {"text": "Section 6 outlines our future plans towards better parsing long sentences.", "labels": [], "entities": [{"text": "parsing long sentences", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.9024301767349243}]}], "datasetContent": [{"text": "We present a method for improving dependency parsing of long sentences.", "labels": [], "entities": [{"text": "dependency parsing of long sentences", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.8433469057083129}]}, {"text": "In particular, we formulate an algorithm for parsing the two most frequent clause structures, namely coordinated clauses 0B0 and governing and dependent clauses 0B1.", "labels": [], "entities": []}, {"text": "The other types of clause structures are processed as usual using full-scale parsing.", "labels": [], "entities": []}, {"text": "The experiments exploit an existing dependency parser trained on complete sentences, namely the MST parser -see Section 3 for details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Part of the treebank (Split), the number of  sentences (Sent.), the number of tokens (Tokens)  and the unlabeled attachment score (UAS) of MST.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 113, "end_pos": 145, "type": "METRIC", "confidence": 0.7861533015966415}]}, {"text": " Table 2: Relative frequency of the five most fre- quent clause charts in PDT 3.0 and CAC 2.0 (Rel.  freq.) and the unlabeled attachment score of MST  evaluated on the particular subsets PDT train, PDT  dtest, PDT etest, CAC 2.0.", "labels": [], "entities": [{"text": "Relative frequency", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9561067223548889}]}, {"text": " Table 3: Parsing evaluation on the 0B0 sen- tences of three different parsing strategies: full- scale parsing (FS) using MST, parsing individ- ual clauses (Clauses), and full-scale parsing using  CCP (CCP).", "labels": [], "entities": [{"text": "full- scale parsing (FS)", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.6470641110624585}, {"text": "parsing individ- ual clauses", "start_pos": 127, "end_pos": 155, "type": "TASK", "confidence": 0.856257438659668}, {"text": "full-scale parsing", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.6028291583061218}]}, {"text": " Table 4: Parsing evaluation on the 0B1 sentences.", "labels": [], "entities": [{"text": "Parsing evaluation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8551861643791199}]}, {"text": " Table 5: Parsing evaluation on the 0B1B0 sen- tences.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9166297912597656}]}, {"text": " Table 6: Parsing evaluation on the sentences con- taining at least two clauses.", "labels": [], "entities": [{"text": "Parsing evaluation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8897178769111633}]}, {"text": " Table 7: Final comparison of full-scale parsing  and CCP.", "labels": [], "entities": [{"text": "full-scale parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.532745361328125}]}]}