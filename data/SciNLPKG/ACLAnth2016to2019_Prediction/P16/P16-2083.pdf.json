{"title": [{"text": "Word Embedding Calculus in Meaningful Ultradense Subspaces", "labels": [], "entities": [{"text": "Word Embedding Calculus in Meaningful Ultradense Subspaces", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.6390423902443477}]}], "abstractContent": [{"text": "We decompose a standard embedding space into interpretable orthogonal sub-spaces and a \"remainder\" subspace.", "labels": [], "entities": []}, {"text": "We consider four interpretable subspaces in this paper: polarity, concreteness, frequency and part-of-speech (POS) sub-spaces.", "labels": [], "entities": []}, {"text": "We introduce anew calculus for subspaces that supports operations like \"\u22121 \u00d7 hate = love\" and \"give me a neutral word for greasy\" (i.e., oleaginous).", "labels": [], "entities": []}, {"text": "This calculus extends analogy computations like \"king\u2212man+woman = queen\".", "labels": [], "entities": []}, {"text": "For the tasks of Antonym Classification and POS Tagging our method outperforms the state of the art.", "labels": [], "entities": [{"text": "Antonym Classification", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7133790254592896}, {"text": "POS Tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.84443199634552}]}, {"text": "We create test sets for Morphological Analogies and for the new task of Polarity Spectrum Creation.", "labels": [], "entities": [{"text": "Morphological Analogies", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.67014180123806}, {"text": "Polarity Spectrum Creation", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.6599168082078298}]}], "introductionContent": [{"text": "Word embeddings are usually trained on an objective that ensures that words occurring in similar contexts have similar embeddings.", "labels": [], "entities": []}, {"text": "This makes them useful for many tasks, but has drawbacks for others; e.g., antonyms are often interchangeable in context and thus have similar word embeddings even though they denote opposites.", "labels": [], "entities": []}, {"text": "If we think of word embeddings as members of a (commutative or Abelian) group, then antonyms should be inverses of (as opposed to similar to) each other.", "labels": [], "entities": []}, {"text": "In this paper, we use DENSIFIER ( to decompose a standard embedding space into interpretable orthogonal subspaces, including a one-dimensional polarity subspace as well as concreteness, frequency and POS subspaces.", "labels": [], "entities": [{"text": "DENSIFIER", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.874273955821991}]}, {"text": "We introduce anew calculus for subspaces in which antonyms are inverses, e.g., \"\u22121 \u00d7 hate = love\".", "labels": [], "entities": []}, {"text": "The formula shows what happens in the polarity subspace; the orthogonal complement (all the remaining subspaces) is kept fixed.", "labels": [], "entities": []}, {"text": "We show below that we can predict an entire polarity spectrum based on the subspace, e.g., the four-word spectrum hate, dislike, like, love.", "labels": [], "entities": []}, {"text": "Similar to polarity, we explore other interpretable subspaces and do operations such as: given a concrete word like friend find the abstract word friendship (concreteness); given the frequent word friend find a less frequent synonym like comrade (frequency); and given the noun friend find the verb befriend (POS).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example word spectra for polarity, con- creteness and frequency on two different corpora.  Queries are bold.", "labels": [], "entities": []}, {"text": " Table 2: Results for Antonym Classification", "labels": [], "entities": [{"text": "Antonym Classification", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9661188125610352}]}, {"text": " Table 3: Results for POS tagging. LSJU = Stanford. SVM = SVMTool. F=FLORS. We show three state- of-the-art taggers (lines 1-3), FLORS extended with 300-dimensional embeddings (4) and extended with  UD embeddings (5).  \u2020: significantly better than the best result in the same column (\u03b1 = .05, one-tailed  Z-test).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.8805429935455322}, {"text": "F", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.9868796467781067}, {"text": "FLORS", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9918473958969116}, {"text": "FLORS", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.968572735786438}]}, {"text": " Table 4: Results for Polarity Spectrum Creation:  MAP, Spearman's \u03c1 (one spectrum) and average \u03c1  (two subspectra)", "labels": [], "entities": [{"text": "Polarity Spectrum Creation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.7187075217564901}, {"text": "MAP", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9928097724914551}]}, {"text": " Table 5: Accuracy @1 on test for Morphological  Analogy.  \u2020: significantly better than the corre- sponding result in the same row (\u03b1 = .05, one- tailed Z-test).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999447762966156}, {"text": "Morphological  Analogy", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8160170614719391}]}]}