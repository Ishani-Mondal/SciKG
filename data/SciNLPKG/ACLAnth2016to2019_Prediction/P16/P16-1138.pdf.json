{"title": [{"text": "Simpler Context-Dependent Logical Forms via Model Projections", "labels": [], "entities": [{"text": "Simpler Context-Dependent Logical Forms", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6755239814519882}]}], "abstractContent": [{"text": "We consider the task of learning a context-dependent mapping from utterances to de-notations.", "labels": [], "entities": []}, {"text": "With only denotations at training time, we must search over a combina-torially large space of logical forms, which is even larger with context-dependent utterances.", "labels": [], "entities": []}, {"text": "To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms.", "labels": [], "entities": []}, {"text": "Though less expressive, we find that these simpler models are much faster and can be surprisingly effective.", "labels": [], "entities": []}, {"text": "Moreover, they can be used to bootstrap the full model.", "labels": [], "entities": []}, {"text": "Finally, we collected three new context-dependent semantic parsing datasets, and develop anew left-to-right parser.", "labels": [], "entities": [{"text": "context-dependent semantic parsing", "start_pos": 32, "end_pos": 66, "type": "TASK", "confidence": 0.6347542107105255}]}], "introductionContent": [{"text": "Suppose we are only told that apiece of text (a command) in some context (state of the world) has some denotation (the effect of the command)-see for an example.", "labels": [], "entities": []}, {"text": "How can we build a system to learn from examples like these with no initial knowledge about what any of the words mean?", "labels": [], "entities": []}, {"text": "We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (.", "labels": [], "entities": []}, {"text": "More recent work learns directly from denotations (), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms.", "labels": [], "entities": []}, {"text": "With no initial lexicon and longer contextdependent texts, our situation is exacerbated.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created three new context-dependent datasets, ALCHEMY, SCENE, and TANGRAMS (see fora summary), which aim to capture a diverse set of context-dependent linguistic phenomena such as ellipsis (e.g., \"mix\" in ALCHEMY), anaphora on entities (e.g., \"he\" in SCENE), and anaphora on actions (e.g., \"repeat step 3\", \"bring it back\" in TANGRAMS).", "labels": [], "entities": [{"text": "TANGRAMS", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9220113754272461}]}, {"text": "For each dataset, we have a set of properties and actions.", "labels": [], "entities": []}, {"text": "In ALCHEMY, properties are color, and amount; actions are pour, drain, and mix.", "labels": [], "entities": []}, {"text": "In SCENE, properties are hat-color and shirt-color; actions are enter, leave, move, and trade-hats.", "labels": [], "entities": []}, {"text": "In TANGRAMS, there is one property (shape), and actions are add, remove, and swap.", "labels": [], "entities": [{"text": "TANGRAMS", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.5721176266670227}]}, {"text": "In addition, we include the position property (pos) in each dataset.", "labels": [], "entities": []}, {"text": "Each example has L = 5 utterances, each denoting some transformation of the world state.", "labels": [], "entities": []}, {"text": "Our datasets are unique in that they are grounded to a world state and have rich linguistic context-dependence.", "labels": [], "entities": []}, {"text": "In the context-dependent ATIS dataset ( used by, logical forms of utterances depend on previous logical forms, though there is no world state and the linguistic phenomena is limited to nominal references.", "labels": [], "entities": [{"text": "ATIS dataset", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.8160587549209595}]}, {"text": "In the map navigation dataset), used by, utterances only reference the current world state.", "labels": [], "entities": []}, {"text": "released a corpus of annotated dialogues, which has interesting linguistic contextdependence, but there is no world state.", "labels": [], "entities": []}, {"text": "Our strategy was to automatically generate sequences of world states and ask Amazon Mechanical Turk (AMT) workers to describe the successive transformations.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.815426101287206}]}, {"text": "Specifically, we started with a random world state w 0 . For each i = 1, . .", "labels": [], "entities": []}, {"text": ", L, we sample a valid action and argument (e.g., pour(beaker1, beaker2)).", "labels": [], "entities": []}, {"text": "To encourage context-dependent descriptions, we upweight recently used actions and arguments (e.g., the next action is more like to be drain(beaker2) rather than drain(beaker5)).", "labels": [], "entities": []}, {"text": "Next, we presented an AMT worker with states w 0 , . .", "labels": [], "entities": [{"text": "AMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9490227103233337}]}, {"text": ", w Land asked the worker to write a description in between each pair of successive states.", "labels": [], "entities": []}, {"text": "In initial experiments, we found it rather nontrivial to obtain interesting linguistic contextdependence in these micro-domains: often a context-independent utterance such as \"beaker 2\" is just clearer and not much longer than a possibly ambiguous \"it\".", "labels": [], "entities": []}, {"text": "We modified the domains to encourage more context.", "labels": [], "entities": []}, {"text": "For example, in SCENE, we removed any visual indication of absolute position and allowed people to only move next to other people.", "labels": [], "entities": [{"text": "SCENE", "start_pos": 16, "end_pos": 21, "type": "TASK", "confidence": 0.8694267272949219}]}, {"text": "This way, workers would say \"to the left of the man in the red hat\" rather than \"to position 2\".", "labels": [], "entities": []}, {"text": "Our experiments aim to explore the computationexpressivity tradeoff in going from Model A to Model B to Model C. We would expect that under the computational constraint of a finite beam size, Model A will be hurt the most, but with an: Test set accuracy and oracle accuracy for examples containing L = 3 and L = 5 utterances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9159970879554749}, {"text": "accuracy", "start_pos": 265, "end_pos": 273, "type": "METRIC", "confidence": 0.9758585691452026}]}, {"text": "Model C surpasses Model B in both accuracy and oracle on ALCHEMY and SCENE, whereas Model B does better in TANGRAMS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995452761650085}, {"text": "TANGRAMS", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.7437877655029297}]}, {"text": "infinite beam, Model A should perform better.", "labels": [], "entities": []}, {"text": "We evaluate all models on accuracy, the fraction of examples that a model predicts correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994159936904907}]}, {"text": "A predicted logical form z is deemed to be correct for an example (w 0 , x, w L ) if the predicted logical form z executes to the correct final world state w L . We also measure the oracle accuracy, which is the fraction of examples whereat least one z on the beam executes tow L . All experiments train for 6 iterations using AdaGrad ( and L 1 regularization with a coefficient of 0.001.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.972015917301178}, {"text": "AdaGrad", "start_pos": 327, "end_pos": 334, "type": "DATASET", "confidence": 0.9342778325080872}]}, {"text": "We use abeam size of 500 within each utterance, and prune to the top 5 between utterances.", "labels": [], "entities": []}, {"text": "For the first two iterations, Models B and C train on only the first utterance of each example (L = 1).", "labels": [], "entities": []}, {"text": "In the remaining iterations, the models train on two utterance examples.", "labels": [], "entities": []}, {"text": "We then evaluate on examples with L = 1, . .", "labels": [], "entities": []}, {"text": ", 5, which tests our models ability to extrapolate to longer texts.", "labels": [], "entities": []}, {"text": "We compare models B and C on the three real datasets for both L = 3 and L = 5 utterances (Model A was too expensive to use).", "labels": [], "entities": []}, {"text": "shows that on 5 utterance examples, the flatter Model C achieves an average accuracy of 20% higher than the more compositional Model B. Similarly, the average oracle accuracy is 39% higher.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9970694184303284}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9525291323661804}]}, {"text": "This suggests that (i) the correct logical form often falls off the beam for Model B due to a larger search space, and (ii) the expressivity of Model C is sufficient in many cases.", "labels": [], "entities": []}, {"text": "On the other hand, Model B outperforms Model C on the TANGRAMS dataset.", "labels": [], "entities": [{"text": "TANGRAMS dataset", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9495293498039246}]}, {"text": "This happens for two reasons.", "labels": [], "entities": []}, {"text": "The TANGRAMS dataset has the smallest search space, since all of the utterances refer to objects using position only.", "labels": [], "entities": [{"text": "TANGRAMS dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8553964495658875}]}, {"text": "Additionally, many utterances reference logical forms that Model C is unable to express, such as \"repeat the first step\", or \"add it back\".", "labels": [], "entities": []}, {"text": "shows how the models perform as the number of utterances per example varies.", "labels": [], "entities": []}, {"text": "When the search space is small (fewer number of utterances), Model B outperforms or is competitive with Model C. However, as the search space increases (tighter computational constraints), Model C does increasingly better.", "labels": [], "entities": []}, {"text": "Overall, both models perform worse as L increases, since to predict the final world state w L correctly, a model essentially needs to predict an entire sequence of logical forms z 1 , . .", "labels": [], "entities": []}, {"text": ", z L , and errors cascade.", "labels": [], "entities": [{"text": "errors", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9904503226280212}]}, {"text": "Furthermore, for larger L, the utterances tend to have richer context-dependence.", "labels": [], "entities": []}, {"text": "Due to the large search space, running model A on real data is impractical.", "labels": [], "entities": []}, {"text": "In order feasibly evaluate Model A, we constructed an artificial dataset.", "labels": [], "entities": []}, {"text": "The worlds are created using the procedure described in Section 2.2.", "labels": [], "entities": []}, {"text": "We use a simple template to generate utterances (e.g., \"drain 1 from the 2 green beaker\").", "labels": [], "entities": []}, {"text": "To reduce the search space for Model A, we only allow actions (e.g., drain) to align to verbs and property values (e.g., green) to align to adjectives.", "labels": [], "entities": []}, {"text": "Using these linguistic constraints provides a slightly optimistic assessment of Model A's performance.", "labels": [], "entities": []}, {"text": "We train on a dataset of 500 training examples and evaluate on 500 test examples.", "labels": [], "entities": []}, {"text": "We repeat this procedure for varying beam sizes, from 40 to 260.", "labels": [], "entities": []}, {"text": "The model only uses features (F1) through (F3).", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9932838678359985}]}, {"text": "Since Model A is more expressive, we would expect it to be more powerful when we have no computational constraints.", "labels": [], "entities": []}, {"text": "shows that this is indeed the case: When the beam size is greater than 250, all models attain an oracle of 1, and Model A outperforms Model B, which performs similarly to Model C. This is because the alignments provide a powerful signal for constructing the logical forms.", "labels": [], "entities": []}, {"text": "Without alignments, Models B and C learn noisier features, and accuracy suffers accordingly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9994582533836365}]}, {"text": "Model A performs the best with unconstrained computation, and Model C performs the best with constrained computation.", "labels": [], "entities": []}, {"text": "Is there someway to bridge the two?", "labels": [], "entities": []}, {"text": "Even though  Model A is unable to learn anything with beam size < 240.", "labels": [], "entities": []}, {"text": "However, for beam sizes larger than 240, Model A attains 100% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9987749457359314}]}, {"text": "Model C does better than Models A and B when the beam size is small < 40, but otherwise performs comparably to Model B. Bootstrapping Model A using Model C parameters outperforms all of the other models and attains 100% even with smaller beams.", "labels": [], "entities": []}, {"text": "Model C has limited expressivity, it can still learn to associate words like \"green\" with their corresponding predicate green.", "labels": [], "entities": []}, {"text": "These should be useful for Model A too.", "labels": [], "entities": []}, {"text": "To operationalize this, we first train Model C and use the parameters to initialize model A.", "labels": [], "entities": []}, {"text": "Then we train Model A. shows that although Model A and C predict different logical forms, the initialization allows Model C to A to perform well in constrained beam settings.", "labels": [], "entities": []}, {"text": "This bootstrapping   works here because Model C is a projection of Model A, and thus they share the same features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Test set accuracy and oracle accuracy for  examples containing L = 3 and L = 5 utterances.  Model C surpasses Model B in both accuracy and  oracle on ALCHEMY and SCENE, whereas Model  B does better in TANGRAMS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9687855243682861}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9829397201538086}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9988817572593689}, {"text": "TANGRAMS", "start_pos": 211, "end_pos": 219, "type": "DATASET", "confidence": 0.6566349267959595}]}, {"text": " Table 5: Percentage of errors for Model B and C:  Model B suffers predominantly from computation  constraints, while Model C suffers predominantly  from a lack of expressivity.", "labels": [], "entities": []}]}