{"title": [{"text": "Topic Extraction from Microblog Posts Using Conversation Structures", "labels": [], "entities": [{"text": "Topic Extraction from Microblog Posts", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8657790422439575}]}], "abstractContent": [{"text": "Conventional topic models are ineffective for topic extraction from microblog messages since the lack of structure and context among the posts renders poor message-level word co-occurrence patterns.", "labels": [], "entities": [{"text": "topic extraction from microblog messages", "start_pos": 46, "end_pos": 86, "type": "TASK", "confidence": 0.8576212406158448}]}, {"text": "In this work, we organize microblog posts as conversation trees based on re-posting and replying relations, which enrich context information to alleviate data sparseness.", "labels": [], "entities": []}, {"text": "Our model generates words according to topic dependencies derived from the conversation structures.", "labels": [], "entities": []}, {"text": "In specific , we differentiate messages as leader messages, which initiate key aspects of previously focused topics or shift the focus to different topics, and follower messages that do not introduce any new information but simply echo topics from the messages that they repost or reply.", "labels": [], "entities": []}, {"text": "Our model captures the different extents that leader and follower messages may contain the key topical words, thus further enhances the quality of the induced topics.", "labels": [], "entities": []}, {"text": "The results of thorough experiments demonstrate the effectiveness of our proposed model.", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing popularity of microblog platforms results in a huge volume of user-generated short posts.", "labels": [], "entities": []}, {"text": "Automatically modeling topics out of such massive microblog posts can uncover the hidden semantic structures of the underlying collection and can be useful to downstream applications such as microblog summarization (), user profiling (, event tracking () and soon.", "labels": [], "entities": [{"text": "microblog summarization", "start_pos": 191, "end_pos": 214, "type": "TASK", "confidence": 0.6796000301837921}, {"text": "event tracking", "start_pos": 237, "end_pos": 251, "type": "TASK", "confidence": 0.799743115901947}]}, {"text": "Popular topic models, like Probabilistic Latent Semantic Analysis (pLSA)) * * Part of this work was conducted when the first author was visiting Aston University. and Latent Dirichlet Allocation (LDA) (, model the semantic relationships between words based on their co-occurrences in documents.", "labels": [], "entities": [{"text": "Probabilistic Latent Semantic Analysis (pLSA", "start_pos": 27, "end_pos": 71, "type": "TASK", "confidence": 0.6935481329758962}]}, {"text": "They have demonstrated their success in conventional documents such as news reports and scientific articles, but perform poorly when directly applied to short and colloquial microblog content due to severe sparsity in microblog messages (.", "labels": [], "entities": []}, {"text": "A common way to deal with short text sparsity is to aggregate short messages into long pseudodocuments.", "labels": [], "entities": []}, {"text": "Most of the studies heuristically aggregate messages based on authorship, shared words (, or hashtags.", "labels": [], "entities": []}, {"text": "Some works directly take into account the word relations to alleviate document-level word sparseness (.", "labels": [], "entities": []}, {"text": "More recently, a self-aggregation-based topic model called SATM) was proposed to aggregate texts jointly with topic inference.", "labels": [], "entities": []}, {"text": "However, we argue that the existing aggregation strategies are suboptimal for modeling topics in short texts.", "labels": [], "entities": []}, {"text": "Microblogs allow users to share and comment on messages with friends through reposting or replying, similar to our everyday conversations.", "labels": [], "entities": []}, {"text": "Intuitively, the conversation structures cannot only enrich context, but also provide useful clues for identifying relevant topics.", "labels": [], "entities": []}, {"text": "This is nonetheless ignored in previous approaches.", "labels": [], "entities": []}, {"text": "Moreover, the occurrence of non-topic words such as emotional, sentimental, functional and even meaningless words are very common in microblog posts, which may distract the models from recognizing topic-related key words and thus fail to produce coherent and meaningful topics.", "labels": [], "entities": []}, {"text": "We propose a novel topic model by utilizing the structures of conversations in microblogs.", "labels": [], "entities": []}, {"text": "We link microblog posts using reposting and replying rela-tions to build conversation trees.", "labels": [], "entities": []}, {"text": "Particularly, the root of a conversation tree refers to the original post and its edges represent the reposting/replying relations.", "labels": [], "entities": []}, {"text": "[O] Just an hour ago, a series of coordinated terrorist attacks occurred in Paris !!!", "labels": [], "entities": []}, {"text": "[R2] Gunmen and suicide bombers hit a concert hall.", "labels": [], "entities": []}, {"text": "More than 100 are killed already.", "labels": [], "entities": []}, {"text": "I can't believe it's real.", "labels": [], "entities": []}, {"text": "I've just been there last month.", "labels": [], "entities": []}, {"text": "[ [R6] poor guys, terrible: An example of conversation tree.", "labels": [], "entities": []}, {"text": "[O]: the original post;: the i-th repost/reply; Arrow lines: reposting/replying relations; Dark black posts: leaders to be detected; Underlined italic words: key words representing topics illustrates an example of a conversation tree, in which messages can initiate anew topic such as and or raise anew aspect (subtopic) of the previously discussed topics such as and.", "labels": [], "entities": []}, {"text": "These messages are named as leaders, which contain salient content in topic description, e.g., the italic and underlined words in.", "labels": [], "entities": []}, {"text": "The remaining messages, named as followers, do not raise new issues but simply respond to their reposted or replied messages following what has been raised by the leaders and often contain non-topic words, e.g., OMG, OK, agree, etc.", "labels": [], "entities": [{"text": "OMG", "start_pos": 212, "end_pos": 215, "type": "METRIC", "confidence": 0.9337713718414307}]}, {"text": "Conversation tree structures from microblogs have been previously shown helpful to microblog summarization (), but have never been explored for topic modeling.", "labels": [], "entities": [{"text": "microblog summarization", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.7127366065979004}, {"text": "topic modeling", "start_pos": 144, "end_pos": 158, "type": "TASK", "confidence": 0.7041919082403183}]}, {"text": "We follows to detect leaders and followers across paths of conversation trees using Conditional Random Fields (CRF) trained on annotated data.", "labels": [], "entities": []}, {"text": "The detected leader/follower information is then incorporated as prior knowledge into our proposed topic model.", "labels": [], "entities": []}, {"text": "Our experimental results show that our model, which captures parent-child topic correlations in conversation trees and generates topics by considering messages being leaders or followers separately, is able to induce high-quality topics and outperforms a number of competitive baselines.", "labels": [], "entities": []}, {"text": "In summary, our contributions are three-fold: \u2022 We propose a novel topic model, which explicitly exploits the topic dependencies contained in conversation structures to enhance topic assignments.", "labels": [], "entities": []}, {"text": "\u2022 Our model differentiates the generative process of topical and non-topic words, according to the message where a word is drawn from being a leader or a follower.", "labels": [], "entities": []}, {"text": "This helps the model distinguish the topic-specific information from background noise.", "labels": [], "entities": []}, {"text": "\u2022 Our model outperforms state-of-the-art topic models when evaluated on a large real-world microblog dataset containing over 60K conversation trees, which is publicly available 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our LeadLDA model, we conducted experiments on real-world microblog dataset collected from Sina Weibo that has the same 140-character limitation and shares the similar market penetration as Twitter).", "labels": [], "entities": [{"text": "microblog dataset collected from Sina Weibo", "start_pos": 70, "end_pos": 113, "type": "DATASET", "confidence": 0.8694814840952555}]}, {"text": "For the hyper-parameters of LeadLDA, we fixed \u03b1 = 50/K, \u03b2 = 0.1, following the common practice in previous works (.", "labels": [], "entities": [{"text": "LeadLDA", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9577248096466064}]}, {"text": "Since there is no analogue of \u03b3 and \u03b4 in prior works, where \u03b3 controls topic dependencies of follower messages to their ancestors and \u03b4 controls the different tendencies of: Statistics of our three evaluation datasets leaders and followers covering topical and nontopic words.", "labels": [], "entities": []}, {"text": "We tuned \u03b3 and \u03b4 by grid search on a large development set containing around 120K posts and obtained \u03b3 = 50/K, \u03b4 = 0.5.", "labels": [], "entities": []}, {"text": "Because the content of posts are often incomplete and informal, it is difficult to manually annotate topics in a large scale.", "labels": [], "entities": []}, {"text": "Therefore, we follow to utilize hashtags led by '#', which are manual topic labels provided by users, as ground-truth categories of microblog messages.", "labels": [], "entities": []}, {"text": "We collected the real-time trending hashtags on Sina Weibo and utilized the hashtag-search API to crawl the posts matching the given hashtag queries.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.8709289729595184}]}, {"text": "In the end, we built a corpus containing 596,318 posts during May 1 -July 31, 2014.", "labels": [], "entities": []}, {"text": "To examine the performance of models on various topic distributions, we split the corpus into 3 datasets, each containing messages of one month.", "labels": [], "entities": []}, {"text": "Similar to, for each dataset, we manually selected 50 frequent hashtags as topics, e.g. #mh17, #worldcup, etc.", "labels": [], "entities": []}, {"text": "The experiments were conducted on the subsets of posts with the selected hashtags.", "labels": [], "entities": []}, {"text": "shows the statistics of the three subsets used in our experiments.", "labels": [], "entities": []}, {"text": "We preprocessed the datasets before topic extraction in the following steps: 1) Use FudanNLP toolkit () for word segmentation, stop words removal and POS tagging for Chinese Weibo messages; 2) Generate a vocabulary for each dataset and remove words occurring less than 5 times; 3) Remove all hashtags in texts before input them to models, since the models are expected to extract topics without knowing the hashtags, which are ground-truth topics; 4) For LeadLDA, we use the CRF-based leader detection model ( to classify messages as leaders and followers.", "labels": [], "entities": [{"text": "topic extraction", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.758690744638443}, {"text": "word segmentation", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7230382114648819}, {"text": "stop words removal", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7197531263033549}, {"text": "POS tagging", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.7480825781822205}, {"text": "CRF-based leader detection", "start_pos": 475, "end_pos": 501, "type": "TASK", "confidence": 0.535371204217275}]}, {"text": "The leader detection model was implemented by using CRF++ , which was trained on the public dataset composed of 1,300 conversation paths and achieved state-of-the-art 73.7% F1-score of classification accuracy ().", "labels": [], "entities": [{"text": "leader detection", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8654734492301941}, {"text": "F1-score", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9996858835220337}, {"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.7064902782440186}]}, {"text": "We evaluated topic models with two sets of K, i.e., the number of topics.", "labels": [], "entities": []}, {"text": "One is K = 50, to match the count of hashtags following, and the other is K = 100, much larger than the \"real\" number of topics.", "labels": [], "entities": []}, {"text": "We compared LeadLDA with the following 5 state-of-the-art basedlines.", "labels": [], "entities": [{"text": "LeadLDA", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.9116730093955994}]}, {"text": "TreeLDA: Analogous to Zhao et al.", "labels": [], "entities": [{"text": "TreeLDA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.847131609916687}]}, {"text": "(2011), where they aggregated messages posted by the same author, TreeLDA aggregates messages from one conversation tree as a pseudo-document.", "labels": [], "entities": []}, {"text": "Additionally, it includes a background word distribution to capture non-topic words controlled by a general Beta prior without differentiating leaders and followers.", "labels": [], "entities": []}, {"text": "TreeLDA can be considered as a degeneration of LeadLDA, where topics assigned to all messages are generated from the topic distributions of the conversation trees they are on.", "labels": [], "entities": []}, {"text": "StructLDA: It is another variant of LeadLDA, where topics assigned to all messages are generated based on topic transitions from their parents.", "labels": [], "entities": []}, {"text": "The strTM () utilized a similar model to capture the topic dependencies of adjacent sentences in a document.", "labels": [], "entities": []}, {"text": "Following strTM, we add a dummy topic T start emitting no word to the \"pseudo parents\" of root messages.", "labels": [], "entities": []}, {"text": "Also, we add the same background word distribution to capture non-topic words as TreeLDA does.", "labels": [], "entities": [{"text": "TreeLDA", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9354216456413269}]}, {"text": "BTM: Biterm Topic Model (BTM) 6 (Yan et al., 2013) directly models topics of all word pairs (biterms) in each post, which outperformed LDA, Mixture of Unigrams model, and the model proposed by that aggregated posts by authorship to enrich context.", "labels": [], "entities": [{"text": "BTM: Biterm Topic Model (BTM) 6", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.7844076620207893}]}, {"text": "SATM: A general unified model proposed by that aggregates documents and infers topics simultaneously.", "labels": [], "entities": [{"text": "SATM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7410873770713806}]}, {"text": "We implemented SATM and examined its effectiveness specifically on microblog data.", "labels": [], "entities": []}, {"text": "GMTM: To tackle word sparseness, utilized Gaussian Mixture Model (GMM) to cluster word embeddings generated by a log-linear word2vec model . The hyper-parameters of BTM, SATM and GMTM were set according to the best hyperparameters reported in their original papers.", "labels": [], "entities": [{"text": "GMTM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8810701370239258}]}, {"text": "For TreeLDA and StructLDA, the parameter settings were kept the same as LeadLDA since they are its variants.", "labels": [], "entities": []}, {"text": "And the background switchers were parameterized by symmetric Beta prior on 0.5, following.", "labels": [], "entities": []}, {"text": "We ran Gibbs samplings (in LeadLDA, TreeLDA, StructLDA, BTM and SATM) and EM algorithm (in GMTM) with 1,000 iterations to ensure convergence.", "labels": [], "entities": [{"text": "TreeLDA", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8048756718635559}, {"text": "EM algorithm", "start_pos": 74, "end_pos": 86, "type": "METRIC", "confidence": 0.8972079157829285}, {"text": "GMTM", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.9170384407043457}]}, {"text": "Topic model evaluation is inherently difficult.", "labels": [], "entities": [{"text": "Topic model evaluation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7158896923065186}]}, {"text": "In previous works, perplexity is a popular metric to evaluate the predictive abilities of topic models given held-out dataset with unseen words ().", "labels": [], "entities": []}, {"text": "However, have demonstrated that models with high perplexity do not necessarily generate semantically coherent topics inhuman perception.", "labels": [], "entities": []}, {"text": "Therefore, we conducted objective and subjective analysis on the coherence of produced topics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of our three evaluation datasets", "labels": [], "entities": []}, {"text": " Table 3: Absolute values of coherence scores.  Lower is better. K50: 50 topics; K100: 100 topics;  N: # of top words ranked by topic-word probabil- ities; TREE: TreeLDA; STR: StructLDA; LEAD:  LeadLDA.", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9833359718322754}, {"text": "TREE", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9905015230178833}, {"text": "LEAD", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.994904637336731}]}]}