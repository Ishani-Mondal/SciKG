{"title": [{"text": "Learning Semantically and Additively Compositional Distributional Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS).", "labels": [], "entities": [{"text": "Dependency-based Compositional Semantics (DCS)", "start_pos": 80, "end_pos": 126, "type": "TASK", "confidence": 0.6696379482746124}]}, {"text": "We show theoretical evidence that the vector compositions in our model conform to the logic of DCS.", "labels": [], "entities": []}, {"text": "Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.811548501253128}]}, {"text": "We evaluate this utility on sentence completion task and report anew state-of-the-art.", "labels": [], "entities": [{"text": "sentence completion task", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7940799991289774}]}], "introductionContent": [{"text": "A major goal of semantic processing is to map natural language utterances to representations that facilitate calculation of meanings, execution of commands, and/or inference of knowledge.", "labels": [], "entities": []}, {"text": "Formal semantics supports such representations by defining words as some functional units and combining them via a specific logic.", "labels": [], "entities": []}, {"text": "A simple and illustrative example is the Dependency-based Compositional Semantics (DCS) (.", "labels": [], "entities": [{"text": "Dependency-based Compositional Semantics (DCS", "start_pos": 41, "end_pos": 86, "type": "TASK", "confidence": 0.6337432444095612}]}, {"text": "DCS composes meanings from denotations of words (i.e. sets of things to which the words apply); say, the denotations of the concept drug and the event ban is shown in, where drug is a list of drug names and ban is a list of the subjectcomplement pairs in any ban event; then, a list of banned drugs can be constructed by first taking the COMP column of all records in ban (projection \"\u03c0 COMP \"), and then intersecting the results with drug (intersection \"\u2229\").", "labels": [], "entities": []}, {"text": "This procedure defined how words can be combined to form a meaning.", "labels": [], "entities": []}, {"text": "Better yet, the procedure can be concisely illustrated by the DCS tree of \"banned drugs\", which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2).", "labels": [], "entities": [{"text": "DCS tree of \"banned drugs", "start_pos": 62, "end_pos": 87, "type": "DATASET", "confidence": 0.8985320727030436}]}, {"text": "DCS has been shown useful in question answering () and textual entailment recognition).", "labels": [], "entities": [{"text": "question answering", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8926270306110382}, {"text": "textual entailment recognition", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.797605574131012}]}, {"text": "Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words, and progress is made in combining the word vectors to form meanings of phrases/sentences ().", "labels": [], "entities": []}, {"text": "However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics.", "labels": [], "entities": []}, {"text": "We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process.", "labels": [], "entities": []}, {"text": "Still, to find a link is challenging because any vector compositions that realize such a link must conform to the logic of the formal semantics.", "labels": [], "entities": []}, {"text": "In this paper, we establish a link between DCS and certain vector compositions, achieving a vector-based DCS by replacing denotations of words with word vectors, and realizing the composition operations such as intersection and projection as addition and linear mapping, respectively.", "labels": [], "entities": []}, {"text": "For example, to construct a vector for \"banned drugs\", one takes the word vector v ban and multiply it by a matrix M COMP , corresponding to the projection \u03c0 COMP ; then, one adds the result to the word vector v drug to realize the intersection operation ().", "labels": [], "entities": []}, {"text": "We provide a method to train the: (a) The DCS tree of \"banned drugs\", which controls (b) the calculation of its denotation.", "labels": [], "entities": [{"text": "DCS tree of \"banned drugs", "start_pos": 42, "end_pos": 67, "type": "DATASET", "confidence": 0.9031988183657328}]}, {"text": "In this paper, we learn word vectors and matrices such that (c) the same calculation is realized in distributional semantics.", "labels": [], "entities": []}, {"text": "The constructed query vector can be used to (d) retrieve a list of coarsegrained candidate answers to that query.", "labels": [], "entities": []}, {"text": "word vectors and linear mappings (i.e. matrices) jointly from unlabeled corpora.", "labels": [], "entities": []}, {"text": "The rationale for our model is as follows.", "labels": [], "entities": []}, {"text": "First, recent research has shown that additive composition of word vectors is an approximation to the situation where two words have overlapping context; therefore, it is suitable to implement an \"and\" or intersection operation (Section 3).", "labels": [], "entities": []}, {"text": "We design our model such that the resulted distributional representations are expected to have additive compositionality.", "labels": [], "entities": []}, {"text": "Second, when intersection is realized as addition, it is natural to implement projection as linear mapping, as suggested by the logical interactions between the two operations (Section 3).", "labels": [], "entities": []}, {"text": "Experimentally, we show that vectors and matrices learned by our model exhibit favorable characteristics as compared with vectors trained by) or those learned from syntactic dependencies (Section 5.1).", "labels": [], "entities": []}, {"text": "Finally, additive composition brings our model a strong ability to calculate similar vectors for similar phrases, whereas syntactic-semantic roles (e.g. SUBJ, COMP) can be distinguished by different projection matrices (e.g. M SUBJ , M COMP ).", "labels": [], "entities": []}, {"text": "We achieve near state-of-the-art performance on a wide range of phrase similarity tasks (Section 5.2) and relation classification (Section 5.3).", "labels": [], "entities": [{"text": "phrase similarity tasks", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.7920740445454916}, {"text": "relation classification", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.9157030582427979}]}, {"text": "Furthermore, we show that a vector as constructed above for \"banned drugs\" can be used as a query vector to retrieve a coarse-grained candi-  Figure 2: DCS tree fora sentence date list of banned drugs, by sorting its dot products with answer vectors that are also learned by our model).", "labels": [], "entities": []}, {"text": "This is due to the ability of our approach to provide a language model that can find likely words to fill in the blanks such as \" is a banned drug\" or \"the drug is banned by . .", "labels": [], "entities": []}, {"text": "\". A highlight is the calculation being done as if a query is \"executed\" by the DCS tree of \"banned drugs\".", "labels": [], "entities": [{"text": "DCS tree of \"banned drugs", "start_pos": 80, "end_pos": 105, "type": "DATASET", "confidence": 0.9335031112035116}]}, {"text": "We quantitatively evaluate this utility on sentence completion task () and report anew state-of-the-art (Section 5.4).", "labels": [], "entities": [{"text": "sentence completion task", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8043259779612223}]}], "datasetContent": [{"text": "For training vector-based DCS, we use Wikipedia Extractor 2 to extract texts from the 2015-12-01 dump of English Wikipedia . Then, we use Stanford Parser ( to parse all sentences and convert the UD trees into DCS trees by handwritten rules.", "labels": [], "entities": [{"text": "DCS", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.6448692083358765}]}, {"text": "We assign a weight to each path of the DCS trees as follows.", "labels": [], "entities": [{"text": "DCS trees", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.818679541349411}]}, {"text": "Problem with the naive regularizer M \u22121 M \u2212 I 2 is that, when the scale of M goes larger, it will drive M \u22121 smaller, which may lead to degeneration.", "labels": [], "entities": []}, {"text": "For any path P passing through k intermediate nodes of degrees n 1 , . .", "labels": [], "entities": []}, {"text": ", n k , respectively, we set Note that n i \u2265 2 because there is a path P passing through the node; and Weight(P ) = 1 if P consists of a single edge.", "labels": [], "entities": [{"text": "Weight(P )", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.944148987531662}]}, {"text": "The equation is intended to degrade long paths which pass through several high-valency nodes.", "labels": [], "entities": []}, {"text": "We use a random walk algorithm to sample paths such that the expected times a path is sampled equals its weight.", "labels": [], "entities": []}, {"text": "As a result, the sampled path lengths range from 1 to 19, average 2.1, with an exponential tail.", "labels": [], "entities": []}, {"text": "We convert all words which are sampled less than 1000 times to * UNKNOWN * /POS, and all prepositions occurring less than 10000 times to an *UNKNOWN* field.", "labels": [], "entities": []}, {"text": "As a result, we obtain a vocabulary of 109k words and 211 field names.", "labels": [], "entities": []}, {"text": "Using the sampled paths, vectors and matrices are trained as in Section 4 (vecDCS).", "labels": [], "entities": [{"text": "vecDCS", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.6030635237693787}]}, {"text": "The vector dimension is set to d = 250.", "labels": [], "entities": []}, {"text": "We compare with three baselines: (i) all matrices are fixed to identity (\"no matrix\"), in order to investigate the effects of meaning changes caused by syntactic-semantic roles and prepositions; (ii) the regularizer enforcing M \u22121 N to be actually the inverse matrix of MN is set to \u03b3 = 0 (\"no inverse\"), in order to investigate the effects of a semantically motivated constraint; and (iii) applying the same training scheme to UD trees directly, by modeling UD relations as matrices (\"vecUD\").", "labels": [], "entities": []}, {"text": "In this case, one edge is assigned one UD relation rel, so we implement the transfor-: Spearman's \u03c1 on phrase similarity mation from child to parent by M rel , and from parent to child by M \u22121 rel . The same hyper-parameters are used to train vecUD.", "labels": [], "entities": [{"text": "phrase similarity mation from child to parent", "start_pos": 103, "end_pos": 148, "type": "TASK", "confidence": 0.7715247656617846}, {"text": "vecUD", "start_pos": 243, "end_pos": 248, "type": "DATASET", "confidence": 0.8877989649772644}]}, {"text": "By comparing vecDCS with vecUD we investigate if applying the semantics framework of DCS makes any difference.", "labels": [], "entities": []}, {"text": "Additionally, we compare with the GloVe (6B, 300d) vector).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.968741774559021}]}, {"text": "Norms of all word vectors are normalized to 1 and Frobenius norms of all matrices are normalized to \u221a d.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Similar training instances clustered by cosine similarities between features", "labels": [], "entities": []}, {"text": " Table 7: Accuracy (%) on sentence completion", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992865920066833}, {"text": "sentence completion", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7565977573394775}]}]}