{"title": [{"text": "Improving Neural Machine Translation Models with Monolingual Data", "labels": [], "entities": [{"text": "Improving Neural Machine Translation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8834402561187744}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7859154224395752}]}, {"text": "Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 77, "end_pos": 121, "type": "TASK", "confidence": 0.641886293888092}, {"text": "NMT", "start_pos": 174, "end_pos": 177, "type": "TASK", "confidence": 0.8656770586967468}]}, {"text": "In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolin-gual data without changing the neural network architecture.", "labels": [], "entities": []}, {"text": "By pairing monolin-gual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English\u2194German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish\u2192English (+2.1-3.4 BLEU), obtaining new state-of-the-art results.", "labels": [], "entities": [{"text": "WMT 15 task English\u2194German", "start_pos": 174, "end_pos": 200, "type": "TASK", "confidence": 0.5949660241603851}, {"text": "BLEU", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.9172466993331909}, {"text": "IWSLT 14 task Turkish\u2192English", "start_pos": 244, "end_pos": 273, "type": "TASK", "confidence": 0.6263662626345953}, {"text": "BLEU", "start_pos": 284, "end_pos": 288, "type": "METRIC", "confidence": 0.9595886468887329}]}, {"text": "We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English\u2192German.", "labels": [], "entities": [{"text": "IWSLT 15 task English\u2192German", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.542608305811882}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7859154224395752}]}, {"text": "Target-side monolingual data plays an important role in boosting fluency for phrase-based statisti- cal machine translation, and we investigate the use of monolingual data for NMT.", "labels": [], "entities": [{"text": "phrase-based statisti- cal machine translation", "start_pos": 77, "end_pos": 123, "type": "TASK", "confidence": 0.5779800961414973}, {"text": "NMT", "start_pos": 176, "end_pos": 179, "type": "TASK", "confidence": 0.8727072477340698}]}, {"text": "Language models trained on monolingual data have played a central role in statistical machine translation since the first IBM models ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7372516592343649}]}, {"text": "There are two major reasons for their importance.", "labels": [], "entities": []}, {"text": "Firstly, word-based and phrase-based translation models make strong independence assumptions, with the probability of translation units estimated independently from context, and language models, by making different independence assumptions, can model how well these translation units fit together.", "labels": [], "entities": []}, {"text": "Secondly, the amount of available monolingual data in the target language typically far exceeds the amount of parallel data, and models typically improve when trained on more data, or data more similar to the translation task.", "labels": [], "entities": []}, {"text": "In (attentional) encoder-decoder architectures for neural machine translation, the decoder is essentially an RNN language model that is also conditioned on source context, so the first rationale, adding a language model to compensate for the independence assumptions of the translation model, does not apply.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.7749452392260233}]}, {"text": "However, the data argument is still valid in NMT, and we expect monolingual data to be especially helpful if parallel data is sparse, or a poor fit for the translation task, for instance because of a domain mismatch.", "labels": [], "entities": [{"text": "translation task", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.9078007936477661}]}, {"text": "In contrast to previous work, which integrates a separately trained RNN language model into the NMT model (, we explore strategies to include monolingual training data in the training process without changing the neural network architecture.", "labels": [], "entities": []}, {"text": "This makes our approach applicable to different NMT architectures.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are as follows: \u2022 we show that we can improve the machine translation quality of NMT systems by mixing monolingual target sentences into the training set.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7506616115570068}]}, {"text": "\u2022 we investigate two different methods to fill the source side of monolingual training instances: using a dummy source sentence, and using a source sentence obtained via backtranslation, which we call synthetic.", "labels": [], "entities": []}, {"text": "We find that the latter is more effective.", "labels": [], "entities": []}, {"text": "\u2022 we successfully adapt NMT models to anew domain by fine-tuning with either monolingual or parallel in-domain data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate NMT training on parallel text, and with additional monolingual data, on English\u2194German and Turkish\u2192English, using training and test data from WMT 15 for English\u2194German, IWSLT 15 for English\u2192German, and IWSLT 14 for Turkish\u2192English.", "labels": [], "entities": [{"text": "WMT", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.696678102016449}, {"text": "IWSLT", "start_pos": 181, "end_pos": 186, "type": "DATASET", "confidence": 0.7700357437133789}, {"text": "IWSLT", "start_pos": 214, "end_pos": 219, "type": "DATASET", "confidence": 0.8100596070289612}]}], "tableCaptions": [{"text": " Table 1: English\u2194German training data.", "labels": [], "entities": [{"text": "English\u2194German training data", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.516687023639679}]}, {"text": " Table 2: Turkish\u2192English training data.", "labels": [], "entities": [{"text": "Turkish\u2192English training data", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.6576279997825623}]}, {"text": " Table 3: English\u2192German translation performance (BLEU) on WMT training/test sets. Ens-4: ensemble  of 4 models. Number of training instances varies due to differences in training time and speed.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9158612191677094}, {"text": "WMT training", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.7795133590698242}]}, {"text": " Table 4: English\u2192German translation performance (BLEU) on IWSLT test sets (TED talks). Single  models.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9176984131336212}, {"text": "IWSLT test sets (TED talks)", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.9194949098995754}]}, {"text": " Table 5: German\u2192English translation perfor- mance (BLEU) on WMT training/test sets (new- stest2014; newstest2015).", "labels": [], "entities": [{"text": "perfor- mance (BLEU)", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.676251212755839}, {"text": "WMT training/test sets", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.6822363376617432}]}, {"text": " Table 7: English\u2192German translation perfor- mance (BLEU) on WMT training/test sets (new- stest2014; newstest2015). Systems differ in how  the synthetic training data is obtained. Ensembles  of 4 models (unless specified otherwise).", "labels": [], "entities": [{"text": "perfor- mance (BLEU)", "start_pos": 37, "end_pos": 57, "type": "METRIC", "confidence": 0.6332303484280905}, {"text": "WMT training/test sets", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.7494531035423279}]}, {"text": " Table 6: Turkish\u2192English translation performance (tokenized BLEU) on IWSLT test sets (TED talks).  Single models. Number of training instances varies due to early stopping.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9928116202354431}, {"text": "IWSLT test sets (TED talks", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.9023516774177551}]}, {"text": " Table 8:  Phrase-based SMT results  (English\u2192German) on WMT test sets (aver- age of newstest201{4,5}), and IWSLT test sets  (average of tst201{3,4,5}), and average BLEU  gain from adding synthetic data for both PBSMT  and NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7719476222991943}, {"text": "WMT test sets", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9456299742062887}, {"text": "aver- age", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9010249773661295}, {"text": "IWSLT test sets", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.8674081563949585}, {"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9994106292724609}, {"text": "PBSMT", "start_pos": 212, "end_pos": 217, "type": "DATASET", "confidence": 0.9508936405181885}, {"text": "NMT", "start_pos": 223, "end_pos": 226, "type": "DATASET", "confidence": 0.7913163900375366}]}]}