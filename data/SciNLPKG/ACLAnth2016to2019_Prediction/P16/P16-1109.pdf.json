{"title": [{"text": "Scaling a Natural Language Generation System", "labels": [], "entities": [{"text": "Scaling a Natural Language Generation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6671569645404816}]}], "abstractContent": [{"text": "A key goal in natural language generation (NLG) is to enable fast generation even with large vocabularies, grammars and worlds.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.8231022457281748}]}, {"text": "In this work, we build upon a recently proposed NLG system, Sentence Tree Realization with UCT (STRUCT).", "labels": [], "entities": [{"text": "Sentence Tree Realization", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.8514293630917867}]}, {"text": "We describe four enhancements to this system: (i) pruning the grammar based on the world and the communicative goal, (ii) intelligently caching and pruning the com-binatorial space of semantic bindings, (iii) reusing the lookahead search tree at different search depths, and (iv) learning and using a search control heuristic.", "labels": [], "entities": []}, {"text": "We evaluate the resulting system on three datasets of increasing size and complexity, the largest of which has a vocabulary of about 10K words, a grammar of about 32K lexical-ized trees and a world with about 11K entities and 23K relations between them.", "labels": [], "entities": []}, {"text": "Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s.", "labels": [], "entities": []}, {"text": "These results are based on a sequential, interpreted implementation and are significantly better than the state of the art for planning-based NLG systems.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this section, we evaluate three hypotheses: (1) S-STRUCT can handle real-world datasets, as they scale in terms of (a) grammar size, (b) world size, (c) entities/relations in the goal, (d) lookahead required to generate sentences, (2) S-STRUCT scales better than STRUCT to such datasets and (3) Each of the enhancements above provides a positive contribution to STRUCT's scalability in isolation.", "labels": [], "entities": []}, {"text": "We collected data in the form of grammars, worlds and goals for our experiments, starting from the WSJ corpus of the Penn TreeBank (.", "labels": [], "entities": [{"text": "WSJ corpus of the Penn TreeBank", "start_pos": 99, "end_pos": 130, "type": "DATASET", "confidence": 0.9258302052815756}]}, {"text": "We parsed this with an LTAG parser to generate the best parse and derivation tree).", "labels": [], "entities": []}, {"text": "The parser generated valid parses for 18,159 of the WSJ sentences.", "labels": [], "entities": [{"text": "WSJ sentences", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9112585186958313}]}, {"text": "To pick the best parse fora given sentence, we choose the parse which minimizes the PAR-SEVAL bracket-crossing metric against the goldstandard (.", "labels": [], "entities": [{"text": "PAR-SEVAL bracket-crossing metric", "start_pos": 84, "end_pos": 117, "type": "METRIC", "confidence": 0.9144106308619181}, {"text": "goldstandard", "start_pos": 130, "end_pos": 142, "type": "METRIC", "confidence": 0.8648903965950012}]}, {"text": "This ensures that the major structures of the parse tree are retained.", "labels": [], "entities": []}, {"text": "We then pick the 31 most frequently occurring XTAG trees (giving us 74% coverage of the parsed sentences) and annotate them with compositional semantics.", "labels": [], "entities": []}, {"text": "The final result of this process was a corpus of semantically annotated WSJ sentences along with their parse and derivation trees . To show the scalability of the improved STRUCT system, we extracted 3 datasets of increasing size and complexity from the semantically annotated WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 277, "end_pos": 287, "type": "DATASET", "confidence": 0.8495945930480957}]}, {"text": "We nominally refer to these datasets as Small, Medium, and Large.", "labels": [], "entities": []}, {"text": "Summary statistics of the data sets are shown in.", "labels": [], "entities": []}, {"text": "For each test set, we take the grammar to be all possible lexicalizations of the unlexicalized trees given the anchors of the test set.", "labels": [], "entities": []}, {"text": "We set the world as the union of all communicative goals in the test set.", "labels": [], "entities": []}, {"text": "The PLTAG probabilities are derived from the entire parseable portion of the WSJ.", "labels": [], "entities": [{"text": "PLTAG", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.7445092797279358}, {"text": "WSJ", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.7060329914093018}]}, {"text": "Due to the data sparsity issues (, we use unlexicalized probabilities.", "labels": [], "entities": []}, {"text": "The reward function constants C were set to.", "labels": [], "entities": []}, {"text": "In the tree policy, c was set to 0.5.", "labels": [], "entities": []}, {"text": "These are as in the original STRUCT system.", "labels": [], "entities": [{"text": "STRUCT system", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.7536467015743256}]}, {"text": "\u03bb was chosen as 100 after evaluating {0, 10, 100, 1000, 10000} on a tuning set.", "labels": [], "entities": []}, {"text": "In addition to test sets, we extract an independent training set using 100 goals to learn the heuristic H(s, a).", "labels": [], "entities": []}, {"text": "We train a separate perceptron for each test set and incorporate this into the S-STRUCT algorithm as described in Section 3.4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary statistics for test data sets", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9476323127746582}]}, {"text": " Table 1. For each test set, we take the grammar  to be all possible lexicalizations of the unlexical- ized trees given the anchors of the test set. We set  the world as the union of all communicative goals  in the test set. The PLTAG probabilities are de- rived from the entire parseable portion of the WSJ.  Due to the data sparsity issues (", "labels": [], "entities": [{"text": "WSJ", "start_pos": 304, "end_pos": 307, "type": "DATASET", "confidence": 0.7361387610435486}]}]}