{"title": [{"text": "Neural Word Segmentation Learning for Chinese", "labels": [], "entities": [{"text": "Neural Word Segmentation Learning", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7850161343812943}]}], "abstractContent": [{"text": "Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.608221968015035}, {"text": "character-based sequence labeling task", "start_pos": 82, "end_pos": 120, "type": "TASK", "confidence": 0.7397242039442062}]}, {"text": "In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history.", "labels": [], "entities": []}, {"text": "Our model employs a gated combination neural network over characters to produce distributed representations of word candidates , which are then given to along short-term memory (LSTM) language scoring model.", "labels": [], "entities": []}, {"text": "Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches , our models achieve competitive or better performances with previous state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most east Asian languages including Chinese are written without explicit word delimiters, therefore, word segmentation is a preliminary step for processing those languages.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7029504328966141}]}, {"text": "Since Xue (2003), most methods formalize the Chinese word segmentation (CWS) as a sequence labeling problem with character position tags, which can be handled with su-pervised learning methods such as Maximum Entropy () and Conditional Random Fields ().", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.7906402399142584}]}, {"text": "However, those methods heavily depend on the choice of handcrafted features.", "labels": [], "entities": []}, {"text": "Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.8902294933795929}, {"text": "feature engineering", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7901347875595093}]}, {"text": "For the task of CWS, adapted the general neural network architecture for sequence labeling proposed in, and used character embeddings as input to a two-layer network.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.6590272635221481}]}, {"text": "improved upon ( by explicitly modeling the interactions between local context and previous tag.", "labels": [], "entities": []}, {"text": "proposed a gated recursive neural network to model the feature combinations of context characters.", "labels": [], "entities": []}, {"text": "used an LSTM architecture to capture potential long-distance dependencies, which alleviates the limitation of the size of context window but introduced another window for hidden states.", "labels": [], "entities": []}, {"text": "Despite the differences, all these models are designed to solve CWS by assigning labels to the characters in the sequence one by one.", "labels": [], "entities": []}, {"text": "At each time step of inference, these models compute the tag scores of character based on (i) context features within a fixed sized local window and (ii) tagging history of previous one.", "labels": [], "entities": []}, {"text": "Nevertheless, the tag-tag transition is insufficient to model the complicated influence from previous segmentation decisions, though it could sometimes be a crucial clue to later segmentation decisions.", "labels": [], "entities": []}, {"text": "The fixed context window size, which is broadly adopted by these methods for feature engineering, also restricts the flexibility of modeling diverse distances.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8510370552539825}]}, {"text": "Moreover, word-level information, which is being the greater granularity unit as suggested in), remains", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed segmenter, we use two popular datasets, PKU and MSR, from the second International Chinese Word Segmentation Bakeoff ().", "labels": [], "entities": [{"text": "PKU", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.759441614151001}, {"text": "International Chinese Word Segmentation Bakeoff", "start_pos": 94, "end_pos": 141, "type": "DATASET", "confidence": 0.8196480393409729}]}, {"text": "These datasets are commonly used by previous state-of-the-art models and neural network models.", "labels": [], "entities": []}, {"text": "Both datasets are preprocessed by replacing the continuous English characters and digits with a unique token.", "labels": [], "entities": []}, {"text": "All experiments are conducted with standard Bakeoff scoring program 1 calculating precision, recall, and F 1 -score.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9785741567611694}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9995998740196228}, {"text": "F 1 -score", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9893577545881271}]}], "tableCaptions": [{"text": " Table 3: Performances of different models on  PKU dataset.", "labels": [], "entities": [{"text": "PKU dataset", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9606817662715912}]}, {"text": " Table 4: Comparison of using different Chinese  idiom dictionaries. 3", "labels": [], "entities": []}, {"text": " Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5", "labels": [], "entities": []}, {"text": " Table 6: Comparison with previous state-of-the-art models. Results with * used external dictionary or  corpus.", "labels": [], "entities": []}]}