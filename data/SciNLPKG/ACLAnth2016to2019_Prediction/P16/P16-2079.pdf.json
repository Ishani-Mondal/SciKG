{"title": [{"text": "Machine Comprehension using Rich Semantic Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "Machine comprehension tests the sys-tem's ability to understand apiece of text through a reading comprehension task.", "labels": [], "entities": []}, {"text": "For this task, we propose an approach using the Abstract Meaning Representation (AMR) formalism.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR) formalism", "start_pos": 48, "end_pos": 95, "type": "TASK", "confidence": 0.7977041815008435}]}, {"text": "We construct meaning representation graphs for the given text and for each question-answer pair by merging the AMRs of comprising sentences using cross-sentential phenomena such as coreference and rhetorical structures.", "labels": [], "entities": []}, {"text": "Then, we reduce machine comprehension to a graph containment problem.", "labels": [], "entities": []}, {"text": "We posit that there is a latent mapping of the question-answer meaning representation graph onto the text meaning representation graph that explains the answer.", "labels": [], "entities": []}, {"text": "We present a unified max-margin framework that learns to find this mapping (given a corpus of texts and question-answer pairs), and uses what it learns to answer questions on novel texts.", "labels": [], "entities": []}, {"text": "We show that this approach leads to state of the art results on the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to efficiently represent and reason with natural language is a fundamental yet longstanding goal in NLP.", "labels": [], "entities": []}, {"text": "This has led to a series of efforts in broad-coverage semantic representation (or \"sembanking\").", "labels": [], "entities": [{"text": "broad-coverage semantic representation", "start_pos": 39, "end_pos": 77, "type": "TASK", "confidence": 0.7977375388145447}]}, {"text": "Recently, AMR, anew semantic representation in standard neo-Davidsonian) framework has been proposed.", "labels": [], "entities": [{"text": "AMR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.38860374689102173}]}, {"text": "AMRs are rooted, labeled graphs which incorporate PropBank style semantic roles, within-sentence coreference, named entities and the notion of types, modality, negation, quantification, etc.", "labels": [], "entities": []}, {"text": "In this paper, we describe an approach to use", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: We use MCTest-500 dataset (), a freely available set of 500 stories (300 train, 50 dev and 150 test) and associated questions to evaluate our model.", "labels": [], "entities": [{"text": "MCTest-500 dataset", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.9701889455318451}]}, {"text": "Each story in MCTest has four multiple-choice questions, each with four answer choices.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.8667399287223816}]}, {"text": "Each question has exactly one correct answer.", "labels": [], "entities": []}, {"text": "Each question is also annotated as 'single' or 'multiple'.", "labels": [], "entities": []}, {"text": "The questions annotated 'single' require just one sentence in the passage to answer them.", "labels": [], "entities": []}, {"text": "For 'multiple' questions it should not be possible to find the answer to the question with just one sentence of the passage.", "labels": [], "entities": []}, {"text": "Ina sense, 'multiple' questions are harder than 'single' questions as they require more complex inference.", "labels": [], "entities": []}, {"text": "We will present the results breakdown for 'single' or 'multiple' category questions as well.", "labels": [], "entities": []}, {"text": "Baselines: We compare our approach to the following baselines: (1-3) The first three baselines are taken from.", "labels": [], "entities": []}, {"text": "SW and SW+D use a sliding window and match a bag of words constructed from the question and the candidate answer to the text.", "labels": [], "entities": []}, {"text": "RTE uses textual entailment by selecting the hypothesis that has the highest likelihood of being entailed by the passage.", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5882120728492737}]}, {"text": "Results: We compare our AMR subgraph containment approach 2 where we consider our modifications for negation and multi-task learning as well in.", "labels": [], "entities": [{"text": "AMR subgraph containment", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7737934589385986}]}, {"text": "We can observe that our models have a comparable performance to all the baselines including the neural network approaches and all previous approaches proposed for this task.", "labels": [], "entities": []}, {"text": "Further, when we incorporate multi-task learning, our approach achieves the state of the art.", "labels": [], "entities": []}, {"text": "Also, our approaches have a considerable improvement over the baselines for 'multiple' questions.", "labels": [], "entities": []}, {"text": "This shows the MCTest-500 dataset.", "labels": [], "entities": [{"text": "MCTest-500 dataset", "start_pos": 15, "end_pos": 33, "type": "DATASET", "confidence": 0.9820977449417114}]}, {"text": "The table shows accuracy on the test set of MCTest-500.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9998026490211487}, {"text": "MCTest-500", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.8847267031669617}]}, {"text": "All differences between the baselines (except SYN+FRM+SEM) and our approaches, and the improvements due to negation and multi-task learning are significant (p < 0.05) using the two-tailed paired T-test.", "labels": [], "entities": [{"text": "FRM+SEM", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.7562901178995768}]}, {"text": "the benefit of our latent structure that allows us to combine evidence from multiple sentences.", "labels": [], "entities": []}, {"text": "The negation heuristic helps significantly, especially for 'single' questions (majority of negation cases in the MCTest dataset are for the \"single\" questions).", "labels": [], "entities": [{"text": "negation heuristic", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9245895147323608}, {"text": "MCTest dataset", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.9730407297611237}]}, {"text": "The multi-task method which performs a classification based on the subtasks for machine comprehension defined in does better than QAClassification that learns the question answer classification.", "labels": [], "entities": [{"text": "question answer classification", "start_pos": 163, "end_pos": 193, "type": "TASK", "confidence": 0.6179428001244863}]}, {"text": "QAClassification in turn performs better than QClassification that learns the question classification only.", "labels": [], "entities": [{"text": "QAClassification", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.7981672286987305}]}, {"text": "These results, together, provide validation for our approach of subgraph matching over meaning representation graphs, and the incorporation of negation and multi-task learning.", "labels": [], "entities": [{"text": "subgraph matching", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7914869487285614}]}], "tableCaptions": [{"text": " Table 1: Comparison of variations of our method against several baselines on", "labels": [], "entities": []}]}