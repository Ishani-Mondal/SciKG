{"title": [{"text": "Cross-Lingual Word Representations via Spectral Graph Embeddings", "labels": [], "entities": [{"text": "Cross-Lingual Word Representations", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6055070062478384}]}], "abstractContent": [{"text": "Cross-lingual word embeddings are used for cross-lingual information retrieval or domain adaptations.", "labels": [], "entities": [{"text": "cross-lingual information retrieval", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.6508120000362396}, {"text": "domain adaptations", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7030095905065536}]}, {"text": "In this paper, we extend Eigenwords, spectral monolin-gual word embeddings based on canoni-cal correlation analysis (CCA), to cross-lingual settings with sentence-alignment.", "labels": [], "entities": []}, {"text": "For incorporating cross-lingual information , CCA is replaced with its generalization based on the spectral graph em-beddings.", "labels": [], "entities": []}, {"text": "The proposed method, which we refer to as Cross-Lingual Eigenwords (CL-Eigenwords), is fast and scalable for computing distributed representations of words via eigenvalue decomposition.", "labels": [], "entities": []}, {"text": "Numerical experiments of English-Spanish word translation tasks show that CL-Eigenwords is competitive with state-of-the-art cross-lingual word embedding methods.", "labels": [], "entities": [{"text": "English-Spanish word translation tasks", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.7265816256403923}]}], "introductionContent": [{"text": "There have been many methods proposed for word embeddings.", "labels": [], "entities": []}, {"text": "Neural network based models are popular, and one of the most major approaches is the skip-gram model (, and some extended methods have also been developed (.", "labels": [], "entities": []}, {"text": "The skip-gram model has many interesting syntactic and semantic properties, and it can be seen as the factorization of a word-context matrix whose elements represent pointwise mutual information ().", "labels": [], "entities": []}, {"text": "However, word embeddings based on neural networks (without neat implementation) can be very slow in general, and it is sometimes difficult to understand how they work.", "labels": [], "entities": []}, {"text": "Recently, a simple spectral method, called Eigenwords, for word embeddings.", "labels": [], "entities": []}, {"text": "Word vectors of the two languages match quite well, although they are computed using sentence-level alignment without knowing word-level alignment.", "labels": [], "entities": []}, {"text": "100-dim word representations are used for PCA computation. is proposed (.", "labels": [], "entities": [{"text": "PCA computation.", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.9401108920574188}]}, {"text": "It is based on canonical correlation analysis (CCA) for computing word vectors by maximizing correlations between words and their contexts.", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.7968391080697378}]}, {"text": "Eigenword algorithms are fast and scalable, yet giving good performance comparable to neural network approaches for capturing the meaning of words from their context.", "labels": [], "entities": []}, {"text": "The skip-gram model, originally proposed for monolingual corpora, has been extended to crosslingual settings.", "labels": [], "entities": []}, {"text": "Given two vector representations of two languages, a linear transformation between the two spaces is trained from a set of word pairs for translation task), while other researchers use CCA for learning linear projections to a common vector space where translation pairs are strongly correlated).", "labels": [], "entities": []}, {"text": "These methods require wordalignment in the training data, while some multilingual corpora have only coarse information such as a set of sentence pairs or paragraph pairs.", "labels": [], "entities": []}, {"text": "Recently, extensions of the skip-gram model requiring only sentence-alignment have been developed by introducing cross-lingual losses in the objective of the original models (.", "labels": [], "entities": []}, {"text": "In this paper, instead of the skip-gram model, we extend Eigenwords ( to cross-lingual settings with sentence-alignment.", "labels": [], "entities": []}, {"text": "Our main idea is to replace CCA, which is applicable to only two different kinds of data, with a generalized method () based on spectral graph embeddings ( so that the Eigenwords can deal with two or more languages for cross-lingual word embeddings.", "labels": [], "entities": []}, {"text": "Our proposed method, referred to as Cross-Lingual Eigenwords (CL-Eigenwords), requires only sentence-alignment for capturing cross-lingual relationships.", "labels": [], "entities": []}, {"text": "The method is very simple in mathematics as well as computation; it involves a generalized eigenvalue problem, which can be solved by fast and scalable algorithms such as the randomized eigenvalue decomposition ().", "labels": [], "entities": []}, {"text": "shows an illustrative example of crosslingual word vectors obtained by CL-Eigenwords.", "labels": [], "entities": []}, {"text": "Although only sentence-alignment is available in the corpus, word-level translation is automatically captured in the vector representations; the same words (countries and capitals) in the two languages are placed in close proximity to each other; greece is close to grecia and rome is close to roma.", "labels": [], "entities": [{"text": "word-level translation", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6738739907741547}]}, {"text": "In addition, the same kinds of relationships between word pairs share similar directions in the vector space; the direction from sweden to stockholm is nearly parallel to the direction from finland to helsinki.", "labels": [], "entities": []}, {"text": "We evaluate the word vectors obtained by our method on the English-Spanish cross-lingual translation task and compare the results with those of state-of-the-art methods, showing that our proposed method is competitive with those existing methods.", "labels": [], "entities": [{"text": "cross-lingual translation task", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.7644822001457214}]}, {"text": "We use Europarl corpus for learning the vector representation of words.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9903964698314667}]}, {"text": "Although the experiments in this paper are conducted using bilingual corpus, our method can be easily applied to three or more languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "The implementation of our method is available on GitHub . Following the previous works, we use only 1 https://github.com/shimo-lab/kadingir the first 500K lines of English-Spanish sentencealigned parallel corpus of Europarl ( for numerical experiments.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 215, "end_pos": 223, "type": "DATASET", "confidence": 0.9020872712135315}]}], "tableCaptions": [{"text": " Table 1: Computational times (in minutes) and word translation accuracies (in percent, higher is better)  evaluated by Precision@n using the 1,000 test words (the 1st to 1,000th most frequent words or the  5,001st to 6,000th most frequent words). Shown are for Spanish (es) to English (en) translation and  for English (en) to Spanish (es) translation.  *  BilBOWA is executed on 3 threads, while CL-LSI and  CL-Eigenwords are executed on a single thread.", "labels": [], "entities": [{"text": "word translation", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7370055466890335}, {"text": "Spanish (es) to English (en) translation", "start_pos": 262, "end_pos": 302, "type": "TASK", "confidence": 0.6235284090042115}, {"text": "English (en) to Spanish (es) translation", "start_pos": 312, "end_pos": 352, "type": "TASK", "confidence": 0.6335057765245438}, {"text": "BilBOWA", "start_pos": 358, "end_pos": 365, "type": "METRIC", "confidence": 0.9844101667404175}]}]}