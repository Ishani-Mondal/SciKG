{"title": [{"text": "Inner Attention based Recurrent Neural Networks for Answer Selection", "labels": [], "entities": [{"text": "Answer Selection", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.9836483001708984}]}], "abstractContent": [{"text": "Attention based recurrent neural networks have shown advantages in representing natural language sentences (Hermann et al., 2015; Rockt\u00e4schel et al., 2015; Tan et al., 2015).", "labels": [], "entities": []}, {"text": "Based on recurrent neural networks (RNN), external attention information was added to hidden representations to get an attentive sentence representation.", "labels": [], "entities": []}, {"text": "Despite the improvement over non-attentive models, the attention mechanism under RNN is not well studied.", "labels": [], "entities": []}, {"text": "In this work, we analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively.", "labels": [], "entities": []}, {"text": "Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 191, "end_pos": 212, "type": "TASK", "confidence": 0.8947950998942057}]}], "introductionContent": [{"text": "Answer selection (AS) is a crucial subtask of the open domain question answering (QA) problem.", "labels": [], "entities": [{"text": "Answer selection (AS)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9427275896072388}, {"text": "open domain question answering (QA) problem", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.7942837849259377}]}, {"text": "Given a question, the goal is to choose the answer from a set of pre-selected sentences.", "labels": [], "entities": []}, {"text": "Traditional AS models are based on lexical features such as parsing tree edit distance.", "labels": [], "entities": []}, {"text": "Neural networks based models are proposed to represent the meaning of a sentence in a vector space and then compare the question and answer candidates in this hidden space (, which have shown great success in AS.", "labels": [], "entities": [{"text": "AS", "start_pos": 209, "end_pos": 211, "type": "TASK", "confidence": 0.9871860146522522}]}, {"text": "However, these models represent the question and sentence separately, which may ignore the information subject to the question when representing the answer.", "labels": [], "entities": []}, {"text": "For example, given a candidate answer: Michael Jordan abruptly retired from Chicago Bulls before the beginning of the 1993-94 NBA season to pursue a career in baseball.", "labels": [], "entities": []}, {"text": "For a question: When did Michael Jordan retired from NBA?", "labels": [], "entities": []}, {"text": "we should focus on the beginning of the 1993-94 in the sentence; however, when we were asked: Which sports does Michael Jordan participates after his retirement from NBA?", "labels": [], "entities": []}, {"text": "we should pay more attention to pursue a career in baseball.", "labels": [], "entities": []}, {"text": "Recent years, attention based models are proposed in light of this purpose and have shown great success in many NLP tasks such as machine translation (), question answering () and recognizing textual entailments.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7805645763874054}, {"text": "question answering", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.887761265039444}, {"text": "recognizing textual entailments", "start_pos": 180, "end_pos": 211, "type": "TASK", "confidence": 0.8145044843355814}]}, {"text": "When building the representation of a sentence, some attention information is added to the hidden state.", "labels": [], "entities": []}, {"text": "For example, in attention based recurrent neural networks models () each time-step hidden representation is weighted by attention.", "labels": [], "entities": []}, {"text": "Inspired by the attention mechanism, some attention-based RNN answer selection models have been proposed) in which the attention when computing answer representation is from question representation.", "labels": [], "entities": [{"text": "RNN answer selection", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.6667553087075552}]}, {"text": "However, in the RNN architecture, at each time step a word is added and the hidden state is updated recurrently, so those hidden states near the end of the sentence are expected to capture more information . Consequently, after adding the attention information to the time sequence hidden representations, the near-the-end hidden variables will be more attended due to their comparatively abundant semantic accumulation, which may result in a biased attentive weight towards the later coming words in RNN.", "labels": [], "entities": []}, {"text": "In this work, we analyze this attention bias problem qualitatively and quantitatively, and then propose three new models to solve this problem.", "labels": [], "entities": []}, {"text": "Different from previous attention based RNN models in which attention information is added after RNN computation, we add the attention before computing the sentence representation.", "labels": [], "entities": []}, {"text": "Concretely, the first one uses the question attention to adjust word representation (i.e. word embedding) in the answer directly, and then we use RNN to model the attentive word sequence.", "labels": [], "entities": []}, {"text": "However, this model attends a sentence word byword which may ignore the relation between words.", "labels": [], "entities": []}, {"text": "For example, if we were asked: what is his favorite food?", "labels": [], "entities": []}, {"text": "one answer candidate is: He likes hot dog best.", "labels": [], "entities": []}, {"text": "hot or dog maybe not relate to the question by itself, but they are informative as a whole in the context.", "labels": [], "entities": []}, {"text": "So we propose the second model in which every word representation in answer is impacted by not only question attention but also the context representation of the word (i.e. the last hidden state).", "labels": [], "entities": []}, {"text": "In our last model, inspired by previous work on adding gate into inner activation of RNN to control the long and short term information flow, we embed the attention to the inner activation gate of RNN to influence the computation of RNN hidden representation.", "labels": [], "entities": []}, {"text": "In addition, inspired by recent work called Occam's Gate in which the activation of input units are penalized to be as less as possible, we add regulation to the summation of the attention weights to impose sparsity.", "labels": [], "entities": []}, {"text": "Overall, in this work we make three contributions: (1) We analyze the attention bias problem in traditional attention based RNN models.", "labels": [], "entities": []}, {"text": "(2) We propose three inner attention based RNN models and achieve new state-of-the-art results in answer selection.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.8955795168876648}]}, {"text": "(3) We use Occam's Razor to regulate the attention weights which shows advantage in long sentence representation.", "labels": [], "entities": [{"text": "Occam's Razor", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8780069351196289}, {"text": "long sentence representation", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.7397687037785848}]}], "datasetContent": [{"text": "Common Setup: We use the off-the-shelf 100-dimension word embeddings from word2vec 4 , and initiate all weights and attention matrices by fixing their largest singular values to 1 (.", "labels": [], "entities": []}, {"text": "IARNN-OCCAM base regulation hyperparameter \u03bb q is set to 0.05, we add L 2 penalty with a coefficient of 10 \u22125 . Dropout () is further applied to every parameters with probability 30%.", "labels": [], "entities": [{"text": "IARNN-OCCAM base regulation hyperparameter \u03bb q", "start_pos": 0, "end_pos": 46, "type": "METRIC", "confidence": 0.7639817098776499}]}, {"text": "We use Adadelta with \u03c1 = 0.90 to update parameters.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.6973677277565002}]}, {"text": "We choose three datasets for evaluation: InsuranceQA, WikiQA and TREC-QA.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9083251357078552}, {"text": "WikiQA", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.89911949634552}, {"text": "TREC-QA", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.5696051716804504}]}, {"text": "These datasets contain questions from different domains.", "labels": [], "entities": []}, {"text": "presents some statistics about these datasets.", "labels": [], "entities": []}, {"text": "We adopt a max-margin hinge loss as training objective.", "labels": [], "entities": []}, {"text": "The results are reported in terms of MAP and MRR in WikiQA and TREC-QA and accuracy in InsuranceQA.", "labels": [], "entities": [{"text": "MAP", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.993219792842865}, {"text": "MRR", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9953893423080444}, {"text": "WikiQA", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9531586170196533}, {"text": "TREC-QA", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.6138495802879333}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9997439980506897}, {"text": "InsuranceQA", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.9717017412185669}]}, {"text": "We use bidirectional GRU for all models.", "labels": [], "entities": [{"text": "GRU", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9223058819770813}]}, {"text": "We share the GRU parameter between question and answer which has shown significant improvement on performance and convergency rate (.", "labels": [], "entities": [{"text": "GRU", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9929859042167664}, {"text": "convergency rate", "start_pos": 114, "end_pos": 130, "type": "METRIC", "confidence": 0.9205688536167145}]}, {"text": "There are two common baseline systems for above three datasets: \u2022 GRU: A non-attentive GRU-RNN that models the question and answer separately.", "labels": [], "entities": []}, {"text": "\u2022 OARNN: Outer attention-based RNN models (OARNN) with GRU which is detailed in Section 5.1.", "labels": [], "entities": [{"text": "OARNN", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.8375616669654846}, {"text": "GRU", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9956692457199097}]}, {"text": "WikiQA: Performances on WikiQA dataset in which all answers are collected from Wikipedia.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9426867365837097}, {"text": "WikiQA dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9237964749336243}]}, {"text": "In addition to the original (question,positive,negative) triplets, we randomly select a bunch of negative answer candidates from answer sentence pool and finally we get a relatively abundant 50,298 triplets.", "labels": [], "entities": []}, {"text": "We use cosine similarity to compare the question and candidate answer sentence.", "labels": [], "entities": []}, {"text": "The hidden variable's length is set to 165 and batch size is set to 1.", "labels": [], "entities": []}, {"text": "We use sigmoid as GRU inner active function, we keep word embedding fixed during training.", "labels": [], "entities": []}, {"text": "Margin M was set to 0.15 which is tuned in the development set.", "labels": [], "entities": [{"text": "Margin M", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.6277903914451599}]}, {"text": "We adopt three additional baseline systems applied to WikiQA: The result is shown in.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.8699284791946411}]}, {"text": "InsuranceQA () is a domain specific answer selection dataset in which all questions is related to insurance.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9424839019775391}]}, {"text": "Its vocabulary size is comparatively small, we set the batch size to 16 and the hidden variable size to 145, hinge loss margin M is adjusted to 0.12 by evaluation behavior.", "labels": [], "entities": [{"text": "hinge loss margin M", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.9506888836622238}]}, {"text": "Word embeddings are also learned during training.", "labels": [], "entities": []}, {"text": "We adopt the Geometric mean of Euclidean and Sigmoid Dot (GESD) proposed in to measure the similarity be- propose a question similarity model to extract features from word alignment between two questions which is suitable to FAQ based QA.", "labels": [], "entities": [{"text": "Geometric mean", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.9287684559822083}]}, {"text": "It needs to mention that the system marked with \u2020 are learned on TREC-QA original full training data.", "labels": [], "entities": [{"text": "TREC-QA original full training data", "start_pos": 65, "end_pos": 100, "type": "DATASET", "confidence": 0.8519470572471619}]}, {"text": "tween two representations: which shows advantage over cosine similarity in experiments.", "labels": [], "entities": []}, {"text": "We report accuracy instead of MAP/MRR because one question only has one right answers in InsuranceQA.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999045193195343}, {"text": "MAP", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9848932027816772}, {"text": "MRR", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8745928406715393}, {"text": "InsuranceQA", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.8566397428512573}]}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "TREC-QA was created by based on Text REtrieval Conference (TREC) QA track (8-13) data.", "labels": [], "entities": [{"text": "TREC-QA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8359224200248718}, {"text": "Text REtrieval Conference (TREC) QA track (8-13) data", "start_pos": 32, "end_pos": 85, "type": "DATASET", "confidence": 0.5933631857236227}]}, {"text": "The size of hidden variable was set to 80, M was set to 0.1.", "labels": [], "entities": [{"text": "M", "start_pos": 43, "end_pos": 44, "type": "METRIC", "confidence": 0.9971426129341125}]}, {"text": "This dataset is comparatively small so we set word embedding vector Q: how old was monica lewinsky during the affair ? Monica Samille Lewinsky ( born July ) is an American woman with whom United States President Bill Clinton admitted to having had an``an`` improper relationship '' while she worked at the White House in 1995 and 1996 . Monica Samille Lewinsky ( born July ) is an American woman with whom United States President Bill Clinton admitted to having had an``an`` improper relationship '' while she worked at the White House in 1995 and 1996 . size to 50 and update it during training.", "labels": [], "entities": [{"text": "size", "start_pos": 555, "end_pos": 559, "type": "METRIC", "confidence": 0.9559744596481323}]}, {"text": "It needs to mention that we do not use the original TREC-QA training data but the smaller one which has been edited by human.", "labels": [], "entities": [{"text": "TREC-QA training data", "start_pos": 52, "end_pos": 73, "type": "DATASET", "confidence": 0.805479884147644}]}, {"text": "The result is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of three answer selection datasets. For the TREC-QA, we use the cleaned dataset  that has been edit by human. For WikiQA and TREC-QA we remove all the questions that has no right  or wrong answers.", "labels": [], "entities": []}, {"text": " Table 2: Performances on WikiQA", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.7916939854621887}]}, {"text": " Table 3: Experiment result in InsuranceQA, (Feng  et al., 2015) is a CNN architecture without atten- tion mechanism.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.8595964312553406}]}, {"text": " Table 4: Result of different systems in Trec- QA.(", "labels": [], "entities": [{"text": "Trec- QA", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.847028394540151}]}]}