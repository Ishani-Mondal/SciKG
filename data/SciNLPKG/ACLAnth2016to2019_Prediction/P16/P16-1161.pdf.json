{"title": [{"text": "Target-Side Context for Discriminative Models in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7839332421620687}]}], "abstractContent": [{"text": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance.", "labels": [], "entities": [{"text": "Discriminative translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6413150876760483}, {"text": "statistical machine translation", "start_pos": 83, "end_pos": 114, "type": "TASK", "confidence": 0.670910914738973}]}, {"text": "We propose a novel extension of this work using target context information.", "labels": [], "entities": []}, {"text": "Surprisingly, we show that this model can be efficiently integrated directly in the decoding process.", "labels": [], "entities": []}, {"text": "Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs.", "labels": [], "entities": []}, {"text": "We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence.", "labels": [], "entities": []}, {"text": "Our work is freely available as part of Moses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discriminative lexicons address some of the core challenges of phrase-based MT (PBMT) when translating to morphologically rich languages, such as Czech, namely sense disambiguation and morphological coherence.", "labels": [], "entities": [{"text": "phrase-based MT (PBMT)", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.686018967628479}]}, {"text": "The first issue is semantic: given a source word or phrase, which of its possible meanings (i.e., which stem or lemma) should we choose?", "labels": [], "entities": []}, {"text": "Previous work has shown that this can be addressed using a discriminative lexicon.", "labels": [], "entities": []}, {"text": "The second issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate?", "labels": [], "entities": []}, {"text": "In this work, we integrate such a model directly into the SMT decoder.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.8700211942195892}]}, {"text": "This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context.", "labels": [], "entities": []}, {"text": "This allows the model to not only help with semantics but also to improve morphological and syntactic coherence.", "labels": [], "entities": []}, {"text": "For sense disambiguation, source context is the main source of information, as has been shown in previous work),, inter alia.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7100363224744797}]}, {"text": "Consider the first set of examples in, produced by a strong baseline PBMT system.", "labels": [], "entities": []}, {"text": "The English word \"shooting\" has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film.", "labels": [], "entities": []}, {"text": "When the cue word \"film\" is close, the phrase-based model is able to use it in one phrase with the ambiguous \"shooting\", disambiguating correctly the translation.", "labels": [], "entities": []}, {"text": "When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead.", "labels": [], "entities": []}, {"text": "Wider source context information is required for correct disambiguation.", "labels": [], "entities": []}, {"text": "While word/phrase senses can usually be inferred from the source sentence, the correct selection of surface forms requires also information from the target.", "labels": [], "entities": []}, {"text": "Note that we can obtain some information from the source.", "labels": [], "entities": []}, {"text": "For example, an English subject is often translated into a Czech subject; in which case the Czech word should be in nominative case.", "labels": [], "entities": []}, {"text": "But there are many decisions that happen during decoding which determine morphological and syntactic properties of words -verbs can have translations which differ in valency frames, they maybe translated in either active or passive voice (in which case subject and object would be switched), nouns may have different possible translations which differ in gender, etc.", "labels": [], "entities": []}, {"text": "The correct selection of surface forms plays a crucial role in preserving meaning in morphologically rich languages because it is morphology rather than word order that expresses relations between words.", "labels": [], "entities": []}, {"text": "(Word order tends to be Input PBMT Output shooting of the film . nat\u00e1\u010den\u00ed filmu . shooting camera of film . shooting of the expensive film . st\u0159elby na drah\u00b4ydrah\u00b4y film . shootings gun at expensive film . the man saw a cat . mu\u017e uvid\u011bl ko\u010dku . man saw cat acc . the man saw a black cat . mu\u017e spat\u0159ilspat\u0159il\u02c7spat\u0159il\u010dernou ko\u010dku . man saw black acc cat acc . the man saw a yellowish cat . mu\u017e spat\u0159il na\u017eloutl\u00e1 ko\u010dka . man saw yellowish nom cat nom .: Examples of problems of PBMT: lexical selection and morphological coherence.", "labels": [], "entities": []}, {"text": "Each translation has a corresponding gloss in italics.", "labels": [], "entities": []}, {"text": "relatively free and driven more by semantic constraints rather than syntactic constraints.)", "labels": [], "entities": []}, {"text": "The language model is only partially able to capture this phenomenon.", "labels": [], "entities": []}, {"text": "It has a limited scope and perhaps more seriously, it suffers from data sparsity.", "labels": [], "entities": []}, {"text": "The units captured by both the phrase table and the LM are mere sequences of words.", "labels": [], "entities": []}, {"text": "In order to estimate their probability, we need to observe them in the training data (many times, if the estimates should be reliable).", "labels": [], "entities": []}, {"text": "However, the number of possible n-grams grows exponentially as we increase n, leading to unrealistic requirements on training data sizes.", "labels": [], "entities": []}, {"text": "This implies that the current models can (and often do) miss relationships between words even within their theoretical scope.", "labels": [], "entities": []}, {"text": "The second set of sentences in demonstrates the problem of data sparsity for morphological coherence.", "labels": [], "entities": []}, {"text": "While the phrase-based system can correctly transfer the morphological case of \"cat\" and even \"black cat\", the less usual \"yellowish cat\" is mistranslated into nominative case, even though the correct phrase \"yellowish ||| na\u017eloutlou\" exists in the phrase table.", "labels": [], "entities": []}, {"text": "A model with a suitable representation of two preceding words could easily infer the correct casein this example.", "labels": [], "entities": []}, {"text": "Our contributions are the following: \u2022 We show that the addition of a feature-rich discriminative model significantly improves translation quality even for large data sizes and that target-side context information consistently further increases this improvement.", "labels": [], "entities": [{"text": "translation", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.9675987362861633}]}, {"text": "\u2022 We provide an analysis of the outputs which confirms that source-context features indeed help with semantic disambiguation (as is well known).", "labels": [], "entities": [{"text": "semantic disambiguation", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.7358164191246033}]}, {"text": "Importantly, we also show that our novel use of target context improves morphological and syntactic coherence.", "labels": [], "entities": []}, {"text": "\u2022 In addition to extensive experimentation on translation from English to Czech, we also evaluate English to German, English to Polish and English to Romanian tasks, with improvements on translation quality in all tasks, showing that our work is broadly applicable.", "labels": [], "entities": [{"text": "translation from English to Czech", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.8703602194786072}]}, {"text": "\u2022 We describe several optimizations which allow target-side features to be used efficiently in the context of phrase-based decoding.", "labels": [], "entities": []}, {"text": "\u2022 Our implementation is freely available in the widely used open-source MT toolkit Moses, enabling other researchers to explore discriminative modelling with target context in MT.", "labels": [], "entities": [{"text": "MT toolkit Moses", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.7934934894243876}, {"text": "MT", "start_pos": 176, "end_pos": 178, "type": "TASK", "confidence": 0.9485000967979431}]}], "datasetContent": [{"text": "We run the main set of experiments on English to Czech translation.", "labels": [], "entities": [{"text": "English to Czech translation", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.575737938284874}]}, {"text": "To verify that our method is applicable to other language pairs, we also present experiments in English to German, Polish, and Romanian.", "labels": [], "entities": []}, {"text": "In all experiments, we use Treex (Popel and\u017dabokrtsk\u00b4y and\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2010) to lemmatize and tag the source data and also to obtain dependency parses of all English sentences.", "labels": [], "entities": [{"text": "Treex (Popel and\u017dabokrtsk\u00b4y and\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2010)", "start_pos": 27, "end_pos": 93, "type": "DATASET", "confidence": 0.8512937054038048}]}], "tableCaptions": [{"text": " Table 2: BLEU scores obtained on the WMT14  test set. We report the performance of the baseline,  the source-context model and the full model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991312623023987}, {"text": "WMT14  test set", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9536611239115397}]}, {"text": " Table 3: BLEU scores of the baseline and of the  full model for English to German, Polish, and Ro- manian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994695782661438}]}]}