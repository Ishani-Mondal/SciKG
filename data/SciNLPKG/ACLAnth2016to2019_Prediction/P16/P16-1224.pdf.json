{"title": [], "abstractContent": [{"text": "We introduce anew language learning setting relevant to building adaptive natural language interfaces.", "labels": [], "entities": []}, {"text": "It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks).", "labels": [], "entities": []}, {"text": "The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities.", "labels": [], "entities": []}, {"text": "We created a game called SHRDLURN in a blocks world and collected interactions from 100 people playing it.", "labels": [], "entities": []}, {"text": "First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance.", "labels": [], "entities": []}, {"text": "Second, we compare computer strategies, showing that modeling prag-matics on a semantic parsing model accelerates learning for more strategic players.", "labels": [], "entities": []}], "introductionContent": [{"text": "Wittgenstein (1953) famously said that language derives its meaning from use, and introduced the concept of language games to illustrate the fluidity and purpose-orientedness of language.", "labels": [], "entities": []}, {"text": "He described how a builder B and an assistant A can use a primitive language consisting of four words-'block', 'pillar', 'slab', 'beam'-to successfully communicate what block to pass from A to B.", "labels": [], "entities": []}, {"text": "This is only one such language; many others would also work for accomplishing the cooperative goal.", "labels": [], "entities": []}, {"text": "This paper operationalizes and explores the idea of language games in a learning setting, which we call interactive learning through language games The human types in an utterance, and the computer (which does not know the goal state) tries to interpret the utterance and perform the corresponding action.", "labels": [], "entities": []}, {"text": "The computer initially knows nothing about the language, but through the human's feedback, learns the human's language while making progress towards the game goal.", "labels": [], "entities": []}, {"text": "In the ILLG setting, the two parties do not initially speak a common language, but nonetheless need to collaboratively accomplish a goal.", "labels": [], "entities": []}, {"text": "Specifically, we created a game called SHRD-LURN, 1 in homage to the seminal work of.", "labels": [], "entities": []}, {"text": "As shown in, the objective is to transform a start state into a goal state, but the only action the human can take is entering an utterance.", "labels": [], "entities": []}, {"text": "The computer parses the utterance and produces a ranked list of possible interpretations according to its current model.", "labels": [], "entities": []}, {"text": "The human scrolls through the list and chooses the intended one, simultaneously advancing the state of the blocks and providing feedback to the computer.", "labels": [], "entities": []}, {"text": "Both the human and the computer wish to reach the goal state (only known to the human) with as little scrolling as possible.", "labels": [], "entities": []}, {"text": "For the computer to be successful, it has to learn the human's language quickly over the course of the game, so that the human can accomplish the goal more efficiently.", "labels": [], "entities": []}, {"text": "Conversely, the human must also accommodate the computer, at least partially understanding what it can and cannot do.", "labels": [], "entities": []}, {"text": "We model the computer in the ILLG as a semantic parser (Section 3), which maps natural language utterances (e.g., 'remove red') into logical forms (e.g., remove(with(red))).", "labels": [], "entities": []}, {"text": "The semantic parser has no seed lexicon and no annotated logical forms, so it just generates many candidate logical forms.", "labels": [], "entities": []}, {"text": "Based on the human's feedback, it performs online gradient updates on the parameters corresponding to simple lexical features.", "labels": [], "entities": []}, {"text": "During development, it became evident that while the computer was eventually able to learn the language, it was learning less quickly than one might hope.", "labels": [], "entities": []}, {"text": "For example, after learning that 'remove red' maps to remove(with(red)), it would think that 'remove cyan' also mapped to remove(with(red)), whereas a human would likely use mutual exclusivity to rule out that hypothesis.", "labels": [], "entities": []}, {"text": "We therefore introduce a pragmatics model in which the computer explicitly reasons about the human, in the spirit of previous work on pragmatics.", "labels": [], "entities": []}, {"text": "To make the model suitable for our ILLG setting, we introduce anew online learning algorithm.", "labels": [], "entities": []}, {"text": "Empirically, we show that our pragmatic model improves the online accuracy by 8% compared to our best non-pragmatic model on the 10 most successful players (Section 5.3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9776594042778015}]}, {"text": "What is special about the ILLG setting is the real-time nature of learning, in which the human also learns and adapts to the computer.", "labels": [], "entities": []}, {"text": "While the human can teach the computer any languageEnglish, Arabic, Polish, a custom programming language-a good human player will choose to use utterances that the computer is more likely to learn quickly.", "labels": [], "entities": []}, {"text": "In the parlance of communication theory, the human accommodates the computer).", "labels": [], "entities": [{"text": "communication theory", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8377299606800079}]}, {"text": "Using Amazon Mechanical Turk, we collected and analyzed around 10k utterances from 100 games of SHRD-LURN.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 6, "end_pos": 28, "type": "DATASET", "confidence": 0.8806494871775309}, {"text": "SHRD-LURN", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.8328115940093994}]}, {"text": "We show that successful players tend to use compositional utterances with a consistent vocabulary and syntax, which matches the inductive biases of the computer (Section 5.2).", "labels": [], "entities": []}, {"text": "In addition, through this interaction, many players adapt to the computer by becoming more consistent, more precise, and more concise.", "labels": [], "entities": []}, {"text": "On the practical side, natural language systems are often trained once and deployed, and users must live with their imperfections.", "labels": [], "entities": []}, {"text": "We believe that studying the ILLG setting will be integral for creating adaptive and customizable systems, especially for resource-poor languages and new domains where starting from close to scratch is unavoidable.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Example utterances, along with the average number of scrolls for that player in parentheses.  Success is measured by the number of scrolls, where the more successful players need less scrolls. 1)  The 20 most successful players tend to use consistent and concise language whose semantics is similar  to our logical language. 2) Average players tend to be slightly more verbose and inconsistent (left and  right), or significantly different from our logical langauge (middle). 3) Reasons for being unsuccessful  vary. Left: no tokenization, middle: used a coordinate system and many conjunctions; right: confused in  the beginning, and used a language very different from our logical language.", "labels": [], "entities": []}, {"text": " Table 4: Average online accuracy under vari- ous settings. memorize: featurize entire utter- ance and logical form non-compositionally; half  model: featurize the utterances with unigrams, bi- grams, and skip-grams but conjoin with the entire  logical form; full model: the model described in  Section 3; +prag: the models above, with our on- line pragmatics algorithm described in Section 4.  Both compositionality and pragmatics improve ac- curacy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9825842976570129}]}]}