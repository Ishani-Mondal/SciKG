{"title": [{"text": "DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents", "labels": [], "entities": [{"text": "Information Retrieval Approach", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.7896702686945597}]}], "abstractContent": [{"text": "Most current chatbot engines are designed to reply to user utterances based on existing utterance-response (or Q-R) 1 pairs.", "labels": [], "entities": []}, {"text": "In this paper, we present DocChat, a novel information retrieval approach for chat-bot engines that can leverage unstructured documents, instead of Q-R pairs, to respond to utterances.", "labels": [], "entities": []}, {"text": "A learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly.", "labels": [], "entities": []}, {"text": "We evaluate our proposed approach in both English and Chi-nese: (i) For English, we evaluate Doc-Chat on WikiQA and QASent, two answer sentence selection tasks, and compare it with state-of-the-art methods.", "labels": [], "entities": [{"text": "answer sentence selection tasks", "start_pos": 128, "end_pos": 159, "type": "TASK", "confidence": 0.6906536445021629}]}, {"text": "Reasonable improvements and good adaptability are observed.", "labels": [], "entities": []}, {"text": "(ii) For Chinese, we compare DocChat with XiaoIce 2 , a famous chitchat engine in China, and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building chatbot engines that can interact with humans with natural language is one of the most challenging problems in artificial intelligence.", "labels": [], "entities": []}, {"text": "Along with the explosive growth of social media, like community question answering (CQA) websites (e.g., Yahoo Answers and WikiAnswers) and social media websites (e.g., Twitter and Weibo), * Contribution during internship at Microsoft Research.", "labels": [], "entities": [{"text": "question answering (CQA)", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.8411996483802795}]}, {"text": "For convenience sake, we denote all utterance-response pairs (either QA pairs or conversational exchanges from social media websites like Twitter) as Q-R pairs in this paper.", "labels": [], "entities": []}, {"text": "http://www.msxiaoice.com the amount of utterance-response (or Q-R) pairs has experienced massive growth in recent years, and such a corpus greatly promotes the emergence of various data-driven chatbot approaches.", "labels": [], "entities": []}, {"text": "Instead of multiple rounds of conversation, we only consider a much simplified task, short text conversation (STC) in which the response R is a short text and only depends on the last user utterance Q.", "labels": [], "entities": [{"text": "short text conversation (STC)", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.6283552596966425}]}, {"text": "Previous methods for the STC task mostly rely on Q-R pairs and fall into two categories: Retrieval-based methods (e.g.,.", "labels": [], "entities": [{"text": "STC task", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.9286136031150818}]}, {"text": "This type of methods first retrieve the most possibl\u00ea Q, \u02c6 RR pair from a set of existing Q-R pairs, which best matches current utterance Q based on semantic matching models, then tak\u00ea R as the response R.", "labels": [], "entities": []}, {"text": "One disadvantage of such a method is that, for many specific domains, collecting such Q-R pairs is intractable.", "labels": [], "entities": []}, {"text": "Generation based methods (e.g.,.", "labels": [], "entities": []}, {"text": "This type of methods usually uses an encoder-decoder framework which first encode Q as a vector representation, then feed this representation to decoder to generate response R.", "labels": [], "entities": []}, {"text": "Similar to retrieval-based methods, such approaches also depend on existing Q-R pairs as training data.", "labels": [], "entities": []}, {"text": "Like other language generation tasks, such as machine translation and paraphrasing, the fluency and naturality of machine generated text is another drawback.", "labels": [], "entities": [{"text": "language generation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7618048191070557}, {"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7767636477947235}]}, {"text": "To overcome the issues mentioned above, we present a novel response retrieval approach, DocChat, to find responses based on unstructured documents.", "labels": [], "entities": []}, {"text": "For each user utterance, instead of looking for the best Q-R pair or generating a word sequence based on language generation techniques, our method selects a sentence from given documents directly, by ranking all possible sentences based on features designed at different levels of granularity.", "labels": [], "entities": []}, {"text": "On one hand, using documents rather than Q-R pairs greatly improve the adapt-ability of chatbot engines on different chatting topics.", "labels": [], "entities": []}, {"text": "On the other hand, all responses come from existing documents, which guarantees their fluency and naturality.", "labels": [], "entities": []}, {"text": "We also show promising results in experiments, on both QA and chatbot scenarios.", "labels": [], "entities": []}], "datasetContent": [{"text": "Take into account response ranking task and answer selection task are similar, we first evaluate DocChat in a QA scenario as a simulation.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.8787674605846405}, {"text": "DocChat", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.8446125388145447}]}, {"text": "Here, response ranking is treated as the answer selection task, and response triggering is treated as the answer triggering task.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.8644230663776398}, {"text": "response triggering", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7424776256084442}, {"text": "answer triggering", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7261412888765335}]}, {"text": "For ranking features, 17M 'question-related questions' pairs crawled from Baidu Zhidao are used to train word alignments for h W 2W ; sentences from Chinese Wikipedia are used to train word embeddings for h W 2V and a topic model for h UT M ; the same bilingual phrase (Beijing is a historical city that can be traced back to 3,000 years ago.): XiaoIce response is more colloquial, as it comes from Q-R pairs; while DocChat response is more formal, as it comes from documents.", "labels": [], "entities": []}, {"text": "h ST M . As there is no knowledge base based labeled data for Chinese, we ignore relation-level feature h RE and type-level feature h T E . For ranking weights, we generate 90,321 Q, C pairs based on Baidu Zhidao Q-A pairs by the automatic method described in Section 4.8.", "labels": [], "entities": []}, {"text": "This data set is used to train the learning to rank model feature weights {\u03bb k } by SGD.", "labels": [], "entities": []}, {"text": "For documents, we randomly select 10 WeChat official accounts, and index their documents separately.", "labels": [], "entities": [{"text": "WeChat official accounts", "start_pos": 37, "end_pos": 61, "type": "DATASET", "confidence": 0.8877769907315572}]}, {"text": "The average number of documents is 600.", "labels": [], "entities": []}, {"text": "Human annotators are asked to freely issue 100 queries to each official account to get XiaoIce response.", "labels": [], "entities": [{"text": "XiaoIce", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.8942373394966125}]}, {"text": "Thus, we obtain 100 query, XiaoIce response pairs for each official account.", "labels": [], "entities": []}, {"text": "We also send the same 100 queries of each official account to DocChat based on official account's corresponding document index, and obtain another 100 query, DocChat response pairs.", "labels": [], "entities": [{"text": "DocChat", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9600245356559753}]}, {"text": "Given these 1,000 query, XiaoIce response, DocChat response triples, we let human annotators do a side-by-side evaluation, by asking them which response is better for each query.", "labels": [], "entities": [{"text": "XiaoIce", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.799281656742096}]}, {"text": "Note that, the source of each response is masked during evaluation procedure.", "labels": [], "entities": []}, {"text": "Better (or Worse) denotes a DocChat response is better (or worse) than a XiaoIce response, Tie denotes a DocChat response and a XiaoIce response are equally good or bad.", "labels": [], "entities": [{"text": "Tie", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9315333962440491}]}, {"text": "From we observe that: (1) 156 DocChat responses (58+47+51) out of 1,000 queries are triggered.", "labels": [], "entities": []}, {"text": "The trigger rate of DocChat is 15.6%.", "labels": [], "entities": [{"text": "trigger rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9899258017539978}, {"text": "DocChat", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.890335738658905}]}, {"text": "We check un-triggered queries, and find most of them are chitchat, such as \"hi\", \"hello\", \"who are you?\".", "labels": [], "entities": []}, {"text": "(2) Better cases are more than worse cases.", "labels": [], "entities": []}, {"text": "Most queries in better cases are nonchitchat ones, and their contents are highly related to the domain of their corresponding WeChat official accounts.", "labels": [], "entities": [{"text": "WeChat official accounts", "start_pos": 126, "end_pos": 150, "type": "DATASET", "confidence": 0.9571569363276163}]}, {"text": "(3) Our proposed method is a perfect complement for chitchat engines on inBetter Worse Tie Compare to XiaoIce 58 47 51: Chatbot side-by-side evaluation.", "labels": [], "entities": [{"text": "XiaoIce 58 47", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9571833411852518}]}, {"text": "In both QA and chatbot, response triggering is important.", "labels": [], "entities": [{"text": "response triggering", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8019181191921234}]}, {"text": "Similar to, we also evaluate answer triggering using Precision, Recall, and F1 score as metrics.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8988152742385864}, {"text": "Precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9980728626251221}, {"text": "Recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9609438180923462}, {"text": "F1 score", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9855692982673645}]}, {"text": "We use the WikiQA de-   velopment set to tune the scaling factor \u03b1 and trigger threshold \u03c4 that are described in Section 5, where \u03b1 is set to 0.9 and \u03c4 is set to 0.5.", "labels": [], "entities": [{"text": "WikiQA de-   velopment set", "start_pos": 11, "end_pos": 37, "type": "METRIC", "confidence": 0.7436424255371094}, {"text": "trigger threshold \u03c4", "start_pos": 71, "end_pos": 90, "type": "METRIC", "confidence": 0.9324901700019836}]}, {"text": "shows the evaluation results compare to.", "labels": [], "entities": []}, {"text": "We think the improvements come from the fact that our response ranking model are more discriminative, as more semantic-level features are leveraged.", "labels": [], "entities": []}, {"text": "XiaoIce is a famous Chinese chatbot engine, which can be found in many platforms including WeChat official accounts (like business pages on Facebook Messenger).", "labels": [], "entities": [{"text": "XiaoIce", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9357408285140991}, {"text": "WeChat official accounts", "start_pos": 91, "end_pos": 115, "type": "DATASET", "confidence": 0.9198071161905924}]}, {"text": "The documents that each official account maintains and post to their followers can be easily obtained from the Web.", "labels": [], "entities": []}, {"text": "Meanwhile, a WeChat official account can choose to authorize XiaoIce to respond to its followers' utterances.", "labels": [], "entities": [{"text": "WeChat official account", "start_pos": 13, "end_pos": 36, "type": "DATASET", "confidence": 0.9136384328206381}, {"text": "XiaoIce", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.902617335319519}]}, {"text": "We design an interesting evaluation below to compare DocChat with XiaoIce, based on the publicly available documents.", "labels": [], "entities": [{"text": "DocChat", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9034123420715332}, {"text": "XiaoIce", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.9655449390411377}]}], "tableCaptions": [{"text": " Table 1: Impacts of features at different levels.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of AS task on WikiQA.", "labels": [], "entities": [{"text": "AS task", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.81692174077034}, {"text": "WikiQA", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.9311858415603638}]}, {"text": " Table 3: Evaluation of AS on QASent.", "labels": [], "entities": [{"text": "AS", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.5677812695503235}, {"text": "QASent", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.6227023005485535}]}, {"text": " Table 4: Impacts of different feature groups.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of AT on WikiQA.", "labels": [], "entities": [{"text": "AT", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.7937852144241333}, {"text": "WikiQA", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9375874400138855}]}]}