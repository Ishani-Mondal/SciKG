{"title": [{"text": "Collective Entity Resolution with Multi-Focal Attention", "labels": [], "entities": [{"text": "Collective Entity Resolution", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6388094127178192}]}], "abstractContent": [{"text": "Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base (KB).", "labels": [], "entities": [{"text": "Entity resolution", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8465102314949036}]}, {"text": "Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities that are related in the KB.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7407191097736359}]}, {"text": "We explore attention-like mechanisms for coherence, where the evidence for each candidate is based on a small set of strong relations, rather than relations to all other entities in the document.", "labels": [], "entities": []}, {"text": "The rationale is that document-wide support may simply not exist for non-salient entities, or entities not densely connected in the KB.", "labels": [], "entities": []}, {"text": "Our proposed system outperforms state-of-the-art systems on the CoNLL", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9356675744056702}]}], "introductionContent": [{"text": "Entity resolution (ER) is the task of mapping mentions of entities in text to corresponding records in a knowledge base (KB) (.", "labels": [], "entities": [{"text": "Entity resolution (ER)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8499590694904328}]}, {"text": "ER is a challenging problem because mentions are often ambiguous on their own, and can only be resolved given appropriate context.", "labels": [], "entities": [{"text": "ER", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9435727596282959}]}, {"text": "For example, the mention Beirut may refer to the capital of Lebanon, the band from New Mexico, or a drinking game).", "labels": [], "entities": []}, {"text": "Names may also refer to entities that are not in the KB, a problem known as NIL detection.", "labels": [], "entities": [{"text": "NIL detection", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.7878502011299133}]}, {"text": "Most ER systems consist of a mention model, a context model, and a coherence model, * Currently at Tel Aviv University \u2020 Currently at IIT Bombay).", "labels": [], "entities": []}, {"text": "The mention model associates each entity with its possible textual representations (also known as aliases or surface forms).", "labels": [], "entities": []}, {"text": "The context model helps resolve an ambiguous mention using textual features extracted from the surrounding context.", "labels": [], "entities": []}, {"text": "The coherence model, the focus of this work, encourages all mentions to resolve to entities that are related to each other.", "labels": [], "entities": []}, {"text": "Relations maybe established via the KB, Web links, embeddings, or other resources.", "labels": [], "entities": []}, {"text": "Coherence models often define an objective function that includes local and pairwise candidate scores, where the pairwise scores correspond to some notion of coherence or relation strength.", "labels": [], "entities": []}, {"text": "Support fora candidate is typically aggregated over relations to all other entities in the document.", "labels": [], "entities": []}, {"text": "One problem with this approach is that it may dilute evidence for entities that are not salient in the document, or not well-connected in the KB.", "labels": [], "entities": []}, {"text": "Our work aims to address this issue.", "labels": [], "entities": []}, {"text": "We introduce a novel coherence model with an attention mechanism, where the score for each candidate only depends on a small subset of mentions.", "labels": [], "entities": []}, {"text": "Attention has recently been used with considerable empirical success in tasks such as translation () and image caption generation (.", "labels": [], "entities": [{"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9830788373947144}, {"text": "image caption generation", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.8545989592870077}]}, {"text": "We argue that attention is also desirable for collective ER due to the discussed imbalance in the number of relations for different entities.", "labels": [], "entities": [{"text": "collective ER", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.5123613327741623}]}, {"text": "Attention models typically have a single focus, implemented using the softmax function.", "labels": [], "entities": []}, {"text": "Our model allows each candidate to focus on multiple mentions, and, to implement it, we introduce a novel smooth version of the multi-focus attention Figure 1: Illustration of the ER problem for three mentions \"Beirut\", \"New Mexico\" and \"Santa Fe\".", "labels": [], "entities": []}, {"text": "each mention has three possible disambiguations.", "labels": [], "entities": []}, {"text": "Edges link disambiguations that have Wikipedia links between their respective pages.", "labels": [], "entities": []}, {"text": "function, which generalizes soft-max.", "labels": [], "entities": []}, {"text": "Our system uses mention and context models similar to those of, along with our novel multi-focal attention model to enforce coherence, leading to significant performance improvements on and TAC).", "labels": [], "entities": [{"text": "TAC", "start_pos": 190, "end_pos": 193, "type": "METRIC", "confidence": 0.9278497695922852}]}, {"text": "In particular, we achieve a 20% relative reduction in error from on CoNLL, and a 22% error reduction from Cucerzan (2012) on TAC 2012.", "labels": [], "entities": [{"text": "error", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.8146029710769653}, {"text": "CoNLL", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.9059526920318604}, {"text": "error reduction", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.9749144613742828}, {"text": "TAC 2012", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.884758859872818}]}, {"text": "Our contributions thus consist of defining a novel multi-focal attention model and applying it successfully to an entity resolution system.", "labels": [], "entities": [{"text": "entity resolution", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7641521692276001}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Alias-entity map statistics on CoNLL  test-b, 4483 gold mentions. Mention recall is the  percentage of mentions with at least one known  entity; gold recall is the percentage of mentions  where the gold entity was included in the candi- dates. Unique aliases map to exactly one entity.  The last column shows the number of candidates  averaged over test-b mentions.", "labels": [], "entities": [{"text": "CoNLL  test-b", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9537218809127808}, {"text": "Mention recall", "start_pos": 76, "end_pos": 90, "type": "METRIC", "confidence": 0.7191675901412964}, {"text": "gold recall", "start_pos": 155, "end_pos": 166, "type": "METRIC", "confidence": 0.643836498260498}]}, {"text": " Table 2: YAGO+KB alias-entity map statistics on  the TAC KBP datasets, restricted to non-NIL men- tions.", "labels": [], "entities": [{"text": "TAC KBP datasets", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9326099753379822}]}, {"text": " Table 3: CoNLL test-b evaluation for recent com- petitive systems and our models, using different  alias-entity maps. \"KB+HP*\" means we train and  score entities using KB+HP, but output entities  only in HP.", "labels": [], "entities": []}, {"text": " Table 4: Results on the TAC 2010 (top), TAC  2011 (middle), and TAC 2012 bottom evaluation  datasets.", "labels": [], "entities": [{"text": "TAC 2010", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.7885115742683411}, {"text": "TAC  2011", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.809107095003128}, {"text": "TAC 2012 bottom evaluation  datasets", "start_pos": 65, "end_pos": 101, "type": "DATASET", "confidence": 0.831193995475769}]}]}