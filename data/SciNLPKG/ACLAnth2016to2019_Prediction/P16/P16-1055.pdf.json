{"title": [{"text": "How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "How much is 131 million US dollars?", "labels": [], "entities": []}, {"text": "To help readers put such numbers in context , we propose anew task of automatically generating short descriptions known as perspectives, e.g. \"$131 million is about the cost to employ everyone in Texas over a lunch period\".", "labels": [], "entities": []}, {"text": "First, we collect a dataset of numeric mentions in news articles , where each mention is labeled with a set of rated perspectives.", "labels": [], "entities": []}, {"text": "We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation.", "labels": [], "entities": [{"text": "formula construction", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.735947847366333}, {"text": "description generation", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.74428591132164}]}, {"text": "In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility.", "labels": [], "entities": []}, {"text": "In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neu-ral network.", "labels": [], "entities": []}, {"text": "Our system obtains a 15.2% F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.", "labels": [], "entities": [{"text": "F 1", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9929664134979248}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9970967769622803}]}], "introductionContent": [{"text": "When posed with a mention of a number, such as \"Cristiano Ronaldo, the player who Madrid acquired for . ] a $131 million\"), it is often difficult to comprehend the scale of large (or small) absolute values like $131 million.", "labels": [], "entities": []}, {"text": "Studies have shown that providing relative comparisons, or perspectives, such as \"about the cost to employ everyone in Texas over a lunch period\" significantly improves comprehension when measured in terms of memory retention or outlier detection (.", "labels": [], "entities": []}, {"text": "Figure 1: An overview of the perspective generation task: given a numeric mention, generate a short description (a perspective) that allows the reader to appreciate the scale of the mentioned number.", "labels": [], "entities": [{"text": "perspective generation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7213870584964752}]}, {"text": "In our system, we first construct a formula over facts in our knowledge base and then generate a description of that formula.", "labels": [], "entities": []}, {"text": "Previous work in the HCI community has relied on either manually generated perspectives () or present a fact as is from a knowledge base.", "labels": [], "entities": []}, {"text": "As a result, these approaches are limited to contexts in which a relevant perspective already exists.", "labels": [], "entities": []}, {"text": "In this paper, we generate perspectives by composing facts from a knowledge base.", "labels": [], "entities": []}, {"text": "For example, we might describe $100,000 to be \"about twice the median income fora year\", and describe $5 million to be the \"about how much the average person makes over their lifetime\".", "labels": [], "entities": []}, {"text": "Leveraging compositionality allows us to achieve broad coverage of numbers from a relatively small collection of familiar facts, e.g. median income and a person's lifetime.", "labels": [], "entities": []}, {"text": "Using compositionality in perspectives is also concordant with our understanding of how people learn to appreciate scale.", "labels": [], "entities": []}, {"text": "find that students learning to appreciate scale do so mainly by anchoring with familiar concepts, e.g. $50,000 is slightly less than the median income in the US, and by unitization, i.e. improvising a system of units that is more relatable, e.g. using the Earth as a measure of mass when describing the mass of Jupiter to be that of 97 Earths.", "labels": [], "entities": []}, {"text": "Here, compositionality naturally unitizes the constituent facts: in the examples above, money was unitized in terms of median income, and time was unitized in a person's lifetime.", "labels": [], "entities": []}, {"text": "Unitization and anchoring have also been proposed by as the basis of a design methodology for constructing visual perspectives called concrete scales.", "labels": [], "entities": []}, {"text": "When generating compositional perspectives, we must address two key challenges: constructing familiar, relevant and meaningful formulas and generating easy-to-understand descriptions or perspectives.", "labels": [], "entities": []}, {"text": "We tackle the first challenge using an overgenerate-and-rank paradigm, selecting formulas using signals from familiarity, compositionality, numeric proximity and semantic similarity.", "labels": [], "entities": []}, {"text": "We treat the second problem of generation as a translation problem and use a sequence-tosequence recurrent neural network (RNN) to generate perspectives from a formula.", "labels": [], "entities": []}, {"text": "We evaluate individual components of our system quantitatively on a dataset collected using crowdsourcing.", "labels": [], "entities": []}, {"text": "Our formula construction method improves on F 1 over a non-compositional baseline by about 17.8%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9633211493492126}]}, {"text": "Our generation method improves over a simple baseline by 12.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9992582201957703}]}], "datasetContent": [{"text": "We break our data collection task into two steps, mirroring formula selection and description generation: first, we collect descriptions of formulas constructed exhaustively from our knowledge base (for generation), and then we use these descriptions to collect preferences for perspectives (for construction).", "labels": [], "entities": [{"text": "description generation", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7305213809013367}]}, {"text": "We manually constructed a knowledge base with 142 tuples and 9 fundamental units 1 from the United States Bu-reau of Statistics, the orders of magnitude topic on Wikipedia and other Wikipedia pages.", "labels": [], "entities": [{"text": "United States Bu-reau of Statistics", "start_pos": 92, "end_pos": 127, "type": "DATASET", "confidence": 0.7252510905265808}]}, {"text": "The facts chosen are somewhat crude; for example, though \"the cost of an employee\" is a very context dependent quantity, we take its value to be the median cost for an employer in the United States, $71,000.", "labels": [], "entities": []}, {"text": "Presenting facts at a coarse level of granularity makes them more familiar to the general reader while still being appropriate for perspective generation: the intention is to convey the right scale, not necessarily the precise quantity.", "labels": [], "entities": [{"text": "perspective generation", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7207505106925964}]}, {"text": "We collected 53,946 sentences containing numeric mentions from the newswire section of LDC2011T07 using simple regular expression patterns like The values and units of the numeric mentions in each sentence were normalized and converted to fundamental units (e.g. from miles to length).", "labels": [], "entities": [{"text": "newswire section of LDC2011T07", "start_pos": 67, "end_pos": 97, "type": "DATASET", "confidence": 0.8899965137243271}]}, {"text": "We then randomly selected up to 200 mentions of each of the 9 types in bins with boundaries 10 \u22123 , 1, 10 3 , 10 6 , 10 9 , 10 12 leading to 4,931 mentions that are stratified by unit and magnitude.", "labels": [], "entities": []}, {"text": "Finally, we chose mentions which could be described by at least one numeric expression, resulting in the 2,041 mentions that we use in our experiments ().", "labels": [], "entities": []}, {"text": "We note that there is a slight bias towards mentions of money and people because these are more common in the news corpus.", "labels": [], "entities": []}, {"text": "Next, we exhaustively generate valid formulas from our knowledge base.", "labels": [], "entities": []}, {"text": "We represent the knowledge base as a graph over units with vertices and edges annotated with tuples ().", "labels": [], "entities": []}, {"text": "Every vertex in this graph is labeled with a unit u and contains the set of tuples with this unit: {t \u2208 K : t.unit = u}.", "labels": [], "entities": []}, {"text": "Additionally, for every vertex in the graph with a unit of the form u 1 /u 2 , where u 2 has no denominator, we add an edge from u 1 /u 2 to u 1 , annotated with all tuples of type u 2 : in we add an edge from money/person to money annotated with the three person tuples in.", "labels": [], "entities": []}, {"text": "The set of formulas with unit u is obtained by enumerating all paths in the graph which terminate at the vertex u.", "labels": [], "entities": []}, {"text": "The multiplier of the formula is set so that the value of were well represented in the corpus.", "labels": [], "entities": []}, {"text": "2 Some types had fewer than 200 mentions for some bins.", "labels": [], "entities": []}, {"text": "the formula matches the value of the mention.", "labels": [], "entities": []}, {"text": "For example, the formula in was constructed by traversing the graph from money/time/person to money: we start with a tuple in money/time/person (cost of an employee) and then multiply by a tuple with unit time (time for lunch) and then by unit person (population of Texas), thus traversing two edges to arrive at money.", "labels": [], "entities": []}, {"text": "Using the 142 tuples in our knowledge base, we generate a total of 1,124 formulas sans multiplier.", "labels": [], "entities": []}, {"text": "The main goal of collecting descriptions of formulas is to train a language generation system, though these descriptions will also be useful while collecting training data for formula selection.", "labels": [], "entities": [{"text": "formula selection", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.738610714673996}]}, {"text": "For every unit in our knowledge base and every value in the set {10 \u22127 , 10 \u22126 . .", "labels": [], "entities": []}, {"text": ", 10 10 }, we generated all valid formulas.", "labels": [], "entities": []}, {"text": "We further restricted this set to formulas with a multiplier between 1/100 and 100, based on the rationale that human cognition of scale sharply drops beyond an order of magnitude (Tretter et al., 2006).", "labels": [], "entities": []}, {"text": "In total, 5000 formulas were presented to crowdworkers on Amazon Mechanical Turk, with a prompt asking them to rephrase the formula as an English expression).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.9611286719640096}]}, {"text": "We obtained 5-7 descriptions of each formula, leading to a total of 31,244 unique descriptions.", "labels": [], "entities": []}, {"text": "Collecting data on formula preference.", "labels": [], "entities": []}, {"text": "Finally, given a numeric mention, we ask crowdworkers which perspectives from the description dataset they prefer.", "labels": [], "entities": []}, {"text": "Note that formulas generated fora particular mention may differ in multiplier with a formula in the description dataset.", "labels": [], "entities": []}, {"text": "We thus relax our constraints on factual accuracy while collecting this formula preference dataset: for each mention x, we choose a random perspective from the description dataset described above corresponding to a formula whose value is within a factor of 2 from the mention's value, x.value.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9483305811882019}]}, {"text": "A smaller factor led to too many mentions without a valid comparison, while a larger one led to blatant factual inaccuracies.", "labels": [], "entities": []}, {"text": "The perspectives were partitioned into sets of four and displayed to crowdworkers along with a \"None of the above\" option with the following prompt: \"We would like you to pickup to two of these descriptions that are useful in understanding the scale of the highlighted number\" ().", "labels": [], "entities": []}, {"text": "A formula is rated to be useful by simple majority.", "labels": [], "entities": []}, {"text": "Figure 6 provides a summary of the dataset collected, visualizing how many formulas are useful, controlling for the size of the formula.", "labels": [], "entities": []}, {"text": "The exhaustive generation procedure produces a large number of spurious formulas like \"20 \u00d7 trash generated in the US \u00d7 a minute \u00d7 number of employees on Medicare\".", "labels": [], "entities": []}, {"text": "Nonetheless, compositional formulas are quite useful in the appropriate context; presents some mentions with highly rated perspectives and formulas.", "labels": [], "entities": []}, {"text": "In addition to the automatic evaluations for each component of the system, we also ran an end-toend human evaluation on an independent set of 211 mentions collected using the same methodology described in Section 3.", "labels": [], "entities": []}, {"text": "Crowdworkers were asked to choose between perspectives generated by our full system (LR+RNN) and those generated by the baseline of picking the numerically closest tuple in the knowledge base (BASELINE).", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9946204423904419}]}, {"text": "They could also indicate if either both or none of the shown perspectives appeared useful.", "labels": [], "entities": []}, {"text": "8 summarizes the results of the evaluation and an error analysis conducted by the authors.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9474316537380219}]}, {"text": "Errors were characterized as either being errors in generation (e.g.) or violations of the criteria in selecting good formulas described in Section 4).", "labels": [], "entities": []}, {"text": "The other category mostly contains cases where the output generated by LR+RNN appears reasonable by the above criteria but was not chosen by a majority of workers.", "labels": [], "entities": []}, {"text": "A few of the mentions shown did not properly describe a numeric quantity, e.g. \". .", "labels": [], "entities": []}, {"text": "claimed responsibility fora 2009 gun massacre . .", "labels": [], "entities": [{"text": "2009 gun massacre", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.5595575571060181}]}, {"text": "\" and were labeled invalid mentions.", "labels": [], "entities": []}, {"text": "The most common error is the selection of a formula that is not contextually relevant to the mentioned text because no such Input formula Generated perspective 7 \u00d7 the cost of an employee \u00d7 a week 7 times the cost of employing one person for one week 1/10 \u00d7 the cost of an employee \u00d7 the population of California \u00d7 the time taken fora football game one tenth the cost of an employee during a football game by the population of California 1 \u00d7 coffee consumption \u00d7 a minute \u00d7 population of the world the amount of coffee consumed in one minute on the world 6 \u00d7 weight of a person \u00d7 population of California six times the weight of the people who is worth: Examples of perspectives generated by the sequence-to-sequence RNN.", "labels": [], "entities": [{"text": "Input", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9916587471961975}]}, {"text": "The model is able to capture rephrasings of fact descriptions and reordering of the facts.", "labels": [], "entities": []}, {"text": "However, it often confuses prepositions and, very rarely, can produce nonsensical utterances.: Examples of perspectives generated by our system that frame the mentioned quantity to be larger or smaller (top to bottom) than initially the authors thought.", "labels": [], "entities": []}, {"text": "formula exists within the knowledge base (within an order of magnitude of the mentioned value): a larger knowledge base would significantly decrease these errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of numeric mentions, perspectives and their corresponding formulas in the dataset.  All the examples except the last one are rated to be useful by crowdworkers.", "labels": [], "entities": []}, {"text": " Table 4: The top three examples outputted by  the ranking system with the scores reported by the  system.", "labels": [], "entities": []}, {"text": " Table 6: Examples of perspectives generated by the sequence-to-sequence RNN. The model is able to  capture rephrasings of fact descriptions and reordering of the facts. However, it often confuses preposi- tions and, very rarely, can produce nonsensical utterances.", "labels": [], "entities": []}, {"text": " Table 7: Results of an end-to-end human evaluation of the output produced by our perspective generation  system (LR+RNN) and a baseline (BASELINE) that picks the numerically closest tuple in the knowledge  base for each mention.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9907478094100952}]}]}