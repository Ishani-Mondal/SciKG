{"title": [{"text": "Latent Predictor Networks for Code Generation", "labels": [], "entities": [{"text": "Code Generation", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.8317395150661469}]}], "abstractContent": [{"text": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs.", "labels": [], "entities": [{"text": "language generation", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7459382116794586}]}, {"text": "We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions.", "labels": [], "entities": []}, {"text": "Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training.", "labels": [], "entities": []}, {"text": "Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification.", "labels": [], "entities": []}, {"text": "We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearth-stone.", "labels": [], "entities": []}, {"text": "On these, and a third preexisting corpus, we demonstrate that marginalis-ing multiple predictors allows our model to outperform strong benchmarks.", "labels": [], "entities": []}], "introductionContent": [{"text": "The generation of both natural and formal languages often requires models conditioned on diverse predictors ().", "labels": [], "entities": []}, {"text": "Most models take the restrictive approach of employing a single predictor, such as a word softmax, to predict all tokens of the output sequence.", "labels": [], "entities": []}, {"text": "To illustrate its limitation, suppose we wish to generate the answer to the question \"Who wrote The Foundation?\" as \"The Foundation was written by Isaac Asimov\".", "labels": [], "entities": []}, {"text": "The generation of the words \"Issac Asimov\" and \"The Foundation\" from a word softmax trained on annotated data is unlikely to succeed as these words are sparse.", "labels": [], "entities": []}, {"text": "A robust model might, for example, employ one pre- dictor to copy \"The Foundation\" from the input, and a another one to find the answer \"Issac Asimov\" by searching through a database.", "labels": [], "entities": []}, {"text": "However, training multiple predictors is in itself a challenging task, as no annotation exists regarding the predictor used to generate each output token.", "labels": [], "entities": []}, {"text": "Furthermore, predictors generate segments of different granularity, as database queries can generate multiple tokens while a word softmax generates a single token.", "labels": [], "entities": []}, {"text": "In this work we introduce Latent Predictor Networks (LPNs), a novel neural architecture that fulfills these desiderata: at the core of the architecture is the exact computation of the marginal likelihood over latent predictors and generated segments allowing for scalable training.", "labels": [], "entities": []}, {"text": "We introduce anew corpus for the automatic generation of code for cards in Trading Card Games (TCGs), on which we validate our model . TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding pool of cards.", "labels": [], "entities": [{"text": "Hearthstone (HS)", "start_pos": 179, "end_pos": 195, "type": "DATASET", "confidence": 0.8122389614582062}]}, {"text": "Examples of such cards are shown in.", "labels": [], "entities": []}, {"text": "Each card is identified by its attributes (e.g., name and cost) and has an effect that is described in a text box.", "labels": [], "entities": []}, {"text": "Digital implementations of these games implement the game logic, which includes the card effects.", "labels": [], "entities": []}, {"text": "This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer.", "labels": [], "entities": [{"text": "data extraction", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7411630004644394}]}, {"text": "This dataset presents additional challenges to prior work in code generation (, including the handling of structured input-i.e. cards are composed by multiple sequences (e.g., name and description)-and attributes (e.g., attack and cost), and the length of the generated sequences.", "labels": [], "entities": [{"text": "code generation", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.8328197002410889}]}, {"text": "Thus, we propose an extension to attention-based neural models () to attend over structured inputs.", "labels": [], "entities": []}, {"text": "Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions.", "labels": [], "entities": [{"text": "code compression", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7408843636512756}]}, {"text": "Experiments performed on our new datasets, and a further pre-existing one, suggest that our extensions outperform strong benchmarks.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: We first describe the data collection process (Section 2) and formally define our problem and our baseline method (Section 3).", "labels": [], "entities": [{"text": "data collection", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7262489199638367}]}, {"text": "Then, we propose our extensions, namely, the structured attention mechanism (Section 4) and the LPN architecture (Section 5).", "labels": [], "entities": []}, {"text": "We follow with the description of our code compression algorithm (Section 6).", "labels": [], "entities": [{"text": "code compression", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7737949192523956}]}, {"text": "Our model is validated by comparing with multiple benchmarks (Section 7).", "labels": [], "entities": []}, {"text": "Finally, we contextualize our findings with related work (Section 8) and present the conclusions of this work (Section 9).", "labels": [], "entities": []}], "datasetContent": [{"text": "We obtain data from open source implementations of two different TCGs, MTG in Java 2 and HS in Python.", "labels": [], "entities": []}, {"text": "The statistics of the corpora are illustrated in.", "labels": [], "entities": []}, {"text": "In both corpora, each card is implemented in a separate class file, which we strip of imports and comments.", "labels": [], "entities": []}, {"text": "We categorize the content of each card into two different groups: singular fields that contain only one value; and text fields, which contain multiple words representing different units of meaning.", "labels": [], "entities": []}, {"text": "In MTG, there are six singular fields (attack, defense, rarity, set, id, and  health) and four text fields (cost, type, name, and description), whereas HS cards have eight singular fields (attack, health, cost and durability, rarity, type, race and class) and two text fields (name and description).", "labels": [], "entities": [{"text": "MTG", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.7728721499443054}]}, {"text": "Text fields are tokenized by splitting on whitespace and punctuation, with exceptions accounting for domain specific artifacts (e.g., Green mana is described as \"{G}\" in MTG).", "labels": [], "entities": []}, {"text": "Empty fields are replaced with a \"NIL\" token.", "labels": [], "entities": []}, {"text": "The code for the HS card in is shown in.", "labels": [], "entities": [{"text": "HS card", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.7421167194843292}]}, {"text": "The effect of \"drawing cards until the player has as many cards as the opponent\" is implemented by computing the difference between the players' hands and invoking the draw method that number of times.", "labels": [], "entities": []}, {"text": "This illustrates that the mapping between the description and the code is nonlinear, as no information is given in the text regarding the specifics of the implementation.", "labels": [], "entities": []}, {"text": "Datasets Tests are performed on the two datasets provided in this paper, described in Table 1.", "labels": [], "entities": []}, {"text": "Additionally, to test the model's ability of generalize to other domains, we report results in the Django dataset (), comprising of 16000 training, 1000 development and 1805 test annotations.", "labels": [], "entities": [{"text": "Django dataset", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9687053859233856}]}, {"text": "Each data point consists of a line of Python code together with a manually created natural language description.", "labels": [], "entities": []}, {"text": "Neural Benchmarks We implement two standard neural networks, namely a sequence-tosequence model) and an attention-based model ().", "labels": [], "entities": []}, {"text": "The former is adapted to work with multiple input fields by concatenating them, while the latter uses our proposed attention model.", "labels": [], "entities": []}, {"text": "These models are denoted as \"Sequence\" and \"Attention\".", "labels": [], "entities": [{"text": "Attention", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9590039253234863}]}, {"text": "Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing.", "labels": [], "entities": [{"text": "Machine Translation Baselines", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8542120456695557}, {"text": "semantic parsing", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7208530306816101}]}, {"text": "Unfortunately, these approaches define strong assumptions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains (.", "labels": [], "entities": []}, {"text": "However, the work in provides evidence that using machine translation systems without committing to such assumptions can lead to results competitive with the systems described above.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7150850892066956}]}, {"text": "We follow the same approach and create a phrase-based ( model and a hierarchical model (or PCFG) as benchmarks for the work presented here.", "labels": [], "entities": []}, {"text": "As these models are optimized to generate words, not characters, we implement a tokenizer that splits on all punctuation characters, except for the \" \" character.", "labels": [], "entities": []}, {"text": "We also facilitate the task by splitting CamelCase words (e.g., class TirionFordring \u2192 class Tirion Fordring).", "labels": [], "entities": []}, {"text": "Otherwise all class names would not be generated correctly by these methods.", "labels": [], "entities": []}, {"text": "We used the models implemented in Moses to generate these baselines using standard parameters, using IBM Alignment Model 4 for word alignments, MERT for tuning () and a 4-gram Kneser-Ney Smoothed language model).", "labels": [], "entities": [{"text": "IBM Alignment Model 4", "start_pos": 101, "end_pos": 122, "type": "DATASET", "confidence": 0.7939843982458115}, {"text": "word alignments", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7284233570098877}, {"text": "MERT", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9940463304519653}]}, {"text": "These models will be denoted as \"Phrase\" and \"Hierarchical\", respectively.", "labels": [], "entities": []}, {"text": "Retrieval Baseline It was reported in) that a simple retrieval method that outputs the most similar input for each sample, measured using Levenshtein Distance, leads to good results.", "labels": [], "entities": [{"text": "Levenshtein Distance", "start_pos": 138, "end_pos": 158, "type": "METRIC", "confidence": 0.57906274497509}]}, {"text": "We implement this baseline by computing the average Levenshtein Distance for each input field.", "labels": [], "entities": [{"text": "Levenshtein Distance", "start_pos": 52, "end_pos": 72, "type": "METRIC", "confidence": 0.6317883878946304}]}, {"text": "This baseline is denoted \"Retrieval\".", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.6477377414703369}]}, {"text": "Evaluation A typical metric is to compute the accuracy of whether the generated code exactly matches the reference code.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.999349057674408}]}, {"text": "This is informative as it gives an intuition of how many samples can be used without further human post-editing.", "labels": [], "entities": []}, {"text": "However, it does not provide an illustration on the degree of closeness to achieving the correct code.", "labels": [], "entities": []}, {"text": "Thus, we also test using BLEU-4 () at the token level.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.998532772064209}]}, {"text": "There are clearly problems with these metrics.", "labels": [], "entities": []}, {"text": "For instance, source code can be correct without matching the reference.", "labels": [], "entities": []}, {"text": "The code in, could have also been implemented by calling the draw function in an cycle that exists once both players have the same number of cards in their hands.", "labels": [], "entities": []}, {"text": "Some tasks, such as the generation of queries (, have overcome this problem by executing the query and checking if the result is the same as the annotation.", "labels": [], "entities": []}, {"text": "However, we shall leave the study of these methologies for future work, as adapting these methods for our tasks is not trivial.", "labels": [], "entities": []}, {"text": "For instance, the correctness cards with conditional (e.g. if player has no cards, then draw a card) or non-deterministc (e.g. put a random card in your hand) effects cannot be simply validated by running the code.", "labels": [], "entities": []}, {"text": "Setup The multiple input types) are hyper-parametrized as follows: The C2W model (cf. \"C2W\" row) used to obtain continuous vectors for word types uses character embeddings of size 100 and LSTM states of size 300, and generates vectors of size 300.", "labels": [], "entities": []}, {"text": "We also report on results using word lookup tables of size 300, where we replace singletons with a special unknown token with probability 0.5 during training, which is then used for out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "For text fields, the context (cf. \"Bi-LSTM\" row) is encoded with a Bi-LSTM of size 300 for the forward and backward states.", "labels": [], "entities": []}, {"text": "Finally, a linear layer maps the different input tokens into a common space with of size 300 (cf. \"Linear\" row).", "labels": [], "entities": []}, {"text": "As for the attention model, we used an hidden layer of size 200 before applying the non-linearity (row \"Tanh\").", "labels": [], "entities": []}, {"text": "As for the decoder, we encode output characters with size 100 (cf. \"output (y)\" row), and an LSTM state of size 300 and an input representation of size 300 (cf. \"State(h+z)\" row).", "labels": [], "entities": []}, {"text": "For each pointer network (e.g., \"Copy From Name\" box), the intersection between the input units and the state units are performed with a vector of size 200.", "labels": [], "entities": []}, {"text": "Training is performed using mini-batches of 20 samples using AdaDelta and we report results using the iteration with the highest BLEU score on the validation set (tested at intervals of 5000 mini-batches).", "labels": [], "entities": [{"text": "AdaDelta", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9535809755325317}, {"text": "BLEU score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9777026176452637}]}, {"text": "Decoding is performed with abeam of 1000.", "labels": [], "entities": []}, {"text": "As for compression, we performed a grid search over compressing the code from 0% to 80% of the original average length over intervals of 20% for the HS and Django datasets.", "labels": [], "entities": [{"text": "HS and Django datasets", "start_pos": 149, "end_pos": 171, "type": "DATASET", "confidence": 0.7222971171140671}]}, {"text": "On the MTG dataset, we are forced to compress the code up to 80% due to performance issues when training with extremely long sequences.", "labels": [], "entities": [{"text": "MTG dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9168483912944794}]}], "tableCaptions": [{"text": " Table 1: Statistics of the two TCG datasets.", "labels": [], "entities": [{"text": "TCG datasets", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9072853326797485}]}, {"text": " Table 2: First 10 compressed units in MTG. We  replaced newlines with \u21d3 and spaces with .", "labels": [], "entities": [{"text": "MTG", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.7703118920326233}]}, {"text": " Table 3: BLEU and Accuracy scores for the pro- posed task on two in-domain datasets (HS and  MTG) and an out-of-domain dataset (Django).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9996302127838135}, {"text": "Accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9996007084846497}]}, {"text": " Table 4: Results with increasing compression rates  with a regular softmax (cf. \"Softmax\") and a LPN  (cf. \"LPN\"). Performance values (cf. \"Seconds Per  Card\" block) are computed using one CPU.", "labels": [], "entities": []}]}