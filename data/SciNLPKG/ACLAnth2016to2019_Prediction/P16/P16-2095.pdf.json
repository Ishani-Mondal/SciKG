{"title": [{"text": "Metrics for Evaluation of Word-Level Machine Translation Quality Estimation", "labels": [], "entities": [{"text": "Word-Level Machine Translation Quality Estimation", "start_pos": 26, "end_pos": 75, "type": "TASK", "confidence": 0.6812354683876037}]}], "abstractContent": [{"text": "The aim of this paper is to investigate suitable evaluation strategies for the task of word-level quality estimation of machine translation.", "labels": [], "entities": [{"text": "word-level quality estimation of machine translation", "start_pos": 87, "end_pos": 139, "type": "TASK", "confidence": 0.6254197756449381}]}, {"text": "We suggest various metrics to replace F 1-score for the \"BAD\" class, which is currently used as main metric.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9686098694801331}, {"text": "BAD", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.8933966755867004}]}, {"text": "We compare the metrics' performance on real system outputs and synthetically generated datasets and suggest a reliable alternative to the F 1-BAD score-the multiplication of F 1-scores for different classes.", "labels": [], "entities": [{"text": "F 1-BAD score-the", "start_pos": 138, "end_pos": 155, "type": "METRIC", "confidence": 0.9579745928446451}]}, {"text": "Other metrics have lower discriminative power and are biased by unfair labellings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation (QE) of machine translation (MT) is a task of determining the quality of an automatically translated text without any oracle (reference) translation.", "labels": [], "entities": [{"text": "Quality estimation (QE) of machine translation (MT)", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8514421473849904}]}, {"text": "This task has lately been receiving significant attention: from confidence estimation (i.e. estimation of how confident a particular MT system is on a word or a phrase) it evolved to systemindependent QE and is performed at the word level (), sentence level () and document level ( ).", "labels": [], "entities": []}, {"text": "The emergence of a large variety of approaches to QE led to need for reliable ways to compare them.", "labels": [], "entities": [{"text": "QE", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9747539758682251}]}, {"text": "The evaluation metrics that have been used to compare the performance of systems participating in QE shared tasks have received some criticisms.", "labels": [], "entities": [{"text": "QE shared tasks", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.6549001932144165}]}, {"text": "shows that Pearson correlation better suits for the evaluation of sentence-level QE systems than mean absolute error (MAE), often used for this purpose.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.7824753224849701}, {"text": "mean absolute error (MAE)", "start_pos": 97, "end_pos": 122, "type": "METRIC", "confidence": 0.941635807355245}]}, {"text": "Pearson correlation evaluates how well a system captures the regularities in the data, whereas MAE essentially measures the difference between the true and the predicted scores and in many cases can be minimised by always predicting the average score as given by the training set labels.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.7311956882476807}, {"text": "MAE", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9745270609855652}]}, {"text": "Word-level QE is commonly framed as a binary task, i.e., the classification of every translated word as \"OK\" or \"BAD\".", "labels": [], "entities": [{"text": "Word-level QE", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.45234815776348114}, {"text": "BAD", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9385958909988403}]}, {"text": "This task has been evaluated in terms of F 1 -score for the \"BAD\" class, a metric that favours 'pessimistic' systems -i.e. systems that tend to assign the \"BAD\" label to most words.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9881958663463593}]}, {"text": "A trivial baseline strategy that assigns the label \"BAD\" to all words can thus receive a high score while being completely uninformative (.", "labels": [], "entities": [{"text": "BAD", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9193412065505981}]}, {"text": "However, no analysis of the word-level metrics' performance has been done and no alternative metrics have been proposed that are more reliable than the F 1 -BAD score.", "labels": [], "entities": [{"text": "F 1 -BAD score", "start_pos": 152, "end_pos": 166, "type": "METRIC", "confidence": 0.9464515566825866}]}, {"text": "In this paper we compare existing evaluation metrics for word-level QE, suggest a number of alternatives, and show that one of these alternatives leads to more objective and reliable results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment described above has a notable drawback: we evaluated metrics on the outputs of systems which had been tuned to maximise the F 1 -BAD score.", "labels": [], "entities": [{"text": "F 1 -BAD score", "start_pos": 139, "end_pos": 153, "type": "METRIC", "confidence": 0.9242393493652343}]}, {"text": "This means that the system rankings produced by other metrics maybe unfairly considered inaccurate.", "labels": [], "entities": []}, {"text": "Therefore, we suggest a more objective metric evaluation procedure which uses only synthetic datasets.", "labels": [], "entities": []}, {"text": "We generate datasets with different proportion of errors, compute the metrics' values and their statistical significance and then compare the metrics' discriminative power.", "labels": [], "entities": []}, {"text": "This procedure is further referred to as repeated sampling, because we sample artificial datasets multiple times.", "labels": [], "entities": []}, {"text": "Our goal is for the synthetic datasets to simulate real systems' output.", "labels": [], "entities": []}, {"text": "We achieve this by using the following procedure for synthetic data generation: \u2022 Choose the proportion of errors to introduce in the synthetic data.", "labels": [], "entities": [{"text": "synthetic data generation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.6837557951609293}]}, {"text": "\u2022 Collect all sequences that contain incorrect labels from the outputs of real systems.", "labels": [], "entities": []}, {"text": "\u2022 Randomly choose the sequences from this set until the overall number of errors reaches the chosen threshold.", "labels": [], "entities": []}, {"text": "\u2022 Take the rest of segments from the goldstandard labelling (so that they contain no errors).", "labels": [], "entities": [{"text": "goldstandard labelling", "start_pos": 37, "end_pos": 59, "type": "DATASET", "confidence": 0.8313219249248505}]}, {"text": "Thus our artificial datasets contain a specific number of errors, and all of them come from real systems.", "labels": [], "entities": []}, {"text": "We can generate datasets with very small differences in quality and identify metrics according to which this difference is more significant.", "labels": [], "entities": []}, {"text": "Let us compare the discriminative power of metrics m 1 and m 2 . We choose two error thresholds e 1 and e 2 . Then we sample a relatively small number (e.g. 100) of random datasets withe 1 errors.", "labels": [], "entities": []}, {"text": "Then -100 random datasets withe 2 errors.", "labels": [], "entities": []}, {"text": "We compute the values for both metrics on the two sets of random samples and for each metric we test if the difference between the results for the two sets is significant (we compute the statistic significance using non-paired t-test with Bonferroni correction).", "labels": [], "entities": []}, {"text": "Since we sampled the synthetic datasets a small number of times it is likely that the metrics will not detect any significant differences between them.", "labels": [], "entities": []}, {"text": "In this case we repeat the process with a larger (e.g. 200) number of samples and compare the p-values for two metrics again.", "labels": [], "entities": []}, {"text": "By gradually increasing the number of samples at some point we will find that one of the metrics recognises the differences in scores as statistically significant, while another one does not.", "labels": [], "entities": []}, {"text": "This means that this metric has higher discriminative power: it needs less samples to determine that the systems they are different.", "labels": [], "entities": []}, {"text": "The procedure is outlined in Algorithm 1.", "labels": [], "entities": []}, {"text": "In our experiments in order to make p-values more stable we repeat each sampling round (sampling of a set withe i errors 100, 200, etc. times) 1,000 times and use the average of p-values.", "labels": [], "entities": []}, {"text": "We used fixed sets of sample numbers:  2000 1000 500 200 phr F 1 -mult 10000 5000 5000 1000 500 200 SeqCor 10000 5000 5000 1000 500 500 phr F 1 -BAD 10000 10000 5000 1000 500 500 needs to observe significant differences between datasets which differ in this number of errors.", "labels": [], "entities": []}, {"text": "Numbers in cells are minimum numbers of samplings.", "labels": [], "entities": []}, {"text": "We do not show error differences greater than 0.2 because all metrics identify them well.", "labels": [], "entities": [{"text": "error", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9714832305908203}]}, {"text": "All metrics are sorted by discriminative power from best to worst, i.e. metrics at the top of the table require less samplings to tell one synthetic dataset from another.", "labels": [], "entities": []}, {"text": "As in the previous experiment, here the discriminative power of the multiplication of F 1 -scores is the highest.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.908220648765564}]}, {"text": "Surprisingly, MCC performs equally well.", "labels": [], "entities": [{"text": "MCC", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8081347942352295}]}, {"text": "Similarly to the experiment with real systems, the F 1 -BAD metric performs worse than the F 1 -multiply metric, but here their difference is more salient.", "labels": [], "entities": [{"text": "F 1 -BAD metric", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.8556906461715699}]}, {"text": "All phrase-motivated metrics show worse results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for all metrics. Numbers in synthetic dataset columns denote the number of system  submissions that were rated lower than the corresponding synthetic dataset.", "labels": [], "entities": []}, {"text": " Table 2: Repeated sampling: the minimum number of samplings required to discriminate between sam- ples with a different proportions of errors.", "labels": [], "entities": []}]}