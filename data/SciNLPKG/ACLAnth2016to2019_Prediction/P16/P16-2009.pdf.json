{"title": [{"text": "On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider two graph models of semantic change.", "labels": [], "entities": [{"text": "semantic change", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.8868239223957062}]}, {"text": "The first is a time-series model that relates embedding vectors from onetime period to embedding vectors of previous time periods.", "labels": [], "entities": []}, {"text": "In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the word's meaning across two time points.", "labels": [], "entities": []}, {"text": "We apply our two models to corpora across three different languages.", "labels": [], "entities": []}, {"text": "We find that semantic change is linear in two senses.", "labels": [], "entities": [{"text": "semantic change", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.8430185616016388}]}, {"text": "Firstly, today's embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods.", "labels": [], "entities": []}, {"text": "Secondly, self-similarity of words decays linearly in time.", "labels": [], "entities": []}, {"text": "We consider both findings as new laws/hypotheses of semantic change.", "labels": [], "entities": [{"text": "semantic change", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.8008190095424652}]}], "introductionContent": [{"text": "Meaning is not uniform, neither across space, nor across time.", "labels": [], "entities": []}, {"text": "Across space, different languages tend to exhibit different polysemous associations for corresponding terms).", "labels": [], "entities": []}, {"text": "Across time, several wellknown examples of meaning change in English have been documented.", "labels": [], "entities": [{"text": "meaning change", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.7584352791309357}]}, {"text": "For example, the word gay's meaning has shifted, during the 1970s, from an adjectival meaning of cheerful at the beginning of the 20 th century to its present meaning of homosexual ().", "labels": [], "entities": []}, {"text": "Similarly, technological progress has led to semantic broadening of terms such as transmission, mouse, or apple.", "labels": [], "entities": []}, {"text": "In this work, we consider two graph models of semantic change.", "labels": [], "entities": [{"text": "semantic change", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.8772530257701874}]}, {"text": "Our first model is a dynamic model in that the underlying paradigm is a (time-)series of graphs.", "labels": [], "entities": []}, {"text": "Each node in the series of graphs corresponds to one word, associated with which is a semantic embedding vector.", "labels": [], "entities": []}, {"text": "We then ask how the embedding vectors in onetime period (graph) can be predicted from the embedding vectors of neighbor words in previous time periods.", "labels": [], "entities": []}, {"text": "In particular, we postulate that there is a linear functional relationship that couples a word's today's meaning with its neighbor's meanings in the past.", "labels": [], "entities": []}, {"text": "When estimating the coefficients of this model, we find that the linear form appears indeed very plausible.", "labels": [], "entities": []}, {"text": "This functional form then allows us to address further questions, such as negative relationships between words -which indicate semantic differentiation overtime -as well as projections into the future.", "labels": [], "entities": []}, {"text": "We call our second graph model time-indexed self-similarity graphs.", "labels": [], "entities": []}, {"text": "In these graphs, each node corresponds to a time point and the link between two time points indicates the semantic similarity of a specific word across the two time points under consideration.", "labels": [], "entities": []}, {"text": "The analysis of these graphs reveals that most words obey a law of linear semantic 'decay': semantic self-similarity decreases linearly overtime.", "labels": [], "entities": []}, {"text": "In our work, we capture semantics by means of word embeddings derived from context-predicting neural network architectures, which have become the state-of-the-art in distributional semantics modeling ().", "labels": [], "entities": [{"text": "distributional semantics modeling", "start_pos": 166, "end_pos": 199, "type": "TASK", "confidence": 0.7387283245722452}]}, {"text": "Our approach and results are partly independent of this representation, however, in that we take a structuralist approach: we derive new, 'second-order embeddings' by modeling the meaning of words by means of their semantic similarity relations to all other words in the vocabulary (.", "labels": [], "entities": []}, {"text": "Thus, future research may in principle substitute the deep-learning architectures for semantics considered hereby any other method capable of producing semantic similarity values between lexical units.", "labels": [], "entities": []}, {"text": "This work is structured as follows.", "labels": [], "entities": []}, {"text": "In \u00a72, we discuss related work.", "labels": [], "entities": []}, {"text": "In \u00a73.1 and 3.2, respectively, we formally introduce the two graph models outlined.", "labels": [], "entities": []}, {"text": "In \u00a74, we detail our experiments and in \u00a75, we conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data As corpus for English, we use the Corpus of Historical American (COHA).", "labels": [], "entities": [{"text": "Corpus of Historical American (COHA)", "start_pos": 39, "end_pos": 75, "type": "DATASET", "confidence": 0.7338496957506452}]}, {"text": "This covers texts from the time period 1810 to 2000.", "labels": [], "entities": []}, {"text": "We extract two slices: the years 1900-2000 and 1810-2000.", "labels": [], "entities": []}, {"text": "For both slices, each time period t is one decade, e.g., T = {1810, 1820, 1830, . .", "labels": [], "entities": []}, {"text": "For each slice, we only keep words associated to the word classes nouns, adjectives, and verbs.", "labels": [], "entities": []}, {"text": "For computational and estimation purposes, we also only consider words that occur at least 100 times in each time period.", "labels": [], "entities": [{"text": "estimation", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.9355890154838562}]}, {"text": "To induce word embeddings w \u2208 Rd for each word w \u2208 V , we use word2vec () with default parametrizations.", "labels": [], "entities": []}, {"text": "We do so for each time period t \u2208 T independently.", "labels": [], "entities": []}, {"text": "We then use these embeddings to derive the new embeddings as in Eq.", "labels": [], "entities": []}, {"text": "Throughout, we use cosine similarity as sim measure.", "labels": [], "entities": []}, {"text": "For German, we consider a proprietary dataset of the German newspaper SZ 5 for which T = {1994, 1995, . .", "labels": [], "entities": [{"text": "German newspaper SZ 5", "start_pos": 53, "end_pos": 74, "type": "DATASET", "confidence": 0.8004263192415237}, {"text": "T", "start_pos": 85, "end_pos": 86, "type": "METRIC", "confidence": 0.9699450731277466}]}, {"text": "We lemmatize and POS tag the data and likewise only consider nouns, verbs and adjectives, making the same frequency constraints as in English.", "labels": [], "entities": [{"text": "POS", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9126101732254028}]}, {"text": "Finally, we use the PL (Migne, 1855) as data set for Latin.", "labels": [], "entities": [{"text": "PL (Migne, 1855)", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.7829833726088206}]}, {"text": "Here, T = {300, 400, . .", "labels": [], "entities": [{"text": "T", "start_pos": 6, "end_pos": 7, "type": "METRIC", "confidence": 0.9856120347976685}]}, {"text": "We use the same preprocessing, frequency, and word class constraints as for English and German.", "labels": [], "entities": []}, {"text": "Throughout, our datasets are well-balanced in terms of size.", "labels": [], "entities": []}, {"text": "For example, the English COHA datasets contain about 24M-30M tokens for each decade from 1900 to 2000, where the decades 1990 and 2000 contain slighly more data than the earlier decades.", "labels": [], "entities": [{"text": "English COHA datasets", "start_pos": 17, "end_pos": 38, "type": "DATASET", "confidence": 0.7978521784146627}]}, {"text": "The pre-1900 decades contain 18-24M tokens, with only the decades 1810 and 1820 containing very little data (1M and 7M tokens, respectively).", "labels": [], "entities": []}, {"text": "The corpora are also balanced by genre.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Coefficients (%) in regression of D t 0 on  t 0 , and adjusted R 2 values (%).", "labels": [], "entities": [{"text": "Coefficients", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9605734348297119}, {"text": "adjusted R 2", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.7563496828079224}]}, {"text": " Table 3: English data, 1900-2000. R 2 and predic- tion error in %.", "labels": [], "entities": [{"text": "English data", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.8914073705673218}, {"text": "R 2", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9835880100727081}, {"text": "predic- tion error", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.952328622341156}]}]}