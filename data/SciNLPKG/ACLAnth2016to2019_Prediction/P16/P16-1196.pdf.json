{"title": [{"text": "Continuous Profile Models in ASL Syntactic Facial Expression Synthesis", "labels": [], "entities": [{"text": "ASL Syntactic Facial Expression Synthesis", "start_pos": 29, "end_pos": 70, "type": "TASK", "confidence": 0.8370332241058349}]}], "abstractContent": [{"text": "To create accessible content for deaf users, we investigate automatically synthesizing animations of American Sign Language (ASL), including grammatically important facial expressions and head movements.", "labels": [], "entities": []}, {"text": "Based on recordings of humans performing various types of syntactic face and head movements (which include idiosyn-cratic variation), we evaluate the efficacy of Continuous Profile Models (CPMs) at identifying an essential \"latent trace\" of the performance, for use in producing ASL animations.", "labels": [], "entities": [{"text": "ASL animations", "start_pos": 279, "end_pos": 293, "type": "TASK", "confidence": 0.9693019390106201}]}, {"text": "A metric-based evaluation and a study with deaf users indicated that this approach was more effective than a prior method for producing animations.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "ASL is a low-resource language, and it does not have a writing system in common use.", "labels": [], "entities": [{"text": "ASL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8617869019508362}]}, {"text": "Therefore, ASL corpora are generally small in size and in limited supply; they are usually produced through manual annotation of video recordings.", "labels": [], "entities": []}, {"text": "Thus, researchers generally work with relatively small datasets.", "labels": [], "entities": []}, {"text": "In this work, we make use of two datasets that consist of video recordings of humans performing ASL with annotation labeling the times in the video when each of the five types of syntactic facial expressions listed in section 1.1 occur.", "labels": [], "entities": [{"text": "ASL", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9622088670730591}]}, {"text": "The training dataset used in this study was described in (, and consists of 199 examples of facial expressions performed by a female signer recorded at Boston University.", "labels": [], "entities": [{"text": "Boston University", "start_pos": 152, "end_pos": 169, "type": "DATASET", "confidence": 0.9550788998603821}]}, {"text": "While the Training dataset can naturally be partitioned into five subsets, based on each of the five syntactic facial expression types, because adjacent  facial expressions or phrase durations may affect the performance of ASL facial expressions, in this work, we sub-divide the dataset further, into ten sub-groups, as summarized in.", "labels": [], "entities": [{"text": "Training dataset", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8699003458023071}, {"text": "ASL facial expressions", "start_pos": 223, "end_pos": 245, "type": "TASK", "confidence": 0.832250714302063}]}, {"text": "The \"gold-standard\" dataset used in this study was shared with the research community by ; we use 10 examples of ASL facial expressions (one for each sub-group listed in) performed by a male signer who was recorded at the Linguistic and Assistive Technologies laboratory.", "labels": [], "entities": []}, {"text": "To extract face and head movement information from the video, a face-tracker was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 features used in this study: head x, heady, head z, head pitch, head yaw, head roll, raise l i brow, raiser i brow, raise l m brow, raiser m brow, raise lo brow, raiser o brow, squeeze l brow, squeezer brow.", "labels": [], "entities": []}, {"text": "The first six values represent head location and orientation.", "labels": [], "entities": []}, {"text": "The next six values represent vertical movement of the outer (\"o \"), middle (\"m \"), or inner (\"i \") portion of the right (\"r \") or left (\"l \") eyebrows.", "labels": [], "entities": []}, {"text": "The final values represent horizontal movement of the eyebrows.", "labels": [], "entities": []}, {"text": "This section presents two forms of evaluation of the CPM latent trace model for ASL facial expression synthesis.", "labels": [], "entities": [{"text": "ASL facial expression synthesis", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.9187809973955154}]}, {"text": "In Section 3.1, the CPM model will be compared to a \"gold-standard\" performance of each sub-category of ASL facial expression using a distance-metric-based evaluation, and in Section 3.2, the results of a user-study will be presented, in which ASL signers evaluated animations of ASL based upon the CPM model.", "labels": [], "entities": [{"text": "ASL facial expression", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.8086127638816833}]}, {"text": "To provide a basis of comparison, in this section, we evaluate the CPM approach in comparison to an alternative approach that we call 'Centroid', which we described in prior work in, where we used a multivariate DTW to select one of the time series in the training set as a representative performance of the facial expression.", "labels": [], "entities": []}, {"text": "The centroid examples are actual recordings of human ASL signers that are used to drive an animation.", "labels": [], "entities": []}, {"text": "Appendix A lists the codenames of the videos from the training dataset selected as centroids and the codenames of the videos used in the gold-standard dataset).", "labels": [], "entities": [{"text": "Appendix", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9536182880401611}]}, {"text": "The gold-standard recordings of a male ASL signer were described in Section 2.1.", "labels": [], "entities": [{"text": "ASL signer", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.8882925808429718}]}, {"text": "In addition to the video recordings (which were processed to extract face and head movement data), we have annotation of the timing of the facial expressions and the sequence of signs performed on the hands.", "labels": [], "entities": []}, {"text": "To compare the quality of our CPM model and that of the Centroid approach, we used each method to produce a candidate sequence of face and head movements for the sentence performed by the human in the gold-standard recording.", "labels": [], "entities": []}, {"text": "Thus, the extracted facial expressions from the human recording can serve as a gold standard for how the face and head should move.", "labels": [], "entities": []}, {"text": "In this section, we compare: (a) the distance of the CPM latent trace from the gold standard to (b) the distance of the centroid form the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 138, "end_pos": 151, "type": "DATASET", "confidence": 0.9216563701629639}]}, {"text": "It is notable that these gold-standard recordings were previously \"unseen\" during the creation of the CPM or Centroid models, that is, they were not used in the training data set during the creation of either model.", "labels": [], "entities": []}, {"text": "Since there was variability in the length of the latent trace, centroid, and gold-standard videos, fora fairer comparison, we first resampled these time series, using cubic interpolation, to match the duration (in milliseconds) of the gold-standard ASL sentence, and then we used multivariate DTW to estimate their distance, following the methodology of ( and.", "labels": [], "entities": []}, {"text": "In prior work, we had shown that a scoring algorithm based on DTW had moderate (yet significant) correlation with scores that participants assigned to ASL animation with facial expressions.", "labels": [], "entities": [{"text": "ASL animation", "start_pos": 151, "end_pos": 164, "type": "TASK", "confidence": 0.9069640636444092}]}, {"text": "shows an example of a DTW distance scoring between the gold standard and each of the latent trace and the centroid, for one face feature  (horizontal movement of the left eyebrow) during a Negative A facial expression.", "labels": [], "entities": [{"text": "DTW distance scoring", "start_pos": 22, "end_pos": 42, "type": "METRIC", "confidence": 0.7030807137489319}, {"text": "Negative A facial expression", "start_pos": 189, "end_pos": 217, "type": "TASK", "confidence": 0.7043880224227905}]}, {"text": "Given that the centroid and the training data for the latent trace are driven by recordings of a (female) signer and the gold standard is a different (male) signer, there are differences between these facial expressions due to idiosyncratic aspects of individual signers.", "labels": [], "entities": []}, {"text": "Thus the metric evaluation in this section is challenging because it is an inter-signer evaluation.", "labels": [], "entities": []}, {"text": "illustrates the overall calculated DTW distances, including a graph with the results broken down per subcategory of ASL facial expression.", "labels": [], "entities": [{"text": "DTW distances", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.5719743072986603}, {"text": "ASL facial expression", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.8477147817611694}]}, {"text": "The results indicate that the CPM latent trace is closer to the gold standard than the centroid is.", "labels": [], "entities": []}, {"text": "Note that the distance values are not zero since the latent trace and the centroid are being compared to a recording from a different signer on novel, previously unseen, ASL sentences.", "labels": [], "entities": []}, {"text": "The results in these graphs suggest that the latent trace model out-performed the centroid approach.", "labels": [], "entities": []}, {"text": "To further assess our ASL synthesis approach, we conducted a user study where ASL signers watched short animations of ASL sentences with identical hand movements but differing in their face, head, and torso movements.", "labels": [], "entities": [{"text": "ASL synthesis", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9771981239318848}]}, {"text": "There were three conditions in this between-subjects study: a) animations with a static neutral face throughout the animation (as a lower baseline), b) animations with facial expressions driven by the centroid human recording, and c) animations with facial expressions driven by the CPM latent trace based on multiple recordings of a human performing that type of facial expression.", "labels": [], "entities": []}, {"text": "illustrates screenshots of each stimulus type fora YesNo A facial expression.", "labels": [], "entities": []}, {"text": "The specific sentences used for this study were drawn from a standard test set of stimuli released to the research community by ) for evaluating animations of sign language with facial expressions.", "labels": [], "entities": []}, {"text": "All three types of stimuli (neutral, centroid and latent trace), shared identical animation-control scripts specifying the hand and arm movements; these scripts were hand-crafted by ASL signers in a pose-by-pose manner.", "labels": [], "entities": []}, {"text": "For the neutral animations, we did not specify any torso, head, nor face movements; rather, we left them in their neutral pose throughout the sentences.", "labels": [], "entities": []}, {"text": "As for the centroid and latent trace animations, we applied the head and face movements (as specified by the centroid model or by the latent trace model) only to the portion of the animation where the facial expression of interest occurs, leaving the head and face for the rest of the animation to a neutral pose.", "labels": [], "entities": []}, {"text": "For instance, during a stimulus that contains a Whquestion, the face and head are animated only during the Wh-question, but they are left in a neutral pose for the rest of the stimulus (which may include other sentences).", "labels": [], "entities": []}, {"text": "The period of time when the facial expression occurred was time-aligned with the subset of words (the sequence of signs performed on the hands) for the appropriate syntactic domain; the phrase-beginning and phraseending was aligned with the performance of the facial expression.", "labels": [], "entities": []}, {"text": "Thus, the difference in appearance between our animation stimuli was subtle: The only portion of the animations that differed between the three conditions (neutral, centroid, and latent-trace) was the face and the head movements during the span of time when the syntactic facial expression should occur (e.g., during the Wh-question).", "labels": [], "entities": []}, {"text": "We resampled the centroid and CPM time series, using cubic interpolation, to match the duration (in milliseconds) of the animation they would be applied to.", "labels": [], "entities": []}, {"text": "To convert the centroid and latent trace time series into the input for the animationgeneration system, we used the MPEG4-featuresto-animation pipeline described in (.", "labels": [], "entities": []}, {"text": "That platform is based upon the opensource EMBR animation system for producing human animation; specifically, the facial expressions were represented as an EMBR PoseSequence with a pose defined every 133 milliseconds.", "labels": [], "entities": []}, {"text": "In prior work, we investigated key methodological considerations in conducting a study to evaluate sign language animations with deaf users, including the use of appropriate baselines for comparison, appropriate presentation of questions and instructions, demographic and technology experience factors influencing acceptance of signing avatars, and other factors that we have considered in the design of this current study.", "labels": [], "entities": []}, {"text": "Our recent work () has established a set of demographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to evaluate animation.", "labels": [], "entities": [{"text": "ASL signers", "start_pos": 174, "end_pos": 185, "type": "TASK", "confidence": 0.839511513710022}]}, {"text": "Specifically, we screened for participants that identified themselves as \"deaf/Deaf\" or \"hard-of-hearing,\" who had grownup using ASL at home or had attended an ASL-based school as a young child, such as a residential or daytime school.", "labels": [], "entities": []}, {"text": "Deaf researchers (all fluent ASL signers) recruited and collected data from participants, during meetings conducted in ASL.", "labels": [], "entities": []}, {"text": "Initial advertise-ments were sent to local email distribution lists and Facebook groups.", "labels": [], "entities": []}, {"text": "A total of 17 participants met the above criteria, where 14 participants selfidentified as deaf/Deaf and 3 as hard-of-hearing.", "labels": [], "entities": []}, {"text": "Of our participants in the study, 10 had attended a residential school for deaf students, and 7, a daytime school for deaf students.", "labels": [], "entities": []}, {"text": "14 participants had learned ASL prior to age 5, and the remaining 3 had been using ASL for over 7 years.", "labels": [], "entities": [{"text": "ASL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9564328789710999}]}, {"text": "There were 8 men and 9 women of ages 19-29 (average age 22.8).", "labels": [], "entities": []}, {"text": "In prior work, we have advocated that participants in studies evaluating sign language animation complete a two standardized surveys about their technology experience (MediaSharing and AnimationAttitude) and that researchers report these values for participants, to enable comparison across studies.", "labels": [], "entities": []}, {"text": "In our study, participant scores for MediaSharing varied between 3 and 6, with a mean score of 4.3, and scores for AnimationAttitude varied from 2 to 6, with a mean score of 3.8.", "labels": [], "entities": []}, {"text": "At the beginning of the study, participants viewed a sample animation, to familiarize them with the experiment and the questions they would be asked about each animation.", "labels": [], "entities": []}, {"text": "(This sample used a different stimulus than the other ten animations shown in the study.)", "labels": [], "entities": []}, {"text": "Next, they responded to a set of questions that measured their subjective impression of each animation, using a 1-to-10 scalar response.", "labels": [], "entities": []}, {"text": "Each question was conveyed using ASL through an onscreen video, and the following English question text was shown on the questionnaire: (a) Good ASL grammar?", "labels": [], "entities": []}, {"text": "(10=Per-fect, 1=Bad); (b) Easy to understand?", "labels": [], "entities": []}, {"text": "(10=Clear, 1=Confusing); (c) Natural?", "labels": [], "entities": []}, {"text": "(10=Moves like person, 1=Like robot).", "labels": [], "entities": []}, {"text": "These questions have been used in many prior experimental studies to evaluate animations of ASL, e.g. (, and were shared with research community as a standard evaluation tool in).", "labels": [], "entities": [{"text": "ASL", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.97111976146698}]}, {"text": "To calculate a single score for each animation, the scalar response scores for the three questions were averaged.", "labels": [], "entities": []}, {"text": "shows distributions of subjective scores as boxplots with a 1.5 interquartile range (IQR).", "labels": [], "entities": [{"text": "interquartile range (IQR)", "start_pos": 64, "end_pos": 89, "type": "METRIC", "confidence": 0.889380669593811}]}, {"text": "For comparison, means are denoted with a star and their values are labeled above each boxplot.", "labels": [], "entities": []}, {"text": "When comparing the subjective scores that participants assigned to the animations in, we found a significant difference (KruskalWallis test used since the data was not normally distributed) between the latent trace and centroid (p < 0.005) and between the latent trace and neutral (p < 0.05).", "labels": [], "entities": []}, {"text": "In summary, our CPM modeling approach for generating an animation out-performed an animation produced from an actual recording of a single human performance (the \"centroid\" approach).", "labels": [], "entities": [{"text": "CPM modeling", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9446647465229034}]}, {"text": "In prior methodological studies, we demonstrated that it is valid to use either videos of humans or animations (driven by a human performance) as the baseline for comparison in a study of ASL animation (.", "labels": [], "entities": [{"text": "ASL animation", "start_pos": 188, "end_pos": 201, "type": "TASK", "confidence": 0.9646121561527252}]}, {"text": "As suggested by, the differences in face and head movements between the Centroid and CPM conditions were subtle, yet fluent ASL signers rated the CPM animations higher in this study.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. For each dataset, all the train- ing examples are stretched (resampled using cubic  interpolation) to meet the length of the longest ex- ample in the set. The length of time series, N ,  corresponds to the duration in video frames of the  longest example in the data set. The recordings in  the training set have 14 dimensions, corresponding  to the 14 facial features listed in Section 2.1. As  discussed above, the latent trace has a time axis of  length M , which is approximately double the tem- poral resolution of the original training examples.", "labels": [], "entities": [{"text": "tem- poral resolution", "start_pos": 505, "end_pos": 526, "type": "METRIC", "confidence": 0.7695475518703461}]}, {"text": " Table 2: Training data and the obtained latent  traces for each of the CPM models on ASL facial  expression subcategories.", "labels": [], "entities": []}, {"text": " Table 3: Codenames of videos selected as centoids  and gold standards for comparison in section 3.1.", "labels": [], "entities": []}]}