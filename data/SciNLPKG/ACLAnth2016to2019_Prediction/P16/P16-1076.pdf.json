{"title": [{"text": "CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases", "labels": [], "entities": [{"text": "CFO", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8400660157203674}, {"text": "Conditional Focused Neural Question Answering", "start_pos": 5, "end_pos": 50, "type": "TASK", "confidence": 0.8249157190322876}]}], "abstractContent": [{"text": "How can we enable computers to automatically answer questions like \"Who created the character Harry Potter\"?", "labels": [], "entities": [{"text": "Who created the character Harry Potter\"", "start_pos": 68, "end_pos": 107, "type": "TASK", "confidence": 0.650848022529057}]}, {"text": "Carefully built knowledge bases provide rich sources of facts.", "labels": [], "entities": []}, {"text": "However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question.", "labels": [], "entities": []}, {"text": "In particular, we focus on the most common questions-ones that can be answered with a single fact in the knowledge base.", "labels": [], "entities": []}, {"text": "We propose CFO, a Conditional Focused neural-network-based approach to answering fac-toid questions with knowledge bases.", "labels": [], "entities": [{"text": "CFO", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.6801548600196838}]}, {"text": "Our approach first zooms in a question to find more probable candidate subject mentions , and infers the final answers with a unified conditional probabilistic framework.", "labels": [], "entities": []}, {"text": "Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions-the largest public one to date.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9993370175361633}]}, {"text": "It outperforms the current state of the art by an absolute margin of 11.8%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Community-driven question answering (QA) websites such as Quora, Yahoo-Answers, and Answers.com are accumulating millions of users and hundreds of millions of questions.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8353452205657959}, {"text": "Quora", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9355359077453613}]}, {"text": "A large portion of the questions are about facts or trivia.", "labels": [], "entities": []}, {"text": "It has been along pursuit to enable machines to answer such questions automatically.", "labels": [], "entities": []}, {"text": "In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions.", "labels": [], "entities": []}, {"text": "A knowledge * Part of the work was done while at base (KB) consists of structured representation of facts in the form of subject-relation-object triples.", "labels": [], "entities": []}, {"text": "Lately, several large-scale generalpurpose KBs have been constructed, including, Freebase), NELL, and DBpedia ().", "labels": [], "entities": [{"text": "Freebase", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9654169082641602}, {"text": "DBpedia", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.9336005449295044}]}, {"text": "Typically, structured queries with predefined semantics (e.g. SPARQL) can be issued to retrieve specified facts from such KBs.", "labels": [], "entities": []}, {"text": "Thus, answering factoid questions will be straightforward once they are converted into the corresponding structured form.", "labels": [], "entities": [{"text": "answering factoid questions", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.827203114827474}]}, {"text": "However, due to complexity of language, converting natural language questions to structure forms remains an open challenge.", "labels": [], "entities": []}, {"text": "Among all sorts of questions, there is one category that only requires a single fact (triple) in KB as the supporting evidence.", "labels": [], "entities": []}, {"text": "As atypical example, the question \"Who created the character Harry Potter\" can be answered with the single fact.", "labels": [], "entities": []}, {"text": "In this work, we refer to such questions as single-fact questions.", "labels": [], "entities": []}, {"text": "Previously, it has been observed that single-fact questions constitute the majority of factoid questions in community QA sites.", "labels": [], "entities": []}, {"text": "Despite the simplicity, automatically answering such questions remains far from solved -the latest best result on a dataset of 108k single-fact questions is only 63.9% in terms of accuracy ( . To find the answer to a single-fact question, it suffices to identify the subject entity and relation (implicitly) mentioned by the question, and then forms a corresponding structured query.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9992651343345642}]}, {"text": "The problem can be formulated into a probabilistic form.", "labels": [], "entities": []}, {"text": "Given a single-fact question q, finding the subjectrelation pair\u02c6spair\u02c6 pair\u02c6s, \u02c6 r from the KB K which maximizes the conditional probability p(s, r|q), i.e. Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q).", "labels": [], "entities": []}, {"text": "It is very challenging because of a) the vast amount of facts -a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language -there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision -most combinations of s, r, q are not expressed in training data.", "labels": [], "entities": []}, {"text": "Faced with these challenges, existing methods have exploited to incorporate prior knowledge into semantic parsers, to design models and representations with better generalization property, to utilize large-margin ranking objective to estimate the model parameters, and to prune the search space during inference.", "labels": [], "entities": []}, {"text": "Noticeably, models based on neural networks and distributed representations have largely contributed to the recent progress (see section 2).", "labels": [], "entities": []}, {"text": "In this paper, we propose CFO, a novel method to answer single-fact questions with large-scale knowledge bases.", "labels": [], "entities": [{"text": "CFO", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.6289500594139099}]}, {"text": "The contributions of this paper are, \u2022 we employ a fully probabilistic treatment of the problem with a novel conditional parameterization using neural networks, \u2022 we propose the focused pruning method to reduce the search space during inference, and \u2022 we investigate two variations to improve the generalization of representations for millions of entities under highly sparse supervision.", "labels": [], "entities": []}, {"text": "In experiments, CFO achieves 75.7% in terms of top-1 accuracy on the largest dataset to date, outperforming the current best record by an absolute margin of 11.8%.", "labels": [], "entities": [{"text": "CFO", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.600034236907959}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9874370098114014}]}], "datasetContent": [{"text": "In this section, we conduct experiments to evaluate the proposed system empirically.", "labels": [], "entities": []}, {"text": "We train and evaluate our method on the SIMPLE-QUESTIONS dataset.", "labels": [], "entities": [{"text": "SIMPLE-QUESTIONS dataset", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.742507204413414}]}, {"text": "However, these datasets are quite restricted in sample size -the former includes 5,810 samples (train + test) and the latter includes 917 ones.", "labels": [], "entities": []}, {"text": "They are fewer than the number of relations in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9797634482383728}]}, {"text": "To train the focused labeling model, the information about whether a word is part of the subject mention is needed.", "labels": [], "entities": []}, {"text": "We obtain such information by reverse linking from the ground-truth subject to its mention in the question.", "labels": [], "entities": []}, {"text": "Given a question q corresponding to subject s, we match the name and aliases of s to all n-grams that can be generated from q.", "labels": [], "entities": []}, {"text": "Once a match is found, we label the matched n-gram as the subject mention.", "labels": [], "entities": []}, {"text": "In the case of multiple matches, only the longest matched n-gram is used as the correct one.", "labels": [], "entities": []}, {"text": "For evaluation, we consider the same metric introduced in , which takes the prediction as correct if both the subject and relation are correctly retrieved.", "labels": [], "entities": []}, {"text": "Based on this metric, we compare CFO with a few baseline systems, which include both the Memory Network QA system ( , and systems with alternative components and parameterizations from existing work ().", "labels": [], "entities": [{"text": "CFO", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7943593859672546}]}, {"text": "We did not compare with alternative subject networks because the only existing method () relies on unique textual name of each entity, which does not generally hold in knowledge bases (except in REVERB).", "labels": [], "entities": [{"text": "REVERB", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.5896878242492676}]}, {"text": "Alternative approaches for pruning method, relation network, and entity representation are described below.", "labels": [], "entities": [{"text": "entity representation", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.7429516613483429}]}, {"text": "Pruning methods We consider two baseline methods previously used to prune the search space.", "labels": [], "entities": []}, {"text": "The first baseline is the N-Gram pruning method introduced in Section 3, as it has been successfully used in previous work.", "labels": [], "entities": []}, {"text": "Basically, it establishes the candidate pool by retaining subject-relation pairs whose subject can be linked to one of the n-grams generated from the question.", "labels": [], "entities": []}, {"text": "The second one is NGram+, a revised version of the N-Gram pruning with additional heuristics ( . Instead of considering all n-grams that can be linked to entities in KB, heuristics related to overlapping n-grams, stop words, interrogative pronouns, and soon are exploited to further shrink the n-gram pool.", "labels": [], "entities": []}, {"text": "Accordingly, the search space is restricted to subject-relation pairs whose subject can be linked to one of the remaining n-grams after applying the heuristic filtering.", "labels": [], "entities": []}, {"text": "Relation scoring network We compare our proposed method with two previously used models.", "labels": [], "entities": [{"text": "Relation scoring", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.73316490650177}]}, {"text": "The first baseline is the embedding average model (Embed-AVG) used in (.", "labels": [], "entities": []}, {"text": "Basically, it takes the element-wise average of the word embeddings of the question to be the question representation.", "labels": [], "entities": []}, {"text": "The second one is the letter-tri-gram CNN (LTG-CNN) used in (, where the question and relation are separately embedded into the vector space by two parameter shared LTG-CNNs.", "labels": [], "entities": []}, {"text": "In addition, () observed better performance of the LTG-CNN when substituting the subject mention with a special symbol.", "labels": [], "entities": [{"text": "LTG-CNN", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.909590482711792}]}, {"text": "Naturally, this can be combined with the proposed focused labeling, since the latter is able to identify the potential subject mention in the question.", "labels": [], "entities": []}, {"text": "So, we train another LTG-CNN with symbolized questions, which is denoted as LTG-CNN+.", "labels": [], "entities": []}, {"text": "Note that this model is only tested when the focused labeling pruning is used.", "labels": [], "entities": []}, {"text": "Entity representation In section 4.2, we describe two possible ways to improve the vector representation of the subject, TransE pretrained embedding and type vectors.", "labels": [], "entities": []}, {"text": "To evaluate their effectiveness, we also include this variation in the experiment, and compare their performance with randomly initialized entity embeddings.", "labels": [], "entities": []}, {"text": "During training, all word embeddings are initialized using the pretrained GloVe (, and then fine tuned in subsequent training.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.8582750558853149}]}, {"text": "The word embedding dimension is set to 300, and the BiGRU hidden size 256.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.6970784068107605}]}, {"text": "For pretraining the entity embeddings using TransE (see section 4.2), only triples included in FB5M are used.", "labels": [], "entities": [{"text": "FB5M", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9747308492660522}]}, {"text": "All other parameters are randomly initialized uniformly from [\u22120.08, 0.08], following.", "labels": [], "entities": []}, {"text": "Both hinge loss margins \u03b3 sand \u03b3 rare set to 0.1.", "labels": [], "entities": []}, {"text": "Negative sampling sizes Ms and Mr are both 1024.", "labels": [], "entities": []}, {"text": "For optimization, parameters are trained using mini-batch AdaGrad) with Momentum ( tuned to be 0.001 for question embedding with type vector, 0.03 for LTG-CNN methods, and 0.02 for rest of the models.", "labels": [], "entities": [{"text": "Momentum", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9187295436859131}]}, {"text": "Momentum rate is set to 0.9 for all models, and the mini-batch size is 256.", "labels": [], "entities": [{"text": "Momentum rate", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9764634072780609}]}, {"text": "In addition, vertical dropout () is used to regularize all BiGRUs in our experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on SIMPLEQUESTIONS testing set.  *  indi- cates using ensembles. N-Gram+ uses additional heuristics.  The proposed CFO (focused pruning + BiGRU + type vector)  achieves the top accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9734262824058533}]}, {"text": " Table 2: Comparison of different space pruning methods. N-Gram+ uses additional heuristics. Single-and multi-subject refers  to the number of distinct subjects in candidates. The proposed focused pruning achieves best scores.", "labels": [], "entities": []}, {"text": " Table 3: System performance with different subject network  structures.", "labels": [], "entities": []}]}