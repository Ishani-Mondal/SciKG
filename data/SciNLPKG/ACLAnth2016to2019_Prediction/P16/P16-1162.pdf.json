{"title": [{"text": "Neural Machine Translation of Rare Words with Subword Units", "labels": [], "entities": [{"text": "Neural Machine Translation of Rare Words", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8079658349355062}]}], "abstractContent": [{"text": "Neural machine translation (NMT) models typically operate with a fixed vocabulary , but translation is an open-vocabulary problem.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7750129550695419}, {"text": "translation", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.9715740084648132}]}, {"text": "Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary.", "labels": [], "entities": [{"text": "translation of out-of-vocabulary words", "start_pos": 28, "end_pos": 66, "type": "TASK", "confidence": 0.8704918175935745}]}, {"text": "In this paper , we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units.", "labels": [], "entities": [{"text": "open-vocabulary translation", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6968874037265778}]}, {"text": "This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via com-positional translation), and cognates and loanwords (via phonological and morphological transformations).", "labels": [], "entities": []}, {"text": "We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English\u2192German and English\u2192Russian by up to 1.1 and 1.3 BLEU, respectively.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7161516547203064}, {"text": "WMT 15 translation tasks English\u2192German", "start_pos": 279, "end_pos": 318, "type": "TASK", "confidence": 0.795295638697488}, {"text": "BLEU", "start_pos": 360, "end_pos": 364, "type": "METRIC", "confidence": 0.9992009997367859}]}], "introductionContent": [{"text": "Neural machine translation has recently shown impressive results.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8593713641166687}]}, {"text": "However, the translation of rare words is an open problem.", "labels": [], "entities": [{"text": "translation of rare words", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.9048763513565063}]}, {"text": "The vocabulary of neural models is typically limited to 30 000-50 000 words, but translation is an open-vocabulary prob- The research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp. z o.o.", "labels": [], "entities": []}, {"text": "-Samsung R&D Institute Poland.", "labels": [], "entities": [{"text": "Samsung R&D Institute Poland", "start_pos": 1, "end_pos": 29, "type": "DATASET", "confidence": 0.8915682733058929}]}, {"text": "lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level.", "labels": [], "entities": []}, {"text": "As an example, consider compounds such as the German Abwasser|behandlungs|anlange 'sewage water treatment plant', for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector.", "labels": [], "entities": []}, {"text": "For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up ().", "labels": [], "entities": []}, {"text": "We note that such techniques make assumptions that often do not hold true in practice.", "labels": [], "entities": []}, {"text": "For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example.", "labels": [], "entities": []}, {"text": "Also, word-level models are unable to translate or generate unseen words.", "labels": [], "entities": []}, {"text": "Copying unknown words into the target text, as done by, is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabets differ.", "labels": [], "entities": []}, {"text": "We investigate NMT models that operate on the level of subword units.", "labels": [], "entities": []}, {"text": "Our main goal is to model open-vocabulary translation in the NMT network itself, without requiring a back-off model for rare words.", "labels": [], "entities": [{"text": "open-vocabulary translation", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7368822693824768}, {"text": "NMT network", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.869836300611496}]}, {"text": "In addition to making the translation process simpler, we also find that the subword models achieve better accuracy for the translation of rare words than large-vocabulary models and back-off dictionaries, and are able to productively generate new words that were not seen at training time.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.968525767326355}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9982072114944458}]}, {"text": "Our analysis shows that the neural networks are able to learn compounding and transliteration from subword representations.", "labels": [], "entities": []}, {"text": "This paper has two main contributions: \u2022 We show that open-vocabulary neural ma-chine translation is possible by encoding (rare) words via subword units.", "labels": [], "entities": [{"text": "open-vocabulary neural ma-chine translation", "start_pos": 54, "end_pos": 97, "type": "TASK", "confidence": 0.6343288347125053}]}, {"text": "We find our architecture simpler and more effective than using large vocabularies and back-off dictionaries ().", "labels": [], "entities": []}, {"text": "\u2022 We adapt byte pair encoding (BPE), a compression algorithm, to the task of word segmentation.", "labels": [], "entities": [{"text": "byte pair encoding (BPE)", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.7392817536989847}, {"text": "word segmentation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7400405704975128}]}, {"text": "BPE allows for the representation of an open vocabulary through a fixed-size vocabulary of variable-length character sequences, making it a very suitable word segmentation strategy for neural network models.", "labels": [], "entities": [{"text": "BPE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6448891758918762}, {"text": "word segmentation", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.740486353635788}]}], "datasetContent": [{"text": "We aim to answer the following empirical questions: \u2022 Can we improve the translation of rare and unseen words in neural machine translation by representing them via subword units?", "labels": [], "entities": [{"text": "translation of rare and unseen words in neural machine translation", "start_pos": 73, "end_pos": 139, "type": "TASK", "confidence": 0.74117870926857}]}, {"text": "\u2022 Which segmentation into subword units performs best in terms of vocabulary size, text size, and translation quality?", "labels": [], "entities": []}, {"text": "We perform experiments on data from the shared translation task of WMT 2015.", "labels": [], "entities": [{"text": "shared translation task of WMT 2015", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.6542701721191406}]}, {"text": "For English\u2192German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens.", "labels": [], "entities": []}, {"text": "For English\u2192Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens.", "labels": [], "entities": []}, {"text": "We tokenize and truecase the data with the scripts provided in Moses ().", "labels": [], "entities": []}, {"text": "We use newstest2013 as development set, and report results on newstest2014 and newstest2015.", "labels": [], "entities": []}, {"text": "We report results with BLEU (mteval-v13a.pl), and CHRF3), a character n-gram F 3 score which was found to correlate well with human judgments, especially for translations out of English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9987321496009827}, {"text": "CHRF3", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.7466795444488525}, {"text": "character n-gram F 3 score", "start_pos": 60, "end_pos": 86, "type": "METRIC", "confidence": 0.6882995128631592}]}, {"text": "Since our main claim is concerned with the translation of rare and unseen words, we report separate statistics for these.", "labels": [], "entities": [{"text": "translation of rare and unseen words", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.8993042906125387}]}, {"text": "We measure these through unigram F 1 , which we calculate as the harmonic mean of clipped unigram precision and recall.", "labels": [], "entities": [{"text": "unigram F 1", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.7889565229415894}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.7262505888938904}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9982320666313171}]}, {"text": "We perform all experiments with Groundhog 7 (.", "labels": [], "entities": []}, {"text": "We generally follow settings by previous work (.", "labels": [], "entities": []}, {"text": "All networks have a hidden layer size of 1000, and an embedding layer size of 620.", "labels": [], "entities": []}, {"text": "Following, we only keep a shortlist of \u03c4 = 30000 words in memory.", "labels": [], "entities": []}, {"text": "During training, we use Adadelta (Zeiler, 2012), a minibatch size of 80, and reshuffle the training set between epochs.", "labels": [], "entities": [{"text": "Adadelta (Zeiler, 2012)", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.48938724398612976}]}, {"text": "We train a network for approximately 7 days, then take the last 4 saved models (models being saved every 12 hours), and continue training each with a fixed embedding layer (as suggested by) for 12 hours.", "labels": [], "entities": []}, {"text": "We perform two independent training runs for each models, once with cut-off for gradient clipping () of 5.0, once with a cut-off of 1.0 -the latter produced better single models for most settings.", "labels": [], "entities": []}, {"text": "We report results of the system that performed best on our development set (newstest2013), and of an ensemble of all 8 models.", "labels": [], "entities": [{"text": "newstest2013", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.877502977848053}]}, {"text": "We use abeam size of 12 for beam search, with probabilities normalized by sentence length.", "labels": [], "entities": [{"text": "beam search", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9271993339061737}]}, {"text": "We use a bilingual dictionary based on fast-align ().", "labels": [], "entities": []}, {"text": "For our baseline, this serves as back-off dictionary for rare words.", "labels": [], "entities": []}, {"text": "We also use the dictionary to speedup translation for all experiments, only performing the softmax over a filtered list of candidate translations (like, we use K = 30000; K = 10).", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9579954743385315}]}, {"text": "English\u2192German translation results are shown in; English\u2192Russian results in.", "labels": [], "entities": []}, {"text": "Our baseline WDict is a word-level model with a back-off dictionary.", "labels": [], "entities": []}, {"text": "It differs from WUnk in that the latter uses no back-off dictionary, and just represents out-of-vocabulary words as UNK . The back-off dictionary improves unigram F 1 for rare and unseen words, although the improvement is smaller for English\u2192Russian, since the back-off dictionary is incapable of transliterating names.", "labels": [], "entities": [{"text": "WUnk", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.904329776763916}, {"text": "UNK", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.8947080969810486}, {"text": "unigram F 1", "start_pos": 155, "end_pos": 166, "type": "METRIC", "confidence": 0.7554752031962076}]}, {"text": "All subword systems operate without a back-off dictionary.", "labels": [], "entities": []}, {"text": "We first focus on unigram F 1 , where all systems improve over the baseline, especially for rare words (36.8%\u219241.8% for EN\u2192DE; 26.5%\u219229.7% for EN\u2192RU).", "labels": [], "entities": [{"text": "unigram F 1", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.6558713316917419}]}, {"text": "For OOVs, the baseline strategy of copying unknown words works well for English\u2192German.", "labels": [], "entities": [{"text": "copying unknown words", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8503785133361816}]}, {"text": "However, when alphabets differ, like in English\u2192Russian, the subword models do much better.", "labels": [], "entities": []}, {"text": "Unigram F 1 scores indicate that learning the BPE symbols on the vocabulary union (BPEJ90k) is more effective than learning them separately (BPE-60k), and more effective than using character bigrams with a shortlist of 50 000 unsegmented words (C2-50k), but all reported subword segmentations are viable choices and outperform the back-off dictionary baseline.", "labels": [], "entities": [{"text": "Unigram F 1", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.5621051987012228}, {"text": "BPEJ90k", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.5877038240432739}]}, {"text": "Our subword representations cause big improvements in the translation of rare and unseen words, but these only constitute 9-11% of the test sets.", "labels": [], "entities": [{"text": "translation of rare and unseen words", "start_pos": 58, "end_pos": 94, "type": "TASK", "confidence": 0.8498419125874838}]}, {"text": "Since rare words tend to carry central information in a sentence, we suspect that BLEU and CHRF3 underestimate their effect on translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9983025789260864}]}, {"text": "Still, we also see improvements over the baseline in total unigram F 1 , as well as BLEU and CHRF3, and the subword ensembles outperform the WDict baseline by 0.3-1.3 BLEU and 0.6-2 CHRF3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9989379048347473}, {"text": "WDict baseline", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.8663062453269958}, {"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.997569739818573}]}, {"text": "There is some inconsistency between BLEU and CHRF3, which we attribute to the fact that BLEU has a precision bias, and CHRF3 a recall bias.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9964189529418945}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9964178800582886}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9989770650863647}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9986178874969482}]}, {"text": "For English\u2192German, we observe the best BLEU score of 25.3 with C2-50k, but the best CHRF3 score of 54.1 with BPE-J90k.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9805797338485718}, {"text": "CHRF3 score", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9587181806564331}, {"text": "BPE-J90k", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.6618563532829285}]}, {"text": "For comparison to the (to our knowledge) best non-neural MT system on this data set, we report syntaxbased SMT results (.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9575004577636719}, {"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.853779673576355}]}, {"text": "We observe that our best systems outperform the syntax-based system in terms of BLEU, but not in terms of CHRF3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9976376295089722}, {"text": "CHRF3", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.9287383556365967}]}, {"text": "Regarding other neural systems, report a BLEU score of 25.9 on newstest2015, but we note that they use an ensemble of 8 independently trained models, and also report strong improvements from applying dropout, which we did not use.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9996920824050903}, {"text": "newstest2015", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9830207824707031}]}, {"text": "We are confident that our improvements to the translation of rare words are orthogonal to improvements achievable through other improvements in the network archi-tecture, training algorithm, or better ensembles.", "labels": [], "entities": [{"text": "translation of rare words", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8775544911623001}]}, {"text": "For English\u2192Russian, the state of the art is the phrase-based system by . It outperforms our WDict baseline by 1.5 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9985388517379761}]}, {"text": "The subword models area step towards closing this gap, and BPE-J90k yields an improvement of 1.3 BLEU, and 2.0 CHRF3, over WDict.", "labels": [], "entities": [{"text": "BPE-J90k", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.6985164284706116}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9993021488189697}, {"text": "CHRF3", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.976504385471344}, {"text": "WDict", "start_pos": 123, "end_pos": 128, "type": "DATASET", "confidence": 0.8885271549224854}]}, {"text": "As a further comment on our translation results, we want to emphasize that performance variability is still an open problem with NMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9581546187400818}]}, {"text": "On our development set, we observe differences of up to 1 BLEU between different models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9986321330070496}]}, {"text": "For single systems, we report the results of the model that performs best on dev (out of 8), which has a stabilizing effect, but how to control for randomness deserves further attention in future research.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: English\u2192German translation performance (BLEU, CHRF3 and unigram F 1 ) on newstest2015.  Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F 1 (with ensembles) is computed for  all words (n = 44085), rare words (not among top 50 000 in training set; n = 2900), and OOVs (not in  training set; n = 1168).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9944287538528442}, {"text": "newstest2015", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.976520836353302}, {"text": "OOVs", "start_pos": 286, "end_pos": 290, "type": "METRIC", "confidence": 0.8755648136138916}]}, {"text": " Table 1: Corpus statistics for German training  corpus with different word segmentation tech- niques. #UNK: number of unknown tokens in  newstest2013. : (Koehn and Knight, 2003); *:  (Creutz and Lagus, 2002); : (Liang, 1983).", "labels": [], "entities": [{"text": "UNK", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.5948238372802734}]}, {"text": " Table 3: English\u2192Russian translation performance (BLEU, CHRF3 and unigram F 1 ) on newstest2015.  Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F 1 (with ensembles", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9966710209846497}, {"text": "newstest2015", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9715443253517151}]}]}