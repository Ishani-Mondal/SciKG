{"title": [], "abstractContent": [{"text": "We enrich a curated resource of common-sense knowledge by formulating the problem as one of knowledge base completion (KBC).", "labels": [], "entities": []}, {"text": "Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set.", "labels": [], "entities": []}, {"text": "However , the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases.", "labels": [], "entities": []}, {"text": "We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones.", "labels": [], "entities": []}, {"text": "We find strong performance from a bilinear model using a simple additive architecture to model phrases.", "labels": [], "entities": []}, {"text": "We manually evaluate our trained model's ability to assign quality scores to novel tuples, finding that it can propose tu-ples at the same quality level as medium-confidence tuples from ConceptNet.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many ambiguities in natural language processing (NLP) can be resolved by using knowledge of various forms.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.7834303180376688}]}, {"text": "Our focus is on the type of knowledge that is often referred to as \"commonsense\" or \"background\" knowledge.", "labels": [], "entities": []}, {"text": "This knowledge is rarely expressed explicitly in textual corpora ().", "labels": [], "entities": []}, {"text": "Some researchers have developed techniques for inferring this knowledge from patterns in raw text), while others have developed curated resources of commonsense knowledge via manual annotation ( or games with a purpose).", "labels": [], "entities": []}, {"text": "Curated resources typically have high precision but suffer from alack of coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9986451268196106}]}, {"text": "For cer-: ConceptNet tuples with left term \"soak in hotspring\"; final column is confidence score.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 80, "end_pos": 96, "type": "METRIC", "confidence": 0.948334276676178}]}, {"text": "tain resources, researchers have developed methods to automatically increase coverage by inferring missing entries.", "labels": [], "entities": []}, {"text": "These methods are commonly categorized under the heading of knowledge base completion (KBC).", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.7378459870815277}]}, {"text": "KBC is widelystudied for knowledge bases like Freebase () which contain large sets of entities and relations among them (), including recent work using neural networks ().", "labels": [], "entities": [{"text": "Freebase", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9676900506019592}]}, {"text": "We improve the coverage of commonsense resources by formulating the problem as one of knowledge base completion.", "labels": [], "entities": []}, {"text": "We focus on a particular curated commonsense resource called ConceptNet.", "labels": [], "entities": []}, {"text": "ConceptNet contains tuples consisting of a left term, a relation, and aright term.", "labels": [], "entities": []}, {"text": "The relations come from a fixed set.", "labels": [], "entities": []}, {"text": "While terms in Freebase tuples are entities, ConceptNet terms can be arbitrary phrases.", "labels": [], "entities": []}, {"text": "Some examples are shown in.", "labels": [], "entities": []}, {"text": "An NLP application may wish to query ConceptNet for information about soaking in a hotspring, but may use different words from those contained in the ConceptNet tuples.", "labels": [], "entities": []}, {"text": "Our goal is to do on-the-fly knowledge base completion so that queries can be answered robustly without requiring the precise linguistic forms contained in ConceptNet.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7446406284968058}]}, {"text": "To do this, we develop neural network models to embed terms and provide scores to arbi-trary tuples.", "labels": [], "entities": []}, {"text": "We train them on ConceptNet tuples and evaluate them by their ability to distinguish true and false held-out tuples.", "labels": [], "entities": [{"text": "ConceptNet tuples", "start_pos": 17, "end_pos": 34, "type": "DATASET", "confidence": 0.9222601652145386}]}, {"text": "We consider several functional architectures, comparing two composition functions for embedding terms and two functions for converting term embeddings into tuple scores.", "labels": [], "entities": []}, {"text": "We find that all architectures are able to outperform several baselines and reach similar performance on classifying held-out tuples.", "labels": [], "entities": []}, {"text": "We also experiment with several training objectives for KBC, finding that a simple cross entropy objective with randomly-generated negative examples performs best while also being fastest.", "labels": [], "entities": []}, {"text": "We manually evaluate our trained model's ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as medium-confidence tuples from ConceptNet.", "labels": [], "entities": []}, {"text": "We release all of our resources, including our ConceptNet KBC task data, large sets of randomly-generated tuples scored with our model, training code, and pretrained models with code for calculating the confidence of novel tuples.", "labels": [], "entities": [{"text": "ConceptNet KBC task data", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.8362978994846344}]}], "datasetContent": [{"text": "We now evaluate our tuple models.", "labels": [], "entities": []}, {"text": "We measure whether our models can distinguish true and false tuples by training a model on a large set of tuples and testing on a held-out set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies (%) on DEV2 of two base- lines using three different sets of word embed- dings. Our ConceptNet-trained embeddings out- perform GloVe and PARAGRAM embeddings.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9808885455131531}, {"text": "DEV2", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.891465961933136}]}, {"text": " Table 4: Loss function runtime comparison (sec- onds per epoch) of the DNN models.", "labels": [], "entities": []}, {"text": " Table 5: Accuracies (%) of baselines and final  model configurations on DEV2 and TEST. \"+ data\"  uses enlarged training set of size 300,000, and then  doubles this training set by including tuples with  conjugated forms; see text for details. Human per- formance on DEV2 was estimated from a sample  of size 100.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9927404522895813}, {"text": "DEV2", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.920626699924469}, {"text": "TEST", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.7071847915649414}, {"text": "DEV2", "start_pos": 267, "end_pos": 271, "type": "DATASET", "confidence": 0.9116547107696533}]}, {"text": " Table 3: Accuracies (%) on DEV2 of models trained with two loss functions (cross entropy (CE) and  hinge) and three sampling strategies (random, mix, and max). The best accuracy for each model is shown  in bold. Cross entropy with random sampling is best across models and is also fastest (see", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9982659220695496}, {"text": "DEV2", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.9466866850852966}, {"text": "cross entropy (CE)", "start_pos": 76, "end_pos": 94, "type": "METRIC", "confidence": 0.6430538356304168}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9981616139411926}]}, {"text": " Table 6: Top Wikipedia tuples for 3 relations with  t 1 = bus, scored by Bilinear AVG model.", "labels": [], "entities": []}, {"text": " Table 7: Average quality scores from manual eval- uation of novel tuples. Each row corresponds to a  different set of tuples. See text for details.", "labels": [], "entities": []}]}