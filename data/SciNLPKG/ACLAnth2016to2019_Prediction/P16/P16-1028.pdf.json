{"title": [{"text": "Two Discourse Driven Language Models for Semantics", "labels": [], "entities": [{"text": "Semantics", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.5826942324638367}]}], "abstractContent": [{"text": "Natural language understanding often requires deep semantic knowledge.", "labels": [], "entities": [{"text": "Natural language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7278860608736674}]}, {"text": "Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction.", "labels": [], "entities": []}, {"text": "We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities.", "labels": [], "entities": []}, {"text": "For each model, we investigate four implementations: a \"stan-dard\" N-gram language model and three discriminatively trained \"neural\" language models that generate embeddings for semantic frames.", "labels": [], "entities": []}, {"text": "The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically-we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 250, "end_pos": 273, "type": "TASK", "confidence": 0.707725778222084}, {"text": "discourse parsing", "start_pos": 278, "end_pos": 295, "type": "TASK", "confidence": 0.6962282210588455}]}], "introductionContent": [{"text": "Natural language understanding often necessitates deep semantic knowledge.", "labels": [], "entities": [{"text": "Natural language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7036717136700948}]}, {"text": "This knowledge needs to be captured at multiple levels, from words to phrases, to sentences, to larger units of discourse.", "labels": [], "entities": []}, {"text": "At each level, capturing meaning frequently requires context sensitive abstraction and disambiguation, as shown in the following example: Ex.1 was robbed by. was arrested by the police.", "labels": [], "entities": []}, {"text": "Ex.2 was robbed by. was rescued by the police.", "labels": [], "entities": []}, {"text": "In both cases, one needs to resolve the pronoun \"he\" to either \"Robert\" or \"Kevin\".", "labels": [], "entities": []}, {"text": "To make the correct decisions, one needs to know that the subject of \"rob\" is more likely than the object of \"rob\" to be the object of \"arrest\" while the object of \"rob\" is more likely to be the object of \"rescue\".", "labels": [], "entities": []}, {"text": "Thus, beyond understanding individual predicates (e.g., at the semantic role labeling level), there is a need to place them and their arguments in a global context.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7013447880744934}]}, {"text": "However, just modeling semantic frames is not sufficient; consider a variation of Ex.1: Ex.3 Kevin was robbed by Robert, but the police mistakenly arrested him.", "labels": [], "entities": []}, {"text": "In this case, \"him\" should refer to \"Kevin\" as the discourse marker \"but\" reverses the meaning, illustrating that it is necessary to take discourse markers into account when modeling semantics.", "labels": [], "entities": []}, {"text": "In this paper we propose that these aspects of semantic knowledge can be modeled as a Semantic Language Model (SemLM).", "labels": [], "entities": []}, {"text": "Just like the \"standard\" syntactic language models (LM), we define a basic vocabulary, a finite representation language, and a prediction task, which allows us to model the distribution over the occurrence of elements in the vocabulary as a function of their (well-defined) context.", "labels": [], "entities": []}, {"text": "In difference from syntactic LMs, we represent natural language at a higher level of semantic abstraction, thus facilitating modeling deep semantic knowledge.", "labels": [], "entities": []}, {"text": "We propose two distinct discourse driven language models to capture semantics.", "labels": [], "entities": []}, {"text": "In our first semantic language model, the Frame-Chain SemLM, we model all semantic frames and discourse markers in the text.", "labels": [], "entities": []}, {"text": "Each document is viewed as a single chain of semantic frames and discourse markers.", "labels": [], "entities": []}, {"text": "Moreover, while the vocabulary of discourse markers is rather small, the number of different surface form semantic frames that could appear in the text is very large.", "labels": [], "entities": []}, {"text": "To achieve a better level of abstraction, we disambiguate semantic frames and map them to their PropBank/FrameNet represen-tation.", "labels": [], "entities": [{"text": "PropBank/FrameNet represen-tation", "start_pos": 96, "end_pos": 129, "type": "DATASET", "confidence": 0.8342213779687881}]}, {"text": "Thus, in Ex.3, the resulting frame chain is \"rob.01 -but -arrest.01\" (\"01\" indicates the predicate sense).", "labels": [], "entities": []}, {"text": "Our second semantic language model is called Entity-Centered SemLM.", "labels": [], "entities": []}, {"text": "Here, we model a sequence of semantic frames and discourse markers involved in a specific co-reference chain.", "labels": [], "entities": []}, {"text": "For each co-reference chain in a document, we first extract semantic frames corresponding to each co-referent mention, disambiguate them as before, and then determine the discourse markers between these frames.", "labels": [], "entities": []}, {"text": "Thus, each unique frame contains both the disambiguated predicate and the argument label of the mention.", "labels": [], "entities": []}, {"text": "In Ex.3, the resulting sequence is \"rob.01#obj -but -arrest.01#obj\" (here \"obj\" indicates the argument label for \"Kevin\" and \"him\" respectively).", "labels": [], "entities": []}, {"text": "While these two models capture somewhat different semantic knowledge, we argue later in the paper that both models can be induced at high quality, and that they are suitable for different NLP tasks.", "labels": [], "entities": []}, {"text": "For both models of SemLM, we study four language model implementations: N-gram, skipgram (), continuous bagof-words () and log-bilinear language model.", "labels": [], "entities": []}, {"text": "Each model defines its own prediction task.", "labels": [], "entities": []}, {"text": "In total, we produce eight different SemLMs.", "labels": [], "entities": []}, {"text": "Except for Ngram model, others yield embeddings for semantic frames as they are neural language models.", "labels": [], "entities": []}, {"text": "In our empirical study, we evaluate both the quality of all SemLMs and their application to coreference resolution and shallow discourse parsing tasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.9459717273712158}, {"text": "shallow discourse parsing tasks", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.6867582499980927}]}, {"text": "Following the traditional evaluation standard of language models, we first use perplexity as our metric.", "labels": [], "entities": []}, {"text": "We also follow the script learning literature and evaluate on the narrative cloze test, i.e. randomly removing a token from a sequence and test the system's ability to recover it.", "labels": [], "entities": []}, {"text": "We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (); for entitycentered SemLMs, we use Ontonotes ( ).", "labels": [], "entities": [{"text": "hold-out dataset from the New York Times Corpus", "start_pos": 48, "end_pos": 95, "type": "DATASET", "confidence": 0.7085490822792053}, {"text": "PropBank", "start_pos": 151, "end_pos": 159, "type": "DATASET", "confidence": 0.8889085054397583}]}, {"text": "By comparing the results on these test sets, we show that we do not incur noticeable degradation when building SemLMs using preprocessing tools.", "labels": [], "entities": []}, {"text": "Moreover, we show that SemLMs improves the performance of co-reference resolution, as well as that of predicting the sense of discourse connectives for both explicit and implicit ones.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7411628514528275}]}, {"text": "The main contributions of our work can be summarized as follows: 1) The design of two novel discourse driven Semantic Language models, building on text abstraction and neural embeddings; 2) The implementation of high quality SemLMs that are shown to improve state-of-theart NLP systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset We use the New York Times Corpus 2 (from year 1987 to 2007) for training.", "labels": [], "entities": [{"text": "New York Times Corpus 2 (from year 1987 to 2007)", "start_pos": 19, "end_pos": 67, "type": "DATASET", "confidence": 0.9263198922077814}]}, {"text": "It contains a bit more than 1.8M documents in total.", "labels": [], "entities": []}, {"text": "Preprocessing We pre-process all documents with semantic role labeling () and part-of-speech tagger).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6356362899144491}]}, {"text": "We also implement the explicit discourse connective identification module in shallow discourse parsing (.", "labels": [], "entities": [{"text": "discourse connective identification", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6524604757626852}, {"text": "shallow discourse parsing", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.6408177614212036}]}, {"text": "Additionally, we utilize within document entity coreference () to produce coreference chains.", "labels": [], "entities": []}, {"text": "To obtain all annotations, we employ the Illinois NLP tools 3 .  In this section, we first evaluate the quality of SemLMs through perplexity and a narrative cloze test.", "labels": [], "entities": [{"text": "Illinois NLP tools", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9585014780362447}]}, {"text": "More importantly, we show that the proposed SemLMs can help improve the performance of coreference resolution and shallow discourse parsing.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.9615023136138916}, {"text": "shallow discourse parsing", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.6117821534474691}]}, {"text": "This further proves that we successfully capture semantic sequence information which can potentially benefit a wide range of semantic related NLP tasks.", "labels": [], "entities": []}, {"text": "We have designed two models for SemLM: frame-chain (FC) and entity-centered (EC).", "labels": [], "entities": [{"text": "SemLM", "start_pos": 32, "end_pos": 37, "type": "TASK", "confidence": 0.9473717212677002}]}, {"text": "By training on both types of sequences respectively, we implement four different language models: TRI, SG, CBOW, LB.", "labels": [], "entities": [{"text": "TRI", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.8874892592430115}]}, {"text": "We focus the evaluation efforts on these eight SemLMs.", "labels": [], "entities": []}, {"text": "Datasets We use three datasets.", "labels": [], "entities": []}, {"text": "We first randomly sample 10% of the New York Times Corpus documents (roughly two years of data), denoted the NYT Hold-out Data.", "labels": [], "entities": [{"text": "New York Times Corpus documents", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.8618886113166809}, {"text": "NYT Hold-out Data", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.8925593098004659}]}, {"text": "All our SemLMs are trained on the remaining NYT data and tested on this hold-out data.", "labels": [], "entities": [{"text": "NYT data", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9146873950958252}]}, {"text": "We generate semantic sequences for the training and test data using the methodology described in Sec.", "labels": [], "entities": []}, {"text": "5. We use PropBank data with gold frame annotations as another test set.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9290186166763306}]}, {"text": "In this case, we only generate frame-chain SemLM sequences by applying semantic unit generation techniques on gold frames, as described in Sec 5.2.", "labels": [], "entities": [{"text": "semantic unit generation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.6728540857632955}]}, {"text": "When we test on Gold PropBank Data with Frame Chains, we use frame-chain SemLMs trained from all NYT data.", "labels": [], "entities": [{"text": "Gold PropBank Data", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.94609268506368}, {"text": "NYT data", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9278035461902618}]}, {"text": "Similarly, we use Ontonotes data () with gold frame and co-reference annotations as the third test set, Gold Ontonotes Data with Coref Chains.", "labels": [], "entities": []}, {"text": "We only generate entitycentered SemLMs by applying semantic unit generation techniques on gold frames and gold coreference chains, as described in Sec 5.2.", "labels": [], "entities": [{"text": "semantic unit generation", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6748343308766683}]}, {"text": "Baselines We use Uni-gram (UNI) and Bi-gram (BG) as two language model baselines.", "labels": [], "entities": [{"text": "Bi-gram (BG)", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9350917339324951}]}, {"text": "In addition, we use the point-wise mutual information (PMI) for token prediction.", "labels": [], "entities": [{"text": "token prediction", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.9415791034698486}]}, {"text": "Essentially, PMI scores each pair of tokens according to their cooccurrences.", "labels": [], "entities": []}, {"text": "It predicts a token in the sequence by choosing the one with the highest total PMI with all other tokens in the sequence.", "labels": [], "entities": [{"text": "PMI", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9753956198692322}]}, {"text": "We use the ordered PMI (OP) as our baseline, which is a variation of PMI by considering asymmetric counting ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics on SemLM vocabularies and  sequences. \"F-s\" stands for single frame while  \"F-c\" stands for compound frame; \"Conn\" means  discourse marker. \"#seq\" is the number of se- quences, and \"#token\" is the total number of to- kens (semantic units). We also compute the av- erage token in a sequence i.e. \"#t/s\". We com- pare frame-chain (FC) and entity-centered (EC)  SemLMs to the usual syntactic language model  setting i.e. \"LM\".", "labels": [], "entities": []}, {"text": " Table 3: Perplexities for SemLMs. UNI, BG,  TRI, CBOW, SG, LB are different language model  implementations while \"FC\" and \"EC\" stand for  the two SemLM models studied, respectively.  \"FC-FM\" and \"EC-FM\" indicate that we removed  the \"FrameNet Mapping\" step (Sec. 5.2). LB con- sistently produces the lowest perplexities for both  frame-chain and entity-centered SemLMs.", "labels": [], "entities": [{"text": "UNI", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9331576824188232}, {"text": "FrameNet Mapping", "start_pos": 236, "end_pos": 252, "type": "TASK", "confidence": 0.6569972485303879}]}, {"text": " Table 4: Narrative cloze test results for SemLMs. UNI, BG, TRI, CBOW, SG, LB are different lan- guage model implementations while \"FC\" and \"EC\" stand for our two SemLM models, respectively.  \"FC-FM\" and \"EC-FM\" mean that we remove the FrameNet mappings. \"w/o DIS\" indicates the removal  of discourse makers in SemLMs. \"Rel-Impr\" indicates the relative improvement of the best performing  SemLM over the strongest baseline. We evaluate on two metrics: mean reciprocal rank (MRR)/recall at  30 (Recall@30). LB outperforms other methods for both frame-chain and entity-centered SemLMs.", "labels": [], "entities": [{"text": "UNI", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9456899166107178}, {"text": "mean reciprocal rank (MRR)/recall", "start_pos": 452, "end_pos": 485, "type": "METRIC", "confidence": 0.896078280040196}]}, {"text": " Table 4. Consistent with  the results in the perplexity evaluation, LB out- performs other methods for both frame-chain and  entity-centered SemLMs across all test sets. It is  interesting to see that UNI performs better than  BG in this prediction task. This finding is also  reflected in the results reported in", "labels": [], "entities": [{"text": "BG", "start_pos": 228, "end_pos": 230, "type": "METRIC", "confidence": 0.9097126722335815}]}, {"text": " Table 6: Shallow discourse parsing results with frame-chain SemLM features. \"FC\" stands for the  frame-chain SemLM. \"TRI\" is the tri-gram model while \"LB\" is the log-bilinear model. \"p c \", \"em\"  are conditional probability and frame embedding features, resp. \"w/o DIS\" indicates the case where we  remove all discourse makers for SemLMs. We do the experiments by adding SemLM features to the base  system. The improvement achieved by \"FC-LB (p c + em)\" over the baseline is statistically significant.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7217752635478973}, {"text": "TRI", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9875167012214661}]}]}