{"title": [{"text": "Investigating Language Universal and Specific Properties in Word Embeddings", "labels": [], "entities": [{"text": "Investigating Language Universal and Specific Properties in Word Embeddings", "start_pos": 0, "end_pos": 75, "type": "TASK", "confidence": 0.8286292288038466}]}], "abstractContent": [{"text": "Recently, many NLP tasks have benefited from distributed word representation.", "labels": [], "entities": [{"text": "distributed word representation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6476343671480814}]}, {"text": "However, it remains unknown whether embedding models are really immune to the typological diversity of languages, despite the language-independent architecture.", "labels": [], "entities": []}, {"text": "Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space.", "labels": [], "entities": []}, {"text": "Experiment results reveal the language universal and specific properties encoded in various word representation.", "labels": [], "entities": []}, {"text": "Additionally, strong evidence supports the utility of word form, especially for inflectional languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word representation is a core issue in natural language processing.", "labels": [], "entities": [{"text": "Word representation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7392212450504303}, {"text": "natural language processing", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6603415111700693}]}, {"text": "Context-based word representation, which is inspired by, has achieved huge successes in many NLP applications.", "labels": [], "entities": [{"text": "Context-based word representation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6629408498605093}]}, {"text": "Despite its popularity, character-based approach also comes out as an equal competitor . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by.", "labels": [], "entities": []}, {"text": "Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process, it remains unknown whether embedding models are really immune to the structural variance of languages.", "labels": [], "entities": [{"text": "interpreting word embedding", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.7803411881128947}]}, {"text": "Current research has gaps for understanding model behaviours towards language typological diversity as well as the utility of context and form for different languages.", "labels": [], "entities": []}, {"text": "Thus, we select three representative types of models and design a series of experiments to reveal the universals and specifics of various word representations on decoding linguistic properties.", "labels": [], "entities": []}, {"text": "Our work contributes to shedding new insights into the following topics: a) How do typological differences of language structure influence a word embedding model?", "labels": [], "entities": []}, {"text": "Does a model behave similarly towards phylogenetically-related languages?", "labels": [], "entities": []}, {"text": "b) Is word form a more efficient predictor of a certain grammatical function than word context for specific languages?", "labels": [], "entities": []}, {"text": "c) How do the neurons of a model respond to linguistic features?", "labels": [], "entities": []}, {"text": "Can we explain the utility of context and form by analyzing neuron activation pattern?", "labels": [], "entities": []}], "datasetContent": [{"text": "To study the proposed questions above, we design four series of experiments to comprehensively compare context-based and character-based word representations on different languages, covering syntactic, morphological and semantic properties.", "labels": [], "entities": []}, {"text": "The basic paradigm is to decode interpretable linguistic features from a target collection of word representations.", "labels": [], "entities": []}, {"text": "We hypothesize that there exists a linear/nonlinear map between a word representation x and a high-level sparse feature vector y if the word vector implicitly encode sufficient information   For linear map, we train a matrix \u0398 that maps word embedding x to a sparse feature vector y with the least L 2 error.", "labels": [], "entities": []}, {"text": "For nonlinear map, we train a neural network (MLP) with 4 hidden layers via back propagation.", "labels": [], "entities": []}, {"text": "Their dimensions are 50, 80, 80, and 50 in order.", "labels": [], "entities": []}, {"text": "For each linguistic feature of each language, a mapping model is trained on the randomly-selected 90% of the words with the target feature and tested over the remaining 10%.", "labels": [], "entities": []}, {"text": "Details about the construction of the linguistic feature vectors will be mentioned in the specific section of a certain experiment.", "labels": [], "entities": []}, {"text": "For syntactic and morphological features, we construct the corresponding feature vectors of a word from the Universal Dependencies Treebank (Joakim Nivre and Zhu, 2015) and the Chinese Treebank (CTB 7.0) (.", "labels": [], "entities": [{"text": "Universal Dependencies Treebank", "start_pos": 108, "end_pos": 139, "type": "DATASET", "confidence": 0.6271876494089762}, {"text": "Chinese Treebank (CTB 7.0)", "start_pos": 177, "end_pos": 203, "type": "DATASET", "confidence": 0.9675682485103607}]}, {"text": "For a certain word w with a certain linguistic attribute a (e.g. POS), w maybe annotated with one or different labels (e.g. NOUN, VERB, etc) from the possible label set of a in the whole treebank.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.816240668296814}, {"text": "VERB", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.8378235101699829}]}, {"text": "We calculate the normalized label frequency distribution ya w from the manual annotation of the corpus as the representation of the linguistic attribute a for the word win each language.", "labels": [], "entities": []}, {"text": "For word sentiment feature, we use the manually annotated data collected by.", "labels": [], "entities": [{"text": "word sentiment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7883794009685516}]}, {"text": "The data contains emotion scores fora list of words in several languages.", "labels": [], "entities": []}, {"text": "In our experiment, the original score scale in is transformed into the interval.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Model comparison on decoding POS,  along with WALS word-order features. Type  I: VS+VO+Pre+NR. II: SV+VO+Pre+RN. III:  SV+OV+Pre+NR. IV: SV+OV+Post+RN/Co. V:  SV+OV+Post+NR. VI: SV+ND+Pre+NR. VII:  SV+VO+Pre+NR. VIII: ND+VO+Pre+NR.", "labels": [], "entities": []}, {"text": " Table 3: Model comparison on decoding CASE.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of original and shuffled  character-based word representation on decoding  POS tag.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of morpho-phonological  knowledge transfer on different language pairs.  The reconstruction accuracy is correlated with  the overlapping proportion of grapheme patterns  between source language and target language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9775515198707581}]}]}