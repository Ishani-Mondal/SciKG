{"title": [{"text": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "labels": [], "entities": [{"text": "Cross-lingual Models of Word Embeddings", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5662129878997803}]}], "abstractContent": [{"text": "Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature.", "labels": [], "entities": []}, {"text": "We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typologically different language pairs.", "labels": [], "entities": []}, {"text": "Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity , and extrinsic evaluation on downstream semantic and syntactic applications.", "labels": [], "entities": []}, {"text": "We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP.", "labels": [], "entities": [{"text": "Learning word vector representations", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6535998433828354}]}, {"text": "The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (, inter alia), with improvements observed both on monolingual) and cross-lingual tasks (.", "labels": [], "entities": []}, {"text": "Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision -some can use document-level alignments, others need alignments at the sentence ( or word level, while some require both sentence and word alignments (.", "labels": [], "entities": []}, {"text": "However, a systematic comparison of these models is missing from the literature, making it difficult to analyze which approach is suitable fora particular NLP task.", "labels": [], "entities": []}, {"text": "In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions.", "labels": [], "entities": []}, {"text": "To this end, we train these models on four different language pairs, and evaluate them on both monolingual and cross-lingual tasks.", "labels": [], "entities": []}, {"text": "First, we show that different models can be viewed as instances of a more general framework for inducing cross-lingual word embeddings.", "labels": [], "entities": []}, {"text": "Then, we evaluate these models on both extrinsic and intrinsic tasks.", "labels": [], "entities": []}, {"text": "Our intrinsic evaluation assesses the quality of the vectors on monolingual ( \u00a74.2) and cross-lingual ( \u00a74.3) word similarity tasks, while our extrinsic evaluation spans semantic (cross-lingual document classification \u00a74.4) and syntactic tasks (cross-lingual dependency parsing \u00a74.5).", "labels": [], "entities": [{"text": "cross-lingual dependency parsing \u00a74.5", "start_pos": 245, "end_pos": 282, "type": "TASK", "confidence": 0.7462652206420899}]}, {"text": "Our experiments show that word vectors trained using expensive cross-lingual supervision (word alignments or sentence alignments) perform the best on semantic tasks.", "labels": [], "entities": [{"text": "word alignments or sentence alignments", "start_pos": 90, "end_pos": 128, "type": "TASK", "confidence": 0.7955760180950164}]}, {"text": "On the other hand, for syntactic tasks like cross-lingual dependency parsing, models requiring weaker form of cross-lingual supervision (such as context agnostic translation dictionary) are competitive to models requiring expensive supervision.", "labels": [], "entities": [{"text": "cross-lingual dependency parsing", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6640642484029134}, {"text": "context agnostic translation dictionary", "start_pos": 145, "end_pos": 184, "type": "TASK", "confidence": 0.6882155537605286}]}, {"text": "We also show qualitatively how the nature of cross-lingual supervision used to train word vectors affects the proximity of translation pairs across languages, and of words with similar meaning in the same language in the vector-space.", "labels": [], "entities": []}], "datasetContent": [{"text": "We measure the quality of the induced crosslingual word embeddings in terms of their performance, when used as features in the following tasks: \u2022 monolingual word similarity for English \u2022 Cross-lingual dictionary induction \u2022 Cross-lingual document classification \u2022 Cross-lingual syntactic dependency parsing The first two tasks intrinsically measure how much can monolingual and cross-lingual similarity benefit from cross-lingual training.", "labels": [], "entities": [{"text": "Cross-lingual dictionary induction", "start_pos": 188, "end_pos": 222, "type": "TASK", "confidence": 0.6236589650313059}, {"text": "Cross-lingual document classification", "start_pos": 225, "end_pos": 262, "type": "TASK", "confidence": 0.6965490380922953}, {"text": "Cross-lingual syntactic dependency parsing", "start_pos": 265, "end_pos": 307, "type": "TASK", "confidence": 0.6736855655908585}]}, {"text": "The last two tasks measure the ability of cross-lingually trained vectors to extrinsically facilitate model transfer across languages, for semantic and syntactic applications respectively.", "labels": [], "entities": []}, {"text": "These tasks have been used in previous works () for evaluating cross-lingual embeddings, but no comparison exists which uses them in conjunction.", "labels": [], "entities": []}, {"text": "To ensure fair comparison, all models are trained with embeddings of size 200.", "labels": [], "entities": []}, {"text": "We provide all models with parallel corpora, irrespective of their requirements.", "labels": [], "entities": []}, {"text": "Whenever possible, we also report statistical significance of our results.", "labels": [], "entities": [{"text": "significance", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.536652684211731}]}, {"text": "We first evaluate if the inclusion of cross-lingual knowledge improves the quality of English embeddings.", "labels": [], "entities": []}, {"text": "Word similarity datasets contain word pairs which are assigned similarity ratings by humans.", "labels": [], "entities": []}, {"text": "The task evaluates how well the notion of word similarity according to humans is emulated in the vector space.", "labels": [], "entities": []}, {"text": "Evaluation is based on the Spearman's rank correlation coefficient ( between human rankings and rankings produced by computing cosine similarity between the vectors of two words.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 27, "end_pos": 66, "type": "METRIC", "confidence": 0.5775908052921295}]}, {"text": "We use the SimLex dataset for English () which contains 999 pairs of English words, with a balanced set of noun, adjective and verb pairs.", "labels": [], "entities": [{"text": "SimLex dataset", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.9323965609073639}]}, {"text": "SimLex is claimed to capture word similarity exclusively instead of WordSim-353 () which captures both word similarity and relatedness.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.9482277631759644}]}, {"text": "We declare significant improvement if p < 0.1 according to Steiger's method for calculating the statistical significant differences between two dependent correlation coefficients.", "labels": [], "entities": []}, {"text": "shows the performance of English embeddings induced by all the models by training on different language pairs on the SimLex word similarity task.", "labels": [], "entities": [{"text": "SimLex word similarity task", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.7415908873081207}]}, {"text": "The score obtained by monolingual English embeddings trained on the respective English side of each language is shown in column marked Mono.", "labels": [], "entities": []}, {"text": "In all cases (except BiCCA on ensv), the bilingually trained vectors achieve better scores than the mono-lingually trained vectors.", "labels": [], "entities": [{"text": "BiCCA", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.7596678137779236}]}, {"text": "Overall, across all language pairs, BiCVM is the best performing model in terms of Spearman's correlation, but its improvement over BiSkip and BiVCD is often insignificant.", "labels": [], "entities": [{"text": "Spearman's correlation", "start_pos": 83, "end_pos": 105, "type": "METRIC", "confidence": 0.561841199795405}]}, {"text": "It is notable that 2 of the 3 top performing models, BiCVM and BiVCD, need sentence aligned and document-aligned corpus only, which are easier to obtain than parallel data with word alignments required by BiSkip.", "labels": [], "entities": []}, {"text": "proposed an intrinsic evaluation metric for estimating the quality of English word vectors.", "labels": [], "entities": []}, {"text": "The score produced by QVEC measures how well a given set of word vectors is able to quantify linguistic properties We implemented the code for performing the merging as we could not find a tool provided by the authors.   of words, with higher being better.", "labels": [], "entities": [{"text": "QVEC", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.816005289554596}]}, {"text": "The metric is shown to have strong correlation with performance on downstream semantic applications.", "labels": [], "entities": []}, {"text": "As it can be currently only used for English, we use it to evaluate the English vectors obtained using cross-lingual training of different models.", "labels": [], "entities": []}, {"text": "shows that on average across language pairs, BiSkip achieves the best score, followed by Mono (mono-lingually trained English vectors), BiVCD and BiCCA.", "labels": [], "entities": [{"text": "BiSkip", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.7399932146072388}]}, {"text": "A possible explanation for why Mono scores are better than those obtained by some of the cross-lingual models is that QVEC measures monolingual semantic content based on a linguistic oracle made for English.", "labels": [], "entities": []}, {"text": "Cross-lingual training might affect these semantic properties arbitrarily.", "labels": [], "entities": []}, {"text": "Interestingly, BiCVM which was the best model according to SimLex, ranks last according to QVEC.", "labels": [], "entities": [{"text": "SimLex", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.905377209186554}, {"text": "QVEC", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.9380951523780823}]}, {"text": "The fact that the best models according to QVEC and word similarities are different reinforces observations made in previous work that performance on word similarity tasks alone does not reflect quantification of linguistic properties of words (: Cross-lingual dictionary induction results (top-10 accuracy).", "labels": [], "entities": [{"text": "QVEC", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8553828597068787}, {"text": "Cross-lingual dictionary induction", "start_pos": 247, "end_pos": 281, "type": "TASK", "confidence": 0.6374757488568624}, {"text": "accuracy", "start_pos": 298, "end_pos": 306, "type": "METRIC", "confidence": 0.975141167640686}]}, {"text": "The same trend was also observed across models when computing MRR (mean reciprocal rank).", "labels": [], "entities": [{"text": "MRR", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9259412288665771}]}], "tableCaptions": [{"text": " Table 1: The size of parallel corpora (in millions) of different  language pairs used for training cross-lingual word vectors.", "labels": [], "entities": [{"text": "training cross-lingual word vectors", "start_pos": 91, "end_pos": 126, "type": "TASK", "confidence": 0.5863840878009796}]}, {"text": " Table 2: Word similarity score measured in Spearman's cor- relation ratio for English on SimLex-999. The best score for  each language pair is shown in bold. Scores which are sig- nificantly better (per Steiger's Method with p < 0.1) than  the next lower score are underlined. For example, for en-zh,  BiCVM is significantly better than BiSkip, which in turn is  significantly better than BiVCD.", "labels": [], "entities": []}, {"text": " Table 3: Intrinsic evaluation of English word vectors mea- sured in terms of QVEC score across models. Best scores for  each language pair is shown in bold.", "labels": [], "entities": [{"text": "mea- sured", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.8725369373957316}, {"text": "QVEC score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.8863357901573181}]}, {"text": " Table 4: Cross-lingual dictionary induction results (top-10  accuracy). The same trend was also observed across models  when computing MRR (mean reciprocal rank).", "labels": [], "entities": [{"text": "Cross-lingual dictionary induction", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6443262894948324}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.986866295337677}, {"text": "MRR", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.82290118932724}]}, {"text": " Table 5: Cross-lingual document classification accuracy  when trained on language l1, and evaluated on language l2.  The best score for each language is shown in bold. Scores  which are significantly better (per McNemar's Test with p <  0.05) than the next lower score are underlined. For example,  for sv\u2192en, BiVCD is significantly better than BiSkip, which  in turn is significantly better than BiCVM.", "labels": [], "entities": [{"text": "Cross-lingual document classification", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6801152328650156}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9846744537353516}, {"text": "McNemar's Test", "start_pos": 213, "end_pos": 227, "type": "DATASET", "confidence": 0.8459895253181458}]}, {"text": " Table 7: Labeled attachment score (LAS) for dependency  parsing when trained and tested on language l. Mono refers  to parser trained with mono-lingually induced embeddings.  Scores in bold are better than the Mono scores for each lan- guage, showing improvement from cross-lingual training.", "labels": [], "entities": [{"text": "Labeled attachment score (LAS)", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.8830752372741699}, {"text": "dependency  parsing", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8203879594802856}]}]}