{"title": [{"text": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures", "labels": [], "entities": [{"text": "End-to-End Relation Extraction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7504696647326151}]}], "abstractContent": [{"text": "We present a novel end-to-end neural model to extract entities and relations between them.", "labels": [], "entities": []}, {"text": "Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs.", "labels": [], "entities": []}, {"text": "This allows our model to jointly represent both entities and relations with shared parameters in a single model.", "labels": [], "entities": []}, {"text": "We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8210493326187134}]}, {"text": "Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7329510599374771}, {"text": "F1-score", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9965047836303711}, {"text": "ACE2005", "start_pos": 166, "end_pos": 173, "type": "DATASET", "confidence": 0.9775788187980652}, {"text": "ACE2004", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.9201086163520813}]}, {"text": "We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8).", "labels": [], "entities": [{"text": "F1-score", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9964441657066345}, {"text": "nominal relation classification", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.7208374937375387}]}, {"text": "Finally, we present an extensive ablation analysis of several model components.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP).", "labels": [], "entities": [{"text": "Extracting semantic relations between entities in text", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.905455504144941}, {"text": "information extraction", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7460988461971283}, {"text": "natural language processing (NLP)", "start_pos": 123, "end_pos": 156, "type": "TASK", "confidence": 0.8072759211063385}]}, {"text": "Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER) () and relation extraction (), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance () since relations interact closely with entity information.", "labels": [], "entities": [{"text": "entity recognition (NER)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.8390441417694092}, {"text": "relation extraction", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.75961834192276}]}, {"text": "For instance, to learn that Toefting and Bolton have an OrganizationAffiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important.", "labels": [], "entities": []}, {"text": "Extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation.", "labels": [], "entities": []}, {"text": "Previous joint models have employed feature-based structured learning.", "labels": [], "entities": []}, {"text": "An alternative approach to this end-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8090058962504069}]}, {"text": "There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs).", "labels": [], "entities": []}, {"text": "Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences) and constituent/dependency trees.", "labels": [], "entities": []}, {"text": "Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs () is worse than one using CNNs (dos).", "labels": [], "entities": [{"text": "relation classification tasks", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.9377986192703247}]}, {"text": "These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly.", "labels": [], "entities": []}, {"text": "We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures.", "labels": [], "entities": []}, {"text": "Word sequence and tree structure are known to be complementary information for extracting relations.", "labels": [], "entities": []}, {"text": "For instance, dependencies between words are not enough to predict that source and U.S. have an ORG-AFF relation in the sentence \"This is ...\", one U.S. source said, and the context word said is required for this prediction.", "labels": [], "entities": [{"text": "ORG-AFF", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.9855843186378479}]}, {"text": "Many traditional, feature-based relation classification models extract features from both sequences and parse trees ().", "labels": [], "entities": [{"text": "relation classification", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7079276442527771}]}, {"text": "However, previous RNNbased models focus on only one of these linguistic structures ().", "labels": [], "entities": []}, {"text": "We present a novel end-to-end model to extract relations between entities on both word sequence and dependency tree structures.", "labels": [], "entities": []}, {"text": "Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential (left-to-right and right-to-left) and bidirectional tree-structured (bottom-up and top-down) LSTMRNNs.", "labels": [], "entities": []}, {"text": "Our model first detects entities and then extracts relations between the detected entities using a single incrementally-decoded NN structure, and the NN parameters are jointly updated using both entity and relation labels.", "labels": [], "entities": []}, {"text": "Unlike traditional incremental end-to-end relation extraction models, our model further incorporates two enhancements into training: entity pretraining, which pretrains the entity model, and scheduled sampling, which replaces (unreliable) predicted labels with gold labels in a certain probability.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7660656571388245}]}, {"text": "These enhancements alleviate the problem of low-performance entity detection in early stages of training, as well as allow entity information to further help downstream relation classification.", "labels": [], "entities": [{"text": "entity detection", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.757956862449646}, {"text": "relation classification", "start_pos": 169, "end_pos": 192, "type": "TASK", "confidence": 0.6772094815969467}]}, {"text": "On end-to-end relation extraction, we improve over the state-of-the-art feature-based model, with 12.1% (ACE2005) and 5.7% (ACE2004) relative error reductions in F1-score.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7485204339027405}, {"text": "ACE2005", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.8703559637069702}, {"text": "ACE2004) relative error", "start_pos": 124, "end_pos": 147, "type": "METRIC", "confidence": 0.7355163767933846}, {"text": "F1-score", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9961344003677368}]}, {"text": "On nominal relation classification, our model compares favorably to the state-of-the-art CNNbased model in F1-score.", "labels": [], "entities": [{"text": "nominal relation classification", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.7682351271311442}, {"text": "F1-score", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9837401509284973}]}, {"text": "Finally, we also ablate and compare our various model components, which leads to some key findings (both positive and negative) about the contribution and effectiveness of different RNN structures, input dependency relation structures, different parsing models, external resources, and joint learning settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented our model using the cnn library.", "labels": [], "entities": []}, {"text": "We parsed the texts using the Stanford neural dependency parser) with the original Stanford Dependencies.", "labels": [], "entities": [{"text": "Stanford Dependencies", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.9720973968505859}]}, {"text": "Based on preliminary tuning, we fixed embedding dimensions n w to 200, n p , n d , n e to 25, and dimensions of intermediate layers (n ls , n lt of LSTM-RNNs and n he , n hr of hidden layers) to 100.", "labels": [], "entities": []}, {"text": "We initialized word vectors via word2vec () trained on Wikipedia and randomly initialized all other parameters.", "labels": [], "entities": []}, {"text": "We tuned hyper-parameters using development sets for ACE05 and SemEval-2010 Task 8 to achieve high primary (Micro-and Macro-) F1-scores.", "labels": [], "entities": [{"text": "ACE05", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9465418457984924}, {"text": "F1-scores", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.8986997604370117}]}, {"text": "For ACE04, we directly employed the best parameters for ACE05.", "labels": [], "entities": [{"text": "ACE04", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9285098910331726}, {"text": "ACE05", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.966233491897583}]}, {"text": "The hyperparameter settings are shown in the supplementary material.", "labels": [], "entities": []}, {"text": "For SemEval-2010 Task 8, we also omitted the entity detection and label embeddings since only target nominals are annotated and the task defines no entity types.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7480530738830566}, {"text": "entity detection", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.7310088276863098}]}, {"text": "Our statistical significance results are based on the Approximate Randomization (AR) test).", "labels": [], "entities": [{"text": "Approximate Randomization (AR) test", "start_pos": 54, "end_pos": 89, "type": "METRIC", "confidence": 0.940681666135788}]}], "tableCaptions": [{"text": " Table 1: Comparison with the state-of-the-art on the ACE05 test set and ACE04 dataset.", "labels": [], "entities": [{"text": "ACE05 test set", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9844711621602377}, {"text": "ACE04 dataset", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.970218151807785}]}, {"text": " Table 2: Ablation tests on the ACE05 development dataset. * denotes significance at p<0.05, ** denotes  p<0.01.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.993526816368103}, {"text": "ACE05 development dataset", "start_pos": 32, "end_pos": 57, "type": "DATASET", "confidence": 0.963469405968984}, {"text": "significance", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.9940624833106995}]}, {"text": " Table 3: Comparison of LSTM-RNN structures on the ACE05 development dataset.", "labels": [], "entities": [{"text": "ACE05 development dataset", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.9625916282335917}]}, {"text": " Table 4: Comparison with state-of-the-art models  on SemEval-2010 Task 8 test-set.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 test-set", "start_pos": 54, "end_pos": 82, "type": "DATASET", "confidence": 0.6577156782150269}]}, {"text": " Table 5: Comparison of LSTM-RNN structures on  SemEval-2010 Task 8 development set.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 development set", "start_pos": 48, "end_pos": 83, "type": "DATASET", "confidence": 0.7108774006366729}]}, {"text": " Table 6: Model setting ablations on SemEval- 2010 development set.", "labels": [], "entities": [{"text": "SemEval- 2010 development set", "start_pos": 37, "end_pos": 66, "type": "DATASET", "confidence": 0.7575722217559815}]}]}