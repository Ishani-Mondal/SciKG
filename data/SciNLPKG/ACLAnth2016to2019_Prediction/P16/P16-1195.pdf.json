{"title": [{"text": "Summarizing Source Code using a Neural Attention Model", "labels": [], "entities": [{"text": "Summarizing Source Code", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8687637646993002}]}], "abstractContent": [{"text": "High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums.", "labels": [], "entities": []}, {"text": "Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done fora small fraction of all code that is produced.", "labels": [], "entities": [{"text": "code search", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.7815355360507965}]}, {"text": "In this paper, we present the first completely data-driven approach for generating high level summaries of source code.", "labels": [], "entities": []}, {"text": "Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries.", "labels": [], "entities": []}, {"text": "CODE-NN is trained on anew corpus that is automatically collected from StackOverflow, which we release.", "labels": [], "entities": [{"text": "CODE-NN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9341968894004822}]}, {"text": "Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large margin .", "labels": [], "entities": [{"text": "code summarization", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7436713874340057}, {"text": "code retrieval", "start_pos": 179, "end_pos": 193, "type": "TASK", "confidence": 0.776628851890564}]}], "introductionContent": [{"text": "Billions of lines of source code reside in online repositories (, and high quality code is often coupled with natural language (NL) in the form of instructions, comments, and documentation.", "labels": [], "entities": []}, {"text": "Short summaries of the overall computation the code performs provide a particularly useful form of documentation fora range of applications, such as code search or tutorials.", "labels": [], "entities": [{"text": "code search", "start_pos": 149, "end_pos": 160, "type": "TASK", "confidence": 0.7453060746192932}]}, {"text": "However, such summaries are expensive to manually author.", "labels": [], "entities": []}, {"text": "As a result, this laborious process is only done fora small fraction of all code that is produced.", "labels": [], "entities": []}, {"text": "In this paper, we present the first completely data-driven approach for generating short highlevel summaries of source code snippets in natural language.", "labels": [], "entities": []}, {"text": "We focus on C#, a general-purpose imperative language, and SQL, a declarative language for querying databases.", "labels": [], "entities": []}, {"text": "shows example code snippets with descriptions that summarize the overall function of the code, with the goal to generate high level descriptions, such as lookup a substring in a string.", "labels": [], "entities": []}, {"text": "Generating such a summary is often challenging because the text can include complex, non-local aspects of the code (e.g., consider the phrase 'second largest' in Example 3 in).", "labels": [], "entities": [{"text": "Example", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.8736072778701782}]}, {"text": "In addition to being directly useful for interpreting uncommented code, high-quality generation models can also be used for code retrieval, and in turn, for natural language programming by applying nearest neighbor techniques to a large corpus of automatically summarized code.", "labels": [], "entities": [{"text": "interpreting uncommented code", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.878311812877655}, {"text": "code retrieval", "start_pos": 124, "end_pos": 138, "type": "TASK", "confidence": 0.7264745086431503}]}, {"text": "Natural language generation has traditionally been addressed as a pipeline of modules that decide 'what to say' (content selection) and 'how to say it' (realization) separately).", "labels": [], "entities": [{"text": "Natural language generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.614846815665563}]}, {"text": "Such approaches require supervision at each stage and do not scale well to large domains.", "labels": [], "entities": []}, {"text": "We instead propose an end-to-end neural network called CODE-NN that jointly performs content selection using an attention mechanism, and surface realization using Long Short Term Memory (LSTM) networks.", "labels": [], "entities": [{"text": "content selection", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7622818350791931}, {"text": "surface realization", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7367271482944489}]}, {"text": "The system generates a summary one word at a time, guided by an attention mechanism over embeddings of the source code, and by context from previously generated words provided by a LSTM network.", "labels": [], "entities": []}, {"text": "The simplicity of the model allows it to be learned from the training data without the burden of feature engineering or the use of an expensive approximate decoding algorithm (.", "labels": [], "entities": []}, {"text": "Our model is trained on anew dataset of code snippets with short descriptions, created using data gathered from Stackoverflow, 1 a popular programming help website.", "labels": [], "entities": []}, {"text": "Since access is open and unrestricted, the content is inherently noisy (ungrammatical, non-parsable, lacking content), but as we will see, it still provides strong signal for learning.", "labels": [], "entities": []}, {"text": "To reliably evaluate our model, we also collect a clean, human-annotated test set.", "labels": [], "entities": []}, {"text": "We evaluate CODE-NN on two tasks: code summarization and code retrieval (Section 2).", "labels": [], "entities": [{"text": "code summarization", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7663491666316986}, {"text": "code retrieval", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7759920954704285}]}, {"text": "For summarization, we evaluate using automatic metrics such as METEOR and BLEU-4, together with a human study for naturalness and informativeness of the output.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9806607961654663}, {"text": "METEOR", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9379611015319824}, {"text": "BLEU-4", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9983384609222412}]}, {"text": "The results show that CODE-NN outperforms a number of strong baselines and, to the best of our knowledge, CODE-NN is the first approach that learns to generate summaries of source code from easily gathered online data.", "labels": [], "entities": [{"text": "summaries of source code from easily gathered online", "start_pos": 160, "end_pos": 212, "type": "TASK", "confidence": 0.775983951985836}]}, {"text": "We further use CODE-NN for code retrieval for programming related questions on a recent C# benchmark, and results show that CODE-NN improves the state of the art () for mean reciprocal rank (MRR) by a wide margin.", "labels": [], "entities": [{"text": "code retrieval", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.6971435248851776}, {"text": "mean reciprocal rank (MRR)", "start_pos": 169, "end_pos": 195, "type": "METRIC", "confidence": 0.8431780437628428}]}], "datasetContent": [{"text": "We collected data from StackOverflow (SO), a popular website for posting programming-related questions.", "labels": [], "entities": []}, {"text": "Anonymized versions of all the posts can be freely downloaded.", "labels": [], "entities": []}, {"text": "Each post can have multiple tags.", "labels": [], "entities": []}, {"text": "Using the C# tag for C# and the sql, database and oracle tags for SQL, we were able to collect 934,464 and 977,623 posts respectively.", "labels": [], "entities": []}, {"text": "Each post comprises a short title, a detailed question, and one or more responses, of which one can be marked as accepted.", "labels": [], "entities": []}, {"text": "We found that the text in the question and responses is domain-specific and verbose, mixed with details that are irrelevant for our tasks.", "labels": [], "entities": []}, {"text": "Also, code snippets in responses that were not accepted were frequently incorrect or tangential to the question asked.", "labels": [], "entities": []}, {"text": "Thus, we extracted only the title from the post and use the code snippet from those accepted answers that contain exactly one code snippet (using <code> tags).", "labels": [], "entities": []}, {"text": "We add the resulting (title, query) pairs to our corpus, resulting in a total of 145,841 pairs for C# and 41,340 pairs for SQL.", "labels": [], "entities": []}, {"text": "Cleaning We train a semi-supervised classifier to filter titles like 'Difficult C# if then logic' or 'How can I make this query easier to write?' that bear no relation to the corresponding code snippet.", "labels": [], "entities": []}, {"text": "To do so, we annotate 100 titles as being clean or not clean for each language and use them to bootstrap the algorithm.", "labels": [], "entities": []}, {"text": "We then use the remaining titles in our training set as an unsupervised signal, and obtain a classification accuracy of over 73% on a manually labeled test set for both languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9492422938346863}]}, {"text": "For the final dataset, we retain 66,015 C# (title, query) pairs and 32,337 SQL pairs that are classified as clean, and use 80% of these datasets for training, 10% for validation and 10% for testing.", "labels": [], "entities": []}, {"text": "Parsing Given the informal nature of StackOverflow, the code snippets are approximate answers that are usually incomplete.", "labels": [], "entities": []}, {"text": "For example, we observe that only 12% of the SQL queries parse without any syntactic errors (using zql 5 ).", "labels": [], "entities": [{"text": "SQL queries parse", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7225150664647421}]}, {"text": "We therefore aim to perform a best-effort parse of the code snippet, using modified versions of an ANTLR parser for C# and pythonsqlparse (Albrecht, 2015) for SQL.", "labels": [], "entities": []}, {"text": "We strip out all comments and to avoid being context specific, we replace literals with tokens denoting their types.", "labels": [], "entities": []}, {"text": "In addition, for SQL, we replace table and column names with numbered placeholder tokens while preserving any dependencies in the query.", "labels": [], "entities": []}, {"text": "For example, the SQL query in is represented as SELECT MAX(col0) FROM tab0 WHERE col0 < (SELECT MAX(col0) FROM tab0).", "labels": [], "entities": [{"text": "SELECT MAX(col0) FROM tab0 WHERE col0", "start_pos": 48, "end_pos": 85, "type": "METRIC", "confidence": 0.7662460009256998}]}, {"text": "We assess ranking quality by computing the Mean Reciprocal Rank (MRR) of c * j . For every snippet c j in EVAL (and DEV), we use two of the three references (title and human annotation), namely n j,1 , n j,2 . We then build a retrieval set comprising (c j , n j,1 ) together with 49 random distractor pairs (c , n ), c = c j from the test set.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 43, "end_pos": 69, "type": "METRIC", "confidence": 0.962914913892746}, {"text": "EVAL", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.9033579230308533}]}, {"text": "Using n j,2 as the natural language question, we rank all 50 items in this retrieval set and use the rank of query c * j to compute MRR.", "labels": [], "entities": [{"text": "MRR", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.45573076605796814}]}, {"text": "We average MRR overall returned queries c * j in the test set, and repeat this experiment for several different random sets of distractors.", "labels": [], "entities": [{"text": "MRR", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.6993405818939209}]}], "tableCaptions": [{"text": " Table 1: Statistics for code snippets in our dataset.", "labels": [], "entities": []}, {"text": " Table 3: Performance on EVAL for the GEN task.  Performance on DEV is indicated in parentheses.", "labels": [], "entities": [{"text": "GEN task", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.6643355488777161}, {"text": "DEV", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.6988378167152405}]}, {"text": " Table 4: Naturalness and Informativeness mea- sures of model outputs. Stat. sig. between CODE- NN and others is computed with a 2-tailed Stu- dent's t-test; p < 0.05 except for *.", "labels": [], "entities": []}, {"text": " Table 7: Error analysis on 50 examples in DEV", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.980899453163147}, {"text": "DEV", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.897513210773468}]}]}