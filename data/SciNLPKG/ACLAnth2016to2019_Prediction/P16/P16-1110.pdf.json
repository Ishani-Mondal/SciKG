{"title": [{"text": "ALTO: Active Learning with Topic Overviews for Speeding Label Induction and Document Labeling", "labels": [], "entities": [{"text": "ALTO", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6020817160606384}, {"text": "Speeding Label Induction and Document Labeling", "start_pos": 47, "end_pos": 93, "type": "TASK", "confidence": 0.7151231368382772}]}], "abstractContent": [{"text": "Effective text classification requires experts to annotate data with labels; these training data are time-consuming and expensive to obtain.", "labels": [], "entities": [{"text": "text classification", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7110239714384079}]}, {"text": "If you know what labels you want, active learning can reduce the number of labeled documents needed.", "labels": [], "entities": []}, {"text": "However, establishing the label set remains difficult.", "labels": [], "entities": []}, {"text": "An-notators often lack the global knowledge needed to induce a label set.", "labels": [], "entities": []}, {"text": "We introduce ALTO: Active Learning with Topic Overviews, an interactive system to help humans annotate documents: topic models provide a global overview of what labels to create and active learning directs them to the right documents to label.", "labels": [], "entities": []}, {"text": "Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions, topic models (even by themselves) lead to better label sets, and ALTO's combination is best overall .", "labels": [], "entities": []}], "introductionContent": [{"text": "Many fields depend on texts labeled by human experts; computational linguistics uses such annotation to determine word senses and sentiment); while social science uses \"coding\" to scale up and systemetize content analysis).", "labels": [], "entities": [{"text": "systemetize content analysis", "start_pos": 193, "end_pos": 221, "type": "TASK", "confidence": 0.6671181321144104}]}, {"text": "Classification takes these labeled data as a training set and labels new data automatically.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.969563364982605}]}, {"text": "Creating a broadly applicable and consistent label set that generalizes well is time-consuming and difficult, requiring expensive annotators to examine large swaths of the data.", "labels": [], "entities": []}, {"text": "Effective NLP systems must measure) and reduce annotation cost.", "labels": [], "entities": []}, {"text": "Annotation is hard because it requires both global and local knowledge of the entire dataset.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9263683557510376}]}, {"text": "Global knowledge is required to create the set of labels, and local knowledge is required to annotate the most useful examples to serve as a training set for an automatic classifier.", "labels": [], "entities": []}, {"text": "The former's cost is often hidden in multiple rounds of refining annotation guidelines.", "labels": [], "entities": []}, {"text": "We create a single interface-ALTO (Active Learning with Topic Overviews)-to address both global and local challenges using two machine learning tools: topic models and active learning (we review both in Section 2).", "labels": [], "entities": []}, {"text": "Topic models address the need for annotators to have a global overview of the data, exposing the broad themes of the corpus so annotators know what labels to create.", "labels": [], "entities": []}, {"text": "Active learning selects documents that help the classifier understand the differences between labels and directs the user's attention locally to them.", "labels": [], "entities": []}, {"text": "We provide users four experimental conditions to compare the usefulness of a topic model or a simple list of documents, with or without active learning suggestions (Section 3).", "labels": [], "entities": []}, {"text": "We then describe our data and evaluation metrics (Section 4).", "labels": [], "entities": []}, {"text": "Through both synthetic experiments (Section 5) and a user study (Section 6) with forty participants, we evaluate ALTO and its constituent components by comparing results from the four conditions introduced above.", "labels": [], "entities": [{"text": "ALTO", "start_pos": 113, "end_pos": 117, "type": "TASK", "confidence": 0.8689665198326111}]}, {"text": "We first examine user strategies for organizing documents, user satisfaction, and user efficiency.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the overall effectiveness of the label set in a post study crowdsourced task.: Given a dataset-in this case, the US congressional bills dataset-topics are automatically discovered sorted lists of terms that summarize segments of a document collection.", "labels": [], "entities": [{"text": "US congressional bills dataset-topics", "start_pos": 134, "end_pos": 171, "type": "DATASET", "confidence": 0.64891666918993}]}, {"text": "Topics also are associated with documents.", "labels": [], "entities": []}, {"text": "These topics give users a sense of documents' main themes and help users create high-quality labels.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our data, the machine learning techniques to learn classifiers from examples, and the evaluation metrics to know whether the final labeling of the complete documents collection was successful.", "labels": [], "entities": []}, {"text": "Data Our experiments require corpora to compare user labels with gold standard labels.", "labels": [], "entities": []}, {"text": "We experiment with two corpora: 20Newsgroups and US congressional bills from GovTrack.", "labels": [], "entities": [{"text": "20Newsgroups", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9345000386238098}]}, {"text": "For US congressional bills, GovTrack provides bill information such as the title and text, while the Congressional Bills Project) provides labels and sub-labels for the bills.", "labels": [], "entities": []}, {"text": "Examples of labels are agriculture and health, while sub-labels include agricultural trade and comprehensive healthcare reform.", "labels": [], "entities": []}, {"text": "The twenty top-level labels have been developed by consensus over many years by a team of top political scientists to create a reliable, robust dataset.", "labels": [], "entities": []}, {"text": "We use the 112 th Congress; after filtering, this dataset has 5558 documents.", "labels": [], "entities": [{"text": "112 th Congress", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.6314252813657125}]}, {"text": "We use this dataset in both the synthetic experiments (Section 5) and the user study (Section 6).", "labels": [], "entities": []}, {"text": "The 20 Newsgroups corpus has 19, 997 documents grouped in twenty news groups that are further grouped into six more general topics.", "labels": [], "entities": []}, {"text": "Examples are talk.politics.guns and sci.electronics, which belong to the general topics of politics and science.", "labels": [], "entities": []}, {"text": "We use this dataset in synthetic experiments (Section 5).", "labels": [], "entities": []}, {"text": "Our goal is to create a system that allows users to quickly induce a high-quality label set.", "labels": [], "entities": []}, {"text": "We compare the user-created label sets against the data's gold label sets.", "labels": [], "entities": []}, {"text": "Comparing different clusterings is a difficult task, so we use three clustering evaluation metrics: purity), rand index, and normalized mutual information (.", "labels": [], "entities": [{"text": "purity", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9767798781394958}, {"text": "rand index", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9342647790908813}]}, {"text": "Purity The documents labeled with a good user label should only have one (or a few) gold labels associated with them: this is measured by cluster purity.", "labels": [], "entities": []}, {"text": "Given each user cluster, it measures what fraction of the documents in a user cluster belong to the most frequent gold label in that cluster: where L is the number of labels user creates, . .", "labels": [], "entities": []}, {"text": ", G J } is gold clustering of documents, and N is the total number of documents.", "labels": [], "entities": []}, {"text": "The user \u2126 land gold G j labels are interpreted as sets containing all documents assigned to that label.", "labels": [], "entities": []}, {"text": "Rand index (RI) RI is a pair counting measure, where cluster evaluation is considered as a series of decisions.", "labels": [], "entities": [{"text": "Rand index (RI) RI", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.9069281121095022}, {"text": "pair counting", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.7102613747119904}]}, {"text": "If two documents have the same gold label and the same user label (TP) or if they do not have the same gold label and are not assigned the same user label (TN), the decision is right.", "labels": [], "entities": []}, {"text": "Otherwise, it is wrong.", "labels": [], "entities": []}, {"text": "RI measures the percentage of decisions that are right: Normalized mutual information (NMI) NMI is an information theoretic measure that measures the amount of information one gets about the gold clusters by knowing what the user clusters are: where \u2126 and G are user and gold clusters, H is the entropy and I is mutual information.", "labels": [], "entities": [{"text": "RI", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8803050518035889}]}, {"text": "While purity, RI, and NMI are all normalized within (higher is better), they measure different things.", "labels": [], "entities": [{"text": "purity", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9980078339576721}, {"text": "RI", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9836873412132263}, {"text": "NMI", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9903911352157593}]}, {"text": "Purity measures the intersection between two clusterings, it is sensitive to the number of clusters, and it is not symmetric.", "labels": [], "entities": []}, {"text": "On the other hand, RI and NMI are less sensitive to the number of clusters and are symmetric.", "labels": [], "entities": []}, {"text": "RI measures pairwise agreement in contrast to purity's emphasis on intersection.", "labels": [], "entities": [{"text": "RI", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6495914459228516}]}, {"text": "Moreover, NMI measures shared information between two clusterings.", "labels": [], "entities": []}, {"text": "None of these metrics are perfect: purity can be exploited by putting each document in its own label, RI does not distinguish separating similar documents with distinct labels from giving dissimilar documents the same label, and NMI's ability to compare different numbers of clusters means that it sometimes gives high scores for clusterings by chance.", "labels": [], "entities": [{"text": "purity", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9884174466133118}, {"text": "RI", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9362499117851257}]}, {"text": "Given the diverse nature of these metrics, if a labeling does well in all three of them, we can be relatively confident that it is not a degenerate solution that games the system.", "labels": [], "entities": []}, {"text": "Before running a user study, we test our hypothesis that topic model overviews and active learning selection improve final cluster quality compared to standard baselines: list overview and random selection.", "labels": [], "entities": []}, {"text": "We simulate the four conditions on Congressional Bills and 20 Newsgroups.", "labels": [], "entities": [{"text": "Congressional Bills and 20 Newsgroups", "start_pos": 35, "end_pos": 72, "type": "DATASET", "confidence": 0.8393655300140381}]}, {"text": "Since we believe annotators create more specific labels compared to the gold labels, we use sublabels as simulated user labels and labels as gold labels (we give examples of labels and sub-labels in Section 4.1).", "labels": [], "entities": []}, {"text": "We start with two randomly selected documents that have different sub-labels, assign the corresponding sub-labels, then add more labels based on each condition's preference function (Section 3.3).", "labels": [], "entities": []}, {"text": "We follow the condition's preference function and incrementally add labels until 100 documents have been labeled (100 documents are representative of what a human can label in about an hour).", "labels": [], "entities": []}, {"text": "Given these labels, we compute purity, RI, and NMI overtime.", "labels": [], "entities": [{"text": "purity", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.998663067817688}, {"text": "RI", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.996465802192688}, {"text": "NMI overtime", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9158905148506165}]}, {"text": "This procedure is repeated fifteen times (to account for the randomness of initial document selections and the preference functions with randomness).", "labels": [], "entities": []}, {"text": "11 Synthetic experiment data available at http: //github.com/Pinafore/publications/tree/ Congress (Synth) Newsgroups (Synth) q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Synthetic results validate our hypothesis that topic overview and active learning selection can help label a corpus more efficiently.", "labels": [], "entities": [{"text": "topic overview", "start_pos": 292, "end_pos": 306, "type": "TASK", "confidence": 0.758968323469162}]}, {"text": "LA shows early gains, but tends to falter eventually compared to both topic overview and topic overview combined with active learning selection (TR and TA).", "labels": [], "entities": [{"text": "LA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9885359406471252}]}, {"text": "However, these experiments do not validate ALTO.", "labels": [], "entities": [{"text": "ALTO", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9224487543106079}]}, {"text": "Not all documents require the same time or effort to label, and active learning focuses on the hardest examples, which may confuse users.", "labels": [], "entities": []}, {"text": "Thus, we need to evaluate how effectively actual users annotate a collection's documents.", "labels": [], "entities": []}, {"text": "We analyze the data by dividing the forty-minute labeling task into five minute intervals.", "labels": [], "entities": []}, {"text": "If a participant stops before the time limit, we consider their final dataset to stay the same for any remaining intervals.", "labels": [], "entities": []}, {"text": "shows the measures across study conditions, with similar trends for all three measures.", "labels": [], "entities": []}, {"text": "Topic model overview and active learning both significantly improve final dataset measures.", "labels": [], "entities": [{"text": "Topic model overview", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.720854659875234}]}, {"text": "The topic overview and active selection conditions significantly outperform the list overview and random selection, respectively, on the final label quality metrics.", "labels": [], "entities": []}, {"text": "shows the results of separate 2 \u00d7 2 ANOVAs with ART with each of final purity, RI, and NMI scores.", "labels": [], "entities": [{"text": "ART", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9935463070869446}, {"text": "purity", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.7983353734016418}, {"text": "RI", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9860358238220215}, {"text": "NMI", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9907981157302856}]}, {"text": "There are significant main effects of overview and selection on all three metrics; no interaction effects were significant.", "labels": [], "entities": [{"text": "overview", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7242028117179871}]}, {"text": "Topic models by themselves outperform traditional active learning strategies ().", "labels": [], "entities": []}, {"text": "LA performs better than LR; while active learning was useful, it was not as useful as the topic model overview (TR and TA).", "labels": [], "entities": [{"text": "LA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9652336239814758}]}, {"text": "LA provides an initial benefit.", "labels": [], "entities": [{"text": "LA", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.874298095703125}]}, {"text": "Average purity, NMI and RI were highest with LA for the earliest labeling time intervals.", "labels": [], "entities": [{"text": "purity", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9778934717178345}, {"text": "NMI", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.9912939667701721}, {"text": "RI", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9958414435386658}, {"text": "LA", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9992577433586121}]}, {"text": "Thus, when time is very  limited, using traditional active learning (LA) is preferable to topic overviews; users need time to explore the topics and a subset of documents within them.", "labels": [], "entities": []}, {"text": "shows the metrics after ten minutes.", "labels": [], "entities": [{"text": "metrics", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.955116868019104}]}, {"text": "Separate 2 \u00d7 2 ANOVAs with ART on the means of purity, NMI and RI revealed a significant interaction effect between overview and selection on mean NMI (F (1, 36) = 5.58, p = .024), confirming the early performance trends seen in at least for NMI.", "labels": [], "entities": [{"text": "ART", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.8198941349983215}, {"text": "RI", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9938362240791321}, {"text": "overview", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9695172309875488}, {"text": "F", "start_pos": 152, "end_pos": 153, "type": "METRIC", "confidence": 0.9900916814804077}]}, {"text": "No other main or interaction effects were significant, likely due to low statistical power.", "labels": [], "entities": []}, {"text": "shows the average scores given for the six NASA-TLX questions in different conditions.", "labels": [], "entities": []}, {"text": "Separate 2 \u00d7 2 ANOVA with ART for each of the measures revealed only one significant result: participants who used the topic model overview find the task to be significantly less frustrating (M = 4.2 and median = 2) than those who used the list overview (M = 7.3 and median = 6.5) on a scale from 1 (low frustration) to 20 (high frustration) (F (1, 36) = 4.43, p = .042), confirming that the topic overview helps users organize their thoughts and experience less stress during labeling.", "labels": [], "entities": [{"text": "ART", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8589182496070862}, {"text": "F", "start_pos": 343, "end_pos": 344, "type": "METRIC", "confidence": 0.9690456986427307}]}, {"text": "Participants in the TA and TR conditions rate topic information to be useful in completing the task (M = 5.0 and median = 5) on a scale from 1 (not useful at all) to 7 (very useful).", "labels": [], "entities": [{"text": "TA", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.6568186283111572}, {"text": "M", "start_pos": 101, "end_pos": 102, "type": "METRIC", "confidence": 0.9858152866363525}]}, {"text": "Overall, users are positive about their experience with the system.", "labels": [], "entities": []}, {"text": "Participants in all conditions rate overall satisfaction with the interface positively (M = 5.8 and median = 6) on a scale from 1 (not satisfied at all) to 7 (very satisfied).", "labels": [], "entities": [{"text": "M", "start_pos": 88, "end_pos": 89, "type": "METRIC", "confidence": 0.992021918296814}]}, {"text": "One can argue that using topic overviews for labeling could have a negative effect: users may ignore the document content and focus on topics for labeling.", "labels": [], "entities": []}, {"text": "We tried to avoid this issue by making it clear in the instructions that they need to focus on document content and use topics as a guidance.", "labels": [], "entities": []}, {"text": "On average, the participants in TR create 1.96 labels per topic and the participants in TA created 2.26 labels per topic.", "labels": [], "entities": []}, {"text": "This suggests that participants are going beyond what they see in topics for labeling, at least in the TA condition.", "labels": [], "entities": [{"text": "TA", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.6063125729560852}]}, {"text": "Section 6.2 compares clusters of documents in different conditions against the gold clusters but does not evaluate the quality of the labels themselves.", "labels": [], "entities": []}, {"text": "Since one of the main contributions of ALTO is to accelerate inducing a high quality label set, we use crowdsourcing to assess how the final induced label sets compare in different conditions.", "labels": [], "entities": [{"text": "ALTO", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.7096276879310608}]}, {"text": "For completeness, we also compare labels against a fully automatic labeling method) that does not require human intervention.", "labels": [], "entities": []}, {"text": "We assign automatic labels to documents based on their most prominent topic.", "labels": [], "entities": []}, {"text": "We ask users on a crowdsourcing platform to vote for the \"best\" and \"worst\" label that describes the content of a US congressional bill (we use Crowdflower restricted to US contributors).", "labels": [], "entities": []}, {"text": "Five users label each document and we use the aggregated results generated by Crowdflower.", "labels": [], "entities": [{"text": "Crowdflower", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.9260370135307312}]}, {"text": "The user gets $0.20 for each task.", "labels": [], "entities": []}, {"text": "We randomly choose 200 documents from our dataset (Section 4.1).", "labels": [], "entities": []}, {"text": "For each chosen document, we randomly choose a participant from all four conditions (TA, TR, LA, LR).", "labels": [], "entities": [{"text": "LA", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.8612649440765381}]}, {"text": "The labels assigned in different conditions and the automatic label of the document's prominent topic construct the candidate labels for the document.", "labels": [], "entities": []}, {"text": "Identical labels are merged into one label to avoid showing duplicate labels to users.", "labels": [], "entities": []}, {"text": "If a merged label gets a \"best\" or \"worst\" vote, we split that vote across all the identical instances.", "labels": [], "entities": []}, {"text": "16 shows the average number of \"best\" and \"worst\" votes for each condition and the automatic method.", "labels": [], "entities": []}, {"text": "ALTO (TA) receives the most \"best\" votes and the fewest \"worst\" votes.", "labels": [], "entities": [{"text": "ALTO (TA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5080181459585825}]}, {"text": "LR receives the most worst votes.", "labels": [], "entities": [{"text": "LR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.6399861574172974}]}, {"text": "The automatic labels, interestingly, appear to do at least as well as the list view labels, with a similar number of best votes and fewer worst votes.", "labels": [], "entities": []}, {"text": "This indicates that automatic labels have reasonable quality compared to at least some manually generated labels.", "labels": [], "entities": []}, {"text": "However, when users are provided with a topic model overview-  with or without active learning selection-they can generate label sets that improve upon automatic labels and labels assigned without the topic model overview.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results from 2 \u00d7 2 ANOVA with ART  analyses on the final purity, RI, and NMI metrics.  Only main effects for the factors of overview and  selection are shown; no interaction effects were  statistically significant. Topics and active learning  both had significant effects on quality scores.", "labels": [], "entities": [{"text": "purity", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9499867558479309}, {"text": "RI", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9665035605430603}, {"text": "NMI", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9065272808074951}, {"text": "overview", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9117892384529114}]}]}