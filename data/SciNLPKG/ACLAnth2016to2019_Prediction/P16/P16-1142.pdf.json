{"title": [{"text": "Beyond Plain Spatial Knowledge: Determining Where Entities Are and Are Not Located, and For How Long", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper complements semantic role representations with spatial knowledge beyond indicating plain locations.", "labels": [], "entities": []}, {"text": "Namely, we extract where entities are (and are not) located, and for how long (seconds, hours, days, etc.).", "labels": [], "entities": []}, {"text": "Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts.", "labels": [], "entities": []}, {"text": "Experimental results show that the task can be automated.", "labels": [], "entities": []}], "introductionContent": [{"text": "Extracting meaning from text is crucial for true text understanding and an important component of several natural language processing systems.", "labels": [], "entities": [{"text": "Extracting meaning from text", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9041193574666977}, {"text": "text understanding", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7745245397090912}]}, {"text": "Among many others, previous efforts have focused on extracting causal relations, semantic relations between nominals (, spatial relations () and temporal relations (.", "labels": [], "entities": []}, {"text": "In terms of corpora development and automated approaches, semantic roles are one of the most studied semantic representations.", "labels": [], "entities": []}, {"text": "They have been proven useful for, among others, coreference resolution () and question answering.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9737938046455383}, {"text": "question answering", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9551449418067932}]}, {"text": "While semantic roles provide a useful semantic layer, they capture a portion of the meaning encoded in all but the simplest statements.", "labels": [], "entities": []}, {"text": "Consider the sentence in and the semantic roles of drove (solid arrows).", "labels": [], "entities": []}, {"text": "In addition to these roles, humans intuitively understand that (dashed arrow) (1) John was not located in Berlin before or during drove, (2) he was located in Berlin after drove fora short period of time (presumably, until he was done picking up the package, i.e., fora few minutes to an hour), and then left Berlin and thus (3) was not located there anymore.", "labels": [], "entities": []}, {"text": "Some of this additional spatial knowledge is inherent to the motion verb drive: people cannot drive to the location where they are currently located, and they will be located at the destination of driving after driving takes place.", "labels": [], "entities": []}, {"text": "But determining for how long the agent of drive remains at the destination depends on the arguments of drive: from AGENT v [after an exhausting work day] TIME , it is reasonable to believe that John will be located at home overnight.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.989969789981842}, {"text": "TIME", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.8426732420921326}]}, {"text": "This paper manipulates semantic roles in order to extract temporally-anchored spatial knowledge.", "labels": [], "entities": []}, {"text": "We extract where entities are and are not located, and temporally anchor this information.", "labels": [], "entities": []}, {"text": "Temporal anchors indicate for how long something is (or is not) located somewhere, e.g., for 5 minutes before (or after) an event.", "labels": [], "entities": [{"text": "Temporal anchors", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5635226666927338}]}, {"text": "We target additional spatial knowledge not only between arguments of motion verbs as exemplified above, but also between intra-sentential arguments of any verb.", "labels": [], "entities": []}, {"text": "The main contributions are: (1) crowdsourced annotations on top of OntoNotes 1 indicating where something is and is not located (polarity), and for how long (temporal anchors); (2) detailed annotation analysis using coarse-and fine-grained labels (yes / no vs. seconds, minutes, years, etc.); and (3) experiments detailing results with several feature combinations, and using gold-standard and predicted linguistic information.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present results using gold-standard (Section 6.1) and predicted (Section 6.2) linguistic annotations.", "labels": [], "entities": []}, {"text": "POS tags, parse trees, named entities and semantic roles are taken directly from gold or auto files in the CoNLL-2011 Shared Task release.", "labels": [], "entities": [{"text": "CoNLL-2011 Shared Task release", "start_pos": 107, "end_pos": 137, "type": "DATASET", "confidence": 0.8406453281641006}]}], "tableCaptions": [{"text": " Table 1: Percentage of fine-grained labels for instances annotated with coarse-grained label yes.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9765423536300659}]}, {"text": " Table 2: Weighed Pearson correlations between annotators and the majority label, and percentage of  instances for which at least 7, 6, 5, 4 and 3 annotators (out of 7) agree.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 18, "end_pos": 38, "type": "METRIC", "confidence": 0.9381327629089355}]}, {"text": " Table 5: Results obtained with gold-standard linguistic annotations and coarse-grained labels using the  baseline and several feature combinations (basic, lexical, heads and semantic features).", "labels": [], "entities": []}, {"text": " Table 7: Results obtained with predicted linguistic annotations and coarse-grained labels. spurious is  a new label indicating overgenerated pairs not present in the gold standard.", "labels": [], "entities": [{"text": "spurious", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9641981720924377}]}, {"text": " Table 6: Results obtained with gold linguistic an- notations and fine-grained labels using all features.", "labels": [], "entities": []}]}