{"title": [{"text": "N -gram language models for massively parallel devices", "labels": [], "entities": []}], "abstractContent": [{"text": "For many applications, the query speed of N-gram language models is a computational bottleneck.", "labels": [], "entities": []}, {"text": "Although massively parallel hardware like GPUs offer a potential solution to this bottleneck, exploiting this hardware requires a careful rethinking of basic algorithms and data structures.", "labels": [], "entities": []}, {"text": "We present the first language model designed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency.", "labels": [], "entities": []}, {"text": "Compared with a single-threaded instance of KenLM (Heafield, 2011), a highly optimized CPU-based language model, our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task.", "labels": [], "entities": []}, {"text": "When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures.", "labels": [], "entities": []}, {"text": "Our implementation is freely available at https://github.com/XapaJIaMnu/gLM", "labels": [], "entities": []}], "introductionContent": [{"text": "N -gram language models are ubiquitous in speech and language processing applications such as machine translation, speech recognition, optical character recognition, and predictive text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7919624447822571}, {"text": "speech recognition", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7475249767303467}, {"text": "optical character recognition", "start_pos": 135, "end_pos": 164, "type": "TASK", "confidence": 0.670269101858139}, {"text": "predictive text", "start_pos": 170, "end_pos": 185, "type": "TASK", "confidence": 0.9041195213794708}]}, {"text": "Because they operate overlarge vocabularies, they are often a computational bottleneck.", "labels": [], "entities": []}, {"text": "For example, in machine translation, estimates that decoding a single sentence requires a million language model queries, and estimate that this accounts for more than 50% of decoding CPU time.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7741682529449463}]}, {"text": "To address this problem, we turn to massively parallel hardware architectures, exempli- fied by general purpose graphics processing units (GPUs), whose memory bandwidth and computational throughput has rapidly outpaced that of CPUs over the last decade).", "labels": [], "entities": []}, {"text": "Exploiting this increased power is a tantalizing prospect for any computation-bound problem, so GPUs have begun to attract attention in natural language processing, in problems such as parsing), speech recognition (, and phrase extraction for machine translation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 185, "end_pos": 192, "type": "TASK", "confidence": 0.9684721827507019}, {"text": "speech recognition", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.7883712351322174}, {"text": "phrase extraction", "start_pos": 221, "end_pos": 238, "type": "TASK", "confidence": 0.7853887677192688}, {"text": "machine translation", "start_pos": 243, "end_pos": 262, "type": "TASK", "confidence": 0.7482202053070068}]}, {"text": "As these efforts have shown, it is not trivial to exploit this computational power, because the GPU computational model rewards data parallelism, minimal branching, and minimal access to global memory, patterns ignored by many classic NLP algorithms (Section 2).", "labels": [], "entities": []}, {"text": "We present the first language model data structure designed for this computational model.", "labels": [], "entities": []}, {"text": "Our data structure is a trie in which individual nodes are represented by B-trees, which are searched in parallel (Section 3) and arranged compactly in memory.", "labels": [], "entities": []}, {"text": "Our experiments across a range of parameters in a batch query setting show that this design achieves a throughput six times higher than, a highly efficient CPU implementation (Section 5).", "labels": [], "entities": []}, {"text": "They also show the effects of device saturation and of data structure design decisions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared our open-source GPU language model gLM with the CPU language model).", "labels": [], "entities": []}, {"text": "45 KenLM can use two quite different language model data structures: a fast probing hash table, and a more compact but slower trie, which inspired our own language model design.", "labels": [], "entities": []}, {"text": "Except where noted, our B-tree node size K = 31, and we measure throughput in terms of query speed, which does not include the cost of initializing or copying data structures, or the cost of moving data to or from the GPU.", "labels": [], "entities": [{"text": "GPU", "start_pos": 218, "end_pos": 221, "type": "DATASET", "confidence": 0.947725772857666}]}, {"text": "We performed our GPU experiments on an Nvidia Geforce GTX, a state-of-the-art GPU, released in the first quarter of 2015 and costing 1000 USD.", "labels": [], "entities": [{"text": "Nvidia Geforce GTX", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.9184762438138326}]}, {"text": "Our CPU experiments were performed on two different devices: one for single-threaded tests and one for multi-threaded tests.", "labels": [], "entities": []}, {"text": "For the singlethreaded CPU tests, we used an Intel Quad Core i7 4720HQ CPU released in the first quarter of 2015, costing 280 USD, and achieving 85% of the speed of a state-of-the-art consumer-grade CPU when single-threaded.", "labels": [], "entities": []}, {"text": "For the multi-threaded CPU tests we used two Intel Xeon E5-2680 CPUs, offering a combined 16 cores and 32 threads, costing at the time of their release 3,500 USD together.", "labels": [], "entities": []}, {"text": "Together, their performance specifications are similar to the recently released Intel Xeon E5-2698 v3 (16 cores, 32 threads, costing 3,500USD).", "labels": [], "entities": []}, {"text": "The different CPU configurations are favorable to the CPU implementation in their tested condition: the consumer-grade CPU has higher clock speeds in single-threaded mode than the professional-grade CPU; while the professional-grade CPUs provide many more cores (though at lower clock speeds) when fully saturated.", "labels": [], "entities": []}, {"text": "Except where noted, CPU throughput is reported for the single-threaded condition.", "labels": [], "entities": []}, {"text": "Except where noted, our language model is the Moses 3.0 release English 5-gram language model, containing 88 million n-grams.", "labels": [], "entities": [{"text": "Moses 3.0 release English 5-gram language model", "start_pos": 46, "end_pos": 93, "type": "DATASET", "confidence": 0.7721894127982003}]}, {"text": "Our benchmark task computes perplexity on data extracted from the Common Crawl dataset used for the 2013 Workshop on Machine Translation, which contains 74 million words across 3.2 million sentences.", "labels": [], "entities": [{"text": "Common Crawl dataset used for the 2013 Workshop on", "start_pos": 66, "end_pos": 116, "type": "DATASET", "confidence": 0.8710475365320841}, {"text": "Machine Translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.820594310760498}]}, {"text": "Both gLM and KenLM produce identical perplexities, so we are certain that our implementation is correct.", "labels": [], "entities": []}, {"text": "Except where noted, the faster KenLM Probing backend is used.", "labels": [], "entities": [{"text": "KenLM Probing backend", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.6975417534510294}]}, {"text": "The perplexity task has been used as a basic test of other language model implementations ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Latency (in clock cycles) and size of dif- ferent GPU memory types. Estimates are adapted  from Nvidia Corporation (2015) and depend on  several aspects of hardware configuration.", "labels": [], "entities": [{"text": "Latency", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9948701858520508}, {"text": "Estimates", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9939549565315247}, {"text": "Nvidia Corporation (2015)", "start_pos": 106, "end_pos": 131, "type": "DATASET", "confidence": 0.9360235571861267}]}]}