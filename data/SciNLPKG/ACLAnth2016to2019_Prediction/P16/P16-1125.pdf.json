{"title": [{"text": "Larger-Context Language Modelling with Recurrent Neural Network *", "labels": [], "entities": [{"text": "Larger-Context Language Modelling", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6075853109359741}]}], "abstractContent": [{"text": "In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling.", "labels": [], "entities": []}, {"text": "We call this larger-context language model.", "labels": [], "entities": []}, {"text": "We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other.", "labels": [], "entities": []}, {"text": "Through the evaluation on four corpora (IMDB, BBC, Penn TreeBank, and Fil9), we demonstrate that the proposed model improves per-plexity significantly.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.9458349347114563}, {"text": "BBC", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9493970274925232}, {"text": "Penn TreeBank", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.9825776815414429}, {"text": "Fil9", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.8476603031158447}]}, {"text": "In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM.", "labels": [], "entities": []}, {"text": "By analyzing the trained larger-context language model, we discover that content words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences.", "labels": [], "entities": []}, {"text": "This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of language modelling is to estimate the probability distribution of various linguistic units, e.g., words, sentences).", "labels": [], "entities": [{"text": "language modelling", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.711239829659462}]}, {"text": "Among the earliest techniques were count-based n-gram language models which intend to assign the probability distribution of a given word observed af- * Recently, () independently proposed a similar approach.", "labels": [], "entities": []}, {"text": "ter a fixed number of previous words.", "labels": [], "entities": []}, {"text": "Later proposed feed-forward neural language model, which achieved substantial improvements in perplexity over count-based language models.", "labels": [], "entities": []}, {"text": "showed that this neural language model could simultaneously learn the conditional probability of the latest word in a sequence as well as a vector representation for each word in a predefined vocabulary.", "labels": [], "entities": []}, {"text": "Recently recurrent neural networks have become one of the most widely used models in language modelling (.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8166791200637817}]}, {"text": "Long short-term memory unit (LSTM,) is one of the most common recurrent activation function.", "labels": [], "entities": [{"text": "Long short-term memory unit (LSTM", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.7492218712965647}]}, {"text": "Architecturally, the memory state and output state are explicitly separated by activation gates such that the vanishing gradient and exploding gradient problems described in is avoided.", "labels": [], "entities": []}, {"text": "Motivated by such gated model, a number of variants of RNNs (e.g.,) have been designed to easily capture long-term dependencies.", "labels": [], "entities": []}, {"text": "When modelling a corpus, these language models assume the mutual independence among sentences, and the task is often reduced to assigning a probability to a single sentence.", "labels": [], "entities": []}, {"text": "In this work, we propose a method to incorporate corpus-level discourse dependency into neural language model.", "labels": [], "entities": []}, {"text": "We call this larger-context language model.", "labels": [], "entities": []}, {"text": "It models the influence of context by defining a conditional probability in the form of P (w n |w 1:n\u22121 , S), where w 1 , ..., w n are words from the same sentence, and S represents the context which consists a number of previous sentences of arbitrary length.", "labels": [], "entities": []}, {"text": "We evaluated our model on four different corpora.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that the proposed larger-context language model improve perplex-ity for sentences, significantly reducing per-word perplexity compared to the language models without context information.", "labels": [], "entities": []}, {"text": "Further, through Part-OfSpeech tag analysis, we discovered that content words, including nouns, adjectives and verbs, benefit the most from increasing number of context sentences.", "labels": [], "entities": []}, {"text": "Such discovery led us to the conclusion that larger-context language model improves the unconditional language model by capturing the theme of a document.", "labels": [], "entities": []}, {"text": "To achieve such improvement, we proposed a late fusion approach, which is a modification to the LSTM such that it better incorporates the discourse context from preceding sentences.", "labels": [], "entities": []}, {"text": "In the experiments, we evaluated the proposed approach against early fusion approach with various numbers of context sentences, and demonstrated the late fusion is superior to the early fusion approach.", "labels": [], "entities": []}, {"text": "Our model explores another aspect of contextdependent recurrent language model.", "labels": [], "entities": []}, {"text": "It is novel in that it also provides an insightful way to feed information into LSTM unit, which could benefit all encoder-decoder based applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed larger-context language model on three different corpora.", "labels": [], "entities": []}, {"text": "For detailed statistics, see.", "labels": [], "entities": []}, {"text": "IMDB Movie Reviews A set of movie reviews is an ideal dataset to evaluate many different settings of the proposed larger-context language models, because each review is highly likely of a single theme (the movie under review.)", "labels": [], "entities": [{"text": "IMDB Movie Reviews", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9231958190600077}]}, {"text": "A set of words or the style of writing will be well determined based on the preceding sentences.", "labels": [], "entities": []}, {"text": "We use the IMDB Movie Review Corpus (IMDB) prepared by.", "labels": [], "entities": [{"text": "IMDB Movie Review Corpus (IMDB)", "start_pos": 11, "end_pos": 42, "type": "DATASET", "confidence": 0.8878099407468524}]}, {"text": "This corpus has 75k training reviews and 25k test reviews.", "labels": [], "entities": []}, {"text": "We use the 30k most frequent words in the training corpus for recurrent language models.", "labels": [], "entities": []}, {"text": "BBC Similarly to movie reviews, each new article tends to convey a single theme.", "labels": [], "entities": [{"text": "BBC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9515371918678284}]}, {"text": "We use the BBC corpus prepared by.", "labels": [], "entities": [{"text": "BBC corpus prepared", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.9891560475031534}]}, {"text": "Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style.", "labels": [], "entities": [{"text": "IMDB corpus", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8172083497047424}]}, {"text": "By evaluating the proposed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9373182058334351}, {"text": "BBC corpora", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9236252009868622}]}, {"text": "We use the 10k most frequent words in the training corpus for recurrent language models.", "labels": [], "entities": []}, {"text": "Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9372503161430359}, {"text": "BBC corpora", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9594630599021912}]}, {"text": "Penn Treebank We evaluate a normal recurrent language model, count-based n-gram language model as well as the proposed RLM-BoW-EF-n and RLM-BoW-LF-n with varying n = 1, 2, 4, 8 on the Penn Treebank Corpus.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9923116862773895}, {"text": "Penn Treebank Corpus", "start_pos": 184, "end_pos": 204, "type": "DATASET", "confidence": 0.9971954425175985}]}, {"text": "We preprocess the corpus according to) and use a vocabulary of 10k words from the training corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of IMDB, BBC, Penn TreeBank and Fil9.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8910704255104065}, {"text": "BBC", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9822501540184021}, {"text": "Penn TreeBank", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9863819777965546}, {"text": "Fil9", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8381696343421936}]}]}