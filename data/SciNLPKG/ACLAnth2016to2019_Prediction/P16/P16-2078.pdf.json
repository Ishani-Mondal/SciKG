{"title": [{"text": "Improving Argument Overlap for Proposition-Based Summarisation", "labels": [], "entities": [{"text": "Improving Argument Overlap", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7757829229036967}, {"text": "Proposition-Based Summarisation", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.8367623686790466}]}], "abstractContent": [{"text": "We present improvements to our in-cremental proposition-based summariser, which is inspired by Kintsch and van Dijk's (1978) text comprehension model.", "labels": [], "entities": []}, {"text": "Argument overlap is a central concept in this summariser.", "labels": [], "entities": [{"text": "Argument overlap", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8623841404914856}]}, {"text": "Our new model replaces the old overlap method based on distribu-tional similarity with one based on lexical chains.", "labels": [], "entities": []}, {"text": "We evaluate on anew corpus of 124 summaries of educational texts, and show that our new system outper-forms the old method and several state-of-the-art non-proposition-based summar-isers.", "labels": [], "entities": []}, {"text": "The experiment also verifies that the incremental nature of memory cycles is beneficial in itself, by comparing it to a non-incremental algorithm using the same underlying information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic summarisation is one of the big artificial intelligence challenges in a world of information overload.", "labels": [], "entities": [{"text": "Automatic summarisation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7221522629261017}]}, {"text": "Many summarisers, mostly extractive, have been developed in recent years ().", "labels": [], "entities": [{"text": "summarisers", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9640634059906006}]}, {"text": "Research is moving beyond extraction in various directions: One could perform text manipulation such as compression as a separate step after extraction, or alternatively, one could base a summary on an internal semantic representation such as the proposition.", "labels": [], "entities": []}, {"text": "One summarisation model that allows manipulation of semantic structures of texts was proposed by.", "labels": [], "entities": []}, {"text": "It is a model of human text processing, where the text is turned into propositions and processed incrementally, sentence by sentence.", "labels": [], "entities": [{"text": "human text processing", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6825928191343943}]}, {"text": "The final summary is based on those propositions whose semantic participants (arguments) are wellconnected to others in the text and hence likely to be remembered by a human reading the text, under the assumption of memory limitations.", "labels": [], "entities": []}, {"text": "Such a deep model is attractive because it provides the theoretical possibility of performing inference and generalisation over propositions, even if current NLP technology only supports shallow versions of such manipulations.", "labels": [], "entities": []}, {"text": "This gives it a clear theoretical advantage over nonpropositional extraction systems whose information units are individual words and their connections, e.g. centroids or random-walk models.", "labels": [], "entities": []}, {"text": "We present in this paper anew KvD-based summariser that is word sense-aware, unlike our earlier implementation).", "labels": [], "entities": []}, {"text": "\u00a72 explains the KvD model with respect to summarisation.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9713385701179504}]}, {"text": "\u00a73 and \u00a74 explain why and how we use lexical chains to model argument overlap, a phenomenon which is central to KvD-style summarisation.", "labels": [], "entities": [{"text": "KvD-style summarisation", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.6207442879676819}]}, {"text": "\u00a76 presents experimental evidence that our model of argument overlap is superior to the earlier one.", "labels": [], "entities": [{"text": "argument overlap", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7133901417255402}]}, {"text": "Our summariser additionally beats several extractive state-of-the-art summarisers.", "labels": [], "entities": [{"text": "summariser", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9357999563217163}]}, {"text": "We show that this advantage does not come from our use of lexical chains alone, but also from KvD's incremental processing.", "labels": [], "entities": []}, {"text": "Our second contribution concerns anew corpus of educational texts, presented in \u00a75.", "labels": [], "entities": []}, {"text": "Part of the reason why we prefer a genre other than news is the vexingly good performance of the lead baseline in the news genre.", "labels": [], "entities": []}, {"text": "Traditionally, many summarisers struggled to beat this baseline (.", "labels": [], "entities": []}, {"text": "We believe that the problem is partly due to the journalistic style, which calls for an abstract-like lead.", "labels": [], "entities": []}, {"text": "If we want to measure the content selection ability of summarisers, alternat- ive data sets are needed.", "labels": [], "entities": [{"text": "alternat- ive data sets", "start_pos": 68, "end_pos": 91, "type": "DATASET", "confidence": 0.6242517173290253}]}, {"text": "Satisfyingly, we find that on our corpus the lead baseline is surpassable by intelligent summarisers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distance of lexical relations.", "labels": [], "entities": []}, {"text": " Table 2: ROUGE F-scores by four metrics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9901002049446106}, {"text": "F-scores", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8272756338119507}]}]}