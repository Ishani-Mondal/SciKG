{"title": [{"text": "Deep Fusion LSTMs for Text Semantic Matching", "labels": [], "entities": [{"text": "Text Semantic Matching", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6825426717599233}]}], "abstractContent": [{"text": "Recently, there is rising interest in modelling the interactions of text pair with deep neural networks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model of deep fusion LSTMs (DF-LSTMs) to model the strong interaction of text pair in a recursive matching way.", "labels": [], "entities": []}, {"text": "Specifically, DF-LSTMs consist of two interdependent LSTMs, each of which models a sequence under the influence of another.", "labels": [], "entities": []}, {"text": "We also use external memory to increase the capacity of LSTMs, thereby possibly capturing more complicated matching patterns.", "labels": [], "entities": []}, {"text": "Experiments on two very large datasets demonstrate the efficacy of our proposed architecture.", "labels": [], "entities": []}, {"text": "Furthermore, we present an elaborate qualitative analysis of our models, giving an intuitive understanding how our model worked.", "labels": [], "entities": []}], "introductionContent": [{"text": "Among many natural language processing (NLP) tasks, such as text classification, question answering and machine translation, a common problem is modelling the relevance/similarity of a pair of texts, which is also called text semantic matching.", "labels": [], "entities": [{"text": "text classification", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7581422924995422}, {"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8696231245994568}, {"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7405125796794891}, {"text": "text semantic matching", "start_pos": 221, "end_pos": 243, "type": "TASK", "confidence": 0.7216505805651346}]}, {"text": "Due to the semantic gap problem, text semantic matching is still a challenging problem.", "labels": [], "entities": [{"text": "text semantic matching", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8125613133112589}]}, {"text": "Recently, deep learning is rising a substantial interest in text semantic matching and has achieved some great progresses (.", "labels": [], "entities": [{"text": "text semantic matching", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7701554497083029}]}, {"text": "According to their interaction ways, previous models can be classified into three categories: Weak interaction Models Some early works focus on sentence level interactions, such as ARC-I(), CNTN(  Semi-interaction Models Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN ( , Attention LSTM ().", "labels": [], "entities": []}, {"text": "These models can alleviate the weak interaction problem to some extent.", "labels": [], "entities": []}, {"text": "Strong Interaction Models Some models build the interaction at different granularity (word, phrase and sentence level), such as ARC-II (),, Multi-Perspective CNN (, MV-LSTM ( ), MatchPyramid ( ).", "labels": [], "entities": [{"text": "ARC-II", "start_pos": 128, "end_pos": 134, "type": "DATASET", "confidence": 0.7809085845947266}]}, {"text": "The final matching score depends on these different levels of interactions.", "labels": [], "entities": [{"text": "matching", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8528395295143127}]}, {"text": "In this paper, we adopt a deep fusion strategy to model the strong interactions of two sentences.", "labels": [], "entities": []}, {"text": "Given two texts x 1:m and y 1:n , we define a matching vector h i,j to represent the interaction of the subsequences x 1:i and y 1:j . h i,j depends on the matching vectors h s,t on previous interactions 1 \u2264 s < i and 1 \u2264 t < j.", "labels": [], "entities": []}, {"text": "Thus, text matching can be regarded as modelling the interaction of two texts in a recursive matching way.", "labels": [], "entities": [{"text": "text matching", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.8039597272872925}]}, {"text": "Following this idea, we propose deep fusion long short-term memory neural networks (DFLSTMs) to model the interactions recursively.", "labels": [], "entities": []}, {"text": "More concretely, DF-LSTMs consist of two interconnected conditional LSTMs, each of which models apiece of text under the influence of another.", "labels": [], "entities": []}, {"text": "The output vector of DF-LSTMs is fed into a task-specific output layer to compute the match- ing score.", "labels": [], "entities": [{"text": "match- ing score", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9047160595655441}]}, {"text": "The contributions of this paper can be summarized as follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we investigate the empirical performances of our proposed model on two different text matching tasks: classification task (recognizing textual entailment) and ranking task (matching of question and answer).", "labels": [], "entities": [{"text": "text matching", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.7040872573852539}]}, {"text": "Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences.", "labels": [], "entities": [{"text": "Recognizing textual entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.912240207195282}]}, {"text": "We use the Stanford Natural Language Inference Corpus (SNLI).", "labels": [], "entities": [{"text": "Stanford Natural Language Inference Corpus (SNLI)", "start_pos": 11, "end_pos": 60, "type": "DATASET", "confidence": 0.8280003443360329}]}, {"text": "This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators.", "labels": [], "entities": []}, {"text": "SNLI is two orders of magnitude larger than all other existing RTE corpora.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7668592929840088}]}, {"text": "Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.7150737643241882}]}, {"text": "The results of DF-LSTMs outperform all the competitor models with the same number of hidden states while achieving comparable results to the state-of-the-art and using much fewer parameters, which indicate that it is effective to model the strong interactions of two texts in a recursive matching way.", "labels": [], "entities": []}, {"text": "Matching question answering (MQA) is atypical task for semantic matching (.", "labels": [], "entities": [{"text": "Matching question answering (MQA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8474036455154419}, {"text": "semantic matching", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7538897097110748}]}, {"text": "Given a question, we need select a correct answer from some candidate answers.", "labels": [], "entities": []}, {"text": "In this paper, we use the dataset collected from Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.8285225331783295}]}, {"text": "Answers with the getByCategory function provided in Yahoo!", "labels": [], "entities": []}, {"text": "Answers API, which produces 963, 072 questions and corresponding best answers.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7390184998512268}]}, {"text": "We then select the pairs in which the length of questions and answers are both in the interval, thus obtaining 220, 000 question answer pairs to form the positive pairs.", "labels": [], "entities": []}, {"text": "For negative pairs, we first use each question's best answer as a query to retrieval top 1, 000 re-   sults from the whole answer set with Lucene, where 4 or 9 answers will be selected randomly to construct the negative pairs.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 139, "end_pos": 145, "type": "DATASET", "confidence": 0.9010838866233826}]}, {"text": "The whole dataset 1 is divided into training, validation and testing data with proportion 20 : 1 : 1.", "labels": [], "entities": []}, {"text": "Moreover, we give two test settings: selecting the best answer from 5 and 10 candidates respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyper-parameters for our model on two  tasks.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9535257816314697}]}, {"text": " Table 2: Accuracies of our proposed model against  other neural models on SNLI corpus.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9882468581199646}, {"text": "SNLI corpus", "start_pos": 75, "end_pos": 86, "type": "DATASET", "confidence": 0.8401980996131897}]}, {"text": " Table 4: Results of our proposed model against  other neural models on Yahoo! question-answer  pairs dataset.", "labels": [], "entities": [{"text": "Yahoo! question-answer  pairs dataset", "start_pos": 72, "end_pos": 109, "type": "DATASET", "confidence": 0.9095440745353699}]}]}