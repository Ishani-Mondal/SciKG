{"title": [{"text": "Harnessing Deep Neural Networks with Logic Rules", "labels": [], "entities": [{"text": "Harnessing Deep Neural Networks", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8488550037145615}]}], "abstractContent": [{"text": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models.", "labels": [], "entities": []}, {"text": "We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules.", "labels": [], "entities": []}, {"text": "Specifically, we develop an iterative distillation method that transfers the struc-tured information of logic rules into the weights of neural networks.", "labels": [], "entities": []}, {"text": "We deploy the framework on a CNN for sentiment analysis , and an RNN for named entity recognition.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.9612914323806763}, {"text": "named entity recognition", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.6229427456855774}]}, {"text": "With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural networks provide a powerful mechanism for learning patterns from massive data, achieving new levels of performance on image classification (), speech recognition ( , machine translation (), playing strategic board games, and so forth.", "labels": [], "entities": [{"text": "image classification", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7315314561128616}, {"text": "speech recognition", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.74263034760952}, {"text": "machine translation", "start_pos": 178, "end_pos": 197, "type": "TASK", "confidence": 0.7394797652959824}]}, {"text": "Despite the impressive advances, the widelyused DNN methods still have limitations.", "labels": [], "entities": []}, {"text": "The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9163184762001038}]}, {"text": "It is also difficult to encode human intention to guide the models to capture desired patterns, without expensive direct supervision or ad-hoc initialization.", "labels": [], "entities": []}, {"text": "On the other hand, the cognitive process of human beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences.", "labels": [], "entities": []}, {"text": "Logic rules provide a flexible declarative language for communicating high-level cognition and expressing structured knowledge.", "labels": [], "entities": []}, {"text": "It is therefore desirable to integrate logic rules into DNNs, to transfer human intention and domain knowledge to neural models, and regulate the learning process.", "labels": [], "entities": []}, {"text": "In this paper, we present a framework capable of enhancing general types of neural networks, such as convolutional networks (CNNs) and recurrent networks (RNNs), on various tasks, with logic rule knowledge.", "labels": [], "entities": []}, {"text": "Combining symbolic representations with neural methods have been considered in different contexts.", "labels": [], "entities": []}, {"text": "Neural-symbolic systems () construct a network from a given rule set to execute reasoning.", "labels": [], "entities": []}, {"text": "To exploit a priori knowledge in general neural architectures, recent work augments each raw data instance with useful features ), while network training, however, is still limited to instance-label supervision and suffers from the same issues mentioned above.", "labels": [], "entities": []}, {"text": "Besides, a large variety of structural knowledge cannot be naturally encoded in the featurelabel form.", "labels": [], "entities": []}, {"text": "Our framework enables a neural network to learn simultaneously from labeled instances as well as logic rules, through an iterative rule knowledge distillation procedure that transfers the structured information encoded in the logic rules into the network parameters.", "labels": [], "entities": []}, {"text": "Since the general logic rules are complementary to the specific data labels, a natural \"side-product\" of the integration is the support for semi-supervised learning where unlabeled data is used to better absorb the logical knowledge.", "labels": [], "entities": []}, {"text": "Methodologically, our approach can be seen as a combination of the knowledge distillation () and the posterior regularization (PR) method ().", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.6905957013368607}]}, {"text": "In particular, at each iteration we adapt the posterior constraint principle from PR to construct a rule-regularized teacher, and train the student network of interest to imitate the predictions of the teacher network.", "labels": [], "entities": []}, {"text": "We leverage soft logic to support flexible rule encoding.", "labels": [], "entities": [{"text": "rule encoding", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7226032912731171}]}, {"text": "We apply the proposed framework on both CNN and RNN, and deploy on the task of sentiment analysis (SA) and named entity recognition (NER), respectively.", "labels": [], "entities": [{"text": "sentiment analysis (SA)", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.6630913734436035}, {"text": "named entity recognition (NER)", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.7803415010372797}]}, {"text": "With only a few (one or two) very intuitive rules, both the distilled networks and the joint teacher networks strongly improve over their basic forms (without rules), and achieve better or comparable performance to state-of-the-art models which typically have more parameters and complicated architectures.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to integrate logic rules with general workhorse types of deep neural networks in a principled framework.", "labels": [], "entities": []}, {"text": "The encouraging results indicate our method can be potentially useful for incorporating richer types of human knowledge, and improving other application domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We validate our framework by evaluating its applications of sentiment classification and named entity recognition on a variety of public benchmarks.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.9362520575523376}, {"text": "named entity recognition", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.623474250237147}]}, {"text": "By integrating the simple yet effective rules with 89.4 --6 CNN-multichannel 88.1 81.1 85.0 7 Paragraph-Vec ( 87.8 --8 CRF-PR ( --82.7 9 RNTN 85.4 --10 G-Dropout ( -79.0 82.1: Accuracy (%) of Sentiment Classification.) is the base network corresponding to the \"CNN-non-static\" model in).", "labels": [], "entities": [{"text": "CNN-multichannel 88.1 81.1 85.0", "start_pos": 60, "end_pos": 91, "type": "DATASET", "confidence": 0.9057208448648453}, {"text": "RNTN", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.8675478100776672}, {"text": "G-Dropout", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.5107755064964294}, {"text": "Accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9902305006980896}]}, {"text": "Rows 2-3 are the networks enhanced by our framework: CNN-Rule-p is the student network and CNN-Rule-q is the teacher network.", "labels": [], "entities": []}, {"text": "For MR and CR, we report the average accuracy\u00b1one standard deviation using 10-fold cross validation.", "labels": [], "entities": [{"text": "MR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.6427112817764282}, {"text": "CR", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9735738039016724}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9970828890800476}]}, {"text": "the base networks, we obtain substantial improvements on both tasks and achieve state-of-the-art or comparable results to previous best-performing systems.", "labels": [], "entities": []}, {"text": "Comparison with a diverse set of other rule integration methods demonstrates the unique effectiveness of our framework.", "labels": [], "entities": [{"text": "rule integration", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7719102799892426}]}, {"text": "Our approach also shows promising potentials in the semi-supervised learning and sparse data context.", "labels": [], "entities": []}, {"text": "Throughout the experiments we set the regularization parameter to C = 400.", "labels": [], "entities": []}, {"text": "In sentiment classification we set the imitation parameter to \u03c0 (t) = 1 \u2212 0.9 t , while in NER \u03c0 (t) = min{0.9, 1 \u2212 0.9 t } to downplay the noisy listing rule.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.9551525115966797}]}, {"text": "The confidence levels of rules are set to \u03bb l = 1, except for hard constraints whose confidence is \u221e.", "labels": [], "entities": []}, {"text": "For neural network configuration, we largely followed the reference work, as specified in the following respective sections.", "labels": [], "entities": [{"text": "neural network configuration", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6594522794087728}]}, {"text": "All experiments were performed on a Linux machine with eight 4.0GHz CPU cores, one Tesla K40c GPU, and 32GB RAM.", "labels": [], "entities": []}, {"text": "We implemented neural networks using Theano 2 , a popular deep learning platform.", "labels": [], "entities": [{"text": "Theano 2", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9206861257553101}]}], "tableCaptions": [{"text": " Table 2: Performance of different rule integration  methods on SST2. 1) CNN is the base network; 2)  \"-but-clause\" takes the clause after \"but\" as input; 3)  \"-2 -reg\" imposes a regularization term \u03b3\u03c3 \u03b8 (S) \u2212  \u03c3 \u03b8 (Y ) 2 to the CNN objective, with the strength  \u03b3 selected on dev set; 4) \"-project\" projects the  trained base CNN to the rule-regularized subspace  with Eq.(3); 5) \"-opt-project\" directly optimizes the  projected CNN; 6) \"-pipeline\" distills the pre-trained  \"-opt-project\" to a plain CNN; 7-8) \"-Rule-p\" and \"- Rule-q\" are our models with p being the distilled stu- dent network and q the teacher network. Note that  \"-but-clause\" and \"-2 -reg\" are ad-hoc methods ap- plicable specifically to the \"but\"-rule.", "labels": [], "entities": [{"text": "SST2", "start_pos": 64, "end_pos": 68, "type": "TASK", "confidence": 0.8796736001968384}]}, {"text": " Table 3: Accuracy (%) on SST2 with varying sizes  of labeled data and semi-supervised learning. The  header row is the percentage of labeled examples  for training. Rows 1-3 use only the supervised data.  Rows 4-6 use semi-supervised learning where the re- maining training data are used as unlabeled exam- ples. For \"-semi-PR\" we only report its projected  solution (in analogous to q) which performs better  than the non-projected one (in analogous to p).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9922189712524414}]}]}