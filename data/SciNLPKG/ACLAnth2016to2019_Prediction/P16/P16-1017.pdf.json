{"title": [{"text": "Neural Greedy Constituent Parsing with Dynamic Oracles", "labels": [], "entities": [{"text": "Neural Greedy Constituent Parsing", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6965842545032501}]}], "abstractContent": [{"text": "Dynamic oracle training has shown substantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8144408464431763}, {"text": "constituent parsing", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7874476611614227}]}, {"text": "The present article introduces a dynamic oracle for transition-based constituent parsing.", "labels": [], "entities": [{"text": "transition-based constituent parsing", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.6162295242150625}]}, {"text": "Experiments on the 9 languages of the SPMRL dataset show that a neu-ral greedy parser with morphological features , trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers.", "labels": [], "entities": [{"text": "SPMRL dataset", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.7666500210762024}]}], "introductionContent": [{"text": "Constituent parsing often relies on search methods such as dynamic programming or beam search, because the search space of all possible predictions is prohibitively large.", "labels": [], "entities": [{"text": "Constituent parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9121398329734802}]}, {"text": "In this article, we present a greedy parsing model.", "labels": [], "entities": []}, {"text": "Our main contribution is the design of a dynamic oracle for transitionbased constituent parsing.", "labels": [], "entities": [{"text": "transitionbased constituent parsing", "start_pos": 60, "end_pos": 95, "type": "TASK", "confidence": 0.6225502093633016}]}, {"text": "In NLP, dynamic oracles were first proposed to improve greedy dependency parsing training without involving additional computational costs attest time.", "labels": [], "entities": [{"text": "greedy dependency parsing", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.6750455896059672}]}, {"text": "The training of a transition-based parser involves an oracle, that is a function mapping a configuration to the best transition.", "labels": [], "entities": []}, {"text": "Transition-based parsers usually rely on a static oracle, only welldefined for gold configurations, which transforms trees into sequences of gold actions.", "labels": [], "entities": []}, {"text": "Training against a static oracle restricts the exploration of the search space to the gold sequence of actions.", "labels": [], "entities": []}, {"text": "At test time, due to error propagation, the parser will be in a very different situation than at training time.", "labels": [], "entities": []}, {"text": "It will have to infer good actions from noisy configurations.", "labels": [], "entities": []}, {"text": "To alleviate error propagation, a solution is to train the parser to predict the best action given any configuration, by allowing it to explore a greater part of the search space at train time.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7459715604782104}]}, {"text": "Dynamic oracles are non-deterministic oracles well-defined for any configuration.", "labels": [], "entities": []}, {"text": "They give the best possible transitions for any configuration.", "labels": [], "entities": []}, {"text": "Although dynamic oracles are widely used in dependency parsing and available for most standard transition systems, no dynamic oracle parsing model has yet been proposed for phrase structure grammars.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8409674167633057}, {"text": "phrase structure grammars", "start_pos": 173, "end_pos": 198, "type": "TASK", "confidence": 0.8030176758766174}]}, {"text": "The model we present aims at parsing morphologically rich languages (MRL).", "labels": [], "entities": [{"text": "parsing morphologically rich languages (MRL)", "start_pos": 29, "end_pos": 73, "type": "TASK", "confidence": 0.8273960437093463}]}, {"text": "Recent research has shown that morphological features are very important for MRL parsing.", "labels": [], "entities": [{"text": "MRL parsing", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9808173477649689}]}, {"text": "However, traditional linear models (such as the structured perceptron) need to define rather complex feature templates to capture interactions between features.", "labels": [], "entities": []}, {"text": "Additional morphological features complicate this task.", "labels": [], "entities": []}, {"text": "Instead, we propose to rely on a neural network weighting function which uses a non-linear hidden layer to automatically capture interactions between variables, and embeds morphological features in a vector space, as is usual for words and other symbols.", "labels": [], "entities": []}, {"text": "The article is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present neural transition-based parsing.", "labels": [], "entities": [{"text": "neural transition-based parsing", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.5956349074840546}]}, {"text": "Section 3 motivates learning with a dynamic oracle and presents an algorithm to do so.", "labels": [], "entities": []}, {"text": "Section 4 introduces the dynamic oracle.", "labels": [], "entities": []}, {"text": "Finally, we present parsing experiments in Section 5 to evaluate our proposal.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.969456136226654}]}], "datasetContent": [{"text": "We conducted parsing experiments to evaluate our proposal.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9652960300445557}]}, {"text": "We compare two experimental settings.", "labels": [], "entities": []}, {"text": "In the 'static' setting, the parser is trained only on gold configurations; in the 'dynamic' setting, we use the dynamic oracle and the training method in to explore non-gold configurations.", "labels": [], "entities": []}, {"text": "We used both the SPMRL dataset () in the 'predicted tag' scenario, and the Penn Treebank (, to compare our proposal to existing systems.", "labels": [], "entities": [{"text": "SPMRL dataset", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.8574592769145966}, {"text": "Penn Treebank", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9927762150764465}]}, {"text": "The tags and morphological attributes were predicted using Marmot (, by 10-fold jackknifing for the train and development sets.", "labels": [], "entities": [{"text": "Marmot", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.8482946753501892}]}, {"text": "For the SPMRL dataset, the head annotation was carried outwith the procedures described in Crabb\u00e9", "labels": [], "entities": [{"text": "SPMRL dataset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.7959599494934082}, {"text": "Crabb\u00e9", "start_pos": 91, "end_pos": 97, "type": "DATASET", "confidence": 0.7330676317214966}]}], "tableCaptions": [{"text": " Table 3: Results on the Penn Treebank (Mar- cus et al., 1993).  \u2020 use clusters or word vectors  learned on unannotated data. different architec- ture (2.3Ghz Intel), single processor.", "labels": [], "entities": [{"text": "Penn Treebank (Mar- cus et al., 1993)", "start_pos": 25, "end_pos": 62, "type": "DATASET", "confidence": 0.9162755500186573}]}, {"text": " Table 2: Results on development and test corpora. Metrics are provided by evalb spmrl with  spmrl.prm parameters (http://www.spmrl.org/spmrl2013-sharedtask.html).  \u2020 use  clusters or word vectors learned on unannotated data.  *  Bj\u00f6rkelund et al. (2013).", "labels": [], "entities": []}]}