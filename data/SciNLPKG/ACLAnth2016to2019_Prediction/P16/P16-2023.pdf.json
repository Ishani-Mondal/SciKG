{"title": [{"text": "Improving cross-domain n-gram language modelling with skipgrams", "labels": [], "entities": [{"text": "Improving cross-domain n-gram language modelling", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8508572816848755}]}], "abstractContent": [{"text": "In this paper we improve over the hierarchical Pitman-Yor processes language model in a cross-domain setting by adding skip-grams as features.", "labels": [], "entities": []}, {"text": "We find that adding skipgram features reduces the perplexity.", "labels": [], "entities": []}, {"text": "This reduction is substantial when models are trained on a generic corpus and tested on domain-specific corpora.", "labels": [], "entities": []}, {"text": "We also find that within-domain testing and cross-domain testing require different backoff strategies.", "labels": [], "entities": []}, {"text": "We observe a 30-40% reduction in perplexity in a cross-domain language modelling task, and up to 6% reduction in a within-domain experiment, for both English and Flemish-Dutch.", "labels": [], "entities": [{"text": "cross-domain language modelling task", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.6872044578194618}]}], "introductionContent": [{"text": "Since the seminal paper on hierarchical Bayesian language models based on Pitman-Yor processes), Bayesian language modelling has regained an interest.", "labels": [], "entities": [{"text": "Bayesian language modelling", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.7663386066754659}]}, {"text": "Although Bayesian language models are not new, previously proposed models were reported to be inferior compared to other smoothing methods.", "labels": [], "entities": []}, {"text": "Teh's work was the first to report on improvements over interpolated Kneser-Ney smoothing).", "labels": [], "entities": [{"text": "Kneser-Ney smoothing", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.5799967646598816}]}, {"text": "To overcome the traditional problems of overestimating the probabilities of rare occurrences and underestimating the probabilities of unseen events, a range of smoothing algorithms have been proposed in the literature).", "labels": [], "entities": []}, {"text": "Most methods take a heuristic-frequentist approach combining n-gram probabilities for various values of n, using back-off schemes or interpolation.", "labels": [], "entities": []}, {"text": "showed that MacKay and research on parametric Bayesian language models with a Dirichlet prior could be extended to give better results, but also that one of the best smoothing methods, interpolated Kneser-Ney (, can be derived as an approximation of the Hierarchical Pitman-Yor process language model (HPYLM).", "labels": [], "entities": []}, {"text": "The success of the Bayesian approach to language modelling is due to the use of statistical distributions such as the Dirichlet distribution, and distributions over distributions, such as the Dirichlet process and its two-parameter generalisation, the Pitman-Yor process.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.772838294506073}]}, {"text": "Both are widely studied in the statistics and probability theory communities.", "labels": [], "entities": []}, {"text": "Interestingly, language modelling has acquired the status of a \"fruit fly\" problem in these communities, to benchmark the performance of statistical models.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8083094358444214}]}, {"text": "In this paper we approach language modelling from a computational linguistics point of view, and consider the statistical methods to be the tool with the future goal of improving language models for extrinsic tasks such as speech recognition.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7243096977472305}, {"text": "speech recognition", "start_pos": 223, "end_pos": 241, "type": "TASK", "confidence": 0.7353504747152328}]}, {"text": "We derive our model from, and propose an extension with skipgrams.", "labels": [], "entities": []}, {"text": "A frequentist approach to language modelling with skipgrams is described by, who introduce an approach using skip-n-grams which are interpolated using modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7675397098064423}]}, {"text": "In this paper we show that a Bayesian skip-ngram approach outperforms a frequentist skip-ngram model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train 4-gram language model on the two training corpora, the Google 1 billion word benchmark and the Mediargus corpus.", "labels": [], "entities": [{"text": "Google 1 billion word benchmark", "start_pos": 64, "end_pos": 95, "type": "DATASET", "confidence": 0.7988402605056762}, {"text": "Mediargus corpus", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.9791280031204224}]}, {"text": "We do not perform any preprocessing on the data except tokenisation.", "labels": [], "entities": []}, {"text": "The models are trained with a HPYLM.", "labels": [], "entities": [{"text": "HPYLM", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.95041823387146}]}, {"text": "We do not use sentence beginning and end markers.", "labels": [], "entities": []}, {"text": "The results for the ngram backoff strategy are obtained by training without skipgrams; for limited and full we added skipgram features during training.", "labels": [], "entities": []}, {"text": "At the core of our experimental framework we use cpyp, which is an existing library for nonparametric Bayesian modelling with PY priors with histogram-based sampling.", "labels": [], "entities": []}, {"text": "This library has an example application to showcase its performance with n-gram based language modelling.", "labels": [], "entities": []}, {"text": "Limitations of the library, such as not natively supporting skipgrams, and the lack of other functionality such as thresholding and discarding of certain patterns, led us to extend the library with Colibri Core, 2 a pattern modelling library.", "labels": [], "entities": []}, {"text": "Colibri Core resolves the limitations, and together the libraries area complete language model that handles skipgrams: cococpyp.", "labels": [], "entities": [{"text": "Colibri Core", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9359271824359894}]}, {"text": "Each model is run for 50 iterations (without an explicit burn-in phase), with hyperparameters \u03b8 = 1.0 and \u03b3 = 0.8.", "labels": [], "entities": []}, {"text": "The hyperparameters are resampled every 30 iterations with slice sampling.", "labels": [], "entities": []}, {"text": "We test each model on different test sets, and we collect their intrinsic performance by means of perplexity.", "labels": [], "entities": []}, {"text": "Words in the test set: Results of the full and limited backoff systems, trained on Mediargus, tested on CGN.", "labels": [], "entities": [{"text": "Mediargus", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.989855170249939}, {"text": "CGN", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.9855704307556152}]}, {"text": "Components range from spontaneous (a) to nonspontaneous (o), with components j (news reports) and k (news) being in-domain for the training corpus, and the other components being out-ofdomain.", "labels": [], "entities": []}, {"text": "\u2193% is the relative reduction in perplexity for the column to its left. that were unseen in the training data are ignored in computing the perplexity on test data.", "labels": [], "entities": []}], "tableCaptions": []}