{"title": [{"text": "Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations", "labels": [], "entities": [{"text": "Improved Word Representations", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.6134932537873586}]}], "abstractContent": [{"text": "In this paper, we propose LexVec, anew method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence.", "labels": [], "entities": []}, {"text": "Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks.", "labels": [], "entities": [{"text": "word similarity and analogy tasks", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.80048907995224}]}], "introductionContent": [{"text": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications.", "labels": [], "entities": []}, {"text": "Traditionally, word representations have been obtained using count-based methods (, where the co-occurrence matrix is derived directly from corpus counts) or using association measures like Point-wise Mutual Information (PMI) and Positive PMI (PPMI) (.", "labels": [], "entities": []}, {"text": "Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD () and GloVe (), both achieving state-of-the-art performance on a variety of tasks.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.8196529150009155}]}, {"text": "Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (), the most notable of which is Skipgram with Negative Sampling), which uses a neural network to generate embeddings.", "labels": [], "entities": []}, {"text": "It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences ( . In this paper, we present Lexical Vectors (LexVec), a method for factorizing PPMI matrices that combines characteristics of all these methods.", "labels": [], "entities": []}, {"text": "On the one hand, it uses SGNS window sampling, negative sampling, and stochastic gradient descent (SGD) to minimize a loss function that weights frequent co-occurrences heavily but also takes into account negative co-occurrence.", "labels": [], "entities": [{"text": "SGNS window sampling", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8339913686116537}]}, {"text": "However, since PPMI generally outperforms PMI on semantic similarity tasks, rather than implicitly factorize a shifted PMI matrix (like SGNS), LexVec explicitly factorizes the PPMI matrix.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: First, we describe PPMI-SVD, GloVe, and SGNS ( \u00a72) before introducing the proposed method, LexVec ( \u00a73), and evaluating it on word similarity and analogy tasks ( \u00a74).", "labels": [], "entities": [{"text": "word similarity and analogy tasks", "start_pos": 162, "end_pos": 195, "type": "TASK", "confidence": 0.7399503886699677}]}, {"text": "We conclude with an analysis of results and discussion of future work.", "labels": [], "entities": []}, {"text": "We provide source code for the model at https://github.com/alexandres/ lexvec.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spearman rank correlation on word similarity tasks.", "labels": [], "entities": [{"text": "Spearman rank correlation", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6013254423936208}, {"text": "word similarity tasks", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7379952371120453}]}, {"text": " Table 2: Results on word analogy tasks, given as percent accuracy.", "labels": [], "entities": [{"text": "word analogy tasks", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8430546124776205}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9962834715843201}]}]}