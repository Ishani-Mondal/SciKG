{"title": [{"text": "Intrinsic Subspace Evaluation of Word Embedding Representations", "labels": [], "entities": [{"text": "Intrinsic Subspace Evaluation of Word Embedding Representations", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.7521801590919495}]}], "abstractContent": [{"text": "We introduce anew methodology for intrinsic evaluation of word representations.", "labels": [], "entities": []}, {"text": "Specifically, we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria.", "labels": [], "entities": []}, {"text": "Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points.", "labels": [], "entities": []}, {"text": "We show the limits of these point-based intrinsic evaluations.", "labels": [], "entities": []}, {"text": "We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional word representations or embeddings are currently an active area of research in natural language processing (NLP).", "labels": [], "entities": [{"text": "Distributional word representations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6786883274714152}, {"text": "natural language processing (NLP)", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.8135728041330973}]}, {"text": "The motivation for embeddings is that knowledge about words is helpful in NLP.", "labels": [], "entities": []}, {"text": "Representing words as vocabulary indexes maybe a good approach if large training sets allow us to learn everything we need to know about a word to solve a particular task; but inmost cases it helps to have a representation that contains distributional information and allows inferences like: \"above\" and \"below\" have similar syntactic behavior or \"engine\" and \"motor\" have similar meaning.", "labels": [], "entities": []}, {"text": "Several methods have been introduced to assess the quality of word embeddings.", "labels": [], "entities": []}, {"text": "We distinguish two different types of evaluation in this paper: (i) extrinsic evaluation evaluates embeddings in an NLP application or task and (ii) intrinsic evaluation tests the quality of representations independent of a specific NLP task.", "labels": [], "entities": []}, {"text": "Each single word is a combination of a large number of morphological, lexical, syntactic, semantic, discourse and other features.", "labels": [], "entities": []}, {"text": "Its embedding should accurately and consistently represent these features, and ideally a good evaluation method must clarify this and give away to analyze the results.", "labels": [], "entities": []}, {"text": "The goal of this paper is to build such an evaluation.", "labels": [], "entities": []}, {"text": "Extrinsic evaluation is a valid methodology, but it does not allow us to understand the properties of representations without further analysis; e.g., if an evaluation shows that embedding A works better than embedding B on a task, then that is not an analysis of the causes of the improvement.", "labels": [], "entities": [{"text": "Extrinsic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.777604341506958}]}, {"text": "Therefore, extrinsic evaluations do not satisfy our goals.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation analyzes the generic quality of embeddings.", "labels": [], "entities": []}, {"text": "Currently, this evaluation mostly is done by testing overall distance/similarity of words in the embedding space, i.e., it is based on viewing word representations as points and then computing full-space similarity.", "labels": [], "entities": []}, {"text": "The assumption is that the high dimensional space is smooth and similar words are close to each other.", "labels": [], "entities": []}, {"text": "Several datasets have been developed for this purpose, mostly the result of human judgement; see () for an overview.", "labels": [], "entities": []}, {"text": "We refer to these evaluations as point-based and as full-space because they consider embeddings as points in the space -sub-similarities in subspaces are generally ignored.", "labels": [], "entities": []}, {"text": "Point-based intrinsic evaluation computes a score based on the full-space similarity of two words: a single number that generally does not say anything about the underlying reasons fora lower or higher value of full-space similarity.", "labels": [], "entities": []}, {"text": "This makes it hard to interpret the results of point-based evaluation and maybe the reason that contradictory results have been published; e.g., based on point-based evaluation, some papers have claimed that count-based representations perform as well as learning-based representations ().", "labels": [], "entities": []}, {"text": "Others have claimed the opposite (e.g.,,,).", "labels": [], "entities": []}, {"text": "Given the limits of current evaluations, we propose anew methodology for intrinsic evaluation of embeddings by identifying generic fundamental criteria for embedding models that are important for representing features of words accurately and consistently.", "labels": [], "entities": []}, {"text": "We develop corpus-based tests using supervised classification that directly show whether the representations contain the information necessary to meet the criteria or not.", "labels": [], "entities": []}, {"text": "The fine-grained corpus-based supervision makes the sub-similarities of words important by looking at the subspaces of word embeddings relevant to the criteria, and this enables us to give direct insights into properties of representation models.", "labels": [], "entities": []}, {"text": "evaluate embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference.", "labels": [], "entities": [{"text": "synonym detection", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.8501678705215454}]}, {"text": "introduce tasks with more fine-grained datasets.", "labels": [], "entities": []}, {"text": "These tasks are unsupervised and generally based on cosine similarity; this means that only the overall direction of vectors is considered or, equivalently, that words are modeled as points in a space and only their fullspace distance/closeness is considered.", "labels": [], "entities": []}, {"text": "In contrast, we test embeddings in a classification setting and different subspaces of embeddings are analyzed.", "labels": [], "entities": []}, {"text": "evaluate embeddings based on their correlations with WordNetbased linguistic embeddings.", "labels": [], "entities": [{"text": "WordNetbased linguistic embeddings", "start_pos": 53, "end_pos": 87, "type": "DATASET", "confidence": 0.9014107584953308}]}, {"text": "However, correlation does not directly evaluate how accurately and completely an application can extract a particular piece of information from an embedding.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now design experiments to directly evaluate embeddings on the four criteria.", "labels": [], "entities": []}, {"text": "First, we design a probabilistic context free grammar (PCFG) that generates a corpus that is a manifestation of the underlying phenomenon.", "labels": [], "entities": []}, {"text": "Then we train our embedding models on the corpus.", "labels": [], "entities": []}, {"text": "The embeddings obtained are then evaluated in a classification setting, in which we apply a linear SVM) to classify embeddings.", "labels": [], "entities": []}, {"text": "Finally, we compare the classification results for different embedding models and analyze and summarize them.", "labels": [], "entities": []}, {"text": "Since this paper is about developing anew evaluation methodology, the choice of models is not important as long as the models can serve to show that the proposed methodology reveals interesting differences with respect to the criteria.", "labels": [], "entities": []}, {"text": "On the highest level, we can distinguish two types of distributional representations.", "labels": [], "entities": []}, {"text": "Count vectors live in a highdimensional vector space in which each dimension roughly corresponds to a (weighted) count of cooccurrence in a large corpus.", "labels": [], "entities": []}, {"text": "Learned vectors are learned from large corpora using machine learning methods: unsupervised methods such as LSI (e.g.,,) and supervised methods such as neural networks (e.g.,) and regression (e.g.,).", "labels": [], "entities": []}, {"text": "Because of the recent popularity of learning-based methods, we consider one count-based and five learning-based distributional representation models.", "labels": [], "entities": []}, {"text": "The learning-based models are: uous window model) ( ).", "labels": [], "entities": []}, {"text": "These models learn word embeddings for input and target spaces using neural network models.", "labels": [], "entities": []}, {"text": "For a given context, represented by the input space representations of the left and right neighbors v i\u22121 and v i+1 , LBL, CBOW and CWIN predict the target space vi by combining the contexts.", "labels": [], "entities": []}, {"text": "LBL combines v i\u22121 and v i+1 linearly with position dependent weights and CBOW (resp. CWIN) combines them by adding (resp. concatenation).", "labels": [], "entities": [{"text": "CBOW", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.5508052706718445}]}, {"text": "SKIP and SSKIP predict the context words v i\u22121 or v i+1 given the input space vi . For SSKIP, context words are in different spaces depending on their position to the input word.", "labels": [], "entities": [{"text": "SKIP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8418681025505066}]}, {"text": "In summary, CBOW and SKIP are learning embeddings using bag-of-word (BoW) models, but the other three, CWIN, SSKIP and LBL, are using position dependent models.", "labels": [], "entities": []}, {"text": "We use word2vec 1 for SKIP and CBOW, wang2vec 2 for SSKIP and CWIN, and's implementation 3 for LBL.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8917815089225769}]}, {"text": "The count-based model is position-sensitive PPMI, Levy and Goldberg (2014a)'s explicit vector space representation model.", "labels": [], "entities": []}, {"text": "For a vocabulary of size V , the representation w of w is a vector of size 4V , consisting of four parts corresponding to the relative positions r \u2208 {\u22122, \u22121, 1, 2} with respect to occurrences of win the corpus.", "labels": [], "entities": []}, {"text": "The entry for dimension word v in the part of w corresponding to relative position r is the PPMI (positive pointwise mutual information) weight of wand v for that relative position.", "labels": [], "entities": [{"text": "PPMI (positive pointwise mutual information) weight", "start_pos": 92, "end_pos": 143, "type": "METRIC", "confidence": 0.6162059456110001}]}, {"text": "The four parts of the vector are length normalized.", "labels": [], "entities": [{"text": "length", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9660698771476746}]}, {"text": "In this paper, we use only two relative positions: r \u2208 {\u22121, 1}, so each w has two parts, corresponding to immediate left and right neighbors.", "labels": [], "entities": []}, {"text": "To support the case for sub-space evaluation and also to introduce anew extrinsic task that uses the embeddings directly in supervised classification, we address a fine-grained entity typing task.", "labels": [], "entities": []}, {"text": "Learning taxonomic properties or types of words has been used as an evaluation method for word embeddings.", "labels": [], "entities": []}, {"text": "Since available word typing datasets are quite small (cf.,), entity typing can be a promising alternative, which enables to do supervised classification instead of unsupervised clustering.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.8062727451324463}, {"text": "supervised classification", "start_pos": 127, "end_pos": 152, "type": "TASK", "confidence": 0.692458838224411}]}, {"text": "Entities, like other words, have many properties and therefore belong to several semantic types, e.g., \"Barack Obama\" is a POLITICIAN, AUTHOR and AWARD WINNER.", "labels": [], "entities": [{"text": "POLITICIAN", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.8730356097221375}, {"text": "AUTHOR", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9705233573913574}, {"text": "AWARD", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.943349301815033}]}, {"text": "We perform entity typing by learning types of knowledge base entities from their embeddings; this requires looking at subspaces because each entity can belong to multiple types.", "labels": [], "entities": [{"text": "entity typing", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.7388786375522614}]}, {"text": "We adopt the setup of who present a dataset of Freebase entities; 5 there are 102 types (e.g., and most entities have several.", "labels": [], "entities": []}, {"text": "More specifically, we use a multilayer-perceptron (MLP) with one hidden layer to classify entity embeddings to 102 FIGER types.", "labels": [], "entities": []}, {"text": "To show the limit of point-based evaluation, we also experimentally test an entity typing model based on cosine similarity of entity embeddings.", "labels": [], "entities": []}, {"text": "To each test entity, we assign all types of the entity closest to it in the train set.", "labels": [], "entities": []}, {"text": "We call this approach 1NN (kNN fork = 1).", "labels": [], "entities": []}, {"text": "We take part of ClueWeb, which is annotated with Freebase entities using automatic annotation of FACC1 7 (), as our corpus.", "labels": [], "entities": [{"text": "FACC1 7", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9362643659114838}]}, {"text": "We then replace all mentions of entities with their Freebase identifier and learn embeddings of words and entities in the same space.", "labels": [], "entities": []}, {"text": "Our corpus has around 6 million sentences with at least one annotated entity.", "labels": [], "entities": []}, {"text": "We calculate embeddings using our different models.", "labels": [], "entities": []}, {"text": "Our hyperparameters: for learning-based models: dim=100, neg=10, iterations=20, window=1, sub=10 \u22123 ; for PPMI: SVD-dim=100, neg=1, window=1, cds=0.75, sub=10 \u22123 , eig=0.5.", "labels": [], "entities": []}, {"text": "See () for more information about the meaning of hyperparameters.", "labels": [], "entities": []}, {"text": "gives results on test for all (about 60,000 entities), head (freq > 100; about 12,200 entities) and tail (freq < 5; about 10,000 entities).", "labels": [], "entities": [{"text": "freq", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.8660039901733398}]}, {"text": "The MLP models consistently outperform 1NN on 5 cistern.cis.lmu.de/figment We tried other values of k, but results were not better.", "labels": [], "entities": []}, {"text": "7 lemurproject.org/clueweb12/FACC1 243 all and tail entities.", "labels": [], "entities": [{"text": "FACC1", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.46537941694259644}]}, {"text": "This supports our hypothesis that only part of the information about types that is present in the vectors can be determined by similarity-based methods that use the overall direction of vectors, i.e., full-space similarity.", "labels": [], "entities": []}, {"text": "There is little correlation between results of MLP and 1NN in all and head entities, and the correlation between their results in tail entities is high.", "labels": [], "entities": [{"text": "MLP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.6104434132575989}]}, {"text": "8 For example, for all entities, using 1NN, SKIP is 4.3% (4.1%) better, and using MLP is 1.7% (1.6%) worse than SSKIP (CWIN).", "labels": [], "entities": []}, {"text": "The good performance of SKIP on 1NN using cosine similarity can be related to its objective function, which maximizes the cosine similarity of cooccuring token embeddings.", "labels": [], "entities": []}, {"text": "The important question is not similarity, but whether the information about a specific type exists in the entity embeddings or not.", "labels": [], "entities": []}, {"text": "Our results confirm our previous observation that a classification by looking at subspaces is needed to answer this question.", "labels": [], "entities": []}, {"text": "In contrast, based on full-space similarity, one can infer little about the quality of embeddings.", "labels": [], "entities": []}, {"text": "Based on our results, SSKIP and CWIN embeddings contain more accurate and consistent information because MLP classifier gives better results for them.", "labels": [], "entities": []}, {"text": "However, if we considered 1NN for comparison, SKIP and CBOW would be superior.", "labels": [], "entities": [{"text": "SKIP", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.6678349375724792}, {"text": "CBOW", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.7879743576049805}]}], "tableCaptions": [{"text": " Table 1: Entity typing results using embeddings  learned with different models.", "labels": [], "entities": [{"text": "Entity typing", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8309836387634277}]}]}