{"title": [{"text": "Sequence-based Structured Prediction for Semantic Parsing", "labels": [], "entities": [{"text": "Sequence-based Structured Prediction", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.779479463895162}, {"text": "Semantic Parsing", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.6529108434915543}]}], "abstractContent": [{"text": "We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7872331738471985}]}, {"text": "Building on recent work by (Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations.", "labels": [], "entities": []}, {"text": "In order to use sequence prediction, we need to sequentialize these logical forms.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8114243447780609}]}, {"text": "We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar.", "labels": [], "entities": []}, {"text": "We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNN-based sequential predictor.", "labels": [], "entities": []}, {"text": "Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to map natural language utterances (NL) to logical forms (LF), a process known as semantic parsing, has received a lot of attention recently, in particular in the context of building QuestionAnswering systems (.", "labels": [], "entities": [{"text": "map natural language utterances (NL) to logical forms (LF)", "start_pos": 12, "end_pos": 70, "type": "TASK", "confidence": 0.7828527505581195}, {"text": "semantic parsing", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.7580646574497223}]}, {"text": "In this paper, we focus on such a task where the NL question maybe semantically complex, leading to a logical form query with a fair amount of compositionality, in a spirit close to.", "labels": [], "entities": []}, {"text": "Given the recently shown effectiveness of RNNs (Recurrent Neural Networks), in particular Long Short Term Memory (LSTM) networks, for performing sequence prediction in NLP applications such as machine translation ) and natural language generation, we try to exploit similar techniques for our task.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7067914754152298}, {"text": "machine translation", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.7825255692005157}, {"text": "natural language generation", "start_pos": 219, "end_pos": 246, "type": "TASK", "confidence": 0.6716183722019196}]}, {"text": "However we observe that, contrary to those applications which try to predict intrinsically sequential objects (texts), our task involves producing a structured object, namely a logical form that is tree-like by nature and also has to respect certain a priori constraints in order to be interpretable against the knowledge base.", "labels": [], "entities": []}, {"text": "In our case, building on the work \"Building a Semantic Parser Overnight\" (, which we will refer to as SPO, the LFs are generated by a grammar which is known a priori, and it is this grammar that makes explicit the structural constraints that have to be satisfied by the LFs.", "labels": [], "entities": []}, {"text": "The SPO grammar, along with generating logical forms, generates so-called \"canonical forms\" (CF), which are direct textual realizations of the LF that, although they are not \"natural\" English, transparently convey the meaning of the LF (see for an example).", "labels": [], "entities": []}, {"text": "Based on this grammar, we explore three different ways of representing the LF structure through a sequence of items.", "labels": [], "entities": []}, {"text": "The first one (LF Prediction, or LFP), and simplest, consists in just linearizing the LF tree into a sequence of individual tokens; the second one (CFP) represents the LF through its associated CF, which is itself a sequence of words; and finally the third one (DSP) represents the LF through a derivation sequence (DS), namely the sequence of grammar rules that were chosen to produce this LF.", "labels": [], "entities": []}, {"text": "We then predict the LF via LSTM-based models that take as input the NL question and map it into DT: s0(np0 (np1 (typenp0), cp0 (relnp0, entitynp0)) DS: s0 np0 np1 typenp0 cp0 relnp0 entitynp0 Figure 1: Example of natural language utterance (NL) from the SPO dataset and associated representations considered in this work.", "labels": [], "entities": [{"text": "natural language utterance (NL)", "start_pos": 213, "end_pos": 244, "type": "TASK", "confidence": 0.7341514825820923}, {"text": "SPO dataset", "start_pos": 254, "end_pos": 265, "type": "DATASET", "confidence": 0.8471281230449677}]}, {"text": "CF: canonical form, LF: logical form, DT: derivation tree, DS: derivation sequence.", "labels": [], "entities": []}, {"text": "one of the three sequentializations.", "labels": [], "entities": []}, {"text": "In the three cases, the LSTM predictor cannot on its own ensure the grammaticality of the predicted sequence, so that some sequences do not lead to well-formed LFs.", "labels": [], "entities": [{"text": "LSTM predictor", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.6874494552612305}]}, {"text": "However, in the DSP case (in contrast to LFP and CFP), it is easy to integrate inside the LSTM predictor local constraints which guarantee that only grammatical sequences will be produced.", "labels": [], "entities": [{"text": "CFP", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9149764180183411}]}, {"text": "In summary, the contribution of our paper is twofold.", "labels": [], "entities": []}, {"text": "Firstly, we propose to use sequence prediction for semantic parsing.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7655885219573975}, {"text": "semantic parsing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.8716389834880829}]}, {"text": "Our experimental results show some significant improvements over previous systems.", "labels": [], "entities": []}, {"text": "Secondly, we propose to predict derivation sequences taking into account grammatical constraints and we show that the model performs better than sequence prediction models not exploiting this knowledge.", "labels": [], "entities": []}, {"text": "These results are obtained without employing any reranking or linguistic features such as POS tags, edit distance, paraphrase features, etc., which makes the proposed methodology even more promising.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Characteristics of different target se- quences.", "labels": [], "entities": []}, {"text": " Table 3: Grammatical error rate of different sys- tems on test.", "labels": [], "entities": [{"text": "Grammatical error rate", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8309842944145203}]}]}