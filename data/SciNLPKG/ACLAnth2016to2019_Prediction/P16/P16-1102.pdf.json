{"title": [{"text": "Off-topic Response Detection for Spontaneous Spoken English Assessment", "labels": [], "entities": [{"text": "Off-topic Response Detection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5724208851655325}, {"text": "Spontaneous Spoken English Assessment", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.8124562799930573}]}], "abstractContent": [{"text": "Automatic spoken language assessment systems are becoming increasingly important to meet the demand for English second language learning.", "labels": [], "entities": []}, {"text": "This is a challenging task due to the high error rates of, even state-of-the-art, non-native speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7814072966575623}]}, {"text": "Consequently current systems primarily assess fluency and pronunciation.", "labels": [], "entities": []}, {"text": "However, content assessment is essential for full automation.", "labels": [], "entities": [{"text": "content assessment", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8619605898857117}]}, {"text": "As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech.", "labels": [], "entities": []}, {"text": "Standard approaches to off-topic response detection assess similarity between the response and question based on bag-of-words representations.", "labels": [], "entities": [{"text": "off-topic response detection", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6891401906808218}]}, {"text": "An alternative framework based on Recurrent Neural Network Language Models (RNNLM) is proposed in this paper.", "labels": [], "entities": [{"text": "Recurrent Neural Network Language Models (RNNLM)", "start_pos": 34, "end_pos": 82, "type": "TASK", "confidence": 0.5561516880989075}]}, {"text": "The RNNLM is adapted to the topic of each test question.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.5455451607704163}]}, {"text": "It learns to associate example responses to questions with points in a topic space constructed using these example responses.", "labels": [], "entities": []}, {"text": "Classification is done by ranking the topic-conditional posterior probabilities of a response.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9648246169090271}]}, {"text": "The RNNLMs associate abroad range of responses with each topic, incorporate sequence information and scale better with additional training data, unlike standard methods.", "labels": [], "entities": []}, {"text": "On experiments conducted on data from the Business Language Testing Service (BULATS) this approach outper-forms standard approaches.", "labels": [], "entities": [{"text": "Business Language Testing Service (BULATS", "start_pos": 42, "end_pos": 83, "type": "DATASET", "confidence": 0.6283362805843353}]}], "introductionContent": [{"text": "As English has become the global lingua franca, there is growing demand worldwide for assessment of English as a second language.", "labels": [], "entities": []}, {"text": "To assess spoken communication, spontaneous speech is typically elicited through a series of questions such as 'describe the photo' or 'plan a meeting'.", "labels": [], "entities": []}, {"text": "Grades are awarded based on a candidate's responses.", "labels": [], "entities": []}, {"text": "Automatic assessment systems are becoming attractive as they allow second language assessment programmes to economically scale their operations while decreasing throughput time and provide testing on demand.", "labels": [], "entities": []}, {"text": "Features for automatic graders are derived from the audio and from hypotheses produced by automatic speech recognition (ASR) systems.", "labels": [], "entities": [{"text": "automatic graders", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7017487585544586}, {"text": "automatic speech recognition (ASR)", "start_pos": 90, "end_pos": 124, "type": "TASK", "confidence": 0.8086123367150625}]}, {"text": "The latter is highly errorful due to the large variability in the input speech; disfluencies common to spontaneous speech, nonnative accents and pronunciations.", "labels": [], "entities": []}, {"text": "Current systems, such as ETS' SpeechRater () and Pearson's AZELLA (, primarily assess pronunciation and fluency.", "labels": [], "entities": [{"text": "AZELLA", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.5278864502906799}]}, {"text": "Although these are clearly indicative of spoken language ability, full assessment of spoken communication requires judgement of highlevel content and communication skills, such as response construction and relevance.", "labels": [], "entities": [{"text": "response construction", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.8118109107017517}]}, {"text": "The first stage of this is to assess whether the responses are offtopic, that is, has the candidate misunderstood the question and/or memorised a response.", "labels": [], "entities": []}, {"text": "While there has been little work done on detecting off-topic responses for spoken language assessment, detection of off-topic responses and content assessment has been studied for essay assessment.", "labels": [], "entities": [{"text": "spoken language assessment", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.6502635379632314}, {"text": "essay assessment", "start_pos": 180, "end_pos": 196, "type": "TASK", "confidence": 0.8059949576854706}]}, {"text": "One approach for essay content assessment uses features based on semantic similarity metrics between vector space representations of responses.", "labels": [], "entities": [{"text": "essay content assessment", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8429640332857767}]}, {"text": "Common vector representations include lexical Vector Space Models and Latent Semantic Analysis (LSA).", "labels": [], "entities": [{"text": "Latent Semantic Analysis (LSA)", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.716356948018074}]}, {"text": "This approach was first applied to spoken assess-ment in) and then in.", "labels": [], "entities": []}, {"text": "Following this, investigated the detection of responses for which an automatic assessment system will have difficulty in assigning a valid score, of which offtopic responses area specific type.", "labels": [], "entities": []}, {"text": "A decision tree classifier is used with features based on cosine similarity between a test response and tf-idf vectors of both aggregate example responses and questions, as well as pronunciation and fluency.", "labels": [], "entities": []}, {"text": "In () text reuse and plagiarism in spoken responses are detected using a decision tree classifier based on vector similarity and lexical matching features which compare a response to a set of example 'source texts' . This task is similar to off-topic response detection in that it is based on comparing a test response to example responses.", "labels": [], "entities": [{"text": "text reuse", "start_pos": 6, "end_pos": 16, "type": "TASK", "confidence": 0.7400701940059662}, {"text": "off-topic response detection", "start_pos": 241, "end_pos": 269, "type": "TASK", "confidence": 0.6981249451637268}]}, {"text": "Thus, a standard approach to off-topic response detection would be based on measuring the similarity between vector representations of a spoken response and the test question.", "labels": [], "entities": [{"text": "off-topic response detection", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.7091304560502371}]}, {"text": "A major deficiency of this approach is that it is based on bag-of-words vector representations, which loses information about the sequential nature of speech, which is important to evaluating response construction and relevance.", "labels": [], "entities": [{"text": "response construction", "start_pos": 192, "end_pos": 213, "type": "TASK", "confidence": 0.76423579454422}]}, {"text": "Additionally, adapting the approach to model a range of responses for each topic causes classification time to scale poorly with training data size and the number of questions.", "labels": [], "entities": [{"text": "classification", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.9627297520637512}]}, {"text": "To address these issues a general off-topic content detection framework based on topic adapted Recurrent Neural Network language models (RNNLM) has been developed and applied to off-topic response detection for spoken language assessment.", "labels": [], "entities": [{"text": "off-topic content detection", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7418968876202902}, {"text": "off-topic response detection", "start_pos": 178, "end_pos": 206, "type": "TASK", "confidence": 0.6829164127508799}, {"text": "spoken language assessment", "start_pos": 211, "end_pos": 237, "type": "TASK", "confidence": 0.6085463960965475}]}, {"text": "This framework uses example responses to test questions in training of the language model and construction of the topic-space.", "labels": [], "entities": []}, {"text": "The RNNLM learns to associate the example responses with points in the topic-space.", "labels": [], "entities": []}, {"text": "Classification is done by ranking the topic-conditional posterior probabilities of a response.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9648246169090271}]}, {"text": "The advantage of this approach is that sequence information can betaken into account and broad ranges of responses can be associated with each topic without affecting classifcation speed.", "labels": [], "entities": []}, {"text": "Two topic vector representations are investigated: Latent Dirichlet Allocation (LDA) () and Latent Semantic Analysis (LSA) ().", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 51, "end_pos": 84, "type": "METRIC", "confidence": 0.7855508724848429}, {"text": "Latent Semantic Analysis (LSA)", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.6316808611154556}]}, {"text": "They are compared to standard approaches on data from the Cambridge Business English (BULATS) exam.", "labels": [], "entities": [{"text": "Cambridge Business English (BULATS) exam", "start_pos": 58, "end_pos": 98, "type": "DATASET", "confidence": 0.9022170901298523}]}, {"text": "The rest of this paper is structured as follows: Section 2 discusses the RNNLM adaptation and topic spaces; Section 3 discusses approaches to topic detection; Section 4 presents data sets and experimental infrastructure; Section 5 analyzes experimental results; Section 6 concludes the paper.", "labels": [], "entities": [{"text": "RNNLM adaptation", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.8390757441520691}, {"text": "topic detection", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.7589581906795502}]}], "datasetContent": [{"text": "Data from the Business Language Testing Service (BULATS) English tests is used for training and testing.", "labels": [], "entities": [{"text": "Business Language Testing Service (BULATS) English tests", "start_pos": 14, "end_pos": 70, "type": "DATASET", "confidence": 0.7171169320742289}]}, {"text": "At test time, each response is recognised using an ASR system and the 1-best hypothesis is passed to the topic classifier.", "labels": [], "entities": [{"text": "ASR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8021460175514221}]}, {"text": "The topic detection system decides whether the candidate has spoken off topic by comparing the classifier output to the topic of the question being answered.", "labels": [], "entities": [{"text": "topic detection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8212182819843292}]}, {"text": "Two forms of experiment are conducted in order to assess the performance of the topic-adapted RNNLM.", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.6716252565383911}]}, {"text": "First, a topic classification experiment is run where the ability of the system to accurately recognize the topic of a response is evaluated.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8785538673400879}]}, {"text": "Second, a closed-set off-topic response detection experiment is done.", "labels": [], "entities": [{"text": "closed-set off-topic response detection", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.645401157438755}]}, {"text": "In the experimental configuration used here a response is classified into a topic and the accuracy is measured.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9994729161262512}]}, {"text": "The topic of the question being answered is known and all responses are actually ontopic.", "labels": [], "entities": []}, {"text": "A label (on-topic/off-topic) is given for each response based on the output of the classifier relative to the question topic.", "labels": [], "entities": []}, {"text": "Thus, results presented are in terms of false rejection (FR) and false acceptance (FA) rates rather than precision and recall.", "labels": [], "entities": [{"text": "false rejection (FR)", "start_pos": 40, "end_pos": 60, "type": "METRIC", "confidence": 0.859690535068512}, {"text": "false acceptance (FA) rates", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.8552414774894714}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9993299245834351}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9974297881126404}]}, {"text": "Initial topic detection experiments were run using the DEV test set with both the reference transcriptions (REF) and recognition hypotheses (ASR) to compare different KNN and RNN systems.", "labels": [], "entities": [{"text": "topic detection", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.883255660533905}, {"text": "DEV test set", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9747525056203207}, {"text": "reference transcriptions (REF) and recognition hypotheses (ASR)", "start_pos": 82, "end_pos": 145, "type": "METRIC", "confidence": 0.7433823455463756}]}, {"text": "After this, performance is evaluated on the EVAL test set.", "labels": [], "entities": [{"text": "EVAL test set", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9216218988100687}]}, {"text": "The systems were trained using data sets of 490 and 10004 candidates, as described in section 4.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: ASR performance on DEV.", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8803598284721375}, {"text": "DEV", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9248179197311401}]}, {"text": " Table 3: % False rejection in topic detection using KNN classifier with 6 nearest neighbour and distance  weights and RNNLM classifier on the DEV test set. 280 dim. topic spaces for LDA and LSA, and 560  dim. for LDA+LSA.", "labels": [], "entities": [{"text": "topic detection", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.875452071428299}, {"text": "DEV test set", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.9811960260073344}]}, {"text": " Table 4: N -Best % false rejection performance of  KNN and RNNLM classifiers with the LSA topic  space on the DEV test set", "labels": [], "entities": [{"text": "N -Best % false rejection", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7626040875911713}, {"text": "DEV test set", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9583837588628134}]}, {"text": " Table 5: % Equal Error Rate for LSA topic space  systems on the DEV and EVAL test sets.", "labels": [], "entities": [{"text": "Equal Error Rate", "start_pos": 12, "end_pos": 28, "type": "METRIC", "confidence": 0.909940262635549}, {"text": "DEV and EVAL test sets", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.8032259345054626}]}]}