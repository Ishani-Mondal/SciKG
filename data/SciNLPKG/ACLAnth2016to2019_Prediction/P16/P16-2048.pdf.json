{"title": [{"text": "Exponentially Decaying Bag-of-Words Input Features for Feed-Forward Neural Network in Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7362574338912964}]}], "abstractContent": [{"text": "Recently, neural network models have achieved consistent improvements in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7032296260197958}]}, {"text": "However, most networks only use one-hot encoded input vectors of words as their input.", "labels": [], "entities": []}, {"text": "In this work, we investigated the exponentially decaying bag-of-words input features for feed-forward neural network translation models and proposed to train the decay rates along with other weight parameters.", "labels": [], "entities": [{"text": "feed-forward neural network translation", "start_pos": 89, "end_pos": 128, "type": "TASK", "confidence": 0.6183750033378601}]}, {"text": "This novel bag-of-words model improved our phrase-based state-of-the-art system, which already includes a neural network translation model, by up to 0.5% BLEU and 0.6% TER on three different translation tasks and even achieved a similar performance to the bidirectional LSTM translation model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9989149570465088}, {"text": "TER", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.9957929849624634}, {"text": "LSTM translation", "start_pos": 270, "end_pos": 286, "type": "TASK", "confidence": 0.7066151350736618}]}], "introductionContent": [{"text": "Neural network models have recently gained much attention in research on statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.675589919090271}]}, {"text": "Several groups have reported strong improvements over state-of-the-art baselines when combining phrase-based translation with feedforward neural network-based models (FFNN) (, as well as with recurrent neural network models (RNN)).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7620776891708374}]}, {"text": "Even in alternative translation systems they showed remarkable performance (.", "labels": [], "entities": []}, {"text": "The main drawback of a feed-forward neural network model compared to a recurrent neural network model is that it can only have a limited context length on source and target sides.", "labels": [], "entities": []}, {"text": "Using the Bag-of-Words (BoW) model as additional input of a neural network based language model, () have achieved very similar perplexities on automatic speech recognition tasks in comparison to the long short-term memory (LSTM) neural network, whose structure is much more complex.", "labels": [], "entities": [{"text": "speech recognition tasks", "start_pos": 153, "end_pos": 177, "type": "TASK", "confidence": 0.7853243947029114}]}, {"text": "This suggests that the bagof-words model can effectively store the longer term contextual information, which could show improvements in statistical machine translation as well.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 136, "end_pos": 167, "type": "TASK", "confidence": 0.6644379993279775}]}, {"text": "Since the bag-of-words representation can cover as many contextual words without further modifying the network structure, the problem of limited context window size of feed-forward neural networks is reduced.", "labels": [], "entities": []}, {"text": "Instead of predefining fixed decay rates for the exponentially decaying bag-of-words models, we propose to learn the decay rates from the training data like other weight parameters in the neural network model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Experimental results of translations using exponentially decaying bag-of-words models with  different kinds of decay rates. Improvements by systems marked by  *  have a 95% statistical significance  from the baseline system, whereas  \u2020 denotes the 95% statistical significant improvements with respect to  the BoW Features system (without decay weights). We experimented with several values for the fixed  decay rate (DR) and 0.9 performed best. The applied RNN model is the LSTM bidirectional translation  model proposed in", "labels": [], "entities": [{"text": "translations", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.9637761116027832}, {"text": "BoW Features system", "start_pos": 320, "end_pos": 339, "type": "DATASET", "confidence": 0.9746636946996053}, {"text": "fixed  decay rate (DR)", "start_pos": 409, "end_pos": 431, "type": "METRIC", "confidence": 0.7517177909612656}, {"text": "LSTM bidirectional translation", "start_pos": 485, "end_pos": 515, "type": "TASK", "confidence": 0.545537680387497}]}]}