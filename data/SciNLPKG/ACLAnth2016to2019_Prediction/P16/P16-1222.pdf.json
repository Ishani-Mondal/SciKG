{"title": [{"text": "Chinese Couplet Generation with Neural Network Structures", "labels": [], "entities": []}], "abstractContent": [{"text": "Part of the unique cultural heritage of China is the Chinese couplet.", "labels": [], "entities": []}, {"text": "Given a sentence (namely an antecedent clause), people reply with another sentence (namely a subsequent clause) equal in length.", "labels": [], "entities": []}, {"text": "Moreover , a special phenomenon is that corresponding characters from the same position in the two clauses match each other by following certain constraints on semantic and/or syntactic relatedness.", "labels": [], "entities": []}, {"text": "Automatic couplet generation by computer is viewed as a difficult problem and has not been fully explored.", "labels": [], "entities": [{"text": "couplet generation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8837891221046448}]}, {"text": "In this paper, we formulate the task as a natural language generation problem using neural network structures.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7304292519887289}]}, {"text": "Given the issued antecedent clause, the system generates the subsequent clause via sequential language mod-eling.", "labels": [], "entities": []}, {"text": "To satisfy special characteristics of couplets, we incorporate the attention mechanism and polishing schema into the encoding-decoding process.", "labels": [], "entities": []}, {"text": "The couplet is generated incrementally and iteratively.", "labels": [], "entities": []}, {"text": "A comprehensive evaluation, using per-plexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.998992383480072}]}], "introductionContent": [{"text": "Chinese antithetical couplets, (namely \"\u5bf9 \u8054\"), form a special type of poetry composed of two clauses (i.e., sentences).", "labels": [], "entities": []}, {"text": "The popularity of the game of Chinese couplet challenge manifests itself in many aspects of people's life, e.g., as a means of expressing personal emotion, political views, or communicating messages at festive occasions.", "labels": [], "entities": [{"text": "Chinese couplet challenge", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6695230503877004}]}, {"text": "Hence, Chinese couplets are considered an important cultural heritage.", "labels": [], "entities": []}, {"text": "A couplet is often written in calligraphy on red banners during special occasions such as wedding ceremonies and the Chinese New Year.", "labels": [], "entities": [{"text": "Chinese New Year", "start_pos": 117, "end_pos": 133, "type": "DATASET", "confidence": 0.8172678152720133}]}, {"text": "People also use couplets to celebrate birthdays, mark the openings of a business, and commemorate historical events.", "labels": [], "entities": []}, {"text": "We illustrate areal couplet for Chinese New Year celebration in, and translate the couplet into English character-by-character.", "labels": [], "entities": [{"text": "Chinese New Year celebration", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6307679563760757}]}, {"text": "Usually in the couplet generation game, one person challenges the other person with a sentence (namely an antecedent clause).", "labels": [], "entities": [{"text": "couplet generation", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.9034364819526672}]}, {"text": "The other person then replies with another sentence (namely a subsequent clause) equal in length and term segmentation, in away that corresponding characters from the same position in the two clauses match each other by obeying certain constraints on semantic and/or syntactic relatedness.", "labels": [], "entities": []}, {"text": "We also illustrate the special phenomenon of Chinese couplet in: \"one\" is paired with \"two\", \"term\" is associated with \"character\", \"hundred\" is mapped into \"thousand\", and \"happiness\" is coupled with \"treasures\".", "labels": [], "entities": []}, {"text": "As opposed to free languages, couplets have unique poetic elegance, e.g., aestheticism and conciseness etc.", "labels": [], "entities": []}, {"text": "Filling in the couplet is considered as a challenging task with a set of structural and semantic requirements.", "labels": [], "entities": [{"text": "Filling in the couplet", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9079961776733398}]}, {"text": "Only few best scholars are able to master the skill to manipulate and to organize terms.", "labels": [], "entities": []}, {"text": "The Chinese couplet generation given the antecedent clause can be viewed as a big challenge in the joint area of Artificial Intelligence and Natural Language Processing.", "labels": [], "entities": [{"text": "Chinese couplet generation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.5636329551537832}]}, {"text": "With the fast development of computing techniques, we realize that computers might play an important role in helping people to create couplets: 1) it is rather convenient for computers to sort out appropriate term combinations from a large corpus, and 2) computer programs can take great advantages to recognize, to learn, and even to remember patterns or rules given the corpus.", "labels": [], "entities": []}, {"text": "Although computers are no sub-: An example of a Chinese couplet for Chinese New Year.", "labels": [], "entities": [{"text": "Chinese New Year", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.8413035074869791}]}, {"text": "We mark the character-wise translation under each Chinese character of the couplet so as to illustrate that each character from the same position of the two clauses has the constraint of certain relatedness.", "labels": [], "entities": []}, {"text": "Overall, the couplet can be translated as: the term of \"peaceful and lucky\" (i.e., \u548c\u987a) indicates countless happiness; the two characters \"safe and sound\" (a.k.a., \u5e73 and \u5b89) worth innumerable treasures.", "labels": [], "entities": [{"text": "\u5b89)", "start_pos": 169, "end_pos": 171, "type": "METRIC", "confidence": 0.9413972496986389}]}, {"text": "stitute for human creativity, they can process very large text repositories of couplets.", "labels": [], "entities": []}, {"text": "Furthermore, it is relatively straightforward for the machine to check whether a generated couplet conforms to constraint requirements.", "labels": [], "entities": []}, {"text": "The above observations motivate automatic couplet generation using computational intelligence.", "labels": [], "entities": [{"text": "couplet generation", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8852429986000061}]}, {"text": "Beyond the long-term goal of building an autonomous intelligent system capable of creating meaningful couplets eventually, there are potential short-term applications for augmented human expertise/experience to create couplets for entertainment or educational purposes.", "labels": [], "entities": []}, {"text": "To design the automatic couplet generator, we first need to empirically study the generation criteria.", "labels": [], "entities": [{"text": "couplet generator", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8324446678161621}]}, {"text": "We discuss some of the general generation standards here.", "labels": [], "entities": []}, {"text": "For example, the couplet generally have rigid formats with the same length for both clauses.", "labels": [], "entities": []}, {"text": "Such a syntactic constraint is strict: both clauses have exactly the same length while the length is measured in Chinese characters.", "labels": [], "entities": [{"text": "length", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.963261067867279}]}, {"text": "Each character from the same position of the two clauses have certain constraints.", "labels": [], "entities": []}, {"text": "This constraint is less strict.", "labels": [], "entities": []}, {"text": "Since Chinese language is flexible sometimes, synonyms and antonyms both indicate semantic relatedness.", "labels": [], "entities": []}, {"text": "Also, semantic coherence is a critical feature in couplets.", "labels": [], "entities": []}, {"text": "A well-written couplet is supposed to be semantically coherent among both clauses.", "labels": [], "entities": []}, {"text": "In this paper we are concerned with automatic couplet generation.", "labels": [], "entities": [{"text": "couplet generation", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8244464695453644}]}, {"text": "We propose a neural couplet machine (NCM) based on neural network structures.", "labels": [], "entities": []}, {"text": "Given a large collection of texts, we learn representations of individual characters, and their combinations within clauses as well as how they mutually reinforce and constrain each other.", "labels": [], "entities": []}, {"text": "Given any specified antecedent clause, the system could generate a subsequent clause via sequential language modeling using encoding and decoding.", "labels": [], "entities": []}, {"text": "To satisfy special characteristics of couplets, we incorporate the attention mechanism and polishing schema into the generation process.", "labels": [], "entities": []}, {"text": "The couplet is generated incrementally and iteratively to refine wordings.", "labels": [], "entities": []}, {"text": "Unlike the single-pass generation process, the hidden representations of the draft subsequent clause will be fed into the neural network structure to polish the next version of clause in our proposed system.", "labels": [], "entities": [{"text": "single-pass generation", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.6938331127166748}]}, {"text": "In contrast to previous approaches, our generator makes utilizations of neighboring characters within the clause through an iterative polishing schema, which is novel.", "labels": [], "entities": []}, {"text": "To sum up, our contributions are as follows.", "labels": [], "entities": []}, {"text": "For the first time, we propose a series of neural network-based couplet generation models.", "labels": [], "entities": [{"text": "couplet generation", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7198259234428406}]}, {"text": "We formulate anew system framework to take in the antecedent clauses and to output the subsequent clauses in the couplet pairs.", "labels": [], "entities": []}, {"text": "We tackle the special characteristics of couplets, such as corresponding characters paired in the two clauses, by incorporating the attention mechanism into the generation process.", "labels": [], "entities": []}, {"text": "For the 1 st time, we propose a novel polishing schema to iteratively refine the generated couplet using local pattern of neighboring characters.", "labels": [], "entities": []}, {"text": "The draft subsequent clause from the last iteration will be used as additional information to generate a revised version of the subsequent clause.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly summarize related work of couplet generation.", "labels": [], "entities": [{"text": "couplet generation", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8922522068023682}]}, {"text": "Then Sections 3 and 4 show the overview of our approach paradigm and then detail the neural models.", "labels": [], "entities": []}, {"text": "The experimental results and evaluation are reported in Section 5 and we draw conclusions Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "A large Chinese couplet corpus is necessary to learn the model for couplet generation.", "labels": [], "entities": [{"text": "couplet generation", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8973056674003601}]}, {"text": "There is, however, no large-sized pure couplet collection available.", "labels": [], "entities": []}, {"text": "As mentioned, generally people regard Chinese couplets as a reduced form of Chinese poetry and there are several large Chinese poem datasets publicly).", "labels": [], "entities": []}, {"text": "We are able to mine such sentence pairs out of the poems and filtering those do not conform to couplet constraints, which is a similar process mentioned in.", "labels": [], "entities": []}, {"text": "Moreover, we also crawl couplets from couplet forums where couplet fans discuss, practice and show couplet works.", "labels": [], "entities": []}, {"text": "We performed standard Chinese segmentation into characters.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.6297126710414886}]}, {"text": "In all, we collect 85,116 couplets.", "labels": [], "entities": []}, {"text": "We randomly choose 2,000 couplets for validation and 1,000 couplets for testing, other non-overlap ones for training.", "labels": [], "entities": []}, {"text": "The details are shown in.", "labels": [], "entities": []}, {"text": "Word embeddings () area standard apparatus in neural network-based text processing.", "labels": [], "entities": [{"text": "text processing", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7272616922855377}]}, {"text": "A word is mapped to a low dimensional, real-valued vector.", "labels": [], "entities": []}, {"text": "This process, known as vectorization, captures some underlying meanings.", "labels": [], "entities": []}, {"text": "Given enough data, usage, and context, word embeddings can make highly accurate guesses about the meaning of a particular word.", "labels": [], "entities": []}, {"text": "Embeddings can equivalently be viewed that a word is first represented as a one-hot vector and multiplied by a look-up table).", "labels": [], "entities": []}, {"text": "In our model, we first vectorize all words using their embeddings.", "labels": [], "entities": []}, {"text": "Here we used 128-dimensional word embeddings through vectorization, and they were initialized randomly and learned during training.", "labels": [], "entities": []}, {"text": "We set the width of convolution filters as 3.", "labels": [], "entities": []}, {"text": "The above parameters were chosen empirically.", "labels": [], "entities": []}, {"text": "The objective for training is the cross entropy errors of the predicted character distribution and the actual character distribution in our corpus.", "labels": [], "entities": []}, {"text": "An \u2113 2 regularization term is also added to the objective.", "labels": [], "entities": []}, {"text": "The model is trained with back propagation through time with the length being the time step.", "labels": [], "entities": []}, {"text": "The objective is minimized by stochastic gradient descent with shuffled mini-batches (with a mini-batch size of 100) for optimization.", "labels": [], "entities": []}, {"text": "During training, the cross entropy error of the output is back-propagated through all hidden layers.", "labels": [], "entities": []}, {"text": "Initial learning rate was set to 0.8, and a multiplicative learning rate decay was applied.", "labels": [], "entities": [{"text": "multiplicative learning rate decay", "start_pos": 44, "end_pos": 78, "type": "METRIC", "confidence": 0.6664245277643204}]}, {"text": "We used the validation set for early stopping.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.746812254190445}]}, {"text": "In practice, the training converges after a few epochs.", "labels": [], "entities": []}, {"text": "It is generally difficult to judge the effect of couplets generated by computers.", "labels": [], "entities": []}, {"text": "We propose to evaluate results from 3 different evaluation metrics.", "labels": [], "entities": []}, {"text": "For most of the language generation research, language perplexity is a sanity check.", "labels": [], "entities": [{"text": "language generation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7494470477104187}]}, {"text": "Our first set of experiments involved intrinsic evaluation of the \"perplexity\" evaluation for the generated couplets.", "labels": [], "entities": []}, {"text": "Perplexity is actually an entropy based evaluation.", "labels": [], "entities": []}, {"text": "In this sense, the lower perplexity for the couplets generated, the better performance in purity for the generations, and the couplets are likely to be good.", "labels": [], "entities": [{"text": "purity", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9971941709518433}]}, {"text": "The Bilingual Evaluation Understudy (BLEU) score-based evaluation is usually used for machine translation): given the reference translation(s), the algorithm evaluates the quality of text which has been machinetranslated from the reference translation as ground truth.", "labels": [], "entities": [{"text": "Bilingual Evaluation Understudy (BLEU) score-based", "start_pos": 4, "end_pos": 54, "type": "METRIC", "confidence": 0.7023063940661294}, {"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7855642437934875}]}, {"text": "We adapt the BLEU evaluation under the couplet generation scenario.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9951397180557251}, {"text": "couplet generation", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9176483154296875}]}, {"text": "Take a couplet from the dataset, we generate the computer authored subsequent clause given the antecedent clause, and compare it with the original subsequent clause written by humans.", "labels": [], "entities": []}, {"text": "There is a concern for such an evaluation metric is that BLEU score can only reflect the partial capability of the models; there is (for most cases) only one ground truth for the generated couplet but actually there are more than one appropriate ways to generate a well-written couplet.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.982052356004715}]}, {"text": "The merit of BLEU evaluation is to examine how likely to approximate the computer generated couplet towards human authored ones.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9261310696601868}]}, {"text": "We also include human judgments from 13 evaluators who are graduate students majoring in Chinese literature.", "labels": [], "entities": []}, {"text": "Evaluators are requested to express an opinion over the automatically generated couplets.", "labels": [], "entities": []}, {"text": "A clear criterion is necessary for human evaluation.", "labels": [], "entities": []}, {"text": "We use the evaluation standards discussed in): \"syntactic\" and \"semantic\" satisfaction.", "labels": [], "entities": []}, {"text": "For the syntactic side, evaluators consider whether the subsequent clauses conform the length restriction and word pairing between the two clauses.", "labels": [], "entities": []}, {"text": "For a higher level of semantic side, evaluators then consider whether the two clauses are semantically meaningful and coherent.", "labels": [], "entities": []}, {"text": "Evaluators assign 0-1 scores for both syntactic and semantic criteria ('0'-no, '1'-yes).", "labels": [], "entities": []}, {"text": "The evaluation process is conducted as a blind-review", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Detailed information of the datasets.  Each pair of couplets consist of two clauses.  #Pairs  #Character  TANG Poem  26,833  6,358  SONG Poem  11,324  3,629  Couplet Forum  46,959  8,826", "labels": [], "entities": [{"text": "Character  TANG Poem  26,833  6,358  SONG Poem  11,324  3,629  Couplet Forum  46,959  8,826", "start_pos": 105, "end_pos": 196, "type": "DATASET", "confidence": 0.6574075107391064}]}, {"text": " Table 2: Overall performance comparison against baselines.", "labels": [], "entities": []}]}