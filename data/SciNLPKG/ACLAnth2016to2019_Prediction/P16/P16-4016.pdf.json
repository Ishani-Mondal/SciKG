{"title": [{"text": "META: A Unified Toolkit for Text Retrieval and Analysis", "labels": [], "entities": [{"text": "Text Retrieval and Analysis", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.8317035436630249}]}], "abstractContent": [{"text": "META is developed to unite machine learning, information retrieval, and natural language processing in one easy-to-use toolkit.", "labels": [], "entities": [{"text": "META", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7418019771575928}, {"text": "information retrieval", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8088409602642059}]}, {"text": "Its focus on indexing allows it to perform well on large datasets, supporting online classification and other out-of-core algorithms.", "labels": [], "entities": [{"text": "indexing", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.9648880362510681}]}, {"text": "META's liberal open source license encourages contributions, and its extensive online documentation, forum, and tutorials make this process straightforward.", "labels": [], "entities": [{"text": "META", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9410925507545471}]}, {"text": "We run experiments and show META's performance is competitive with or better than existing software.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluate META's performance in NLP, IR, and ML tasks.", "labels": [], "entities": [{"text": "IR", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.8521855473518372}, {"text": "ML tasks", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.8928491473197937}]}, {"text": "All experiments were performed on a workstation with an Intel(R) Core(TM) i7-5820K CPU, 16 GB of RAM, and a 4 TB 5900 RPM disk.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: (NLP) Training/testing performance for the shift-reduce constituency parsers. All models were trained for 40 iterations  on the standard training split of the Penn Treebank. Accuracy is reported as labeled F1 from evalb on section 23.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 169, "end_pos": 182, "type": "DATASET", "confidence": 0.9934393465518951}, {"text": "Accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9992367029190063}, {"text": "F1", "start_pos": 216, "end_pos": 218, "type": "METRIC", "confidence": 0.9974614381790161}]}, {"text": " Table 3: (NLP) Part-of-speech tagging token-level accura- cies. \"Extra data\" implies the use of large amounts of extra  unlabeled data (e.g. for distributional similarity features).", "labels": [], "entities": [{"text": "Part-of-speech tagging token-level", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.7388139764467875}]}, {"text": " Table 4: (IR) The two TREC datasets used. Uncleaned ver- sions of blog06 and gov2 were 89 GB and 426 GB respec- tively.", "labels": [], "entities": [{"text": "IR", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.7297765016555786}, {"text": "TREC datasets", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.7573584914207458}]}, {"text": " Table 5: (IR) Indexing speed.", "labels": [], "entities": [{"text": "IR) Indexing speed", "start_pos": 11, "end_pos": 29, "type": "METRIC", "confidence": 0.7191029116511345}]}, {"text": " Table 6: (IR) Index size.", "labels": [], "entities": [{"text": "IR) Index size", "start_pos": 11, "end_pos": 25, "type": "METRIC", "confidence": 0.8033321648836136}]}, {"text": " Table 7: (IR) Query speed.", "labels": [], "entities": [{"text": "IR) Query speed", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.6195599138736725}]}, {"text": " Table 8: (IR) Query performance via Mean Average Precision  and Precision at 10 documents.", "labels": [], "entities": [{"text": "Mean Average Precision", "start_pos": 37, "end_pos": 59, "type": "METRIC", "confidence": 0.9235595266024271}, {"text": "Precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9189751148223877}]}, {"text": " Table 9: (ML) Datasets used for k-class categorization.", "labels": [], "entities": []}, {"text": " Table 10: (ML) Accuracy and speed classification results.  Reported time is to both train and test the model. For all  except Webspam, this excludes IO.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9619979858398438}, {"text": "speed classification", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.745614618062973}, {"text": "Reported time", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9694352149963379}, {"text": "Webspam", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9674861431121826}, {"text": "IO", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9794459939002991}]}]}