{"title": [{"text": "The LAMBADA dataset: Word prediction requiring abroad discourse context *", "labels": [], "entities": [{"text": "LAMBADA dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9179365336894989}, {"text": "Word prediction", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8025491833686829}]}], "abstractContent": [{"text": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.8223124742507935}, {"text": "text understanding", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8229241073131561}, {"text": "word prediction task", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.8077905178070068}]}, {"text": "LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only seethe last sentence preceding the target word.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5378578305244446}]}, {"text": "To succeed on LAM-BADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.", "labels": [], "entities": []}, {"text": "We show that LAMBADA exemplifies a wide range of linguistic phenomena , and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9990876913070679}]}, {"text": "We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent spurt of powerful end-to-end-trained neural networks for Natural Language Processing (, a.o.) has sparked interest in tasks to measure the progress they are bringing about in genuine language understanding.", "labels": [], "entities": []}, {"text": "Special care must betaken in evaluating such systems, since their effectiveness at picking statistical generalizations from large corpora can lead to the illusion that they are reaching a deeper degree of understanding than they really are.", "labels": [], "entities": []}, {"text": "For example, the end-to-end system of, trained on large conversational datasets, produces dialogues such as the following: Human: what is your job?", "labels": [], "entities": []}, {"text": "Machine: i'm a lawyer Human: what do you do?", "labels": [], "entities": []}, {"text": "Machine: i'm a doctor Separately, the system responses are appropriate for the respective questions.", "labels": [], "entities": []}, {"text": "However, when taken together, they are incoherent.", "labels": [], "entities": []}, {"text": "The system behaviour is somewhat parrot-like.", "labels": [], "entities": []}, {"text": "It can locally produce perfectly sensible language fragments, but it fails to take the meaning of the broader discourse context into account.", "labels": [], "entities": []}, {"text": "Much research effort has consequently focused on designing systems able to keep information from the broader context into memory, and possibly even perform simple forms of reasoning about it (.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the LAMBADA dataset (LAnguage Modeling Broadened to Account for Discourse Aspects).", "labels": [], "entities": [{"text": "LAMBADA dataset", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.8022821545600891}]}, {"text": "LAMBADA proposes a word prediction task where the target item is difficult to guess (for English speakers) when only the sentence in which it appears is available, but becomes easy when a broader context is presented.", "labels": [], "entities": [{"text": "word prediction task", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8238893151283264}]}, {"text": "Consider Example (1) in.", "labels": [], "entities": []}, {"text": "The sentence Do you honestly think that I would want you to have a ? has a multitude of possible continuations, but the broad context clearly indicates that the missing word is miscarriage.", "labels": [], "entities": []}, {"text": "LAMBADA casts language understanding in the classic word prediction framework of language modeling.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7270793318748474}, {"text": "word prediction", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7103384435176849}]}, {"text": "We can thus use it to test several existing language modeling architectures, including systems with capacity to hold longer-term contextual memories.", "labels": [], "entities": []}, {"text": "In our preliminary experiments, none of these models came even remotely close to human performance, confirming that LAMBADA is a challenging benchmark for research on automated models of natural language understanding.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.7138741612434387}, {"text": "natural language understanding", "start_pos": 187, "end_pos": 217, "type": "TASK", "confidence": 0.6493556896845499}]}], "datasetContent": [{"text": "The CNN/Daily Mail (CNNDM) benchmark recently introduced by is closely related to LAMBADA.", "labels": [], "entities": [{"text": "CNN/Daily Mail (CNNDM) benchmark", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.9178663343191147}]}, {"text": "CNNDM includes a large set of online articles that are published together with short summaries of their main points.", "labels": [], "entities": [{"text": "CNNDM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8401203155517578}]}, {"text": "The task is to guess a named entity that has been removed from one such summary.", "labels": [], "entities": []}, {"text": "Although the data are not normed by subjects, it is unlikely that the missing named entity can be guessed from the short summary alone, and thus, like in LAM-BADA, models need to look at the broader context (the article).", "labels": [], "entities": []}, {"text": "Differences between the two datasets include text genres (news vs. novels; see Section 3.1) and the fact that missing items in CN-NDM are limited to named entities.", "labels": [], "entities": []}, {"text": "Most importantly, the two datasets require models to perform different kinds of inferences over broader passages.", "labels": [], "entities": []}, {"text": "For CNNDM, models must be able to summarize the articles, in order to make sense of the sentence containing the missing word, whereas in LAMBADA the last sentence is not a summary of the broader passage, but a continuation of the same story.", "labels": [], "entities": [{"text": "CNNDM", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.6075543165206909}]}, {"text": "Thus, in order to succeed, models must instead understand what is a plausible development of a narrative fragment or a dialogue.", "labels": [], "entities": []}, {"text": "Another related benchmark, CBT, has been introduced by.", "labels": [], "entities": [{"text": "CBT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.38875505328178406}]}, {"text": "Like LAMBADA, CBT is a collection of book excerpts, with one word randomly removed from the last sentence in a sequence of 21 sentences.", "labels": [], "entities": []}, {"text": "While there are other design differences, the crucial distinction between CBT and LAMBADA is that the CBT passages were not filtered to be human-guessable in the broader context only.", "labels": [], "entities": []}, {"text": "Indeed, according to the post-hoc analysis of a sample of CBT passages reported by Hill and colleagues, in a large proportion of cases in which annotators could guess the missing word from the broader context, they could also guess it from the last sentence alone.", "labels": [], "entities": []}, {"text": "At the same time, in about one fifth of the cases, the annotators could not guess the word even when the broader context was given.", "labels": [], "entities": []}, {"text": "Thus, only a small portion of the CBT passages are really probing the model's ability to understand the broader context, which is instead the focus of LAMBADA.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 151, "end_pos": 158, "type": "DATASET", "confidence": 0.8242285251617432}]}, {"text": "The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (.", "labels": [], "entities": [{"text": "book excerpt completion task", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6648023724555969}, {"text": "MSRCC dataset", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9781964421272278}]}, {"text": "However, the latter limited context to single sentences, not attempting to measure broader passage understanding.", "labels": [], "entities": [{"text": "passage understanding", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.8109164237976074}]}, {"text": "Of course, text understanding can be tested through other tasks, including entailment detection (, answering questions about a text () and measuring inter-clause coherence (.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8310960829257965}, {"text": "entailment detection", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7769854664802551}]}, {"text": "While different tasks can provide complementary insights into the models' abilities, we find word prediction particularly attractive because of its naturalness (it's easy to norm the data with non-expert humans) and simplicity.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.8395213782787323}]}, {"text": "Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sentences entail each other.", "labels": [], "entities": []}, {"text": "Moreover, models can have access to virtually unlimited amounts of training data, as all that is required to train a language model is raw text.", "labels": [], "entities": []}, {"text": "On a more general methodological level, word prediction has the potential to probe almost any aspect of text understanding, including but not limited to traditional narrower tasks such as entailment, co-reference resolution or word sense disambiguation.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.845129519701004}, {"text": "text understanding", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.6927550733089447}, {"text": "co-reference resolution", "start_pos": 200, "end_pos": 223, "type": "TASK", "confidence": 0.7306467741727829}, {"text": "word sense disambiguation", "start_pos": 227, "end_pos": 252, "type": "TASK", "confidence": 0.6421653827031454}]}, {"text": "3.1 Data collection 1 LAMBADA consists of passages composed of a context (on average 4.6 sentences) and a target sentence.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.6504834294319153}]}, {"text": "The context size is the minimum number of complete sentences before the target sentence such that they cumulatively contain at least 50 tokens (this size was chosen in a pilot study).", "labels": [], "entities": []}, {"text": "The task is to guess the last word of the target sentence (the target word).", "labels": [], "entities": []}, {"text": "The constraint that the target word be the last word of the sentence, while not necessary for our research goal, makes the task more natural for human subjects.", "labels": [], "entities": []}, {"text": "The LAMBADA data come from the Book Corpus ().", "labels": [], "entities": [{"text": "LAMBADA data", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7646994292736053}, {"text": "Book Corpus", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9841831028461456}]}, {"text": "The fact that it contains unpublished novels minimizes the potential  The LAMBADA dataset consists of 10,022 passages, divided into 4,869 development and 5,153 test passages (extracted from 1,331 and 1,332 disjoint novels, respectively).", "labels": [], "entities": [{"text": "LAMBADA dataset", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.8414550721645355}]}, {"text": "The average passage consists of 4.6 sentences in the context plus 1 target sentence, fora total length of 75.4 tokens (dev) / 75 tokens (test).", "labels": [], "entities": []}, {"text": "Examples of passages in the dataset are given in.", "labels": [], "entities": []}, {"text": "The training data for language models to be tested on LAMBADA include the full text of 2,662 novels (disjoint from those in dev+test), comprising 203 million words.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.8168734312057495}]}, {"text": "Note that the training data consists of text from the same domain as the dev+test passages, in large amounts but not filtered in the same way.", "labels": [], "entities": []}, {"text": "This is partially motivated by economic considerations (recall that each data point costs $1.24 on average), but, more importantly, it is justified by the intended use of LAM-BADA as a tool to evaluate general-purpose models in terms of how they fare on broad-context understanding (just like our subjects could predict the missing words using their more general text understanding abilities), not as a resource to develop ad-hoc models only meant to predict the final word in the sort of passages encountered in LAMBADA.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 513, "end_pos": 520, "type": "DATASET", "confidence": 0.8759742379188538}]}, {"text": "The development data can be used to fine-tune models to the specifics of the LAM-BADA passages.", "labels": [], "entities": []}, {"text": "Our analysis of the LAMBADA data suggests that, in order for the target word to be predictable in abroad context only, it must be strongly cued in the broader discourse.", "labels": [], "entities": [{"text": "LAMBADA data", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.8566659986972809}]}, {"text": "Indeed, it is typical for LAM-BADA items that the target word (or its lemma) occurs in the context.(a) compares the LAMBADA items to a random 5000-item sample from the input data, that is, the passages that were presented to human subjects in the filtering phase (we sampled from all passages passing the automated filters described in Section 3.1 above, including those that made it to LAMBADA).", "labels": [], "entities": []}, {"text": "The figure shows that when subjects guessed the word (only) in the broad context, often the word itself occurred in the context: More than 80% of LAMBADA passages include the target word in the context, while in the input data that was the case for less than 15% of the passages.", "labels": [], "entities": []}, {"text": "To guess the right word, however, subjects must still put their linguistic and general cognitive skills to good use, as shown by the examples featuring the target word in the context reported in. shows that most target words in LAMBADA are proper nouns (48%), followed by common nouns (37%) and, at a distance, verbs (7.7%).", "labels": [], "entities": []}, {"text": "In fact, proper nouns are hugely overrepresented in LAMBADA, while the other categories are under-represented, compared to the POS distribution in the input.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.5652406811714172}]}, {"text": "A variety of factors converges in making proper nouns easy for subjects in the LAMBADA task.", "labels": [], "entities": [{"text": "LAMBADA task", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.5942346155643463}]}, {"text": "In particular, when the context clearly demands a referential expression, the constraint that the blank be filled by a single word excludes other possibilities such as noun phrases with articles, and there are reasons to suspect that co-reference is easier than other discourse phenomena in our task (see below).", "labels": [], "entities": []}, {"text": "However, although co-reference seems to play a big role, only 0.3% of target words are pronouns.", "labels": [], "entities": []}, {"text": "Common nouns are still pretty frequent in LAMBADA, constituting over one third of the data.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.5075094699859619}]}, {"text": "Qualitative analysis reveals a mixture of phenomena.", "labels": [], "entities": []}, {"text": "Co-reference is again quite common (see Example (3) in), sometimes as \"partial\" co-reference facilitated by bridging mechanisms (shutter-camera; Example (5)) or through the presence of a near synonym ('lose the baby'-miscarriage; Example (1)).", "labels": [], "entities": []}, {"text": "However, we also often find other phenomena, such as the inference of prototypical participants in an event.", "labels": [], "entities": []}, {"text": "For instance, if the passage describes someone having breakfast together with typical food and beverages (see Example), subjects can guess the target word coffee without it having been explicitly mentioned.", "labels": [], "entities": []}, {"text": "In contrast, verbs, adjectives, and adverbs are rare in LAMBADA.", "labels": [], "entities": []}, {"text": "Many of those items can be guessed with local sentence context only, as shown in, which also reports the POS distribution of the set of items that were guessed by subjects based on the target-sentence context only (step 3 in Section 3.1).", "labels": [], "entities": [{"text": "POS distribution", "start_pos": 105, "end_pos": 121, "type": "METRIC", "confidence": 0.9659221768379211}]}, {"text": "Note a higher proportion of verbs, adjectives and adverbs in the latter set in, where the target word is an adjective).", "labels": [], "entities": []}, {"text": "This contrasts with other types of open-class adverbs (e.g., innocently, confidently), which are generally hard to guess with both local and broad context.", "labels": [], "entities": []}, {"text": "The low proportion of these kinds of adverbs and of verbs among guessed items in general suggests that tracking event-related phenomena (such as script-like sequences of events) is harder for subjects than coreferential phenomena, at least as framed in the LAMBADA task.", "labels": [], "entities": [{"text": "tracking event-related phenomena (such as script-like sequences of events)", "start_pos": 103, "end_pos": 177, "type": "TASK", "confidence": 0.740564145825126}]}, {"text": "Further research is needed to probe this hypothesis.", "labels": [], "entities": []}, {"text": "Furthermore, we observe that, while explicit mention in the preceding discourse context is critical for proper nouns, the other categories can often be guessed without having been explicitly introduced.", "labels": [], "entities": []}, {"text": "This is shown in, which depicts the POS distribution of LAMBADA items for which the lemma of the target word is not in the context (corresponding to about 16% of LAMBADA in total).", "labels": [], "entities": []}, {"text": "Qualitative analysis of items with verbs and adjectives as targets suggests that the target word, although not present in the passage, is still strongly implied by the con- The apparent 1% of out-of-context proper nouns shown in(c) is due to lemmatization mistakes (fictional characters for which the lemmatizer did not recognize a link between singular and plural forms, e.g., Wynn -Wynns).", "labels": [], "entities": []}, {"text": "A manual check confirmed that all proper noun target words in LAMBADA are indeed also present in the context. text.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.49700143933296204}]}, {"text": "In about one third of the cases examined, the missing word is \"almost there\".", "labels": [], "entities": []}, {"text": "For instance, the passage contains a word with the same root but a different part of speech (e.g., death-died in Example), or a synonymous expression (as mentioned above for \"miscarriage\"; we find the same phenomenon for verbs, e.g., 'deprived you of water'-dehydrated).", "labels": [], "entities": []}, {"text": "In other cases, correct prediction requires more complex discourse inference, including guessing prototypical participants of a scene (as in the coffee example above), actions or events strongly suggested by the discourse (see Examples (1) and (10), where the mention of an icy road helps in predicting the target driving), or qualitative properties of participants or situations (see).", "labels": [], "entities": [{"text": "correct prediction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.6883290708065033}]}, {"text": "Of course, the same kind of discourse reasoning takes place when the target word is already present in the context (cf. Examples and).", "labels": [], "entities": []}, {"text": "The presence of the word in context does not make the reasoning unnecessary (the task remains challenging), but facilitates the inference.", "labels": [], "entities": []}, {"text": "As a final observation, intriguingly, the LAM-BADA items contain (quoted) direct speech significantly more often than the input items overall (71% of LAMBADA items vs. 61% of items in the input sample), see, e.g., Examples (1) and.", "labels": [], "entities": []}, {"text": "Further analysis is needed to investigate in what way more dialogic discourse might facilitate the prediction of the final target word.", "labels": [], "entities": [{"text": "prediction of the final target word", "start_pos": 99, "end_pos": 134, "type": "TASK", "confidence": 0.835560550292333}]}, {"text": "In sum, LAMBADA contains a myriad of phenomena that, besides making it challenging from the text understanding perspective, are of great interest to the broad Computational Linguistics community.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7424993216991425}]}, {"text": "To return to Example (1), solving it requires a combination of linguistic skills ranging from (morpho)phonology (the plausible target word abortion is ruled out by the indefinite determiner a) through morphosyntax (the slot should be filled by a common singular noun) to pragmatics (understanding what the male participant is inferring from the female participant's words), in addition to general reasoning skills.", "labels": [], "entities": []}, {"text": "It is not surprising, thus, that LAMBADA is so challenging for current models, as we show next.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.5408489108085632}]}, {"text": "Computational methods We tested several existing language models and baselines on LAM-BADA.", "labels": [], "entities": []}, {"text": "We implemented a simple RNN), a Long Short-Term Memory network (LSTM)), a traditional statistical N-Gram language model) with and without cache, and a Memory Network ().", "labels": [], "entities": []}, {"text": "We remark that at least LSTM, Memory Network and, to a certain extent, the cache N-Gram model have, among their supposed benefits, the ability to take broader contexts into account.", "labels": [], "entities": []}, {"text": "Note moreover that variants of RNNs and LSTMs are at the state of the art when tested on standard language modeling benchmarks.", "labels": [], "entities": []}, {"text": "Our Memory Network implementation is similar to the one with which reached the best results on the CBT data set (see Section 2 above).", "labels": [], "entities": [{"text": "CBT data set", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9105024337768555}]}, {"text": "While we could not re-implement the models that performed best on CNNDM (see again Section 2), our LSTM is architecturally similar to the Deep LSTM Reader of, which achieved respectable performance on that data set.", "labels": [], "entities": []}, {"text": "Most importantly, we will show below that most of our models reach impressive performance when tested on a more standard language modeling data set sourced from the same corpus used to build LAMBADA.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 191, "end_pos": 198, "type": "DATASET", "confidence": 0.8358770608901978}]}, {"text": "This control set was constructed by randomly sampling 5K passages of the same shape and size as the ones used to build LAMBADA from the same test novels, but without filtering them in anyway.", "labels": [], "entities": []}, {"text": "Based on the control set results, to be discussed below, we can reasonably claim that the models we are testing on LAM-BADA are very good at standard language modeling, and their low performance on the latter cannot be attributed to poor quality.", "labels": [], "entities": []}, {"text": "In order to test for strong biases in the data, we constructed Sup-CBOW, a baseline model weakly tailored to the task at hand, consisting of a simple neural network that takes as input a bag-ofword representation of the passage and attempts to predict the final word.", "labels": [], "entities": []}, {"text": "The input representation comes from adding pre-trained CBOW vectors () of the words in the passage.", "labels": [], "entities": []}, {"text": "We also considered an unsupervised variant (Unsup-CBOW) where the target word is predicted by cosine similarity between the passage vector and the target word vector.", "labels": [], "entities": []}, {"text": "Finally, we evaluated several variations of a random guessing baseline differing in terms of the word pool to sample from.", "labels": [], "entities": []}, {"text": "The guessed word could be picked from: the full vocabulary, the words that appear in the current passage and a random uppercased word from the passage.", "labels": [], "entities": []}, {"text": "The latter baseline aims at exploiting the potential bias that proper names account fora consistent portion of the LAMBADA data (see above).", "labels": [], "entities": [{"text": "LAMBADA data", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.8806655704975128}]}, {"text": "Note that LAMBADA was designed to challenge language models with harder-than-average examples where broad context understanding is crucial.", "labels": [], "entities": []}, {"text": "However, the average case should not be disregarded either, since we want language models to be able to handle both cases.", "labels": [], "entities": []}, {"text": "For this reason, we trained the models entirely on unsupervised data and expect future work to follow similar principles.", "labels": [], "entities": []}, {"text": "Concretely, we trained the models, as is standard practice, on predicting each upcoming word given the previous context, using the LAMBADA training data (see Section 3.2 above) as input corpus.", "labels": [], "entities": [{"text": "predicting each upcoming word given the previous context", "start_pos": 63, "end_pos": 119, "type": "TASK", "confidence": 0.7902263775467873}, {"text": "LAMBADA training data", "start_pos": 131, "end_pos": 152, "type": "DATASET", "confidence": 0.875084380308787}]}, {"text": "The only exception to this procedure was Sup-CBOW where we extracted from the training novels similar-shaped passages to those in LAMBADA and trained the model on them (about 9M passages).", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.8936572670936584}]}, {"text": "Again, the goal of this model was only to test for potential biases in the data and not to provide a full account for the phenomena we are testing.", "labels": [], "entities": []}, {"text": "We restricted the vocabulary of the models to the 60K most frequent words in the training set (covering 95% of the target words in the development set).", "labels": [], "entities": []}, {"text": "The model hyperparameters were tuned on their accuracy in the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.999491810798645}]}, {"text": "The same trained models were tested on the LAM-BADA and the control sets.", "labels": [], "entities": []}, {"text": "See SM for the tuning details.", "labels": [], "entities": []}, {"text": "Results Results of models and baselines are reported in for LAMBADA is the average success of a model at predicting the target word, i.e., accuracy (unlike in standard language modeling, we know that the missing LAMBADA words can be precisely predicted by humans, so good models should be able to accomplish the same feat, rather than just assigning a high probability to them).", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.8778945207595825}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9993711113929749}]}, {"text": "However, as we observe a bottoming effect with accuracy, we also report perplexity and median rank of correct word, to better compare the models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9977571368217468}, {"text": "median rank", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.8548586070537567}]}, {"text": "As anticipated above, and inline with what we expected, all our models have very good performance when called to perform a standard language modeling task on the control set.", "labels": [], "entities": []}, {"text": "Indeed, 3 of the models (the N-Gram models and LSTM) can guess the right word in about 1/5 of the cases.", "labels": [], "entities": []}, {"text": "The situation drastically changes if we look at the LAMBADA results, where all models are performing very badly.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.5800883769989014}]}, {"text": "Indeed, no model is even able to compete with the simple heuristics of picking a random word from the passage, and, especially, a random capitalized word (easily a proper noun).", "labels": [], "entities": []}, {"text": "At the same time, the low performance of the latter heuristic in absolute terms (7% accuracy) shows that, despite the bias in favour of names in the passage, simply relying on this will not suffice to obtain good performance on LAMBADA, and models should rather pursue deeper forms of analysis of the broader context (the Sup-CBOW baseline, attempting to directly exploit the passage in a shallow way, performs very poorly).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9981208443641663}, {"text": "LAMBADA", "start_pos": 228, "end_pos": 235, "type": "DATASET", "confidence": 0.7450796961784363}, {"text": "Sup-CBOW baseline", "start_pos": 322, "end_pos": 339, "type": "DATASET", "confidence": 0.7896601259708405}]}, {"text": "This confirms again that the difficulty of LAMBADA relies mainly on accounting for the information available in a broader context and not on the task of predicting the exact word missing.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.6649893522262573}]}, {"text": "In comparative terms (and focusing on perplexity and rank, given the uniformly low accuracy results) we observe a stronger performance of the traditional N-Gram models over the neuralnetwork-based ones, possibly pointing to the difficulty of tuning the latter properly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9920108914375305}]}, {"text": "In particular, the best relative performance on LAMBADA is achieved by N-Gram w/cache, which takes passage statistics into account.", "labels": [], "entities": []}, {"text": "While even this model is effectively unable to guess the right word, it achieves a respectable perplexity of 768.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9853439927101135}]}, {"text": "We recognize, of course, that the evaluation we performed is very preliminary, and it must only betaken as a proof-of-concept study of the difficulty of LAMBADA.", "labels": [], "entities": [{"text": "LAMBADA", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.4643537402153015}]}, {"text": "Better results might be obtained simply by performing more extensive tuning, by adding more sophisticated mechanisms such as attention (, and so forth.", "labels": [], "entities": []}, {"text": "Still, we would be surprised if minor modifications of the models we tested led to human-level performance on the task.", "labels": [], "entities": []}, {"text": "We also note that, because of the way we have constructed LAMBADA, standard language models are bound to fail on it by design: one of our first filters (see Section 3.1) was to choose passages where a number of simple language models were failing to predict the upcoming word.", "labels": [], "entities": []}, {"text": "However, future research should find ways around this inherent difficulty.", "labels": [], "entities": []}, {"text": "After all, humans were still able to solve this task, so a model that claims to have good language understanding ability should be able to succeed on it as well.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7057938873767853}]}], "tableCaptions": [{"text": " Table 1: Results of computational methods. Accuracy is expressed in percentage.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9995954632759094}]}]}