{"title": [], "abstractContent": [{"text": "We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT).", "labels": [], "entities": [{"text": "SMT lattices", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.8632861971855164}, {"text": "end-to-end neural machine translation (NMT)", "start_pos": 68, "end_pos": 111, "type": "TASK", "confidence": 0.7717459457261222}]}, {"text": "Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities.", "labels": [], "entities": []}, {"text": "With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.7675550580024719}]}], "introductionContent": [{"text": "We report on investigations motivated by the idea that the structured search spaces defined by syntactic machine translation approaches such as Hiero can be used to guide Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 171, "end_pos": 203, "type": "TASK", "confidence": 0.8286804060141245}]}, {"text": "NMT and Hiero have complementary strengths and weaknesses and differ markedly in how they define probability distributions over translations and what search procedures they use.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9398358464241028}, {"text": "Hiero", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.847428560256958}]}, {"text": "The NMT encoder-decoder formalism provides a probability distribution over translations y = y T 1 of a source sentence x as ( P (y T 1 |x) = g(y t\u22121 , st , ct ) (1) where st = f (s t\u22121 , y t\u22121 , ct ) is a decoder state variable and ct is a context vector depending on the source sentence and the attention mechanism.", "labels": [], "entities": []}, {"text": "This posterior distribution is potentially very powerful, however it does not easily lend itself to sophisticated search procedures.", "labels": [], "entities": []}, {"text": "Decoding is done by 'beam search to find a translation that approximately maximizes the conditional probability' (.", "labels": [], "entities": []}, {"text": "Search looks only one word ahead and no deeper than the beam.", "labels": [], "entities": [{"text": "Search", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6276978254318237}]}, {"text": "Hiero defines asynchronous context-free grammar (SCFG) with rules: X \u2192 \u03b1, \u03b3, where \u03b1 and \u03b3 are strings of terminals and non-terminals in the source and target languages.", "labels": [], "entities": [{"text": "context-free grammar (SCFG)", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6400816023349762}]}, {"text": "A target language sentence y can be a translation of a source language sentence x if there is a derivation D in the grammar which yields both y and x: y = y(D), x = x(D).", "labels": [], "entities": []}, {"text": "This defines a regular language Y over strings in the target language via a projection of the sentence to be translated:).", "labels": [], "entities": []}, {"text": "Scores are defined over derivations via a log-linear model with features {\u03c6 i } and weights \u03bb.", "labels": [], "entities": []}, {"text": "The decoder searches for the translation y(D) in Y with the highest derivation score S(D) : where P LM is an n-gram language model and Hiero decoders attempt to avoid search errors when combining the translation and language model for the translation hypotheses).", "labels": [], "entities": []}, {"text": "These procedures search over avast space of translations, much larger than is considered by the NMT beam search.", "labels": [], "entities": [{"text": "NMT beam search", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.75137331088384}]}, {"text": "However the Hiero context-free grammars that make efficient search possible are weak models of translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.9682175517082214}]}, {"text": "The basic Hiero formalism can be extended through 'soft syntactic constraints' ( or by adding very high dimensional features, however the translation score assigned by the grammar is still only the product of probabilities of individual rules.", "labels": [], "entities": []}, {"text": "From the modelling perspective, this is an overly strong conditional independence assumption.", "labels": [], "entities": []}, {"text": "NMT clearly has the potential advantage in incorporating long-term context into translation scores.", "labels": [], "entities": []}, {"text": "NMT and Hiero differ in how they 'consume' source words.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9407400488853455}, {"text": "Hiero", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8637621402740479}]}, {"text": "Hiero applies the translation rules to the source sentence via the CYK algorithm, with each derivation yielding a complete and unambiguous translation of the source words.", "labels": [], "entities": []}, {"text": "The NMT beam decoder does not have an explicit mechanism for tracking source coverage, and there is evidence that may lead to both 'over-translation' and 'under-translation' ().", "labels": [], "entities": [{"text": "NMT beam decoder", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8094339172045389}]}, {"text": "NMT and Hiero also differ in their internal representations.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9579467177391052}, {"text": "Hiero", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8968423008918762}]}, {"text": "The NMT continuous representation captures morphological, syntactic and semantic similarity) across words and phrases.", "labels": [], "entities": []}, {"text": "However, extending these representations to the large vocabularies needed for open-domain MT is an open area of research (.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9261611104011536}]}, {"text": "By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies (.", "labels": [], "entities": []}, {"text": "Moreover, words and phrases can be easily added to a fully-trained symbolic MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9406692385673523}]}, {"text": "This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9913741946220398}, {"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9642056226730347}]}, {"text": "Adding new words and phrases to an NMT system is not as straightforward, and it is not clear that the advantages of the continuous representation can be extended to the new additions to the vocabularies.", "labels": [], "entities": []}, {"text": "NMT has the advantage of including long-range context in modelling individual translation hypotheses.", "labels": [], "entities": []}, {"text": "Hiero considers a much bigger search space, and can incorporate n-gram language models, but a much weaker translation model.", "labels": [], "entities": []}, {"text": "In this paper we try to exploit the strengths of each approach.", "labels": [], "entities": []}, {"text": "We propose to guide NMT decoding using Hiero.", "labels": [], "entities": [{"text": "NMT decoding", "start_pos": 20, "end_pos": 32, "type": "TASK", "confidence": 0.8594370186328888}, {"text": "Hiero", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.9500644207000732}]}, {"text": "We show that restricting the search space of the NMT decoder to a subset of Y spanned by Hiero effectively counteracts NMT modelling errors.", "labels": [], "entities": []}, {"text": "This can be implemented by generating translation lattices with Hiero, which are then rescored by the NMT decoder.", "labels": [], "entities": [{"text": "NMT decoder", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9101310074329376}]}, {"text": "Our approach addresses the limited vocabulary issue in NMT as we replace NMT OOVs with lattice words from the much larger Hiero vocabulary.", "labels": [], "entities": [{"text": "Hiero vocabulary", "start_pos": 122, "end_pos": 138, "type": "DATASET", "confidence": 0.9309263527393341}]}, {"text": "We also find good gains from neural and Kneser-Ney n-gram language models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate SGNMT on the WMT news-test2014 test sets (the filtered version) for English-German (En-De) and English-French (En-Fr).", "labels": [], "entities": [{"text": "SGNMT", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.6086964011192322}, {"text": "WMT news-test2014 test sets", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.9677814096212387}]}, {"text": "We also report results on WMT news-test2015 En-De.", "labels": [], "entities": [{"text": "WMT news-test2015 En-De", "start_pos": 26, "end_pos": 49, "type": "DATASET", "confidence": 0.9511308868726095}]}, {"text": "The En-De training set includes Europarl v7, Common Crawl, and News Commentary v10.", "labels": [], "entities": [{"text": "En-De training set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.7594227989514669}, {"text": "Europarl", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9504323601722717}, {"text": "Common Crawl", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8830379247665405}]}, {"text": "Sentence pairs with sentences longer than 80 words or length ratios exceeding 2.4:1 were deleted, as were Common Crawl sentences from other languages.", "labels": [], "entities": []}, {"text": "The En-Fr NMT system was trained on preprocessed data used by previous work), but with truecasing like our Hiero baseline.", "labels": [], "entities": [{"text": "En-Fr NMT", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7216163575649261}, {"text": "Hiero baseline", "start_pos": 107, "end_pos": 121, "type": "DATASET", "confidence": 0.9567441046237946}]}, {"text": "Following (Jean et al., 2015a), we use news-test2012 and news-test2013 as a development set.", "labels": [], "entities": []}, {"text": "The NMT vocabulary size is 50k for En-De and 30k for En-Fr, taken as the most frequent words in training ().", "labels": [], "entities": [{"text": "NMT vocabulary size", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.6097273230552673}]}, {"text": "1 provides statistics and shows the severity of the OOV problem for NMT.", "labels": [], "entities": [{"text": "OOV problem", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9141770899295807}, {"text": "NMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8690178990364075}]}, {"text": "The BASIC NMT system is built using the Blocks framework based on the Theano library () with standard hyper-parameters (: the encoder and decoder networks consist of 1000 gated recurrent units ().", "labels": [], "entities": [{"text": "BASIC NMT", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.6334472298622131}, {"text": "Theano library", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9713131785392761}]}, {"text": "The decoder uses a single maxout () output layer with the feed-forward attention model (.", "labels": [], "entities": []}, {"text": "The En-De Hiero system uses rules which encourage verb movement (de.", "labels": [], "entities": []}, {"text": "The rules for En-Fr were extracted from the full data set available at the WMT'15 website using a shallow-1 grammar (de.", "labels": [], "entities": [{"text": "WMT'15 website", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9626394212245941}]}, {"text": "5-gram Kneser-Ney language models (KN-LM) for the Hiero systems were trained on WMT'15 parallel and monolingual data.: BLEU English-German news-test2015 scores calculated with mteval-v13a.pl.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9782610535621643}]}, {"text": "Our SGNMT system 1 is built with the Pyfst interface 2 to OpenFst (Allauzen et al., 2007).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parallel texts and vocabulary coverage on  news-test2014.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores on news-test2014 calculated with multi-bleu.perl. NMT-LV refers to the  RNNSEARCH-LV model from (Jean et al., 2015a) for large output vocabularies.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9985024929046631}]}, {"text": " Table 3: BLEU English-German news-test2015 scores calculated with mteval-v13a.pl.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.991951584815979}]}, {"text": " Table 4: Time for lattice preprocessing operations  on English-German news-test2015.", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9802748560905457}, {"text": "English-German news-test2015", "start_pos": 56, "end_pos": 84, "type": "DATASET", "confidence": 0.7831587791442871}]}]}