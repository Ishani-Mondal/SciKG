{"title": [{"text": "Alleviating Poor Context with Background Knowledge for Named Entity Disambiguation", "labels": [], "entities": [{"text": "Named Entity Disambiguation", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6003796557585398}]}], "abstractContent": [{"text": "Named Entity Disambiguation (NED) algorithms disambiguate mentions of named entities with respect to a knowledge-base, but sometimes the context might be poor or misleading.", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7565460602442423}]}, {"text": "In this paper we introduce the acquisition of two kinds of background information to alleviate that problem: entity similarity and selectional preferences for syntactic positions.", "labels": [], "entities": []}, {"text": "We show, using a generative N\u00e4ive Bayes model for NED, that the additional sources of context are complementary, and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets, yielding the third best and the best results, respectively.", "labels": [], "entities": [{"text": "CoNLL 2003 and TAC KBP DEL 2014 datasets", "start_pos": 140, "end_pos": 180, "type": "DATASET", "confidence": 0.8579137958586216}]}, {"text": "We provide examples and analysis which show the value of the acquired background information.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of Named Entity Disambiguation (NED) is to link each mention of named entities in a document to a knowledge-base of instances.", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.813881645600001}]}, {"text": "The task is also known as Entity Linking or Entity Resolution ().", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8321525156497955}, {"text": "Entity Resolution", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.6820544004440308}]}, {"text": "NED is confounded by the ambiguity of named entity mentions.", "labels": [], "entities": [{"text": "NED", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8122768402099609}]}, {"text": "For instance, according to Wikipedia, Liechtenstein can refer to the micro-state, several towns, two castles or a national football team, among other instances.", "labels": [], "entities": []}, {"text": "Another ambiguous entity is Derbyshire which can refer to a county in England or a cricket team.", "labels": [], "entities": []}, {"text": "Most NED research use knowledge-bases derived or closely related to Wikipedia.", "labels": [], "entities": []}, {"text": "For a given mention in context, NED systems () typically rely on two models: (1) a mention module returns possible entities which can be referred to by the mention, ordered by prior probabilities; (2) a con-: Two examples where NED systems fail, motivating our two background models: similar entities (top) and selectional preferences (bottom).", "labels": [], "entities": []}, {"text": "The logos correspond to the gold label.", "labels": [], "entities": []}, {"text": "text model orders the entities according to the context of the mention, using features extracted from annotated training data.", "labels": [], "entities": []}, {"text": "In addition, some systems check whether the entity is coherent with the rest of entities mentioned in the document, although shows that the coherence module is not required for top performance.", "labels": [], "entities": []}, {"text": "shows two real examples from the development dataset which contains text from News, where the clues in the context are too weak or misleading.", "labels": [], "entities": [{"text": "News", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.940663754940033}]}, {"text": "In fact, two mentions in those examples (Derbyshire in the first and Liechtenstein in the second) are wrongly disambiguated by a bag-of-words context model.", "labels": [], "entities": []}, {"text": "In the first example, the context is very poor, and the system returns the county instead of the cricket team.", "labels": [], "entities": [{"text": "county", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9586257338523865}]}, {"text": "In order to disambiguate it correctly one needs to be aware that Derbyshire, when occurring on News, is most notably associated with cricket.", "labels": [], "entities": [{"text": "Derbyshire", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.8019811511039734}, {"text": "News", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.967965304851532}]}, {"text": "This background information can be acquired from large News corpora such as Reuters (), using distributional methods to construct a list of closely associated entities ().", "labels": [], "entities": [{"text": "Reuters", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.8767839670181274}]}, {"text": "shows entities which are distributionally similar to Derbyshire, ordered by similarity strength.", "labels": [], "entities": []}, {"text": "Although the list might say nothing to someone not acquainted with cricket, all entities in the list are strongly related to cricket: Middlesex used to be a county in the UK that gives name to a cricket club, Nottinghamshire is a county hosting two powerful cricket and football teams, Edgbaston is a suburban area and a cricket ground, the most notable team to carry the name Glamorgan is Glamorgan County Cricket Club, Trevor Barsby is a cricketer, as are all other people in the distributional context.", "labels": [], "entities": []}, {"text": "When using these similar entities as context, our system does return the correct entity for this mention.", "labels": [], "entities": []}, {"text": "In the second example, the words in the context lead the model to return the football team for Liechtenstein, instead of the country, without being aware that the nominal event \"visit to\" prefers locations arguments.", "labels": [], "entities": []}, {"text": "This kind of background information, known as selections preferences, can be easily acquired from corpora.", "labels": [], "entities": []}, {"text": "shows the most frequent entities found as arguments of \"visit to\" in the Reuters corpus.", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.94627645611763}]}, {"text": "When using these filler entities as context, the context model does return the correct entity for this mention.", "labels": [], "entities": []}, {"text": "In this article we explore the addition of two kinds of background information induced from corpora to the usual context of occurrence: (1) given a mention we use distributionally similar entities as additional context; (2) given a mention and the syntactic dependencies in the context sentence, we use the selectional preferences of those syntactic dependencies as additional context.", "labels": [], "entities": []}, {"text": "We test their contribution separately and combined, showing that they introduce complementary information.", "labels": [], "entities": []}, {"text": "Our contributions are the following: (1) we introduce novel background information to provide additional disambiguation context for NED; we integrate this information in a Bayesian generative NED model; (3) we show that similar entities are useful when no textual context is present; (4) we show that selectional preferences are useful when limited context is present; (5) both kinds of background information help improve results of a NED system, yielding the state-of-the-art in the TAC KBP DEL 2014 dataset and getting the third best results in the CoNLL 2003 dataset; (6) we release both resources for free to facilitate reproducibility.", "labels": [], "entities": [{"text": "NED", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9266960024833679}, {"text": "TAC KBP DEL 2014 dataset", "start_pos": 485, "end_pos": 509, "type": "DATASET", "confidence": 0.9323841810226441}, {"text": "CoNLL 2003 dataset", "start_pos": 552, "end_pos": 570, "type": "DATASET", "confidence": 0.9511850674947103}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first introduce the method to acquire background information, followed by the NED system.", "labels": [], "entities": []}, {"text": "Section 4 presents the evaluation datasets, Section 5 the development experiments and Section 6 the overall results.", "labels": [], "entities": []}, {"text": "They are followed by related work, error analysis and the conclusions section.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.8297858834266663}]}], "datasetContent": [{"text": "The evaluation has been performed on one of the most popular datasets, the CoNLL 2003 namedentity disambiguation dataset, also know as the AIDA or CoNLL-Yago dataset.", "labels": [], "entities": [{"text": "CoNLL 2003 namedentity disambiguation dataset", "start_pos": 75, "end_pos": 120, "type": "DATASET", "confidence": 0.8703075647354126}, {"text": "CoNLL-Yago dataset", "start_pos": 147, "end_pos": 165, "type": "DATASET", "confidence": 0.8758139312267303}]}, {"text": "It is composed of 1393 news documents from Reuters Corpora where named entity mentions have been manually identified.", "labels": [], "entities": [{"text": "It is composed of 1393 news documents from Reuters Corpora", "start_pos": 0, "end_pos": 58, "type": "DATASET", "confidence": 0.7264974385499954}]}, {"text": "It is divided in three main parts: train, testa and testb.", "labels": [], "entities": []}, {"text": "We used testa for development experiments, and testb for the final results and comparison with the state-ofthe-art.", "labels": [], "entities": []}, {"text": "We ignored the training part.", "labels": [], "entities": []}, {"text": "In addition, we also report results in the Text Analysis Conference 2014 Diagnostic Entity Linking task dataset (TAC DEL 2014).", "labels": [], "entities": [{"text": "Text Analysis Conference 2014 Diagnostic Entity Linking task dataset (TAC DEL 2014)", "start_pos": 43, "end_pos": 126, "type": "TASK", "confidence": 0.8553581195218223}]}, {"text": "The gold standard for this task is very similar to the CoNLL dataset, where target named entity mentions have been detected by hand.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9654853940010071}]}, {"text": "Through the beginning of the task (2009 to 2013) the TAC datasets were query-driven, that is, the input included a document and a challenging and sometimes partial target-mention to disambiguate.", "labels": [], "entities": [{"text": "TAC datasets", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.7523185014724731}]}, {"text": "As this task also involved mention detection and our techniques are sensitive to mention detection errors, we preferred to factor out that variation and focus on the 2014.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7480032444000244}, {"text": "2014.", "start_pos": 166, "end_pos": 171, "type": "DATASET", "confidence": 0.9472213685512543}]}, {"text": "The evaluation measure used in this paper is micro-accuracy, that is, the percentage of linkable mentions that the system disambiguates correctly, as widely used in the CoNLL dataset.", "labels": [], "entities": [{"text": "micro-accuracy", "start_pos": 45, "end_pos": 59, "type": "METRIC", "confidence": 0.9813636541366577}, {"text": "CoNLL dataset", "start_pos": 169, "end_pos": 182, "type": "DATASET", "confidence": 0.9671328365802765}]}, {"text": "Note The best combination was \u03b1 = 0.05, \u03b2 = 0.1, \u03b3 = 0.55  We started to check the contribution of the acquired background information in the testa section of the CoNLL dataset.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.9760417938232422}]}, {"text": "In fact, we decided to focus first on a subset of testa about sports, and also acquired background information from the sports sub-collection of the Reuters corpus.", "labels": [], "entities": [{"text": "Reuters corpus", "start_pos": 149, "end_pos": 163, "type": "DATASET", "confidence": 0.9649433195590973}]}, {"text": "The rationale was that we wanted to start in a controlled setting, and having assumed that the domain of the test documents and the source of the background information could play a role, we decided to start focusing on the sports domain first.", "labels": [], "entities": []}, {"text": "Another motivation is that we noticed that the ambiguity between locations and sport clubs (e.g. football, cricket, rugby, etc.) is challenging, as shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Document and linkable mention counts  for CoNLL and TAC2014 DEL datasets.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9328389167785645}, {"text": "TAC2014 DEL datasets", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.8632614413897196}]}, {"text": " Table 4: Results on mentions with limited context  on the sports subset of testa, limited to the 41% of  the mentions (cf. Section 5.3)", "labels": [], "entities": []}, {"text": " Table 5: Results on the entire sports subset of  testa: middle column uses the sports subset of  Reuters to acquire background information, right  column uses the full Reuters (cf. Section 5.4).", "labels": [], "entities": [{"text": "Reuters", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9499298334121704}, {"text": "Reuters", "start_pos": 169, "end_pos": 176, "type": "DATASET", "confidence": 0.9529346823692322}]}, {"text": " Table 6: Results on the full testa dataset (cf. Sec- tion 5.5).", "labels": [], "entities": [{"text": "full testa dataset", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.6384089390436808}]}, {"text": " Table 7: Overall micro accuracy results on the  CoNLL testb and TAC 2014 DEL datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8885759115219116}, {"text": "CoNLL testb", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9727237522602081}, {"text": "TAC 2014 DEL datasets", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.8826601058244705}]}, {"text": " Table 8: Overall micro accuracy results on the  CoNLL testb and TAC 2014 DEL datasets, includ- ing the current state-of-the-art. Starred results are  not comparable, see text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.939802885055542}, {"text": "CoNLL testb", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9753262102603912}, {"text": "TAC 2014 DEL datasets", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.8945661336183548}]}]}