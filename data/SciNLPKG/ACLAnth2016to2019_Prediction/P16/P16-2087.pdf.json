{"title": [{"text": "Nonparametric Spherical Topic Modeling with Word Embeddings", "labels": [], "entities": [{"text": "Spherical Topic Modeling", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.7186403075853983}]}], "abstractContent": [{"text": "Traditional topic models do not account for semantic regularities in language.", "labels": [], "entities": []}, {"text": "Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity.", "labels": [], "entities": []}, {"text": "However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere.", "labels": [], "entities": []}, {"text": "Such a representation is well-suited for directional data.", "labels": [], "entities": []}, {"text": "We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Vari-ational Inference.", "labels": [], "entities": []}, {"text": "This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Prior work on topic modeling has mostly involved the use of categorical likelihoods ().", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.8863953351974487}]}, {"text": "Applications of topic models in the textual domain treat words as discrete observations, ignoring the semantics of the language.", "labels": [], "entities": []}, {"text": "Recent developments in distributional representations of words (, * Authors contributed equally and listed alphabetically.", "labels": [], "entities": []}, {"text": "Code is available at https://github.com/ Ardavans/sHDP.", "labels": [], "entities": [{"text": "Ardavans", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8813777565956116}]}], "datasetContent": [{"text": "Setup We perform experiments on two different text corpora: 11266 documents from 20 NEWS-GROUPS 2 and 1566 documents from the NIPS corpus . We utilize 50-dimensional word embeddings trained on text from Wikipedia using word2vec 4 . The vectors are normalized to have unit 2 -norm, which has been shown to provide superior performance ().", "labels": [], "entities": [{"text": "NIPS corpus", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.9704487025737762}]}, {"text": "We evaluate our model using the measure of topic coherence, which has been shown to effectively correlate with human judgement ().", "labels": [], "entities": []}, {"text": "For this, we compute the Pointwise Mutual Information (PMI) using a reference corpus of 300k documents from Wikipedia.", "labels": [], "entities": [{"text": "Pointwise Mutual Information (PMI)", "start_pos": 25, "end_pos": 59, "type": "METRIC", "confidence": 0.7255523353815079}]}, {"text": "The PMI is calculated using cooccurence statistics over pairs of words (u i , u j ) in 20-word sliding windows: Additionally, we also use the metric of normalized PMI (NPMI) to evaluate the models in a similar fashion: We compare our model with two baselines: HDP and the Gaussian LDA model.", "labels": [], "entities": []}, {"text": "We ran G-LDA with various number of topics (k).", "labels": [], "entities": []}, {"text": "Results details the topic coherence averaged overall topics produced by each model.", "labels": [], "entities": []}, {"text": "We observe that our sHDP model outperforms G-LDA by 0.08 points on 20 NEWSGROUPS and by 0.17 points in terms of PMI on the NIPS dataset.", "labels": [], "entities": [{"text": "NEWSGROUPS", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9486067295074463}, {"text": "PMI", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9164311289787292}, {"text": "NIPS dataset", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9832628071308136}]}, {"text": "The NPMI scores also show a similar trend with sHDP obtaining the best scores on both datasets.", "labels": [], "entities": []}, {"text": "We can also see that the individual topics inferred: Examples of top words for the most coherent topics (column-wise) inferred on the NIPS dataset by Gaussian LDA (k=40) and Spherical HDP.", "labels": [], "entities": [{"text": "NIPS dataset", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.9733226001262665}, {"text": "Spherical HDP", "start_pos": 174, "end_pos": 187, "type": "DATASET", "confidence": 0.640019953250885}]}, {"text": "The last row for each model is the topic coherence (PMI) computed using Wikipedia documents as reference.  by sHDP make sense qualitatively and have higher coherence scores than G-LDA.", "labels": [], "entities": []}, {"text": "This supports our hypothesis that using the vMF likelihood helps in producing more coherent topics.", "labels": [], "entities": [{"text": "vMF likelihood", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.7231900990009308}]}, {"text": "sHDP produces 16 topics for the 20 NEWSGROUPS and 92 topics on the NIPS dataset.", "labels": [], "entities": [{"text": "sHDP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8975184559822083}, {"text": "NEWSGROUPS", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9214147925376892}, {"text": "NIPS dataset", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9755355715751648}]}, {"text": "shows a plot of normalized loglikelihood against the runtime of sHDP and G-LDA.", "labels": [], "entities": []}, {"text": "We calculate the normalized value of loglikelihood by subtracting the minimum value from it and dividing it by the difference of maximum Our sHDP implementation is in Python and the G-LDA code is in Java.", "labels": [], "entities": []}, {"text": "G-LDA sHDP: Normalized log-likelihood (in percentage) over a training set of size 1566 documents from the NIPS corpus.", "labels": [], "entities": [{"text": "NIPS corpus", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.9693135023117065}]}, {"text": "Since the log-likelihood values are not comparable for the Gaussian LDA and the sHDP, we normalize them to demonstrate the convergence speed of the two inference schemes for these models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of top words for the most coherent topics (column-wise) inferred on the NIPS dataset  by Gaussian LDA (k=40) and Spherical HDP. The last row for each model is the topic coherence (PMI)  computed using Wikipedia documents as reference.", "labels": [], "entities": [{"text": "NIPS dataset", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9637734293937683}]}, {"text": " Table 2: Average topic coherence for various base- lines (HDP, Gaussian LDA (G-LDA)) and sHDP.  k=number of topics. Best scores are shown in  bold.", "labels": [], "entities": []}]}