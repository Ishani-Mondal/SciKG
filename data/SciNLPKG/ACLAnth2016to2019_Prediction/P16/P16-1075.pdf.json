{"title": [{"text": "Constrained Multi-Task Learning for Automated Essay Scoring", "labels": [], "entities": [{"text": "Automated Essay Scoring", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7082273364067078}]}], "abstractContent": [{"text": "Supervised machine learning models for automated essay scoring (AES) usually require substantial task-specific training data in order to make accurate predictions fora particular writing task.", "labels": [], "entities": [{"text": "automated essay scoring (AES)", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.8015326460202535}]}, {"text": "This limitation hinders their utility, and consequently their deployment in real-world settings.", "labels": [], "entities": []}, {"text": "In this paper, we overcome this shortcoming using a constrained multi-task pairwise-preference learning approach that enables the data from multiple tasks to be combined effectively.", "labels": [], "entities": []}, {"text": "Furthermore, contrary to some recent research , we show that high performance AES systems can be built with little or no task-specific training data.", "labels": [], "entities": []}, {"text": "We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated essay scoring (AES) involves the prediction of a score (or scores) relating to the quality of an extended piece of written text.", "labels": [], "entities": [{"text": "Automated essay scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7513246436913809}]}, {"text": "With the burden involved in manually grading student texts and the increase in the number of ESL (English as a second language) learners worldwide, research into AES is increasingly seen as playing a viable role in assessment.", "labels": [], "entities": []}, {"text": "Automating the assessment process is not only useful for educators but also for learners, as it can provide instant feedback and encourage iterative refinement of their writing.", "labels": [], "entities": []}, {"text": "The AES task has usually been addressed using machine learning.", "labels": [], "entities": [{"text": "AES task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.6118549108505249}]}, {"text": "Given a set of texts and associated gold scores, machine learning approaches aim to build models that can generalise to unseen instances.", "labels": [], "entities": []}, {"text": "Regression, classification, and preferenceranking 1 approaches have all been applied to the task.", "labels": [], "entities": [{"text": "Regression", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9827082753181458}]}, {"text": "In general, machine learning models only perform well when the training and test instances are from similar distributions.", "labels": [], "entities": []}, {"text": "However, it is usually the case that essays are written in response to prompts which are carefully designed to elicit answers according to a number of dimensions (e.g. register, topic, and genre).", "labels": [], "entities": []}, {"text": "For example, shows extracts from two prompts from a publicly available dataset 2 that aim to elicit different genres of persuasive/argumentative responses on different topics.", "labels": [], "entities": []}, {"text": "Most previous work on AES has either ignored the differences between essays written in response to different prompts) with the aim of building general AES systems, or has built prompt-specific models for each prompt independently).", "labels": [], "entities": []}, {"text": "One of the problems hindering the wide-scale adoption and deployment of AES systems is the dependence on prompt-specific training data, i.e. substantial model retraining is often needed when anew prompt is released.", "labels": [], "entities": []}, {"text": "Therefore, systems that can adapt to new writing tasks (i.e. prompts) with relatively few new task-specific training examples are particularly appealing.", "labels": [], "entities": []}, {"text": "For example, a system that is trained using only responses from prompt #1 in may not generalise well to essays written in response to prompt #2, and vice versa.", "labels": [], "entities": []}, {"text": "Even more complications arise when the scoring scale, marking criteria, and/or grade level (i.e. educational stage) vary from task #1 Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends.", "labels": [], "entities": []}, {"text": "Write a letter to your local newspaper in which you state your opinion on the effects computers have on people.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to compare our baseline with previous work, we use the ASAP (Automated Student Assessment Prize) public dataset.", "labels": [], "entities": [{"text": "ASAP (Automated Student Assessment Prize) public dataset", "start_pos": 64, "end_pos": 120, "type": "DATASET", "confidence": 0.6110640598667992}]}, {"text": "Some details of the essays for the eight tasks in the dataset are described in the.", "labels": [], "entities": []}, {"text": "The prompts elicit responses of different genres and of different lengths.", "labels": [], "entities": []}, {"text": "In particular, it is important to note that the prompts have different scoring scales and are associated with different grade levels.", "labels": [], "entities": []}, {"text": "Furthermore, the gold scores are distributed differently even if resolved to a common 0-60 scale.", "labels": [], "entities": []}, {"text": "In order to benchmark our baseline system against previously developed approaches (BLRR and SVM regression () which use this data, we learned task-specific models using 5-fold cross-validation within each of the eight ASAP sets and aim to predict the unresolved original score as per previous work.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.614726185798645}, {"text": "ASAP sets", "start_pos": 218, "end_pos": 227, "type": "DATASET", "confidence": 0.7428175508975983}]}, {"text": "We present the quadratic weighted kappa (QW-\u03ba) of the systems in.", "labels": [], "entities": []}, {"text": "Our baseline preference-ranking model (TAP) outperforms previous approaches on task-specific data.", "labels": [], "entities": []}, {"text": "It is worth noting that we did not tune either of the hyperparameters of TAP.", "labels": [], "entities": [{"text": "TAP", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.7108986377716064}]}, {"text": "The results for BLRR and SVM regression are taken directly from the original work and it is unlikely that we have used the exact same fold split.", "labels": [], "entities": [{"text": "BLRR", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.4420202076435089}, {"text": "SVM regression", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7522704005241394}]}, {"text": "Regardless, the consistent increases mean that TAP represents a strong baseline system upon which we develop our constrained multi-task approach.", "labels": [], "entities": [{"text": "TAP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.6932486891746521}]}, {"text": "In this section, we outline the different learning scenarios, data folds, and evaluation metrics used in our main experiments.", "labels": [], "entities": []}, {"text": "We use both Spearman's \u03c1 correlation and Quadratic-weighted \u03ba (QW-\u03ba) to evaluate the performance of all approaches.", "labels": [], "entities": [{"text": "\u03c1 correlation", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.6797388941049576}, {"text": "Quadratic-weighted \u03ba (QW-\u03ba)", "start_pos": 41, "end_pos": 68, "type": "METRIC", "confidence": 0.9411534190177917}]}, {"text": "Spearman's \u03c1 measures the quality of the ranking of predicted scores produced by the system (i.e. the output from the ranking-preference model).", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.6583161850770315}]}, {"text": "We calculate Spearman's \u03c1 using the ordinal gold score and the realvalued prediction on the original prompt-specific scoring scale of each prompt.", "labels": [], "entities": []}, {"text": "Statistical significant differences between two correlations sharing one dependent variable (i.e. the gold scores) can be determined using test.", "labels": [], "entities": []}, {"text": "QW-\u03ba measures the chance corrected agreement between the predicted scores and the gold scores.", "labels": [], "entities": [{"text": "chance corrected agreement", "start_pos": 18, "end_pos": 44, "type": "METRIC", "confidence": 0.9187244574228922}]}, {"text": "QW-\u03ba can be viewed as a measure of accuracy as it is lower when the predicted scores are further away from the gold scores.", "labels": [], "entities": [{"text": "QW-\u03ba", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6917709112167358}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9992511868476868}]}, {"text": "This metric measures both the quality of the ranking of scores and the quality of the linear regression step of our approach.", "labels": [], "entities": []}, {"text": "These metrics are complementary as they measure different aspects of performance.", "labels": [], "entities": []}, {"text": "We calculate QW-\u03ba using the ordinal gold score and the real-valued prediction rounded to the nearest score on the original prompt-specific scale (see).", "labels": [], "entities": []}, {"text": "All-TAP Tgt-TAP All-MTL-TAP All-MTL-cTAP: Average QW-\u03ba over two folds for all tasks assize of target task training data increases 7 Results and Discussion and show the performance of a number of models for both \u03c1 and \u03ba respectively.", "labels": [], "entities": []}, {"text": "In general, we see that the MTL versions nearly always outperform the baseline TAP when using the same training data.", "labels": [], "entities": [{"text": "MTL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.8909700512886047}, {"text": "TAP", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9156943559646606}]}, {"text": "This shows that multitask learning is superior to simply using the source tasks as extra training data for the AES task.", "labels": [], "entities": []}, {"text": "Interestingly this has not been shown before.", "labels": [], "entities": []}, {"text": "Furthermore, the MTL-cTAP approach tends to be significantly better than the other for many prompts under varying scenarios for both Spearman's \u03c1 and QW-\u03ba.", "labels": [], "entities": []}, {"text": "This shows that models that attempt to directly compare essays scores across certain writing-tasks lead to poorer performance.", "labels": [], "entities": []}, {"text": "When looking at Spearman's \u03c1 in we see that the models that do not use any target task data during training (Src) can achieve a performance which is close to the baseline that only uses all of the available target data (Tgt-TAP).", "labels": [], "entities": []}, {"text": "This indicates that our system can rank essays well without any target task data.", "labels": [], "entities": []}, {"text": "However, it is worth noting that without any target task training data and lacking any prior information as to the distribution of gold scores for the target task, achieving a consistently high accuracy (i.e. QW-\u03ba) is extremely difficult (if not impossible).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9971339702606201}, {"text": "QW-\u03ba)", "start_pos": 209, "end_pos": 214, "type": "METRIC", "confidence": 0.8559103012084961}]}, {"text": "Therefore, only shows results for models that make use of target task data.", "labels": [], "entities": []}, {"text": "For the models trained with data from all eight tasks, we can see that All-MTL-cTAP outperforms both All-TAP and All-MTL-TAP on most of the tasks for both evaluation metrics (\u03c1 and \u03ba).", "labels": [], "entities": []}, {"text": "Interestingly, All-MTL-cTAP also outperforms Tgt-TAP on most of the prompts for both evaluation metrics.", "labels": [], "entities": []}, {"text": "This indicates that All-MTL-cTAP manages to successfully incorporate useful information from the source tasks even when there is ample target-task data.", "labels": [], "entities": []}, {"text": "We next look at scenarios when target-task training data is lacking.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Details of ASAP dataset and a preliminary evaluation of the performance of our TAP baseline  against previous work (", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.7892391085624695}]}, {"text": " Table 3: Average Spearman \u03c1 of systems over two-folds on the ASAP dataset. The best approach per  prompt is in bold.  \u2021 ( \u2020) means that \u03c1 is statistically greater than Src-TAP (top half) and All-TAP (bottom  half) using the Steiger test at the 0.05 level ( \u2021 means significant for both folds,  \u2020 means for one of the  folds), while  means statistically greater than All-MTL-TAP on both folds ( for one fold).", "labels": [], "entities": [{"text": "Average Spearman \u03c1", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.8135184446970621}, {"text": "ASAP dataset", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.9506548643112183}]}, {"text": " Table 4: Average QW-\u03ba of systems over two-folds on the ASAP dataset. The best approach per prompt  is in bold.  \u2021 ( \u2020) means that \u03ba is statistically (p < 0.05) greater than All-TAP using an approximate  randomisation test", "labels": [], "entities": [{"text": "QW-\u03ba", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.7485774159431458}, {"text": "ASAP dataset", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.9399214684963226}]}, {"text": " Table 5: Highest weighted lexical features (i.e. unigrams, bigrams, or trigrams) and their weights in  both shared and task-specific representations of the All-MTL-cTAP model (associated with results in", "labels": [], "entities": []}]}