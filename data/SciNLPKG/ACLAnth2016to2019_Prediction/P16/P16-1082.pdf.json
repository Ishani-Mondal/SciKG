{"title": [{"text": "Modeling Concept Dependencies in a Scientific Corpus", "labels": [], "entities": [{"text": "Modeling Concept Dependencies", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9082872271537781}]}], "abstractContent": [{"text": "Our goal is to generate reading lists for students that help them optimally learn technical material.", "labels": [], "entities": []}, {"text": "Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query.", "labels": [], "entities": []}, {"text": "This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered.", "labels": [], "entities": []}, {"text": "Here we formulate an information-theoretic view of concept dependency and present methods to construct a \"concept graph\" automatically from a text corpus.", "labels": [], "entities": []}, {"text": "We perform the first human evaluation of concept dependency edges (to be published as open data), and the results verify the feasibility of automatic approaches for inferring concepts and their dependency relations.", "labels": [], "entities": []}, {"text": "This result can support search capabilities that maybe tuned to help users learn a subject rather than retrieve documents based on a single query.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpora of technical documents, such as the ACL Anthology, are valuable for learners, but it can be difficult to find the most appropriate documents to read in order to learn about a concept.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9441180229187012}]}, {"text": "This problem is made more complicated by the need to trace the ideas back to those that need to be learned first (e.g., before you can learn about Markov logic networks, you should understand first-order logic and probability).", "labels": [], "entities": []}, {"text": "That is, a crucial question when learning anew subject is \"What do I need to know before I start reading about this?\"", "labels": [], "entities": []}, {"text": "To answer this question, learners typically rely on the guidance of domain experts, who can devise pedagogically valuable reading lists that order doc- uments to progress from prerequisite to target concepts.", "labels": [], "entities": []}, {"text": "Thus, it is desirable to have a model where each concept is linked to the prerequisite concepts it depends upon -a concept graph.", "labels": [], "entities": []}, {"text": "A manually constructed concept graph excerpt related to automatic speech recognition is shown in.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6025669972101847}]}, {"text": "The dependency relation between two concepts is interpreted as whether understanding one concept would help a learner understand the other.", "labels": [], "entities": []}, {"text": "Representing a scientific corpus in this way can improve tasks such as curriculum planning (, automatic reading list generation, and improving education quality.", "labels": [], "entities": [{"text": "curriculum planning", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.672219380736351}, {"text": "automatic reading list generation", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.52780931442976}]}, {"text": "Motivated by the importance of representing the content of a scientific corpus as a concept graph, the challenge we address in this work is to automatically infer the concepts and their dependency relations.", "labels": [], "entities": []}, {"text": "Towards this end, we first instantiate each concept as a topic from statistical topic modeling (.", "labels": [], "entities": []}, {"text": "To link concepts with directed depen-dency edges, we propose the use of informationtheoretic measures, which we compare against baseline methods of computing word similarity, hierarchical clustering, and citation prediction.", "labels": [], "entities": [{"text": "citation prediction", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.875587671995163}]}, {"text": "We then gather human annotations of concept graph nodes and edges learned from the ACL Anthology, which we use to evaluate these methods.", "labels": [], "entities": [{"text": "ACL Anthology", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.9627192616462708}]}, {"text": "The main contributions of this paper are: 1 We introduce the concept graph representation for modeling the technical concepts in a corpus and their relations.", "labels": [], "entities": []}, {"text": "2 We present information-theoretic approaches to infer concept dependence relations.", "labels": [], "entities": []}, {"text": "3 We perform the first human annotation of concept dependence fora technical corpus.", "labels": [], "entities": []}, {"text": "We release the human annotation data for use in future research.", "labels": [], "entities": []}, {"text": "In the following section, we contrast this problem with previous work.", "labels": [], "entities": []}, {"text": "We then describe the concept graph framework (Section 3) and present automatic approaches for inferring concept graphs (Section 4).", "labels": [], "entities": []}, {"text": "The details of human evaluation are presented in Section 5.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7301487326622009}]}, {"text": "We discuss some interesting open questions related to this work in Section 6 before concluding this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are two main approaches to evaluating a concept graph: We can directly evaluate the graph, using human judgments to measure the quality of the concepts and the reliability of the links between them.", "labels": [], "entities": []}, {"text": "Alternatively, we can evaluate the application of a concept graph to a task, such as ordering documents fora reading list or recommending documents to cite when writing a paper.", "labels": [], "entities": []}, {"text": "Our motivation to build a concept graph from a technical corpus is to improve performance at the task of reading list generation.", "labels": [], "entities": [{"text": "reading list generation", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.7135803997516632}]}, {"text": "However, an applied evaluation makes it harder to judge the quality of the concept graph itself.", "labels": [], "entities": []}, {"text": "Each document contains a combination of concepts, which have different ordering restrictions, and other factors also affect the quality of a reading list, such as the classification of document difficulty and type (e.g., survey, tutorial, or experimental results).", "labels": [], "entities": []}, {"text": "As such, we focus on a direct human evaluation of our proposed methods for building a concept graph and leave the measure of applied performance to future work.", "labels": [], "entities": []}, {"text": "For this evaluation, the scientific corpus we use is the ACL Anthology.) for documents where it was observed to be of higher quality.", "labels": [], "entities": [{"text": "ACL Anthology.", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.976090133190155}]}, {"text": "The text was processed to join words that were split across lines with hyphens.", "labels": [], "entities": []}, {"text": "We manually removed documents that were not written in English or where text extraction failed, leaving 20,264 documents, though this filtering was not exhaustive.", "labels": [], "entities": [{"text": "text extraction", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7188428342342377}]}, {"text": "The topic model we used was built using the Mallet) implementation of LDA.", "labels": [], "entities": [{"text": "LDA", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.5646364092826843}]}, {"text": "It is composed of bigrams, filtered of typical English stop words before the generation of bigrams, so that, e.g., \"word to word\" yields the bigram \"word word\".", "labels": [], "entities": []}, {"text": "We generated topic models consisting of between 20 and 400 topics and selected a 300-topic model based on manual inspection.", "labels": [], "entities": []}, {"text": "Documents were linked to concepts based on the document's LDA topic composition.", "labels": [], "entities": []}, {"text": "The concept nodes for each topic were linked in concept dependency relations using each of the methods described in Section 4, producing five concept graphs to evaluate.", "labels": [], "entities": []}, {"text": "We applied the general cross-entropy method to the distribution of top-k bigrams for each concept.", "labels": [], "entities": []}, {"text": "For all methods, the results we report are fork = 20.", "labels": [], "entities": [{"text": "fork", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9731020927429199}]}, {"text": "Changing this value shifts the precisionrecall trade-off, but in our experiments, the relative performance of the methods are generally consistent for different values of k.", "labels": [], "entities": [{"text": "precisionrecall", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.9993801116943359}]}, {"text": "Since it is impractical to manually annotate all pairs of concept nodes from a 300-node graph, we selected a subset of edges for evaluation.", "labels": [], "entities": []}, {"text": "Intuitively, the evaluation set should satisfy the following sampling criteria: (1) The evaluation set should cover the top weighted edges fora precision evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9939436316490173}]}, {"text": "(2) The evaluation set should cover the bottomweighted edges fora recall evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9873825907707214}]}, {"text": "The evaluation set should provide low-biased sampling.", "labels": [], "entities": []}, {"text": "With respect to these requirements, we generated an evaluation edge set as the union of the following three sets: 1 Top-20 edges for each approach (including baseline approaches) 2 A random shuffle selection from the union of machine translation, translation system, mt system, transfer rules, mt systems, lexical transfer, analysis transfer, translation process, transfer generation, transfer component, analysis synthesis, transfer phase, analysis generation, structural transfer, transfer approach, human translation, transfer grammar, analysis phase, translation systems, transfer process Relevant documents: \u2022 the top-50 and bottom-50 edges in terms of the baseline word similarity.", "labels": [], "entities": [{"text": "human translation", "start_pos": 502, "end_pos": 519, "type": "TASK", "confidence": 0.7386889457702637}]}, {"text": "1 3 A random shuffle section from the union of top-100 edges in terms of the proposed approaches.", "labels": [], "entities": []}, {"text": "To measure the quality of the concept dependency edges in our graphs, we compute the average precision for the strongest edges in each concept graph, up to three thresholds: the top 20 edges, the top 150, and all edges with strength > 0.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9788313508033752}]}, {"text": "These precision scores are in as well as the corresponding recall, and f 1 scores for the larger thresholds.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9994931221008301}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9994820952415466}, {"text": "f 1", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9268161654472351}]}, {"text": "Despite the difference in inter-annotator agreement reported in, the ordering of methods by precision is the same whether we consider only the judgments of NLP experts, non-NLP judges, or everyone, so we only report the average across all annotators.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9974835515022278}]}, {"text": "When we examine the results of precision at 20 -the strongest edges predicted by each method -we see that the cross-entropy method performs best.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9981896281242371}]}, {"text": "For comparison, we report the accuracy of a baseline of random numbers between 0 and 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9995641112327576}]}, {"text": "While all methods have better than chance precision, the random baseline has higher recall since it predicts a dependency relation of non-zero strength for all pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.7584333419799805}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9995761513710022}]}, {"text": "As we consider edges predicted with lower confidence, the word similarity approach shows the highest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9975966811180115}]}, {"text": "A limitation of the word similarity baseline is that it is symmetric while concept dependence relations can be asymmetric.", "labels": [], "entities": []}, {"text": "Annotators marked many pairs of concepts as being at least somewhat co-dependent.", "labels": [], "entities": []}, {"text": "E.g., understanding Speech recognition strongly helps you understand Natural language processing, but being familiar with this broader topic also somewhat helps you understand the narrower one.", "labels": [], "entities": [{"text": "understanding Speech recognition", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.7243310411771139}, {"text": "Natural language processing", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.6365468700726827}]}, {"text": "The precision scores we report count both annotations of concept dependence (\"Somewhat\" and \"Very much\") as positive predictions, but other evaluation metrics might show a greater benefit for methods like D CE that can predict dependency with asymmetric strengths.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9963317513465881}, {"text": "D CE", "start_pos": 205, "end_pos": 209, "type": "TASK", "confidence": 0.6015333533287048}]}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement measured as  Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.8928379416465759}]}, {"text": " Table 2: Precision, recall, and f-scores (with different thresholds for which edges are included) for the  methods of predicting dependency relations between concepts described in Section 4.2.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9989884495735168}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9950044751167297}]}]}