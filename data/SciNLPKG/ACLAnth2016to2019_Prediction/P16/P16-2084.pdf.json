{"title": [{"text": "Is \"Universal Syntax\" Universally Useful for Learning Distributed Word Representations?", "labels": [], "entities": [{"text": "Learning Distributed Word Representations", "start_pos": 45, "end_pos": 86, "type": "TASK", "confidence": 0.6411279886960983}]}], "abstractContent": [{"text": "Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) for learning distributed word representations for similarity tasks.", "labels": [], "entities": []}, {"text": "In English, DEPS tend to perform better than the more common, less informed bag-of-words contexts (BOW).", "labels": [], "entities": [{"text": "DEPS", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.586196780204773}, {"text": "BOW", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9162932634353638}]}, {"text": "In this paper, we present the first cross-linguistic comparison of different context types for three different languages.", "labels": [], "entities": []}, {"text": "DEPS are extracted from \"universal parses\" without any language-specific optimization.", "labels": [], "entities": []}, {"text": "Our results suggest that the universal DEPS (UDEPS) are useful for detecting functional similarity (e.g., verb similarity, solving syntactic analogies) among languages , but their advantage over BOW is not as prominent as previously reported on English.", "labels": [], "entities": [{"text": "BOW", "start_pos": 195, "end_pos": 198, "type": "METRIC", "confidence": 0.8229045867919922}]}, {"text": "We also show that simple \"post-parsing\" filtering of useful UDEPS contexts leads to consistent improvements across languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in abroad range of NLP tasks, e.g.,).", "labels": [], "entities": []}, {"text": "The omnipresent word2vec skip-gram model with negative sampling (SGNS) () is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (.", "labels": [], "entities": [{"text": "word representation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7206595689058304}]}, {"text": "The original implementation of SGNS learns word representations from local bag-of-words contexts (BOW).", "labels": [], "entities": []}, {"text": "However, the underlying SGNS model is equally applicable to other context types.", "labels": [], "entities": []}, {"text": "Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) for the task.", "labels": [], "entities": []}, {"text": "In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle).", "labels": [], "entities": []}, {"text": "DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (.", "labels": [], "entities": []}, {"text": "However, these studies have all focused solely on English.", "labels": [], "entities": []}, {"text": "A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn.", "labels": [], "entities": []}, {"text": "Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (, this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline that relies on languageuniversal syntactic processing (UDEPS)?", "labels": [], "entities": [{"text": "universal POS (UPOS) tagging and dependency (UD) parsing", "start_pos": 130, "end_pos": 186, "type": "TASK", "confidence": 0.7063005417585373}, {"text": "DEPS extraction pipeline", "start_pos": 279, "end_pos": 303, "type": "TASK", "confidence": 0.8012931744257609}]}, {"text": "(Q2) Are UDEPS universally better than BOW for learning distributed word representations in other languages?", "labels": [], "entities": [{"text": "BOW", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9696003794670105}]}, {"text": "Regarding Q1, the results show that it is possible to replace original DEPS with UDEPS for English and to obtain benchmarking results with only a slight drop in performance.", "labels": [], "entities": []}, {"text": "As for Q2, the framework is not equally effective in other languages, as suggested by the performance in Italian and German, which sheds new light on the usefulness of BOW and dependency-based contexts.", "labels": [], "entities": []}, {"text": "Further, the results reveal that even a simple preliminary \"post-parsing\" selection of use-ful UDEPS contexts leads to consistent improvements across languages, especially in detecting functional similarity.", "labels": [], "entities": []}, {"text": "This focused contribution is the first crosslinguistic comparison of different context types for learning word representations in three languages, reaching beyond English.", "labels": [], "entities": []}, {"text": "It also constitutes a first completely language-universal and widely applicable framework for UDEPS extraction.", "labels": [], "entities": [{"text": "UDEPS extraction", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.9352514743804932}]}], "datasetContent": [{"text": "Evaluation Our cross-linguistic study is made possible not only thanks to the \"universal NLP\" initiative but also owing to the benchmarking evaluation sets for other languages beyond English (i.e., IT, DE) that have very recently become available, e.g.,     The results are consistent with prior work on the UD treebanks, e.g.,.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 308, "end_pos": 320, "type": "DATASET", "confidence": 0.9375922381877899}]}, {"text": "Training Setup The SGNS preprocessing scheme for English was replicated from () and extended to the other two languages: all tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered.", "labels": [], "entities": []}, {"text": "Exactly the same vocabularies were used with all context types (approx. 185K distinct EN words, 163K DE words, and 83K IT words).", "labels": [], "entities": []}, {"text": "The word2vecf SGNS was trained using standard settings: 15 epochs, 15 negative samples, global learning rate 0.025, subsampling rate 1e \u2212 4.", "labels": [], "entities": [{"text": "global learning rate 0.025", "start_pos": 88, "end_pos": 114, "type": "METRIC", "confidence": 0.8209284394979477}]}, {"text": "All WEs were trained with d = 50, 100, 300, 500, 600.", "labels": [], "entities": []}, {"text": "BOW-based WEs were trained with k = 2 (BOW-2), proven to be the (near-)optimal choice across various semantic tasks in related work (.", "labels": [], "entities": [{"text": "BOW-based WEs", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7446314990520477}, {"text": "BOW-2", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9213783144950867}]}, {"text": "The same k was used for POSIT-based WEs (POSIT-2).", "labels": [], "entities": [{"text": "POSIT-based WEs", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.7052775919437408}]}, {"text": "SimLex pairs, and 0.378 on verb pairs.", "labels": [], "entities": []}, {"text": "9 A comparison with UDEPS-ARC reveals only a slight drop in performance when switching to languageagnostic UDEPS (see(a), Q1).", "labels": [], "entities": []}, {"text": "However, the results are heavily dependent on the actual language: the claims made for English (i.e., DEPS \u2265 BOW) do not extend to other languages (Q2).", "labels": [], "entities": [{"text": "DEPS \u2265 BOW)", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.8197103291749954}]}, {"text": "A comparison of results from Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9681320190429688}]}, {"text": "1 with the task evaluation also shows that excellent tagging and parsing results do not guarantee a strong task performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.646537721157074}]}], "tableCaptions": [{"text": " Table 1: Universal POS tagging accuracy scores  and labeled (LAS) vs unlabeled (UAS) attachment  scores of universal dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.525264710187912}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.7446432113647461}, {"text": "universal dependency parsing", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.6569962501525879}]}, {"text": " Table 4: Results on SimLex in English with SGNS  trained on a reduced EN training set containing the  same number of sentences as the entire IT training  set (\u2248 13M sentences). d = 300.", "labels": [], "entities": []}]}