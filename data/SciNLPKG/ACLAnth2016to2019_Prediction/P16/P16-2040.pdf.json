{"title": [{"text": "An Entity-Focused Approach to Generating Company Descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "Finding quality descriptions on the web, such as those found in Wikipedia articles , of newer companies can be difficult: search engines show many pages with varying relevance, while multi-document summarization algorithms find it difficult to distinguish between core facts and other information such as news stories.", "labels": [], "entities": []}, {"text": "In this paper, we propose an entity-focused, hybrid generation approach to automatically produce descriptions of previously unseen companies, and show that it outperforms a strong summarization baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "As new companies form and grow, it is important for potential investors, procurement departments, and business partners to have access to a 360-degree view describing them.", "labels": [], "entities": []}, {"text": "The number of companies worldwide is very large and, for the vast majority, not much information is available in sources like Wikipedia.", "labels": [], "entities": []}, {"text": "Often, only firmographics data (e.g. industry classification, location, size, and so on) is available.", "labels": [], "entities": [{"text": "industry classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.6415195763111115}]}, {"text": "This creates a need for cognitive systems able to aggregate and filter the information available on the web and in news, databases, and other sources.", "labels": [], "entities": []}, {"text": "Providing good quality natural language descriptions of companies allows for easier access to the data, for example in the context of virtual agents or with text-to-speech applications.", "labels": [], "entities": []}, {"text": "In this paper, we propose an entity-focused system using a combination of targeted (knowledge base driven) and data-driven generation to create company descriptions in the style of Wikipedia descriptions.", "labels": [], "entities": []}, {"text": "The system generates sentences from RDF triples, such as those found in DBPedia and Freebase, about a given company and combines these with sentences on the web that match learned expressions of relationships.", "labels": [], "entities": []}, {"text": "We evaluate our hybrid approach and compare it with a targeted-only approach and a data-driven-only approach, as well as a strong multi-document summarization baseline.", "labels": [], "entities": []}, {"text": "Our results show that the hybrid approach performs significantly better than either approach alone as well as the baseline.", "labels": [], "entities": []}, {"text": "The targeted (TD) approach to company description uses Wikipedia descriptions as a model for generation.", "labels": [], "entities": []}, {"text": "It learns how to realize RDF relations that have the company as their subject: each relation contains a company/entity pair and it is these pairs that drive both content and expression of the company description.", "labels": [], "entities": []}, {"text": "For each company/entity pair, the system finds all the ways in which similar company/entity pairs are expressed in other Wikipedia company descriptions, clustering together sentences that express the same company/entity relation pairs.", "labels": [], "entities": []}, {"text": "It generates templates for the sentences in each cluster, replacing the mentions of companies and entities with typed slots and generates anew description by inserting expressions for the given company and entity in the slots.", "labels": [], "entities": []}, {"text": "All possible sentences are generated from the templates in the cluster, the resulting sentences are ranked and the best sentence for each relation selected to produce the final description.", "labels": [], "entities": []}, {"text": "Thus, the TD approach is a top-down approach, driven to generate sentences expressing the relations found in the company's RDF data using realizations that are typically used on Wikipedia.", "labels": [], "entities": []}, {"text": "In contrast, the data-driven (DD) approach uses a semi-supervised method to select sentences from descriptions about the given company on the web.", "labels": [], "entities": []}, {"text": "Like the TD approach, it also begins with a seed set of relations present in a few companies' DBPedia entries, represented as company/entity pairs, but instead of looking at the corresponding Wikipedia articles, it learns patterns that are typ-ically used to express the relations on the web.", "labels": [], "entities": []}, {"text": "In the process, it uses bootstrapping) to learn new ways of expressing the relations corresponding to each company/entity pair, alternating with learning new pairs that match the learned expression patterns.", "labels": [], "entities": []}, {"text": "Since the bootstrapping process is driven only by company/entity pairs and lexical patterns, it has the potential to learn a wider variety of expressions for each pair and to learn new relations that may exist for each pair.", "labels": [], "entities": []}, {"text": "Thus, this approach lets data for company descriptions on the web determine the possible relations and patterns for expressing those relations in a bottom-up fashion.", "labels": [], "entities": []}, {"text": "It then uses the learned patterns to select matching sentences from the web about a target company.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our approach, we compare the three versions of our output -generated by the TD, DD, and hybrid approach -against multi-document summaries generated (from the same search results used by our DD approach) by TextRank (Mihalcea and).", "labels": [], "entities": [{"text": "TextRank", "start_pos": 218, "end_pos": 226, "type": "DATASET", "confidence": 0.9252846240997314}]}, {"text": "For each one of the approaches as well as the baseline, we generated descriptions for all companies that were part of the S&P500 as of January 2016.", "labels": [], "entities": [{"text": "S&P500 as of January 2016", "start_pos": 122, "end_pos": 147, "type": "DATASET", "confidence": 0.9008155124528068}]}, {"text": "We used our development set of 100 companies for tuning, and the evaluation results are based on the remaining 400.", "labels": [], "entities": []}, {"text": "We conducted two types of experiments.", "labels": [], "entities": []}, {"text": "The first is an automated evaluation, where we use the METEOR score) between the description generated by one of our approaches or by the baseline and the first section of the Wikipedia article for the company.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9732470810413361}]}, {"text": "In Wikipedia articles, the first section typically serves as an introduction or overview of the most important information about the company.", "labels": [], "entities": []}, {"text": "ME-TEOR scores capture the content overlap between the generated description and the Wikipedia text.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8278588652610779}]}, {"text": "To avoid bias from different text sizes, we set the same size limit for all descriptions when comparing them.", "labels": [], "entities": []}, {"text": "We experimented with three settings: 150 words, 500 words, and no size limit.", "labels": [], "entities": []}, {"text": "In addition, we conducted a crowd-sourced evaluation on the CrowdFlower platform.", "labels": [], "entities": [{"text": "CrowdFlower platform", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.9237914979457855}]}, {"text": "In this evaluation, we presented human annotators with two descriptions for the same company, one described by our approach and one by the baseline, in random order.", "labels": [], "entities": []}, {"text": "The annotators were then asked to choose which of the two descriptions is a better overview of the company in question (they were 150 words 500 words no limit: Second experiment results: % of companies for which the approach was chosen as best by the human annotators, and average scores given provided a link to the company's Wikipedia page for reference) and give a score on a 1-5 scale to each description.", "labels": [], "entities": []}, {"text": "For quality assurance, each pair of descriptions was processed by three annotators, and we only included in the results instances where all three agreed.", "labels": [], "entities": [{"text": "quality assurance", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6813285946846008}]}, {"text": "Those constituted 44% of the instances.", "labels": [], "entities": []}, {"text": "In this evaluation we only used the hybrid version, and we limited the length of both the baseline and our output to 150 words to reduce bias from a difference in lengths and keep the descriptions reasonably short for the annotators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: First experiment results: average ME- TEOR scores for various size limits", "labels": [], "entities": [{"text": "ME- TEOR scores", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.8681959211826324}]}, {"text": " Table 2: Second experiment results: % of compa- nies for which the approach was chosen as best by  the human annotators, and average scores given", "labels": [], "entities": []}]}