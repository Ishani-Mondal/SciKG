{"title": [{"text": "Modeling Coverage for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.7970239917437235}]}], "abstractContent": [{"text": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.801575094461441}]}, {"text": "It tends to ignore past alignment information, however, which often leads to over-translation and under-translation.", "labels": [], "entities": []}, {"text": "To address this problem, we propose coverage-based NMT in this paper.", "labels": [], "entities": []}, {"text": "We maintain a coverage vector to keep track of the attention history.", "labels": [], "entities": []}, {"text": "The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.953975260257721}]}], "introductionContent": [{"text": "The past several years have witnessed the rapid progress of end-to-end Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 71, "end_pos": 103, "type": "TASK", "confidence": 0.8286465505758921}]}, {"text": "Unlike conventional Statistical Machine Translation (SMT) (), NMT uses a single and large neural network to model the entire translation process.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.8240747551123301}]}, {"text": "It enjoys the following advantages.", "labels": [], "entities": []}, {"text": "First, the use of distributed representations of words can alleviate the curse of dimensionality (.", "labels": [], "entities": []}, {"text": "Second, there is no need to explicitly design features to capture translation regularities, which is quite difficult in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9889931082725525}]}, {"text": "Instead, NMT is capable of learning representations directly from the training data.", "labels": [], "entities": []}, {"text": "Third, Long Short-Term Memory) enables NMT to cap-ture long-distance reordering, which is a significant challenge in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9947307109832764}]}, {"text": "NMT has a serious problem, however, namely lack of coverage.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.859021008014679}, {"text": "coverage", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9431487321853638}]}, {"text": "In phrase-based SMT (, a decoder maintains a coverage vector to indicate whether a source word is translated or not.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7975614666938782}]}, {"text": "This is important for ensuring that each source word is translated in decoding.", "labels": [], "entities": []}, {"text": "The decoding process is completed when all source words are \"covered\" or translated.", "labels": [], "entities": []}, {"text": "In NMT, there is no such coverage vector and the decoding process ends only when the end-of-sentence mark is produced.", "labels": [], "entities": []}, {"text": "We believe that lacking coverage might result in the following problems in conventional NMT: 1.", "labels": [], "entities": [{"text": "NMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.8602739572525024}]}, {"text": "Over-translation: some words are unnecessarily translated for multiple times; 2.", "labels": [], "entities": []}, {"text": "Under-translation: some words are mistakenly untranslated.", "labels": [], "entities": []}, {"text": "Specifically, in the state-of-the-art attention-based NMT model (, generating a target word heavily depends on the relevant parts of the source sentence, and a source word is involved in generation of all target words.", "labels": [], "entities": []}, {"text": "As a result, over-translation and under-translation inevitably happen because of ignoring the \"coverage\" of source words (i.e., number of times a source word is translated to a target word).", "labels": [], "entities": []}, {"text": "Figure 1(a) shows an example: the Chinese word \"gu\u00af anb`anb`\u0131\" is over translated to \"close(d)\" twice, while \"b` eip\u00f2\" (means \"be forced to\") is mistakenly untranslated.", "labels": [], "entities": []}, {"text": "In this work, we propose a coverage mechanism to NMT (NMT-COVERAGE) to alleviate the overtranslation and under-translation problems.", "labels": [], "entities": []}, {"text": "Basically, we append a coverage vector to the intermediate representations of an NMT model, which are sequentially updated after each attentive read (a) Over-translation and under-translation generated by NMT.", "labels": [], "entities": []}, {"text": "(b) Coverage model alleviates the problems of over-translation and under-translation.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9585042595863342}]}, {"text": "In conventional NMT without coverage, the Chinese word \"gu\u00af anb`anb`\u0131\" is over translated to \"close(d)\" twice, while \"b` eip\u00f2\" (means \"be forced to\") is mistakenly untranslated.", "labels": [], "entities": []}, {"text": "Coverage model alleviates these problems by tracking the \"coverage\" of source words.", "labels": [], "entities": []}, {"text": "during the decoding process, to keep track of the attention history.", "labels": [], "entities": []}, {"text": "The coverage vector, when entering into attention model, can help adjust the future attention and significantly improve the overall alignment between the source and target sentences.", "labels": [], "entities": []}, {"text": "This design contains many particular cases for coverage modeling with contrasting characteristics, which all share a clear linguistic intuition and yet can be trained in a data driven fashion.", "labels": [], "entities": [{"text": "coverage modeling", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9609792828559875}]}, {"text": "Notably, we achieve significant improvement even by simply using the sum of previous alignment probabilities as coverage for each word, as a successful example of incorporating linguistic knowledge into neural network based NLP models.", "labels": [], "entities": []}, {"text": "Experiments show that NMT-COVERAGE significantly outperforms conventional attentionbased NMT on both translation and alignment tasks.", "labels": [], "entities": [{"text": "translation and alignment tasks", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.7971107065677643}]}, {"text": "shows an example, in which NMT-COVERAGE alleviates the over-translation and under-translation problems that NMT without coverage suffers from.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation of translation quality. d denotes the dimension of NN-based coverages, and  \u2020 and  \u2021  indicate statistically significant difference (p < 0.01) from GroundHog and Moses, respectively. \"+\" is  on top of the baseline system GroundHog.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.95597904920578}, {"text": "GroundHog", "start_pos": 169, "end_pos": 178, "type": "DATASET", "confidence": 0.9277611970901489}, {"text": "GroundHog", "start_pos": 242, "end_pos": 251, "type": "DATASET", "confidence": 0.9731509685516357}]}, {"text": " Table 2: Evaluation of alignment quality. The  lower the score, the better the alignment quality.", "labels": [], "entities": []}]}