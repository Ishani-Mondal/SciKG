{"title": [{"text": "A Trainable Spaced Repetition Model for Language Learning", "labels": [], "entities": [{"text": "Language Learning", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6874997466802597}]}], "abstractContent": [{"text": "We present half-life regression (HLR), a novel model for spaced repetition practice with applications to second language acquisition.", "labels": [], "entities": [{"text": "half-life regression (HLR)", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6316190838813782}, {"text": "second language acquisition", "start_pos": 105, "end_pos": 132, "type": "TASK", "confidence": 0.6917924880981445}]}, {"text": "HLR combines psycholinguis-tic theory with modern machine learning techniques, indirectly estimating the \"half-life\" of a word or concept in a student's long-term memory.", "labels": [], "entities": []}, {"text": "We use data from Duolingo-a popular online language learning application-to fit HLR models, reducing error by 45%+ compared to several baselines at predicting student recall rates.", "labels": [], "entities": [{"text": "error", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9970990419387817}, {"text": "recall", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9580748081207275}]}, {"text": "HLR model weights also shed light on which linguistic concepts are systematically challenging for second language learners.", "labels": [], "entities": []}, {"text": "Finally, HLR was able to improve Duolingo daily student engagement by 12% in an operational user study.", "labels": [], "entities": [{"text": "HLR", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8015366792678833}]}], "introductionContent": [{"text": "The spacing effect is the observation that people tend to remember things more effectively if they use spaced repetition practice (short study periods spread out over time) as opposed to massed practice (i.e., \"cramming\").", "labels": [], "entities": []}, {"text": "The phenomenon was first documented by, using himself as a subject in several experiments to memorize verbal utterances.", "labels": [], "entities": []}, {"text": "In one study, after a day of cramming he could accurately recite 12-syllable sequences (of gibberish, apparently).", "labels": [], "entities": []}, {"text": "However, he could achieve comparable results with half as many practices spread out over three days.", "labels": [], "entities": []}, {"text": "The lag effect) is the related observation that people learn even better if the spacing between practices gradually increases.", "labels": [], "entities": []}, {"text": "For example, a learning schedule might begin with re-view sessions a few seconds apart, then minutes, then hours, days, months, and soon, with each successive review stretching out over a longer and longer time interval.", "labels": [], "entities": []}, {"text": "The effects of spacing and lag are wellestablished in second language acquisition research, and benefits have also been shown for gymnastics, baseball pitching, video games, and many other skills.", "labels": [], "entities": [{"text": "second language acquisition", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6372596820195516}]}, {"text": "See,, and for thorough meta-analyses spanning several decades.", "labels": [], "entities": []}, {"text": "Most practical algorithms for spaced repetition are simple functions with a few hand-picked parameters.", "labels": [], "entities": [{"text": "spaced repetition", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7865270376205444}]}, {"text": "This is reasonable, since they were largely developed during the 1960s-80s, when people would have had to manage practice schedules without the aid of computers.", "labels": [], "entities": []}, {"text": "However, the recent popularity of large-scale online learning software makes it possible to collect vast amounts of parallel student data, which can be used to empirically train richer statistical models.", "labels": [], "entities": []}, {"text": "In this work, we propose half-life regression (HLR) as a trainable spaced repetition algorithm, marrying psycholinguistically-inspired models of memory with modern machine learning techniques.", "labels": [], "entities": [{"text": "half-life regression (HLR)", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7010343134403229}]}, {"text": "We apply this model to real student learning data from Duolingo, a popular language learning app, and use it to improve its large-scale, operational, personalized learning system.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare variants of HLR with other spaced repetition algorithms in the context of Duolingo.", "labels": [], "entities": []}, {"text": "First, we evaluate methods against historical log data, and analyze trained model weights for insight.", "labels": [], "entities": []}, {"text": "We then describe two controlled user experiments where we deployed HLR as part of the student model in the production system.", "labels": [], "entities": []}, {"text": "Constant \u00af p = 0.859 0.175 n/a n/a: Evaluation results using historical log data (see text).", "labels": [], "entities": []}, {"text": "Arrows indicate whether lower (\u2193) or higher (\u2191) scores are better.", "labels": [], "entities": []}, {"text": "The best method for each metric is shown in bold, and statistically significant effects (p < 0.001) are marked with *.", "labels": [], "entities": []}, {"text": "We collected two weeks of Duolingo log data, containing 12.9 million student-word lesson and practice session traces similar to (for all students in all courses).", "labels": [], "entities": [{"text": "Duolingo log data", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9464924534161886}]}, {"text": "We then compared three categories of spaced repetition algorithms: \u2022 Half-life regression (HLR), our model from \u00a73.3.", "labels": [], "entities": [{"text": "Half-life regression (HLR)", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.5796556830406189}]}, {"text": "For ablation purposes, we consider four variants: with and without lexeme features (-lex), as well as with and without the halflife term in the loss function (-h).", "labels": [], "entities": []}, {"text": "\u2022 Leitner and Pimsleur, two established baselines that are special cases of HLR, using fixed weights.", "labels": [], "entities": []}, {"text": "See the Appendix fora derivation of the model weights we used.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8711835145950317}]}, {"text": "\u2022 Logistic regression (LR), a standard machine learning 6 baseline.", "labels": [], "entities": [{"text": "Logistic regression (LR)", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.6468500614166259}]}, {"text": "We evaluate two variants: with and without lexeme features (-lex).", "labels": [], "entities": []}, {"text": "We used the first 1 million instances of the data to tune the parameters for our training algorithm.", "labels": [], "entities": []}, {"text": "After trying a handful of values, we settled on \u03bb = 0.1, \u03b1 = 0.01, and learning rate \u03b7 = 0.001.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9606806437174479}]}, {"text": "We used these same training parameters for HLR and LR experiments (the Leitner and Pimsleur models are fixed and do not require training).", "labels": [], "entities": []}, {"text": "For LR models, we include the lag time x\u2206 as an additional feature, since -unlike HLR -it isn't explicitly accounted for in the model.", "labels": [], "entities": []}, {"text": "We experimented with polynomial and exponential transformations of this feature, as well, but found the raw lag time to work best.", "labels": [], "entities": []}, {"text": "shows the evaluation results on the full data set of 12.9 million instances, using the first 90% for training and remaining 10% for testing.", "labels": [], "entities": []}, {"text": "We consider several different evaluation measures fora comprehensive comparison: \u2022 Mean absolute error (MAE) measures how closely predictions resemble their observed outcomes: 1 Since the strength meters in Duolingo's interface are based on model predictions, we use MAE as a measure of prediction quality.", "labels": [], "entities": [{"text": "Mean absolute error (MAE)", "start_pos": 83, "end_pos": 108, "type": "METRIC", "confidence": 0.9703850746154785}]}, {"text": "\u2022 Area under the ROC curve (AUC) -or the Wilcoxon rank-sum test -is a measure of ranking quality.", "labels": [], "entities": [{"text": "ROC curve (AUC)", "start_pos": 17, "end_pos": 32, "type": "METRIC", "confidence": 0.9065917372703552}]}, {"text": "Here, it represents the probability that a model ranks a random correctlyrecalled word as more likely than a random incorrectly-recalled word.", "labels": [], "entities": []}, {"text": "Since our model is used to prioritize words for practice, we use AUC to help evaluate these rankings.", "labels": [], "entities": [{"text": "AUC", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.6843881607055664}]}, {"text": "\u2022 Half-life correlation (COR h ) is the Spearman rank correlation between\u02c6hbetween\u02c6 between\u02c6h \u0398 and the algebraic estimate h described in \u00a73.3.", "labels": [], "entities": [{"text": "Half-life correlation (COR h )", "start_pos": 2, "end_pos": 32, "type": "METRIC", "confidence": 0.8204528292020162}, {"text": "Spearman rank correlation", "start_pos": 40, "end_pos": 65, "type": "METRIC", "confidence": 0.5503915647665659}]}, {"text": "We use this as another measure of ranking quality.", "labels": [], "entities": []}, {"text": "For all three metrics, HLR with lexeme tag features is the best (or second best) approach, followed closely by HLR -lex (no lexeme tags).", "labels": [], "entities": []}, {"text": "In fact, these are the only two approaches with MAE lower than a baseline constant prediction of the average recall rate in the training data, bottom row).", "labels": [], "entities": [{"text": "MAE", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9594529271125793}, {"text": "recall rate", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9808017015457153}]}, {"text": "These HLR variants are also the only methods with positive COR h , although this seems reasonable since they are the only two to directly optimize for it.", "labels": [], "entities": [{"text": "COR h", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9754743576049805}]}, {"text": "While lexeme tag features made limited impact, the h term in the HLR loss function is clearly important: MAE more than doubles without it, and the -h variants are generally worse than the other baselines on at least one metric.", "labels": [], "entities": [{"text": "MAE", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9917252063751221}]}, {"text": "As stated in \u00a73.2, Leitner was the spaced repetition algorithm used in Duolingo's production student model at the time of this study.", "labels": [], "entities": [{"text": "spaced repetition algorithm", "start_pos": 35, "end_pos": 62, "type": "METRIC", "confidence": 0.7813379565874735}]}, {"text": "The Leitner method did yield the highest AUC 7 values among the algorithms we tried.", "labels": [], "entities": [{"text": "AUC 7", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9852907061576843}]}, {"text": "However, the top two HLR variants are not far behind, and they also reduce MAE compared to Leitner by least 45%.", "labels": [], "entities": [{"text": "MAE", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9983264803886414}]}, {"text": "HLR (v. Leitner) +0.3 +0.3 -7.3* II.", "labels": [], "entities": [{"text": "HLR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9701313972473145}]}, {"text": "HLR -lex (v. HLR) +12.0* +1.7* +9.5*: Change (%) in daily student retention for controlled user experiments.", "labels": [], "entities": [{"text": "HLR -lex", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5389582514762878}]}, {"text": "Statistically significant effects (p < 0.001) are marked with *.", "labels": [], "entities": []}, {"text": "The evaluation in \u00a74.1 suggests that HLR is a better approach than the Leitner algorithm originally used by Duolingo (cutting MAE nearly in half).", "labels": [], "entities": []}, {"text": "To see what effect, if any, these gains have on actual student behavior, we ran controlled user experiments in the Duolingo production system.", "labels": [], "entities": [{"text": "Duolingo production system", "start_pos": 115, "end_pos": 141, "type": "DATASET", "confidence": 0.9407198429107666}]}, {"text": "We randomly assigned all students to one of two groups: HLR (experiment) or Leitner (control).", "labels": [], "entities": []}, {"text": "The underlying spaced repetition algorithm determined strength meter values in the skill tree (e.g.,) as well as the ranking of target words for practice sessions (e.g.,), but otherwise the two conditions were identical.", "labels": [], "entities": []}, {"text": "The experiment lasted six weeks and involved just under 1 million students.", "labels": [], "entities": []}, {"text": "For evaluation, we examined changes in daily retention: what percentage of students who engage in an activity return to do it again the following day?", "labels": [], "entities": []}, {"text": "We used three retention metrics: any activity (including contributions to crowdsourced translations, online forum discussions, etc.), new lessons, and practice sessions.", "labels": [], "entities": []}, {"text": "Results are shown in the first row of.", "labels": [], "entities": []}, {"text": "The HLR group showed a slight increase in overall activity and new lessons, but a significant decrease in practice.", "labels": [], "entities": [{"text": "HLR group", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7985002994537354}]}, {"text": "Prior to the experiment, many students claimed that they would practice instead of learning new material \"just to keep the tree gold,\" but that practice sessions did not review what they thought they needed most.", "labels": [], "entities": []}, {"text": "This drop in practice -plus positive anecdotal feedback about stength meter quality from the HLR group -led us to believe that HLR was actually better for student engagement, so we deployed it for all students.", "labels": [], "entities": []}, {"text": "Several months later, active students pointed out that particular words or skills would decay rapidly, regardless of how often they practiced.", "labels": [], "entities": []}, {"text": "Upon closer investigation, these complaints could be traced to lexeme tag features with highly negative weights in the HLR model (e.g.,).", "labels": [], "entities": []}, {"text": "This implied that some feature-based overfitting had occurred, despite the L 2 regularization term in the training procedure.", "labels": [], "entities": []}, {"text": "Duolingo was also preparing to launch several new language courses at the time, and no training data yet existed to fit lexeme tag feature weights for these new languages.", "labels": [], "entities": [{"text": "Duolingo", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.953774094581604}]}, {"text": "Since the top two HLR variants were virtually tied in our \u00a74.1 experiments, we hypothesized that using interaction features alone might alleviate both student frustration and the \"cold-start\" problem of training a model for new languages.", "labels": [], "entities": []}, {"text": "Ina follow-up experiment, we randomly assigned all students to one of two groups: HLR -lex (experiment) and HLR (control).", "labels": [], "entities": []}, {"text": "The experiment lasted two weeks and involved 3.3 million students.", "labels": [], "entities": []}, {"text": "Results are shown in the second row of Table 4.", "labels": [], "entities": []}, {"text": "All three retention metrics were significantly higher for the HLR -lex group.", "labels": [], "entities": [{"text": "retention", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9678147435188293}, {"text": "HLR -lex group", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.6484283953905106}]}, {"text": "The most substantial increase was for any activity, although recurring lessons and practice sessions also improved (possibly as a byproduct of the overall activity increase).", "labels": [], "entities": []}, {"text": "Anecdotally, vocal students from the HLR -lex group who previously complained about rapid decay under the HLR model were also positive about the change.", "labels": [], "entities": [{"text": "HLR -lex group", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.7327786982059479}]}, {"text": "We deployed HLR -lex for all students, and believe that its improvements are at least partially responsible for the consistent 5% month-on-month growth in active Duolingo users since the model was launched.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results using historical log  data (see text). Arrows indicate whether lower (\u2193)  or higher (\u2191) scores are better. The best method  for each metric is shown in bold, and statistically  significant effects (p < 0.001) are marked with *.", "labels": [], "entities": []}, {"text": " Table 3: Lexeme tag weights for English (EN),  Spanish (ES), French (FR), and German (DE).", "labels": [], "entities": []}, {"text": " Table 5: Lexeme tag component abbreviations.", "labels": [], "entities": [{"text": "Lexeme tag component abbreviations", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.7191848605871201}]}]}