{"title": [{"text": "User Embedding for Scholarly Microblog Recommendation", "labels": [], "entities": [{"text": "Scholarly Microblog Recommendation", "start_pos": 19, "end_pos": 53, "type": "DATASET", "confidence": 0.6487606465816498}]}], "abstractContent": [{"text": "Nowadays, many scholarly messages are posted on Chinese microblogs and more and more researchers tend to find scholarly information on microblogs.", "labels": [], "entities": []}, {"text": "In order to exploit microblogging to benefit scientific research, we propose a scholarly mi-croblog recommendation system in this study.", "labels": [], "entities": []}, {"text": "It automatically collects and mines scholarly information from Chinese mi-croblogs, and makes personalized recommendations to researchers.", "labels": [], "entities": []}, {"text": "We propose two different neural network models which learn the vector representations for both users and microblog texts.", "labels": [], "entities": []}, {"text": "Then the recommendation is accomplished based on the similarity between a user's vector and a microblog text's vector.", "labels": [], "entities": []}, {"text": "We also build a dataset for this task.", "labels": [], "entities": []}, {"text": "The two embedding models are evaluated on the da-taset and show good results compared to several baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online social networks such as microblogs have drawn growing attention in recent years, and more and more researchers are involved in microblogging websites.", "labels": [], "entities": []}, {"text": "Besides expressing their own emotions and exchanging their life experiences just like other users, these researchers also write from time to time about their latest findings or recommend useful research resources on their microblogs, which maybe insightful to other researchers in the same field.", "labels": [], "entities": []}, {"text": "We call such microblog texts scholarly microblog texts.", "labels": [], "entities": []}, {"text": "The volume of scholarly microblog texts is huge, which makes it time-consuming fora researcher to browse and find the ones that he or she is interested in.", "labels": [], "entities": []}, {"text": "In this study, we aim to build a personalized recommendation system for recommending scholarly microblogs.", "labels": [], "entities": []}, {"text": "With such a system a researcher can easily obtain the scholarly microblogs he or she has interests in.", "labels": [], "entities": []}, {"text": "The system first collects the latest scholarly microblogs by crawling from manually selected microblog users or by applying scholarly microblog classification methods, as introduced in (.", "labels": [], "entities": [{"text": "scholarly microblog classification", "start_pos": 124, "end_pos": 158, "type": "TASK", "confidence": 0.6399879256884257}]}, {"text": "Second, the system models the relevance of each scholarly microblog to a researcher and make personalized recommendation.", "labels": [], "entities": []}, {"text": "In this study, we focus on the second step of the system and aim to model the interest and preference of a researcher by embedding the researcher into a dense vector.", "labels": [], "entities": []}, {"text": "We also embed each scholarly microblog into a dense vector, and thus the relevance of a scholarly microblog to a researcher can be estimated based on their vector representations.", "labels": [], "entities": []}, {"text": "In this paper, we propose two neural embedding algorithms for learning the vector representations for both users (researchers) and microblog texts.", "labels": [], "entities": []}, {"text": "By extending the paragraph vector representation method proposed by, the vector representations are jointly learned in a single framework.", "labels": [], "entities": [{"text": "paragraph vector representation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.701475203037262}]}, {"text": "By modeling the user preferences into the same vector space with the words and texts, we can obtain the similarity between them in a straightforward way, and use this relevance for microblog recommendation.", "labels": [], "entities": [{"text": "microblog recommendation", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.6408061534166336}]}, {"text": "We build areal evaluation dataset from Sina Weibo.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7801564335823059}]}, {"text": "Evaluation results on the dataset show the efficacy of our proposed methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because there is no API that can directly grant us the access to the follower and followee list for each user without authorization on Sina Weibo, when evaluating the effectiveness of our methods, we randomly choose one hundred positive samples and another four hundred negative samples randomly selected from the crawled microblogs, to simulate the timeline of a user, and use this simulated timeline as the test dataset.", "labels": [], "entities": [{"text": "Sina Weibo", "start_pos": 135, "end_pos": 145, "type": "DATASET", "confidence": 0.9381827712059021}]}, {"text": "The remaining positive samples are used for training.", "labels": [], "entities": []}, {"text": "We adopt two additional baselines: Bag-ofWords and SVM on Bag-of-Words.", "labels": [], "entities": []}, {"text": "For the Bagof-Words baseline, we use the Bag-of-Words vector of each microblog text as the microblog text vector, and average them to obtain user vectors.", "labels": [], "entities": [{"text": "Bagof-Words baseline", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.9177557826042175}]}, {"text": "For the SVM on Bag-of-Words baseline, we randomly choose the same amount of negative samples as that of positive samples for training.", "labels": [], "entities": [{"text": "Bag-of-Words baseline", "start_pos": 15, "end_pos": 36, "type": "DATASET", "confidence": 0.7797587513923645}]}, {"text": "We use the Bag-of-Words vector of each microblog text as the features, and run the SVM algorithm implemented in LibSVM 3 once for every user.", "labels": [], "entities": []}, {"text": "Note that the Average Embedding 3 https://www.csie.ntu.edu.tw/~cjlin/libsvm/ method introduced in Section 3.3 is considered a strong baseline for comparison.", "labels": [], "entities": [{"text": "Average", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9488090872764587}]}, {"text": "For each method and each user, we sort the microblog texts according to their similarity with the user and select the top k microblog texts as recommendation results, where k varies from 10 to 100.", "labels": [], "entities": []}, {"text": "Besides precision and recall values, we also compute mean reciprocal rank (MRR) to measure the recommendation results in our experiments, which is the average of the multiplicative inverse of the rank of the positive samples in the output of the recommending system, and then averaged again across all users.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9994435906410217}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.998580813407898}, {"text": "mean reciprocal rank (MRR)", "start_pos": 53, "end_pos": 79, "type": "METRIC", "confidence": 0.935864269733429}]}, {"text": "Note that when k is set to 100, the precision and recall value will be equal to each other.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9995920062065125}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9977866411209106}]}, {"text": "The comparison results with respect to different k are shown in.", "labels": [], "entities": []}, {"text": "As we can see, the two proposed joint learning methods outperform the simple average embedding method and the two other baselines, indicating the effectiveness of the proposed methods.", "labels": [], "entities": []}, {"text": "Moreover, User2Vec#2 yields better results than User2Vec#1.We believe this is because in User2Vec#2, the word vectors have a direct contribution to the user vectors, which improves the learning effect of the user  vectors learnt in the framework.", "labels": [], "entities": []}, {"text": "Furthermore, the precision/recall scores of the embedding methods (k=100) with respect to different vector dimensions are shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9993659853935242}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9384022951126099}]}, {"text": "We can see that the dimension size has little impact on the recommendation performance, and our proposed two methods always outperform the strong baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Overview of results.", "labels": [], "entities": [{"text": "Overview", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8020315766334534}]}]}