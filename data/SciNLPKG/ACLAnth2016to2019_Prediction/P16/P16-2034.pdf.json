{"title": [{"text": "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification", "labels": [], "entities": [{"text": "Relation Classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.9603422284126282}]}], "abstractContent": [{"text": "Relation classification is an important semantic processing task in the field of natural language processing (NLP).", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9673174023628235}, {"text": "natural language processing (NLP)", "start_pos": 81, "end_pos": 114, "type": "TASK", "confidence": 0.8197526931762695}]}, {"text": "State-of-the-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9465335607528687}, {"text": "dependency parser and named entity recognizers (NER)", "start_pos": 93, "end_pos": 145, "type": "TASK", "confidence": 0.7445020145840116}]}, {"text": "Another challenge is that important information can appear at any position in the sentence.", "labels": [], "entities": []}, {"text": "To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(Att-BLSTM) to capture the most important semantic information in a sentence.", "labels": [], "entities": []}, {"text": "The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.", "labels": [], "entities": [{"text": "SemEval-2010 relation classification task", "start_pos": 32, "end_pos": 73, "type": "TASK", "confidence": 0.8794918656349182}]}], "introductionContent": [{"text": "Relation classification is the task of finding semantic relations between pairs of nominals, which is useful for many NLP applications, such as information extraction (, question answering ().", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9488002359867096}, {"text": "information extraction", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.8304523825645447}, {"text": "question answering", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.8499924838542938}]}, {"text": "For instance, the following sentence contains an example of the Entity-Destination relation between the nominals Flowers and chapel.", "labels": [], "entities": []}, {"text": "\u27e8e 1 \u27e9 Flowers \u27e8/e 1 \u27e9 are carried into the \u27e8e 2 \u27e9 chapel \u27e8/e 2 \u27e9.", "labels": [], "entities": []}, {"text": "\u27e8e 1 \u27e9, \u27e8/e 1 \u27e9, \u27e8e 2 \u27e9, \u27e8/e 2 \u27e9 are four position indicators which specify the starting and ending of the nominals (.", "labels": [], "entities": []}, {"text": "Traditional relation classification methods that employ handcrafted features from lexical resources, are usually based on pattern matching, and have achieved high performance (Bunescu * Correspondence author: zhenyu.qi@ia.ac.cn and.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.8302549719810486}]}, {"text": "One downside of these methods is that many traditional NLP systems are utilized to extract high-level features, such as part of speech tags, shortest dependency path and named entities, which consequently results in the increase of computational cost and additional propagation errors.", "labels": [], "entities": []}, {"text": "Another downside is that designing features manually is time-consuming, and performing poor on generalization due to the low coverage of different training datasets.", "labels": [], "entities": []}, {"text": "Recently, deep learning methods provide an effective way of reducing the number of handcrafted features).", "labels": [], "entities": []}, {"text": "However, these approaches still use lexical resources such as WordNet or NLP systems like dependency parsers and NER to get high-level features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9480828642845154}, {"text": "dependency parsers", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.6841961592435837}]}, {"text": "This paper proposes a novel neural network Att-BLSTM for relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.9328369200229645}]}, {"text": "Our model utilizes neural attention mechanism with Bidirectional Long Short-Term Memory Networks(BLSTM) to capture the most important semantic information in a sentence.", "labels": [], "entities": []}, {"text": "This model doesn't utilize any features derived from lexical resources or NLP systems.", "labels": [], "entities": []}, {"text": "The contribution of this paper is using BLST-M with attention mechanism, which can automatically focus on the words that have decisive effect on classification, to capture the most important semantic information in a sentence, without using extra knowledge and NLP systems.", "labels": [], "entities": [{"text": "BLST-M", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.8899785876274109}]}, {"text": "We conduct experiments on the SemEval-2010 Task 8 dataset, and achieve an F 1-score of 84.0%, higher than most of the existing methods in the literature.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 dataset", "start_pos": 30, "end_pos": 57, "type": "DATASET", "confidence": 0.6602154523134232}, {"text": "F 1-score", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9928349852561951}]}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review related work about relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.9508928060531616}]}, {"text": "Section 3 presents our Att-BLSTM model in detail.", "labels": [], "entities": []}, {"text": "In Section 4, we describe details about the setup of experimental evaluation 207 and the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we have our conclusion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are conducted on).", "labels": [], "entities": []}, {"text": "This dataset contains 9 relationships (with two directions) and an undirected Other class.", "labels": [], "entities": []}, {"text": "There are 10,717 annotated examples, including 8,000 sentences for training, and 2,717 for testing.", "labels": [], "entities": []}, {"text": "We adopt the official evaluation metric to evaluate our systems, which is based on macro-averaged F1-score for the nine actual relations (excluding the Other relation) and takes the directionality into consideration.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.939452052116394}]}, {"text": "In order to compare with the work by, we use the same word vectors proposed by to initialize the embedding layer.", "labels": [], "entities": []}, {"text": "Additionally, to compare with the work by , we also use the 100-dimensional word vectors pretrained by.", "labels": [], "entities": []}, {"text": "Since there is no official development dataset, we randomly select 800 sentence for validation.", "labels": [], "entities": []}, {"text": "The hyper-parameters for our model were tuned on the development set for each task.", "labels": [], "entities": []}, {"text": "Our model was trained using AdaDelta) with a learning rate of 1.0 and a minibatch size 10.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9656786620616913}]}, {"text": "The model parameters were regularized with a perminibatch L2 regularization strength of 10 \u22125 . We evaluate the effect of dropout embedding layer, dropout LSTM layer and dropout the penultimate layer, the model has a better performance, when the dropout rate is set as 0.3, 0.3, 0.5 respectively.", "labels": [], "entities": []}, {"text": "Other parameters in our model are initialized randomly.", "labels": [], "entities": []}, {"text": "compares our Att-BLSTM with other state-of-the-art methods of relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.8633011281490326}]}, {"text": "SVM: This is the top performed system in SemEval-2010.", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9216131567955017}]}, {"text": "leveraged a variety of handcrafted features, and use SVM as the classifier.", "labels": [], "entities": []}, {"text": "They achieved an F 1 -score of 82.2%.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9952986985445023}]}, {"text": "CNN: treated a sentences as a sequential data and exploited the convolutional neural network to learn sentence-level features; they also used a special position vector to represent each word.", "labels": [], "entities": [{"text": "CNN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8729721903800964}]}, {"text": "Then the sentence-level and lexical features were concatenated into a single vector and fed into a softmax classifier for prediction.", "labels": [], "entities": []}, {"text": "This model achieves an F 1 -score of 82.7%.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.992781475186348}]}, {"text": "RNN: Zhang and Wang (2015) employed bidirectional RNN networks with two different dimension word vectors for relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.9040659368038177}]}, {"text": "They achieved an F 1 -score of 82.8% using 300-dimensional word vectors pre-trained by, and an F 1 -score of 80.0% using 50-dimensional word vectors pre-trained by.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9893601834774017}, {"text": "F 1 -score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.988800972700119}]}, {"text": "Our model with the same 50-dimensional word vectors achieves an F 1 -score of 82.5%, about 2.5 percent more than theirs.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9932612031698227}]}, {"text": "SDP-LSTM: utilized four different channels to pickup heterogeneous along the SDP, and they achieved an F 1 -score of 83.7%.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 103, "end_pos": 113, "type": "METRIC", "confidence": 0.9923309683799744}]}, {"text": "Comparing with their model, our model regarding the raw text as a sequence is simpler.", "labels": [], "entities": []}, {"text": "BLSTM:  employed many features derived from NLP tools and lexical resources with bidirectional LSTM networks to learn the sentence level features, and they achieved state-of-the-art performance on the SemEval-2010 Task 8 dataset.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6162864565849304}, {"text": "SemEval-2010 Task 8 dataset", "start_pos": 201, "end_pos": 228, "type": "DATASET", "confidence": 0.6451411023736}]}, {"text": "Our model with the same word vectors achieves a very similar result (84.0%), and our model is more simple.", "labels": [], "entities": []}, {"text": "Our proposed Att-BLSTM model yields an F 1 -score of 84.0%.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.989917516708374}]}, {"text": "It outperforms most of the existing competing approaches, without using lexical resources such as WordNet or NLP systems like dependency parser and NER to get high-level features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.948852002620697}]}], "tableCaptions": []}