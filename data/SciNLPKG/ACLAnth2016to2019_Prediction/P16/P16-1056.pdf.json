{"title": [{"text": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "labels": [], "entities": [{"text": "30M Factoid Question-Answer Corpus", "start_pos": 65, "end_pos": 99, "type": "DATASET", "confidence": 0.7944025546312332}]}], "abstractContent": [{"text": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances.", "labels": [], "entities": []}, {"text": "However, to this date, there are no large-scale question-answer corpora available.", "labels": [], "entities": []}, {"text": "In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to trans-duce facts into natural language questions.", "labels": [], "entities": [{"text": "30M Factoid Question-Answer Corpus", "start_pos": 29, "end_pos": 63, "type": "DATASET", "confidence": 0.776185154914856}]}, {"text": "The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics.", "labels": [], "entities": []}, {"text": "Across all evaluation criteria the question-generation model outperforms the competing template-based baseline.", "labels": [], "entities": []}, {"text": "Furthermore, when presented to human evaluators, the generated questions appear to be comparable in quality to real human-generated questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "A major obstacle for training question-answering (QA) systems has been due to the lack of labeled data.", "labels": [], "entities": [{"text": "training question-answering (QA)", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.637075400352478}]}, {"text": "The question answering field has focused on building QA systems based on traditional information retrieval procedures ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9275317490100861}]}, {"text": "More recently, researchers have started to utilize large-scale knowledge bases (KBs) (), such as Freebase (,) and Cyc (.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9604567289352417}]}, {"text": "Bootstrapping QA systems with such structured knowledge is clearly beneficial, but it is unlikely alone to overcome the lack of labeled data.", "labels": [], "entities": []}, {"text": "To take into account the rich and complex nature of human language, such as paraphrases and ambiguity, it would appear that labeled question and answer pairs are necessary.", "labels": [], "entities": []}, {"text": "The need for such labeled pairs is even more critical for training neural network-based QA systems, where researchers until now have relied mainly on hand-crafted rules and heuristics to synthesize artificial QA corpora (.", "labels": [], "entities": []}, {"text": "Motivated by these recent developments, in this paper we focus on generating questions based on the Freebase KB.", "labels": [], "entities": [{"text": "Freebase KB", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.9596187472343445}]}, {"text": "We frame question generation as a transduction problem starting from a Freebase fact, represented by a triple consisting of a subject, a relationship and an object, which is trans-duced into a question about the subject, where the object is the correct answer (.", "labels": [], "entities": [{"text": "question generation", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7373678535223007}, {"text": "Freebase fact", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9296347200870514}]}, {"text": "We propose several models, largely inspired by recent neural machine translation models (, and we use an approach similar to for dealing with the problem of rare-words.", "labels": [], "entities": []}, {"text": "We evaluate the produced questions in a human-based experiment as well as with respect to automatic evaluation metrics, including the well-established machine translation metrics BLEU and METEOR and a sentence similarity metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9081022143363953}, {"text": "METEOR", "start_pos": 188, "end_pos": 194, "type": "METRIC", "confidence": 0.937541663646698}]}, {"text": "We find that the question-generation model outperforms the competing template-based baseline, and, when presented to untrained human evaluators, the produced questions appear to be indistinguishable from real human-generated questions.", "labels": [], "entities": []}, {"text": "This suggests that the produced questionanswer pairs are of high quality and therefore that they will be useful for training QA systems.", "labels": [], "entities": [{"text": "QA", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.7829460501670837}]}, {"text": "Finally, we use the best performing model to construct anew factoid question-answer corpus -The 30M Factoid Question-Answer Corpus -which is made freely available to the research community.", "labels": [], "entities": [{"text": "30M Factoid Question-Answer Corpus", "start_pos": 96, "end_pos": 130, "type": "DATASET", "confidence": 0.8721124529838562}]}], "datasetContent": [{"text": "We use the SimpleQuestions dataset () in order to train our models.: Statistics of SimpleQuestions at a time and they were asked to phrase a question such that the object of the presented fact becomes the answer of the question.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.8762427568435669}]}, {"text": "Consequently, both the subject and the relationship are explicitly given in each question.", "labels": [], "entities": []}, {"text": "But indirectly characteristics of the object may also be given since the humans have an access to it as well.", "labels": [], "entities": []}, {"text": "Often when phrasing a question the annotators tend to be more informative about the target object by giving specific information about it in the question produced.", "labels": [], "entities": []}, {"text": "For example, in the question What city is the American actress X from?", "labels": [], "entities": []}, {"text": "the city name given in the object informs the human participant that it was in America -information, which was not provided by either the subject or relationship of the fact.", "labels": [], "entities": []}, {"text": "We have also observed that the questions are often ambiguous: that is, one can easily come up with several possible answers that may fit the specifications of the question.", "labels": [], "entities": []}, {"text": "shows statistics of the dataset.", "labels": [], "entities": []}, {"text": "To investigate the performance of our models, we make use of both automatic evaluation metrics and human evaluators.", "labels": [], "entities": []}, {"text": "BLEU () and ME-TEOR () are two widely used evaluation metrics in statistical machine translation and automatic image-caption generation).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9829084873199463}, {"text": "ME-TEOR", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9243670701980591}, {"text": "statistical machine translation", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.6855157713095347}, {"text": "image-caption generation", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.724138930439949}]}, {"text": "Similar to statistical machine translation, where a phrase in the source language is mapped to a phrase in the target language, in this task a KB fact is mapped to a natural language question.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6375472148259481}]}, {"text": "Both tasks are highly constrained, e.g. the set of valid outputs is limited.", "labels": [], "entities": []}, {"text": "This is true in particular for short phrases, such as one sentence questions.", "labels": [], "entities": []}, {"text": "Furthermore, in both tasks, the majority of valid outputs are paraphrases of each other, which BLEU and METEOR have been designed to capture.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9956023693084717}, {"text": "METEOR", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.7702086567878723}]}, {"text": "We therefore believe that BLEU and METEOR constitute reasonable performance metrics for evaluating the generated questions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9983396530151367}, {"text": "METEOR", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.956723690032959}]}, {"text": "Although we believe that METEOR and BLEU are reasonable evaluation metrics, they may have not recognize certain paraphrases, in particular paraphrases of entities.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9073273539543152}, {"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9965290427207947}]}, {"text": "We therefore also make use of a sentence similarity metric, as proposed by, which we will denote Embedding Greedy (Emb. Greedy).", "labels": [], "entities": []}, {"text": "The metric makes use of a word similarity score, which in our experiments is the cosine similarity between two Word2Vec word embeddings (.", "labels": [], "entities": [{"text": "word similarity score", "start_pos": 26, "end_pos": 47, "type": "METRIC", "confidence": 0.6401197910308838}]}, {"text": "The metric finds a (non-exclusive) alignment between words in the two questions, which maximizes the similarity between aligned words, and computes the sentence similarity as the mean over the word similarities between aligned words.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Example questions produced by the model with multiple placeholders are shown in.", "labels": [], "entities": []}, {"text": "The neural network models outperform the templatebased baseline by a clear margin across all metrics.", "labels": [], "entities": []}, {"text": "The template-based baseline is already a relatively strong model, because it makes use of a separate template for each relationship.", "labels": [], "entities": []}, {"text": "Qualitatively the neural networks outperform the baseline model in cases where they are able to levage additional knowledge about the entities (see first, third and fifth example in).", "labels": [], "entities": []}, {"text": "On the other hand, for rare relationships the baseline model appears to perform better, because it is able to produce a reasonable question if only a single example with the same relationship exists in the training set (see eighth example in).", "labels": [], "entities": []}, {"text": "Given enough training data this suggests that neural networks are generally better at the question generation task compared to hand-crafted template-based procedures, and therefore that they maybe useful for generating question answering corpora.", "labels": [], "entities": [{"text": "question generation task", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8016114433606466}, {"text": "generating question answering corpora", "start_pos": 208, "end_pos": 245, "type": "TASK", "confidence": 0.7045376151800156}]}, {"text": "Furthermore, it appears that the best performing models are the models where TransE are trained on the largest set of triples (TransE++).", "labels": [], "entities": []}, {"text": "This set contains, apart from the supporting triples described in Section 4.3, triples involving entities which are highly connected to the entities found in the SimpleQuestions facts.", "labels": [], "entities": []}, {"text": "In total, around 30 millions of facts, which have been used to generate the 30M Factoid Question-Answer Corpus.", "labels": [], "entities": [{"text": "30M Factoid Question-Answer Corpus", "start_pos": 76, "end_pos": 110, "type": "DATASET", "confidence": 0.823942244052887}]}, {"text": "Lastly, it is not clear whether the model with a single placeholder or the model with multiple placeholders performs best.", "labels": [], "entities": []}, {"text": "This motivates the following human study.: Test examples and corresponding questions.", "labels": [], "entities": []}, {"text": "We carryout pairwise preference experiments on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.9678628245989481}]}, {"text": "Initially, we considered carrying out separate experiments for measuring relevancy and fluency respectively, since this is common practice in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.758770614862442}]}, {"text": "However, the relevancy of a question is determined solely by a single factor, i.e. the relationship, since by construction the subject is always in the question.", "labels": [], "entities": []}, {"text": "Measuring relevancy is therefore not very useful in our task.", "labels": [], "entities": []}, {"text": "To verify this we carried out an internal pairwise preference experiment with human subjects, who were repeatedly shown a fact and two questions and asked to select the most relevant question.", "labels": [], "entities": []}, {"text": "We found that 93% of the questions generated by the MP Triples TransE++ model were either judged better or at least as good as the human generated questions w.r.t. relevancy.", "labels": [], "entities": [{"text": "MP Triples TransE++ model", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.841920804977417}]}, {"text": "The remaining 7% questions of the MP Triples TransE++ model questions were also judged relevant questions, although less so compared to the human generated questions.", "labels": [], "entities": [{"text": "MP Triples TransE++ model questions", "start_pos": 34, "end_pos": 69, "type": "DATASET", "confidence": 0.8795381287733713}]}, {"text": "In the next experiment, we therefore measure the holistic quality of the questions.", "labels": [], "entities": []}, {"text": "We setup experiments comparing: HumanBaseline (human and baseline questions), Human-MP (human and MP Triples TransE++ questions) and Baseline-MP (baseline and MP Triples TransE++ questions).", "labels": [], "entities": [{"text": "HumanBaseline", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9353424906730652}]}, {"text": "We show human evaluators a fact along with two questions, one question from each model for the corresponding fact, and ask the them to choose the question which is most relevant to the fact and most natural.", "labels": [], "entities": []}, {"text": "The human evaluator also has the option of not choosing either question.", "labels": [], "entities": []}, {"text": "This is important if both questions are equally good or if neither of the questions make sense.", "labels": [], "entities": []}, {"text": "At the beginning of each experiment, we show the human evaluators two examples of statements and a corresponding pair of questions, where we briefly explain the form of the statements and how questions relate to those statements.", "labels": [], "entities": []}, {"text": "Following the introductory examples, we present the facts and cor-  responding pair of questions one by one.", "labels": [], "entities": []}, {"text": "To avoid presentation bias, we randomly shuffle the order of the examples and the order in which questions are shown by each model.", "labels": [], "entities": []}, {"text": "During each experiment, we also show four check facts and corresponding check questions at random, which any attentive human annotator should be able to answer easily.", "labels": [], "entities": []}, {"text": "We discard responses of human evaluators who fail any of these four checks.", "labels": [], "entities": []}, {"text": "The preference of each example is defined as the question which is preferred by the majority of the evaluators.", "labels": [], "entities": []}, {"text": "Examples where neither of the two questions are preferred by the majority of the evaluators, i.e. when there is an equal number of evaluators who prefer each question, are assigned to a separate preference class called \"comparable\".", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "In total, 3, 810 preferences were recorded by 63 independent human evaluators.", "labels": [], "entities": []}, {"text": "The questions produced by each model model pair were evaluated in 5 batches (HITs).", "labels": [], "entities": []}, {"text": "Each human evaluated 44-75 examples (facts and corresponding question pairs) in each batch and each example was evaluated by 3-5 evaluators.", "labels": [], "entities": []}, {"text": "In agreement with the automatic evaluation metrics, the human evaluators strongly prefer either the human or the neural network model over the template-based baseline.", "labels": [], "entities": []}, {"text": "Furthermore, it appears that humans cannot distinguish between the human-generated questions and the neural network questions, on average showing a preference towards the later over the former ones.", "labels": [], "entities": []}, {"text": "We hypothesize this is because our model penalizes uncommon and unnatural ways to frame questionsand sometimes, includes specific information about the target object that the humans do not (see last example in).", "labels": [], "entities": []}, {"text": "This confirms our earlier assertion, that the neural network questions can be used for building question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7539707720279694}]}], "tableCaptions": [{"text": " Table 3: Test performance for all models w.r.t. BLEU, METEOR and Emb. Greedy performance met- rics, where SP indicates models with a single placeholder and MP models with multiple placeholders.  TransE++ indicates models where the TransE embeddings have been pretrained on a larger set of triples.  The best performance on each metric is marked in bold font.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.998392641544342}, {"text": "METEOR", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.931387722492218}]}, {"text": " Table 5: Pairwise human evaluation preferences computed across evaluators with 95% confidence inter- vals. The preferred model in each experiment is marked in bold font. An asterisk next to the preferred  model indicates a statistically significance likelihood-ratio test, which shows that the model is preferred  in at least half of the presented examples with 95% confidence. The name MP Triples TransE++ indi- cates the model with multiple placeholders and TransE embeddings pretrained on a larger set of triples.  The last column shows the Fleiss' kappa averaged across batches (HITs) with different evaluators and  questions.", "labels": [], "entities": []}]}