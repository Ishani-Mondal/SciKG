{"title": [{"text": "Science Question Answering using Instructional Materials", "labels": [], "entities": [{"text": "Science Question Answering", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6338783005873362}]}], "abstractContent": [{"text": "We provide a solution for elementary science tests using instructional materials.", "labels": [], "entities": []}, {"text": "We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs and instructional materials), and uses what it learns to answer novel elementary science questions.", "labels": [], "entities": []}, {"text": "Our evaluation shows that our framework outper-forms several strong baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "We propose an approach for answering multiplechoice elementary science tests using the science curriculum of the student and other domain specific knowledge resources.", "labels": [], "entities": [{"text": "answering multiplechoice elementary science tests", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.8532625555992126}]}, {"text": "Our approach learns latent answer-entailing structures that align question-answers with appropriate snippets in the curriculum.", "labels": [], "entities": []}, {"text": "The student curriculum usually comprises of a set of textbooks.", "labels": [], "entities": []}, {"text": "Each textbook, in-turn comprises of a set of chapters, each chapter is further divided into sections -each discussing a particular science concept.", "labels": [], "entities": []}, {"text": "Hence, the answer-entailing structure consists of selecting a particular textbook from the curriculum, picking a chapter in the textbook, picking a section in the chapter, picking a few sentences in the section and then aligning words/multi-word expressions (mwe's) in the hypothesis (formed by combining the question and an answer candidate) to words/mwe's in the picked sentences.", "labels": [], "entities": []}, {"text": "The answerentailing structures are further refined using external domain-specific knowledge resources such as science dictionaries, study guides and semistructured tables (see).", "labels": [], "entities": []}, {"text": "These domainspecific knowledge resources can be very useful forms of knowledge representation as shown in previous works.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.7214728742837906}]}, {"text": "Alignment is a common technique in many NLP applications such as MT (), RTE (), QA (, etc.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.967743992805481}, {"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.6686133146286011}]}, {"text": "Yet, there are three key differences between our approach and alignment based approaches for QA in the literature: (i) We incorporate the curriculum hierarchy (i.e. the book, chapter, section bifurcation) into the latent structure.", "labels": [], "entities": [{"text": "QA", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9516592621803284}]}, {"text": "This helps us jointly learn the retrieval and answer selection modules of a QA system.", "labels": [], "entities": []}, {"text": "Retrieval and answer selection are usually designed as isolated or loosely connected components in QA systems leading to loss in performance -our approach mitigates this shortcoming.", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9490107893943787}, {"text": "answer selection", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.8500682413578033}]}, {"text": "(ii) Modern textbooks typically provide a set of review questions after each section to help students understand the material better.", "labels": [], "entities": []}, {"text": "We make use of these review problems to further improve our model.", "labels": [], "entities": []}, {"text": "These review problems have additional value as part of the latent structure is known for these questions.", "labels": [], "entities": []}, {"text": "(ii) We utilize domain-specific knowledge sources such as study guides, science dictionaries or semistructured knowledge tables within our model.", "labels": [], "entities": []}, {"text": "The joint model is trained in max-margin fashion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent.", "labels": [], "entities": []}, {"text": "We train and evaluate our models on a set of 8 th grade science problems, science textbooks and multiple domain-specific knowledge resources.", "labels": [], "entities": []}, {"text": "We achieve superior performance vs. a number of baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset: We used a set of 8 th grade science questions released as the training set in the Allen AI Science Challenge 3 for training and evaluating our model.", "labels": [], "entities": [{"text": "Allen AI Science Challenge 3", "start_pos": 91, "end_pos": 119, "type": "DATASET", "confidence": 0.8925346612930298}]}, {"text": "The dataset comprises of 2500 questions.", "labels": [], "entities": []}, {"text": "Each question has 4 answer candidates, of which exactly one is correct.", "labels": [], "entities": []}, {"text": "We used questions 1-1500 for training, questions 1500-2000 for development and questions 2000-2500 for testing.", "labels": [], "entities": []}, {"text": "We also used publicly available 8 th grade science textbooks available through ck12.org.", "labels": [], "entities": []}, {"text": "The science curriculum consists of seven textbooks on Physics, Chemistry, Biology, Earth Science and Life Science.", "labels": [], "entities": []}, {"text": "Each textbook on an average has 18 chapters, and each chapter in turn is divided into 12 sections on an average.", "labels": [], "entities": []}, {"text": "Also, as described before, each section, on an average, is followed by 3-4 multiple choice review questions (total 1369 review questions).", "labels": [], "entities": []}, {"text": "We collected a number of domain specific science dictionaries, study guides, flash cards and semi-structured tables (Simple English Wiktionary and Aristo Tablestore) available online and create triples and equivalences used as external knowledge.", "labels": [], "entities": []}], "tableCaptions": []}