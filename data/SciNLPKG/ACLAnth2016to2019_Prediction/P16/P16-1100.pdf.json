{"title": [{"text": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models", "labels": [], "entities": [{"text": "Achieving Open Vocabulary Neural Machine Translation", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.5721757213274637}]}], "abstractContent": [{"text": "Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8233360350131989}]}, {"text": "This paper presents a novel word-character solution to achieving open vocabulary NMT.", "labels": [], "entities": []}, {"text": "We build hybrid systems that translate mostly at the word level and consult the character components for rare words.", "labels": [], "entities": []}, {"text": "Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed.", "labels": [], "entities": []}, {"text": "The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models.", "labels": [], "entities": []}, {"text": "On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1\u221211.4 BLEU points over models that already handle unknown words.", "labels": [], "entities": [{"text": "WMT'15 English to Czech translation task", "start_pos": 7, "end_pos": 47, "type": "TASK", "confidence": 0.7832583487033844}, {"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9943044781684875}]}, {"text": "Our best system achieves anew state-of-the-art result with 20.7 BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9993042945861816}]}, {"text": "We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is a simple new architecture forgetting machines to translate.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7892793715000153}]}, {"text": "At its core, NMT is a single deep neural network that is trained end-to-end with several advantages such as simplicity and generalization.", "labels": [], "entities": []}, {"text": "Despite being relatively new, NMT has already achieved: Hybrid NMT -example of a wordcharacter model for translating \"a cute cat\" into \"un joli chat\".", "labels": [], "entities": [{"text": "translating \"a cute cat\" into \"un joli chat\"", "start_pos": 105, "end_pos": 149, "type": "TASK", "confidence": 0.7887259870767593}]}, {"text": "Hybrid NMT translates at the word level.", "labels": [], "entities": []}, {"text": "For rare tokens, the character-level components build source representations and recover target <unk>.", "labels": [], "entities": []}, {"text": "state-of-the-art translation results for several language pairs such as English-French (), English-German (, and English-Czech ().", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9609069228172302}]}, {"text": "While NMT offers many advantages over traditional phrase-based approaches, such as small memory footprint and simple decoder implementation, nearly all previous work in NMT has used quite restricted vocabularies, crudely treating all other words the same with an <unk> symbol.", "labels": [], "entities": []}, {"text": "Sometimes, a post-processing step that patches in unknown words is introduced to alleviate this problem.", "labels": [], "entities": []}, {"text": "propose to annotate occurrences of target <unk> with positional information to track their alignments, after which simple word dictionary lookup or identity copy can be performed to replace <unk> in the translation.", "labels": [], "entities": []}, {"text": "approach the problem similarly but obtain the alignments for unknown words from the attention mechanism.", "labels": [], "entities": []}, {"text": "We refer to these as the unk replacement technique.", "labels": [], "entities": [{"text": "unk replacement", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7436912059783936}]}, {"text": "Though simple, these approaches ignore several important properties of languages.", "labels": [], "entities": []}, {"text": "First, monolingually, words are morphologically related; however, they are currently treated as independent entities.", "labels": [], "entities": []}, {"text": "This is problematic as pointed out by: neural networks can learn good representations for frequent words such as \"distinct\", but fail for rare-but-related words like \"distinctiveness\".", "labels": [], "entities": []}, {"text": "Second, crosslingually, languages have different alphabets, so one cannot na\u00efvely memorize all possible surface word translations such as name transliteration between \"Christopher\" (English) and \"Kry\u02d8 stof\".", "labels": [], "entities": [{"text": "name transliteration between \"Christopher\" (English) and \"Kry\u02d8 stof", "start_pos": 138, "end_pos": 205, "type": "TASK", "confidence": 0.7992112679140908}]}, {"text": "See more on this problem in ().", "labels": [], "entities": []}, {"text": "To overcome these shortcomings, we propose a novel hybrid architecture for NMT that translates mostly at the word level and consults the character components for rare words when necessary.", "labels": [], "entities": []}, {"text": "As illustrated in, our hybrid model consists of a word-based NMT that performs most of the translation job, except for the two (hypothetically) rare words, \"cute\" and \"joli\", that are handled separately.", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9653375744819641}]}, {"text": "On the source side, representations for rare words, \"cute\", are computed on-thefly using a deep recurrent neural network that operates at the character level.", "labels": [], "entities": []}, {"text": "On the target side, we have a separate model that recovers the surface forms, \"joli\", of <unk> tokens character-bycharacter.", "labels": [], "entities": []}, {"text": "These components are learned jointly end-to-end, removing the need fora separate unk replacement step as in current NMT practice.", "labels": [], "entities": [{"text": "unk replacement", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.6883353292942047}, {"text": "NMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.903564989566803}]}, {"text": "Our hybrid NMT offers a twofold advantage: it is much faster and easier to train than characterbased models; at the same time, it never produces unknown words as in the case of word-based ones.", "labels": [], "entities": []}, {"text": "We demonstrate at scale that on the WMT'15 English to Czech translation task, such a hybrid approach provides an additional boost of +2.1\u221211.4 BLEU points over models that already handle unknown words.", "labels": [], "entities": [{"text": "WMT'15 English to Czech translation task", "start_pos": 36, "end_pos": 76, "type": "TASK", "confidence": 0.7538696428140005}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9959756731987}]}, {"text": "We achieve anew state-of-theart result with 20.7 BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9558427035808563}]}, {"text": "Our analysis demonstrates that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.", "labels": [], "entities": []}, {"text": "We provide code, data, and models at http: //nlp.stanford.edu/projects/nmt.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness of our models on the publicly available WMT'15 translation task from English into Czech with newstest2013 (3000 sentences) as a development set and newstest2015 (2656 sentences) as a test set.", "labels": [], "entities": [{"text": "WMT'15 translation task from English into Czech", "start_pos": 70, "end_pos": 117, "type": "TASK", "confidence": 0.8147574748311724}]}, {"text": "Two metrics are used: case-sensitive NIST BLEU () and chrF 3 (Popovi\u00b4cPopovi\u00b4c, 2015).", "labels": [], "entities": [{"text": "NIST", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.48089808225631714}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9459484815597534}]}, {"text": "The latter measures the amounts of overlapping character ngrams and has been argued to be a better metric for translation tasks out of English.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.9225143790245056}]}, {"text": "In terms of preprocessing, we apply only the standard tokenization practice.", "labels": [], "entities": []}, {"text": "We choose for each language a list of 200 characters found infrequent words, which, as shown in, can represent more than 98% of the vocabulary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WMT'15 English-Czech data -shown  are various statistics of our training data such as  sentence, token (word and character counts), as  well as type (sizes of the word and character vo- cabularies). We show in addition the amount of  words in a vocabulary expressed by a list of 200  characters found in frequent words.", "labels": [], "entities": [{"text": "WMT'15 English-Czech data", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.9171546101570129}]}, {"text": " Table 2: WMT'15 English-Czech results -shown are the vocabulary sizes, perplexities, BLEU, and  chrF 3 scores of various systems on newstest2015. Perplexities are listed under two categories, word (w)  and character (c). Best and important results per metric are highlighed.", "labels": [], "entities": [{"text": "WMT'15", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.8845387101173401}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.999703586101532}]}, {"text": " Table 3: Word similarity task -shown are Spear- man's correlation \u03c1 on the Rare Word dataset of  various models (with different vocab sizes |V |).", "labels": [], "entities": [{"text": "Word similarity", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7128004729747772}, {"text": "Spear- man's correlation \u03c1", "start_pos": 42, "end_pos": 68, "type": "METRIC", "confidence": 0.6375377376874288}, {"text": "Rare Word dataset", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.7619285782178243}]}]}