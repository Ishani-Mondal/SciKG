{"title": [{"text": "Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation", "labels": [], "entities": [{"text": "Normalising Medical Concepts in Social Media Texts", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.834611850125449}]}], "abstractContent": [{"text": "Automatically recognising medical concepts mentioned in social media messages (e.g. tweets) enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population.", "labels": [], "entities": []}, {"text": "However , the discrepancy between the type of language used in social media and medical ontologies poses a major challenge.", "labels": [], "entities": []}, {"text": "Existing studies deal with this challenge by employing techniques, such as lexical term matching and statistical machine translation.", "labels": [], "entities": [{"text": "lexical term matching", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.6703063944975535}, {"text": "statistical machine translation", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.6792464752991995}]}, {"text": "In this work, we handle the medical concept normalisation at the semantic level.", "labels": [], "entities": [{"text": "medical concept normalisation", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6343464851379395}]}, {"text": "We investigate the use of neural networks to learn the transition between layman's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology.", "labels": [], "entities": []}, {"text": "We evaluate our approaches using three different datasets, where social media texts are extracted from Twitter messages and blog posts.", "labels": [], "entities": []}, {"text": "Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%.", "labels": [], "entities": [{"text": "medical concept normalisation tasks", "start_pos": 186, "end_pos": 221, "type": "TASK", "confidence": 0.7417227923870087}]}], "introductionContent": [{"text": "Existing studies) have shown that data from social media (e.g. Twitter and Facebook 2 ) can be leveraged to improve the understanding of patients' ex-perience in healthcare, such as the spread of infectious diseases and side-effects of drugs.", "labels": [], "entities": []}, {"text": "However, the lexical and grammatical variability of the language used in social media poses a key challenge for extracting information ().", "labels": [], "entities": []}, {"text": "In particular, the frequent use of informal language, non-standard grammar and abbreviation forms, as well as typos in social media messages has to betaken into account by effective information extraction systems.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.729693740606308}]}, {"text": "The task of medical concept normalisation for social media text, which aims to map a variable length social media message to a medical concept in some external coding system, is faced with a similar challenge).", "labels": [], "entities": [{"text": "medical concept normalisation", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6274031003316244}]}, {"text": "Traditional approaches, e.g. (, used proximity matching or heuristic string matching rules based on dictionary lookup when mapping texts to medical concepts.", "labels": [], "entities": [{"text": "proximity matching", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.6370939165353775}]}, {"text": "For example, incorporated edit-distance when mapping similar texts.", "labels": [], "entities": []}, {"text": "The MetaMap system of Aronson (2001) applied a rule-based approach using pre-defined variants of terms when mapping texts to medical concepts in the UMLS Metathesaurus . However, as shown in, existing string matching techniques may not be able to map the social media message \"moon face and 30 lbs in 6 weeks\" to the medical concept 'Weight Gain', or map \"head spinning a little\" to 'Dizziness', as no words in the social media messages and the description of the medical concepts correspond.", "labels": [], "entities": [{"text": "UMLS Metathesaurus", "start_pos": 149, "end_pos": 167, "type": "DATASET", "confidence": 0.9340265691280365}]}, {"text": "Recent studies, e.g. (, applied machine learning techniques to take into account relationships between different words (e.g. synonyms) when performing normal- isation.", "labels": [], "entities": []}, {"text": "For instance, the DNorm system of, which achieved state-of-the-art performance on several medical concept normalisation tasks for medical articles) and patient records, used a pairwise learning-to-rank technique to learn the similarity between different terms when performing concept normalisation.", "labels": [], "entities": [{"text": "medical concept normalisation tasks for medical articles", "start_pos": 90, "end_pos": 146, "type": "TASK", "confidence": 0.7944655077798026}, {"text": "concept normalisation", "start_pos": 276, "end_pos": 297, "type": "TASK", "confidence": 0.7385089993476868}]}, {"text": "leveraged translations between the informal language used in social media and the formal language used in the description of medical concepts in an ontology.", "labels": [], "entities": []}, {"text": "However, we argue that effective concept normalisation requires a system to take into account the semantics of social media messages and medical concepts.", "labels": [], "entities": [{"text": "concept normalisation", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.7412771284580231}]}, {"text": "For example, to be able to map from the social media message \"i don't hunger or thirst\" to the medical concept 'Loss of Appetite', a normalisation system has to take into account the semantics of the whole message; otherwise, \"i don't hunger or thirst\" maybe mapped to the medical concept 'Hunger', because they contain the term \"hunger\" in common.", "labels": [], "entities": []}, {"text": "In this work, we go beyond string matching.", "labels": [], "entities": [{"text": "string matching", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7522499263286591}]}, {"text": "We propose to learn and exploit the semantic similarity between texts from social media messages and medical concepts using deep neural networks.", "labels": [], "entities": []}, {"text": "In particular, we investigate the use of techniques from two families of deep neural networks, i.e. a convolutional neural network (CNN) and a recurrent neural network (RNN), to learn the mapping between social media texts and medical concepts.", "labels": [], "entities": []}, {"text": "We evaluate our approaches using three different datasets that contain messages from Twitter and blog posts.", "labels": [], "entities": []}, {"text": "Our experimental results show that our proposed approaches significantly outperform existing strong baselines (e.g. DNorm) across all of the three datasets.", "labels": [], "entities": []}, {"text": "The performance improvement is by up to 44%.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are threefold: 1.", "labels": [], "entities": []}, {"text": "We propose two novel approaches based on CNN and RNN for medical concept normalisation.", "labels": [], "entities": [{"text": "CNN", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.9756370186805725}, {"text": "RNN", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9060264229774475}, {"text": "medical concept normalisation", "start_pos": 57, "end_pos": 86, "type": "TASK", "confidence": 0.6819688479105631}]}, {"text": "2. We introduce two datasets with the goldstandard mappings between medical concepts and social media texts extracted from tweets and blog posts, respectively.", "labels": [], "entities": []}, {"text": "3. We thoroughly evaluate our proposed approaches using these two datasets and an existing dataset of tweets related to the topic of adverse drug reactions (ADRs)).", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss related work and position our paper in the literature.", "labels": [], "entities": []}, {"text": "Section 3 introduces our neural network approaches for medical concept normalisation.", "labels": [], "entities": [{"text": "medical concept normalisation", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.7085079749425253}]}, {"text": "We describe our experimental setup and empirically evaluate our proposed approaches in Sections 4 and 5, respectively.", "labels": [], "entities": []}, {"text": "We provide further analysis and discussion of our approaches in Section 6.", "labels": [], "entities": []}, {"text": "Finally, Section 7 provides concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our proposed approaches, we use three different datasets (namely, TwADR-S, TwADR-L and AskAPatient) 4 , where the task is to map asocial media phrase to a relevant medical concept.", "labels": [], "entities": [{"text": "TwADR-L", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.6937810182571411}]}, {"text": "In these datasets, a given social media phrase is mapped to only one medical concept.", "labels": [], "entities": []}, {"text": "shows statistics for the three datasets.", "labels": [], "entities": []}, {"text": "In particular, TwADR-S is the dataset provided by, which contains 201 Twitter phrases and their corresponding SNOMED-CT 5 concept.", "labels": [], "entities": [{"text": "TwADR-S", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.598016083240509}]}, {"text": "The total number of target concepts is 58, while on average a medical concept can be mapped by 3.47 queries with the standard deviation of 5.63.", "labels": [], "entities": []}, {"text": "The TwADR-L dataset is our new dataset that we constructed from a collection of three months of tweets (between July and November 2015), downloaded using the Twitter Streaming API 6 by filtering using the name of a pre-defined set of drugs, which have been used in the literature for ADR profiling (e.g. cognitive enhancers)).", "labels": [], "entities": [{"text": "TwADR-L dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8548931181430817}]}, {"text": "These tweets were sampled and then annotated by undergraduate-level linguists.", "labels": [], "entities": []}, {"text": "This collection contains 1,436 Twitter phrases that can be mapped to one of 2,220 medical concepts from the SIDER 4 database of drug profiles 7 . Note that 1,947 from the 2,220 concepts are not relevant to any of the Twitter phrases.", "labels": [], "entities": [{"text": "SIDER 4 database of drug profiles", "start_pos": 108, "end_pos": 141, "type": "DATASET", "confidence": 0.8954119284947714}]}, {"text": "For the AskAPatient dataset, we extracted the gold-standard mappings of social media messages and medical concepts from the ADR annotation collection of . Our AskAPatient dataset contains 8,662 phrases 8 , each of which can be mapped to one of the 1,036 medical concepts from SNOMED-CT and AMT (the Australian Medicines Terminology).", "labels": [], "entities": [{"text": "AskAPatient dataset", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.86311075091362}, {"text": "ADR annotation collection", "start_pos": 124, "end_pos": 149, "type": "DATASET", "confidence": 0.9099057912826538}, {"text": "AskAPatient dataset", "start_pos": 159, "end_pos": 178, "type": "DATASET", "confidence": 0.8176846504211426}]}, {"text": "We expect this dataset to be less difficult than TwADR-S and TwADR-L, as the nature of blog posts is less informal and ambiguous than Twitter messages.", "labels": [], "entities": []}, {"text": "For each of the datasets, we randomly divide it into ten equally folds, so that our approaches and the baselines would be trained on the same sets of data.", "labels": [], "entities": []}, {"text": "We evaluate our approaches based on the accuracy performance, averaged across the ten folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994495511054993}]}, {"text": "The significant difference between the performance of our approaches and the baselines is measured using the paired t-test (p < 0.05).", "labels": [], "entities": []}, {"text": "In this section, we compare the performance of our CNN and RNN approaches for medical concept normalisation against the six baselines, introduced in Section 4.4.", "labels": [], "entities": [{"text": "medical concept normalisation", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.6363819738229116}]}, {"text": "compares the performances of our proposed approaches with the baselines in terms of accuracy on the three datasets (i.e. TwADR-S, TwADR-L, AskAPatient).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9992156028747559}, {"text": "TwADR-S", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.7397820949554443}]}, {"text": "Overall, as expected, the accuracy performance achieved by our approaches and the baselines on the AskAPatient dataset is higher than the TwADR-L and TwADR-S.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994425177574158}, {"text": "AskAPatient dataset", "start_pos": 99, "end_pos": 118, "type": "DATASET", "confidence": 0.9625650346279144}, {"text": "TwADR-L", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.7926626801490784}, {"text": "TwADR-S", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.853173553943634}]}, {"text": "This is due to nature use of language in Twitter, which is more ambiguous and informal than blog posts.", "labels": [], "entities": []}, {"text": "When comparing among the existing baseline approaches, we observe that DNorm and P-MT are the most effective baselines.", "labels": [], "entities": []}, {"text": "In particular, DNorm outperforms the other baselines for the TwADR-S (accuracy 0.2983) and AskAPatient (accuracy 0.7339) datasets, while P-MT with GNews embeddings is the most effective baseline for the TwADR-L dataset (accuracy 0.3371).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9742185473442078}, {"text": "AskAPatient", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.578472912311554}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.6676428318023682}, {"text": "TwADR-L dataset", "start_pos": 203, "end_pos": 218, "type": "DATASET", "confidence": 0.9092515110969543}, {"text": "accuracy", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9848157167434692}]}, {"text": "In addition, term matchingbased approaches, i.e. TF-IDF (accuracy 0.1638, 0.2293 and 0.5547, respectively) and BM25 (accuracy 0.1638, 0.2300 and 0.5546), achieve almost similar performances, which are also comparable to the performances of EmbSim baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9923865795135498}, {"text": "BM25", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.6103352904319763}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9749774932861328}, {"text": "EmbSim baselines", "start_pos": 240, "end_pos": 256, "type": "DATASET", "confidence": 0.9446874260902405}]}, {"text": "When comparing the effectiveness of different: The accuracy performance of our proposed approaches and the baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.99924635887146}]}, {"text": "Significant differences (p < 0.05, paired t-test) compared to the DNorm, P-MT with GNews embeddings, and P-MT with BMC embeddings, are denoted * , \u2022 and \u2022 , respectively.", "labels": [], "entities": [{"text": "DNorm", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8739535212516785}]}, {"text": "pre-trained embeddings used in EmbSim and P-MT, we observe that GNews is more effective than BMC for both approaches, across all of the three datasets.", "labels": [], "entities": [{"text": "EmbSim", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9331517815589905}]}, {"text": "Next, we discuss the performance of our CNN and RNN approaches.", "labels": [], "entities": [{"text": "CNN and RNN", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.6707737843195597}]}, {"text": "From, we observe that both CNN and RNN markedly outperform all of the existing baselines for all of the three datasets.", "labels": [], "entities": [{"text": "CNN", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.9266571402549744}, {"text": "RNN", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8104367852210999}]}, {"text": "When compared with DNorm and P-MT with GNews baselines, which are the most effective existing baselines, we observe that both CNN and RNN significantly (p < 0.05, paired t-test) outperform the two baselines for all of the three datasets.", "labels": [], "entities": []}, {"text": "Indeed, for the TwADR-L dataset, CNN with GNews (accuracy 0.4478) outperforms DNorm (accuracy 0.3099) by 44%.", "labels": [], "entities": [{"text": "TwADR-L dataset", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9085898101329803}, {"text": "CNN", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7748017907142639}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.966330885887146}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9563392400741577}]}, {"text": "In addition, the choice of embeddings has a marked impact on the achieved performance.", "labels": [], "entities": []}, {"text": "In particular, the GNews embeddings benefit both CNN and RNN more than the BMC embeddings, which is inline with the previous finding that GNews is more useful than BMC for the EmbSim and P-MT baselines.", "labels": [], "entities": [{"text": "CNN", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9025598764419556}, {"text": "RNN", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.6758670210838318}, {"text": "EmbSim and P-MT baselines", "start_pos": 176, "end_pos": 201, "type": "DATASET", "confidence": 0.684854730963707}]}, {"text": "On the other than, the randomly generated embeddings (i.e. Rand) are less useful.", "labels": [], "entities": []}, {"text": "These results show that the semantics captured in word embeddings are useful for both CNN and RNN approaches for medical concept normalisation.", "labels": [], "entities": [{"text": "medical concept normalisation", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.7061232725779215}]}, {"text": "However, for both CNN and RNN, the choice of embeddings that are employed has less impact on the performance for the AskAPatient dataset, which has greater number of training data.", "labels": [], "entities": [{"text": "RNN", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8190656304359436}, {"text": "AskAPatient dataset", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.9370193779468536}]}, {"text": "Furthermore, we observe that the LogisticRegression baseline, a variant of our proposed approach that uses the multi-class logistic regression instead of neural networks for identifying relevance concepts, also outperforms the all of the existing baselines.", "labels": [], "entities": []}, {"text": "However, it performs worse than both CNN and RNN approaches.", "labels": [], "entities": []}, {"text": "This shows that while logistic regression can exploit the semantics of embeddings of individual terms in social media texts (at the word level), it cannot learn the semantics of the whole phrase as effectively as CNN and RNN.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the datasets used in the exper- iments. |Q|: Number of queries. |V Q |: Vocabulary  size of queries. |C|: Number of target concepts.  |V C |: Vocabulary size of definition of target con- cepts. |Q \u2192 C| avg and |Q \u2192 C| SD : Average  number of queries mapped to each target concept,  and its standard deviation (SD). |Q \u2192 C| min and  |Q \u2192 C| max : Mininum and maximum number of  queries mapped to a given target concept, respec- tively.", "labels": [], "entities": [{"text": "standard deviation (SD)", "start_pos": 314, "end_pos": 337, "type": "METRIC", "confidence": 0.840876579284668}, {"text": "Mininum", "start_pos": 370, "end_pos": 377, "type": "METRIC", "confidence": 0.9803203344345093}]}, {"text": " Table 4: The accuracy performance of our CNN  approach with the GNews embeddings, when al- lowing (updated emb.) and not allowing (fixed  emb.) the model to update the input word embed- dings. Significant difference (p < 0.05, paired t- test) between the performance achieved by the two  variants, on each dataset, is denoted \u2022 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9990925788879395}, {"text": "GNews embeddings", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.9413195848464966}, {"text": "paired t- test)", "start_pos": 228, "end_pos": 243, "type": "METRIC", "confidence": 0.7639447689056397}]}]}