{"title": [{"text": "Robust Co-occurrence Quantification for Lexical Distributional Semantics", "labels": [], "entities": [{"text": "Robust Co-occurrence Quantification", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8077104687690735}]}], "abstractContent": [{"text": "Previous optimisations of parameters affecting the word-context association measure used in distributional vector space models have focused either on high-dimensional vectors with hundreds of thousands of dimensions, or dense vectors with dimensionality of a few hundreds; but dimensionality of a few thousands is often applied in compositional tasks as it is still computationally feasible and does not require the dimensionality reduction step.", "labels": [], "entities": []}, {"text": "We present a systematic study of the interaction of the parameters of the association measure and vector dimensionality, and derive parameter selection heuristics that achieve performance across word similarity and relevance datasets competitive with the results previously reported in the literature achieved by highly dimensional or dense models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Words that occur in similar context have similar meaning.", "labels": [], "entities": []}, {"text": "Thus the meaning of a word can be modeled by counting its cooccurrence with neighboring words in a corpus.", "labels": [], "entities": []}, {"text": "Distributional models of meaning represent cooccurrence information in a vector space, where the dimensions are the neighboring words and the values are co-occurrence counts.", "labels": [], "entities": []}, {"text": "Successful models need to be able to discriminate co-occurrence information, as not all co-occurrence counts are equally useful, for instance, the co-occurrence with the article the is less informative than with the noun existence.", "labels": [], "entities": []}, {"text": "The discrimination is usually achieved by weighting of co-occurrence counts.", "labels": [], "entities": []}, {"text": "Another fundamental question in vector space design is the vector space dimensionality and what neighbor words should correspond to them.", "labels": [], "entities": [{"text": "vector space design", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.6557990511258444}]}, {"text": "propose optimisations for co-occurrence-based distributional models, using parameters adopted from predictive models (: shifting and context distribution smoothing.", "labels": [], "entities": [{"text": "context distribution smoothing", "start_pos": 133, "end_pos": 163, "type": "TASK", "confidence": 0.6721835533777872}]}, {"text": "Their experiments and thus their parameter recommendations use highdimensional vector spaces with word vector dimensionality of almost 200K, and many recent state-of-the-art results in lexical distributional semantics have been obtained using vectors with similarly high dimensionality (.", "labels": [], "entities": []}, {"text": "In contrast, much work on compositional distributional semantics employs vectors with much fewer dimensions: e.g. 2K (), 3K (.", "labels": [], "entities": []}, {"text": "The most common reason thereof is that these models assign tensors to functional words.", "labels": [], "entities": []}, {"text": "For a vector space V with k dimensions, a tensor V \u2297V \u00b7 \u00b7 \u00b7\u2297V of rank n has kn dimensions.", "labels": [], "entities": []}, {"text": "Adjectives and intransitive verbs have tensors of rank 2, transitive verbs are of rank 3; for coordinators, the rank can go up to 7.", "labels": [], "entities": []}, {"text": "Taking k = 200K already results in a highly intractable tensor of 8 \u00d7 10 15 dimensions fora transitive verb.", "labels": [], "entities": []}, {"text": "An alternative way of obtaining a vector space with few dimensions, usually with just 100-500, is the use of SVD as apart of Latent Semantic Analysis) or another models such as SGNS () and GloVe).", "labels": [], "entities": []}, {"text": "However, these models take more time to instantiate in comparison to weighting of a co-occurrence matrix, bring more parameters to explore and produce vector spaces with uninterpretable dimensions (vector space dimension interpretation is used by some lexical mod-els, for example,, and the passage from formal semantics to tensor models relies on it).", "labels": [], "entities": [{"text": "vector space dimension interpretation", "start_pos": 198, "end_pos": 235, "type": "TASK", "confidence": 0.6407541111111641}]}, {"text": "In this work we focus on vector spaces that directly weight a co-occurrence matrix and report results for SVD, GloVe and SGNS from the study of for comparison.", "labels": [], "entities": []}, {"text": "The mismatch of recent experiments with nondense models in vector dimensionality between lexical and compositional tasks gives rise to a number of questions: \u2022 To what extent does model performance depend on vector dimensionality?", "labels": [], "entities": []}, {"text": "\u2022 Do parameters influence 200K and 1K dimensional models similarly?", "labels": [], "entities": []}, {"text": "Can the findings of be directly applied to models with a few thousand dimensions?", "labels": [], "entities": []}, {"text": "\u2022 If not, can we derive suitable parameter selection heuristics which take account of dimensionality?", "labels": [], "entities": []}, {"text": "To answer these questions, we perform a systematic study of distributional models with a rich set of parameters on SimLex-999 (), a lexical similairty dataset, and test selected models on MEN (), a lexical relatedness dataset.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.882924497127533}]}, {"text": "These datasets are currently widely used and surpass datasets stemming from information retrieval,), and computational linguistics, RG65, in quantity by having more entries and in quality by attention to evaluated relations (Milajevs and Griffiths, 2016).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7556618452072144}, {"text": "RG65", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.8309550881385803}]}], "datasetContent": [{"text": "For lognSCPMI, models with D < 20K should use global context probabilities and k = 0.7; otherwise, local context probabilities without smoothing should be preferred with k = 1.4.", "labels": [], "entities": []}, {"text": "We evaluate these heuristics by comparing the performance they give on SimLex-999 against that obtained using the best possible parameter selections (determined via an exhaustive search at each dimensionality setting).", "labels": [], "entities": []}, {"text": "We also compare them to the best scores reported by for their model (PMI and SVD), word2vec-SGNS () and GloVe ()-see, where only the betterperforming SPMI and SCPMI are shown.", "labels": [], "entities": []}, {"text": "For lognPMI and lognCPMI, our heuristics pick the best possible models.", "labels": [], "entities": []}, {"text": "For lognSPMI, where performance variance is low, the heuristics do well, giving a performance of no more than 0.01 points below the best configuration.", "labels": [], "entities": []}, {"text": "For 1SPMI and nSPMI the difference is higher.", "labels": [], "entities": []}, {"text": "With lognSCPMI and 1SCPMI, the heuristics follow.", "labels": [], "entities": [{"text": "1SCPMI", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.8034757971763611}]}, {"text": "We also give our best score, SVD, SGNS and GloVe numbers from that study for comparison.", "labels": [], "entities": [{"text": "SVD", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.7700139284133911}, {"text": "SGNS", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.6886061429977417}, {"text": "GloVe", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9855136275291443}]}, {"text": "On the right, our heuristic in comparison to the best and average results together with the models selected using the recommendations presented in. the best selection, but with a wider gap than the SPMI models.", "labels": [], "entities": []}, {"text": "In general n-weighted models do not perform as well as others.", "labels": [], "entities": []}, {"text": "Overall, log n weighting should be used with PMI, CPMI and SCPMI.", "labels": [], "entities": []}, {"text": "High-dimensional SPMI models show the same behaviour, but if D < 10K, no weighting should be applied.", "labels": [], "entities": [{"text": "SPMI", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.9581269025802612}]}, {"text": "SPMI and SCPMI should be preferred over CPMI and PMI.", "labels": [], "entities": []}, {"text": "As shows, our heuristics give performance close to the optimum for any dimensionality, with a large improvement over both an average parameter setting and the parameters suggested by in a high-dimensional setting.", "labels": [], "entities": []}, {"text": "Finally, to see whether the heuristics transfer robustly, we repeat this comparison on the MEN dataset (see.", "labels": [], "entities": [{"text": "MEN dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9695232510566711}]}, {"text": "Again, PMI and CPMI follow the best possible setup, with SPMI and SCPMI showing only a slight drop below ideal performance; and again, the heuristic settings give performance close to the optimum, and significantly higher than average or standard parameters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Our model in comparison to the pre- vious work. On the similarity dataset our model  is 0.008 points behind a PPMI model, however on  the relatedness dataset 0.020 points above. Note  the difference in dimensionality, source corpora  and window size. SVD, SGNS and GloVe num- bers are given for comparison.  *  Results reported  by", "labels": [], "entities": []}]}