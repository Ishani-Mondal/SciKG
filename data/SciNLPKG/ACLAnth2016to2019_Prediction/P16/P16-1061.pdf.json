{"title": [{"text": "Improving Coreference Resolution by Learning Entity-Level Distributed Representations", "labels": [], "entities": [{"text": "Improving Coreference Resolution", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8782843748728434}]}], "abstractContent": [{"text": "A long-standing challenge in coreference resolution has been the incorporation of entity-level information-features defined over clusters of mentions instead of mention pairs.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9728688299655914}]}, {"text": "We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters.", "labels": [], "entities": []}, {"text": "Using these representations, our system learns when combining clusters is desirable.", "labels": [], "entities": []}, {"text": "We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final corefer-ence partition.", "labels": [], "entities": []}, {"text": "The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.", "labels": [], "entities": [{"text": "CoNLL 2012 Shared Task dataset", "start_pos": 109, "end_pos": 139, "type": "DATASET", "confidence": 0.9083258867263794}]}], "introductionContent": [{"text": "Coreference resolution, the task of identifying which mentions in a text refer to the same realworld entity, is fundamentally a clustering problem.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9234777688980103}]}, {"text": "However, many recent state-of-the-art coreference systems operate solely by linking pairs of mentions together).", "labels": [], "entities": []}, {"text": "An alternative approach is to use agglomerative clustering, treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity.", "labels": [], "entities": []}, {"text": "Such systems can take advantage of entity-level information, i.e., features between clusters of mentions instead of between just two mentions.", "labels": [], "entities": []}, {"text": "As an example for why this is useful, it is clear that the clusters {Bill Clinton} and {Clinton, she} are not referring to the same entity, but it is ambiguous whether the pair of mentions Bill Clinton and Clinton are coreferent.", "labels": [], "entities": []}, {"text": "Previous work has incorporated entity-level information through features that capture hard constraints like having gender or number agreement between clusters (.", "labels": [], "entities": []}, {"text": "In this work, we instead train a deep neural network to build distributed representations of pairs of coreference clusters.", "labels": [], "entities": []}, {"text": "This captures entity-level information with a large number of learned, continuous features instead of a small number of hand-crafted categorical ones.", "labels": [], "entities": []}, {"text": "Using the cluster-pair representations, our network learns when combining two coreference clusters is desirable.", "labels": [], "entities": []}, {"text": "At test time it builds up coreference clusters incrementally, starting with each mention in its own cluster and then merging a pair of clusters each step.", "labels": [], "entities": []}, {"text": "It makes these decisions with a novel easy-first cluster-ranking procedure that combines the strengths of cluster-ranking) and easy-first) coreference algorithms.", "labels": [], "entities": []}, {"text": "Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made.", "labels": [], "entities": []}, {"text": "We address this by using a learning-to-search algorithm inspired by SEARN) to train our neural network.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.7842748761177063}]}, {"text": "This approach allows the model to learn which action (a cluster merge) available from the current state (a partially completed coreference clustering) will eventually lead to a high-scoring coreference partition.", "labels": [], "entities": []}, {"text": "Our system uses little manual feature engineering, which means it is easily extended to multiple languages.", "labels": [], "entities": []}, {"text": "We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset.", "labels": [], "entities": [{"text": "CoNLL 2012 Shared Task dataset", "start_pos": 66, "end_pos": 96, "type": "DATASET", "confidence": 0.938942813873291}]}, {"text": "The cluster-ranking model significantly outperforms a mention-ranking model that does not use entity-level information.", "labels": [], "entities": []}, {"text": "We also show that using an easy-first strategy improves the performance of the cluster-ranking model.", "labels": [], "entities": []}, {"text": "Our final system achieves CoNLL F 1 scores of 65.29 for English and 63.66 for Chinese, substantially outperforming other state-of-the-art systems.", "labels": [], "entities": [{"text": "CoNLL F 1 scores", "start_pos": 26, "end_pos": 42, "type": "METRIC", "confidence": 0.8049797415733337}]}], "datasetContent": [{"text": "We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data ().", "labels": [], "entities": [{"text": "CoNLL 2012 Shared Task data", "start_pos": 62, "end_pos": 89, "type": "DATASET", "confidence": 0.9426026463508606}]}, {"text": "The models are evaluated using three of the most popular coreference metrics: MUC, B 3 , and Entity-based CEAF (CEAF \u03c6 4 ).", "labels": [], "entities": [{"text": "B 3", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9506927728652954}, {"text": "Entity-based CEAF (CEAF \u03c6 4 )", "start_pos": 93, "end_pos": 122, "type": "METRIC", "confidence": 0.8658054045268467}]}, {"text": "We generally report the average F 1 score (CoNLL F 1 ) of the three, which is common practice in coreference evaluation.", "labels": [], "entities": [{"text": "F 1 score (CoNLL F 1 )", "start_pos": 32, "end_pos": 54, "type": "METRIC", "confidence": 0.8909042440354824}, {"text": "coreference evaluation", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.9334492385387421}]}, {"text": "We used the most recent version of the CoNLL scorer (version 8.01), which implements the original definitions of the metrics.", "labels": [], "entities": [{"text": "CoNLL scorer", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8537351191043854}]}, {"text": "Our experiments were run using system-produced predicted mentions.", "labels": [], "entities": []}, {"text": "We used the rule-based mention detection algorithm from, which first extracts pronouns and maximal NP projections as candidate mentions and then filters this set with rules that remove spurious mentions such as numeric entities and pleonastic it pronouns.", "labels": [], "entities": [{"text": "rule-based mention detection", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.663713683684667}]}, {"text": "We performed a feature ablation study to determine the importance of the hand-engineered features included in our model.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We find the small number of non-embedding features substantially improves model performance, especially the distance and string matching features.", "labels": [], "entities": []}, {"text": "This is unsurprising, as the additional features are not easily captured byword embeddings and historically such features have been very important in coreference resolvers.", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.9245547354221344}]}, {"text": "We evaluate the benefit of the two-step pretraining for the   training on a fixed trajectory of correct actions instead of using learning to search.", "labels": [], "entities": []}, {"text": "mention-ranking model and report results in Table 2.", "labels": [], "entities": []}, {"text": "Consistent with, we find pretraining to greatly improve the model's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9981837868690491}]}, {"text": "We note in particular that the model benefits from using both pretraining steps from Section 4, which more smoothly transitions the model from a mention-pair classification objective that is easy to optimize to a max-margin objective better suited fora ranking task.", "labels": [], "entities": []}, {"text": "We evaluate the importance of three key details of the cluster ranker: initializing it with the mentionranking model's weights, using an easy-first ordering of mentions, and using learning to search.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We compare initializing the cluster-ranking model randomly with initializing it with the weights learned by the mentionranking model.", "labels": [], "entities": []}, {"text": "Using pretrained weights greatly improves performance.", "labels": [], "entities": []}, {"text": "We believe the clusterranking model has difficulty learning effective weights from scratch due to noise in the signal coming from cluster-level decisions (an overall bad cluster merge may still involve a few cor-rect pairwise links) and the smaller amount of data used to train the cluster-ranking model (many possible actions are pruned away during preprocessing).", "labels": [], "entities": []}, {"text": "We believe the score would be even lower without search-space pruning, which stops the model from considering many bad actions.", "labels": [], "entities": []}, {"text": "We compare the effectiveness of easy-first cluster-ranking with the commonly used left-to-right approach.", "labels": [], "entities": []}, {"text": "Using a left-to-right strategy simply requires changing the preprocessing step ordering the mentions so mentions are sorted by their position in the document instead of their highest scoring coreference link according to the mention-ranking model.", "labels": [], "entities": []}, {"text": "We find the easy-first approach slightly outperforms using a left-to-right ordering of mentions.", "labels": [], "entities": []}, {"text": "We believe this is because delaying hard decisions until later reduces the problem of early mistakes causing later decisions to be made incorrectly.", "labels": [], "entities": []}, {"text": "We also compare learning to search with the simpler approach of training the model on a trajectory of gold coreference decisions (i.e., training on a fixed cost-sensitive classification dataset).", "labels": [], "entities": []}, {"text": "Using this approach significantly decreases performance.", "labels": [], "entities": []}, {"text": "We attribute this to the model not learning how to deal with mistakes when it only sees correct decisions during training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: CoNLL F 1 scores of the mention-ranking  model on the dev sets without mention, docu- ment genre, distance, speaker, and string matching  hand-engineered features.", "labels": [], "entities": [{"text": "CoNLL F 1", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.6558250784873962}]}, {"text": " Table 2: CoNLL F 1 scores of the mention-ranking  model on the dev sets with different pretraining  methods.", "labels": [], "entities": [{"text": "CoNLL F 1", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7460269927978516}]}, {"text": " Table 3: CoNLL F 1 scores of the cluster-ranking  model on the dev sets with various ablations.  -PRETRAINING: initializing model parameters  randomly instead of from the mention-ranking  model, -EASY-FIRST: iterating through mentions  in order of occurrence instead of according to their  highest scoring candidate coreference link, -L2S:", "labels": [], "entities": [{"text": "CoNLL F 1", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8770235776901245}, {"text": "PRETRAINING", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.904761552810669}, {"text": "EASY-FIRST", "start_pos": 197, "end_pos": 207, "type": "METRIC", "confidence": 0.949620246887207}]}, {"text": " Table 5: Comparison with the current state-of-the-art approaches on the CoNLL 2012 test sets. NN  Mention Ranker and NN Cluster Ranker are contributions of this work.", "labels": [], "entities": [{"text": "CoNLL 2012 test sets", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.9632774591445923}]}]}