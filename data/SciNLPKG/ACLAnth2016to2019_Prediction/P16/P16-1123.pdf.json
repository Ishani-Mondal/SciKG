{"title": [{"text": "Relation Classification via Multi-Level Attention CNNs", "labels": [], "entities": [{"text": "Relation Classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9765900075435638}, {"text": "Multi-Level Attention CNNs", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.5526231427987417}]}], "abstractContent": [{"text": "Relation classification is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text.", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9608108699321747}, {"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7247980535030365}]}, {"text": "We propose a novel convolutional neural network architecture for this task, relying on two levels of attention in order to better discern patterns in heterogeneous contexts.", "labels": [], "entities": []}, {"text": "This architecture enables end-to-end learning from task-specific labeled data, forgoing the need for external knowledge such as explicit dependency structures.", "labels": [], "entities": []}, {"text": "Experiments show that our model outper-forms previous state-of-the-art methods, including those relying on much richer forms of prior knowledge.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation classification is the task of identifying the semantic relation holding between two nominal entities in text.", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9693171381950378}]}, {"text": "It is a crucial component in natural language processing systems that need to mine explicit facts from text, e.g. for various information extraction applications as well as for question answering and knowledge base completion).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.7252002954483032}, {"text": "question answering", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.8946200311183929}, {"text": "knowledge base completion", "start_pos": 200, "end_pos": 225, "type": "TASK", "confidence": 0.6300980945428213}]}, {"text": "For instance, given the example input \"Fizzy and meat cause heart disease and.\" with annotated target entity mentions e 1 = \"drinks\" and e 2 = \"diabetes\", the goal would be to automatically recognize that this sentence expresses a causeeffect relationship between e 1 and e 2 , for which we use the notation Cause-Effect(e 1 ,e 2 ).", "labels": [], "entities": []}, {"text": "Accurate relation classification facilitates precise sentence interpretations, discourse processing, and higherlevel NLP tasks ().", "labels": [], "entities": [{"text": "relation classification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7066509574651718}, {"text": "precise sentence interpretations", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6621986130873362}, {"text": "discourse processing", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7137982994318008}]}, {"text": "Thus, * Equal contribution.", "labels": [], "entities": []}, {"text": "Email: liuzy@tsinghua.edu.cn relation classification has attracted considerable attention from researchers over the course of the past decades).", "labels": [], "entities": [{"text": "relation classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.9038379788398743}]}, {"text": "In the example given above, the verb corresponds quite closely to the desired target relation.", "labels": [], "entities": []}, {"text": "However, in the wild, we encounter a multitude of different ways of expressing the same kind of relationship.", "labels": [], "entities": []}, {"text": "This challenging variability can be lexical, syntactic, or even pragmatic in nature.", "labels": [], "entities": []}, {"text": "An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure.", "labels": [], "entities": []}, {"text": "Thus, it is not surprising that numerous featureand kernel-based approaches have been proposed, many of which rely on a full-fledged NLP stack, including POS tagging, morphological analysis, dependency parsing, and occasionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 154, "end_pos": 165, "type": "TASK", "confidence": 0.7785724103450775}, {"text": "dependency parsing", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.7805489301681519}]}, {"text": "In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources.", "labels": [], "entities": []}, {"text": "A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification ().", "labels": [], "entities": [{"text": "relation classification", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.947659820318222}]}, {"text": "Still, these models often fail to identify critical cues, and many of them still require an external dependency parser.", "labels": [], "entities": []}, {"text": "We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches.", "labels": [], "entities": []}, {"text": "Our key contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "Our CNN architecture relies on a novel multi-level attention mechanism to capture both entity-specific attention (primary attention at the input level, with respect to the target entities) and relation-specific pooling attention (secondary attention with respect to the target relations).", "labels": [], "entities": []}, {"text": "This allows it to detect more subtle cues despite the heterogeneous structure of the input sentences, enabling it to automatically learn which parts are relevant fora given classification.", "labels": [], "entities": []}, {"text": "2. We introduce a novel pair-wise margin-based objective function that proves superior to standard loss functions.", "labels": [], "entities": []}, {"text": "3. We obtain the new state-of-the-art results for relation classification with an F1 score of 88.0% on the SemEval 2010 Task 8 dataset, outperforming methods relying on significantly richer prior knowledge.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.9274225533008575}, {"text": "F1 score", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.985678642988205}, {"text": "SemEval 2010 Task 8 dataset", "start_pos": 107, "end_pos": 134, "type": "DATASET", "confidence": 0.7931745529174805}]}], "datasetContent": [{"text": "We conduct our experiments on the commonly used, which contains 10,717 sentences for nine types of annotated relations, together with an additional \"Other\" type.", "labels": [], "entities": []}, {"text": "The nine types are: Cause-Effect, Component-Whole, Content-Container, EntityDestination, Entity-Origin, Instrument-Agency, Member-Collection, Message-Topic, and ProductProducer, while the relation type \"Other\" indicates that the relation expressed in the sentence is not among the nine types.", "labels": [], "entities": []}, {"text": "However, for each of the aforementioned relation types, the two entities can also appear in inverse order, which implies that the sentence needs to be regarded as expressing a different relation, namely the respective inverse one.", "labels": [], "entities": []}, {"text": "For example, Cause-Effect(e 1 ,e 2 ) and CauseEffect(e 2 ,e 1 ) can be considered two distinct relations, so the total number |Y| of relation types is 19.", "labels": [], "entities": []}, {"text": "The SemEval-2010 Task 8 dataset consists of a training set of 8,000 examples, and a test set with the remaining examples.", "labels": [], "entities": [{"text": "SemEval-2010 Task 8 dataset", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.6530212759971619}]}, {"text": "We evaluate the models using the official scorer in terms of the Macro-F1 score over the nine relation pairs (excluding Other).", "labels": [], "entities": []}, {"text": "We use the word2vec skip-gram model () to learn initial word representations on Wikipedia.", "labels": [], "entities": []}, {"text": "Other matrices are initialized with random values following a Gaussian distribution.", "labels": [], "entities": []}, {"text": "We apply a cross-validation procedure on the training data to select suitable hyperparameters.", "labels": [], "entities": []}, {"text": "The choices generated by this process are given in.", "labels": [], "entities": []}, {"text": "Learning rate 0.0001: Hyperparameters.  approaches.", "labels": [], "entities": [{"text": "Learning rate", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8875889480113983}]}, {"text": "We observe that our novel attentionbased architecture achieves new state-of-the-art results on this relation classification dataset.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.7873792350292206}]}, {"text": "AttInput-CNN relies only on the primal attention at the input level, performing standard max-pooling after the convolution layer to generate the network output w O , in which the new objective function is utilized.", "labels": [], "entities": []}, {"text": "With Att-Input-CNN, we achieve an F1-score of 87.5%, thus already outperforming not only the original winner of the SemEval task, an SVM-based approach (82.2%), but also the wellknown CR-CNN model (84.1%) with a relative improvement of 4.04%, and the newly released DRNNs (85.8%) with a relative improvement of 2.0%, although the latter approach depends on the Stanford parser to obtain dependency parse information.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9996125102043152}, {"text": "dependency parse information", "start_pos": 387, "end_pos": 415, "type": "TASK", "confidence": 0.7560417453447977}]}, {"text": "Our full dual attention model Att-Pooling-CNN achieves an even more favorable F1-score of 88%.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9996503591537476}]}, {"text": "provides the experimental results for the two variants of the model given by Eqs. and in Section 3.3.", "labels": [], "entities": [{"text": "Eqs.", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8460090756416321}]}, {"text": "Our main model outperforms the other variants on this dataset, although the variants may still prove useful when applied to other tasks.", "labels": [], "entities": []}, {"text": "To better quantify the contribution of the different components of our model, we also conduct an ablation study evaluating several simplified models.", "labels": [], "entities": []}, {"text": "The first simplification is to use our model without the input attention mechanism but with the pooling attention layer.", "labels": [], "entities": []}, {"text": "The second removes both attention mechanisms.", "labels": [], "entities": []}, {"text": "The third removes both forms of attention and additionally uses a regular objective function based on the inner product s = r \u00b7 w fora sentence representation rand relation class embedding w.", "labels": [], "entities": []}, {"text": "We observe that all three of our components lead to noticeable improvements over these baselines.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparison with results published in the  literature, where ' * ' refers to models from Nguyen  and Grishman (2015).", "labels": [], "entities": []}]}