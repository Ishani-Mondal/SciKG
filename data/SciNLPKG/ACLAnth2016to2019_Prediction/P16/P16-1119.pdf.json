{"title": [{"text": "Annotating and Predicting Non-Restrictive Noun Phrase Modifications", "labels": [], "entities": [{"text": "Predicting Non-Restrictive Noun Phrase Modifications", "start_pos": 15, "end_pos": 67, "type": "TASK", "confidence": 0.7847951173782348}]}], "abstractContent": [{"text": "The distinction between restrictive and non-restrictive modification in noun phrases is a well studied subject in linguistics.", "labels": [], "entities": []}, {"text": "Automatically identifying non-restrictive modifiers can provide NLP applications with shorter, more salient arguments, which were found beneficial by several recent works.", "labels": [], "entities": []}, {"text": "While previous work showed that restrictiveness can be annotated with high agreement, no large scale corpus was created, hindering the development of suitable classification algorithms.", "labels": [], "entities": []}, {"text": "In this work we devise a novel crowdsourcing annotation methodology, and an accompanying large scale corpus.", "labels": [], "entities": []}, {"text": "Then, we present a robust automated system which identifies non-restrictive modifiers, notably improving over prior methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguistic literature provides a large body of research distinguishing between two types of modifiers within noun phrases: (1) Restrictive modifiers, which constitute an integral part of the entity referenced by the NP, e.g., the underlined modifier in \"She wore the necklace that her mother gave her\", versus (2) Non-restrictive modifiers, which provide an additional or parenthetical information on an already definite entity, e.g., \"The speaker thanked president Obama who just came into the room\" ().", "labels": [], "entities": []}, {"text": "The distinction between the two types is semantic in nature and relies heavily on the context of the NP.", "labels": [], "entities": []}, {"text": "Evidently, many syntactic constructions can appear in both restrictive and non-restrictive uses.", "labels": [], "entities": []}, {"text": "While the previous examples were of relative clauses, demonstrates this distinction in various other syntactic constructions.", "labels": [], "entities": []}, {"text": "Identifying and removing non-restrictive modifiers yields shorter NP arguments, which proved beneficial in many NLP tasks.", "labels": [], "entities": []}, {"text": "In the context of abstractive summarization ( or sentence compression (), non-restrictive modifiers can be removed to shorten sentences, while restrictive modification should be preserved.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.5846898406744003}, {"text": "sentence compression", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7242336273193359}]}, {"text": "Further, recent work in information extraction showed that shorter arguments can be beneficial for downstream tasks.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8869995772838593}]}, {"text": "built an Open-IE system which focuses on shorter argument spans, and demonstrated its usefulness in a state-of-the-art Knowledge Base Population system.", "labels": [], "entities": []}, {"text": "compared the performance of several off-the-shelf analyzers in different semantic tasks.", "labels": [], "entities": []}, {"text": "Most relevant to this work is the comparison between Open-IE and Semantic Role Labeling ().", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7188070217768351}]}, {"text": "Specifically, they suggest that SRL's longer arguments introduce noise which hurts performance for downstream tasks.", "labels": [], "entities": [{"text": "SRL", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9753736853599548}]}, {"text": "Finally, in question answering, omitting nonrestrictive modification can assist in providing more concise answers, or in matching between multiple answer occurrences.", "labels": [], "entities": [{"text": "question answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8444681763648987}]}, {"text": "Despite these benefits, there is currently no consistent large scale annotation of restrictiveness, which hinders the development of automatic tools for its classification.", "labels": [], "entities": []}, {"text": "In prior art in this field, used trained annotators to mark restrictiveness in a large corpus.", "labels": [], "entities": []}, {"text": "Although they reached good agreement levels in restrictiveness annotation, their corpus suffered from inconsistencies, since it conflated restrictiveness annotation with inconsistent modifier span annotation.", "labels": [], "entities": []}, {"text": "The contributions of this work are twofold.", "labels": [], "entities": []}, {"text": "Primarily, we propose a novel crowdsroucing anno-tation methodology which decouples the binary (restrictive / non-restrictive) distinction from the modifier span annotation (Section 3).", "labels": [], "entities": []}, {"text": "Following this methodology, in Section 4 we present a large scale annotated corpus, which will allow further research into the automatic identification of nonrestrictive modification.", "labels": [], "entities": [{"text": "automatic identification of nonrestrictive modification", "start_pos": 127, "end_pos": 182, "type": "TASK", "confidence": 0.6885073900222778}]}, {"text": "Additionally, we developed a strong automatic classifier, which learns from our new corpus (Section 5).", "labels": [], "entities": []}, {"text": "This classifier uses new linguistically motivated features which are robust enough to perform well over automatically predicted parse trees.", "labels": [], "entities": []}, {"text": "The corpus and the automatic classifier are both made publicly available.", "labels": [], "entities": []}, {"text": "While there is still much room for improvement, especially in some of the harder, more context-dependent, cases (most notably, prepositional and adjectival modifiers), our system provides an applicable means for identifying nonrestrictive modification in a realistic NLP setting.", "labels": [], "entities": [{"text": "identifying nonrestrictive modification", "start_pos": 212, "end_pos": 251, "type": "TASK", "confidence": 0.7180992166201273}]}], "datasetContent": [{"text": "We use the QA-SRL test section (containing 412 NP modifiers) to evaluate each of the systems described in Section 5 on gold and predicted trees, both provided in the CoNLL 2009 dataset (the predicted dependency relations were obtained using MaltParser ().", "labels": [], "entities": [{"text": "QA-SRL test section", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.8678206006685892}, {"text": "CoNLL 2009 dataset", "start_pos": 166, "end_pos": 184, "type": "DATASET", "confidence": 0.9726394216219584}]}, {"text": "The gold setting allows us to test the performance of the systems without accumulating parser errors.", "labels": [], "entities": []}, {"text": "In addition, it allows us to partition and analyze our dataset according to the gold modifier type.", "labels": [], "entities": []}, {"text": "The predicted setting, on the other hand, allows us to evaluate our classifier in a real-world application scenario, given automatic parsing output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics by modifier types, which were identified by part of speech (pos) and depen- dency label (rel) (Section 4.1). The number of instances (#) and non-restrictiveness percentage refer to  the full crowdsourced annotation. Agreement (Cohen's \u03ba and percent of matching instances) is reported  for the expert-annotated data", "labels": [], "entities": [{"text": "Agreement", "start_pos": 244, "end_pos": 253, "type": "METRIC", "confidence": 0.9939519762992859}]}, {"text": " Table 3: Test set performance of the 3 different systems described in Sections 5 and 6 on gold trees from  the CoNLL 2009 dataset, across the different categories defined in Section 4.", "labels": [], "entities": [{"text": "CoNLL 2009 dataset", "start_pos": 112, "end_pos": 130, "type": "DATASET", "confidence": 0.9491414626439413}]}, {"text": " Table 4: Feature ablation tests on gold trees. Each  row specifies a different feature set -\"All\" speci- fies the entire feature set from", "labels": [], "entities": []}]}