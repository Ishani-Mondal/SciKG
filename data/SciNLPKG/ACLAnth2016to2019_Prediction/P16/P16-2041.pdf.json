{"title": [{"text": "Annotating Relation Inference in Context via Question Answering", "labels": [], "entities": [{"text": "Annotating Relation Inference in Context via Question Answering", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8640265837311745}]}], "abstractContent": [{"text": "We present anew annotation method for collecting data on relation inference in context.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7389461696147919}]}, {"text": "We convert the inference task to one of simple factoid question answering, allowing us to easily scale up to 16,000 high-quality examples.", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.732215146223704}]}, {"text": "Our method corrects a major bias in previous evaluations, making our dataset much more realistic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recognizing entailment between natural-language relations (predicates) is a key challenge in many semantic tasks.", "labels": [], "entities": [{"text": "Recognizing entailment between natural-language relations (predicates)", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.9126960560679436}]}, {"text": "For instance, in question answering (QA), it is often necessary to \"bridge the lexical chasm\" between the asker's choice of words and those that appear in the answer text.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.896511173248291}]}, {"text": "Relation inference can be notoriously difficult to automatically recognize because of semantic phenomena such as polysemy and metaphor: Q: Which drug treats headaches?", "labels": [], "entities": [{"text": "Relation inference", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9388312995433807}]}, {"text": "A: Aspirin eliminates headaches.", "labels": [], "entities": []}, {"text": "In this context, \"eliminates\" implies \"treats\" and the answer is indeed \"aspirin\".", "labels": [], "entities": []}, {"text": "However, this rule does not always hold for other cases -\"eliminates patients\" has a very different meaning from \"treats patients\".", "labels": [], "entities": []}, {"text": "Hence, context-sensitive methods are required to solve relation inference.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7478322684764862}]}, {"text": "Many methods have tried to address relation inference, from DIRT () through Sherlock ( to the more recent work on PPDB () and RELLY (.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8585926294326782}, {"text": "RELLY", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.7612578272819519}]}, {"text": "However, the way these methods are evaluated remains largely inconsistent.", "labels": [], "entities": []}, {"text": "Some papers that deal with phrasal inference in general ( use an extrinsic task, such as a recent recognizing textual entailment (RTE) benchmark.", "labels": [], "entities": [{"text": "phrasal inference", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7912342846393585}, {"text": "recognizing textual entailment (RTE)", "start_pos": 98, "end_pos": 134, "type": "TASK", "confidence": 0.6381589422623316}]}, {"text": "By nature, extrinsic tasks incorporate a variety of linguistic phenomena, making it harder to analyze the specific issues of relation inference.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.8073214888572693}]}, {"text": "The vast majority of papers that do focus on relation inference perform some form of post-hoc evaluation ().", "labels": [], "entities": [{"text": "relation inference", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9233467876911163}]}, {"text": "Typically, the proposed algorithm generates several inference rules between two relation templates, which are then evaluated manually.", "labels": [], "entities": []}, {"text": "Some studies evaluate the rules out of context (is the rule \"X eliminates Y \"\u2192\"X treats Y \" true?), while others apply them to textual data and evaluate the validity of the rule in context (given \"aspirin eliminates headaches\", is \"aspirin treats headaches\" true?).", "labels": [], "entities": []}, {"text": "Not only are these post-hoc evaluations oblivious to recall, their \"human in the loop\" approach makes them expensive and virtually impossible to accurately replicate.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.988102376461029}]}, {"text": "Hence, there is areal need for pre-annotated datasets for intrinsic evaluation of relation inference in context.", "labels": [], "entities": []}, {"text": "constructed such a dataset by applying DIRT-trained inference rules to sampled texts, and then crowd-annotating whether each original text (premise) entails the text generated from applying the inference rule (hypothesis).", "labels": [], "entities": []}, {"text": "However, this process is biased; by using DIRT to generate examples, the dataset is inherently blind to the many cases where relation inference exists, but is not captured by DIRT.", "labels": [], "entities": []}, {"text": "We present anew dataset for evaluating relation inference in context, which is unbiased towards one method or another, and natural to annotate.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7998623251914978}]}, {"text": "To create this dataset, we design a QA setting where annotators are presented with a single ques- tion and several automatically-retrieved text fragments.", "labels": [], "entities": []}, {"text": "The annotators' goal is to mark which of the text fragments provide a potential answer to the question (see.", "labels": [], "entities": []}, {"text": "Since the entities in the text fragments are aligned with those in the question, this process implicitly annotates which relations entail the one in the question.", "labels": [], "entities": []}, {"text": "For example, in, if \"[US PRESIDENT] increased taxes\" provides an answer to \"Which US president raised taxes?\", then \"increased\" implies \"raised\" in that context.", "labels": [], "entities": []}, {"text": "Because this task is so easy to annotate, we were able to scale up to 16,371 annotated examples (3,147 positive) with 91.3% precision for only $375 via crowdsourcing.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9988679885864258}]}, {"text": "Finally, we evaluate a collection of existing methods and common practices on our dataset, and observe that even the best combination of methods cannot recall more than 25% of the positive examples without dipping below 80% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 224, "end_pos": 233, "type": "METRIC", "confidence": 0.9950311183929443}]}, {"text": "This places into perspective the huge amount of relevant cases of relation inference inherently ignored by the bias in (.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7925056219100952}]}, {"text": "Moreover, this result shows that while our annotation task is easy for humans, it is difficult for existing algorithms, making it an appealing challenge for future research on relation inference.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.9319145679473877}]}, {"text": "Our code 1 and data 2 are publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "To the best of our knowledge, there are only three pre-annotated datasets for evaluating relation inference in context.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8229302763938904}]}, {"text": "Each example in these datasets consists of two binary relations, premise and hypothesis, and a label indicat-ing whether the hypothesis is inferred from the premise.", "labels": [], "entities": []}, {"text": "These relations are essentially Open IE () assertions, and can be represented as (subject, relation, object) tuples.", "labels": [], "entities": []}, {"text": "annotated inference between typed relations (\" eliminates\"\u2192\" treats [SYMP-TOM]\"), restricting the definition of \"context\".", "labels": [], "entities": []}, {"text": "They also used the non-standard type-system from (, which limits the dataset's applicability to other corpora.", "labels": [], "entities": []}, {"text": "annotated inference between instantiated relations sharing at least one argument (\"aspirin eliminates headaches\"\u2192\"drugs treat headaches\").", "labels": [], "entities": []}, {"text": "While this format captures a more natural notion of context, it also conflates the task of relation inference with that of entity inference (\"aspirin\"\u2192\"drug\").", "labels": [], "entities": []}, {"text": "Both datasets were annotated by experts.", "labels": [], "entities": []}, {"text": "Zeichner et al. annotated inference between instantiated relations sharing both arguments: aspirin eliminates headaches \u2192 aspirin treats headaches aspirin eliminates headaches aspirin murders headaches This format provides abroad definition of context on one hand, while isolating the task of relation inference.", "labels": [], "entities": [{"text": "relation inference", "start_pos": 293, "end_pos": 311, "type": "TASK", "confidence": 0.7650002539157867}]}, {"text": "In addition, methods that can be evaluated on this type of data, can also be directly embedded into downstream applications, motivating subsequent work to use it as a benchmark.", "labels": [], "entities": []}, {"text": "We therefore create our own dataset in this format.", "labels": [], "entities": []}, {"text": "The main drawback of Zeichner et al.'s process is that it is biased towards a specific relation inference method, DIRT).", "labels": [], "entities": []}, {"text": "Essentially, Zeichner et al. conducted a post-hoc evaluation of DIRT and recorded the results.", "labels": [], "entities": [{"text": "DIRT", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7631971836090088}]}, {"text": "While their approach does not suffer from the major disadvantages of post-hoc evaluation -cost and irreplicability -it ignores instances that do not behave according to DIRT's assumptions.", "labels": [], "entities": []}, {"text": "These invisible examples amount to an enormous chunk of the inference performed when answering questions, which are covered by our approach (see \u00a74).", "labels": [], "entities": []}, {"text": "ner; (2) to allow for cheap, consistent, and scalable annotations based on an intuitive QA setting.", "labels": [], "entities": []}], "tableCaptions": []}