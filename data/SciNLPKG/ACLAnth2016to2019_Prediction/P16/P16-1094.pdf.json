{"title": [], "abstractContent": [{"text": "We present persona-based models for handling the issue of speaker consistency in neural response generation.", "labels": [], "entities": [{"text": "neural response generation", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.742160697778066}]}, {"text": "A speaker model encodes personas in distributed em-beddings that capture individual characteristics such as background information and speaking style.", "labels": [], "entities": []}, {"text": "A dyadic speaker-addressee model captures properties of interactions between two interlocutors.", "labels": [], "entities": []}, {"text": "Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9993717074394226}, {"text": "consistency", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.8800533413887024}]}], "introductionContent": [{"text": "As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (.", "labels": [], "entities": []}, {"text": "One major issue for these data-driven systems is their propensity to select the response with greatest likelihood-in effect a consensus response of the humans represented in the training data.", "labels": [], "entities": []}, {"text": "Outputs are frequently vague or non-committal (, and when not, they can be wildly inconsistent, as illustrated in.", "labels": [], "entities": []}, {"text": "In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent \"persona\" needed to model humanlike behavior, whether as personal assistants, per-* The entirety of this work was conducted at Microsoft.", "labels": [], "entities": []}, {"text": "sonalized avatar-like agents, or game characters.", "labels": [], "entities": []}, {"text": "For present purposes, we will define PERSONA as the character that an artificial agent, as actor, plays or performs during conversational interactions.", "labels": [], "entities": [{"text": "PERSONA", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9055803418159485}]}, {"text": "A persona can be viewed as a composite of elements of identity (background facts or user profile), language behavior, and interaction style.", "labels": [], "entities": []}, {"text": "A persona is also adaptive, since an agent may need to present different facets to different human interlocutors depending on the interaction.", "labels": [], "entities": []}, {"text": "Fortunately, neural models of conversation generation () provide a straightforward mechanism for incorporating personas as embeddings.", "labels": [], "entities": [{"text": "conversation generation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.805708110332489}]}, {"text": "We therefore explore two per-sona models, a single-speaker SPEAKER MODEL and a dyadic SPEAKER-ADDRESSEE MODEL, within a sequence-to-sequence (SEQ2SEQ) framework ().", "labels": [], "entities": []}, {"text": "The Speaker Model integrates a speaker-level vector representation into the target part of the SEQ2SEQ model.", "labels": [], "entities": []}, {"text": "Analogously, the Speaker-Addressee model encodes the interaction patterns of two interlocutors by constructing an interaction representation from their individual embeddings and incorporating it into the SEQ2SEQ model.", "labels": [], "entities": []}, {"text": "These persona vectors are trained on human-human conversation data and used attest time to generate personalized responses.", "labels": [], "entities": []}, {"text": "Our experiments on an open-domain corpus of Twitter conversations and dialog datasets comprising TV series scripts show that leveraging persona vectors can improve relative performance up to 20% in BLEU score and 12% in perplexity, with a commensurate gain in consistency as judged by human annotators.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 198, "end_pos": 208, "type": "METRIC", "confidence": 0.9853029251098633}, {"text": "consistency", "start_pos": 260, "end_pos": 271, "type": "METRIC", "confidence": 0.9948015213012695}]}], "datasetContent": [{"text": "Data Collection Training data for the Speaker Model was extracted from the Twitter FireHose for the six-month period beginning January 1, 2012.", "labels": [], "entities": [{"text": "Twitter FireHose", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.8785826563835144}]}, {"text": "We limited the sequences to those where the responders had engaged in at least 60 (and at most 300) 3-turn conversational interactions during the period, in other words, users who reasonably frequently engaged in conversation.", "labels": [], "entities": []}, {"text": "This yielded a set of 74,003 users who took part in a minimum of 60 and a maximum of 164 conversational turns (average: 92.24, median: 90).", "labels": [], "entities": []}, {"text": "The dataset extracted using responses by these \"conversationalists\" contained 24,725,711 3-turn sliding-window (context-message-response) conversational sequences.", "labels": [], "entities": []}, {"text": "In addition, we sampled 12000 3-turn conversations from the same user set from the Twitter FireHose for the three-month period beginning July 1, 2012, and set these aside as development, validation, and test sets (4000 conversational sequences each).", "labels": [], "entities": []}, {"text": "Note that development, validation, and test sets for this data are single-reference, which is by design.", "labels": [], "entities": []}, {"text": "Multiple reference responses would typically require acquiring responses from different people, which would confound different personas.", "labels": [], "entities": []}, {"text": "Training Protocols We trained four-layer SEQ2SEQ models on the Twitter corpus following the approach of ().", "labels": [], "entities": []}, {"text": "Details are as follows: \u2022 4 layer LSTM models with 1,000 hidden cells for each layer.", "labels": [], "entities": []}, {"text": "\u2022 Batch size is set to 128.", "labels": [], "entities": [{"text": "Batch size", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.48881183564662933}]}, {"text": "\u2022 Learning rate is set to 1.0.", "labels": [], "entities": [{"text": "Learning rate", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9559626579284668}]}, {"text": "\u2022 Gradients are clipped to avoid gradient explosion with a threshold of 5.", "labels": [], "entities": [{"text": "Gradients", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.991259753704071}]}, {"text": "\u2022 Vocabulary size is limited to 50,000.", "labels": [], "entities": [{"text": "Vocabulary size", "start_pos": 2, "end_pos": 17, "type": "METRIC", "confidence": 0.724895179271698}]}, {"text": "\u2022 Dropout rate is set to 0.2.", "labels": [], "entities": [{"text": "Dropout rate", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.8219371736049652}]}, {"text": "Source and target LSTMs use different sets of parameters.", "labels": [], "entities": []}, {"text": "We ran 14 epochs, and training took roughly a month to finish on a Tesla K40 GPU machine.", "labels": [], "entities": []}, {"text": "As only speaker IDs of responses were specified when compiling the Twitter dataset, experiments on this dataset were limited to the Speaker Model.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.7562515735626221}]}, {"text": "The Twitter Persona Dataset was collected for this paper for experiments with speaker ID information.", "labels": [], "entities": [{"text": "Twitter Persona Dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.761257270971934}, {"text": "speaker ID information", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.800043006738027}]}, {"text": "To obtain a point of comparison with prior state-of-the-art work (, we measure our baseline (non-persona) LSTM model against prior work on the dataset of ( , which we call the Twitter Sordoni Dataset.", "labels": [], "entities": [{"text": "Twitter Sordoni Dataset", "start_pos": 176, "end_pos": 199, "type": "DATASET", "confidence": 0.7934592366218567}]}, {"text": "We only use its test-set portion, which contains responses for 2114 context and messages.", "labels": [], "entities": []}, {"text": "It is important to note that the Sordoni dataset offers up to 10 references per message, while the Twitter Persona dataset has only 1 reference per message.", "labels": [], "entities": [{"text": "Sordoni dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.973115473985672}, {"text": "Twitter Persona dataset", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.9329575300216675}]}, {"text": "Thus BLEU scores cannot be compared across the two Twitter datasets (BLEU scores on 10 references are generally much higher than with 1 reference).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9975225329399109}, {"text": "Twitter datasets", "start_pos": 51, "end_pos": 67, "type": "DATASET", "confidence": 0.8330217897891998}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9981575608253479}]}, {"text": "Details of this dataset are in ( ).", "labels": [], "entities": []}, {"text": "Following ( we used BLEU () for parameter tuning and evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.998496413230896}]}, {"text": "BLEU has been shown to correlate well with human judgment on the response generation task, as demonstrated in ( . Besides BLEU scores, we also report perplexity as an indicator of model capability.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9908278584480286}, {"text": "response generation task", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.8022778034210205}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9978057742118835}]}], "tableCaptions": [{"text": " Table 2: BLEU on the Twitter Sordoni dataset (10 references).  We contrast our baseline against an SMT baseline (", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992552399635315}, {"text": "Twitter Sordoni dataset", "start_pos": 22, "end_pos": 45, "type": "DATASET", "confidence": 0.8994810382525126}]}]}