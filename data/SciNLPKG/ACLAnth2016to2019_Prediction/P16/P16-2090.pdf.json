{"title": [{"text": "Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection", "labels": [], "entities": [{"text": "Reinflection", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.6544274091720581}]}], "abstractContent": [{"text": "Morphological reinflection is the task of generating a target form given a source form, a source tag and a target tag.", "labels": [], "entities": [{"text": "Morphological reinflection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8541373610496521}]}, {"text": "We propose anew way of modeling this task with neural encoder-decoder models.", "labels": [], "entities": []}, {"text": "Our approach reduces the amount of required training data for this architecture and achieves state-of-the-art results, making encoder-decoder models applicable to morphological reinflection even for low-resource languages.", "labels": [], "entities": []}, {"text": "We further present anew automatic correction method for the outputs based on edit trees.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological analysis and generation of previously unseen word forms is a fundamental problem in many areas of natural language processing (NLP).", "labels": [], "entities": [{"text": "Morphological analysis and generation of previously unseen word forms", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.7918882899814181}, {"text": "natural language processing (NLP)", "start_pos": 112, "end_pos": 145, "type": "TASK", "confidence": 0.8193245430787405}]}, {"text": "Its accuracy is crucial for the success of downstream tasks like machine translation and question answering.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995067119598389}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8215216994285583}, {"text": "question answering", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8959928452968597}]}, {"text": "Accordingly, learning morphological inflection patterns from labeled data is an important challenge.", "labels": [], "entities": []}, {"text": "The task of morphological reinflection (MRI) consists of producing an inflected form fora given source form, source tag and target tag.", "labels": [], "entities": [{"text": "morphological reinflection (MRI)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.8509975075721741}]}, {"text": "A special case is morphological inflection (MI), the task of finding an inflected form fora given lemma and target tag.", "labels": [], "entities": [{"text": "morphological inflection (MI)", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.6945477068424225}]}, {"text": "An English example is \"tree\"+PLURAL \u2192 \"trees\".", "labels": [], "entities": [{"text": "PLURAL", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9844248294830322}]}, {"text": "Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (.", "labels": [], "entities": [{"text": "MI", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9783505797386169}, {"text": "MRI", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.7558247447013855}]}, {"text": "In this work, we propose the neural encoderdecoder MED -Morphological Encoder-Decoder -a character-level sequence-to-sequence attention model that is a language-independent solution for MRI.", "labels": [], "entities": [{"text": "MRI", "start_pos": 186, "end_pos": 189, "type": "TASK", "confidence": 0.9619631171226501}]}, {"text": "In contrast to prior work, we train a single model that is trained on all source to target mappings of the language that are attested in the training set.", "labels": [], "entities": []}, {"text": "This radically reduces the amount of training data needed for the encoder-decoder because most MRI patterns occur in many source-target tag pairs.", "labels": [], "entities": []}, {"text": "In our model design, what is learned for one pair can be transferred to others.", "labels": [], "entities": []}, {"text": "The key enabler for this single-model approach is a novel representation we use for MRI.", "labels": [], "entities": [{"text": "MRI", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9378872513771057}]}, {"text": "We encode the input as a single sequence of (i) the morphological tags of the source form, (ii) the morphological tags of the target form and (iii) the sequence of letters of the source form.", "labels": [], "entities": []}, {"text": "The output is the sequence of letters of the target form.", "labels": [], "entities": []}, {"text": "As the decoder produces each letter, the attention mechanism can focus on the input letter sequence for parts of the output that simply copy the input.", "labels": [], "entities": []}, {"text": "For other parts of the output, e.g., an inflectional ending that is predicted using the target tags, the attention mechanism can focus on the target morphological tags.", "labels": [], "entities": []}, {"text": "In more complex cases, simultaneous attention can be paid to subsequences of all three input types -source tags, target tags and input letter sequence.", "labels": [], "entities": []}, {"text": "We can train a single generic encoder-decoder per language on this representation that can handle all tag pairs, thus making it possible to make efficient use of the available training data.", "labels": [], "entities": []}, {"text": "MED outperformed other systems on the SIGMORPHON16 shared task 1 for all ten languages that were covered (.", "labels": [], "entities": []}, {"text": "We also present POET -Prefer Observed Edit Trees -a new generic method for correcting the output of an MRI system.", "labels": [], "entities": [{"text": "POET -Prefer Observed Edit", "start_pos": 16, "end_pos": 42, "type": "METRIC", "confidence": 0.9081554174423218}]}, {"text": "The combination of MED and POET is state-of-the-art or close to it on a CELEX-based evaluation of MRI even though this evaluation makes it difficult to exploit gener-alizations across tag pairs.", "labels": [], "entities": [{"text": "MED", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9819694757461548}, {"text": "POET", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9945898056030273}]}], "datasetContent": [{"text": "We compare MED with the three models of as well as with two recently proposed models: (i) discriminative string transduction, the SIGMORPHON16 baseline, and (ii)'s encoder-decoder model.", "labels": [], "entities": [{"text": "MED", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8102370500564575}, {"text": "SIGMORPHON16 baseline", "start_pos": 130, "end_pos": 151, "type": "DATASET", "confidence": 0.7019613683223724}]}, {"text": "We call the latter MODEL*TAG as it requires training as many models as there are target tags.", "labels": [], "entities": [{"text": "MODEL*TAG", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.7896925012270609}]}, {"text": "We evaluate MED on two MRI tasks: CELEX and SIGMORPHON16.", "labels": [], "entities": [{"text": "MED", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7933213114738464}, {"text": "CELEX", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9535073637962341}, {"text": "SIGMORPHON16", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.6663572788238525}]}, {"text": "This task is based on complete inflection tables for German extracted from CELEX.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.7974796891212463}]}, {"text": "For this experiment we follow.", "labels": [], "entities": []}, {"text": "We use four pairs of morphological tags and corresponding word forms from the German part of the CELEX morphological database.", "labels": [], "entities": [{"text": "CELEX morphological database", "start_pos": 97, "end_pos": 125, "type": "DATASET", "confidence": 0.9326453606287638}]}, {"text": "The 4 different transduction tasks are: 13SIA \u2192 13SKE, 2PIE \u2192 13PKE, 2PKE \u2192 z and rP \u2192 pA.", "labels": [], "entities": []}, {"text": "An example for this task would be to produce the output gesteuert (target tag pA) for the source steuert (source tag rP).", "labels": [], "entities": []}, {"text": "To do so, the system has to learn that the prefix ge-, which is used for many participles in German, has to be added to the beginning of the original word form.", "labels": [], "entities": []}, {"text": "We use the same data splits as, dividing the original 2500 samples for each tag into five folds, each consisting of 500 training and 1000 development and 1000 test samples.", "labels": [], "entities": []}, {"text": "We train a separate model for each fold and report exact match accuracy, averaged over the five folds, as our final result.", "labels": [], "entities": [{"text": "exact match", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.794209897518158}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.5520951747894287}]}, {"text": "This task covers eight languages and does not provide complete paradigms, but only a set of quadruples, each consisting of word form, source tag, target tag and target form.", "labels": [], "entities": []}, {"text": "The main difference to CELEX is that the number of tag pairs is large, resulting in much less training data per tag pair.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8878595232963562}]}, {"text": "The number of tag pairs varies by language with Georgian being an extreme case; it has 28 tag pairs in dev that appear less than 10 times in train.", "labels": [], "entities": []}, {"text": "For each language, we have around 12,800 training and 1600 development samples.", "labels": [], "entities": []}, {"text": "We report exact match accuracy on the development set, as the final test data of the shared task is not publically available yet.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8550640940666199}]}, {"text": "MED+POET is better than prior work on one task, close in performance on two and worse by a small amount on the third.", "labels": [], "entities": [{"text": "MED+", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.926798015832901}, {"text": "POET", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6559718251228333}]}, {"text": "Unlike's models, MED does not use any hand-crafted features.", "labels": [], "entities": [{"text": "MED", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.6162328124046326}]}, {"text": "MED's results are weakest on 13SIA.", "labels": [], "entities": [{"text": "MED", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.56299889087677}, {"text": "13SIA", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.5952001214027405}]}, {"text": "Typical errors on this task include epenthesis (e.g., zirkle vs. zirkele) and irregular verbs (e.g., abhing vs. abh\u00e4ngte).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Exact match accuracy of MRI on CELEX. Re- sults of (Dreyer et al., 2008)'s model are from their pa- per; backoff: ngrams+x model; lat-class: ngrams+x+latent  class model; lat-region: ngrams+x+latent class+latent re- gion model; baseline: SIGMORPHON16 baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9198718667030334}, {"text": "CELEX", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9576342701911926}]}, {"text": " Table 2:  Exact match accuracy of MRI on SIG- MORPHON16; baseline: SIGMORPHON16 baseline;  MED/average: average of five MED models (standard devia- tion in parentheses); MED/ensemble: majority voting of five  MED models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8418275117874146}, {"text": "MED/average", "start_pos": 92, "end_pos": 103, "type": "METRIC", "confidence": 0.9257296919822693}]}]}