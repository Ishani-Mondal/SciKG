{"title": [{"text": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Understanding unstructured text is a major goal within natural language processing.", "labels": [], "entities": []}, {"text": "Comprehension tests pose questions based on short text passages to evaluate such understanding.", "labels": [], "entities": []}, {"text": "In this work, we investigate machine comprehension on the challenging MCTest benchmark.", "labels": [], "entities": [{"text": "MCTest benchmark", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.7989752888679504}]}, {"text": "Partly because of its limited size, prior work on MCTest has focused mainly on engineering better features.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8571010231971741}]}, {"text": "We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy.", "labels": [], "entities": []}, {"text": "The parallel hierarchy enables our model to compare the passage , question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set.", "labels": [], "entities": []}, {"text": "Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text.", "labels": [], "entities": []}, {"text": "When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets anew state of the art for MCTest, outper-forming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15 percentage points).", "labels": [], "entities": []}], "introductionContent": [{"text": "Humans learn in a variety of ways-by communication with each other and by study, the reading of text.", "labels": [], "entities": []}, {"text": "Comprehension of unstructured text by machines, at a near-human level, is a major goal for natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6587392290433248}]}, {"text": "It has garnered * A.", "labels": [], "entities": [{"text": "*", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9576331973075867}, {"text": "A", "start_pos": 18, "end_pos": 19, "type": "METRIC", "confidence": 0.9195813536643982}]}, {"text": "Trischler and Z. Ye contributed equally to this work.", "labels": [], "entities": []}, {"text": "significant attention from the machine learning research community in recent years.", "labels": [], "entities": []}, {"text": "Machine comprehension (MC) is evaluated by posing a set of questions based on a text passage (akin to the reading tests we all took in school).", "labels": [], "entities": [{"text": "Machine comprehension (MC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8037380933761596}]}, {"text": "Such tests are objectively gradable and can be used to assess a range of abilities, from basic understanding to causal reasoning to inference (.", "labels": [], "entities": []}, {"text": "Given a text passage and a question about its content, a system is tested on its ability to determine the correct answer (.", "labels": [], "entities": []}, {"text": "In this work, we focus on MCTest, a complex but data-limited comprehension benchmark, whose multiple-choice questions require not only extraction but also inference and limited reasoning (.", "labels": [], "entities": []}, {"text": "Inference and reasoning are important human skills that apply broadly, beyond language.", "labels": [], "entities": [{"text": "Inference and reasoning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6888005435466766}]}, {"text": "We present a parallel-hierarchical approach to machine comprehension designed to work well in a data-limited setting.", "labels": [], "entities": []}, {"text": "There are many use-cases in which comprehension over limited data would be handy: for example, user manuals, internal documentation, legal contracts, and soon.", "labels": [], "entities": []}, {"text": "Moreover, work towards more efficient learning from any quantity of data is important in its own right, for bringing machines more inline with the way humans learn.", "labels": [], "entities": []}, {"text": "Typically, artificial neural networks require numerous parameters to capture complex patterns, and the more parameters, the more training data is required to tune them.", "labels": [], "entities": []}, {"text": "Likewise, deep models learn to extract their own features, but this is a data-intensive process.", "labels": [], "entities": []}, {"text": "Our model learns to comprehend at a high level even when data is sparse.", "labels": [], "entities": []}, {"text": "The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives.", "labels": [], "entities": []}, {"text": "We refer to a question combined with one of its answer candidates as a hypothesis (to be detailed below).", "labels": [], "entities": []}, {"text": "The seman-tic perspective compares the hypothesis to sentences in the text viewed as single, self-contained thoughts; these are represented using a sum and transformation of word embedding vectors, similarly to.", "labels": [], "entities": []}, {"text": "The word-by-word perspective focuses on similarity matches between individual words from hypothesis and text, at various scales.", "labels": [], "entities": []}, {"text": "As in the semantic perspective, we consider matches over complete sentences.", "labels": [], "entities": []}, {"text": "We also use a sliding window acting on a subsentential scale (inspired by the work of), which implicitly considers the linear distance between matched words.", "labels": [], "entities": []}, {"text": "Finally, this word-level sliding window operates on two different views of story sentences: the sequential view, where words appear in their natural order, and the dependency view, where words are reordered based on a linearization of the sentence's dependency graph.", "labels": [], "entities": []}, {"text": "Words are represented throughout by embedding vectors ().", "labels": [], "entities": []}, {"text": "These distinct perspectives naturally form a hierarchy that we depict in.", "labels": [], "entities": []}, {"text": "Language is hierarchical, so it makes sense that comprehension relies on hierarchical levels of understanding.", "labels": [], "entities": []}, {"text": "The perspectives of our model can be considered a type of feature.", "labels": [], "entities": []}, {"text": "However, they are implemented by parametric differentiable functions.", "labels": [], "entities": []}, {"text": "This is in contrast to most previous efforts on MCTest, whose numerous hand-engineered features cannot be trained.", "labels": [], "entities": []}, {"text": "Our model, significantly, can be trained end-to-end with backpropagation.", "labels": [], "entities": []}, {"text": "To facilitate learning with limited data, we also develop a unique training scheme.", "labels": [], "entities": []}, {"text": "We initialize the model's neural networks to perform specific heuristic functions that yield decent (though not impressive) performance on the dataset.", "labels": [], "entities": []}, {"text": "Thus, the training scheme gives the model a safe, reasonable baseline from which to start learning.", "labels": [], "entities": []}, {"text": "We call this technique training wheels.", "labels": [], "entities": []}, {"text": "Computational models that comprehend (insofar as they perform well on MC datasets) have been developed contemporaneously in several research groups.", "labels": [], "entities": []}, {"text": "Models designed specifically for MCTest include those of, and more recently, , and.", "labels": [], "entities": []}, {"text": "In experiments, our Parallel-Hierarchical model achieves state-of-the-art accuracy on MCTest, outperforming these existing methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.999319314956665}, {"text": "MCTest", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.8130882382392883}]}, {"text": "Below we describe related work, the mathematical details of our model, and our experiments, then analyze our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "MCTest is a collection of 660 elementary-level children's stories and associated questions, written by human subjects.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9076075553894043}]}, {"text": "The stories are fictional, ensuring that the answer must be found in the text itself, and carefully limited to what a young child can understand (.", "labels": [], "entities": []}, {"text": "The more challenging variant consists of 500 stories with four multiple-choice questions each.", "labels": [], "entities": []}, {"text": "Despite the elementary level, stories and questions are more natural and more complex than those found in synthetic MC datasets like bAbI () and CNN (.", "labels": [], "entities": []}, {"text": "MCTest is challenging because it is both complicated and small.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8401283025741577}]}, {"text": "As per, \"it is very difficult to train statistical models only on MCTest.\"", "labels": [], "entities": [{"text": "MCTest", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9382758140563965}]}, {"text": "Its size limits the number of parameters that can be trained, and prevents learning any complex language modeling simultaneously with the capacity to answer questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results on MCTest.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.6123382449150085}]}, {"text": " Table 2: Ablation study on MCTest-500 (all).", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9925124645233154}, {"text": "MCTest-500", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9391744136810303}]}]}