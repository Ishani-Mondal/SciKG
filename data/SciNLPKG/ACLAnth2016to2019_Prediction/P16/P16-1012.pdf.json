{"title": [{"text": "Language Transfer Learning for Supervised Lexical Substitution", "labels": [], "entities": [{"text": "Language Transfer", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6543221026659012}, {"text": "Supervised Lexical Substitution", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.734364648660024}]}], "abstractContent": [{"text": "We propose a framework for lexical substitution that is able to perform transfer learning across languages.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7344318330287933}]}, {"text": "Datasets for this task are available in at least three languages (English, Italian, and German).", "labels": [], "entities": []}, {"text": "Previous work has addressed each of these tasks in isolation.", "labels": [], "entities": []}, {"text": "In contrast, we regard the union of three shared tasks as a combined multilingual dataset.", "labels": [], "entities": []}, {"text": "We show that a supervised system can be trained effectively , even if training and evaluation data are from different languages.", "labels": [], "entities": []}, {"text": "Successful transfer learning between languages suggests that the learned model is in fact independent of the underlying language.", "labels": [], "entities": []}, {"text": "We combine state-of-the-art unsupervised features obtained from syntactic word em-beddings and distributional thesauri in a supervised delexicalized ranking system.", "labels": [], "entities": []}, {"text": "Our system improves overstate of the art in the full lexical substitution task in all three languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The lexical substitution task is defined as replacing a target word in a sentence context with a synonym, which does not alter the meaning of the utterance.", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7965704202651978}]}, {"text": "Although this appears easy to humans, automatically performing such a substitution is challenging, as it implicitly addresses the problem of both determining semantically similar substitutes, as well as resolving the ambiguity of polysemous words.", "labels": [], "entities": []}, {"text": "In fact, lexical substitution was originally conceived as an extrinsic evaluation of Word Sense Disambiguation (WSD) when first proposed by.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7036639153957367}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.7325036724408468}]}, {"text": "However, a system capable of replacing words by appropriate meaning-preserving substitutes can be utilized in downstream tasks that require paraphrasing of input text.", "labels": [], "entities": []}, {"text": "Examples of such use cases include text simplification, text shortening, and summarization.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8044052124023438}, {"text": "text shortening", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8059224486351013}, {"text": "summarization", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9887950420379639}]}, {"text": "Furthermore, lexical substitution can be regarded as an alternative to WSD in downstream tasks requiring word disambiguation.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7205697298049927}, {"text": "WSD", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8341288566589355}, {"text": "word disambiguation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.6991369426250458}]}, {"text": "For example, it was successfully applied in Semantic Textual Similarity (.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8091964522997538}]}, {"text": "A given list of substitution words can be regarded as a vector representation modeling the meaning of a word in context.", "labels": [], "entities": []}, {"text": "As opposed to WSD systems, this is not reliant on a predefined sense inventory, and therefore does not have to deal with issues of coverage, or sense granularity.", "labels": [], "entities": [{"text": "coverage", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9820605516433716}]}, {"text": "On the other hand, performing lexical substitution is more complex than WSD, as a system has to both generate and rank a list of substitution candidates per instance.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7200039625167847}, {"text": "WSD", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.6800174117088318}]}, {"text": "Over the last decade, a number of shared tasks in lexical substitution has been organized and a wide range of methods have been proposed.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7191364169120789}]}, {"text": "Although many approaches are in fact languageindependent, most existing work is tailored to a single language and dataset.", "labels": [], "entities": []}, {"text": "In this work, we investigate lexical substitution as a multilingual task, and report experimental results for English, German and Italian datasets.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7485223114490509}]}, {"text": "We consider a supervised approach to lexical substitution, which casts the task as a ranking problem ().", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8001232147216797}]}, {"text": "We adapt state-of-the-art unsupervised features () in a delexicalized ranking framework and perform transfer learning experiments by training a ranker model from a different language.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate the utility of aggregating data from different languages and train our model on this single multilingual dataset.", "labels": [], "entities": []}, {"text": "We are able to improve the state of the art for the full task on all datasets.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we elaborate on the lexical sub-stitution task and datasets.", "labels": [], "entities": []}, {"text": "Section 3 shows related work of systems addressing each of these tasks.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our method for building a supervised system capable of transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9115102589130402}]}, {"text": "Section 5 shows our experimental results and discussion.", "labels": [], "entities": []}, {"text": "Finally in Section 6 we give a conclusion and outlook to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The lexical substitution task was first defined at.", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7331763108571371}]}, {"text": "A lexical sample of target word is selected from different word classes (nouns, verbs, and adjectives).", "labels": [], "entities": []}, {"text": "Through annotation, a set of valid substitutes was collected for 10-20 contexts per target.", "labels": [], "entities": []}, {"text": "Whereas in the original SE07 task, annotators were free to provide \"up to three, but all equally good\" substitutes, later tasks dropped this restriction.", "labels": [], "entities": []}, {"text": "Substitutes were subsequently aggregated by annotator frequency, creating a ranking of substitutes.", "labels": [], "entities": []}, {"text": "The use of SE07 has become a de-facto standard for system comparison, however equivalent datasets have been produced for other languages.", "labels": [], "entities": [{"text": "SE07", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.8434418439865112}]}, {"text": "Evalita 2009 posed a lexical substitution task for Italian (Toral, 2009, \"EL09\").", "labels": [], "entities": [{"text": "Toral, 2009, \"EL09\")", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.7111767743315015}]}, {"text": "Participants were free to obtain a list of substitution candidates in anyway, most commonly Italian WordNet 1 was used.", "labels": [], "entities": [{"text": "Italian WordNet 1", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.819523831208547}]}, {"text": "A WeightedSense baseline provided by the organizers proved very strong, as all systems scored below it.", "labels": [], "entities": []}, {"text": "This baseline is obtained by aggregating differently weighted semantic relations from multiple human-created lexical resources (.", "labels": [], "entities": []}, {"text": "A German version of the lexical substitution task was organized at, \"GE15\").", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.7691608468691508}]}, {"text": "Likewise, WeightedSense was able to beat both of two participating systems in oot evaluations (.", "labels": [], "entities": []}, {"text": "A variation for cross-lingual lexical substitution was proposed by, in which substitute words are required in a different language than the source sentence.", "labels": [], "entities": [{"text": "cross-lingual lexical substitution", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.7729063232739767}]}, {"text": "The sentence context as well as the target word were given in English, whereas the substitute words should be provided in Spanish (annotators were fluent in both languages).", "labels": [], "entities": []}, {"text": "This variant is motivated by direct application in Machine Translation systems, or as an aid for human-based translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8382214307785034}, {"text": "human-based translation", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.7444596588611603}]}, {"text": "There also ex-ists a larger crowd-sourced dataset of 1012 nouns, as well as an all-words dataset in which all words in each sentence are annotated with lexical expansions ().", "labels": [], "entities": []}, {"text": "Evaluation of lexical substitution adheres to metrics defined by SE07, who provide two evaluation settings 2 ; best evaluating only a system's \"best guess\" of a single target substitute and oot, an unordered evaluation of up to ten substitutes.", "labels": [], "entities": [{"text": "Evaluation of lexical substitution", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6381291747093201}, {"text": "SE07", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.8973029255867004}]}, {"text": "proposed to use Generalized Average Precision (GAP), to compare an output ranking rather than unordered sets of substitutes.", "labels": [], "entities": [{"text": "Generalized Average Precision (GAP)", "start_pos": 16, "end_pos": 51, "type": "METRIC", "confidence": 0.9093668957551321}]}, {"text": "Dataset comparison The proposed lexical substitution datasets (SE07, EL09, GE15) differ in their degree of ambiguity of target items.", "labels": [], "entities": []}, {"text": "If a dataset contains mostly target words that are unambiguous, substitution lists of different instances of the same target are similar, despite occurring in different context.", "labels": [], "entities": []}, {"text": "We can quantify this degree of variation by measuring the overlap of gold substitutes of each target across all contexts.", "labels": [], "entities": [{"text": "overlap", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9589666724205017}]}, {"text": "For this, we adapt the pairwise agreement (PA) metric defined by.", "labels": [], "entities": [{"text": "pairwise agreement (PA) metric", "start_pos": 23, "end_pos": 53, "type": "METRIC", "confidence": 0.7698485602935156}]}, {"text": "Instead of inter-annotator agreement we measure agreement across different context instances.", "labels": [], "entities": []}, {"text": "Let T be a set of lexical target words, and D dataset of instances (t i , Si ) \u2208 D, in which target ti \u2208 T is annotated with a set of substitutes Si . Then we regard for each target word t the substitute sets St \u2282 D fort.", "labels": [], "entities": []}, {"text": "We define a substitute agreement as SA (t) as the mean pairwise dice coefficient between all s 1 , s 2 \u2208 St where s 1 = s 2 . For each dataset D we list the substitute variance SV = 1 \u2212 1 |T | \u2211 t\u2208T SA (t).", "labels": [], "entities": []}, {"text": "shows this metric for the three datasets, as well as for subsets of the dataset according to target part of speech.", "labels": [], "entities": []}, {"text": "It can be seen that the variance in gold substitutes differs substantially between datasets, but not much between target word type within a dataset.", "labels": [], "entities": []}, {"text": "EL09 has the highest degree of variance, suggesting that targets tend to be more ambiguous, whereas GE15 has the lowest degree of variance, suggesting less ambiguity., and probabilistic graphical models.", "labels": [], "entities": [{"text": "EL09", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8233445286750793}]}, {"text": "The current state of the art combines a distributional model with the use of n-gram language models ().", "labels": [], "entities": []}, {"text": "They define the context vector of each word in a background corpus as a substitute vector, which is a vector of suitable filler words for the current n-gram context.", "labels": [], "entities": []}, {"text": "They then obtain a contextualized paraphrase vector by computing a weighted average of substitute vectors in the background corpus, based on their similarity to the current target instance.", "labels": [], "entities": []}, {"text": "In contrast to traditional sparse vector representations obtained through distributional methods, a recent trend is the use of low-dimensional dense vector representations.", "labels": [], "entities": []}, {"text": "The use of such vector representations or word embeddings has been popularized by the continuous bag-of-words (CBOW) and Skip-gram model ().", "labels": [], "entities": []}, {"text": "show a simple and knowledgelean model for lexical substitution based solely on syntactic word embeddings.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.7171605229377747}]}, {"text": "As we leverage this model as a feature in our approach, we will elaborate on this in Section 4.", "labels": [], "entities": []}, {"text": "Another approach for applying word embeddings to lexical substitution is their direct extension with multiple word senses, which can be weighted according to target context ().", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7628890872001648}]}, {"text": "Biemann (2013) first showed that the lexical substitution task can be solved very well when sufficient amount of training data is collected per target.", "labels": [], "entities": [{"text": "lexical substitution task", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7890638113021851}]}, {"text": "An approach based on crowdsourcing human judgments achieved the best performance on the S07 dataset today.", "labels": [], "entities": [{"text": "S07 dataset", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.825708270072937}]}, {"text": "However, judgments had to be collected for each lexical item, and as a consequence the approach cannot scale to an open vocabulary.", "labels": [], "entities": []}, {"text": "As an alternative to per-word supervised systems trained on target instances per lexeme, allwords systems aim to generalize overall lexical items.", "labels": [], "entities": []}, {"text": "proposed such a system by using delexicalization: features are generalized in such away that they are independent of lexical items, and thus generalize beyond the training set and across targets.", "labels": [], "entities": []}, {"text": "Originally, a maximum entropy classifier was trained on target-substitute instances and used for pointwise ranking of substitution candidates.", "labels": [], "entities": []}, {"text": "Ina follow-up work it was shown that learning-to-rank methods could drastically improve this approach, achieving state-ofthe-art performance with a LambdaMART ranker ().", "labels": [], "entities": []}, {"text": "In this work we will build upon this model and further generalize not only across lexical items but across different languages.", "labels": [], "entities": []}, {"text": "For both EL09 and GE15, existing approaches have been adapted.", "labels": [], "entities": [{"text": "EL09", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8993132710456848}, {"text": "GE15", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.677564799785614}]}, {"text": "For the Italian dataset, a distributional method was combined with LSA).", "labels": [], "entities": [{"text": "Italian dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8454887270927429}, {"text": "LSA", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.8884775638580322}]}, {"text": "The best performing system applied a WSD system and language models ().", "labels": [], "entities": [{"text": "WSD", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.7207874059677124}]}, {"text": "For the German dataset, Hintz and Biemann (2015) adapted the supervised approach by), achieving best performance for nouns and adjectives.", "labels": [], "entities": [{"text": "German dataset", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.885380744934082}]}, {"text": "Jackov (2015) used a deep semantic analysis framework employing an internal dependency relation knowledge base, which achieved the best performance for verbs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Degree of variation in gold answers", "labels": [], "entities": [{"text": "Degree", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9556511044502258}]}, {"text": " Table 3: Transfer learning results for the open can- didate task (candidates from lexical resources)", "labels": [], "entities": []}, {"text": " Table 4: Transfer learning results on the ranking- only task (candidates pooled from gold)", "labels": [], "entities": []}, {"text": " Table 5: Feature ablation results for the full and  ranking-only task (universal model trained on all  data)", "labels": [], "entities": []}, {"text": " Table 6: Experimental results of our method com- pared to related work for all three lexical substitu- tion tasks", "labels": [], "entities": []}]}