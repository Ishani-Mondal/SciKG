{"title": [{"text": "Deep multi-task learning with low level tasks supervised at lower layers", "labels": [], "entities": []}], "abstractContent": [{"text": "In all previous work on deep multi-task learning we are aware of, all task super-visions are on the same (outermost) layer.", "labels": [], "entities": []}, {"text": "We present a multi-task learning architecture with deep bi-directional RNNs, where different tasks supervision can happen at different layers.", "labels": [], "entities": []}, {"text": "We present experiments in syntactic chunking and CCG supertag-ging, coupled with the additional task of POS-tagging.", "labels": [], "entities": [{"text": "syntactic chunking", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7298055589199066}]}, {"text": "We show that it is consistently better to have POS supervision at the innermost rather than the outermost layer.", "labels": [], "entities": [{"text": "POS", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9477663636207581}]}, {"text": "We argue that this is because \"low-level\" tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks.", "labels": [], "entities": []}, {"text": "Finally, we also show how this architecture can be used for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7933202385902405}]}], "introductionContent": [{"text": "We experiment with a multi-task learning (MTL) architecture based on deep bi-directional recurrent neural networks (bi-RNNs)).", "labels": [], "entities": [{"text": "multi-task learning (MTL)", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.6656144857406616}]}, {"text": "MTL can be seen as away of regularizing model induction by sharing representations (hidden layers) with other inductions.", "labels": [], "entities": [{"text": "regularizing model induction", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.846901516119639}]}, {"text": "We use deep bi-RNNs with task supervision from multiple tasks, sharing one or more bi-RNNs layers among the tasks.", "labels": [], "entities": []}, {"text": "Our main contribution is the novel insight that (what has historically been thought of as) low-level tasks are better modeled in the low layers of such an architecture.", "labels": [], "entities": []}, {"text": "This is in contrast to previous work on deep MTL , in which supervision for all tasks happen at the same (outermost) layer.", "labels": [], "entities": [{"text": "MTL", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8215541243553162}]}, {"text": "Multiple-tasks supervision at the outermost layer has a strong tradition in neural net models in vision and elsewhere).", "labels": [], "entities": []}, {"text": "However, in NLP it is natural to think of some levels of analysis as feeding into others, typically with low-level tasks feeding into highlevel ones; e.g., POS tags as features for syntactic chunking) or parsing (.", "labels": [], "entities": []}, {"text": "Our architecture can be seen as a seamless way to combine multi-task and cascaded learning.", "labels": [], "entities": []}, {"text": "We also show how the proposed architecture can be applied to domain adaptation, in a scenario in which we have high-level task supervision in the source domain, and lower-level task supervision in the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7762225568294525}]}, {"text": "As a point of comparison, Collobert et al.", "labels": [], "entities": []}, {"text": "(2011) improved deep convolutional neural network models of syntactic chunking by also having task supervision from POS tagging at the outermost level.", "labels": [], "entities": [{"text": "syntactic chunking", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7078303396701813}, {"text": "POS tagging", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.6798462718725204}]}, {"text": "In our work, we use recurrent instead of convolutional networks, but our main contribution is observing that we obtain better performance by having POS task supervision at a lower layer.", "labels": [], "entities": []}, {"text": "(2011) also experiment with NER and SRL, they only obtain improvements from MTL with POS and syntactic chunking.", "labels": [], "entities": []}, {"text": "We show that similar gains can be obtained for CCG supertagging.", "labels": [], "entities": []}, {"text": "Our contributions (i) We present a MTL architecture for sequence tagging with deep bi-RNNs; (ii) We show that having task supervision from all tasks at the outermost level is often suboptimal; (iii) we show that this architecture can be used for domain adaptation.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.6877271682024002}, {"text": "domain adaptation", "start_pos": 246, "end_pos": 263, "type": "TASK", "confidence": 0.7896834015846252}]}], "datasetContent": [{"text": "We experiment with POS-tagging, syntactic chunking and CCG supertagging.", "labels": [], "entities": [{"text": "syntactic chunking", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.6747048497200012}]}, {"text": "See examples of the different tasks below: In-domain MTL In these experiments, POS, Chunking and CCG data are from the English Penn Treebank.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 119, "end_pos": 140, "type": "DATASET", "confidence": 0.8895165125528971}]}, {"text": "We use sections 0-18 for training POS and CCG supertagging, 15-18 for training chunking, 19 for development, 20 for evaluating chunking, and 23 for evaluating CCG supertagging.", "labels": [], "entities": [{"text": "chunking", "start_pos": 79, "end_pos": 87, "type": "TASK", "confidence": 0.7958094477653503}]}, {"text": "These splits were motivated by the need for comparability with previous results.", "labels": [], "entities": []}, {"text": "LAYERS DOMAINS CHUNKS POS BROADCAST (6) BC-NEWS (8) MAGAZINES   We do MTL training for either (POS+chunking) or (POS+CCG), with POS being the lower-level task.", "labels": [], "entities": [{"text": "LAYERS DOMAINS CHUNKS POS", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.5884344130754471}, {"text": "BROADCAST", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.5224105715751648}, {"text": "BC-NEWS", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.753386378288269}, {"text": "MAGAZINES", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.6818525791168213}, {"text": "MTL", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9582608938217163}]}, {"text": "We experiment three architectures: single task training for higher-level tasks (no POS layer), MTL with both tasks feeding off of the outer layer, and MTL where POS feeds off of the inner (1st) layer and the higher-level task on the outer Our CHUNKS results are competitive with stateof-the-art., for example, reported an F 1 -score of 95.15% on the CHUNKS data.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 322, "end_pos": 332, "type": "METRIC", "confidence": 0.9915343523025513}, {"text": "CHUNKS data", "start_pos": 350, "end_pos": 361, "type": "DATASET", "confidence": 0.8756206333637238}]}, {"text": "Our model also performs considerably better than the MTL model in.", "labels": [], "entities": []}, {"text": "Note that our relative improvements are also bigger than those reported by.", "labels": [], "entities": []}, {"text": "Our CCG super tagging results are also slighly better than a recently reported result in (93.00%).", "labels": [], "entities": [{"text": "CCG super tagging", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7452237208684286}]}, {"text": "Our results are significantly better (p < 0.05) than our baseline, and POS supervision at the lower layer is consistently better than standard MTL.", "labels": [], "entities": [{"text": "POS supervision", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9564019739627838}]}, {"text": "We also experimented with NER (CoNLL 2003), super senses (SemCor), and the Streusle Corpus of texts annotated with MWE brackets and super sense tags.", "labels": [], "entities": [{"text": "NER (CoNLL 2003)", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.6742950439453125}, {"text": "Streusle Corpus of texts", "start_pos": 75, "end_pos": 99, "type": "DATASET", "confidence": 0.9327265471220016}]}, {"text": "In none of these cases, MTL led to improvements.", "labels": [], "entities": [{"text": "MTL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9658607840538025}]}, {"text": "This suggests that MTL only works when tasks are sufficiently similar, e.g., all of syntactic nature.", "labels": [], "entities": [{"text": "MTL", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.986466109752655}]}, {"text": "(2011) also observed a drop in NER performance and insignificant improvements for SRL.", "labels": [], "entities": [{"text": "NER", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9603119492530823}, {"text": "SRL", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9113361835479736}]}, {"text": "We believe this is an important observation, since previous work on deep MTL often suggests that most tasks benefit from each other.", "labels": [], "entities": [{"text": "MTL", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.8741001486778259}]}, {"text": "Domain adaptation We experiment with domain adaptation for syntactic chunking, based on OntoNotes 4.0.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7378928661346436}, {"text": "domain adaptation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6768228858709335}, {"text": "syntactic chunking", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6417436599731445}]}, {"text": "We use WSJ newswire as our source domain, and broadcast, broadcasted news, magazines, and weblogs as target domains.", "labels": [], "entities": [{"text": "WSJ newswire", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.981124073266983}]}, {"text": "We assume main task (syntactic chunking) supervision for the source domain, and lower-level POS supervision for the target domains.", "labels": [], "entities": [{"text": "syntactic chunking) supervision", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.8105685263872147}]}, {"text": "The results in indicate that the method is effective for domain adaptation when we have POS supervision for the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8031416237354279}]}, {"text": "We believe this result is worth exploring further, as the scenario in which we have target-domain training data for low-level tasks such as POS tagging, but not for the task we are interested in, is common.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 140, "end_pos": 151, "type": "TASK", "confidence": 0.7684058845043182}]}, {"text": "The method is effective only when the lower-level POS supervision is applied at the lower layer, supporting the importance of supervising different tasks at different layers.", "labels": [], "entities": []}, {"text": "Rademacher complexity is the ability of models to fit random noise.", "labels": [], "entities": []}, {"text": "We use the procedure in to measure Rademacher complexity, i.e., computing the average fit to k random relabelings of the training data.", "labels": [], "entities": []}, {"text": "The subtask in our set-up acts like a regularizer, increasing the inductive bias of our model, preventing it from learning random patterns in data.", "labels": [], "entities": []}, {"text": "Rademacher complexity measures the decrease in ability to learn such patterns.", "labels": [], "entities": [{"text": "Rademacher complexity", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.7682080864906311}]}, {"text": "We use the CHUNKS data in these experiments.", "labels": [], "entities": [{"text": "CHUNKS data", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.7255564481019974}]}, {"text": "A model that does not fit to the random data, will be right in 1/22 cases (with 22 labels).", "labels": [], "entities": []}, {"text": "We report the Rademacher complexities relative to this.", "labels": [], "entities": []}, {"text": "1.298 1.034 0.990 Our deep single task model increases performance over this baseline by 30%.", "labels": [], "entities": []}, {"text": "In contrast, we see that when we predict both POS and the target task at the top layer, Rademacher complexity is lower and close to a random baseline.", "labels": [], "entities": [{"text": "Rademacher complexity", "start_pos": 88, "end_pos": 109, "type": "METRIC", "confidence": 0.7303701341152191}]}, {"text": "Interestingly, regularization seems to be even more effective, when the subtask is predicted from a lower layer.", "labels": [], "entities": [{"text": "regularization", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.9407908916473389}]}], "tableCaptions": [{"text": " Table 1: Domain adaptation results for chunking across four domains (averages over micro-F 1 s for  individual files). The number in brackets is # files per domain in OntoNotes 4.0. We use the two first  files in each folder for POS supervision (for train+dev).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7320755422115326}]}]}