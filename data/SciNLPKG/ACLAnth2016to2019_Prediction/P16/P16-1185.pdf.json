{"title": [{"text": "Semi-Supervised Learning for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7084113558133444}]}], "abstractContent": [{"text": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation.", "labels": [], "entities": [{"text": "end-to-end neural machine translation (NMT)", "start_pos": 6, "end_pos": 49, "type": "TASK", "confidence": 0.8054988809994289}]}, {"text": "Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT.", "labels": [], "entities": []}, {"text": "We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (mono-lingual corpora) data.", "labels": [], "entities": []}, {"text": "The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively.", "labels": [], "entities": []}, {"text": "Our approach cannot only exploit the monolingual corpora of the target language, but also of the source language.", "labels": [], "entities": []}, {"text": "Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.", "labels": [], "entities": [{"text": "Chinese-English dataset", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.8034914135932922}, {"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9793144464492798}]}], "introductionContent": [{"text": "End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years).", "labels": [], "entities": [{"text": "End-to-end neural machine translation (NMT)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.737319541828973}]}, {"text": "Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (), NMT has proven to excel in model- * Yang Liu is the corresponding author.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 90, "end_pos": 127, "type": "TASK", "confidence": 0.7738149364789327}]}, {"text": "ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating) and attention mechanisms (.", "labels": [], "entities": []}, {"text": "However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models.", "labels": [], "entities": [{"text": "training translation", "start_pos": 110, "end_pos": 130, "type": "TASK", "confidence": 0.5678585916757584}]}, {"text": "This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT.", "labels": [], "entities": []}, {"text": "Unfortunately, parallel corpora are usually only available fora handful of researchrich languages and restricted to limited domains such as government documents and news reports.", "labels": [], "entities": []}, {"text": "In contrast, SMT is capable of exploiting abundant target-side monolingual corpora to boost fluency of translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9893168807029724}]}, {"text": "Therefore, the unavailability of large-scale, high-quality, and wide-coverage parallel corpora hinders the applicability of NMT.", "labels": [], "entities": []}, {"text": "As a result, several authors have tried to use abundant monolingual corpora to improve NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8573881983757019}]}, {"text": "propose two methods, which are referred to as shallow fusion and deep fusion, to integrate a language model into NMT.", "labels": [], "entities": []}, {"text": "The basic idea is to use the language model to score the candidate words proposed by the translation model at each time step or concatenating the hidden states of the language model and the decoder.", "labels": [], "entities": []}, {"text": "Although their approach leads to significant improvements, one possible downside is that the network architecture has to be modified to integrate the language model.", "labels": [], "entities": []}, {"text": "Alternatively, propose two approaches to exploiting monolingual corpora that is transparent to network architectures.", "labels": [], "entities": []}, {"text": "The first approach pairs monolingual sentences with dummy input.", "labels": [], "entities": []}, {"text": "Then, the parameters of encoder Figure 1: Examples of (a) source autoencoder and (b) target autoencoder on monolingual corpora.", "labels": [], "entities": []}, {"text": "Our idea is to leverage autoencoders to exploit monolingual corpora for NMT.", "labels": [], "entities": []}, {"text": "Ina source autoencoder, the source-to-target model P (y|x; \u2212 \u2192 \u03b8 ) serves as an encoder to transform the observed source sentence x into a latent target sentence y (highlighted in grey), from which the target-to-source model P (x |y; \u2190 \u2212 \u03b8 ) reconstructs a copy of the observed source sentence x from the latent target sentence.", "labels": [], "entities": []}, {"text": "As a result, monolingual corpora can be combined with parallel corpora to train bidirectional NMT models in a semi-supervised setting. and attention model are fixed when training on these pseudo parallel sentence pairs.", "labels": [], "entities": []}, {"text": "In the second approach, they first train a nerual translation model on the parallel corpus and then use the learned model to translate a monolingual corpus.", "labels": [], "entities": [{"text": "nerual translation", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.665086954832077}]}, {"text": "The monolingual corpus and its translations constitute an additional pseudo parallel corpus.", "labels": [], "entities": []}, {"text": "Similar ideas have also been suggested in conventional SMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9907309412956238}]}, {"text": "report that their approach significantly improves translation quality across a variety of language pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9697355031967163}]}, {"text": "In this paper, we propose semi-supervised learning for neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.6486504773298899}]}, {"text": "Given labeled (i.e., parallel corpora) and unlabeled (i.e., monolingual corpora) data, our approach jointly trains source-to-target and target-to-source translation models.", "labels": [], "entities": []}, {"text": "The key idea is to append a reconstruction term to the training objective, which aims to reconstruct the observed monolingual corpora using an autoencoder.", "labels": [], "entities": []}, {"text": "In the autoencoder, the source-to-target and target-to-source models serve as the encoder and decoder, respectively.", "labels": [], "entities": []}, {"text": "As the inference is intractable, we propose to sample the full search space to improve the efficiency.", "labels": [], "entities": []}, {"text": "Specifically, our approach has the following advantages: 1.", "labels": [], "entities": []}, {"text": "Transparent to network architectures: our approach does not depend on specific architectures and can be easily applied to arbitrary end-to-end NMT systems.", "labels": [], "entities": []}, {"text": "2. Both the source and target monolingual corpora can be used: our approach can benefit NMT not only using target monolingual corpora in a conventional way, but also the monolingual corpora of the source language.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English NIST datasets show that our approach results in significant improvements in both directions over state-of-the-art SMT and NMT systems.", "labels": [], "entities": [{"text": "Chinese-English NIST datasets", "start_pos": 15, "end_pos": 44, "type": "DATASET", "confidence": 0.5984781185785929}, {"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9783042669296265}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison with MOSES and RNNSEARCH. MOSES is a phrase-based statistical machine  translation system (", "labels": [], "entities": [{"text": "phrase-based statistical machine  translation", "start_pos": 58, "end_pos": 103, "type": "TASK", "confidence": 0.5579178854823112}]}]}