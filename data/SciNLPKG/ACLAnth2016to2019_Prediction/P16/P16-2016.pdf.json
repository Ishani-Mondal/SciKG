{"title": [{"text": "Empty element recovery by spinal parser operations", "labels": [], "entities": [{"text": "Empty element recovery", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6570289731025696}]}], "abstractContent": [{"text": "This paper presents a spinal parsing algorithm that can jointly detect empty elements.", "labels": [], "entities": [{"text": "spinal parsing", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7017096281051636}]}, {"text": "This method achieves state-of-the-art performance on English and Japanese empty element recovery problems.", "labels": [], "entities": [{"text": "empty element recovery", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.606627474228541}]}], "introductionContent": [{"text": "Empty categories, which are used in Penn Treebank style annotations to represent complex syntactic phenomena like constituent movement and discontinuous constituents, provide important information for understanding the semantic structure of sentences.", "labels": [], "entities": [{"text": "Penn Treebank style annotations", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.9601945877075195}]}, {"text": "Previous studies attempt empty element recovery by casting it as linear tagging, PCFG parsing) or post-processing of syntactic parsing).", "labels": [], "entities": [{"text": "empty element recovery", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.6997809012730917}, {"text": "PCFG parsing", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.792682945728302}]}, {"text": "To the best of our knowledge, the results reported by) are the best yet reported, so we pursue a method that uses syntactic parsing to jointly solve the empty element recovery problem.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.7450502514839172}, {"text": "empty element recovery", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.6707313259442648}]}, {"text": "Our proposal uses the spinal Tree Adjoining Grammar (TAG) formalism of ().", "labels": [], "entities": []}, {"text": "The spinal TAG has a set of elementary trees, called spines, each consisting of a lexical anchor with a series of unary projections.", "labels": [], "entities": []}, {"text": "Figure 1 displays (a) a head-annotated constituent tree and (b) spines extracted from the tree.", "labels": [], "entities": []}, {"text": "This paper presents a transition-based algorithm together with several operations to combine spines for constructing full parse trees with empty elements.", "labels": [], "entities": []}, {"text": "Compared with the PCFG parsing approaches, one advantage of our method is its flexible feature representations, which allow the incorporation of constituency-, dependency-and spine-based features.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.751929521560669}]}, {"text": "Of particular interest, the motivation for our spinal TAG-based approach comes from the . .", "labels": [], "entities": []}, {"text": "intuition that features extracted from spines can be expected to be useful for empty element recovery in the same way as constituency-based vertical higher-order conjunctive features are used in recent post-processing methods (.", "labels": [], "entities": [{"text": "empty element recovery", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.6824107567469279}]}, {"text": "Experiments on English and Japanese datasets empirically show that our system outperforms existing alternatives.", "labels": [], "entities": []}], "datasetContent": [{"text": "Finally, to show that our method works well on other languages, we conduct experiments on the Japanese Keyaki Treebank (.", "labels": [], "entities": [{"text": "Japanese Keyaki Treebank", "start_pos": 94, "end_pos": 118, "type": "DATASET", "confidence": 0.8983537554740906}]}, {"text": "For this data, we modified blatt to keep function labels And, in order to consider segmentation errors, we also modified eevalb to calculate not word but character span in a sentence.", "labels": [], "entities": []}, {"text": "We follow the experiments in ( and show the results in.", "labels": [], "entities": []}, {"text": "Our method significantly outperforms the state-of-the-art post-processing method in Japanese.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Result Analysis: M denotes the number  of matches of system outputs (O) with the gold.", "labels": [], "entities": [{"text": "Result Analysis", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9323204159736633}, {"text": "M", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.9306496381759644}]}, {"text": " Table 1: Results on the English Penn Treebank (Section 23): to calculate the scores for Tagger, we  obtained a parse tree by supplying the 1-best Tagger output with the Berkeley parser trained on Sections  02-21 including empty elements (using the option \"-useGoldPOS\").", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.9266277352968851}]}]}