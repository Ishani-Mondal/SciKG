{"title": [{"text": "Recognizing Salient Entities in Shopping Queries", "labels": [], "entities": [{"text": "Recognizing Salient Entities in Shopping Queries", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8729260961214701}]}], "abstractContent": [{"text": "Over the past decade, e-Commerce has rapidly grown enabling customers to purchase products with the click of a button.", "labels": [], "entities": []}, {"text": "But to be able to do so, one has to understand the semantics of a user query and identify that in digital lifestyle tv, digital lifestyle is a brand and tv is a product.", "labels": [], "entities": []}, {"text": "In this paper, we develop a series of struc-tured prediction algorithms for semantic tagging of shopping queries with the product, brand, model and product family types.", "labels": [], "entities": [{"text": "semantic tagging of shopping queries", "start_pos": 76, "end_pos": 112, "type": "TASK", "confidence": 0.7989538550376892}]}, {"text": "We model wide variety of features and show an alternative way to capture knowledge base information using embed-dings.", "labels": [], "entities": []}, {"text": "We conduct an extensive study over 37, 000 manually annotated queries and report performance of 90.92 F 1 independent of the query length.", "labels": [], "entities": [{"text": "F 1", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9149216413497925}]}], "introductionContent": [{"text": "Recent study shows that yearly e-Commerce sales in the U.S. top 100 Billion.", "labels": [], "entities": []}, {"text": "This leads to substantially increased interest in building semantic taggers that can accurately recognize product, brand, model and product family types in shopping queries to better understand and match the needs of online shoppers.", "labels": [], "entities": []}, {"text": "Despite the necessity for semantic understanding, yet most widely used approaches for product retrieval categorize the query and the offer into a shopping taxonomy and use the predicted category as a proxy for retrieving the relevant products.", "labels": [], "entities": []}, {"text": "Unfortunately, such procedure falls short and leads to inaccurate product retrieval.", "labels": [], "entities": []}, {"text": "Recent efforts) focused on building CRF taggers that recognize basic entity types in shopping query such as brands, types and models.) conducted a study over 4000 shopping queries and showed promising results when huge knowledge bases are present.;) focused on using Hearst patterns to learn semantic lexicons.", "labels": [], "entities": [{"text": "CRF taggers", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.7793974280357361}]}, {"text": "While such methods are promising, they cannot be used to recognize all product entities in a query.", "labels": [], "entities": []}, {"text": "In parallel to the semantic query understanding task, there have been semantic tagging efforts on the product offer side.", "labels": [], "entities": [{"text": "semantic query understanding", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.7416981061299642}, {"text": "semantic tagging", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7191270291805267}]}, {"text": "(Putthividhya and Hu, 2011) recognize brand, size and color entities in eBay product offers, while) recognized similar fields in Bing product catalogs.", "labels": [], "entities": []}, {"text": "Despite these efforts, to date there are three important questions, which have not been answered, but we address in our work.", "labels": [], "entities": []}, {"text": "(1) What is an alternative method when product knowledge bases are not present?", "labels": [], "entities": []}, {"text": "(2) Is the performance of the semantic taggers agnostic to the query length?", "labels": [], "entities": []}, {"text": "(3) Can we minimize manual feature engineering for shopping query log tagging using neural networks?", "labels": [], "entities": [{"text": "shopping query log tagging", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.747655987739563}]}, {"text": "The main contributions of the paper are: \u2022 Building semantic tagging framework for shopping queries.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.6866750419139862}]}, {"text": "\u2022 Leveraging missing knowledge base entries through word embeddings learned on large amount of unlabeled query logs.", "labels": [], "entities": []}, {"text": "\u2022 Annotating 37, 000 shopping queries with product, brand, model and product family entity types.", "labels": [], "entities": []}, {"text": "\u2022 Conducting a comparative and efficiency study of multiple structured prediction algorithms and settings.", "labels": [], "entities": []}, {"text": "\u2022 Showing that long short-term memory networks reaches the best performance of 90.92 F 1 and is agnostic to query length.", "labels": [], "entities": [{"text": "F 1", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.910617858171463}]}], "datasetContent": [{"text": "Data Set To the best of our knowledge, there is no publicly available shopping query data annotated with product, brand, model, product family and other categories.", "labels": [], "entities": []}, {"text": "To conduct our experiments, we collect 2.5M shopping queries through click https://code.google.com/p/word2vec/ logs ().", "labels": [], "entities": []}, {"text": "We randomly sampled 37, 000 unique queries from the head, torso and tail of a commercial web search engine and asked two independent annotators to tag the data.", "labels": [], "entities": []}, {"text": "We measured the Kappa agreement of the editors and found .92 agreement, which is sufficient to warrant the goodness of the annotations.", "labels": [], "entities": []}, {"text": "We randomly split the data into 80% for training and 20% for testing.", "labels": [], "entities": []}, {"text": "We tune all parameters on the training set using 5-fold cross validation and report performance on the test set.", "labels": [], "entities": []}, {"text": "All results are calculated with the CONLL evaluation script 4 . Performance w.r.t.", "labels": [], "entities": [{"text": "CONLL evaluation script", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.9136723081270853}]}, {"text": "Features shows the performance of the different models and feature combinations.", "labels": [], "entities": []}, {"text": "We use the individual features as a baseline.", "labels": [], "entities": []}, {"text": "The obtained results show that these are insufficient to solve such a complex task.", "labels": [], "entities": []}, {"text": "We compared the performance of the KB and WE features when combined with (LEX+ORTO+PSTNL) information.", "labels": [], "entities": [{"text": "LEX+ORTO+PSTNL)", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.7988869547843933}]}, {"text": "As we can see, both KB and WE reach comparable performance.", "labels": [], "entities": [{"text": "WE", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.6477067470550537}]}, {"text": "This study shows that training embeddings on large in-domain data of shopping queries is a reliable and cheap source for knowledge base construction, when such information is not present.", "labels": [], "entities": [{"text": "knowledge base construction", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.636890838543574}]}, {"text": "In our study the best performance is reached when all features are combined.", "labels": [], "entities": []}, {"text": "Among all machine learning classifiers for which we manually designed features, structured perception reaches the best performance of 88.13 F 1 score.", "labels": [], "entities": [{"text": "88.13 F 1 score", "start_pos": 134, "end_pos": 149, "type": "METRIC", "confidence": 0.7536307871341705}]}, {"text": "In addition to the feature combination and model comparison, we also study in the training time of each model in log scale against its F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9606433709462484}]}, {"text": "SEARN is the fastest algorithm to train,   while CRF takes the longest time to train.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9178980588912964}, {"text": "CRF", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.7004343271255493}]}, {"text": "Among all STRUCTPERCEPTRON offers the best balance between efficiency and performance in areal time setting.", "labels": [], "entities": []}, {"text": "Entity Category shows the performance of the algorithms with the manually designed features against the automatically induced ones with LSTM-CRF.", "labels": [], "entities": []}, {"text": "We show the performance of each individual product entity category.", "labels": [], "entities": []}, {"text": "Compared to all models and settings, LSTM-CRF reaches the best performance of 90.92 F 1 score.", "labels": [], "entities": [{"text": "F 1", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9303981065750122}]}, {"text": "The most challenging entity types are product family and model, due to their \"wild\" and irregular nature.", "labels": [], "entities": []}, {"text": "Query Length Finally, we also study the performance of our approach with respect to the different query length.", "labels": [], "entities": []}, {"text": "shows the F 1 score of the two best performing algorithms LSTM-CRF and STRUCTPERCEPTRON against the different query length in the test set.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9846701423327128}, {"text": "STRUCTPERCEPTRON", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.9213078022003174}]}, {"text": "Around 83% of the queries have length between 2 to 5 words, the rest are either very short or very long ones.", "labels": [], "entities": []}, {"text": "As it can be seen in, independent of the query length, our models reach the same performance for short and long queries.", "labels": [], "entities": []}, {"text": "This shows that the models are robust and agnostic to the query length.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results from feature study.", "labels": [], "entities": []}, {"text": " Table 2: Entity category distribution.", "labels": [], "entities": [{"text": "Entity category distribution", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8084896604220072}]}, {"text": " Table 3: Per category performance.", "labels": [], "entities": []}]}