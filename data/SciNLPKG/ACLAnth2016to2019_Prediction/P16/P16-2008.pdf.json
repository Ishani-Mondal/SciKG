{"title": [{"text": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings", "labels": [], "entities": [{"text": "Sequence-to-Sequence Generation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8513002395629883}, {"text": "Spoken Dialogue", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.8060017824172974}]}], "abstractContent": [{"text": "We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax dependency trees from input dialogue acts, and we use it to directly compare two-step generation with separate sentence planning and surface realization stages to a joint, one-step approach.", "labels": [], "entities": [{"text": "sentence planning and surface realization", "start_pos": 273, "end_pos": 314, "type": "TASK", "confidence": 0.6771513998508454}]}, {"text": "We were able to train both setups successfully using very little training data.", "labels": [], "entities": []}, {"text": "The joint setup offers better performance, surpassing state-of-the-art with regards to n-gram-based scores while providing more relevant outputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "In spoken dialogue systems (SDS), the task of natural language generation (NLG) is to convert a meaning representation (MR) produced by the dialogue manager into one or more sentences in a natural language.", "labels": [], "entities": [{"text": "spoken dialogue systems (SDS)", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6737730453411738}, {"text": "natural language generation (NLG)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.8231161534786224}]}, {"text": "It is traditionally divided into two subtasks: sentence planning, which decides on the overall sentence structure, and surface realization, determining the exact word forms and linearizing the structure into a string).", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7174012511968613}, {"text": "surface realization", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.76332688331604}]}, {"text": "While some generators keep this division and use a two-step pipeline (, others apply a joint model for both tasks.", "labels": [], "entities": []}, {"text": "We present anew, conceptually simple NLG system for SDS that is able to operate in both modes: it either produces natural language strings or generates deep syntax dependency trees, which are subsequently processed by an external surface realizer . This allows us to show a direct comparison of two-step generation, where sentence planning and surface realization are separated, with a joint, one-step approach.", "labels": [], "entities": [{"text": "SDS", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9589999914169312}]}, {"text": "Our generator is based on the sequence-tosequence (seq2seq) generation technique (, combined with beam search and an n-best list reranker to suppress irrelevant information in the outputs.", "labels": [], "entities": []}, {"text": "Unlike most previous NLG systems for SDS (e.g.,), it is trainable from unaligned pairs of MR and sentences alone.", "labels": [], "entities": [{"text": "SDS", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9713515043258667}]}, {"text": "We experiment with using much less training data than recent systems based on recurrent neural networks (RNN), and we find that our generator learns successfully to produce both strings and deep syntax trees on the BAGEL restaurant information dataset ( . It is able to surpass n-gram-based scores achieved previously by, offering a simpler setup and more relevant outputs.", "labels": [], "entities": [{"text": "BAGEL restaurant information dataset", "start_pos": 215, "end_pos": 251, "type": "DATASET", "confidence": 0.9153098315000534}]}, {"text": "We introduce the generation setting in Section 2 and describe our generator architecture in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 details our experiments, Section 5 analyzes the results.", "labels": [], "entities": []}, {"text": "We summarize related work in Section 6 and offer conclusions in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform our experiments on the BAGEL data set of , which contains 202 DA from the restaurant information domain with two natural language paraphrases each, describing restaurant locations, price ranges, food types etc.", "labels": [], "entities": [{"text": "BAGEL data set", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8337937593460083}]}, {"text": "Some properties such as restaurant names or phone numbers are delexicalized (replaced with \"X\" symbols) to avoid data sparsity.", "labels": [], "entities": []}, {"text": "Unlike , we do not use We adopt the delexicalization scenario used by  and manually annotated alignment of slots and values in the input DA to target words and phrases and let the generator learn it from data, which simplifies training data preparation but makes our task harder.", "labels": [], "entities": []}, {"text": "We lowercase the data and treat plural -s as separate tokens for generating into strings, and we apply automatic analysis from the Treex NLP toolkit (Popel and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2010) to obtain deep syntax trees for training tree-based generator setups.", "labels": [], "entities": [{"text": "Treex NLP toolkit (Popel and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2010)", "start_pos": 131, "end_pos": 208, "type": "DATASET", "confidence": 0.9471891191270616}]}, {"text": "3 Same as , we apply 10-fold cross-validation, with 181 training DA and 21 testing DA.", "labels": [], "entities": [{"text": "DA", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.986381471157074}]}, {"text": "In addition, we reserve 10 DA from the training set for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.9695370197296143}]}, {"text": "To train our seq2seq generator, we use the Adam optimizer ( to minimize unweighted sequence cross-entropy.", "labels": [], "entities": []}, {"text": "We perform 10 runs with different random initialization of the network and up to 1,000 passes over the training data, validating after each pass and selecting the parameters that yield the highest BLEU score on the validation set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 197, "end_pos": 207, "type": "METRIC", "confidence": 0.9820941388607025}]}, {"text": "Neither beam search nor the reranker are used for validation.", "labels": [], "entities": [{"text": "beam search", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.790001392364502}, {"text": "validation", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.9692907929420471}]}, {"text": "We use the Adam optimizer minimizing crossentropy to train the reranker as well.", "labels": [], "entities": []}, {"text": "We perform a single run of up to 100 passes over the data, and we also validate after each pass and select the parameters giving minimal Hamming distance on both validation and training set.", "labels": [], "entities": [{"text": "Hamming", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.7112800478935242}]}, {"text": "8 The input vocabulary size is around 45 (DA types, slots, and values added up) and output vocabulary sizes are around 170 for string generation and 180 for tree generation (45 formemes and 135 lemmas).", "labels": [], "entities": []}, {"text": "We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two references to measure BLEU and NIST scores () on the validation and test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9986397624015808}, {"text": "NIST scores", "start_pos": 147, "end_pos": 158, "type": "METRIC", "confidence": 0.5365159064531326}]}, {"text": "Based on a few preliminary experiments, the learning rate is set to 0.001, embedding size 50, LSTM cell size 128, and batch size 20.", "labels": [], "entities": []}, {"text": "Reranking penalty for decoding is 100.", "labels": [], "entities": [{"text": "Reranking penalty", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.9667166471481323}]}, {"text": "Training is terminated early if the top 10 so far achieved validation BLEU scores do not change for 100 passes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9480094909667969}]}, {"text": "We use the same settings as with the seq2seq generator.", "labels": [], "entities": []}, {"text": "The validation set is given 10 times more importance.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.938295304775238}]}], "tableCaptions": [{"text": " Table 1: Results on the BAGEL data set", "labels": [], "entities": [{"text": "BAGEL data set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.8418919444084167}]}]}