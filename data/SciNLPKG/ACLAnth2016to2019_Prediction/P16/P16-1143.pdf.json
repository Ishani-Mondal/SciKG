{"title": [{"text": "LEXSEMTM: A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed.", "labels": [], "entities": []}, {"text": "This paper analyses a state-of-the-art method for sense distribution learning, and op-timises it for application to the entire vocabulary of a given language.", "labels": [], "entities": [{"text": "sense distribution learning", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.80636594692866}]}, {"text": "The optimised method is then used to produce LEXSEMTM: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polyse-mous, English simplex lemmas, which is released as a public resource to the community.", "labels": [], "entities": []}, {"text": "Finally, the quality of this data is investigated, and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR, and at least on par with SEMCOR-based distributions otherwise.", "labels": [], "entities": [{"text": "LEXSEMTM sense", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.9389696717262268}, {"text": "SEMCOR", "start_pos": 179, "end_pos": 185, "type": "DATASET", "confidence": 0.8424926996231079}]}], "introductionContent": [{"text": "Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see and).", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.802383542060852}]}, {"text": "In particular, there has recently been a lot of work on unsupervised techniques for these problems.", "labels": [], "entities": []}, {"text": "This includes unsupervised methods for performing WSD (), as well as complementary problems dealing with word senses ().", "labels": [], "entities": [{"text": "WSD", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9337766766548157}]}, {"text": "One such application has been the automatic learning of sense distributions.", "labels": [], "entities": [{"text": "automatic learning of sense distributions", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.7314440369606018}]}, {"text": "A sense distribution is a probability distribution over the senses of a given lemma.", "labels": [], "entities": []}, {"text": "For example, if the noun crane had two senses, bird and machine, then a hypothetical sense distribution could indicate that the noun is expected to take the machine meaning 60% of the time and the bird meaning 40% of the time in a representative corpus.", "labels": [], "entities": []}, {"text": "Sense distributions (or simple \"first sense\" information) are used widely in tasks including information extraction (, novel word sense detection (), semi-automatic dictionary construction (, lexical simplification, and textual entailment).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.8101346790790558}, {"text": "word sense detection", "start_pos": 125, "end_pos": 145, "type": "TASK", "confidence": 0.6762751638889313}, {"text": "semi-automatic dictionary construction", "start_pos": 150, "end_pos": 188, "type": "TASK", "confidence": 0.6027015844980875}, {"text": "textual entailment", "start_pos": 220, "end_pos": 238, "type": "TASK", "confidence": 0.7112201601266861}]}, {"text": "Automatically acquired sense distributions themselves are also used to improve unsupervised WSD, for example by providing a most frequent sense heuristic ( or by improving unsupervised usage sampling strategies ().", "labels": [], "entities": [{"text": "WSD", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.91475909948349}]}, {"text": "Furthermore, the improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data (;).", "labels": [], "entities": []}, {"text": "In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation.", "labels": [], "entities": []}, {"text": "The most prominent example of such a resource is WORDNET, where the sense frequency data is based on SEMCOR), a 220,000 word corpus that has been manually tagged with WORDNET senses.", "labels": [], "entities": [{"text": "WORDNET", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9068921208381653}]}, {"text": "This data is full of glaring irregularities due to its age and the limited size of the corpus; for example, the word pipe has its most frequent sense listed as tobacco pipe, whereas one might expect this to be tube carrying water or gas in modern English ().", "labels": [], "entities": []}, {"text": "This is likely due to the more common use of the tobacco pipe sense in mid-20th century literature.", "labels": [], "entities": []}, {"text": "The problem is particularly highlighted by the fact that out of the approximately 28,000 polysemous simplex lemmas in WORDNET 3.0, approximately 61% have no sense annotations at all, and less than half of the remaining lemmas have at least 5 sense annotations!", "labels": [], "entities": [{"text": "WORDNET 3.0", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.9157517850399017}]}, {"text": "Unfortunately, there has been alack of work investigating how to apply sense learning techniques at the scale of a full lexical resource such as WORDNET.", "labels": [], "entities": [{"text": "WORDNET", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.9212873578071594}]}, {"text": "Updating language-wide sense frequency resources would require learning sense distributions over the entire vocabularies of languages, which could be extremely computationally expensive.", "labels": [], "entities": []}, {"text": "To make things worse, domain differences could require learning numerous distributions per word.", "labels": [], "entities": []}, {"text": "Despite this, though, we would not want to make these techniques scalable at the expense of sense distribution quality.", "labels": [], "entities": []}, {"text": "Therefore, we would like to understand the tradeoff between the accuracy and computation time of these techniques, and optimise this tradeoff.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.999234676361084}]}, {"text": "This could be particularly critical in applying them in an industrial setting.", "labels": [], "entities": []}, {"text": "The current state-of-the-art technique for unsupervised sense distribution learning is HDP-WSI ().", "labels": [], "entities": []}, {"text": "In order to address the above concerns, we provide a series of investigations exploring how to best optimise HDP-WSI for largescale application.", "labels": [], "entities": []}, {"text": "We then use our optimised technique to produce LEXSEMTM, 1 a semantic and sense frequency dataset of unprecedented size, spanning the entire vocabulary of English.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9113523364067078}]}, {"text": "Finally, we use crowdsourced data to produce anew set of gold-standard sense distributions to accompany LEXSEMTM.", "labels": [], "entities": []}, {"text": "We use these to investigate the quality of the sense frequency data in LEXSEMTM with respect to SEMCOR.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.8510823249816895}]}], "datasetContent": [{"text": "We evaluate HCA-WSI in comparison to HDP-WSI using one of the sense tagged datasets of which was also used by.", "labels": [], "entities": [{"text": "HCA-WSI", "start_pos": 12, "end_pos": 19, "type": "DATASET", "confidence": 0.8415313363075256}]}, {"text": "This dataset consists of 40 English lemmas, and for each lemma it contains a set of usages of varying size from the BNC and a gold-standard sense distribution that was created by hand-annotating a subset of the usages with WORDNET 1.7 senses.", "labels": [], "entities": [{"text": "BNC", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.9563015103340149}]}, {"text": "Using this dataset, we can calculate the quality of a candidate sense distribution by calculating its Jensen Shannon divergence (JSD) with respect to the corresponding gold-standard distribution.", "labels": [], "entities": [{"text": "Jensen Shannon divergence (JSD)", "start_pos": 102, "end_pos": 133, "type": "METRIC", "confidence": 0.7438337107499441}]}, {"text": "JSD is a measure of dissimilarity between two probability distributions, so a lower JSD score means the distribution is more similar to the goldstandard, and is therefore assumed to be of higher quality.", "labels": [], "entities": [{"text": "JSD score", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.8985943496227264}]}, {"text": "Given our finding on topic counts in Section 3, HCA was run using a fixed number of 10 topics.", "labels": [], "entities": []}, {"text": "Other settings were configured as recommended in the HCA documentation, or according to the HDP settings used by.", "labels": [], "entities": [{"text": "HCA documentation", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.9013645052909851}]}, {"text": "This setup is also used in subsequent experiments, except where stated otherwise.", "labels": [], "entities": []}, {"text": "We proceeded by calculating the JSD scores of all lemmas in this dataset, using both methods.", "labels": [], "entities": [{"text": "JSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.4742022752761841}]}, {"text": "We performed a Wilcoxon signed-rank test on the two sequences of JSD scores, in order to test the hypothesis that switching to HCA-WSI has a systematic impact on sense distribution quality.", "labels": [], "entities": []}, {"text": "We found that the mean JSD score for HDP-WSI was 0.209 \u00b1 0.116, slightly lower than the mean JSD score for HCA-WSI of 0.211 \u00b1 0.117.", "labels": [], "entities": [{"text": "JSD score", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.75364089012146}]}, {"text": "However the two-sided p-value from the test was 0.221, which is insignificant at any reasonable decision threshold.", "labels": [], "entities": []}, {"text": "In addition, we compared the time taken 11 to run topic modelling for every lemma using both methods, the results of which are displayed in.", "labels": [], "entities": []}, {"text": "These results show that the computation time of HCA-WSI is consistently lower than that of HDP-WSI, by over an order of magnitude.", "labels": [], "entities": [{"text": "HCA-WSI", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.7841177582740784}]}, {"text": "We conclude that HCA-WSI is far more computationally efficient than HDP-WSI, and there is no significant evidence that it gives worse sense distributions.", "labels": [], "entities": []}, {"text": "Therefore, HCA-WSI is used instead of HDP-WSI for the remainder of the paper.", "labels": [], "entities": [{"text": "HCA-WSI", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8048461675643921}]}, {"text": "We now discuss the creation of the LEXSEMTM (\"Lexical Semantic Topic Models\") dataset, which contains trained topic models for the majority of simplex English lemmas.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.927534818649292}]}, {"text": "These can be aligned to any sense repository with glosses to produce sense distributions, or used directly in other applications.", "labels": [], "entities": []}, {"text": "In addition, the dataset contains distributions over WORDNET 3.0 senses.", "labels": [], "entities": []}, {"text": "In order to produce domain-neutral sense distributions reflecting usage in modern English, we sampled all lemma usages from English Wikipedia.", "labels": [], "entities": []}, {"text": "Our Wikipedia corpus was tokenised and POS-tagged using OpenNLP and lemmatised using Morpha ().", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.938919186592102}, {"text": "OpenNLP", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9666356444358826}]}, {"text": "We trained topic models for every simplex lemma in WORDNET 3.0 with at least 20 usages in our processed Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.8611017167568207}]}, {"text": "This included lemmas for all POS (nouns, verbs, adjectives, and adverbs), and also nonpolysemous lemmas.", "labels": [], "entities": []}, {"text": "In Section 5, we concluded that approximately 5,000-10,000 usages were needed for convergent results with the BNC dataset.", "labels": [], "entities": [{"text": "BNC dataset", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.9820947051048279}]}, {"text": "On the other hand, given that we are working on a different corpus and with a wider range of lemmas there is uncertainty in this number, so we conservatively sampled up to 40,000 usages per lemma, if available.", "labels": [], "entities": []}, {"text": "These usages were sampled from the corpus by locating all sentences where either the surface or lemmatised forms of the sentence contained the target lemma, along with a matching POStag.", "labels": [], "entities": []}, {"text": "Processing of lemma usages was done almost identically to.", "labels": [], "entities": []}, {"text": "However, because we found the usages contained substantially fewer tokens on average compared to the BNC dataset, we included two sentences rather than one on either side of the target lemma location where possible (giving 5 sentences in total), which gave a better match in usage size.", "labels": [], "entities": [{"text": "BNC dataset", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.9676763713359833}]}, {"text": "Topic models were trained using HCA, using almost the same setup as described in Section 4.1.", "labels": [], "entities": []}, {"text": "However, since some highly-polysemous lemmas may require a greater number of topics than the lemmas in the BNC dataset, we conservatively increased the number of topics used from 10 to 20.", "labels": [], "entities": [{"text": "BNC dataset", "start_pos": 107, "end_pos": 118, "type": "DATASET", "confidence": 0.9628584086894989}]}, {"text": "We similarly increased the number of Gibbs sampling iterations from 300 to 1,000.", "labels": [], "entities": []}, {"text": "15 Finally, for each polysemous lemma that we trained a topic model for, we also produced a sense distribution over WORDNET 3.0 senses, using the default topic-sense alignment method discussed in Section 3.", "labels": [], "entities": []}, {"text": "In total, 62,721 lemmas were processed, and 8,801 of these had the desired number of at least 5,000 usages.", "labels": [], "entities": []}, {"text": "Counting only polysemous lemmas for which we also provide sense distributions, 25,155 were processed in total, and 6,853 of these had at least 5,000 usages.", "labels": [], "entities": []}, {"text": "This works out to approximately 88% coverage of polysemous WORDNET 3.0 lemmas in total, or 24% coverage with at least 5,000 usages (as compared to 39% coverage by lemmas in SEMCOR, or 17% with at least 5 sense-tagged occurrences in SEMCOR).", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 173, "end_pos": 179, "type": "DATASET", "confidence": 0.8944718837738037}, {"text": "SEMCOR", "start_pos": 232, "end_pos": 238, "type": "DATASET", "confidence": 0.899326741695404}]}, {"text": "Our final major contribution is an analysis of how our LEXSEMTM sense distributions compare with SEMCOR.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.8917984962463379}]}, {"text": "We produce anew set of goldstandard sense distributions fora diverse set of simplex English lemmas tagged with WORDNET 3.0 senses, created using crowdsourced annotations of English Wikipedia usages.", "labels": [], "entities": []}, {"text": "We use these gold-standard distributions to investigate when LEXSEMTM should be used in place of SEM-COR, and release them as a public resource, to facilitate the evaluation of future work involving LEXSEMTM.", "labels": [], "entities": []}, {"text": "We now use these gold-standard distributions to evaluate the sense distributions in LEXSEMTM relative to SEMCOR.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.853863000869751}]}, {"text": "For each of the 50 lemmas that we created gold-standard distributions for, we evaluate the corresponding LEXSEMTM distribution against the gold-standard.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9703568816184998}]}, {"text": "In addition, we create benchmark sense distributions for each lemma from SEMCOR counts using maximum likelihood estimation, which we also evaluate against the gold-standards.", "labels": [], "entities": [{"text": "SEMCOR counts", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.7560073733329773}, {"text": "maximum likelihood estimation", "start_pos": 93, "end_pos": 122, "type": "METRIC", "confidence": 0.7631353537241617}]}, {"text": "Evaluation of sense distribution quality using gold-standard distributions is done by calculating JSD, as in Section 4.1.", "labels": [], "entities": [{"text": "JSD", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.6797393560409546}]}, {"text": "First, we performed this comparison of LEXSEMTM to SEMCOR JSD scores for all 50 lemmas at once.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9699979424476624}, {"text": "SEMCOR JSD scores", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.5711787641048431}]}, {"text": "As in Section 4.1, we calculated the JSD scores for every lemma using each method individually, and compared the difference in values pairwise for statistical significance using a Wilcoxon signed-rank test.", "labels": [], "entities": [{"text": "JSD", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.6109606027603149}]}, {"text": "The results of this comparison are detailed in (final row: Group = All), which shows that JSD is clearly lower for LEXSEMTM distributions compared to SEMCOR, as would be hoped.", "labels": [], "entities": [{"text": "JSD", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.4864153265953064}, {"text": "SEMCOR", "start_pos": 150, "end_pos": 156, "type": "DATASET", "confidence": 0.722338080406189}]}, {"text": "This difference is statistically significant at p < 0.05.", "labels": [], "entities": []}, {"text": "We then performed the same comparison separately within each SEMCOR frequency group (Table 1).", "labels": [], "entities": [{"text": "SEMCOR frequency group", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.893242617448171}]}, {"text": "First of all, we can see that LEXSEMTM sense distributions strongly outperform SEMCORbased distributions in Group 1 (lemmas missing from SEMCOR).", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 137, "end_pos": 143, "type": "DATASET", "confidence": 0.8478569984436035}]}, {"text": "This is as would be expected, since the SEMCOR-based distributions for this group are based on which sense is listed first in WORDNET, which in the absence of SEMCOR counts is arbitrary.", "labels": [], "entities": [{"text": "WORDNET", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.883718729019165}]}, {"text": "On the other hand, in all other groups (lemmas in SEMCOR) the difference between LEXSEMTM and SEMCOR is not statisti- For lemmas with no SEMCOR annotations, we assign one count to the first-listed sense in WORDNET 3.0.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8186606168746948}, {"text": "LEXSEMTM", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9592126607894897}, {"text": "SEMCOR", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.7353763580322266}, {"text": "WORDNET 3.0", "start_pos": 206, "end_pos": 217, "type": "DATASET", "confidence": 0.8641703426837921}]}, {"text": "cally significant (p > 0.1 in all cases).", "labels": [], "entities": []}, {"text": "This still remains true when we pool together the results from these groups (second last row of: Group = 2-5).", "labels": [], "entities": []}, {"text": "While it appears that LEXSEMTM may still be outperforming SEMCOR on average over these groups (lower JSD on average), we do not have enough statistical power to be sure, given the high variance.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.962946355342865}, {"text": "SEMCOR", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.5228095054626465}, {"text": "JSD", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.7305816411972046}]}, {"text": "Returning to the initial question regarding a SEMCOR frequency cutoff, the only strong conclusion we can make is that LEXSEMTM is clearly superior for lemmas missing from SEMCOR.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.99149090051651}, {"text": "SEMCOR", "start_pos": 171, "end_pos": 177, "type": "DATASET", "confidence": 0.8625714182853699}]}, {"text": "Although it appears that LEXSEMTM may outperform SEMCOR for lemmas with higher SEMCOR frequencies, the variance in our results is too high to be sure of this, let alone define a frequency cutoff.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9749431610107422}]}, {"text": "However, given that LEXSEMTM sense distributions never appear to be worse than SEM-COR-based distributions, regardless of SEMCOR frequency -and may actually be marginally superior -it seems reasonable to use our sense distributions in general in place of SEMCOR.", "labels": [], "entities": []}, {"text": "We can contrast this result to the findings of, who found that the automatic first sense learning method of outperformed SEMCOR for words with SEMCOR frequency less than 5.", "labels": [], "entities": []}, {"text": "However, their analysis was based on the accuracy of the first sense heuristic, rather than the entire sense distribution, and they used very different datasets to us.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9991833567619324}]}, {"text": "21 Furthermore, their SEMCOR frequency cutoff result was only statistically significant for some variations of their method, and they evaluated over more lemmas meaning that statistical significance was easier to obtain.", "labels": [], "entities": [{"text": "SEMCOR frequency cutoff", "start_pos": 22, "end_pos": 45, "type": "METRIC", "confidence": 0.5273702243963877}]}, {"text": "Given these reasons, their results likely do not contradict ours.", "labels": [], "entities": []}, {"text": "Given that LEXSEMTM contains sense frequencies for 88% of polysemous simplex lemmas in WORDNET, compared to only 39% for SEM-COR, the strong performance of our LEXSEMTM sense distributions for lemmas missing from SEMCOR is extremely significant.", "labels": [], "entities": [{"text": "WORDNET", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9095615744590759}, {"text": "SEMCOR", "start_pos": 213, "end_pos": 219, "type": "DATASET", "confidence": 0.844753623008728}]}, {"text": "Technically these results are only relevant for lemmas where LEXSEMTM was trained on at least 5,000 us- ages, which reduces the coverage of LEXSEMTM to 24%.", "labels": [], "entities": []}, {"text": "However, even then this gives us sense frequencies for 1,602 polysemous lemmas missing from SEMCOR, which accounts for over 5% of polysemous simplex lemmas in WORDNET.", "labels": [], "entities": [{"text": "SEMCOR", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.7354648113250732}, {"text": "WORDNET", "start_pos": 159, "end_pos": 166, "type": "DATASET", "confidence": 0.9653712511062622}]}, {"text": "Furthermore, based on some additional ongoing analysis comparing LEXSEMTM distributions directly to SEMCOR-based distributions across all of LEXSEMTM (not presented here), it appears the decrease in sense distribution quality for lemmas trained on fewer than 5,000 usages is on average fairly small.", "labels": [], "entities": []}, {"text": "This is corroborated by our results in: we can observe for the lemmas in the BNC dataset that when the number of usages was reduced to 500, the mean change in JSD for each lemma was almost always less than 0.02 and never greater than 0.04, which is small compared to the difference between LEXSEMTM and SEMCOR in each SEMCOR frequency group.", "labels": [], "entities": [{"text": "BNC dataset", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9798436462879181}, {"text": "JSD", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.9263679385185242}, {"text": "SEMCOR", "start_pos": 303, "end_pos": 309, "type": "DATASET", "confidence": 0.8481864333152771}]}, {"text": "This strongly suggests that our conclusions can be extended to lemmas with low LEXSEMTM frequency, though more work is needed to confirm this.", "labels": [], "entities": [{"text": "LEXSEMTM frequency", "start_pos": 79, "end_pos": 97, "type": "METRIC", "confidence": 0.977168470621109}]}], "tableCaptions": [{"text": " Table 1: Sense distribution quality for gold-standard dataset lemmas, comparing LEXSEMTM results to  the SEMCOR benchmark.", "labels": [], "entities": [{"text": "LEXSEMTM", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9367213845252991}, {"text": "SEMCOR benchmark", "start_pos": 106, "end_pos": 122, "type": "DATASET", "confidence": 0.7475054860115051}]}]}