{"title": [], "abstractContent": [{"text": "In word alignment certain source words are only needed for fluency reasons and do not have a translation on the target side.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.8031812608242035}]}, {"text": "Most word alignment models assume a target NULL word from which they generate these untranslatable source words.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7639027237892151}]}, {"text": "Hypothesising a target NULL word is not without problems, however.", "labels": [], "entities": []}, {"text": "For example, because this NULL word has a position, it interferes with the distribution over alignment jumps.", "labels": [], "entities": []}, {"text": "We present a word alignment model that accounts for untranslatable source words by generating them from preceding source words.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7202954590320587}]}, {"text": "It thereby removes the need fora target NULL word and only models alignments between word pairs that are actually observed in the data.", "labels": [], "entities": []}, {"text": "Translation experiments on English paired with Czech, German, French and Japanese show that the model outperforms its traditional IBM counterparts in terms of BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.9833775460720062}]}], "introductionContent": [{"text": "When the IBM models ( were designed, someway of accounting for words that likely have no translation was needed.", "labels": [], "entities": []}, {"text": "The modellers back then decided to introduce a NULL word on the target (generating) side . All words on the source side without a proper target translation would then be generated by that NULL word.", "labels": [], "entities": []}, {"text": "While this solution is technically valid, it neglects that those untranslatable words are required for source fluency.", "labels": [], "entities": []}, {"text": "Moreover, the NULL word, although hypothetical in nature, does have a position.", "labels": [], "entities": []}, {"text": "It is well-known that this NULL posi-tion is problematic for distortion-based alignment models.", "labels": [], "entities": []}, {"text": "Alignments to NULL demand a special treatment as they would otherwise induce very long jumps that one does not usually observe in distortion-based alignment models.", "labels": [], "entities": []}, {"text": "Examples of this can be found in, who drop the NULL word entirely and thus force all source words to align lexically, and, who choose a fixed NULL probability.", "labels": [], "entities": []}, {"text": "In the present work, we introduce a family of IBM-style alignment models that can express dependencies between translated and untranslated source words.", "labels": [], "entities": [{"text": "IBM-style alignment", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6175946891307831}]}, {"text": "The models do not use NULL words and instead allow untranslatable source words to be generated from translated words in their context.", "labels": [], "entities": []}, {"text": "This is achieved by modelling source word collocations.", "labels": [], "entities": []}, {"text": "From a technical point of view the model can be seen as a mixture of an alignment and a language model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present translation experiments on English paired with German, French, Czech and Japanese, thereby covering four language families.", "labels": [], "entities": []}, {"text": "We compare our model and the Bayesian IBM models 1 and 2 of against IBM model 2 as a baseline.", "labels": [], "entities": []}, {"text": "Data We use the news commentary data from the WMT 2014 translation task 6 for German, French and Czech paired with English.", "labels": [], "entities": [{"text": "WMT 2014 translation task 6", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.7529995441436768}]}, {"text": "We use newstest-2013 as development data and we use the newstest-2014 for testing.", "labels": [], "entities": []}, {"text": "We use all available monolingual data from WMT 2014 for language modelling.", "labels": [], "entities": [{"text": "WMT 2014", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.7637423574924469}, {"text": "language modelling", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7667630612850189}]}, {"text": "All data are truecased and sentences (b) Symmetrised: alignments obtained in both directions independently and heuristically symmetrised (grow-diag-final-and).: Translation results from and into English.", "labels": [], "entities": [{"text": "Translation", "start_pos": 161, "end_pos": 172, "type": "TASK", "confidence": 0.9615257382392883}]}, {"text": "Alignments in the top (1a) and bottom (1b) tables were obtained in the target-to-source direction and symmetrised, respectively.", "labels": [], "entities": []}, {"text": "Differences are computed with respect to the directional IBM model 2 in its original parameterisation.", "labels": [], "entities": [{"text": "IBM model 2", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9535924792289734}]}, {"text": "The best Bayesian model in each column is boldfaced. with more than 100 words discarded as is standardly done in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.960252046585083}]}, {"text": "The Japanese training data consist of 200.000 randomly extracted sentence pairs from the NTCIR-8 Patent Translation Task.", "labels": [], "entities": [{"text": "Japanese training data", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.6098577280839285}, {"text": "NTCIR-8 Patent Translation Task", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.828694611787796}]}, {"text": "The full data are used for language modelling.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8145687282085419}]}, {"text": "We use the NTCIR-7 dev sets for tuning and the NTCIR-9 test set for testing.", "labels": [], "entities": [{"text": "NTCIR-7 dev sets", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.9085176189740499}, {"text": "NTCIR-9 test set", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9829970002174377}]}, {"text": "Training The maximum likelihood IBM model 2 is initialized with model 1 parameter estimates and trained for 5 EM iterations.", "labels": [], "entities": []}, {"text": "Following Mermer and Sara\u00e7lar (2011), we initialize the Gibbs samplers of all Bayesian models with the Viterbi alignment from IBM model 1.", "labels": [], "entities": []}, {"text": "We run each sampler for 1000 iterations and take a sample after every 25 th iteration.", "labels": [], "entities": []}, {"text": "We do not use burn-in.", "labels": [], "entities": []}, {"text": "8 Hyperparameters All Bayesian models are trained with \u03b1 = 0.0001 and \u03b2 = 0.0001 to induce sparse lexical distributions.", "labels": [], "entities": []}, {"text": "We also set s = 1 and r = 0.1 when IBM1 is the alignment component in our model.", "labels": [], "entities": [{"text": "IBM1", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.6900363564491272}]}, {"text": "This has the effect of biasing the model towards using the align- The Japanese data was provided to us by a colleague with the pre-processing steps already performed, with sentences shortened to at most 40 words.", "labels": [], "entities": [{"text": "Japanese data", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.8648449182510376}]}, {"text": "Our algorithm can handle sentences of any length and there is actually no need to restrict the sentence lengths.", "labels": [], "entities": []}, {"text": "8 Burn-in is simply a heuristic that is not guaranteed to improve the samples in anyway.", "labels": [], "entities": []}, {"text": "stat.umn.edu/ \u02dc geyer/mcmc/burn.html for further details.", "labels": [], "entities": []}, {"text": "For the IBM2 version we even set r = 0.01 since IBM2 is a more trustworthy alignment model.", "labels": [], "entities": [{"text": "IBM2", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9098100662231445}]}, {"text": "For IBM2, we furthermore set \u03b3 = 1 to obtain a flat distortion prior.", "labels": [], "entities": []}, {"text": "Observe that experiments presented here use the same fixed hyperparameters for all language pairs.", "labels": [], "entities": []}, {"text": "We tried to add another level to our model by imposing Gamma priors on the hyperparameters.", "labels": [], "entities": []}, {"text": "The hyperparameters were then inferred using slice sampling after each Gibbs iteration.", "labels": [], "entities": []}, {"text": "When run on the German-English and CzechEnglish data, this strategy increased the posterior probability of the states visited by our sampler but had no effect on BLEU.", "labels": [], "entities": [{"text": "CzechEnglish data", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.9195397794246674}, {"text": "posterior probability", "start_pos": 82, "end_pos": 103, "type": "METRIC", "confidence": 0.9398556649684906}, {"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9950668811798096}]}, {"text": "This may indicate that either the hand-chosen hyperparameters are adequate for the task or that the model generally performs well fora large range of hyperparameters.", "labels": [], "entities": []}, {"text": "Translation We train Moses systems () with 5-gram language models with modified Kneser-Ney-smoothing using) and orientation-based lexicalised reordering.", "labels": [], "entities": []}, {"text": "We tune the systems with MERT on the dev sets.", "labels": [], "entities": [{"text": "MERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8082125186920166}]}, {"text": "We report the BLEU score () for all models averaged over 5 MERT runs.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9855814576148987}, {"text": "MERT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9913034439086914}]}], "tableCaptions": [{"text": " Table 1: Translation results from and into English. Alignments in the top (1a) and bottom (1b) tables  were obtained in the target-to-source direction and symmetrised, respectively. Differences are computed  with respect to the directional IBM model 2 in its original parameterisation", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9788180589675903}, {"text": "IBM model 2", "start_pos": 241, "end_pos": 252, "type": "DATASET", "confidence": 0.9311420321464539}]}]}