{"title": [{"text": "Tables as Semi-structured Knowledge for Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.794259250164032}]}], "abstractContent": [{"text": "Question answering requires access to a knowledge base to check facts and reason about information.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9259775876998901}]}, {"text": "Knowledge in the form of natural language text is easy to acquire , but difficult for automated reasoning.", "labels": [], "entities": []}, {"text": "Highly-structured knowledge bases can facilitate reasoning, but are difficult to acquire.", "labels": [], "entities": []}, {"text": "In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this trade-off.", "labels": [], "entities": []}, {"text": "We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations, easily and efficiently via crowd-sourcing.", "labels": [], "entities": []}, {"text": "We then use this annotated data to train a semi-structured feature-driven model for question answering that uses tables as a knowledge base.", "labels": [], "entities": [{"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.834438681602478}]}, {"text": "In benchmark evaluations, we significantly outperform both a strong un-structured retrieval baseline and a highly-structured Markov Logic Network model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) has emerged as a practical research problem for pushing the boundaries of artificial intelligence (AI).", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9605437636375427}]}, {"text": "Dedicated projects and open challenges to the research community include examples such as Facebook AI Research's challenge problems for AI-complete QA () and the Allen Institute for AI's (AI2) Aristo project along with its recently completed Kaggle competition . The reason for this emergence is the diversity of core language and reasoning problems that a complex, integrated task like QA exposes: information extraction), semantic modelling), logic and reasoning (, and inference ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 399, "end_pos": 421, "type": "TASK", "confidence": 0.7376671135425568}]}, {"text": "Complex tasks such as QA require some form of knowledge base to store facts about the world and reason over them.", "labels": [], "entities": [{"text": "QA", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9200190901756287}]}, {"text": "By knowledge base, we mean any form of knowledge: structured (e.g., tables, ontologies, rules) or unstructured (e.g., natural language text).", "labels": [], "entities": []}, {"text": "For QA, knowledge has been harvested and used in a number of different modes and formalisms: large-scale extracted and curated knowledge bases), structured models such as Markov Logic Networks (, and simple text corpora in information retrieval approaches (.", "labels": [], "entities": []}, {"text": "There is, however, a fundamental trade-off in the structure and regularity of a formalism and its ability to be curated, modelled or reasoned with easily.", "labels": [], "entities": []}, {"text": "For example, simple text corpora contain no structure, and are therefore hard to reason within a principled manner.", "labels": [], "entities": []}, {"text": "Nevertheless, they are easily and abundantly available.", "labels": [], "entities": []}, {"text": "In contrast, Markov Logic Networks come with a wealth of theoretical knowledge connected with their usage in principled inference.", "labels": [], "entities": []}, {"text": "However, they are difficult to induce automatically from text or to build manually.", "labels": [], "entities": []}, {"text": "In this paper we explore tables as semistructured knowledge for multiple-choice question (MCQ) answering.", "labels": [], "entities": [{"text": "multiple-choice question (MCQ) answering", "start_pos": 64, "end_pos": 104, "type": "TASK", "confidence": 0.6927600204944611}]}, {"text": "Specifically, we focus on tables that represent general knowledge facts, with cells that contain free-form text (Secton 3 details the nature and semantics of these tables).", "labels": [], "entities": []}, {"text": "The structural properties of tables, along with their free-form text content represents a semi-structured balanced compromise in the trade-off between degree of structure and ubiquity.", "labels": [], "entities": []}, {"text": "We present two main contributions, with tables and their structural properties playing a crucial role in both.", "labels": [], "entities": []}, {"text": "First, we crowd-source a collection of over 9000 MCQs with alignment annotations to table elements, using tables as guidelines in efficient data harvesting.", "labels": [], "entities": []}, {"text": "Second, we develop a feature-driven model that uses these MCQs to perform QA, while factchecking and reasoning over tables.", "labels": [], "entities": []}, {"text": "Others have used tables in the context of QA.", "labels": [], "entities": [{"text": "QA", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.6756044626235962}]}, {"text": "Question bank creation for tables has been investigated, but without structural guidelines or the alignment information that we propose.", "labels": [], "entities": [{"text": "Question bank creation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5631090104579926}]}, {"text": "Similarly, tables have been used in QA reasoning () but have not explicitly attempted to encode all the semantics of table structure (see Section 3.1).", "labels": [], "entities": [{"text": "QA reasoning", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.8746908903121948}]}, {"text": "To the best of our knowledge, no previous work uses tables for both creation and reasoning in a connected framework.", "labels": [], "entities": []}, {"text": "We evaluate our model on MCQ answering for three benchmark datasets.", "labels": [], "entities": []}, {"text": "Our results consistently and significantly outperform a strong retrieval baseline as well as a Markov Logic network model ().", "labels": [], "entities": []}, {"text": "We thus show the benefits of semi-structured data and models over unstructured or highly-structured counterparts.", "labels": [], "entities": []}, {"text": "We also validate our curated MCQ dataset and its annotations as an effective tool for training QA models.", "labels": [], "entities": [{"text": "MCQ dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9225859940052032}]}, {"text": "Finally, we find that our model learns generalizations that permit inference when exact answers may not even be contained in the knowledge base.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created a HIT (the MTurk acronym for Human Intelligence Task) for every non-filler cell (see Section 3) from each one of the 65 manually constructed tables of the Aristo Tablestore.", "labels": [], "entities": [{"text": "HIT", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9466195106506348}, {"text": "Aristo Tablestore", "start_pos": 166, "end_pos": 183, "type": "DATASET", "confidence": 0.93516805768013}]}, {"text": "We paid annotators 10 cents per MCQ, and asked for 1 annotation per HIT for most tables.", "labels": [], "entities": [{"text": "HIT", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.8224777579307556}]}, {"text": "For an initial set of four tables which we used in a pilot study, we asked for three annotations per HIT . We required Turkers to have a HIT approval rating of 95% or higher, with a minimum of at least 500 HITs approved.", "labels": [], "entities": [{"text": "HIT", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8599188327789307}, {"text": "HIT approval rating", "start_pos": 137, "end_pos": 156, "type": "METRIC", "confidence": 0.7782989939053854}]}, {"text": "We restricted the demographics of our workers to the US.", "labels": [], "entities": []}, {"text": "compares our method with other studies conducted at AI2 to generate MCQs.", "labels": [], "entities": []}, {"text": "These methods attempt to generate new MCQs from existing The goal was to obtain diversity in the MCQs created fora target cell.", "labels": [], "entities": []}, {"text": "The results were not sufficiently conclusive to warrant a threefold increase in the cost of creation.", "labels": [], "entities": []}, {"text": "ones, or write them from scratch, but do not involve tables in anyway.", "labels": [], "entities": []}, {"text": "Our annotation procedure leads to faster data creation, with consistent output quality that resulted in the lowest percentage of rejected HITs.", "labels": [], "entities": []}, {"text": "Manual inspection of the generated output also revealed that questions are of consistently good quality.", "labels": [], "entities": []}, {"text": "They are good enough for training machine learning models and many are good enough as evaluation data for QA.", "labels": [], "entities": [{"text": "QA", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.7719572186470032}]}, {"text": "A sample of generated MCQs is presented in.", "labels": [], "entities": []}, {"text": "We implemented some simple checks to evaluate the data before approving HITs.", "labels": [], "entities": [{"text": "HITs", "start_pos": 72, "end_pos": 76, "type": "TASK", "confidence": 0.42145419120788574}]}, {"text": "These included things like checking whether an MCQ has at least three choices and whether choices are repeated.", "labels": [], "entities": []}, {"text": "We had to further prune our data to discard some MCQs due to corrupted data or badly constructed MCQs.", "labels": [], "entities": []}, {"text": "A total of 159 MCQs were lost through the cleanup.", "labels": [], "entities": []}, {"text": "In the end our complete data consists of 9092 MCQs, which is -to the best of our knowledge -orders of magnitude larger than any existing collection of science exam style MCQs available for research.", "labels": [], "entities": []}, {"text": "These MCQs also come with alignment information to tables, rows, columns and cells.", "labels": [], "entities": []}, {"text": "The dataset, bundled together with the Aristo Tablestore, can be freely downloaded 7 .  We train FRETS (Section 5) on the TabMCQ dataset (Section 4) using adaptive gradient descent with an L2 penalty of 1.0 and a mini-batch size of 500 training instances.", "labels": [], "entities": [{"text": "Aristo Tablestore", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.957285612821579}, {"text": "FRETS", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9971662163734436}, {"text": "TabMCQ dataset", "start_pos": 122, "end_pos": 136, "type": "DATASET", "confidence": 0.9789573550224304}]}, {"text": "We train two variants: one consisting of all the features from, the other -a compact model -consisting of the most important features (above a threshold) from the first model by feature-weight.", "labels": [], "entities": []}, {"text": "These features are noted by in the final column of.", "labels": [], "entities": []}, {"text": "We run experiments on three 4th grade science exam MCQ datasets: the publicly available Regents dataset, the larger but unreleased dataset called Monarch, and a third even larger public dataset of Elementary School Science Questions (ESSQ) . For the first two datasets we use the test splits only, since the training sets were directly studied to construct the Aristo Tablestore, which was in turn used to generate our TabMCQ training data.", "labels": [], "entities": [{"text": "4th grade science exam MCQ datasets", "start_pos": 28, "end_pos": 63, "type": "DATASET", "confidence": 0.5373636533816656}, {"text": "Regents dataset", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9465545117855072}, {"text": "Aristo Tablestore", "start_pos": 361, "end_pos": 378, "type": "DATASET", "confidence": 0.8760704398155212}, {"text": "TabMCQ training data", "start_pos": 419, "end_pos": 439, "type": "DATASET", "confidence": 0.8758921027183533}]}, {"text": "On ESSQ we use all the questions since they are independent of the tables.", "labels": [], "entities": [{"text": "ESSQ", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.9247584939002991}]}, {"text": "The Regents test set consists of 129 MCQs, the Monarch test set of 250 MCQs, and ESSQ of 855 MCQs.", "labels": [], "entities": [{"text": "Regents test set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9141790866851807}, {"text": "Monarch test set", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9889850815137228}, {"text": "ESSQ", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9652659296989441}]}, {"text": "Since we are investigating semi-structured models, we compare against two baselines.", "labels": [], "entities": []}, {"text": "The first is an unstructured information retrieval method, which uses the Lucene search engine.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.700053870677948}, {"text": "Lucene search engine", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.95067298412323}]}, {"text": "To apply Lucene to the tables, we ignore their structure and simply use rows as plain-text sentences.", "labels": [], "entities": []}, {"text": "The score for top retrieved hits are used to rank the different choices of MCQs.", "labels": [], "entities": []}, {"text": "The second baseline is the highly-structured Markov-logic Network (MLN) model from as reported in, who use the model as a baseline 10 . Note that achieve a score of 71.3 on Regents Test, which is higher than FRETS' scores (see), but their results are not comparable to ours because they use an ensemble of algorithms.", "labels": [], "entities": [{"text": "Regents Test", "start_pos": 173, "end_pos": 185, "type": "DATASET", "confidence": 0.9385166764259338}, {"text": "FRETS", "start_pos": 208, "end_pos": 213, "type": "DATASET", "confidence": 0.8100831508636475}]}, {"text": "In contrast, we use a single algorithm with a much smaller collection of knowledge.", "labels": [], "entities": []}, {"text": "FRETS rivals the best individual algorithm from their work.", "labels": [], "entities": [{"text": "FRETS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7159124612808228}]}, {"text": "We primarily use the tables from the Aristo Tablestore as knowledge base data in three different settings: with only tables constructed for Regents (40 tables), with only supplementary tables constructed for Monarch (25 tables), and with all ta-) for all our experiments were trained on 300 million words of Newswire English from the monolingual section of the WMT-2011 shared task data.", "labels": [], "entities": [{"text": "Aristo Tablestore", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9504829943180084}, {"text": "Regents", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.937533974647522}, {"text": "Monarch", "start_pos": 208, "end_pos": 215, "type": "DATASET", "confidence": 0.9626849889755249}, {"text": "WMT-2011 shared task data", "start_pos": 361, "end_pos": 386, "type": "DATASET", "confidence": 0.8066651076078415}]}, {"text": "These vectors were improved post-training by retrofitting () them to PPDB (.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9356076717376709}]}, {"text": "The results of these experiments is presented in.", "labels": [], "entities": []}, {"text": "All numbers are reported in percentage accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.876395583152771}]}, {"text": "We perform statistical significance testing on these results using Fisher's exact test with a p-value of 0.05 and report them in our discussions.", "labels": [], "entities": []}, {"text": "First, FRETS -in both full and compact form -consistently outperforms the baselines, often by large margins.", "labels": [], "entities": [{"text": "FRETS", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.640896737575531}]}, {"text": "For Lucene, the improvements overall but the Waterloo corpus baseline are statistically significant.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.8566868305206299}, {"text": "Waterloo corpus baseline", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.9774022301038107}]}, {"text": "Thus FRETS is able to capitalize on data more effectively and rival an unstructured model with access to orders of magnitude more data.", "labels": [], "entities": [{"text": "FRETS", "start_pos": 5, "end_pos": 10, "type": "DATASET", "confidence": 0.8011742830276489}]}, {"text": "For MLN, the improvements are statistically significant in the case of Regents and Regents+Monarch tables.", "labels": [], "entities": [{"text": "MLN", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8378815054893494}, {"text": "Regents", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9008011221885681}, {"text": "Regents+Monarch tables", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.8886687159538269}]}, {"text": "FRETS is thus performing better than a highly structured model while making use of a much simpler data formalism.", "labels": [], "entities": [{"text": "FRETS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7731564044952393}]}, {"text": "Our models are able to effectively generalize.", "labels": [], "entities": []}, {"text": "With Monarch tables, the Lucene baseline is little better than random (25%  Finally, dropping some features in the compact model doesn't always hurt performance, in comparison with the full model.", "labels": [], "entities": [{"text": "Lucene baseline", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.8391017019748688}]}, {"text": "This indicates that potentially higher scores are possible by a principled and detailed feature selection process.", "labels": [], "entities": []}, {"text": "In these experiments the difference between the two FRETS models on equivalent data is statistically insignificant.", "labels": [], "entities": [{"text": "FRETS", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.5839409828186035}]}], "tableCaptions": [{"text": " Table 2: Comparison of different ways of generat- ing MCQs with MTurk.", "labels": [], "entities": [{"text": "generat- ing MCQs", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8810196816921234}, {"text": "MTurk", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8858447670936584}]}, {"text": " Table 5: Evaluation results on three benchmark datasets using different sets of tables as knowledge bases.  Best results on a dataset are highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 5. All numbers are reported in percentage  accuracy. We perform statistical significance test- ing on these results using Fisher's exact test with a  p-value of 0.05 and report them in our discussions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.8204197883605957}, {"text": "statistical significance test- ing", "start_pos": 71, "end_pos": 105, "type": "METRIC", "confidence": 0.7466805219650269}]}]}