{"title": [], "abstractContent": [{"text": "One major drawback of phrase-based translation is that it segments an input sentence into continuous phrases.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7462106347084045}]}, {"text": "To support linguistically informed source discon-tinuity, in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model.", "labels": [], "entities": []}, {"text": "The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase.", "labels": [], "entities": []}, {"text": "We use beam search to combine translations of each subgraph left-to-right to produce a complete translation.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English and German-English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9984546899795532}]}, {"text": "By explicitly modeling the graph segmen-tation, our system obtains further improvement , especially on German-English.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) starts from sequence-based models.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8361628005901972}]}, {"text": "The well-known phrasebased (PB) translation model ( has significantly advanced the progress of SMT by extending translation units from single words to phrases.", "labels": [], "entities": [{"text": "phrasebased (PB) translation", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6241517722606659}, {"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9957771301269531}]}, {"text": "By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion.", "labels": [], "entities": [{"text": "word insertion", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.7678888738155365}]}, {"text": "However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . .", "labels": [], "entities": []}, {"text": "pas to English not cannot be learned.", "labels": [], "entities": []}, {"text": "To solve this, syntax-based models () take tree structures into consideration to learn translation patterns by using non-terminals for generalization.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on Chinese-English (ZH-EN) and German-English (DE-EN) translation tasks.", "labels": [], "entities": [{"text": "German-English (DE-EN) translation tasks", "start_pos": 54, "end_pos": 94, "type": "TASK", "confidence": 0.629212886095047}]}, {"text": "). We implement a Treelet model in Moses which produces translations from left to right and uses beam search for decoding.", "labels": [], "entities": []}, {"text": "DTU extends the PB model by allowing discontinuous phrases (.", "labels": [], "entities": [{"text": "DTU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9457907676696777}]}, {"text": "We implement DTU with source discontinuity in Moses.", "labels": [], "entities": []}, {"text": "GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT.", "labels": [], "entities": [{"text": "GBMT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.967652440071106}, {"text": "GSM", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.8588044047355652}, {"text": "GBMT", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9342198967933655}]}, {"text": "Both systems are implemented in Moses.", "labels": [], "entities": []}, {"text": "Word alignment is performed by GIZA++ ( with the heuristic function growdiag-final-and.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7246755361557007}]}, {"text": "We use SRILM) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting  Each score is an average over three MIRA runs).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.8527345061302185}, {"text": "English Gigaword corpus 5th edition", "start_pos": 76, "end_pos": 111, "type": "DATASET", "confidence": 0.8105961978435516}, {"text": "MIRA", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9360707402229309}]}, {"text": "* means a system is significantly better than PBMT at p \u2264 0.01.", "labels": [], "entities": []}, {"text": "Bold figures mean a system is significantly better than Treelet at p \u2264 0.01.", "labels": [], "entities": []}, {"text": "+ means a system is significantly better than DTU at p \u2264 0.01.", "labels": [], "entities": [{"text": "DTU", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.856349527835846}]}, {"text": "In this table, we mark a system by comparing it with previous ones.", "labels": [], "entities": []}, {"text": "We find that our GBMT system is significantly better than PBMT as measured by all three metrics across all test sets.", "labels": [], "entities": []}, {"text": "Specifically, the improvements are up to +1.5/+0.5 BLEU, +0.3/+0.2 METEOR, and -0.8/-0.4 TER on ZH-EN and DE-EN, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9987610578536987}, {"text": "METEOR", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9982683658599854}, {"text": "TER", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9979612827301025}, {"text": "ZH-EN", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.8893598318099976}, {"text": "DE-EN", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.5929766893386841}]}, {"text": "This improvement is reasonable as our system allows discontinuous phrases which can reduce data sparsity and handle long-distance relations.", "labels": [], "entities": []}, {"text": "Another argument for discontinuous phrases is that they allow the decoder to use larger translation units which tend to produce better translations (.", "labels": [], "entities": []}, {"text": "However, this argument was only verified on ZH-EN.", "labels": [], "entities": [{"text": "ZH-EN", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.9650279879570007}]}, {"text": "Therefore, we are interested in seeing whether we have the same observation in our experiments on both language pairs.", "labels": [], "entities": []}, {"text": "We count the used translation rules in MT02 and WMT11 based on different target lengths.", "labels": [], "entities": [{"text": "MT02", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.787270724773407}, {"text": "WMT11", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8827900886535645}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We find that both DTU and GBMT indeed tend to use larger translation units on ZH-EN.", "labels": [], "entities": [{"text": "GBMT", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.5176936984062195}]}, {"text": "However, more smaller translation units are used on DE-EN.", "labels": [], "entities": [{"text": "DE-EN", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.963707685470581}]}, {"text": "We presume this is because long-distance reordering is performed more often on ZH-EN than on DE-EN.", "labels": [], "entities": []}, {"text": "Based on the fact that the distortion function d measures the reordering distance, we find that the average distortion value in PB on ZH-EN MT02 is 18.4 and We have the same finding on all test sets.: The number of rules in DTU and GBMT.", "labels": [], "entities": [{"text": "ZH-EN MT02", "start_pos": 134, "end_pos": 144, "type": "DATASET", "confidence": 0.8617582321166992}, {"text": "DTU", "start_pos": 224, "end_pos": 227, "type": "DATASET", "confidence": 0.8163312077522278}, {"text": "GBMT", "start_pos": 232, "end_pos": 236, "type": "DATASET", "confidence": 0.842261016368866}]}], "tableCaptions": [{"text": " Table 2: The number of sentences in our corpora.", "labels": [], "entities": []}, {"text": " Table 3: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN).", "labels": [], "entities": []}, {"text": " Table 5: BLEU scores of a Moses hierarchi- cal phrase-based system (HPB) and our system  (GBMT) with a word-based lexical reordering  model (LR).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978499412536621}]}]}