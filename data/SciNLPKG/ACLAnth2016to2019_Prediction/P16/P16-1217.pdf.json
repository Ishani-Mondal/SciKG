{"title": [{"text": "Automatic Labeling of Topic Models Using Text Summaries", "labels": [], "entities": [{"text": "Automatic Labeling of Topic Models", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7878363132476807}, {"text": "Text Summaries", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7228855490684509}]}], "abstractContent": [{"text": "Labeling topics learned by topic models is a challenging problem.", "labels": [], "entities": [{"text": "Labeling topics learned", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9078852335611979}]}, {"text": "Previous studies have used words, phrases and images to label topics.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use text summaries for topic labeling.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7455682158470154}]}, {"text": "Several sentences are extracted from the most related documents to form the summary for each topic.", "labels": [], "entities": []}, {"text": "In order to obtain summaries with both high relevance, coverage and discrimination for all the topics, we propose an algorithm based on sub-modular optimization.", "labels": [], "entities": [{"text": "coverage", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9729844927787781}]}, {"text": "Both automatic and manual analysis have been conducted on two real document collections, and we find 1) the summaries extracted by our proposed algorithm are superior over the summaries extracted by existing popular summarization methods; 2) the use of summaries as labels has obvious advantages over the use of words and phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical topic modelling plays very important roles in many research areas, such as text mining, natural language processing and information retrieval.", "labels": [], "entities": [{"text": "Statistical topic modelling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8183582623799642}, {"text": "text mining", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.8384338319301605}, {"text": "natural language processing", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.628908654054006}, {"text": "information retrieval", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.8135299980640411}]}, {"text": "Popular topic modeling techniques include Latent Dirichlet Allocation (LDA) ( and Probabilistic Latent Semantic Analysis (pLSA)).", "labels": [], "entities": []}, {"text": "These techniques can automatically discover the abstract \"topics\" that occur in a collection of documents.", "labels": [], "entities": []}, {"text": "They model the documents as a mixture of topics, and each topic is modeled as a probability distribution over words.", "labels": [], "entities": []}, {"text": "Although the discovered topics' word distributions are sometimes intuitively meaningful, a major challenge shared by all such topic models is to accurately interpret the meaning of each topic (.", "labels": [], "entities": []}, {"text": "The interpretation of each topic is very important when people want to browse, understand and leverage the topic.", "labels": [], "entities": []}, {"text": "However, it is usually very hard fora user to understand the discovered topics based only on the multinomial distribution of words.", "labels": [], "entities": []}, {"text": "For example, here are the top terms fora discovered topic: {fire miles area north southern people coast homes south damage northern river state friday central water rain high california weather}.", "labels": [], "entities": []}, {"text": "It is not easy fora user to fully understand this topic if the user is not very familiar with the document collection.", "labels": [], "entities": []}, {"text": "The situation may become worse when the user faces with a number of discovered topics and the sets of top terms of the topics are often overlapping with each other on many practical document collections.", "labels": [], "entities": []}, {"text": "In order to address the above challenge, a few previous studies have proposed to use phrases, concepts and even images for labeling the discovered topics (.", "labels": [], "entities": []}, {"text": "For example, we may automatically extract the phrase \"southern california\" to represent the example topic mentioned earlier.", "labels": [], "entities": []}, {"text": "These topic labels can help the user to understand the topics to some extent.", "labels": [], "entities": []}, {"text": "However, the use of phrases or concepts as topic labels are not very satisfactory in practice, because the phrases or concepts are still very short, and the information expressed in these short labels is not adequate for user's understanding.", "labels": [], "entities": []}, {"text": "The case will become worse when some ambiguous phrase is used or multiple discrete phrases with poor coherence are used fora topic.", "labels": [], "entities": []}, {"text": "To address the drawbacks of the above short labels, we need to provide more contextual information and consider using long text descriptions to represent the topics.", "labels": [], "entities": []}, {"text": "The long text descriptions can be used independently or used as beneficial complement to the short labels.", "labels": [], "entities": []}, {"text": "For example, below is part of the summary label produced by our proposed method and it provides much more contextual information for understanding the topic.", "labels": [], "entities": []}, {"text": "Showers and thunderstorms developed in parched areas of the southeast , from western north carolina into south central alabama , north central and northeast texas and the central and southern gulf coast . \u2026 The quake was felt over a large area , extending from santa rosa , about 60 miles north of san francisco , to the santa cruz area 70 miles to the south \u2026.", "labels": [], "entities": []}, {"text": "Fourteen homes were destroyed in baldwin park 20 miles northeast of downtown los angeles and five were damaged along with five commercial buildings when 75 mph gusts snapped power lines , igniting afire at allan paper co.", "labels": [], "entities": []}, {"text": ", fire officials said . \u2026 The contributions of this paper are summarized as follows: 1) We are the first to invesitage using text summaries for topic labeling; 2) We propose a summarization algorithm based on submodular optimization to extract summaries with both high relevance, coverage and discrimination for all topics.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 144, "end_pos": 158, "type": "TASK", "confidence": 0.6944324523210526}, {"text": "summarization", "start_pos": 176, "end_pos": 189, "type": "TASK", "confidence": 0.9684514403343201}]}, {"text": "3) Automatic and manual analysis reveals the usefulness and advantages of the summaries produced by our algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used two document collections as evaluation datasets, as in (): AP news and SIGMOD proceedings.", "labels": [], "entities": [{"text": "AP news", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9226032495498657}]}, {"text": "The AP news dataset contains a set of 2250 AP news articles, which are provided by TREC.", "labels": [], "entities": [{"text": "AP news dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.962863008181254}, {"text": "TREC", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.936448335647583}]}, {"text": "There is a total of 43803 sentences in the AP news dataset and the vocabulary size is 37547 (after removing stop words).", "labels": [], "entities": [{"text": "AP news dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9714173277219137}]}, {"text": "The SIGMOD proceeding dataset contains a set of 2128 abstracts of SIGMOD proceedings between the year 1976 and 2015, downloaded from the ACM digital library.", "labels": [], "entities": [{"text": "SIGMOD proceeding dataset", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.7560315529505411}, {"text": "ACM digital library", "start_pos": 137, "end_pos": 156, "type": "DATASET", "confidence": 0.9517602523167928}]}, {"text": "There is a total of 15211sen-tences in the SIGMOD proceeding dataset and the vocabulary size is 13688.", "labels": [], "entities": [{"text": "SIGMOD proceeding dataset", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.8702219128608704}]}, {"text": "For topic modeling, we adopted the most popular LDA to discover topics in the two datasets, respectively.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.900944709777832}]}, {"text": "Particularly, we used the LDA module implemented in the MALLET toolkit . Without loss of generality, we extracted 25 topics from the AP news dataset and 25 topics from the SIGMOD proceeding dataset.", "labels": [], "entities": [{"text": "MALLET toolkit", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.8213242292404175}, {"text": "AP news dataset", "start_pos": 133, "end_pos": 148, "type": "DATASET", "confidence": 0.9654731551806132}, {"text": "SIGMOD proceeding dataset", "start_pos": 172, "end_pos": 197, "type": "DATASET", "confidence": 0.814247747262319}]}, {"text": "The parameter values of our proposed summarization method is either directly borrowed from previous works or empirically set as follows: \u00ed \u00b5\u00ed\u00bb\u00bc = 0.05, \u00ed \u00b5\u00ed\u00bb\u00bd = 250, \u00ed \u00b5\u00ed\u00bb\u00be = 300 and \u00ed \u00b5\u00ed\u00bc\u0080 = 0.15. 1 http://mallet.cs.umass.edu/ We have two goals in the evaluation: comparison of different summarization methods for topic labeling, and comparison of different kinds of labels (summaries, words, and phrases).", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9704758524894714}]}, {"text": "In particular, we compare our proposed summarization method (denoted as Our Method) with the following typical summarization methods and all of them extract summaries from the same candidate sentence set for each topic: MEAD: It uses a heuristic way to obtain each sentence's score by summing the scores based on different features ( ): centroidbased weight, position and similarity with first sentence.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9729328751564026}, {"text": "MEAD", "start_pos": 220, "end_pos": 224, "type": "METRIC", "confidence": 0.9835774898529053}]}, {"text": "LexRank: It constructs a graph based on the sentences and their similarity relationships and then applies the PageRank algorithm for sentence ranking (.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9120849967002869}]}, {"text": "TopicLexRank: It is an improved version of LexRank by considering the probability distribution of top 500 words in a topic as a prior vector, and then applies the topic-sensitive PageRank algorithm for sentence ranking, similar to.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9095214009284973}, {"text": "sentence ranking", "start_pos": 202, "end_pos": 218, "type": "TASK", "confidence": 0.751986563205719}]}, {"text": "We also compare the following three different kinds of labels: Word label: It shows ten topic words as labels for each topic, which is the most intuitive interpretation of the topic.", "labels": [], "entities": []}, {"text": "Phrase label: It uses three phrases as labels for each topic, and the phrase labels are extracted by using the method proposed in (, which is very closely related to our work and considered a strong baseline in this study.", "labels": [], "entities": [{"text": "Phrase label", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.764243870973587}]}, {"text": "Summary Label: It uses a topic summary with a length of 250 words to label each topic and the summary is produced by our proposed method.", "labels": [], "entities": []}, {"text": "word label: user information attribute model privacy quality record result individual provide phrase label: data set ; data analysis ; data integration summary label: An essential element for privacy metric is the measure of how much adversaries can know about an individual ' sensitive attribute ( sa ) if they know the individual ' quasi-identifier ( qi) \u2026.We present an automated solution that elicit user preference on attribute and value , employing different disambiguation technique ranging from simple keyword matching , to more sophisticated probabilistic model \u2026.Privgene need significantly less perturbation than previous method , and it achieve higher overall result quality , even for model fitting task where ga is not the first choice without privacy consideration \u2026.", "labels": [], "entities": [{"text": "data integration summary label", "start_pos": 135, "end_pos": 165, "type": "TASK", "confidence": 0.772060863673687}, {"text": "model fitting task", "start_pos": 698, "end_pos": 716, "type": "TASK", "confidence": 0.8022857507069906}]}], "tableCaptions": [{"text": " Table 2. We can  see that our method has almost the best coverage  ratio and the produced summary can cover most  important words in a topic.", "labels": [], "entities": []}, {"text": " Table 3. Comparison of the average and max similar- ity between different topic summaries", "labels": [], "entities": []}, {"text": " Table 4. Manual comparison of different summariza- tion methods on AP news dataset", "labels": [], "entities": [{"text": "AP news dataset", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9040878812472025}]}, {"text": " Table 5. Manual comparison of different summariza- tion methods on SIGMOD proceeding dataset", "labels": [], "entities": [{"text": "SIGMOD proceeding dataset", "start_pos": 68, "end_pos": 93, "type": "DATASET", "confidence": 0.7441095312436422}]}, {"text": " Table 6. Manual comparison of different kinds of la- bels on AP news dataset", "labels": [], "entities": [{"text": "AP news dataset", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.9582737882932028}]}, {"text": " Table 7. Manual comparison of different kinds of la- bels on AP news dataset", "labels": [], "entities": [{"text": "AP news dataset", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.959221621354421}]}]}