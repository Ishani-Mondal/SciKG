{"title": [{"text": "Controlled and Balanced Dataset for Japanese Lexical Simplification", "labels": [], "entities": [{"text": "Japanese Lexical Simplification", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.6604150036970774}]}], "abstractContent": [{"text": "We propose anew dataset for evaluating a Japanese lexical simplification method.", "labels": [], "entities": [{"text": "Japanese lexical simplification", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.5687828958034515}]}, {"text": "Previous datasets have several deficiencies.", "labels": [], "entities": []}, {"text": "All of them substitute only a single target word, and some of them extract sentences only from newswire corpus.", "labels": [], "entities": []}, {"text": "In addition, most of these datasets do not allow ties and integrate simplification ranking from all the annotators without considering the quality.", "labels": [], "entities": []}, {"text": "In contrast, our dataset has the following advantages: (1) it is the first controlled and balanced dataset for Japanese lexical simplification with high correlation with human judgment and (2) the consistency of the simplification ranking is improved by allowing candidates to have ties and by considering the reliability of annotators.", "labels": [], "entities": [{"text": "Japanese lexical simplification", "start_pos": 111, "end_pos": 142, "type": "TASK", "confidence": 0.6928135752677917}, {"text": "consistency", "start_pos": 197, "end_pos": 208, "type": "METRIC", "confidence": 0.9879497289657593}]}], "introductionContent": [{"text": "Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.87929767370224}]}, {"text": "We define complex word as a word that has lexical and subjective difficulty in a sentence.", "labels": [], "entities": []}, {"text": "It can help in reading comprehension for children and language learners.", "labels": [], "entities": [{"text": "reading comprehension", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8096197247505188}]}, {"text": "This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context (.", "labels": [], "entities": []}, {"text": "Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9633342623710632}]}, {"text": "However, few resources are available for the automatic evaluation of lexical simplification. and De Belder and Moens (2010) created benchmark datasets for evaluating English lexical simplification.", "labels": [], "entities": [{"text": "lexical simplification.", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.6942174881696701}, {"text": "English lexical simplification", "start_pos": 166, "end_pos": 196, "type": "TASK", "confidence": 0.5962528089682261}]}, {"text": "In addition, extracted simplification candidates and constructed an evaluation dataset using English Wikipedia and Simple English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.9384061098098755}, {"text": "Simple English Wikipedia", "start_pos": 115, "end_pos": 139, "type": "DATASET", "confidence": 0.726037601629893}]}, {"text": "In contrast, such a parallel corpus does not exist in Japanese.", "labels": [], "entities": []}, {"text": "constructed an evaluation dataset for Japanese lexical simplification 1 in languages other than English.", "labels": [], "entities": [{"text": "Japanese lexical simplification", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.7084612647692362}]}, {"text": "However, there are four drawbacks in the dataset of: (1) they extracted sentences only from a newswire corpus; (2) they substituted only a single target word; (3) they did not allow ties; and (4) they did not integrate simplification ranking considering the quality.", "labels": [], "entities": []}, {"text": "Hence, we propose anew dataset addressing the problems in the dataset of.", "labels": [], "entities": []}, {"text": "The main contributions of our study are as follows: \u2022 It is the first controlled and balanced dataset for Japanese lexical simplification.", "labels": [], "entities": [{"text": "Japanese lexical simplification", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.772285004456838}]}, {"text": "We extract sentences from a balanced corpus and control sentences to have only one complex word.", "labels": [], "entities": []}, {"text": "Experimental results show that our dataset is more suitable than previous datasets for evaluating systems with respect to correlation with human judgment.", "labels": [], "entities": []}, {"text": "\u2022 The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators.", "labels": [], "entities": [{"text": "consistency", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9962323307991028}]}, {"text": "Our dataset is available at GitHub 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Japanese lexical simplification Kajiwara and Yamamoto (2015) followed to construct an evaluation dataset for Japanese lexical simplification.", "labels": [], "entities": [{"text": "Japanese lexical simplification", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.6816899577776591}]}, {"text": "Namely, they split the data creation process into two steps: substitute extraction and simplification ranking.", "labels": [], "entities": [{"text": "data creation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.72207972407341}, {"text": "substitute extraction", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7278028726577759}, {"text": "simplification ranking", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.8375210464000702}]}, {"text": "During the substitute extraction task, they collected substitutes of each target word in 10 different contexts.", "labels": [], "entities": [{"text": "substitute extraction task", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.8567099571228027}]}, {"text": "These contexts were randomly selected from a newswire corpus.", "labels": [], "entities": []}, {"text": "The target word was a content word (noun, verb, adjective, or adverb), and was neither a simple word nor part of any compound words.", "labels": [], "entities": []}, {"text": "They gathered substitutes from five annotators using crowdsourcing.", "labels": [], "entities": []}, {"text": "These procedures were the same as for De.", "labels": [], "entities": [{"text": "De.", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.7268606722354889}]}, {"text": "During the simplification ranking task, annotators were asked to reorder the target word and its substitutes in a single order without allowing ties.", "labels": [], "entities": []}, {"text": "They used crowdsourcing to find five annotators different from those who performed the substitute extraction task.", "labels": [], "entities": [{"text": "substitute extraction task", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7957208355267843}]}, {"text": "Simplification ranking was integrated on the basis of the average of the simplification ranking from each annotator to generate a gold-standard ranking that might include ties.", "labels": [], "entities": []}, {"text": "During the substitute extraction task, agreement among the annotators was 0.664, whereas during the simplification ranking task, Spearman's rank correlation coefficient score was 0.332.", "labels": [], "entities": [{"text": "substitute extraction task", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.779585083325704}, {"text": "agreement", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9972816705703735}, {"text": "rank correlation coefficient score", "start_pos": 140, "end_pos": 174, "type": "METRIC", "confidence": 0.8468151092529297}]}, {"text": "Spearman's score of this work was lower than that of by 0.064.", "labels": [], "entities": []}, {"text": "Thus, there was a big blur between annotators, and the simplification ranking collected using crowdsourcing tended to have a lower quality.", "labels": [], "entities": []}, {"text": "(feel exalted)\" is simplified, another complex word \" (skill)\" is left in a sentence.", "labels": [], "entities": []}, {"text": "Lexical simplification is a task of simplifying complex words in a sentence.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8814228177070618}]}, {"text": "Previous datasets may include multiple complex words in a sentence but target only one complex word.", "labels": [], "entities": []}, {"text": "Not only the target word but also other complex words should be considered as well, but annotation of substitutes and simplification ranking to all complex words in a sentence produces a huge number of patterns, therefore takes a very high cost of annotation.", "labels": [], "entities": []}, {"text": "For example, when three complex words which have 10 substitutes each in a sentence, annotators should consider 10 3 patterns.", "labels": [], "entities": []}, {"text": "Thus, it is desired that a sentence includes only simple words after the target word is substituted.", "labels": [], "entities": []}, {"text": "Therefore, in this work, we extract sentences containing only one complex word.", "labels": [], "entities": []}, {"text": "Ties are not permitted in simplification ranking.", "labels": [], "entities": [{"text": "simplification ranking", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9276591539382935}]}, {"text": "When each annotator assigns a simplification ranking to a substitution list, a tie cannot be assigned in previous datasets (.", "labels": [], "entities": []}, {"text": "This deteriorates ranking consistency if some substitutes have a similar simplicity.", "labels": [], "entities": [{"text": "consistency", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.7636224031448364}]}, {"text": "De Belder and Moens (2012) allow ties in simplification ranking and report considerably higher agreement among annotators than.", "labels": [], "entities": []}, {"text": "The method of ranking integration is na\u00a8\u0131vena\u00a8\u0131ve. and use an average score to integrate rankings, but it might be biased by outliers.", "labels": [], "entities": [{"text": "ranking integration", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7660709321498871}]}, {"text": "De Belder and Moens (2012) report a slight increase in agreement by greedily removing annotators to maximize the agreement score.", "labels": [], "entities": [{"text": "agreement", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9868764877319336}]}, {"text": "We create a balanced dataset for the evaluation of Japanese lexical simplification.", "labels": [], "entities": [{"text": "Japanese lexical simplification", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6256271302700043}]}, {"text": "to perform substitute extraction, substitute evaluation, and substitute ranking.", "labels": [], "entities": [{"text": "substitute extraction", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.7751571536064148}, {"text": "substitute evaluation", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.8264544606208801}, {"text": "substitute ranking", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.7274050712585449}]}, {"text": "In each task, we requested the annotators to complete at least 95% of their previous assignments correctly.", "labels": [], "entities": []}, {"text": "They were native Japanese speakers.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the ranking integration, the Spearman rank correlation coefficient was calculated.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 56, "end_pos": 93, "type": "METRIC", "confidence": 0.7721549645066261}]}, {"text": "The baseline integration ranking used an average score (.", "labels": [], "entities": []}, {"text": "Our proposed method excludes outlier annotators by using a reliability score calculated using the method developed by.", "labels": [], "entities": [{"text": "reliability score", "start_pos": 59, "end_pos": 76, "type": "METRIC", "confidence": 0.952633798122406}]}, {"text": "Pairwise agreement is calculated between each pair of sets (p 1 , p 2 \u2208 P ) from all the possible pairings (P) (Equation 1).", "labels": [], "entities": [{"text": "Pairwise agreement", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.8214811384677887}]}, {"text": "The agreement among annotators from the substitute evaluation phase was 0.669, and agreement among the students is 0.673, which is similar to the level found in crowdsourcing.", "labels": [], "entities": [{"text": "agreement", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9843413233757019}]}, {"text": "This score is almost the same as that from    the Spearman rank correlation coefficient of the substitute ranking phase was 0.522.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 50, "end_pos": 87, "type": "METRIC", "confidence": 0.6906811222434044}]}, {"text": "This score is higher than that from by 0.190.", "labels": [], "entities": []}, {"text": "This clearly shows the importance of allowing ties during the substitute ranking task.", "labels": [], "entities": []}, {"text": "shows the results of the ranking integration.", "labels": [], "entities": []}, {"text": "Our method achieved better accuracy in ranking integration than previous methods and is similar to the results from De.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9992480874061584}, {"text": "ranking integration", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8085216283798218}]}, {"text": "This shows that the reliability score can be used for improving the quality.", "labels": [], "entities": [{"text": "reliability score", "start_pos": 20, "end_pos": 37, "type": "METRIC", "confidence": 0.9807922840118408}]}, {"text": "shows the number of sentences and average substitutes in each genre.", "labels": [], "entities": []}, {"text": "In our dataset, the number of acquired substitutes is 8,636 words and the average number of substitutes is 4.30 words per sentence.", "labels": [], "entities": []}, {"text": "illustrates apart of our dataset.", "labels": [], "entities": []}, {"text": "Substitutes that include particles are found in 75 context (3.7%).", "labels": [], "entities": []}, {"text": "It is shown that if particles are not permitted in substitutes, we obtain only two substitutes (4 and 7).", "labels": [], "entities": []}, {"text": "By permitting substitutes to include particles, we are able to obtain 7 substitutes.", "labels": [], "entities": []}, {"text": "In ranking substitutes, Spearman rank correlation coefficient is 0.729, which is substantially higher than crowdsourcing's score.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 24, "end_pos": 61, "type": "METRIC", "confidence": 0.7628078311681747}]}, {"text": "Thus, it is necessary to consider annotation method.", "labels": [], "entities": []}, {"text": "In this section, we evaluate our dataset using five simple lexical simplification methods.", "labels": [], "entities": []}, {"text": "We calcu- or those of our dataset.", "labels": [], "entities": []}, {"text": "We ranked substitutes according to the metrics, and calculated the 1-best accuracy for each target word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.8805542588233948}]}, {"text": "Finally, to compare two datasets, we used the Pearson product-moment correlation coefficient between our dataset and the dataset of against the annotated data.", "labels": [], "entities": [{"text": "Pearson product-moment correlation coefficient", "start_pos": 46, "end_pos": 92, "type": "METRIC", "confidence": 0.9308097958564758}]}, {"text": "shows the result of this experiment.", "labels": [], "entities": []}, {"text": "The Pearson coefficient shows that our dataset correlates with human annotation better than the dataset of, possibly because we controlled each sentence to include only one complex word.", "labels": [], "entities": [{"text": "Pearson coefficient", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9604683518409729}]}, {"text": "Because our dataset is balanced, the accuracy of Web corpus-based metrics (Frequency and Number of Users) closer than the dataset of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995543360710144}, {"text": "Frequency", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9193073511123657}]}], "tableCaptions": [{"text": " Table 1: Comparison of the datasets. In this work, nouns include sahen nouns, verbs include sahen verbs,  and adjectives include adjectival nouns.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy and correlation of the datasets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983701109886169}, {"text": "correlation", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9806586503982544}]}]}