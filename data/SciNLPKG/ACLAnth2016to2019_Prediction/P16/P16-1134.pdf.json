{"title": [{"text": "Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields", "labels": [], "entities": [{"text": "Segment-Level Sequence Modeling", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9320163130760193}]}], "abstractContent": [{"text": "Most of the sequence tagging tasks in natural language processing require to recognize segments with certain syntactic role or semantic meaning in a sentence.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7100376188755035}]}, {"text": "They are usually tackled with Conditional Random Fields (CRFs), which do indirect word-level modeling over word-level features and thus cannot make full use of segment-level information.", "labels": [], "entities": []}, {"text": "Semi-Markov Conditional Random Fields (Semi-CRFs) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem.", "labels": [], "entities": []}, {"text": "This paper presents Gated Recursive Semi-CRFs (grSemi-CRFs), which model segments directly and automatically learn segment-level features through a gated recursive convolutional neural network.", "labels": [], "entities": []}, {"text": "Our experiments on text chunking and named entity recognition (NER) demonstrate that grSemi-CRFs generally outperform other neural models.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.7577192485332489}, {"text": "named entity recognition (NER)", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.7817039291063944}]}], "introductionContent": [{"text": "Most of the sequence tagging tasks in natural language processing (NLP) are segment-level tasks, such as text chunking and named entity recognition (NER), which require to recognize segments (i.e., a set of continuous words) with certain syntactic role or semantic meaning in a sentence.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7494725584983826}, {"text": "natural language processing (NLP)", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.7468032042185465}, {"text": "text chunking", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.7042972445487976}, {"text": "named entity recognition (NER)", "start_pos": 123, "end_pos": 153, "type": "TASK", "confidence": 0.8042682111263275}]}, {"text": "These tasks are usually tackled with Conditional Random Fields (CRFs) (), which do word-level modeling as putting each word a tag, by using some predefined tagging schemes, e.g., the \"IOB\" scheme.", "labels": [], "entities": []}, {"text": "Such tagging schemes are lossy transformations of original segment tags: They do indicate the boundary of adjacent segments but lose the length information of segments to some extent.", "labels": [], "entities": []}, {"text": "Besides, CRFs can only employ word-level features, which are either hand-crafted or extracted with deep neural networks, such as window-based neural networks) and bidirectional Long Short-Term Memory networks (BI-LSTMs) ( ).", "labels": [], "entities": [{"text": "CRFs", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.9522948265075684}]}, {"text": "Therefore, CRFs cannot make full use of segmentlevel information, such as inner properties of segments, which cannot be fully encoded in wordlevel features.", "labels": [], "entities": []}, {"text": "Semi-Markov Conditional Random Fields (Semi-CRFs) () are proposed to model segments directly and thus readily utilize segment-level features that encode useful segment information.", "labels": [], "entities": []}, {"text": "Existing work has shown that Semi-CRFs outperform CRFs on segment-level tagging tasks such as sequence segmentation), NER (), web data extraction () and opinion extraction.", "labels": [], "entities": [{"text": "segment-level tagging tasks", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.7233718633651733}, {"text": "sequence segmentation", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.6378153562545776}, {"text": "web data extraction", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7074254949887594}, {"text": "opinion extraction", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.8473575115203857}]}, {"text": "However, Semi-CRFs need many more features compared to CRFs as they need to model segments with different lengths.", "labels": [], "entities": []}, {"text": "As manually designing the features is tedious and often incomplete, how to automatically extract good features becomes a very important problem for Semi-CRFs.", "labels": [], "entities": []}, {"text": "A naive solution that builds multiple feature extractors, each of which extracts features for segments with a specific length, is apparently time-consuming.", "labels": [], "entities": []}, {"text": "Moreover, some of these separate extractors may underfit as the segments with specific length maybe very rare in the training data.", "labels": [], "entities": []}, {"text": "By far, SemiCRFs are lacking of an automatic segment-level feature extractor.", "labels": [], "entities": [{"text": "segment-level feature extractor", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6281651059786478}]}, {"text": "In this paper, we fill the research void by proposing Gated Recursive Semi-Markov Conditional Random Fields (grSemi-CRFs), which can automatically learn features for segment-level sequence tagging tasks.", "labels": [], "entities": [{"text": "segment-level sequence tagging tasks", "start_pos": 166, "end_pos": 202, "type": "TASK", "confidence": 0.6753980219364166}]}, {"text": "Unlike previous approaches which usually use a neural-based feature extractor with a CRF layer, a grSemi-CRF consists of a gated recursive convolutional neural network (grConv) () with a Semi-CRF layer.", "labels": [], "entities": []}, {"text": "The grConv is a variant of recursive neural networks.", "labels": [], "entities": []}, {"text": "It builds a pyramid-like structure to extract segment-level features in a hierarchical way.", "labels": [], "entities": []}, {"text": "This feature hierarchy well matches the intuition that long segments are combinations of their short sub-segments.", "labels": [], "entities": []}, {"text": "This idea was first explored in to build an encoder in neural machine translation and then extended to solve other problems, such as sentence-level classification ( and Chinese word segmentation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.7261917591094971}, {"text": "sentence-level classification", "start_pos": 133, "end_pos": 162, "type": "TASK", "confidence": 0.7345870137214661}, {"text": "Chinese word segmentation", "start_pos": 169, "end_pos": 194, "type": "TASK", "confidence": 0.60712398091952}]}, {"text": "The advantages of grSemi-CRFs are two folds.", "labels": [], "entities": []}, {"text": "First, thanks to the pyramid architecture of grConvs, grSemi-CRFs can extract all the segmentlevel features using one single feature extractor, and there is no underfitting problem as all parameters of the feature extractor are shared globally.", "labels": [], "entities": []}, {"text": "Besides, unlike recurrent neural network (RNN) models, the training and inference of grSemiCRFs are very fast as there is no time dependency and all the computations can be done in parallel.", "labels": [], "entities": []}, {"text": "Second, thanks to the semi-Markov structure of Semi-CRFs, grSemi-CRFs can model segments in sentences directly without the need to introduce extra tagging schemes, which solves the problem that segment length information cannot be fully encoded in tags.", "labels": [], "entities": []}, {"text": "Besides, grSemi-CRFs can also utilize segment-level features which can flexibly encode segment-level information such as inner properties of segments, compared to word-level features as used in CRFs.", "labels": [], "entities": []}, {"text": "By combining grConvs with Semi-CRFs, we propose anew way to automatically extract segment-level features for SemiCRFs.", "labels": [], "entities": []}, {"text": "Our major contributions can be summarized as: (1) We propose grSemi-CRFs, which solve both the automatic feature extraction problem for Semi-CRFs and the indirect word-level modeling problem in CRFs.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7122564762830734}]}, {"text": "As a result, grSemiCRFs can do segment-level modeling directly and make full use of segment-level features; (2) We evaluate grSemi-CRFs on two segmentlevel sequence tagging tasks, text chunking and NER.", "labels": [], "entities": [{"text": "segmentlevel sequence tagging", "start_pos": 143, "end_pos": 172, "type": "TASK", "confidence": 0.638990968465805}, {"text": "text chunking", "start_pos": 180, "end_pos": 193, "type": "TASK", "confidence": 0.7664368152618408}]}, {"text": "Experimental results show the effectiveness of our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate grSemi-CRFs on two segment-level sequence tagging NLP tasks: text chunking and named entity recognition (NER).", "labels": [], "entities": [{"text": "segment-level sequence tagging NLP", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.7299217283725739}, {"text": "text chunking", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.7279741764068604}, {"text": "named entity recognition (NER)", "start_pos": 91, "end_pos": 121, "type": "TASK", "confidence": 0.75927205880483}]}, {"text": "For text chunking, we use the CONLL 2000 text chunking shared dataset 6), in which the objective is to divide the whole sentence into different segments according to their syntactic roles, such as noun phrases (\"NP\"), verb phrases (\"VP\") and adjective phrases (\"ADJP\").", "labels": [], "entities": [{"text": "text chunking", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7437707185745239}, {"text": "CONLL 2000 text chunking shared dataset 6", "start_pos": 30, "end_pos": 71, "type": "DATASET", "confidence": 0.8421487893377032}]}, {"text": "We call it a \"segment-rich\" tasks as the number of phrases are much higher than that of non-phrases which is tagged with others (\"O\").", "labels": [], "entities": []}, {"text": "We evaluate performance overall the chunks instead of only noun pharse (NP) chunks.", "labels": [], "entities": []}, {"text": "For NER, we use the CONLL 2003 named entity recognition shared dataset 7, in which segments are tagged with one of four entity types: person (\"PER\"), location (\"LOC\"), organization (\"ORG\") and miscellaneous(\"MISC\"), or others (\"O\") which is used to denote non-entities.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8536443114280701}, {"text": "CONLL 2003 named entity recognition shared dataset 7", "start_pos": 20, "end_pos": 72, "type": "DATASET", "confidence": 0.8534630388021469}]}, {"text": "We call it a \"segment-sparse\" task as entities are rare while non-entities are common.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyperparameter settings for our model.", "labels": [], "entities": []}, {"text": " Table 3: Results of grSemi-CRF with external  information, measured in F 1 score. None = no  external information, Emb = Senna embeddings,  Brown = Brown clusters, Gaz = gazetteers and  All = Emb + Brown + Gaz. NYT and RCV1 in  the parenthesis denote the corpus used to generate  Brown clusters. \"-\" means no results. Notice that  gazetteers are only applied to NER.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9593366583188375}, {"text": "NYT", "start_pos": 212, "end_pos": 215, "type": "DATASET", "confidence": 0.9500335454940796}]}, {"text": " Table 2: Experimental results over the CONLL-2000 and CONLL-2003 shared datasets, measured in F 1  score. Numbers in parentheses are the F 1 score when using gazetteers. JESS-CM (Suzuki and Isozaki,  2008) is a semi-supervised model, in which 15M or 1B denotes the number of unlabeled words it uses  for training.", "labels": [], "entities": [{"text": "CONLL-2000", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9550432562828064}, {"text": "CONLL-2003 shared datasets", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.8013853232065836}, {"text": "F 1  score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9815270702044169}, {"text": "F 1 score", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9768016139666239}, {"text": "JESS-CM", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.898643434047699}]}, {"text": " Table 4: F 1 scores of grSemi-CRF with scalar or  vectorial gating coefficients. Numbers in paren- theses are the F 1 score when using gazetteers.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9674991965293884}, {"text": "F 1 score", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9493638674418131}]}]}