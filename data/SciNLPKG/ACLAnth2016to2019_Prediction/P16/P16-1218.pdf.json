{"title": [{"text": "Graph-based Dependency Parsing with Bidirectional LSTM", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.769486129283905}]}], "abstractContent": [{"text": "In this paper, we propose a neural network model for graph-based dependency parsing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual information instead of using high-order fac-torization, and enable our model to use much fewer features than previous work.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7191095501184464}, {"text": "Bidirectional LSTM (BLSTM)", "start_pos": 99, "end_pos": 125, "type": "METRIC", "confidence": 0.9090481042861939}]}, {"text": "In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network.", "labels": [], "entities": []}, {"text": "Although our model uses only first-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models.", "labels": [], "entities": [{"text": "English Peen Treebank", "start_pos": 71, "end_pos": 92, "type": "DATASET", "confidence": 0.9419716596603394}, {"text": "Chinese Penn Treebank", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.8291473984718323}, {"text": "dependency parsing", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.7575298249721527}]}], "introductionContent": [{"text": "Dependency parsing is a fundamental task for language processing which has been investigated for decades.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9180625975131989}, {"text": "language processing", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7991152703762054}]}, {"text": "It has been applied in a wide range of applications such as information extraction and machine translation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8998342454433441}, {"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8316839933395386}]}, {"text": "Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7315912246704102}]}, {"text": "Typical graph-based models factor the dependency tree into subgraphs, including single arcs (), sibling or grandparent arcs or higher-order substructures ( and then score the whole tree by summing scores of the subgraphs.", "labels": [], "entities": []}, {"text": "In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights.", "labels": [], "entities": []}, {"text": "However, conventional graph-based models heavily rely on feature engineering and their performance is restricted by the design of features.", "labels": [], "entities": []}, {"text": "In addition, standard decoding algorithm) only works for the first-order model which limits the scope of feature selection.", "labels": [], "entities": []}, {"text": "To incorporate high-order features, Eisner algorithm must be somehow extended or modified, which is usually done at high cost in terms of efficiency.", "labels": [], "entities": []}, {"text": "The fourth-order graph-based model, which seems the highest-order model so far to our knowledge, requires O(n 5 ) time and O(n 4 ) space.", "labels": [], "entities": []}, {"text": "Due to the high computational cost, highorder models are normally restricted to producing only unlabeled parses to avoid extra cost introduced by inclusion of arc-labels into the parse trees.", "labels": [], "entities": []}, {"text": "To alleviate the burden of feature engineering, presented an effective neural network model for graph-based dependency parsing.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7973503768444061}, {"text": "graph-based dependency parsing", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.6672134200731913}]}, {"text": "They only use atomic features such as word unigrams and POS tag unigrams and leave the model to automatically learn the feature combinations.", "labels": [], "entities": []}, {"text": "However, their model requires many atomic features and still relies on high-order factorization strategy to further improve the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.998264729976654}]}, {"text": "Different from previous work, we propose an LSTM-based dependency parsing model in this paper and aim to use LSTM network to capture richer contextual information to support parsing decisions, instead of adopting a high-order factorization.", "labels": [], "entities": [{"text": "LSTM-based dependency parsing", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.6017887890338898}, {"text": "parsing", "start_pos": 174, "end_pos": 181, "type": "TASK", "confidence": 0.9731692671775818}]}, {"text": "The main advantages of our model are as follows: \u2022 By introducing Bidirectional LSTM, our model shows strong ability to capture potential long range contextual information and exhibits improved accuracy in recovering long distance dependencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9984985589981079}]}, {"text": "It is different to previous work in which a similar effect is usually achieved by high-order factorization.", "labels": [], "entities": []}, {"text": "More-over, our model also eliminates the need for setting feature selection windows and reduces the number of features to a minimum level.", "labels": [], "entities": []}, {"text": "\u2022 We propose an LSTM-based sentence segment embedding method named LSTMMinus, in which distributed representation of sentence segment is learned by using subtraction between LSTM hidden vectors.", "labels": [], "entities": []}, {"text": "Experiment shows this further enhances our model's ability to access to sentence-level information.", "labels": [], "entities": []}, {"text": "\u2022 Last but important, our model is a first-order model using standard Eisner algorithm for decoding, the computational cost remains at the lowest level among graph-based models.", "labels": [], "entities": []}, {"text": "Our model does not trade-off efficiency for accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9990355968475342}]}, {"text": "We evaluate our model on the English Penn Treebank and Chinese Penn Treebank, experiments show that our model achieves competitive parsing accuracy compared with conventional high-order models, however, with a much lower computational cost.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.926576574643453}, {"text": "Chinese Penn Treebank", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.8691129883130392}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9213933348655701}]}], "datasetContent": [{"text": "In this section, we present our experimental setup and the main result of our work.", "labels": [], "entities": []}, {"text": "We conduct our experiments on the English Penn Treebank (PTB) and the Chinese Penn Treebank (CTB) datasets.", "labels": [], "entities": [{"text": "English Penn Treebank (PTB)", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.950803260008494}, {"text": "Chinese Penn Treebank (CTB) datasets", "start_pos": 70, "end_pos": 106, "type": "DATASET", "confidence": 0.9430065495627267}]}, {"text": "For English, we follow the standard splits of PTB3.", "labels": [], "entities": [{"text": "PTB3", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.978905439376831}]}, {"text": "Using section 2-21 for training, section 22 as development set and 23 as test set.", "labels": [], "entities": []}, {"text": "We conduct experiments on two different constituency-todependency-converted Penn Treebank data sets.", "labels": [], "entities": [{"text": "Penn Treebank data sets", "start_pos": 76, "end_pos": 99, "type": "DATASET", "confidence": 0.980090394616127}]}, {"text": "The first one, Penn-YM, was created by the Penn2Malt tool 2 based on head rules.", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.9697535634040833}, {"text": "Penn2Malt tool", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.9405457079410553}]}, {"text": "The second one, Penn-SD, use Stanford Basic Dependencies ( and was converted by version 3.3.0 3 of the Stanford parser.", "labels": [], "entities": [{"text": "Penn-SD", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9889163970947266}]}, {"text": "The Stanford POS Tagger () with ten-way jackknifing of the training data is used for assigning POS tags (accuracy \u2248 97.2%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9992269277572632}]}, {"text": "For Chinese, we adopt the same split of CTB5 as described in.", "labels": [], "entities": [{"text": "CTB5", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.9195037484169006}]}, {"text": "Following (), we use gold segmentation and POS tags for the input.", "labels": [], "entities": []}, {"text": "We first make comparisons with previous graphbased models of different orders as shown in Ta-ble 2.", "labels": [], "entities": []}, {"text": "We use MSTParser 4 for conventional firstorder model and secondorder model).", "labels": [], "entities": []}, {"text": "We also include the results of conventional high-order models () and the neural network model of.", "labels": [], "entities": []}, {"text": "Different from typical high-order models (, which need to extend their decoding algorithm to score new types of higher-order dependencies.", "labels": [], "entities": []}, {"text": "generalized the Eisner algorithm to handle arbitrary features over higher-order dependencies and controlled complexity via approximate decoding with cube pruning.", "labels": [], "entities": []}, {"text": "They further improve their work by using perceptron update strategies for inexact hypergraph search ( and forcing inference to maintain both label and structural ambiguity through a secondary beam.", "labels": [], "entities": []}, {"text": "Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation . The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM which is same to: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5.", "labels": [], "entities": [{"text": "UAS", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9960989952087402}, {"text": "LAS (labeled attachment scores)", "start_pos": 63, "end_pos": 94, "type": "METRIC", "confidence": 0.7807328502337137}, {"text": "parsing", "start_pos": 141, "end_pos": 148, "type": "TASK", "confidence": 0.9529322385787964}, {"text": "Penn-YM", "start_pos": 296, "end_pos": 303, "type": "DATASET", "confidence": 0.9867070913314819}, {"text": "Penn-SD", "start_pos": 305, "end_pos": 312, "type": "DATASET", "confidence": 0.9370965361595154}]}, {"text": "basic model outperforms previous first-order graph-based models by a substantial margin, even outperforms Zhang and McDonald (2012)'s unlimited-order model.", "labels": [], "entities": []}, {"text": "Moreover, incorporating segment information further improves our model's accuracy, which shows that segment embeddings do capture richer contextual information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9995974898338318}]}, {"text": "By using segment embeddings, our improved model could be comparable to high-order graph-based models . With regard to parsing speed, our model also shows advantage of efficiency.", "labels": [], "entities": [{"text": "parsing", "start_pos": 118, "end_pos": 125, "type": "TASK", "confidence": 0.9830396771430969}]}, {"text": "Our model uses only first-order factorization and requires O(n 3 ) time to decode.", "labels": [], "entities": []}, {"text": "Third-order model requires O(n 4 ) time and fourth-order model requires O(n 5 ) time.", "labels": [], "entities": [{"text": "O", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.9498676657676697}, {"text": "O", "start_pos": 72, "end_pos": 73, "type": "METRIC", "confidence": 0.9325879216194153}]}, {"text": "By using approximate decoding, the unlimitedorder model of requires O(k \u00b7log(k)\u00b7n 3 ) time, where k is the beam size.", "labels": [], "entities": [{"text": "O", "start_pos": 68, "end_pos": 69, "type": "METRIC", "confidence": 0.9967684745788574}]}, {"text": "The computational cost of our model is the lowest among graph-based models.", "labels": [], "entities": []}, {"text": "Moreover, although using LSTM requires much computational cost.", "labels": [], "entities": []}, {"text": "However, compared with Pei's 1st-order model, our model decreases the number of atomic features from 21 to 3, this allows our model to require a much smaller matrix computation in the scoring model, which cancels out the extra computation cost introduced by the LSTM computation.", "labels": [], "entities": []}, {"text": "Our basic model is the fastest among first-order and second-order models.", "labels": [], "entities": []}, {"text": "Incorporating segment information slows down the parsing speed while it is still slightly faster than conventional first-order model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9742452502250671}]}, {"text": "To compare with conventional high-order models on practical parsing speed, we can make an indirect comparison according to.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9665709137916565}]}, {"text": "Conventional first-order model is about 10 times faster than Note that our model can't be strictly comparable with third-order model ( and fourthorder model since they are unlabeled model.", "labels": [], "entities": []}, {"text": "However, our model is comparable with all the three unlimited-order models presented in, and), since they all are labeled models as ours.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison with previous graph-based models on Penn-YM.", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9841817021369934}]}, {"text": " Table 3: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5.", "labels": [], "entities": [{"text": "Penn-YM", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9909713268280029}, {"text": "Penn-SD", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9557929635047913}, {"text": "CTB5", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.7437734603881836}]}, {"text": " Table 4: Model performance of different way to  learn segment embeddings.", "labels": [], "entities": []}]}