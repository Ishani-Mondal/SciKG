{"title": [{"text": "A Joint Model for Answer Sentence Ranking and Answer Extraction", "labels": [], "entities": [{"text": "Answer Sentence Ranking", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.9526988863945007}, {"text": "Answer Extraction", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.9337573945522308}]}], "abstractContent": [{"text": "Answer sentence ranking and answer extraction are two key challenges in question answering that have traditionally been treated in isolation , i.e., as independent tasks.", "labels": [], "entities": [{"text": "Answer sentence ranking", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8751720984776815}, {"text": "answer extraction", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8730458319187164}, {"text": "question answering", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7590568959712982}]}, {"text": "In this article , we (1) explain how both tasks are related at their core by a common quantity, and (2) propose a simple and intuitive joint probabilis-tic model that addresses both via joint computation but task-specific application of that quantity.", "labels": [], "entities": []}, {"text": "In our experiments with two TREC datasets, our joint model substantially outper-forms state-of-the-art systems in both tasks.", "labels": [], "entities": [{"text": "TREC datasets", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8112528026103973}]}], "introductionContent": [{"text": "One of the original goals of AI was to build machines that can naturally interact with humans.", "labels": [], "entities": []}, {"text": "Over time, the challenges became apparent and language processing emerged as one of AI's most puzzling areas.", "labels": [], "entities": [{"text": "language processing", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7417910695075989}]}, {"text": "Nevertheless, major breakthroughs have still been made in several important tasks; with IBM's Watson) significantly outperforming human champions in the quiz contest Jeopardy!, question answering (QA) is definitely one such task.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 177, "end_pos": 200, "type": "TASK", "confidence": 0.9139685034751892}]}, {"text": "QA comes in various forms, each supporting specific kinds of user requirements.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.738631546497345}]}, {"text": "Consider a scenario where a system is given a question and a set of sentences each of which mayor may not contain an answer to that question.", "labels": [], "entities": []}, {"text": "The goal of answer extraction is to extract a precise answer in the form of a short span of text in one or more of those sentences.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8952895104885101}]}, {"text": "In this form, QA meets users' immediate information needs.", "labels": [], "entities": []}, {"text": "Answer sentence ranking, on the other hand, is the task of assigning a rank to each sentence so that the ones that are more likely to contain an answer are ranked higher.", "labels": [], "entities": [{"text": "Answer sentence ranking", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8335246642430624}]}, {"text": "In this form, QA is similar to information retrieval and presents greater opportunities for further exploration and learning.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7799413502216339}]}, {"text": "In this article, we propose a novel approach to jointly solving these two-studied yet open QA problems.", "labels": [], "entities": []}, {"text": "Most answer sentence ranking algorithms operate under the assumption that the degree of syntactic and/or semantic similarity between questions and answer sentences is a sufficiently strong predictor of answer sentence relevance (.", "labels": [], "entities": [{"text": "answer sentence ranking", "start_pos": 5, "end_pos": 28, "type": "TASK", "confidence": 0.7242805361747742}]}, {"text": "On the other hand, answer extraction algorithms frequently assess candidate answer phrases based primarily on their own properties relative to the question (e.g., whether the question is a who question and the phrase refers to a person), making inadequate or no use of sentence-level evidence (.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8739432394504547}]}, {"text": "Both these assumptions, however, are simplistic, and fail to capture the core requirements of the two tasks.", "labels": [], "entities": []}, {"text": "shows a question, and three candidate answer sentences only one of which (S (1) ) actually answers the question.", "labels": [], "entities": []}, {"text": "Ranking models that rely solely on text similarity are highly likely to incorrectly assign similar ranks to S (1) and S . Such models would fail to utilize the key piece of evidence against S (2) that it does not contain any temporal information, necessary to answer a when question.", "labels": [], "entities": []}, {"text": "Similarly, an extraction model that relies only on the features of a candidate phrase might extract the temporal expression \"the year 1666\" in S as an answer despite a clear lack of sentence-level evidence.", "labels": [], "entities": []}, {"text": "In view of the above, we propose a joint model Q When was the Hale Bopp comet discovered?", "labels": [], "entities": []}, {"text": "The comet was first spotted by Hale and Bopp, both US astronomers, on July 22, 1995.", "labels": [], "entities": []}, {"text": "S (2) Hale-Bopp, a large comet, was observed for the first time in China.", "labels": [], "entities": []}, {"text": "S (3) The law of gravity was discovered in the year 1666 by Sir Isaac Newton.", "labels": [], "entities": []}, {"text": "for answer sentence ranking and answer extraction that utilizes both sentence and phrase-level evidence to solve each task.", "labels": [], "entities": [{"text": "answer sentence ranking", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.750621477762858}, {"text": "answer extraction", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8565804064273834}]}, {"text": "More concretely, we (1) design task-specific probabilistic models for ranking and extraction, exploiting features of candidate answer sentences and their phrases, respectively, and (2) combine the two models in a simple, intuitive step to build a joint probabilistic model for both tasks.", "labels": [], "entities": [{"text": "ranking and extraction", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7275577982266744}]}, {"text": "This twostep approach facilitates construction of new joint models from any existing solutions to the two tasks.", "labels": [], "entities": []}, {"text": "On a publicly available TREC dataset (), our joint model demonstrates an improvement in ranking by over 10 MAP and MRR scores over the current state of the art.", "labels": [], "entities": [{"text": "TREC dataset", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9024804532527924}, {"text": "MAP", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9119041562080383}, {"text": "MRR", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.8530627489089966}]}, {"text": "It also outperforms stateof-the-art extraction systems on two TREC datasets ().", "labels": [], "entities": [{"text": "stateof-the-art extraction", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.6880298256874084}, {"text": "TREC datasets", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.8863436877727509}]}], "datasetContent": [{"text": "Given a set of questions, the precision of an answer extraction system is the proportion of its extracted answers that are correct (i.e. match the corresponding gold regexp pattern).", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9993185997009277}, {"text": "answer extraction", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7140325903892517}]}, {"text": "Recall is the proportion of questions for which the system extracted a correct answer.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9879859089851379}]}, {"text": "The F 1 score is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9833449323972067}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9995266199111938}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9971224665641785}]}, {"text": "It captures the system's accuracy and coverage in a single metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9993882179260254}, {"text": "coverage", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9948268532752991}]}, {"text": "(2013c) report an extraction dataset containing 99 test questions, derived from the MIT109 test collection () of TREC pairs.", "labels": [], "entities": [{"text": "MIT109 test collection", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9848793546358744}]}, {"text": "Each question in this dataset has 10 candidate answer sentences.", "labels": [], "entities": []}, {"text": "We compare the performance of our joint probabilistic model with that of their extraction model, which extracts answers from top candidate sentences identified by their coupled ranker (Section 2.3).", "labels": [], "entities": []}, {"text": "Models are trained on their training set of 2,205 questions and 22,043 candidate QA pairs.", "labels": [], "entities": []}, {"text": "As shown in shows an example, where the correct answer chunk \"Steve Sloan\" appears in all four candidate sentences, of which only the first is actually relevant to the question.", "labels": [], "entities": []}, {"text": "The standalone model assigns high scores to all four instances and as a result observes a high overall score for the chunk.", "labels": [], "entities": []}, {"text": "The joint model, on the other hand, recognizes the false positives, and consequently observes a smaller overall score for the chunk.", "labels": [], "entities": []}, {"text": "However, this desired behavior eventually results in a wrong extraction.", "labels": [], "entities": []}, {"text": "These results have key implications for the evaluation of answer extraction systems: metrics that assess performance on individual QA pairs can enable finer-grained evaluation than what end-to-end extraction metrics offer.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.8836587071418762}]}], "tableCaptions": [{"text": " Table 2: Summary of the Wang et al. (2007) corpus.", "labels": [], "entities": [{"text": "Wang et al. (2007) corpus", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.7238284721970558}]}, {"text": " Table 3: Answer sentence ranking results.", "labels": [], "entities": [{"text": "Answer sentence ranking", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8867583473523458}]}, {"text": " Table 4: Answer extraction results on the Wang et al.  (2007) test set.", "labels": [], "entities": [{"text": "Answer extraction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8777227103710175}, {"text": "Wang et al.  (2007) test set", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.6182555158933004}]}, {"text": " Table 5: F 1 % of the STandalone and the Joint  Probabilistic extraction model across question types.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9963909983634949}, {"text": "STandalone", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.6844022870063782}, {"text": "Joint  Probabilistic extraction", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.5053473313649496}]}, {"text": " Table 6: Scores computed by the STandalone and the Joint Probabilistic model for candidate chunks (boldfaced) in  four (Wang et al., 2007) test sentences. Joint model scores for non-answer chunks (rows 2 and 4) are much lower.", "labels": [], "entities": [{"text": "STandalone", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.8786203861236572}]}, {"text": " Table 7: Performances of two joint extraction models on  the Yao et al. (2013c) test set.", "labels": [], "entities": [{"text": "joint extraction", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7217187434434891}, {"text": "Yao et al. (2013c) test set", "start_pos": 62, "end_pos": 89, "type": "DATASET", "confidence": 0.7344642943806119}]}, {"text": " Table 8: Scores computed by the STandalone and the Joint Probabilistic model for NP chunks (boldfaced) in four  Yao et al. (2013c) test sentences for the question: Who is the detective on 'Diagnosis Murder'? The standalone model  assigns high probabilities to non-answer chunks in the last three sentences, subsequently corrected by the joint model.", "labels": [], "entities": [{"text": "STandalone", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.902880072593689}, {"text": "Diagnosis Murder'", "start_pos": 190, "end_pos": 207, "type": "TASK", "confidence": 0.8226818839708964}]}]}