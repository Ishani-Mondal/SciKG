{"title": [{"text": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs", "labels": [], "entities": []}], "abstractContent": [{"text": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE).", "labels": [], "entities": [{"text": "answer selection (AS)", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.853669273853302}, {"text": "paraphrase identification (PI)", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.8079515635967255}, {"text": "textual entailment (TE)", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.6791036784648895}]}, {"text": "Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence's representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features.", "labels": [], "entities": []}, {"text": "This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences.", "labels": [], "entities": []}, {"text": "(i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs.", "labels": [], "entities": []}, {"text": "(ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart.", "labels": [], "entities": []}, {"text": "These interdependent sentence pair representations are more powerful than isolated sentence representations.", "labels": [], "entities": []}, {"text": "(iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks.", "labels": [], "entities": [{"text": "TE", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.7908919453620911}]}, {"text": "We release code at: https://github.com/ yinwenpeng/Answer_Selection.", "labels": [], "entities": []}], "introductionContent": [{"text": "How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (, paraphrase identification (PI) (, textual entailment (TE)  Most prior work derives each sentence's representation separately, rarely considering the impact of the other sentence.", "labels": [], "entities": [{"text": "answer selection (AS)", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8067838788032532}, {"text": "paraphrase identification (PI)", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.7485647022724151}]}, {"text": "This neglects the mutual influence of the two sentences in the context of the task.", "labels": [], "entities": []}, {"text": "It also contradicts what humans do when comparing two sentences.", "labels": [], "entities": []}, {"text": "We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations.", "labels": [], "entities": []}, {"text": "Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other.", "labels": [], "entities": []}, {"text": "demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on.", "labels": [], "entities": []}, {"text": "For AS, correctly answering s 0 requires attention on \"gross\": s + 1 contains a corresponding unit (\"earned\") while s \u2212 1 does not.", "labels": [], "entities": [{"text": "AS", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9881002902984619}]}, {"text": "For PI, focus should be removed from \"today\" to correctly recognize < s 0 , s + 1 > as paraphrases and < s 0 , s \u2212 1 > as non-paraphrases.", "labels": [], "entities": [{"text": "PI", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9008811712265015}, {"text": "focus", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9850408434867859}]}, {"text": "For TE, we need to focus on \"full of people\" (to recognize TE for < s 0 , s + 1 >) and on \"outdoors\" / \"indoors\" (to recognize non-TE for < s 0 , s \u2212 1 >).", "labels": [], "entities": [{"text": "TE", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8993677496910095}, {"text": "TE", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9181987643241882}]}, {"text": "These examples show the need for an architecture that computes different representations of s i for different s 1\u2212i (i \u2208 {0, 1}).", "labels": [], "entities": []}, {"text": "Convolutional Neural Networks (CNNs) ( are widely used to model sentences ( and sentence pairs, especially in classification tasks.", "labels": [], "entities": []}, {"text": "CNNs are supposed to be good at extracting robust and abstract features of input.", "labels": [], "entities": []}, {"text": "This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences.", "labels": [], "entities": []}, {"text": "The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks.", "labels": [], "entities": [{"text": "ABCNN", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8595656752586365}, {"text": "sentence pair modeling tasks", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8005317375063896}]}, {"text": "Some prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g., employ word alignment to match related parts of the two sentences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.7095541507005692}]}, {"text": "In contrast, our attention scheme based on CNNs models relatedness between two parts fully automatically.", "labels": [], "entities": []}, {"text": "Moreover, attention at multiple levels of granularity, not only at word level, is achieved as we stack multiple convolution layers that increase abstraction.", "labels": [], "entities": []}, {"text": "Prior work on attention in deep learning (DL) mostly addresses long short-term memory networks (LSTMs).", "labels": [], "entities": [{"text": "attention in deep learning (DL)", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.6819401000227246}]}, {"text": "LSTMs achieve attention usually in a word-to-word scheme, and word representations mostly encode the whole context within the sentence ().", "labels": [], "entities": []}, {"text": "It is not clear whether this is the best strategy; e.g., in the AS example in, it is possible to determine that \"how much\" in s 0 matches \"$161.5 million\" in s 1 without taking the entire sentence contexts into account.", "labels": [], "entities": []}, {"text": "This observation was also investigated by where an information retrieval system retrieves sentences with tokens labeled as DATE by named entity recognition or as CD by POS tagging if there is a \"when\" question.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 168, "end_pos": 179, "type": "TASK", "confidence": 0.6976316869258881}]}, {"text": "However, labels or POS tags require extra tools.", "labels": [], "entities": []}, {"text": "CNNs benefit from incorporating attention into representations of local phrases detected by filters; in contrast, LSTMs encode the whole context to form attention-based word representations -a strategy that is more complex than the CNN strategy and (as our experiments suggest) performs less well for some tasks.", "labels": [], "entities": []}, {"text": "Apart from these differences, it is clear that attention has as much potential for CNNs as it does for LSTMs.", "labels": [], "entities": []}, {"text": "As far as we know, this is the first NLP paper that incorporates attention into CNNs.", "labels": [], "entities": []}, {"text": "Our ABCNNs get state-of-the-art in AS and TE tasks, and competitive performance in PI, then obtains further improvements overall three tasks when linguistic features are used.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the proposed architectures on three tasks: answer selection (AS), paraphrase identification (PI) and textual entailment (TE).", "labels": [], "entities": [{"text": "answer selection (AS)", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.8444742918014526}, {"text": "paraphrase identification (PI)", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.7645583331584931}]}, {"text": "Words are initialized by 300-dimensional word2vec embeddings and not changed during training.", "labels": [], "entities": []}, {"text": "A single randomly initialized embedding is created for all unknown words by uniform sampling from [-.01,.01].", "labels": [], "entities": []}, {"text": "We employ Adagrad (Duchi et al., 2011) and L 2 regularization.", "labels": [], "entities": [{"text": "Adagrad", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.761972188949585}]}, {"text": "Each network in the experiments below consists of (i) an initialization block b 1 that initializes words by word2vec embeddings, (ii) a stack of k \u2212 1 convolution-pooling blocks b 2 , . .", "labels": [], "entities": []}, {"text": ", bk , computing increasingly abstract features, and (iii) one final LR layer (logistic regression layer) as shown in.", "labels": [], "entities": []}, {"text": "The input to the LR layer consists of kn features -each block provides n similarity scores, e.g., n cosine similarity scores.", "labels": [], "entities": []}, {"text": "shows the two sentence vectors output by the final block bk of the stack (\"sentence representation 0\", \"sentence representation 1\"); this is the basis of the last n similarity scores.", "labels": [], "entities": []}, {"text": "As we explained in the final paragraph of Section 3, we perform all-ap pooling for all blocks, not just for bk . Thus we get one sentence representation each for s 0 and s 1 for each block b 1 , . .", "labels": [], "entities": []}, {"text": ", bk . We compute n similarity scores for each block (based on the block's two sentence representations).", "labels": [], "entities": []}, {"text": "Thus, we compute a total of kn similarity scores and these scores are input to the LR layer.", "labels": [], "entities": [{"text": "kn similarity scores", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.7452911933263143}]}, {"text": "Depending on the task, we use different methods for computing the similarity score: see below.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 66, "end_pos": 82, "type": "METRIC", "confidence": 0.9826264977455139}]}, {"text": "In our training regime, we first train a network consisting of just one convolution-pooling block b 2 . We then create anew network by adding a block b 3 , initialize its b 2 block with the previously learned weights for b 2 and train b 3 keeping the previously learned weights for b 2 fixed.", "labels": [], "entities": []}, {"text": "We repeat this procedure until all k \u2212 1 convolution-pooling blocks are trained.", "labels": [], "entities": []}, {"text": "We found that this training regime gives us good performance and shortens training times considerably.", "labels": [], "entities": []}, {"text": "Since similarity scores of lower blocks are kept unchanged once they have been learned, this also has the nice effect that \"simple\" similarity scores (those based on surface features) are learned first and subsequent training phases can focus on complementary scores derived from more complex abstract features.", "labels": [], "entities": []}, {"text": "We found that performance increases if we do not use the output of the LR layer as the final decision, but instead train a linear SVM or a logistic regression with default parameters 2 directly on the input to the LR layer (i.e., on the kn similarity scores that are generated by the k-block stack after network training is completed).", "labels": [], "entities": []}, {"text": "Direct training of SVMs/LR seems to get closer to the global optimum than gradient descent training of CNNs.", "labels": [], "entities": []}, {"text": "shows hyperparameters, tuned on dev.", "labels": [], "entities": []}, {"text": "We use addition and LSTMs as two shared baselines for all three tasks, i.e., for AS, PI and TE.", "labels": [], "entities": [{"text": "TE", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.931739091873169}]}, {"text": "We now describe these two shared baselines.", "labels": [], "entities": []}, {"text": "We sum up word embeddings element-wise to form each sentence representation.", "labels": [], "entities": []}, {"text": "The classifier input is then the concatenation of the two sentence representations.", "labels": [], "entities": []}, {"text": "Before this work, most attention mechanisms in NLP were implemented in recurrent neural networks for text generation tasks such as machine translation (e.g., , ).", "labels": [], "entities": [{"text": "text generation tasks", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.7727164626121521}, {"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7367687076330185}]}, {"text": "present an attention-LSTM for natural language inference.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6571829319000244}]}, {"text": "Since this model is the pioneering attention based RNN system for sentence pair classification, we consider it as a baseline system (\"A-LSTM\") for all our three tasks.", "labels": [], "entities": [{"text": "sentence pair classification", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7111550072828928}]}, {"text": "The A-LSTM has the same configuration as our ABCNNs in terms of word initialization (300-dimensional word2vec embeddings) and the dimensionality of all hidden layers (50).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyperparameters. lr: learning rate. #CL: num- ber convolution layers. w: filter width. The number of  convolution kernels d i (i > 0) is 50 throughout.", "labels": [], "entities": []}, {"text": " Table 4: Results for PI on MSRP", "labels": [], "entities": [{"text": "PI", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9701645374298096}, {"text": "MSRP", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.7951326370239258}]}, {"text": " Table 5: SICK data: Converting the original sentences  (ORIG) into the NONOVER format", "labels": [], "entities": [{"text": "SICK", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9391704797744751}, {"text": "ORIG", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9411832094192505}]}]}