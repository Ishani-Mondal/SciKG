{"title": [], "abstractContent": [{"text": "We propose two models for verbalizing numbers , a key component in speech recognition and synthesis systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7151365727186203}]}, {"text": "The first model uses an end-to-end recurrent neural network.", "labels": [], "entities": []}, {"text": "The second model, drawing inspiration from the linguistics literature, uses finite-state transducers constructed with a minimal amount of training data.", "labels": [], "entities": []}, {"text": "While both models achieve near-perfect performance, the latter model can be trained using several orders of magnitude less data than the former, making it particularly useful for low-resource languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many speech and language applications require text tokens to be converted from one form to another.", "labels": [], "entities": []}, {"text": "For example, in text-to-speech synthesis, one must convert digit sequences (32) into number names (thirtytwo), and appropriately verbalize date and time47 \u2192 twelve forty-seven) and abbreviations (kg \u2192 kilograms) while handling allomorphy and morphological concord (e.g.,.", "labels": [], "entities": [{"text": "text-to-speech synthesis", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7577789127826691}]}, {"text": "Quite a bit of recent work on SMS (e.g., and text from social media sites (e.g.,) has focused on detecting and expanding novel abbreviations (e.g., cn u plz hlp).", "labels": [], "entities": []}, {"text": "Collectively, such conversions all fall under the rubric of text normalization), but this term means radically different things in different applications.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7474135160446167}]}, {"text": "For instance, it is not necessary to detect and verbalize dates and times when preparing social media text for downstream information extraction, but this is essential for speech applications.", "labels": [], "entities": [{"text": "downstream information extraction", "start_pos": 111, "end_pos": 144, "type": "TASK", "confidence": 0.7278923193613688}]}, {"text": "While expanding novel abbreviations is also important for speech), numbers, times, dates, measure phrases and the like are far more common in a wide variety of text genres.", "labels": [], "entities": []}, {"text": "Following, we refer to categories such as cardinal numbers, times, and dateseach of which is semantically well-circumscribedas semiotic classes.", "labels": [], "entities": []}, {"text": "Some previous work on text normalization proposes minimally-supervised machine learning techniques for normalizing specific semiotic classes, such as abbreviations (e.g.,.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7888811826705933}]}, {"text": "This paper continues this tradition by contributing minimally-supervised models for normalization of cardinal number expressions (e.g., ninetyseven).", "labels": [], "entities": [{"text": "ninetyseven", "start_pos": 136, "end_pos": 147, "type": "METRIC", "confidence": 0.800048291683197}]}, {"text": "Previous work on this semiotic class include formal linguistic studies by and and computational models proposed by and.", "labels": [], "entities": []}, {"text": "Of all semiotic classes, numbers are by far the most important for speech, as cardinal (and ordinal) numbers are not only semiotic classes in their own right, but knowing how to verbalize numbers is important for most of the other classes: one cannot verbalize times, dates, measures, or currency expressions without knowing how to verbalize that language's numbers as well.", "labels": [], "entities": []}, {"text": "One computational approach to number name verbalization) employs a cascade of two finite-state transducers (FSTs).", "labels": [], "entities": [{"text": "number name verbalization", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7804031173388163}]}, {"text": "The first FST factors the integer, expressed as a digit sequence, into sums of products of powers often (i.e., in the case of a base-ten number system).", "labels": [], "entities": [{"text": "FST", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7864833474159241}]}, {"text": "This is composed with a second FST that defines how the numeric factors are verbalized, and may also handle allomorphy or morphological concord in languages that require it.", "labels": [], "entities": [{"text": "FST", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.6568284034729004}]}, {"text": "Number names can be relatively easy (as in English) or complex (as in Russian; and thus these FSTs maybe relatively easy or quite difficult to develop.", "labels": [], "entities": [{"text": "Number names", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7819936573505402}, {"text": "FSTs", "start_pos": 94, "end_pos": 98, "type": "TASK", "confidence": 0.9103298783302307}]}, {"text": "While the Google text-tospeech (TTS) (see) and automatic speech recognition (ASR) systems depend on hand-built number name grammars for about 70 languages, developing these grammars for new languages requires extensive research and labor.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 47, "end_pos": 81, "type": "TASK", "confidence": 0.7885581851005554}]}, {"text": "For some languages, a professional linguist can develop anew grammar in as little as a day, but other languages may require days or weeks of effort.", "labels": [], "entities": []}, {"text": "We have also found that it is very common for these handwritten grammars to contain difficult-to-detect errors; indeed, the computational models used in this study revealed several long-standing bugs in handwritten number grammars.", "labels": [], "entities": []}, {"text": "The amount of time, effort, and expertise required to produce error-free number grammars leads us to consider machine learning solutions.", "labels": [], "entities": []}, {"text": "Yet it is important to note that number verbalization poses a dauntingly high standard of accuracy compared to nearly all other speech and language tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9984904527664185}]}, {"text": "While one might forgive a TTS system that reads the ambiguous abbreviation plz as plaza rather than the intended please, it would be inexcusable for the same system to ever read 72 as four hundred seventy two, even if it rendered the vast majority of numbers correctly.", "labels": [], "entities": []}, {"text": "To set the stage for this work, we first ( \u00a72-3) briefly describe several experiments with a powerful and popular machine learning technique, namely recurrent neural networks (RNNs).", "labels": [], "entities": []}, {"text": "When provided with a large corpus of parallel data, these systems are highly accurate, but may still produce occasional errors, rendering it unusable for applications like TTS.", "labels": [], "entities": [{"text": "TTS", "start_pos": 172, "end_pos": 175, "type": "TASK", "confidence": 0.7792090773582458}]}, {"text": "In order to give the reader some background on the relevant linguistic issues, we then review some crosslinguistic properties of cardinal number expressions and propose a finite-state approach to number normalization informed by these linguistic properties ( \u00a74).", "labels": [], "entities": [{"text": "number normalization", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.752899706363678}]}, {"text": "The core of the approach is an algorithm for inducing language-specific number grammar rules.", "labels": [], "entities": []}, {"text": "We evaluate this technique on data from four languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "As part of a separate strand of research, we have been experimenting with various recurrent neural network (RNN) architectures for problems in text normalization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7895407974720001}]}, {"text": "In one set of experiments, we trained RNNs to learn a mapping from digit sequences marked with morphosyntactic (case and gender) information, and their expression as Russian cardinal number names.", "labels": [], "entities": []}, {"text": "The motivation for choosing Russian is that the number name system of this language, like that of many Slavic languages, is quite complicated, and therefore serves as a good test of the abilities of any text normalization system.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.7322549521923065}]}, {"text": "The architecture used was similar to a network employed by for graphemeto-phoneme conversion, a superficially similar sequence-to-sequence mapping problem.", "labels": [], "entities": [{"text": "graphemeto-phoneme conversion", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.7262314558029175}]}, {"text": "We used a recurrent network with an input layer, four hidden feed-forward LSTM layers, and a connectionist temporal classification (CTC) output layer with a softmax activation function ().", "labels": [], "entities": [{"text": "connectionist temporal classification (CTC)", "start_pos": 93, "end_pos": 136, "type": "TASK", "confidence": 0.7294065852959951}]}, {"text": "1 Two of the hidden layers modeled forward sequences and the other two backward sequences.", "labels": [], "entities": []}, {"text": "There were 32 input nodes-corresponding to characters-and 153 output nodes-corresponding to predicted number name words.", "labels": [], "entities": []}, {"text": "Each of the hidden layers had 256 nodes.", "labels": [], "entities": []}, {"text": "The full architecture is depicted in.", "labels": [], "entities": []}, {"text": "The system was trained on 22M unique digit se-quences ranging from one to one million; these were collected by applying an existing TTS text normalization system to several terabytes of web text.", "labels": [], "entities": [{"text": "TTS text normalization", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.6887340545654297}]}, {"text": "Each training example consisted of a digit sequence, gender and case features, and the Russian cardinal number verbalization of that number.", "labels": [], "entities": []}, {"text": "Thus, for example, the system has to learn to produce the feminine instrumental form of 60.", "labels": [], "entities": []}, {"text": "Examples of these mappings are shown in, and the various inflected forms of a single cardinal number are given in.", "labels": [], "entities": []}, {"text": "In preliminary experiments, it was discovered that short digit sequences were poorly modeled due to undersampling, so an additional 240,000 short sequence samples (of three or fewer digits) were added to compensate.", "labels": [], "entities": []}, {"text": "2.2M examples (10%) were held out as a development set.", "labels": [], "entities": []}, {"text": "The system was trained for one day, after which it had a 0% label error rate (LER) on the development data set.", "labels": [], "entities": [{"text": "0% label error rate (LER)", "start_pos": 57, "end_pos": 82, "type": "METRIC", "confidence": 0.8269915729761124}, {"text": "development data set", "start_pos": 90, "end_pos": 110, "type": "DATASET", "confidence": 0.8294150829315186}]}, {"text": "When decoding 240,000 tokens of held-out test data with this model, we achieved very high accuracy (LER < .0001).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9995055198669434}, {"text": "LER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9922035932540894}]}, {"text": "The few remaining errors, however, area serious obstacle to using this system for TTS.", "labels": [], "entities": [{"text": "TTS", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9449399709701538}]}, {"text": "The model appears to make no mistakes applying inflectional suffixes to unseen data.", "labels": [], "entities": []}, {"text": "Plausibly, this task was made easier by our positioning of the morphological feature string at the end of the input, making it local to the output inflectional suffix (at least for the last word in the number expression).", "labels": [], "entities": []}, {"text": "But it does make errors with respect to the numeric value of the expression.", "labels": [], "entities": []}, {"text": "For example, for 9801 plu.ins.", "labels": [], "entities": []}, {"text": "(\u0434\u0435\u0432\u044f\u0442\u044c\u044e \u0442\u044b\u0441\u044f\u0447\u0430\u043c\u0438 \u0432\u043e\u0441\u044c\u043c\u044c\u044e\u0441\u0442\u0430\u043c\u0438 \u043e\u0434\u043d\u0438\u043c\u0438), the system produced \u0434\u0435\u0432\u044f\u0442\u044c\u044e \u0442\u044b\u0441\u044f\u0447\u0430\u043c\u0438 \u0441\u0435\u043c\u044c\u044e\u0441\u0442\u0430\u043c\u0438 \u043e\u0434\u043d\u0438\u043c\u0438 (9701 plu.ins.): the morphology is correct, but the numeric value is wrong.", "labels": [], "entities": []}, {"text": "This pattern of errors was exactly the opposite of what we want for speech applications.", "labels": [], "entities": []}, {"text": "One might forgive a TTS system that reads 9801 with the correct numeric value but in the wrong case form: a listener would likely notice the error but would usually not be misled about the message being conveyed.", "labels": [], "entities": []}, {"text": "In contrast, reading it as nine thousand seven hundred and one is completely unacceptable, as this would actively mislead the listener.", "labels": [], "entities": []}, {"text": "It is worth pointing out that the training set used here-22M examples-was quite large, and we were only able to obtain such a large amount of labeled data because we already had a high-quality handbuilt grammar designed to do exactly this transduction.", "labels": [], "entities": []}, {"text": "It is simply unreasonable to expect that one could obtain this amount of parallel data fora new language (e.g., from naturally-occurring examples, or from speech transcriptions).", "labels": [], "entities": []}, {"text": "This problem is especially acute for low-resource languages (i.e., most of the world's languages), where data is by definition scarce, but where it is also hard to find highquality linguistic resources or expertise, and where a machine learning approach is thus most needed.", "labels": [], "entities": []}, {"text": "In conclusion, the system does not perform as well as we demand, nor is it in any case a practical solution due to the large amount of training data needed.", "labels": [], "entities": []}, {"text": "The RNN appears to have done an impressive job of learning the complex inflectional morphology of Russian, but it occasionally chooses the wrong number names altogether.", "labels": [], "entities": [{"text": "RNN", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7685750126838684}]}], "tableCaptions": [{"text": " Table 3: Accuracies on a test corpus of 1,000 random Russian citation-form number-name examples for the two RNN  architectures. \"Overlap\" indicates the percentage of the test examples that are also found in the training data.", "labels": [], "entities": [{"text": "Overlap", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.9669029116630554}]}, {"text": " Table 4. Arc labels that contain  parentheses indicate \"push\" and \"pop\" stack operations, respectively, and must balance along a path.", "labels": [], "entities": []}]}