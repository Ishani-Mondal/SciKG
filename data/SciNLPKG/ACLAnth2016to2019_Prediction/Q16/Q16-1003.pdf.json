{"title": [{"text": "A Bayesian Model of Diachronic Meaning Change", "labels": [], "entities": [{"text": "Diachronic Meaning Change", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.7260457376639048}]}], "abstractContent": [{"text": "Word meanings changeover time and an automated procedure for extracting this information from text would be useful for historical exploratory studies, information retrieval or question answering.", "labels": [], "entities": [{"text": "historical exploratory studies", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.7611360549926758}, {"text": "information retrieval", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.7997108101844788}, {"text": "question answering", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.9117386639118195}]}, {"text": "We present a dynamic Bayesian model of diachronic meaning change, which infers temporal word representations as a set of senses and their prevalence.", "labels": [], "entities": []}, {"text": "Unlike previous work, we explicitly model language change as a smooth, gradual process.", "labels": [], "entities": []}, {"text": "We experimentally show that this model-ing decision is beneficial: our model performs competitively on meaning change detection tasks whilst inducing discernible word senses and their development overtime.", "labels": [], "entities": [{"text": "meaning change detection", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.7382577061653137}]}, {"text": "Application of our model to the SemEval-2015 temporal classification benchmark datasets further reveals that it performs on par with highly optimized task-specific systems.", "labels": [], "entities": [{"text": "SemEval-2015 temporal classification benchmark datasets", "start_pos": 32, "end_pos": 87, "type": "DATASET", "confidence": 0.7273981750011445}]}], "introductionContent": [{"text": "Language is a dynamic system, constantly evolving and adapting to the needs of its users and their environment.", "labels": [], "entities": []}, {"text": "Words in all languages naturally exhibit a range of senses whose distribution or prevalence varies according to the genre and register of the discourse as well as its historical context.", "labels": [], "entities": []}, {"text": "As an example, consider the word cute which according to the Oxford English Dictionary (OED, Stevenson 2010) first appeared in the early 18th century and originally meant clever or keenwitted.", "labels": [], "entities": [{"text": "Oxford English Dictionary (OED, Stevenson 2010)", "start_pos": 61, "end_pos": 108, "type": "DATASET", "confidence": 0.9287715355555216}]}, {"text": "By the late 19th century cute was used in the same sense as cunning.", "labels": [], "entities": []}, {"text": "Today it mostly refers to objects or people perceived as attractive, pretty or sweet.", "labels": [], "entities": []}, {"text": "Another example is the word mouse which initially was only used in the rodent sense.", "labels": [], "entities": []}, {"text": "The OED dates the computer pointing device sense of mouse to 1965.", "labels": [], "entities": [{"text": "OED", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9194589853286743}]}, {"text": "The latter sense has become particularly dominant in recent decades due to the everincreasing use of computer technology.", "labels": [], "entities": []}, {"text": "The arrival of large-scale collections of historic texts and online libraries such as the Internet Archive and Google Books have greatly facilitated computational investigations of language change.", "labels": [], "entities": []}, {"text": "The ability to automatically detect how the meaning of words evolves overtime is potentially of significant value to lexicographic and linguistic research but also to real world applications.", "labels": [], "entities": [{"text": "detect how the meaning of words evolves overtime", "start_pos": 29, "end_pos": 77, "type": "TASK", "confidence": 0.655420608818531}]}, {"text": "Timespecific knowledge would presumably render word meaning representations more accurate, and benefit several downstream tasks where semantic information is crucial.", "labels": [], "entities": []}, {"text": "Examples include information retrieval and question answering, where time-related information could increase the precision of query disambiguation and document retrieval (e.g., by returning documents with newly created senses or filtering out documents with obsolete senses).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7925098240375519}, {"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8778004944324493}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9986573457717896}, {"text": "query disambiguation", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.6859658360481262}, {"text": "document retrieval", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.72016741335392}]}, {"text": "In this paper we present a dynamic Bayesian model of diachronic meaning change.", "labels": [], "entities": []}, {"text": "Word meaning is modeled as a set of senses, which are tracked over a sequence of contiguous time intervals.", "labels": [], "entities": [{"text": "Word meaning", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6835137754678726}]}, {"text": "We infer temporal meaning representations, consisting of a word's senses (as a probability distribution over words) and their relative prevalence.", "labels": [], "entities": []}, {"text": "Our model is thus able to detect that mouse had one sense until the mid-20th century (characterized by words such as {cheese, tail, rat}) and subsequently acquired a", "labels": [], "entities": []}], "datasetContent": [{"text": "As discussed earlier our model departs from previous approaches (e.g.,) in that it learns globally consistent temporal representations for each word.", "labels": [], "entities": []}, {"text": "In order to assess whether temporal dependencies are indeed beneficial, we implemented a stripped-down version of our model (SCAN-NOT) which does not have any temporal dependencies between individual time steps (i.e., without the chain iGMRF priors).", "labels": [], "entities": []}, {"text": "Word meaning is still represented as senses and sense prevalence is modeled as a distribution over senses for each time interval.", "labels": [], "entities": []}, {"text": "However, time intervals are now independent.", "labels": [], "entities": []}, {"text": "Inference works as described in Section 3.3, without having to learn the \u03ba precision parameters.", "labels": [], "entities": []}, {"text": "In this section and the next we will explicitly evaluate the temporal representations (i.e., probability distributions) induced by our model, and discuss its performance in the context of previous work.", "labels": [], "entities": []}, {"text": "Large-scale evaluation of meaning change is notoriously difficult, and many evaluations are based on limited hand-annotated goldstandard data sets., however, bypass this issue by evaluating the output of their system against WordNet.", "labels": [], "entities": [{"text": "evaluation of meaning change", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6394229903817177}, {"text": "WordNet", "start_pos": 225, "end_pos": 232, "type": "DATASET", "confidence": 0.9815360903739929}]}, {"text": "Here, we consider their automatic evaluation of sense-births, i.e., the emergence of novel senses.", "labels": [], "entities": []}, {"text": "We assume that novel senses are detected at a focus time t 2 whilst being compared to a reference time t 1 . WordNet is used to confirm that the proposed novel sense is indeed distinct from all other induced senses fora given word.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9661657810211182}]}, {"text": "Method Mitra et al.'s (2015) evaluation method presupposes a system which is able to detect senses fora set of target words and identify which ones are novel.", "labels": [], "entities": []}, {"text": "Our model does not automatically yield novelty scores for the induced senses.", "labels": [], "entities": [{"text": "novelty scores", "start_pos": 39, "end_pos": 53, "type": "METRIC", "confidence": 0.9609217643737793}]}, {"text": "However, propose several ways to perform this task post-hoc.", "labels": [], "entities": []}, {"text": "We use their relevance score, which is based on the intuition that keywords (or collocations) which characterize the difference of a focus corpus from a reference corpus are indicative of word sense novelty.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.922859400510788}]}, {"text": "We identify keywords fora focus corpus with respect to a reference corpus using method which is based on smoothed relative frequencies.", "labels": [], "entities": []}, {"text": "The novelty of an induced sense scan be then defined in terms of the aggregate keyword probabilities given that sense (and focus time of interest): where Wis a keyword list and t 2 the focus time.", "labels": [], "entities": []}, {"text": "suggest a straightforward extrapolation from sense novelty to word novelty: t 2 =1980-1999 union soviet united american union european war civil military people liberty dos system window disk pc operate program run computer de dos entertainment television industry program time business people world president entertainment company station radio station television local program network space tv broadcast air t 1 =1960-1969 t 2 =1990-1999 environmental supra note law protection id agency impact policy factor federal users computer window information software system wireless drive web building available virtual reality virtual computer center experience week community separation increase disk hard disk drive program computer file store ram business embolden where rel(c) is the highest novelty score assigned to any of the target word's senses.", "labels": [], "entities": [{"text": "virtual reality virtual computer center experience week community separation", "start_pos": 607, "end_pos": 683, "type": "TASK", "confidence": 0.5956439541445838}]}, {"text": "A high rel(c) score suggests that a word has undergone meaning change.", "labels": [], "entities": [{"text": "rel(c) score", "start_pos": 7, "end_pos": 19, "type": "METRIC", "confidence": 0.9474066972732544}]}, {"text": "We obtained candidate terms and their associated novel senses from the DATE corpus, using the relevance metric described above.", "labels": [], "entities": [{"text": "DATE corpus", "start_pos": 71, "end_pos": 82, "type": "DATASET", "confidence": 0.9089214205741882}]}, {"text": "The novel senses from the focus period and all senses induced for the reference period, except for the one corresponding to the novel sense, were passed onto WordNet-based evaluator which proceeds as follows.", "labels": [], "entities": []}, {"text": "Firstly, each induced sense sis mapped to the WordNet synset u with the maximum overlap: Next, a predicted novel sense n is deemed truly novel if its mapped synset is distinct from any synset mapped to a different induced sense: Finally, overall precision is calculated as the fraction of sense-births confirmed by WordNet overall birth-candidates proposed by the model.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9607747793197632}, {"text": "precision", "start_pos": 246, "end_pos": 255, "type": "METRIC", "confidence": 0.9985107779502869}]}, {"text": "Like we only report results on target words for which all induced senses could be successfully mapped to a synset.", "labels": [], "entities": []}, {"text": "In this experiment we evaluate whether model induced temporal word representations capture perceived word novelty.", "labels": [], "entities": []}, {"text": "Specifically, we adopt the evaluation framework (and dataset) introduced in Gulordava and Baroni (2011) and discussed below.", "labels": [], "entities": []}, {"text": "Method Gulordava and Baroni (2011) do not model word senses directly; instead they obtain distributional representations of words from the Google Books (bigram) data for two time slices, namely the 1960s (reference corpus) and 1990s (focus corpus).", "labels": [], "entities": [{"text": "Google Books (bigram) data", "start_pos": 139, "end_pos": 165, "type": "DATASET", "confidence": 0.7637303471565247}]}, {"text": "To detect change in meaning, they measure cosine similarity between the vector representations of a target word in the reference and focus corpus.", "labels": [], "entities": []}, {"text": "It is assumed that low similarity indicates significant meaning change.", "labels": [], "entities": [{"text": "similarity", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.9020941853523254}]}, {"text": "To evaluate the output of their system, they created a test set of 100 target words (nouns, verbs, and adjectives), and asked five annotators to rate each word with respect to its degree of meaning change between the 1960s and the 1990s.", "labels": [], "entities": []}, {"text": "The annotators used a 4-point ordinal scale (0: no change, 1: almost no change, 2: somewhat change, 3: changed significantly).", "labels": [], "entities": []}, {"text": "Words were subsequently ranked according to the mean rating given by the annotators.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement on the novel sense detection task was 0.51 (pairwise Pearson correlation) and can be regarded as an upper bound on model performance.", "labels": [], "entities": [{"text": "sense detection task", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8157487710316976}, {"text": "pairwise Pearson correlation)", "start_pos": 70, "end_pos": 99, "type": "METRIC", "confidence": 0.802094504237175}]}, {"text": "In the previous sections we demonstrated how SCAN captures meaning change between two periods.", "labels": [], "entities": [{"text": "SCAN", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.9788424968719482}]}, {"text": "In this section, we assess our model on an extrinsic task which relies on meaning representations spanning several time slices.", "labels": [], "entities": []}, {"text": "We quantitatively evaluate our model on the SemEval-2015 benchmark datasets released as part of the Diachronic Text Evaluation exercise (Popescu and Strapparava 2015; DTE).", "labels": [], "entities": [{"text": "SemEval-2015 benchmark datasets released", "start_pos": 44, "end_pos": 84, "type": "DATASET", "confidence": 0.847616121172905}, {"text": "Diachronic Text Evaluation exercise (Popescu and Strapparava 2015; DTE)", "start_pos": 100, "end_pos": 171, "type": "DATASET", "confidence": 0.6569077745079994}]}, {"text": "In the following we first present the DTE subtasks, and then move onto describe our training data, parameter settings, and systems used for comparison to our model.  is an umbrella term used by the SemEval-2015 organizers to represent three subtasks aiming to assess the performance of computational methods used to identify when apiece of text was written.", "labels": [], "entities": []}, {"text": "A similar problem is tackled in Chambers (2012) who label documents with time stamps whilst focusing on explicit time expressions and their discriminatory power.", "labels": [], "entities": []}, {"text": "The SemEval data consists of news snippets, which range between a few words and multiple sentences.", "labels": [], "entities": [{"text": "SemEval data", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.860653430223465}]}, {"text": "A set of training snippets, as well as gold-annotated development and test datasets are provided.", "labels": [], "entities": []}, {"text": "DTE subtasks 1 and 2 involve temporal classification: given a news snippet and a set of non-overlapping time intervals covering the period 1700 through 2010, the system's task is to select the interval corresponding to the snippet's year of origin.", "labels": [], "entities": [{"text": "DTE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8199788928031921}, {"text": "temporal classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.756564199924469}]}, {"text": "Temporal intervals are consecutive and constructed such that the correct interval is centered around the actual year of origin.", "labels": [], "entities": []}, {"text": "For both tasks temporal intervals are created at three levels of granularity (fine, medium, and coarse).", "labels": [], "entities": []}, {"text": "Subtask 1 involves snippets which contain an explicit cue for time of origin.", "labels": [], "entities": []}, {"text": "The presence of a temporal cue was determined by the organizers by checking the entities' informativeness in external resources.", "labels": [], "entities": []}, {"text": "Consider the example below: The mentions of French president de Gaulle and nuclear warfare suggest that the snippet was written after the mid-1950s and indeed it was published in 1962.", "labels": [], "entities": []}, {"text": "A hypothetical system would then have to decide amongst the following classes: The first set of classes correspond to fine-grained intervals of 2-years, the second set to medium-grained intervals of 6-years and the third set to coarsegrained intervals of 12-years.", "labels": [], "entities": []}, {"text": "For the snippet in example are the correct ones.", "labels": [], "entities": []}, {"text": "Subtask 2 involves temporal classification of snippets which lack explicit temporal cues, but contain implicit ones, e.g., as indicated by lexical choice or spelling.", "labels": [], "entities": [{"text": "temporal classification of snippets", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.7907978743314743}]}, {"text": "The snippet in example (9) was published in 1891 and the spelling of to-day, which was common up to the early 20th century, is an implicit cue: The local wheat market was not quite so strong to-day as yesterday.", "labels": [], "entities": [{"text": "spelling of to-day", "start_pos": 57, "end_pos": 75, "type": "METRIC", "confidence": 0.8963996767997742}]}, {"text": "Analogously to subtask 1, systems must select the right temporal interval from a set of contiguous time intervals of differing granularity.", "labels": [], "entities": []}, {"text": "For this task, which is admittedly harder, levels of temporal granularity are coarser corresponding to 6-year, 12-year and 20-year intervals.", "labels": [], "entities": []}, {"text": "Participating SemEval Systems We compared our model against three other systems which participated in the SemEval task.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.9138992726802826}]}, {"text": "8 AMBRA () adopts a learning-to-rank modeling approach and uses several stylistic, grammatical, and lexical features.", "labels": [], "entities": [{"text": "AMBRA", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.37579238414764404}]}, {"text": "IXA () uses a combination of approaches to determine the period of time in which apiece of news was written.", "labels": [], "entities": [{"text": "IXA", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6391466856002808}]}, {"text": "This involves searching for specific mentions of time within the text, searching for named entities present in the text and then establishing their reference time by linking these to Wikipedia, using Google n-grams, and linguistic features indicative of language change.", "labels": [], "entities": []}, {"text": "Finally, UCD employs SVMs for classification using a variety of informative features (e.g., POS-tag n-grams, syntactic phrases), which were optimized for the task through automatic feature selection.", "labels": [], "entities": []}, {"text": "Models and Parameters We trained our model for individual words and obtained representations of their meaning for different points in time.", "labels": [], "entities": []}, {"text": "Our set of target words consisted of all nouns which occurred in the development datasets for DTE subtasks 1 and 2 as well as all verbs which occurred at least twice in this dataset.", "labels": [], "entities": []}, {"text": "After removing infrequent words we were left with 883 words (out of 1,116) which we used in this evaluation.", "labels": [], "entities": []}, {"text": "Target words were not optimized with respect to the test data in anyway; it is thus reasonable to expect better performance with an adjusted set of words.", "labels": [], "entities": []}, {"text": "We set the model time interval to \u2206T = 5 years and the number of senses per word to K = 8.", "labels": [], "entities": []}, {"text": "We also evaluated SCAN-NOT, the stripped-down version of SCAN, with identical parameters.", "labels": [], "entities": []}, {"text": "Both SCAN and SCAN-NOT predict the time of origin fora test snippet as follows.", "labels": [], "entities": []}, {"text": "We first detect mentions of target words in the snippet.", "labels": [], "entities": []}, {"text": "Then, for each mention c we construct a document, akin to the training documents, consisting of c and its context w, the \u00b15 words surrounding c.", "labels": [], "entities": []}, {"text": "Given {c, w}, we approximate: Results on Diachronic Text Evaluation Tasks 1 and 2 fora random baseline, our SCAN model, its strippeddown version without iGMRFs (SCAN-NOT), the SemEval submissions (IXA, AMBRA and UCD), and SVMs trained with SCAN features (SVM SCAN), and with additional character n-gram features (SVM SCAN+ngram).", "labels": [], "entities": [{"text": "Diachronic Text Evaluation Tasks", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.6645765379071236}]}, {"text": "Results are shown for three levels of granularity, a strict precision measure p, and a distance-discounting measure acc.", "labels": [], "entities": [{"text": "precision measure p", "start_pos": 60, "end_pos": 79, "type": "METRIC", "confidence": 0.958936333656311}]}, {"text": "a distribution overtime intervals as: where the superscript (c) indicates parameters from the word-specific model, we marginalize over senses and assume a uniform distribution overtime slices p (c) (t).", "labels": [], "entities": []}, {"text": "Finally, we combine the word-wise predictions into a final distribution p(t) = c p (c) (t|, w), and predict the time t with highest probability.", "labels": [], "entities": []}, {"text": "Supervised Classification We also apply our model in a supervised setting, i.e., by extracting features for classifier prediction.", "labels": [], "entities": [{"text": "Supervised Classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5969459712505341}, {"text": "classifier prediction", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.8712879717350006}]}, {"text": "Specifically, we trained a multiclass SVM (Chang and Lin, 2011) on the training data provided by the SemEval organizers (for DTE tasks 1 and 2).", "labels": [], "entities": []}, {"text": "For each observed word within each snippet, we added as feature its most likely sense k given t, the true time of origin: We also trained a multiclass SVM which uses character n-gram (n \u2208 {1, 2, 3}) features in addition to the model features.", "labels": [], "entities": []}, {"text": "identified character n-grams as the most predictive feature for temporal text classification using SVMs.", "labels": [], "entities": [{"text": "temporal text classification", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6449391742547353}]}, {"text": "Their system (UCD) achieved the best published scores in DTE subtask 2.", "labels": [], "entities": [{"text": "DTE subtask", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.8519672751426697}]}, {"text": "Following their approach, we included all n-grams that were observed more than 20 times in the DTE training data.", "labels": [], "entities": [{"text": "DTE training data", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.8840132554372152}]}, {"text": "Results We employed two evaluation measures proposed by the DTE organizers.", "labels": [], "entities": [{"text": "DTE organizers", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9341281652450562}]}, {"text": "These are precision p, i.e., the percentage of times a system has predicted the correct time period.", "labels": [], "entities": [{"text": "precision p", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9833591878414154}]}, {"text": "And accuracy acc which is more lenient, and penalizes system predictions proportional to their distance from the true interval.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994589686393738}]}, {"text": "We compute the p and acc scores for our models using the evaluation script provided by the SemEval organizers.", "labels": [], "entities": []}, {"text": "summarizes our results for DTE subtasks 1 and 2.", "labels": [], "entities": []}, {"text": "We compare SCAN against a baseline which selects a time interval at random 9 averaged over five runs.", "labels": [], "entities": [{"text": "SCAN", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.8999391198158264}]}, {"text": "We also show results fora stripped-down version of our model without the iGMRFs (SCAN-NOT) and for the systems which participated in SemEval.", "labels": [], "entities": []}, {"text": "For subtask 1, the two versions of SCAN outperform all SemEval systems across the board.", "labels": [], "entities": []}, {"text": "SCAN-NOT occasionally outperforms SCAN in the strict precision metric, however, the full SCAN model consistently achieves better accuracy scores which are more representative since they factor in the proximity of the prediction to the true value.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9630206227302551}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9979724287986755}]}, {"text": "In subtask 2, the UCD and SVM SCAN+ngram systems perform comparably.", "labels": [], "entities": [{"text": "UCD", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.9221774339675903}]}, {"text": "They both use SVMs for the classification task, however our own model employs a less expressive feature set based on SCAN and character n-grams, and does not take advantage of feature selection which would presumably enhance performance.", "labels": [], "entities": []}, {"text": "With the exception of AMBRA, all other participating systems used external resources (such as Wikipedia and Google n-grams); it is thus fair to assume they had access to at least as much training data as our SCAN model.", "labels": [], "entities": [{"text": "AMBRA", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9217736721038818}]}, {"text": "Consequently, the gap in performance cannot solely be attributed to a difference in the size of the training data.", "labels": [], "entities": []}, {"text": "We also observe that IXA and SCAN, given identical granularity, perform better on subtask 1, while AMBRA and our own SVM-based systems exhibit the opposite trend.", "labels": [], "entities": [{"text": "AMBRA", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.8482053279876709}]}, {"text": "The IXA system uses a combination of knowledge sources in order to determine when apiece of news was written, including explicit mentions of temporal expressions within the text, named entities, and linked information to those named entities from Wikipedia.", "labels": [], "entities": []}, {"text": "AMBRA on the other hand exploits more shallow stylistic, grammatical and lexical features within the learning-to-rank paradigm.", "labels": [], "entities": [{"text": "AMBRA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.930893063545227}]}, {"text": "An interesting direction for future work would be to investigate which features are most appropriate for different DTE tasks.", "labels": [], "entities": [{"text": "DTE tasks", "start_pos": 115, "end_pos": 124, "type": "TASK", "confidence": 0.8577151298522949}]}, {"text": "Overall, it is encouraging to see that the generic temporal word representations inferred by SCAN lead to competitively performing models on both temporal classification tasks without any explicit tuning.", "labels": [], "entities": [{"text": "temporal classification tasks", "start_pos": 146, "end_pos": 175, "type": "TASK", "confidence": 0.7967570622762045}]}], "tableCaptions": [{"text": " Table 1: Size and coverage of our three training corpora  (after pre-processing).", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9859463572502136}]}, {"text": " Table 3: Spearman's \u03c1 rank correlations between system  novelty rankings and the human-produced ratings. All  correlations are statistically significant (p < 0.02). Re- sults for SCAN and SCAN-NOT are averages over five  trained models.", "labels": [], "entities": [{"text": "Re- sults", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9572475155194601}]}, {"text": " Table 4: Results on Diachronic Text Evaluation Tasks 1 and 2 for a random baseline, our SCAN model, its stripped- down version without iGMRFs (SCAN-NOT), the SemEval submissions (IXA, AMBRA and UCD), and SVMs trained  with SCAN features (SVM SCAN), and with additional character n-gram features (SVM SCAN+ngram). Results are  shown for three levels of granularity, a strict precision measure p, and a distance-discounting measure acc.", "labels": [], "entities": [{"text": "Diachronic Text Evaluation Tasks", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.7060910239815712}, {"text": "precision measure p", "start_pos": 375, "end_pos": 394, "type": "METRIC", "confidence": 0.9491907954216003}]}]}