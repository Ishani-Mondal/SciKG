{"title": [{"text": "Learning to Understand Phrases by Embedding the Dictionary", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional models that learn rich semantic word representations area success story of recent NLP research.", "labels": [], "entities": []}, {"text": "However, developing models that learn useful representations of phrases and sentences has proved far harder.", "labels": [], "entities": []}, {"text": "We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics.", "labels": [], "entities": []}, {"text": "Neural language embedding models can be effectively trained to map dictionary definitions (phrases) to (lexical) representations of the words defined by those definitions.", "labels": [], "entities": []}, {"text": "We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers.", "labels": [], "entities": [{"text": "general-knowledge crossword question answerers", "start_pos": 145, "end_pos": 191, "type": "TASK", "confidence": 0.567954421043396}]}, {"text": "On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering.", "labels": [], "entities": []}, {"text": "The results highlight the effectiveness of both neu-ral embedding architectures and definition-based training for developing models that understand phrases and sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much recent research in computational semantics has focussed on learning representations of arbitrary-length phrases and sentences.", "labels": [], "entities": []}, {"text": "This task is challenging partly because there is no obvious gold standard of phrasal representation that could be used * Work mainly done at the University of in training and evaluation.", "labels": [], "entities": []}, {"text": "Consequently, it is difficult to design approaches that could learn from such a gold standard, and also hard to evaluate or compare different models.", "labels": [], "entities": []}, {"text": "In this work, we use dictionary definitions to address this issue.", "labels": [], "entities": []}, {"text": "The composed meaning of the words in a dictionary definition (a tall, long-necked, spotted ruminant of Africa) should correspond to the meaning of the word they define (giraffe).", "labels": [], "entities": []}, {"text": "This bridge between lexical and phrasal semantics is useful because high quality vector representations of single words can be used as a target when learning to combine the words into a coherent phrasal representation.", "labels": [], "entities": []}, {"text": "This approach still requires a model capable of learning to map between arbitrary-length phrases and fixed-length continuous-valued word vectors.", "labels": [], "entities": []}, {"text": "For this purpose we experiment with two broad classes of neural language models (NLMs): Recurrent Neural Networks (RNNs), which naturally encode the order of input words, and simpler (feedforward) bag-of-words (BOW) embedding models.", "labels": [], "entities": []}, {"text": "Prior to training these NLMs, we learn target lexical representations by training the Word2Vec software () on billions of words of raw text.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.937960147857666}]}, {"text": "We demonstrate the usefulness of our approach by building and releasing two applications.", "labels": [], "entities": []}, {"text": "The first is a reverse dictionary or concept finder: a system that returns words based on user descriptions or definitions ().", "labels": [], "entities": [{"text": "concept finder", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7127330452203751}]}, {"text": "Reverse dictionaries are used by copywriters, novelists, translators and other professional writers to find words for notions or ideas that might be on the tip of their tongue.", "labels": [], "entities": [{"text": "Reverse dictionaries are used by copywriters, novelists, translators and other professional writers to find words for notions or ideas that might be on the tip of their tongue", "start_pos": 0, "end_pos": 175, "type": "Description", "confidence": 0.7856067657470703}]}, {"text": "For instance, a travel-writer might look to enhance her prose by searching for examples of a country that people associate with warm weather or an activity that is mentally or physically demanding.", "labels": [], "entities": []}, {"text": "We show that an NLM-based reverse dictionary trained on only a handful of dictionaries identifies novel definitions and concept descriptions comparably or better than commercial systems, which rely on significant task-specific engineering and access to much more dictionary data.", "labels": [], "entities": []}, {"text": "Moreover, by exploiting models that learn bilingual word representations), we show that the NLM approach can be easily extended to produce a potentially useful cross-lingual reverse dictionary.", "labels": [], "entities": []}, {"text": "The second application of our models is as a general-knowledge crossword question answerer.", "labels": [], "entities": [{"text": "general-knowledge crossword question answerer", "start_pos": 45, "end_pos": 90, "type": "TASK", "confidence": 0.6915009692311287}]}, {"text": "When trained on both dictionary definitions and the opening sentences of Wikipedia articles, NLMs produce plausible answers to (non-cryptic) crossword clues, even those that apparently require detailed world knowledge.", "labels": [], "entities": []}, {"text": "Both BOW and RNN models can outperform bespoke commercial crossword solvers, particularly when clues contain a greater number of words.", "labels": [], "entities": [{"text": "BOW", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.8861388564109802}, {"text": "crossword solvers", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.718042254447937}]}, {"text": "Qualitative analysis reveals that NLMs can learn to relate concepts that are not directly connected in the training data and can thus generalise well to unseen input.", "labels": [], "entities": []}, {"text": "To facilitate further research, all of our code, training and evaluation sets (together with a system demo) are published online with this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To our knowledge there are no established means of measuring reverse dictionary performance.", "labels": [], "entities": []}, {"text": "In the only previous academic research on English reverse dictionaries that we are aware of, evaluation was conducted on 300 word-definition pairs written by lexicographers (.", "labels": [], "entities": []}, {"text": "Since these are not publicly available we developed new evaluation sets and make them freely available for future evaluations.", "labels": [], "entities": []}, {"text": "The evaluation items are of three types, designed to test different properties of the models.", "labels": [], "entities": []}, {"text": "To create the seen evaluation, we randomly selected 500 words from the WordNet training data (seen by all models), and then randomly selected a definition for each word.", "labels": [], "entities": [{"text": "WordNet training data", "start_pos": 71, "end_pos": 92, "type": "DATASET", "confidence": 0.9481948614120483}]}, {"text": "Testing models on the resulting 500 word-definition pairs assesses their ability to recall or decode previously encoded information.", "labels": [], "entities": []}, {"text": "For the unseen evaluation, we randomly selected 500 words from WordNet and excluded all definitions of these words from the training data of all models.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9764853715896606}]}, {"text": "Finally, fora fair comparison with OneLook, which has both the seen and unseen pairs in its internal database, we built anew dataset of concept descriptions that do not appear in the training data for any model.", "labels": [], "entities": [{"text": "OneLook", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9520047307014465}]}, {"text": "To do so, we randomly selected 200 adjectives, nouns or verbs from among the top 3000 most frequent tokens in the British National Corpus () (but outside the top 100).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 114, "end_pos": 137, "type": "DATASET", "confidence": 0.923504114151001}]}, {"text": "We then asked ten native English speakers to write a single-sentence 'description' of these words.", "labels": [], "entities": []}, {"text": "To ensure the resulting descriptions were good quality, for each description we asked two participants who did not produce that description to list any words that fitted the description (up to a maximum of three).", "labels": [], "entities": []}, {"text": "If the target word was not produced by one of the two checkers, the original participant was asked to re-write the description until the validation was passed.", "labels": [], "entities": []}, {"text": "These concept descriptions, together with other evaluation sets, can be downloaded from our website for future comparisons.", "labels": [], "entities": []}, {"text": "Given a test description, definition, or question, all models produce a ranking of possible word answers based on the proximity of their representations of the input phrase and all possible output words.", "labels": [], "entities": []}, {"text": "To quantify the quality of a given ranking, we report three statistics: the median rank of the correct Re-writing was required in 6 of the 200 cases.", "labels": [], "entities": []}, {"text": "General Knowledge crossword questions come in different styles and forms.", "labels": [], "entities": [{"text": "General Knowledge crossword questions", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6814765930175781}]}, {"text": "We used the Eddie James crossword website to compile a bank of sentencelike general-knowledge questions.", "labels": [], "entities": [{"text": "Eddie James crossword website", "start_pos": 12, "end_pos": 41, "type": "DATASET", "confidence": 0.7153058350086212}]}, {"text": "Eddie James is one of the UK's leading crossword compilers, working for several national newspapers.", "labels": [], "entities": [{"text": "crossword compilers", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8719312250614166}]}, {"text": "Our long question set consists of the first 150 questions (starting from puzzle #1) from his general-knowledge crosswords, excluding clues of fewer than four words and those whose answer was not a single word (e.g. kingjames).", "labels": [], "entities": []}, {"text": "To evaluate models on a different type of clue, we also compiled a set of shorter questions based on the Guardian Quick Crossword.", "labels": [], "entities": [{"text": "Guardian Quick Crossword", "start_pos": 105, "end_pos": 129, "type": "DATASET", "confidence": 0.9530168573061625}]}, {"text": "Guardian questions still require general factual or linguistic knowledge, but are generally shorter and somewhat more cryptic than the longer Eddie James clues.", "labels": [], "entities": []}, {"text": "We again formed a list of 150 questions, beginning on 1 January 2015 and excluding any questions with multiple-word answers.", "labels": [], "entities": []}, {"text": "For clear contrast, we excluded those few questions of length greater than four words.", "labels": [], "entities": []}, {"text": "Of these 150 clues, a subset of 30 were single-word clues.", "labels": [], "entities": []}, {"text": "All evaluation datasets are available online with the paper.", "labels": [], "entities": []}, {"text": "As with the reverse dictionary experiments, candidates are extracted from models by inputting definitions and returning words corresponding to the closest embeddings in the target space.", "labels": [], "entities": []}, {"text": "In this case, however, we only consider candidate words whose length matches the length specified in the clue.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of different reverse dictionary models in different evaluation settings. *Low variance in mult  models is due to consistently poor scores, so not highlighted.", "labels": [], "entities": []}, {"text": " Table 6: Performance of different models on crossword questions of different length. The two commercial systems  are evaluated via their web interface so only accuracy@10 can be reported in those cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9989597797393799}]}]}