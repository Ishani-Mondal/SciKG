{"title": [], "abstractContent": [{"text": "Automatic satire detection is a subtle text classification task, for machines and at times, even for humans.", "labels": [], "entities": [{"text": "Automatic satire detection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7610476215680441}, {"text": "text classification task", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8010748823483785}]}, {"text": "In this paper we argue that satire detection should be approached using common-sense inferences, rather than traditional text classification methods.", "labels": [], "entities": [{"text": "satire detection", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.9763321876525879}, {"text": "text classification", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7482308447360992}]}, {"text": "We present a highly structured latent variable model capturing the required inferences.", "labels": [], "entities": []}, {"text": "The model abstracts over the specific entities appearing in the articles, grouping them into generalized categories, thus allowing the model to adapt to previously unseen situations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Satire is a writing technique for passing criticism using humor, irony or exaggeration.", "labels": [], "entities": [{"text": "Satire", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9542136788368225}, {"text": "passing criticism", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.9212448298931122}]}, {"text": "It is often used in contemporary politics to ridicule individual politicians, political parties or society as a whole.", "labels": [], "entities": []}, {"text": "We restrict ourselves in this paper to such political satire articles, broadly defined as articles whose purpose is not to report real events, but rather to mock their subject matter.", "labels": [], "entities": []}, {"text": "Satirical writing often builds on real facts and expectations, pushed to absurdity to express humorous insights about the situation.", "labels": [], "entities": [{"text": "Satirical writing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9185166358947754}]}, {"text": "As a result, the difference between real and satirical articles can be subtle and often confusing to readers.", "labels": [], "entities": []}, {"text": "With the recent rise of social media outlets, satirical articles have become increasingly popular and have famously fooled several leading news agencies . These misinterpretations can often https://newrepublic.com/article/118013/ satire-news-websites-are-cashing-gullibleoutraged-readers Vice President Joe Biden suddenly barged in, asking if anyone could \"hook up with a Dixie cup\" of their urine.", "labels": [], "entities": []}, {"text": "\"C'mon, you gotta help me get some clean whiz.", "labels": [], "entities": []}, {"text": "Shinseki, Donovan, I'm looking in your direction\" said Biden.", "labels": [], "entities": []}, {"text": "\"Do you want to hit this?\" a man asked President Barack Obama in a bar in Denver Tuesday night.", "labels": [], "entities": []}, {"text": "The president laughed but didn't indulge.", "labels": [], "entities": []}, {"text": "It wasn't the only time Obama was offered weed on his night out.", "labels": [], "entities": []}, {"text": "be attributed to careless reading, as there is a clear line between unusual events finding their way to the news and satire, which intentionally places key political figures in unlikely humorous scenarios.", "labels": [], "entities": []}, {"text": "The two can be separated by carefully reading the articles, exposing the satirical nature of the events described in such articles.", "labels": [], "entities": []}, {"text": "In this paper we follow this intuition.", "labels": [], "entities": []}, {"text": "We look into the satire detection task), predicting if a given news article is real or satirical, and suggest that this prediction task should be defined over common-sense inferences, rather than looking at it as a lexical text classification task, which bases the decision on word-level features.", "labels": [], "entities": [{"text": "satire detection task", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7782776057720184}, {"text": "predicting if a given news article is real or satirical", "start_pos": 41, "end_pos": 96, "type": "TASK", "confidence": 0.7908716142177582}]}, {"text": "To further motivate this observation, consider the two excerpts in.", "labels": [], "entities": []}, {"text": "Both excerpts mention top-ranking politicians (the President and Vice President) in a drug-related context, and contain informal slang utterances, inappropriate for the subjects' position.", "labels": [], "entities": []}, {"text": "The difference between the two examples is apparent when analyzing the situation described in the two articles: The first example (top), describes the Vice President speaking inappropriately in a work setting, clearly an unrealistic situation.", "labels": [], "entities": []}, {"text": "In the second (bottom) the President is spoken to inappropriately, an unlikely, yet not unrealistic, situation.", "labels": [], "entities": [{"text": "President", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.9347014427185059}]}, {"text": "From the perspective of our prediction task, it is advisable to base the prediction on a structured representation capturing the events and their participants, described in the text.", "labels": [], "entities": []}, {"text": "The absurdity of the situation described in satirical articles is often not unique to the specific individuals appearing in the narrative.", "labels": [], "entities": []}, {"text": "In our example, both politicians are interchangeable: placing the president in the situation described in the first excerpt would not make it less absurd.", "labels": [], "entities": []}, {"text": "It is therefore desirable to make a common-sense inference about high-ranking politicians in this scenario.", "labels": [], "entities": []}, {"text": "We follow these intuitions and suggest a novel approach for the satire prediction task.", "labels": [], "entities": [{"text": "satire prediction task", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.9299320181210836}]}, {"text": "Our model, COMSENSE, makes predictions by making common-sense inferences over a simplified narrative representation.", "labels": [], "entities": []}, {"text": "Similarly to prior work we represent the narrative structure by capturing the main entities (and tracking their mentions throughout the text), their activities, and their utterances.", "labels": [], "entities": []}, {"text": "The result of this process is a Narrative Representation Graph (NRG).", "labels": [], "entities": []}, {"text": "depicts examples of this representation for the excerpts in.", "labels": [], "entities": []}, {"text": "Given an NRG, our model makes inferences quantifying how likely are each of the represented events and interactions to appear in areal, or satirical context.", "labels": [], "entities": []}, {"text": "Annotating the NRG for such inferences is a challenging task, as the space of possible situations is extremely large.", "labels": [], "entities": []}, {"text": "Instead, we frame the required inferences as a highly-structured latent variable model, trained discriminatively as part of the prediction task.", "labels": [], "entities": []}, {"text": "Without explicit supervision, the model assigns categories to the NRG vertices (for example, by grouping politicians into a single category, or by grouping inappropriate slang utterances, regardless of specific word choice).", "labels": [], "entities": []}, {"text": "These category assignments form the infrastructure for higher-level reasoning, as they allows the model to identify the commonalities between unrelated people, their actions and their words.", "labels": [], "entities": []}, {"text": "The model learns commonsense patterns leading to real or satirical decisions based on these categories.", "labels": [], "entities": []}, {"text": "We express these patterns as parametrized rules (acting as global features in the prediction model), and base the prediction on their activation values.", "labels": [], "entities": []}, {"text": "In our example, these rules can capture the combination of (E P olitician ) \u2227 (Q slang )\u2192 Satire, where E P olitician and Q slang are latent variable assignments to entity and utterance categories respectively.", "labels": [], "entities": []}, {"text": "Our experiments look into two variants of satire prediction: using full articles, and the more challenging sub-task of predicting if a quote is real given its speaker.", "labels": [], "entities": [{"text": "satire prediction", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7809950113296509}, {"text": "predicting if a quote is real", "start_pos": 119, "end_pos": 148, "type": "TASK", "confidence": 0.8119011123975118}]}, {"text": "We use two datasets collected 6 years apart.", "labels": [], "entities": []}, {"text": "The first collected in 2009) and an additional dataset collected recently.", "labels": [], "entities": []}, {"text": "Since satirical articles tend to focus on current events, the two datasets describe different people and world events.", "labels": [], "entities": []}, {"text": "To demonstrate the robustness of our COMSENSE approach we use the first dataset for training, and the second as out-of-domain test data.", "labels": [], "entities": []}, {"text": "We compare COMSENSE to several competing systems including a state-of-the-art Convolutional Neural Network.", "labels": [], "entities": []}, {"text": "Our experiments show that COMSENSE outperforms all other models.", "labels": [], "entities": []}, {"text": "Most interestingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since our goal is to identify satirical articles, given significantly more real articles, we report the Fmeasure of the positive class.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9825538992881775}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "We can see that in all cases the COMSENSE model obtains the best results.", "labels": [], "entities": []}, {"text": "We note that in both tasks, when learning in the outof-domain settings performance drops sharply, however the gap between the COMSENSE model and other models increases in these settings, showing that it is less prone to overfitting.", "labels": [], "entities": []}, {"text": "Interestingly, for the satire detection (SD) task, the COMSENSE Q model performs best for the indomain setting, and COMSENSE F gives the best performance in the out-of-domain settings.", "labels": [], "entities": [{"text": "satire detection (SD) task", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.8949200510978699}, {"text": "COMSENSE F", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.7262836694717407}]}, {"text": "We hypothesize that this is due to a phenomenon we call \"overfitting to document structure\".", "labels": [], "entities": []}, {"text": "Lexical models tend to base the decision on word choices specific to the training data, and as a result when tested on out of domain data, which describes new events and entities, performance drops sharply.", "labels": [], "entities": []}, {"text": "Instead, the COM-SENSE Q model focuses on properties of quotations and entities appearing in the text.", "labels": [], "entities": []}, {"text": "In the SD'09 datasets, this information helps focus the learner, as the real and satire articles are structured differently (for example, satire articles frequently contain multiple quotes).", "labels": [], "entities": [{"text": "SD'09 datasets", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.8787749111652374}]}, {"text": "This structure is not maintained when working with out-of-domain data, and indeed in these settings the model benefits from using additional information offered by the full model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the SD task", "labels": [], "entities": [{"text": "SD task", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9750257730484009}]}, {"text": " Table 3: Results for the DIST task", "labels": [], "entities": [{"text": "DIST task", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.7079243659973145}]}]}