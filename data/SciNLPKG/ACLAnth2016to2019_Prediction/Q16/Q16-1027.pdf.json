{"title": [{"text": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7157898346583048}]}], "abstractContent": [{"text": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8337104717890421}, {"text": "machine translation (MT)", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8603433251380921}]}, {"text": "However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 143, "end_pos": 145, "type": "TASK", "confidence": 0.9451107978820801}]}, {"text": "In this work, we introduce anew type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers.", "labels": [], "entities": []}, {"text": "Fast-forward connections play an essential role in propagating the gradients and building a deep topol-ogy of depth 16.", "labels": [], "entities": []}, {"text": "On the WMT'14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points.", "labels": [], "entities": [{"text": "WMT'14 English-to-French task", "start_pos": 7, "end_pos": 36, "type": "DATASET", "confidence": 0.7603007157643636}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9997228980064392}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9975302815437317}]}, {"text": "This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9989830851554871}]}, {"text": "We can still achieve BLEU=36.3 even without using an attention mechanism.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9996028542518616}]}, {"text": "After special handling of unknown words and model ensem-bling, we obtain the best score reported to date on this task with BLEU=40.4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9997254014015198}]}, {"text": "Our models are also validated on the more difficult WMT'14 English-to-German task.", "labels": [], "entities": [{"text": "WMT'14 English-to-German task", "start_pos": 52, "end_pos": 81, "type": "DATASET", "confidence": 0.7855341633160909}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8383271296819051}, {"text": "machine translation (MT) problem", "start_pos": 80, "end_pos": 112, "type": "TASK", "confidence": 0.8759903659423193}]}, {"text": "Unlike conventional statistical machine translation (SMT) systems () which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.7892757852872213}]}, {"text": "Moreover, NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems () and image caption generation (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.8754404187202454}, {"text": "image caption generation", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.8422019084294637}]}, {"text": "In general, there are two types of NMT topologies: the encoder-decoder network) and the attention network (.", "labels": [], "entities": []}, {"text": "The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword.", "labels": [], "entities": []}, {"text": "The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words.", "labels": [], "entities": []}, {"text": "Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9927224516868591}]}, {"text": "However, a single neural model of either of the above types has not been competitive with the best conventional system () when evaluated on the WMT'14 English-to-French task.", "labels": [], "entities": [{"text": "WMT'14 English-to-French task", "start_pos": 144, "end_pos": 173, "type": "DATASET", "confidence": 0.8356208999951681}]}, {"text": "The best BLEU score from a single model with six layers is only 31.) while the conventional method of () achieves 37.0.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9984844326972961}]}, {"text": "We focus on improving the single model perfor-mance by increasing the model depth.", "labels": [], "entities": []}, {"text": "Deep topology has been proven to outperform the shallow architecture in computer vision.", "labels": [], "entities": [{"text": "Deep topology", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7119038701057434}]}, {"text": "In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers (.", "labels": [], "entities": []}, {"text": "But in NMT, the biggest depth used successfully is only six ().", "labels": [], "entities": [{"text": "NMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.6609558463096619}]}, {"text": "We attribute this problem to the properties of the Long ShortTerm Memory (LSTM)) which is widely used in NMT.", "labels": [], "entities": [{"text": "Long ShortTerm Memory (LSTM))", "start_pos": 51, "end_pos": 80, "type": "METRIC", "confidence": 0.7515803376833597}, {"text": "NMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.874893069267273}]}, {"text": "In the LSTM, there are more non-linear activations than in convolution layers.", "labels": [], "entities": []}, {"text": "These activations significantly decrease the magnitude of the gradient in the deep topology, especially when the gradient propagates in recurrent form.", "labels": [], "entities": []}, {"text": "There are also many efforts to increase the depth of the LSTM such as the work by, where the shortcuts do not avoid the nonlinear and recurrent computation.", "labels": [], "entities": []}, {"text": "In this work, we introduce anew type of linear connections for multi-layer recurrent networks.", "labels": [], "entities": []}, {"text": "These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16.", "labels": [], "entities": []}, {"text": "In addition, we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder.", "labels": [], "entities": []}, {"text": "This topology can be used for both the encoder-decoder network and the attention network.", "labels": [], "entities": []}, {"text": "On the WMT'14 Englishto-French task, this is the deepest NMT topology that has ever been investigated.", "labels": [], "entities": [{"text": "WMT'14 Englishto-French task", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.8737378716468811}]}, {"text": "With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers () by 6.2 BLEU points.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9812215268611908}, {"text": "BLEU", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9982739686965942}]}, {"text": "This is also the first time on this task that a single NMT model achieves state-of-the-art performance and outperforms the best conventional SMT system () with an improvement of 0.7.", "labels": [], "entities": [{"text": "SMT", "start_pos": 141, "end_pos": 144, "type": "TASK", "confidence": 0.9864147305488586}]}, {"text": "Even without using the attention mechanism, we can still achieve 36.3 with a single model.", "labels": [], "entities": [{"text": "36.3", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9885376691818237}]}, {"text": "After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9808093011379242}]}, {"text": "When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4.", "labels": [], "entities": []}, {"text": "As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about).", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9846838116645813}, {"text": "BLEU score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9855621457099915}]}, {"text": "Our models are also validated on the more difficult WMT'14 English-toGerman task.", "labels": [], "entities": [{"text": "WMT'14 English-toGerman task", "start_pos": 52, "end_pos": 80, "type": "DATASET", "confidence": 0.8849925597508749}]}], "datasetContent": [{"text": "We evaluate our method mainly on the widely used WMT'14 English-to-French translation task.", "labels": [], "entities": [{"text": "WMT'14 English-to-French translation task", "start_pos": 49, "end_pos": 90, "type": "TASK", "confidence": 0.7736517637968063}]}, {"text": "In order to validate our model on more difficult language pairs, we also provide results on the WMT'14 English-to-German translation task.", "labels": [], "entities": [{"text": "WMT'14 English-to-German translation task", "start_pos": 96, "end_pos": 137, "type": "TASK", "confidence": 0.7297648414969444}]}, {"text": "Our models are implemented in the PADDLE (PArallel Distributed Deep LEarning) platform.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The effect of F-F. We list the BLEU scores of  Deep-Att with and without F-F. Because of the param- eter exploding problem, we can not list the model per- formance of larger depth without F-F. For n e = 1 and  n d = 1, F-F connections only contribute to the represen- tation at interface part (see Eq. 7).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9987439513206482}]}, {"text": " Table 3: BLEU scores with different LSTM layer width  in Deep-Att. After using two times larger LSTM layer  width of 1024, we can only obtain BLEU score of 33.8.  It is still behind the corresponding Deep-Att with F-F.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994221925735474}, {"text": "BLEU score", "start_pos": 143, "end_pos": 153, "type": "METRIC", "confidence": 0.9875930845737457}]}, {"text": " Table 4: The effect of the interleaved bi-directional en- coder. We list the BLEU scores of our largest Deep-Att  and Deep-ED models. The encoder term Bi denotes that  the interleaved bi-directional encoder is used. Uni de- notes a model where all LSTM layers work in forward  direction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9992458820343018}, {"text": "Bi", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.9046465158462524}]}, {"text": " Table 5: BLEU score of Deep-Att with different model  depth. With n e = 1 and n d = 1, F-F connections only  contribute to the representation at interface part where f t  is included (see Eq. 7).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986467957496643}]}, {"text": " Table 6: Comparison of encoders with different number  of columns and LSTM layer width.", "labels": [], "entities": []}, {"text": " Table 7: English-to-German task: BLEU scores of single  neural models. We also list the conventional SMT system  for comparison.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9992387294769287}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9880905151367188}]}, {"text": " Table 8: BLEU scores of different models. The first  two blocks are our results of two single models and mod- els with post processing. In the last block we list two  baselines of the best conventional SMT system and NMT  system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9980276226997375}, {"text": "SMT", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.9877033233642578}]}, {"text": " Table 9: BLEU scores of the subset of the test set without  considering unknown words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993557333946228}]}]}