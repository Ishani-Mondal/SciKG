{"title": [{"text": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8347797989845276}]}, {"text": "Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors.", "labels": [], "entities": []}, {"text": "The BiLSTM is trained jointly with the parser objective , resulting in very effective feature ex-tractors for parsing.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7401195764541626}, {"text": "parser", "start_pos": 39, "end_pos": 45, "type": "TASK", "confidence": 0.9603881239891052}, {"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.9736829996109009}]}, {"text": "We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser.", "labels": [], "entities": []}, {"text": "The resulting parsers have very simple architec-tures, and match or surpass the state-of-the-art accuracies on English and Chinese.", "labels": [], "entities": []}], "introductionContent": [{"text": "The focus of this paper is on feature representation for dependency parsing, using recent techniques from the neural-networks (\"deep learning\") literature.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.8578120470046997}]}, {"text": "Modern approaches to dependency parsing can be broadly categorized into graph-based and transition-based parsers (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8132624626159668}]}, {"text": "Graph-based parsers) treat parsing as a search-based structured prediction problem in which the goal is learning a scoring function over dependency trees such that the correct tree is scored above all other trees.", "labels": [], "entities": []}, {"text": "Transition-based parsers) treat parsing as a sequence of actions that produce a parse tree, and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process.", "labels": [], "entities": []}, {"text": "Perhaps the simplest graph-based parsers are arc-factored (first order) models), in which the scoring function fora tree decomposes over the individual arcs of the tree.", "labels": [], "entities": []}, {"text": "More elaborate models look at larger (overlapping) parts, requiring more sophisticated inference and training algorithms).", "labels": [], "entities": []}, {"text": "The basic transition-based parsers work in a greedy manner, performing a series of locally-optimal decisions, and boast very fast parsing speeds.", "labels": [], "entities": []}, {"text": "More advanced transition-based parsers introduce some search into the process using abeam ( or dynamic programming).", "labels": [], "entities": []}, {"text": "Regardless of the details of the parsing framework being used, a crucial step in parser design is choosing the right feature function for the underlying statistical model.", "labels": [], "entities": [{"text": "parser design", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9324924945831299}]}, {"text": "Recent work (see Section 2.2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of \"core\" features and leaving it up to the machine-learning machinery to come up with good feature combinations.", "labels": [], "entities": []}, {"text": "However, the need to carefully define a set of core features remains.", "labels": [], "entities": []}, {"text": "For example, the work of uses 18 different elements in its feature function, while the work of uses 21 different elements.", "labels": [], "entities": []}, {"text": "Other works, notably and, propose more sophisticated feature representations, in which the feature engineering is replaced with architecture engineering.", "labels": [], "entities": []}, {"text": "In this work, we suggest an approach which is much simpler in terms of both feature engineering and architecture engineering.", "labels": [], "entities": []}, {"text": "Our proposal (Section 3) is centered around BiRNNs (, and more specifically BiLSTMs, which are strong and trainable sequence models (see Section 2.3).", "labels": [], "entities": []}, {"text": "The BiLSTM excels at representing elements in a sequence (i.e., words) together with their contexts, capturing the element and an \"infinite\" window around it.", "labels": [], "entities": []}, {"text": "We represent each word by its BiLSTM encoding, and use a concatenation of a minimal set of such BiLSTM encodings as our feature function, which is then passed to a non-linear scoring function (multi-layer perceptron).", "labels": [], "entities": []}, {"text": "Crucially, the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem.", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.9175916016101837}]}, {"text": "If we set aside the inherent complexity of the BiLSTM itself and treat it as a black box, our proposal results in a pleasingly simple feature extractor.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures, transition-based (Section 4) as well as a graph-based (Section 5).", "labels": [], "entities": []}, {"text": "In the graphbased parser, we jointly train a structured-prediction model on top of a BiLSTM, propagating errors from the structured objective all the way back to the BiLSTM feature-encoder.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to perform such end-to-end training of a structured prediction model and a recurrent feature extractor for non-sequential outputs.", "labels": [], "entities": []}, {"text": "Aside from the novelty of the BiLSTM feature extractor and the end-to-end structured training, we rely on existing models and techniques from the parsing and structured prediction literature.", "labels": [], "entities": [{"text": "BiLSTM feature extractor", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6349864403406779}, {"text": "parsing and structured prediction", "start_pos": 146, "end_pos": 179, "type": "TASK", "confidence": 0.6844365447759628}]}, {"text": "We stick to the simplest parsers in each categorygreedy inference for the transition-based architecture, and a first-order, arc-factored model for the graph-based architecture.", "labels": [], "entities": []}, {"text": "Despite the simplicity of the parsing architectures and the feature functions, we achieve near state-of-the-art parsing accuracies in both English (93.1 UAS) and Chinese (86.6 UAS), using a first-order parser with two features and while training solely on Treebank data, without relying on semi-supervised signals such as pre-trained word embeddings, word-clusters (, or tech-niques such as tri-training (.", "labels": [], "entities": []}, {"text": "When also including pre-trained word embeddings, we obtain further improvements, with accuracies of 93.9 UAS (English) and 87.6 UAS (Chinese) fora greedy transition-based parser with 11 features, and 93.6 UAS (En) / 87.4 (Ch) fora greedy transitionbased parser with 4 features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our parsing model on English and Chinese data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9661999940872192}]}, {"text": "For comparison purposes we follow the setup of.", "labels": [], "entities": []}, {"text": "Data For English, we used the Stanford Dependency (SD) (   same predicted POS-tags as used in;.", "labels": [], "entities": []}, {"text": "This dataset contains a few non-projective trees.", "labels": [], "entities": []}, {"text": "Punctuation symbols are excluded from the evaluation.", "labels": [], "entities": []}, {"text": "For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of) with gold partof-speech tags, also following (.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5.1 (CTB5)", "start_pos": 24, "end_pos": 56, "type": "DATASET", "confidence": 0.9719032304627555}]}, {"text": "When using external word embeddings, we also use the same data as.", "labels": [], "entities": []}, {"text": "Implementation Details The parsers are implemented in python, using the PyCNN toolkit 11 for neural network training.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.7731344103813171}, {"text": "PyCNN toolkit 11", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.9257963101069132}]}, {"text": "The code is available at the github repository https://github.com/ elikip/bist-parser.", "labels": [], "entities": []}, {"text": "We use the LSTM variant implemented in PyCNN, and optimize using the Adam optimizer ( The word and POS embeddings e(w i ) and e(p i ) are initialized to random values and trained together with the rest of the parsers' networks.", "labels": [], "entities": []}, {"text": "In some experiments, we introduce also pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "In those cases, the vector representation of a word is a concatenation of its randomlyinitialized vector embedding with its pre-trained word vector.", "labels": [], "entities": []}, {"text": "Both are tuned during training.", "labels": [], "entities": []}, {"text": "We use the same word vectors as in During training, we employ a variant of word dropout, and replace a word with the unknown-word symbol with probability that is inversely proportional to the frequency of the word.", "labels": [], "entities": []}, {"text": "A word w appearing #(w) times in the training corpus is replaced with the unknown symbol with probability punk (w) = \u03b1 #(w)+\u03b1 . If a word was dropped the external embedding of the word is also dropped with probability 0.5.", "labels": [], "entities": []}, {"text": "We train the parsers for up to 30 iterations, and choose the best model according to the UAS accuracy on the development set.", "labels": [], "entities": [{"text": "UAS", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.4118635058403015}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.5290043354034424}]}, {"text": "Hyperparameter Tuning We performed a very minimal hyper-parameter search with the graph-323 based parser, and use the same hyper-parameters for both parsers.", "labels": [], "entities": []}, {"text": "The hyper-parameters of the final networks used for all the reported experiments are detailed in  Main Results lists the test-set accuracies of our best parsing models, compared to other state-ofthe-art parsers from the literature.", "labels": [], "entities": [{"text": "Main Results", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9272928237915039}]}, {"text": "It is clear that our parsers are very competitive, despite using very simple parsing architectures and minimal feature extractors.", "labels": [], "entities": []}, {"text": "When not using external embeddings, the first-order graph-based parser with 2 features outperforms all other systems that are not using external resources, including the third-order TurboParser.", "labels": [], "entities": []}, {"text": "The greedy transition based parser with 4 features also matches or outperforms most other parsers, including the beam-based transition parser with heavily engineered features of Zhang and Nivre (2011) and the Stack-LSTM parser of, as well as the same parser when trained using a dynamic oracle ( . Moving from the simple (4 features) to the extended (11 features) feature set leads to some gains inaccuracy for both English and Chinese.", "labels": [], "entities": []}, {"text": "Interestingly, when adding external word embeddings the accuracy of the graph-based parser degrades.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995787739753723}]}, {"text": "We are not sure why this happens, and leave the exploration of effective semi-supervised parsing with the graph-based model for future work.", "labels": [], "entities": []}, {"text": "The greedy parser does manage to benefit from the external embeddings, and using them we also see gains from moving from the simple to the extended feature set.", "labels": [], "entities": []}, {"text": "Both feature sets result in very competitive re-12 Unfortunately, many papers still report English parsing results on the deficient Yamada and Matsumoto head rules (PTB-YM) rather than the more modern Stanford-dependencies (PTB-SD).", "labels": [], "entities": [{"text": "Stanford-dependencies (PTB-SD)", "start_pos": 201, "end_pos": 231, "type": "DATASET", "confidence": 0.6792835965752602}]}, {"text": "We note that the PTB-YM and PTB-SD results are not strictly comparable, and in our experience the PTB-YM results are usually about half a UAS point higher.", "labels": [], "entities": [{"text": "PTB-YM", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.8311426639556885}, {"text": "PTB-SD", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.7837218046188354}, {"text": "UAS", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9079309105873108}]}, {"text": "sults, with the extended feature set yielding the best reported results for Chinese, and ranked second for English, after the heavily-tuned beam-based parser of.", "labels": [], "entities": []}, {"text": "Additional Results We perform some ablation experiments in order to quantify the effect of the different components on our best models  Loss augmented inference is crucial for the success of the graph-based parser, and the multi-task learning scheme for the arc-labeler contributes nicely to the unlabeled scores.", "labels": [], "entities": []}, {"text": "Dynamic oracle training yields nice gains for both English and Chinese.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test-set parsing results of various state-of-the-art parsing systems on the English (PTB) and Chinese (CTB) datasets. The", "labels": [], "entities": [{"text": "English (PTB) and Chinese (CTB) datasets", "start_pos": 86, "end_pos": 126, "type": "DATASET", "confidence": 0.6283288627862931}]}, {"text": " Table 2: Hyper-parameter values used in experiments", "labels": [], "entities": [{"text": "Hyper-parameter", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9224585890769958}]}, {"text": " Table 3: Ablation experiments results (dev set) for the graph-", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9920755624771118}]}]}