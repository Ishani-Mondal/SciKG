{"title": [{"text": "Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models", "labels": [], "entities": [{"text": "Part-Of-Speech Tagging", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6795590072870255}]}], "abstractContent": [{"text": "We tackle unsupervised part-of-speech (POS) tagging by learning hidden Markov models (HMMs) that are particularly well-suited for the problem.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6720555305480957}]}, {"text": "These HMMs, which we call anchor HMMs, assume that each tag is associated with at least one word that can have no other tag, which is a relatively benign condition for POS tagging (e.g., \"the\" is a word that appears only under the determiner tag).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 168, "end_pos": 179, "type": "TASK", "confidence": 0.9243628084659576}]}, {"text": "We exploit this assumption and extend the non-negative matrix factorization framework of Arora et al.", "labels": [], "entities": []}, {"text": "(2013) to design a consistent estimator for anchor HMMs.", "labels": [], "entities": []}, {"text": "In experiments, our algorithm is competitive with strong base-lines such as the clustering method of Brown et al.", "labels": [], "entities": []}, {"text": "(1992) and the log-linear model of Berg-Kirkpatrick et al.", "labels": [], "entities": []}, {"text": "Furthermore, it produces an interpretable model in which hidden states are automatically lexicalized by words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech (POS) tagging without supervision is a quintessential problem in unsupervised learning for natural language processing (NLP).", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5785391509532929}, {"text": "natural language processing (NLP)", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.7760749161243439}]}, {"text": "A major application of this task is reducing annotation cost: for instance, it can be used to produce rough syntactic annotations fora new language that has no labeled data, which can be subsequently refined by human annotators.", "labels": [], "entities": []}, {"text": "Hidden Markov models (HMMs) area natural choice of model and have been a workhorse for this problem.", "labels": [], "entities": []}, {"text": "Early works estimated vanilla HMMs * Currently on leave at Google Inc.", "labels": [], "entities": []}, {"text": "New with standard unsupervised learning methods such as the expectation-maximization (EM) algorithm, but it quickly became clear that they performed very poorly in inducing POS tags.", "labels": [], "entities": []}, {"text": "Later works improved upon vanilla HMMs by incorporating specific structures that are well-suited for the task, such as a sparse prior or a hard-clustering assumption.", "labels": [], "entities": []}, {"text": "In this work, we tackle unsupervised POS tagging with HMMs whose structure is deliberately suitable for POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8384512066841125}, {"text": "POS tagging", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.8829999566078186}]}, {"text": "These HMMs impose an assumption that each hidden state is associated with an observation state (\"anchor word\") that can appear under no other state.", "labels": [], "entities": []}, {"text": "For this reason, we denote this class of restricted HMMs by anchor HMMs.", "labels": [], "entities": []}, {"text": "Such an assumption is relatively benign for POS tagging; it is reasonable to assume that each POS tag has at least one word that occurs only under that tag.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.8724164664745331}]}, {"text": "For example, in English, \"the\" is an anchor word for the determiner tag; \"laughed\" is an anchor word for the verb tag.", "labels": [], "entities": []}, {"text": "We build on the non-negative matrix factorization (NMF) framework of to derive a consistent estimator for anchor HMMs.", "labels": [], "entities": []}, {"text": "We make several new contributions in the process.", "labels": [], "entities": []}, {"text": "First, to our knowledge, there is no previous work directly building on this framework to address unsupervised sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.635524719953537}]}, {"text": "Second, we generalize the NMF-based learning algorithm to obtain extensions that are important for empirical performance.", "labels": [], "entities": []}, {"text": "Third, we perform extensive experiments on unsupervised POS tagging and report competitive results against strong baselines such as the clustering method of and the log-linear model of.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8848764002323151}]}, {"text": "One characteristic of the approach is the immediate interpretability of inferred hidden states.", "labels": [], "entities": []}, {"text": "Because each hidden state is associated with an observation, we can examine the set of such anchor observations to qualitatively evaluate the learned model.", "labels": [], "entities": []}, {"text": "In our experiments on POS tagging, we find that anchor observations correspond to possible POS tags across different languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9176567196846008}]}, {"text": "This property can be useful when we wish to develop a tagger fora new language that has no labeled data; we can label only the anchor words to achieve a complete labeling of the data.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we establish the notation we use throughout.", "labels": [], "entities": []}, {"text": "In Section 3, we define the model family of anchor HMMs.", "labels": [], "entities": []}, {"text": "In Section 4, we derive a matrix decomposition algorithm for estimating the parameters of an anchor HMM.", "labels": [], "entities": []}, {"text": "In Section 5, we present our experiments on unsupervised POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.8195314109325409}]}, {"text": "In Section 6, we discuss related works.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our A-HMM learning algorithm on the task of unsupervised POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.7179540395736694}]}, {"text": "The goal of this task is to induce the correct sequence of POS tags (hidden states) given a sequence of words (observation states).", "labels": [], "entities": []}, {"text": "The anchor condition corresponds to assuming that each POS tag has at least one word that occurs only under that tag.", "labels": [], "entities": []}, {"text": "We use the universal treebank dataset (version 2.0) which contains sentences annotated with 12 POS tag types for 10 languages ).", "labels": [], "entities": [{"text": "universal treebank dataset", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.6572998861471812}]}, {"text": "All models are trained with 12 hidden states.", "labels": [], "entities": []}, {"text": "We use the English portion to experiment with different hyperparameter configurations.", "labels": [], "entities": []}, {"text": "At test time, we fix a configuration (based on the English portion) and apply it across all languages.", "labels": [], "entities": []}, {"text": "The list of compared methods is given below: BW The Baum-Welch algorithm, an EM algorithm for HMMs (.", "labels": [], "entities": [{"text": "BW", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9772301316261292}]}, {"text": "CLUSTER A parameter estimation scheme for HMMs based on Brown clustering (.", "labels": [], "entities": [{"text": "CLUSTER", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7532006502151489}]}, {"text": "We run the Brown clustering algorithm 1 to obtain 12 word clusters C 1 . .", "labels": [], "entities": []}, {"text": "C 12 . Then we set the emission parameters o(x|h), transition parameters t(h |h), and prior \u03c0(h) to be the maximumlikelihood estimates under the fixed clusters.", "labels": [], "entities": []}, {"text": "ANCHOR Our algorithm Learn-Anchor-HMM in but with the constrained optimization and for estimating \u03c0 and T . ANCHOR-FEATURES Same as ANCHOR but employs the feature augmentation scheme described in Section 4.4.4.", "labels": [], "entities": [{"text": "ANCHOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6472434401512146}, {"text": "ANCHOR-FEATURES", "start_pos": 108, "end_pos": 123, "type": "METRIC", "confidence": 0.8634040951728821}]}, {"text": "LOG-LINEAR The unsupervised log-linear model described in.", "labels": [], "entities": [{"text": "LOG-LINEAR", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8701716661453247}]}, {"text": "Instead of emission parameters o(x|h), the model maintains a miniature log-linear model with a weight vector wand a feature function \u03c6.", "labels": [], "entities": []}, {"text": "The probability of a word x given tag h is computed as The model can be trained by maximizing the likelihood of observed sequences.", "labels": [], "entities": []}, {"text": "We use L-BFGS to directly optimize this objective.", "labels": [], "entities": []}, {"text": "This approach obtains the current state-of-the-art accuracy on finegrained (45 tags) English WSJ dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.999134361743927}, {"text": "finegrained (45 tags) English WSJ dataset", "start_pos": 63, "end_pos": 104, "type": "DATASET", "confidence": 0.6416275463998318}]}, {"text": "We use maximum marginal decoding for HMM predictions: i.e., at each position, we predict the most likely tag given the entire sentence.", "labels": [], "entities": [{"text": "HMM predictions", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8924042284488678}]}], "tableCaptions": [{"text": " Table 2: Many-to-one accuracy on each language using 12 universal tags. The first four models are HMMs estimated  with the Baum-Welch algorithm (BW), the clustering algorithm of", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9926612377166748}]}, {"text": " Table 3: Anchor words found in each language (model ANCHOR-FEATURES).", "labels": [], "entities": [{"text": "ANCHOR-FEATURES", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9694711565971375}]}, {"text": " Table 4: Most likely words under each anchor word (English model ANCHOR-FEATURES). Emission probabilities  o(x|h) are given in parentheses.", "labels": [], "entities": [{"text": "ANCHOR-FEATURES", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.9752444624900818}, {"text": "Emission probabilities  o", "start_pos": 84, "end_pos": 109, "type": "METRIC", "confidence": 0.9268545707066854}]}, {"text": " Table 5: Verifying model assumptions on the universal treebank. The anchor assumption is satisfied in every language.  The Brown assumption (each word has exactly one possible tag) is violated but not by a large margin. The lower table  shows the most frequent anchor word and its count under each tag on the English portion.", "labels": [], "entities": []}, {"text": " Table 6: Log likelihood normalized by the number of  words on English (along with accuracy). For BW, we  report the mean of 10 random restarts run for 1,000 it- erations.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.5547415018081665}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9997082352638245}, {"text": "BW", "start_pos": 98, "end_pos": 100, "type": "DATASET", "confidence": 0.52065110206604}]}]}