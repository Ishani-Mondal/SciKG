{"title": [{"text": "J-NERD: Joint Named Entity Recognition and Disambiguation with Rich Linguistic Features", "labels": [], "entities": [{"text": "J-NERD", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8546345233917236}, {"text": "Joint Named Entity Recognition", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.5112503618001938}]}], "abstractContent": [{"text": "Methods for Named Entity Recognition and Disambiguation (NERD) perform NER and NED in two separate stages.", "labels": [], "entities": [{"text": "Named Entity Recognition and Disambiguation (NERD", "start_pos": 12, "end_pos": 61, "type": "TASK", "confidence": 0.7641804388591221}, {"text": "NER", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8200424909591675}]}, {"text": "Therefore, NED maybe penalized with respect to precision by NER false positives, and suffers in recall from NER false negatives.", "labels": [], "entities": [{"text": "NED", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.5815392136573792}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9992801547050476}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9993600249290466}]}, {"text": "Conversely, NED does not fully exploit information computed by NER such as types of mentions.", "labels": [], "entities": []}, {"text": "This paper presents J-NERD, anew approach to perform NER and NED jointly, by means of a prob-abilistic graphical model that captures mention spans, mention types, and the mapping of mentions to entities in a knowledge base.", "labels": [], "entities": []}, {"text": "We present experiments with different kinds of texts from the CoNLL'03, ACE'05, and ClueWeb'09-FACC1 corpora.", "labels": [], "entities": [{"text": "CoNLL'03", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9313132762908936}, {"text": "ACE'05", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.8754878640174866}, {"text": "ClueWeb'09-FACC1 corpora", "start_pos": 84, "end_pos": 108, "type": "DATASET", "confidence": 0.8780835270881653}]}, {"text": "J-NERD consistently outperforms state-of-the-art competitors in end-to-end NERD precision, recall, and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9877142906188965}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9992449283599854}, {"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9993588328361511}]}], "introductionContent": [{"text": "Motivation: Methods for Named Entity Recognition and Disambiguation, NERD for short, typically proceed in two stages: \u2022 At the NER stage, text spans of entity mentions are detected and tagged with coarse-grained types like Person, Organization, Location, etc.", "labels": [], "entities": [{"text": "Named Entity Recognition and Disambiguation", "start_pos": 24, "end_pos": 67, "type": "TASK", "confidence": 0.7259527325630188}]}, {"text": "This is typically performed by a trained Conditional Random Field (CRF) over word sequences (e.g.,).", "labels": [], "entities": []}, {"text": "\u2022 At the NED stage, mentions are mapped to entities in a knowledge base (KB) based on contextual similarity measures and the semantic coherence of the selected entities (e.g., Cucerzan This two-stage approach has limitations.", "labels": [], "entities": []}, {"text": "First, NER may produce false positives that can misguide NED.", "labels": [], "entities": [{"text": "NER", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9778443574905396}]}, {"text": "Second, NER may miss out on some entity mentions, and NED has no chance to compensate for these false negatives.", "labels": [], "entities": [{"text": "NER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.5345637202262878}, {"text": "NED", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.6948485374450684}]}, {"text": "Third, NED is notable to help NER, for example, by disambiguating \"easy\" mentions (e.g., of prominent entities with more or less unique names), and then using the entities and knowledge about them as enriched features for NER.", "labels": [], "entities": []}, {"text": "Example: Consider the following sentences: David played for manu, real, and la galaxy.", "labels": [], "entities": []}, {"text": "His wife posh performed with the spice girls.", "labels": [], "entities": []}, {"text": "This is difficult for NER because of the absence of upper-case spelling, which is not untypical in social media, for example.", "labels": [], "entities": [{"text": "NER", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9508351683616638}]}, {"text": "Most NER methods will miss out on multi-word mentions or words that are also common nouns (\"spice\") or adjectives (\"posh\", \"real\").", "labels": [], "entities": [{"text": "NER", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9388185739517212}]}, {"text": "Typically, NER would pass only the mentions \"David\", \"manu\", and \"la\" to the NED stage, which then is prone to many errors like mapping the first two mentions to any prominent people with first names David and Manu, and mapping the third one to the city of Los Angeles.", "labels": [], "entities": []}, {"text": "With NER and NED performed jointly, the possible disambiguation of \"la galaxy\" to the soccer club can guide NER to tag the right mentions with the right types (e.g., recognizing that \"manu\" could be a short name fora soccer team), which in turn helps NED to map \"David\" to the right entity David Beckham.", "labels": [], "entities": []}, {"text": "Contribution: This paper presents a novel kind of probabilistic graphical model for the joint recognition and disambiguation of named-entity mentions in natural-language texts.", "labels": [], "entities": [{"text": "joint recognition and disambiguation of named-entity mentions in natural-language texts", "start_pos": 88, "end_pos": 175, "type": "TASK", "confidence": 0.8193040072917939}]}, {"text": "With this integrated approach to NERD, we aim to overcome the limitations of the two-stage NER/NED methods discussed above.", "labels": [], "entities": [{"text": "NERD", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.9713266491889954}]}, {"text": "Our method, called J-NERD 1 , is based on a supervised, non-linear graphical model that combines multiple per-sentence models into an entitycoherence-aware global model.", "labels": [], "entities": []}, {"text": "The global model detects mention spans, tags them with coarsegrained types, and maps them to entities in a single joint-inference step based on the Viterbi algorithm (for exact inference) or Gibbs sampling (for approximate inference).", "labels": [], "entities": []}, {"text": "The J-NERD method comprises the following novel contributions: \u2022 a tree-shaped model for each sentence, whose structure is derived from the dependency parse tree and thus captures linguistic context in a deeper way compared to prior work with CRF's for NER and NED; \u2022 richer linguistic features not considered in prior work, harnessing dependency parse trees and verbal patterns that indicate mention types as part of their nsubj or dobj arguments; \u2022 an inference method that maintains the uncertainty of both mention candidates (i.e., token spans) and entity candidates for competing mention candidates, and makes joint decisions, as opposed to fixing mentions before reasoning on their disambiguation.", "labels": [], "entities": []}, {"text": "We present experiments with three major datasets: the CoNLL'03 collection of newswire articles, the ACE'05 corpus of news and blogs, and the ClueWeb'09-FACC1 corpus of web pages.", "labels": [], "entities": [{"text": "CoNLL'03 collection of newswire articles", "start_pos": 54, "end_pos": 94, "type": "DATASET", "confidence": 0.9351411461830139}, {"text": "ACE'05 corpus of news and blogs", "start_pos": 100, "end_pos": 131, "type": "DATASET", "confidence": 0.9471724530061086}, {"text": "ClueWeb'09-FACC1 corpus of web pages", "start_pos": 141, "end_pos": 177, "type": "DATASET", "confidence": 0.9046230554580689}]}, {"text": "Baselines that we compare J-NERD with include AIDAlight (), Spotlight (, and TagMe, and the recent joint NER/NED method of.", "labels": [], "entities": []}, {"text": "J-NERD consistently outperforms these competitors in terms of both precision and recall.", "labels": [], "entities": [{"text": "J-NERD", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8465404510498047}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9996854066848755}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9989483952522278}]}], "datasetContent": [{"text": "We evalute the output quality at the NER level alone and for the end-to-end NERD task.", "labels": [], "entities": [{"text": "NER", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.7005981802940369}, {"text": "NERD task", "start_pos": 76, "end_pos": 85, "type": "TASK", "confidence": 0.8738038837909698}]}, {"text": "We do not evaluate NED alone, as this would require giving a ground-truth set of mentions to the systems to rule out that NER errors affect NED.", "labels": [], "entities": [{"text": "NED", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.94427490234375}]}, {"text": "Most competitors do not have interfaces for such a controlled NEDonly evaluation.", "labels": [], "entities": []}, {"text": "Each test collection has ground-truth annotations (G) consisting of text spans for mentions, NER types of the mentions, and mapping mentions to entities in the KB or to Out-of-KB.", "labels": [], "entities": []}, {"text": "Recall that the Out-of-KB case captures entities that are not in the KB at all.", "labels": [], "entities": []}, {"text": "Let X be the output of system X: detected mentions, NER types, NED mappings.", "labels": [], "entities": []}, {"text": "Following the), we define precision and recall of X for endto-end NERD as: where agreement means that X and G overlap in the text spans (i.e., have at least one token in common) fora mention, have the same NER type, and have the same mapping to an entity or Out-of-KB.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9992856383323669}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9990732669830322}]}, {"text": "The F 1 score of X is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9840686122576395}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9994490742683411}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9971229434013367}]}, {"text": "For evaluating the mention-boundary detection alone, we consider only the overlap of text spans; for evaluating NER completely, we consider both mention overlap and agreement based on the assigned NER types.", "labels": [], "entities": [{"text": "mention-boundary detection", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6684253662824631}, {"text": "NER", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.8602026700973511}]}, {"text": "In the rest of our experiments, we focus on J-NERD tree-global and the task of end-to-end NERD.", "labels": [], "entities": [{"text": "J-NERD tree-global", "start_pos": 44, "end_pos": 62, "type": "DATASET", "confidence": 0.7530008852481842}, {"text": "NERD", "start_pos": 90, "end_pos": 94, "type": "TASK", "confidence": 0.9258304238319397}]}], "tableCaptions": [{"text": " Table 2: Experiments on CoNLL-YAGO2.", "labels": [], "entities": []}, {"text": " Table 3: Comparison between joint models and pipelined  models on end-to-end NERD.", "labels": [], "entities": []}, {"text": " Table 3. J-NERD  achieves the highest precision of 81.9% for end- to-end NERD, outperforming all competitors by a  significant margin. This results in achieving the  best F 1 score of 78.7%, which is 1.2% higher than  P-NERD and 1.4% higher than AIDA-light. Note  that", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9992457628250122}, {"text": "F 1 score", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9901517629623413}, {"text": "AIDA-light", "start_pos": 247, "end_pos": 257, "type": "DATASET", "confidence": 0.8800230622291565}]}, {"text": " Table 4: Experiments on NER against state-of-the-art  NER systems.", "labels": [], "entities": [{"text": "NER", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9203854203224182}]}, {"text": " Table 5: Feature Influence on CoNLL-YAGO2.", "labels": [], "entities": [{"text": "CoNLL-YAGO2", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.8441435098648071}]}, {"text": " Table 6: NERD results on ACE.", "labels": [], "entities": [{"text": "NERD", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.8386213779449463}, {"text": "ACE", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.5671750903129578}]}, {"text": " Table 7: NERD results on ClueWeb.", "labels": [], "entities": [{"text": "NERD", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.8773528933525085}]}]}