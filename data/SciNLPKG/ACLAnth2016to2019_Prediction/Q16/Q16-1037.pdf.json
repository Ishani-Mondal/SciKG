{"title": [{"text": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "labels": [], "entities": [{"text": "Ability", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9059022665023804}]}], "abstractContent": [{"text": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities.", "labels": [], "entities": [{"text": "language processing", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7835380136966705}]}, {"text": "Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?", "labels": [], "entities": []}, {"text": "We begin addressing this question using number agreement in English subject-verb dependencies.", "labels": [], "entities": []}, {"text": "We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models.", "labels": [], "entities": [{"text": "number prediction", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.6977164149284363}]}, {"text": "In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9989016056060791}, {"text": "errors", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9839795827865601}]}, {"text": "The frequency of such errors rose sharply in the language-modeling setting.", "labels": [], "entities": [{"text": "frequency", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9702815413475037}]}, {"text": "We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures maybe required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks (RNNs) are highly effective models of sequential data.", "labels": [], "entities": []}, {"text": "The rapid adoption of RNNs in NLP systems in recent years, in particular of RNNs with gating mechanisms such as long short-term memory (LSTM) units) or gated recurrent units (GRU) (, has led to significant gains in language modeling (), parsing, machine translation () and other tasks.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 215, "end_pos": 232, "type": "TASK", "confidence": 0.7494384348392487}, {"text": "parsing", "start_pos": 237, "end_pos": 244, "type": "TASK", "confidence": 0.9779000878334045}, {"text": "machine translation", "start_pos": 246, "end_pos": 265, "type": "TASK", "confidence": 0.8065866231918335}]}, {"text": "The effectiveness of RNNs 1 is attributed to their ability to capture statistical contingencies that may span an arbitrary number of words.", "labels": [], "entities": []}, {"text": "The word France, for example, is more likely to occur somewhere in a sentence that begins with Paris than in a sentence that begins with Penguins.", "labels": [], "entities": []}, {"text": "The fact that an arbitrary number of words can intervene between the mutually predictive words implies that they cannot be captured by models with a fixed window such as n-gram models, but can in principle be captured by RNNs, which do not have an architecturally fixed limit on dependency length.", "labels": [], "entities": []}, {"text": "RNNs are sequence models: they do not explicitly incorporate syntactic structure.", "labels": [], "entities": []}, {"text": "Indeed, many word co-occurrence statistics can be captured by treating the sentence as an unstructured list of words (ParisFrance); it is therefore unsurprising that RNNs can learn them well.", "labels": [], "entities": [{"text": "ParisFrance", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.6399241089820862}]}, {"text": "Other dependencies, however, are sensitive to the syntactic structure of the sentence.", "labels": [], "entities": []}, {"text": "To what extent can RNNs learn to model such phenomena based only on sequential cues?", "labels": [], "entities": []}, {"text": "Previous research has shown that RNNs (in particular LSTMs) can learn artificial context-free languages) as well as nesting and indentation in a programming language.", "labels": [], "entities": []}, {"text": "The goal of the present work is to probe their ability to learn natural language hierarchical (syntactic) structures from a corpus without syntactic annotations.", "labels": [], "entities": []}, {"text": "As a first step, we focus on a particular dependency that is commonly regarded as evidence for hierarchical structure inhuman language: English subject-verb agreement, the phenomenon in which the form of a verb depends on whether the subject is singular or plural (the kids play but the kid plays; see additional details in Section 2).", "labels": [], "entities": [{"text": "English subject-verb agreement", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.5419415434201559}]}, {"text": "If an RNN-based model succeeded in learning this dependency, that would indicate that it can learn to approximate or even faithfully implement syntactic structure.", "labels": [], "entities": []}, {"text": "Our main interest is in whether LSTMs have the capacity to learn structural dependencies from a natural corpus.", "labels": [], "entities": []}, {"text": "We therefore begin by addressing this question under the most favorable conditions: training with explicit supervision.", "labels": [], "entities": []}, {"text": "In the setting with the strongest supervision, which we refer to as the number prediction task, we train it directly on the task of guessing the number of a verb based on the words that preceded it (Sections 3 and 4).", "labels": [], "entities": [{"text": "number prediction task", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7727058231830597}, {"text": "guessing the number of a verb based on the words that preceded it", "start_pos": 132, "end_pos": 197, "type": "TASK", "confidence": 0.6640288440080789}]}, {"text": "We further experiment with a grammaticality judgment training objective, in which we provide the model with full sentences annotated as to whether or not they violate subject-verb number agreement, without an indication of the locus of the violation (Section 5).", "labels": [], "entities": []}, {"text": "Finally, we trained the model without any grammatical supervision, using a language modeling objective (predicting the next word).", "labels": [], "entities": [{"text": "predicting the next word", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.852389857172966}]}, {"text": "Our quantitative results (Section 4) and qualitative analysis (Section 7) indicate that most naturally occurring agreement cases in the Wikipedia corpus are easy: they can be resolved without syntactic information, based only on the sequence of nouns preceding the verb.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 136, "end_pos": 152, "type": "DATASET", "confidence": 0.88004931807518}]}, {"text": "This leads to high overall accuracy in all models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9991114735603333}]}, {"text": "Most of our experiments focus on the supervised number prediction model.", "labels": [], "entities": [{"text": "supervised number prediction", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.6489551464716593}]}, {"text": "The accuracy of this model was lower on harder cases, which require the model to encode or approximate structural information; nevertheless, it succeeded in recovering the majority of agreement cases even when four nouns of the opposite number intervened between the subject and the verb (17% errors).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993188381195068}]}, {"text": "Baseline models failed spectacularly on these hard cases, performing far below chance levels.", "labels": [], "entities": [{"text": "chance", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9835695624351501}]}, {"text": "Fine-grained analysis revealed that mistakes are much more common when no overt cues to syntactic structure (in particular function words) are available, as is the casein noun-noun compounds and reduced relative clauses.", "labels": [], "entities": []}, {"text": "This indicates that the number prediction model indeed managed to capture a decent amount of syntactic knowledge, but was overly reliant on function words.", "labels": [], "entities": [{"text": "number prediction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8134275674819946}]}, {"text": "Error rates increased only mildly when we switched to more indirect supervision consisting only of sentence-level grammaticality annotations without an indication of the crucial verb.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9859842658042908}]}, {"text": "By contrast, the language model trained without explicit grammatical supervision performed worse than chance on the harder agreement prediction cases.", "labels": [], "entities": [{"text": "agreement prediction", "start_pos": 123, "end_pos": 143, "type": "TASK", "confidence": 0.7367937564849854}]}, {"text": "Even a state-ofthe-art large-scale language model () was highly sensitive to recent but structurally irrelevant nouns, making more than five times as many mistakes as the number prediction model on these harder cases.", "labels": [], "entities": [{"text": "number prediction", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.6957819610834122}]}, {"text": "These results suggest that explicit supervision is necessary for learning the agreement dependency using this architecture, limiting its plausibility as a model of child language acquisition.", "labels": [], "entities": []}, {"text": "From a more applied perspective, this result suggests that for tasks in which it is desirable to capture syntactic dependencies (e.g., machine translation or language generation), language modeling objectives should be supplemented by supervision signals that directly capture the desired behavior.", "labels": [], "entities": [{"text": "machine translation or language generation", "start_pos": 135, "end_pos": 177, "type": "TASK", "confidence": 0.7256285965442657}, {"text": "language modeling", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7035957276821136}]}], "datasetContent": [{"text": "Comparison to simple recurrent networks: How much of the success of the network is due to the LSTM cells?", "labels": [], "entities": []}, {"text": "We repeated the number prediction experiment with a simple recurrent network (SRN), with the same number of hidden units.", "labels": [], "entities": [{"text": "number prediction", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7827983498573303}]}, {"text": "The SRN's performance was inferior to the One technical exception was that we did not replace lowfrequency words with their part-of-speech, since the Google LM is a large-vocabulary language model, and does not have parts-of-speech as part of its vocabulary.", "labels": [], "entities": []}, {"text": "LSTM's, but the average performance fora given number of agreement attractors does not suggest a qualitative difference between the cell types: the SRN makes about twice as many errors as the LSTM across the board).", "labels": [], "entities": []}, {"text": "Training only on difficult dependencies: Only a small proportion of the dependencies in the corpus had agreement attractors).", "labels": [], "entities": []}, {"text": "Would the network generalize better if dependencies with intervening nouns were emphasized during training?", "labels": [], "entities": []}, {"text": "We repeated our number prediction experiment, this time training the model only on dependencies with at least one intervening noun (of any number).", "labels": [], "entities": [{"text": "number prediction", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7822566330432892}]}, {"text": "We doubled the proportion of training sentences to 20%, since the total size of the corpus was smaller (226K dependencies).", "labels": [], "entities": []}, {"text": "This training regime resulted in a 27% decrease in error rate on dependencies with exactly one attractor (from 4.1% to 3.0%).", "labels": [], "entities": [{"text": "error rate", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9808663427829742}]}, {"text": "This decrease is statistically significant, and encouraging given that the total number of dependencies in training was much lower, which complicates the learning of word embeddings.", "labels": [], "entities": []}, {"text": "Error rates mildly decreased in dependencies with more attractors as well, suggesting some generalization.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9911851286888123}]}, {"text": "Surprisingly, a similar experiment using the grammaticality judgment task led to a slight increase in error rate.", "labels": [], "entities": [{"text": "grammaticality judgment task", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7793601751327515}, {"text": "error rate", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.98714679479599}]}, {"text": "While tentative at this point, these results suggest that oversampling difficult training cases maybe beneficial; a curriculum progressing from easier to harder dependencies may provide additional gains.", "labels": [], "entities": []}], "tableCaptions": []}