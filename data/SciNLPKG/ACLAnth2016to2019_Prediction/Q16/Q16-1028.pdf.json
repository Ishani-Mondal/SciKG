{"title": [{"text": "A Latent Variable Model Approach to PMI-based Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods.", "labels": [], "entities": []}, {"text": "Many use non-linear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods.", "labels": [], "entities": []}, {"text": "This paper proposes anew generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007).", "labels": [], "entities": [{"text": "generative", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9673009514808655}]}, {"text": "The method-ological novelty is to use the prior to compute closed form expressions for word statistics.", "labels": [], "entities": []}, {"text": "This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparame-ter choices.", "labels": [], "entities": []}, {"text": "It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al.", "labels": [], "entities": []}, {"text": "(2013a) and many subsequent papers.", "labels": [], "entities": []}, {"text": "Experimental support is provided for the gen-erative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector representations of words (word embeddings) try to capture relationships between words as distance or angle, and have many applications in computational linguistics and machine learning.", "labels": [], "entities": []}, {"text": "They are constructed by various models whose unifying philosophy is that the meaning of a word is defined by \"the company it keeps\", namely, co-occurrence statistics.", "labels": [], "entities": []}, {"text": "The simplest methods use word vectors that explicitly represent cooccurrence statistics.", "labels": [], "entities": []}, {"text": "Reweighting heuristics are known to improve these methods, as is dimension reduction).", "labels": [], "entities": [{"text": "dimension reduction", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7843815982341766}]}, {"text": "Some reweighting methods are nonlinear, which include taking the square root of co-occurrence counts (), or the logarithm, or the related Pointwise Mutual Information (PMI)).", "labels": [], "entities": []}, {"text": "These are collectively referred to as Vector Space Models, surveyed in (.", "labels": [], "entities": []}, {"text": "Neural network language models propose another way to construct embeddings: the word vector is simply the neural network's internal representation for the word.", "labels": [], "entities": []}, {"text": "This method is nonlinear and nonconvex.", "labels": [], "entities": []}, {"text": "It was popularized via word2vec, a family of energy-based models in (), followed by a matrix factorization approach called GloVe).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.8461012840270996}]}, {"text": "The first paper also showed how to solve analogies using linear algebra on word embeddings.", "labels": [], "entities": []}, {"text": "Experiments and theory were used to suggest that these newer methods are related to the older PMI-based models, but with new hyperparameters and/or term reweighting methods ().", "labels": [], "entities": []}, {"text": "But note that even the old PMI method is a bit mysterious.", "labels": [], "entities": [{"text": "PMI", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9475135803222656}]}, {"text": "The simplest version considers asymmetric matrix with each row/column indexed by a word.", "labels": [], "entities": []}, {"text": "The entry for (w, w ) is PMI(w, w ) = log p(w,w ) p(w)p(w ) , where p(w, w ) is the empirical probability of words w, w appearing within a window of certain size in the corpus, and p(w) is the marginal probability of w.", "labels": [], "entities": [{"text": "PMI", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6976362466812134}]}, {"text": "(More complicated models could use asymmetric matrices with columns corresponding to context words or phrases, and also involve tensorization.)", "labels": [], "entities": []}, {"text": "Then word vectors are obtained by lowrank SVD on this matrix, or a related matrix with term reweightings.", "labels": [], "entities": []}, {"text": "In particular, the PMI matrix is found to be closely approximated by a low rank matrix: there exist word vectors in say 300 dimensions, which is much smaller than the number of words in the dictionary, such that v w , v w \u2248 PMI(w, w ) where \u2248 should be interpreted loosely.", "labels": [], "entities": [{"text": "PMI", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8987593054771423}]}, {"text": "There appears to be no theoretical explanation for this empirical finding about the approximate low rank of the PMI matrix.", "labels": [], "entities": []}, {"text": "The current paper addresses this.", "labels": [], "entities": []}, {"text": "Specifically, we propose a probabilistic model of text generation that augments the log-linear topic model of with dynamics, in the form of a random walkover a latent discourse space.", "labels": [], "entities": [{"text": "text generation", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7552177906036377}]}, {"text": "The chief methodological contribution is using the model priors to analytically derive a closedform expression that directly explains (1.1); see Theorem 2.2 in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 builds on this insight to give a rigorous justification for models such as word2vec and GloVe, including the hyperparameter choices for the latter.", "labels": [], "entities": []}, {"text": "The insight also leads to a mathematical explanation for why these word embeddings allow analogies to be solved using linear algebra; see Section 4.", "labels": [], "entities": []}, {"text": "Section 5 shows good empirical fit to this model's assumtions and predictions, including the surprising one that word vectors are pretty uniformly distributed (isotropic) in space.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide experiments empirically supporting our generative model.", "labels": [], "entities": [{"text": "generative", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.9728178977966309}]}, {"text": "All word embedding vectors are trained on the English Wikipedia (March 2015 dump).", "labels": [], "entities": [{"text": "English Wikipedia (March 2015 dump", "start_pos": 46, "end_pos": 80, "type": "DATASET", "confidence": 0.9422204593817393}]}, {"text": "It is pre-processed by standard approach (removing nontextual elements, sentence splitting, and tokenization), leaving about 3 billion tokens.", "labels": [], "entities": [{"text": "sentence splitting", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7368378043174744}]}, {"text": "Words that appeared less than 1000 times in the corpus are ignored, resulting in a vocabulary of 68, 430.", "labels": [], "entities": []}, {"text": "The cooccurrence is then computed using windows of 10 tokens to each side of the focus word.", "labels": [], "entities": []}, {"text": "Our embedding vectors are trained by optimizing the SN objective using AdaGrad () with initial learning rate of 0.05 and 100 iterations.", "labels": [], "entities": [{"text": "SN", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9527506232261658}]}, {"text": "The PMI objective derived from (2.5) was also used.", "labels": [], "entities": [{"text": "PMI objective", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8322770893573761}]}, {"text": "SN has average (weighted) term-wise error of 5%, and PMI has 17%.", "labels": [], "entities": [{"text": "term-wise error", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.73392054438591}, {"text": "PMI", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.4557032287120819}]}, {"text": "We observed that SN vectors typically fit the model better and have better performance, which can be explained by larger errors in PMI, as implied by Theorem 2.2.", "labels": [], "entities": []}, {"text": "So, we only report the results for SN.", "labels": [], "entities": [{"text": "SN", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.7629828453063965}]}], "tableCaptions": [{"text": " Table 1: The accuracy on two word analogy task testbeds:  G (the GOOGLE testbed); M (the MSR testbed). Per- formance is close to the state of the art despite using a  generative model with provable properties.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995837807655334}, {"text": "GOOGLE testbed", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.7592002153396606}, {"text": "MSR testbed", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.7827017903327942}]}, {"text": " Table 3: The accuracy of the RD algorithm (i.e., the  cheater method) on the GOOGLE testbed. The RD al- gorithm is described in the text. For comparison, the row  \"w/o RD\" shows the accuracy of the old method without  using RD.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996151924133301}, {"text": "GOOGLE testbed", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9411954283714294}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9992071986198425}]}, {"text": " Table 4: The accuracy of the RD-nn algorithm on the  GOOGLE testbed. The algorithm is described in the text.  For comparison, the row \"w/o RD-nn\" shows the accu- racy of the old method without using RD-nn.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9997498393058777}, {"text": "GOOGLE testbed", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.973859965801239}]}]}