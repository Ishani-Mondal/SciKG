{"title": [{"text": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "labels": [], "entities": [{"text": "Easy-First Dependency Parsing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.34751789768536884}]}], "abstractContent": [{"text": "We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network en-coders.", "labels": [], "entities": []}, {"text": "To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency-based syntactic representations of sentences are central to many language processing tasks (.", "labels": [], "entities": []}, {"text": "Dependency parse-trees encode not only the syntactic structure of a sentence but also many aspects of its semantics.", "labels": [], "entities": []}, {"text": "A recent trend in NLP is concerned with encoding sentences as vectors (\"sentence embeddings\"), which can then be used for further prediction tasks.", "labels": [], "entities": []}, {"text": "Recurrent neural networks (RNNs), and in particular methods based on the LSTM architecture), work very well for modeling sequences, and constantly obtain state-of-the-art results on both languagemodeling and prediction tasks (see, e.g. ().", "labels": [], "entities": []}, {"text": "Several works attempt to extend recurrent neural networks to work on trees (see Section 8 fora brief overview), giving rise to the so-called recursive neural networks.", "labels": [], "entities": []}, {"text": "However, recursive neural networks do not cope well with trees with arbitrary branching factors -most work require the encoded trees to be binary-branching, or have a fixed maximum arity.", "labels": [], "entities": []}, {"text": "Other attempts allow arbitrary branching factors, at the expense of ignoring the order of the modifiers.", "labels": [], "entities": []}, {"text": "In contrast, we propose a tree-encoding that naturally supports trees with arbitrary branching factors, making it particularly appealing for dependency trees.", "labels": [], "entities": []}, {"text": "Our tree encoder uses recurrent neural networks as a building block: we model the left and right sequences of modifiers using RNNs, which are composed in a recursive manner to form a tree (Section 3).", "labels": [], "entities": []}, {"text": "We use our tree representation for encoding the partially-built parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of.", "labels": [], "entities": []}, {"text": "Using the Hierarchical Tree LSTM representation, and without using any external embeddings, our parser achieves parsing accuracies of 92.6 UAS and 90.2 LAS on the PTB (Stanford dependencies) and 86.1 UAS and 84.4 LAS on the Chinese treebank, while relying on greedy decoding.", "labels": [], "entities": [{"text": "UAS", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9377378225326538}, {"text": "PTB", "start_pos": 163, "end_pos": 166, "type": "DATASET", "confidence": 0.9765843749046326}, {"text": "Chinese treebank", "start_pos": 224, "end_pos": 240, "type": "DATASET", "confidence": 0.9438759386539459}]}, {"text": "To the best of our knowledge, this is the first work to demonstrate competitive parsing accuracies for full-scale parsing while relying solely on recursive, compositional tree representations, and without using a reranking framework.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 8.", "labels": [], "entities": []}, {"text": "While the parsing experiments demonstrate the suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman- tic tasks such as sentiment analysis, sentence similarity judgements () and textual entailment).", "labels": [], "entities": [{"text": "predicting parsing", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.7859295010566711}, {"text": "sentiment analysis", "start_pos": 327, "end_pos": 345, "type": "TASK", "confidence": 0.9437135756015778}, {"text": "sentence similarity judgements", "start_pos": 347, "end_pos": 377, "type": "TASK", "confidence": 0.727516104777654}]}], "datasetContent": [{"text": "We evaluated our parsing model to English and Chinese data.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.970367431640625}]}, {"text": "For comparison purposes we followed the setup of . Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank, using the standard train/dev/test splitswith the same predicted POS-tags as used in.", "labels": [], "entities": [{"text": "Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion", "start_pos": 81, "end_pos": 148, "type": "DATASET", "confidence": 0.8388303560870034}, {"text": "Penn Treebank", "start_pos": 156, "end_pos": 169, "type": "DATASET", "confidence": 0.9632263779640198}]}, {"text": "This dataset contains a few non-projective trees.", "labels": [], "entities": []}, {"text": "Punctuation symbols are excluded from the evaluation.", "labels": [], "entities": []}, {"text": "For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of) with gold partof-speech tags, also following (.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5.1 (CTB5)", "start_pos": 24, "end_pos": 56, "type": "DATASET", "confidence": 0.9719032304627555}]}, {"text": "When using external word embeddings, we also use the same data as ).", "labels": [], "entities": []}, {"text": "Experimental configurations We evaluated the parser in several configurations BOT-TOMUPPARSER is the baseline parser, not using the tree-encoding, and instead representing each item in pending solely by the vector-representation (word and POS) of its headword.", "labels": [], "entities": [{"text": "BOT-TOMUPPARSER", "start_pos": 78, "end_pos": 93, "type": "METRIC", "confidence": 0.995776355266571}]}, {"text": "BOTTOMUPPARSER+HTLSTM is using our Hierarchical Tree LSTM representation.", "labels": [], "entities": [{"text": "BOTTOMUPPARSER", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.905789315700531}]}, {"text": "BOTTOMUPPARSER+HTLSTM+BI-LSTM is the Hierarchical Tree LSTM where we additionally use a BI-LSTM encoding for the head words.", "labels": [], "entities": [{"text": "BOTTOMUPPARSER", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.941555380821228}, {"text": "BI-LSTM", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9776877760887146}]}, {"text": "Finally, we added external, pre-trained word embeddings to the BOTTOMUPPARSER+HTLSTM+BI-LSTM setup.", "labels": [], "entities": [{"text": "BOTTOMUPPARSER", "start_pos": 63, "end_pos": 77, "type": "METRIC", "confidence": 0.9854654669761658}, {"text": "BI-LSTM", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.8341789841651917}]}, {"text": "We also evaluated the final parsers in a -POS setup, in which we did not feed the parser with any POS-tags.", "labels": [], "entities": []}, {"text": "Results Results for English and Chinese are presented in respectively.", "labels": [], "entities": []}, {"text": "For comparison, we also show the results of the Stack-LSTM transition-based parser model of , which we consider to be a state-of-the-art greedy model which is also very competitive with searchbased models, with and without pre-trained embeddings, and with and without POS-tags.", "labels": [], "entities": []}, {"text": "The trends are consistent across the two languages.", "labels": [], "entities": []}, {"text": "The baseline Bottom-Up parser performs very poorly.", "labels": [], "entities": []}, {"text": "This is expected, as only the headword of each subtree is used for prediction.", "labels": [], "entities": []}, {"text": "When adding the tree-encoding, results jump to near stateof-the-art accuracy, suggesting that the composed vector representation is indeed successful in capturing predictive structural information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9978461265563965}]}, {"text": "Replacing the head-words with their BI-LSTM encodings results in another increase inaccuracy for English, outperforming the Dyer et al (S-LSTM no external) models on the test-set.", "labels": [], "entities": [{"text": "BI-LSTM", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.8694083094596863}]}, {"text": "Adding the external pre-trained embeddings further improves the results for both our parser and Dyer et al's model, closing the gap between them.", "labels": [], "entities": []}, {"text": "When POS-tags are not provided as input, the numbers for both parsers drop.", "labels": [], "entities": []}, {"text": "The drop is small for English and large for Chinese, and our parser seem to suffer a little less than the Dyer et al model.", "labels": [], "entities": []}, {"text": "Importance of the dynamic oracle We also evaluate the importance of using the dynamic oracle and error-exploration training, and find that they are indeed important for achieving high parsing accuracies with our model (: Effect of the error-exploration training (dynamic-oracle) on dev set accuracy in English and Chinese. RAND: random initialization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 290, "end_pos": 298, "type": "METRIC", "confidence": 0.9696298241615295}, {"text": "RAND", "start_pos": 323, "end_pos": 327, "type": "DATASET", "confidence": 0.8818420171737671}]}, {"text": "EXT: pre-trained external embeddings.", "labels": [], "entities": [{"text": "EXT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7241089344024658}]}, {"text": "When training without error-exploration (that is, the parser follows only correct actions during training and not using the dynamic aspect of the oracle), accuracies of unseen sentences drop by between 0.4 and 0.8 accuracy points (average 0.58).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9869043231010437}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9979617595672607}]}, {"text": "This is consistent with previous work on training with error-exploration and dynamic oracles, showing that the technique is not restricted to models trained with sparse linear models.", "labels": [], "entities": []}, {"text": "Comparison to other state-of-the-art parsers Our main point of comparison is the model of Dyer et al, which was chosen because it is (a) a very strong parsing model; and (b) is the closest to ours in the literature: a greedy parsing model making heavy use of LSTMs.", "labels": [], "entities": []}, {"text": "To this end, we tried to make the comparison to Dyer et alas controlled as possible, using the same dependency annotation schemes, as well as the same predicted POS-tags and the pre-trained embeddings (when applicable).", "labels": [], "entities": []}, {"text": "It is also informative to position our results with respect to other state-of-the-art parsing results reported in the literature, as we do in.", "labels": [], "entities": []}, {"text": "Here, some of the comparisons are less direct: some of the results use different dependency annotation schemes 5 , as well as different predicted POS-tags, and different pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "While the numbers are not directly comparable, they do give a good reference as to the expected range of state-of-the-art parsing results.", "labels": [], "entities": []}, {"text": "Our system's English parsing results are in range of state-of-the-art and the Chinese parsing results surpass it.", "labels": [], "entities": [{"text": "English parsing", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.44606080651283264}]}, {"text": "These numbers are achieved while using a greedy, bottom up parsing method without any search, and while relying solely on the compositional tree representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyper-parameter values used in experiments", "labels": [], "entities": [{"text": "Hyper-parameter", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8951823711395264}]}, {"text": " Table 2: English parsing results (SD)", "labels": [], "entities": [{"text": "English parsing", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6585701256990433}]}, {"text": " Table 3: Chinese parsing results (CTB5)", "labels": [], "entities": [{"text": "Chinese parsing", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6570272594690323}]}, {"text": " Table 4: Effect of the error-exploration training  (dynamic-oracle) on dev set accuracy in English and Chi- nese. RAND: random initialization. EXT: pre-trained ex- ternal embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.941783607006073}, {"text": "RAND", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.5602137446403503}]}]}