{"title": [{"text": "Learning to Make Inferences in a Semantic Parsing Task", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce anew approach to training a semantic parser that uses textual entail-ment judgements as supervision.", "labels": [], "entities": []}, {"text": "These judgements are based on high-level inferences about whether the meaning of one sentence follows from another.", "labels": [], "entities": []}, {"text": "When applied to an existing semantic parsing task, they prove to be a useful tool for revealing semantic distinctions and background knowledge not captured in the target representations.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8086890975634257}]}, {"text": "This information is used to improve the quality of the semantic representations being learned and to acquire generic knowledge for reasoning.", "labels": [], "entities": []}, {"text": "Experiments are done on the benchmark Sportscaster corpus (Chen and Mooney, 2008), and a novel RTE-inspired inference dataset is introduced.", "labels": [], "entities": [{"text": "Sportscaster corpus", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.8938554227352142}, {"text": "RTE-inspired inference dataset", "start_pos": 95, "end_pos": 125, "type": "DATASET", "confidence": 0.6242834726969401}]}, {"text": "On this new dataset our method strongly outperforms several strong baselines.", "labels": [], "entities": []}, {"text": "Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task.", "labels": [], "entities": [{"text": "Sportscaster semantic parsing task", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.8329002410173416}]}], "introductionContent": [{"text": "Semantic Parsing is the task of automatically translating natural language text to formal meaning representations (e.g., statements in a formal logic).", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8053717911243439}]}, {"text": "Recent work has centered around learning such translations using parallel data, or raw collections of text-meaning pairs, often by employing methods from statistical machine translation () and parsing.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 154, "end_pos": 185, "type": "TASK", "confidence": 0.6079317331314087}]}, {"text": "Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries.", "labels": [], "entities": [{"text": "database retrieval", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.6935684233903885}]}, {"text": "A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort ().", "labels": [], "entities": []}, {"text": "For example, train a semantic parser in a question-answering domain using the denotation (or answer) of each question as the sole supervision.", "labels": [], "entities": []}, {"text": "Particularly impressive is their system's ability to learn complex linguistic structure not handled by earlier methods that use more direct supervision.", "labels": [], "entities": []}, {"text": "Similarly, Artzi and Zettlemoyer (2013) train a parser that generates higherorder logical representations in a navigation domain using low-level navigation cues.", "labels": [], "entities": []}, {"text": "What is missing in such approaches, however, is an explicit account of entailment (e.g., learning entailment rules from such corpora), which has long been considered one of the basic aims of semantics).", "labels": [], "entities": []}, {"text": "An adequate semantic parser that captures the core aspects of natural language meaning should support inferences about sentence-level entailments (i.e., determining whether the meaning of one sentence follows from another).", "labels": [], "entities": []}, {"text": "In many cases, the target representations being learned remain inexpressive, making it difficult to learn the types of semantic generalizations and world-knowledge needed for modeling entailment (see discussion in).", "labels": [], "entities": []}, {"text": "Attempts to integrate more general knowledge into semantic parsing pipelines have often involved additional hand-engineering or external lexical resources ().", "labels": [], "entities": [{"text": "semantic parsing pipelines", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.7943638563156128}]}, {"text": "We propose a different learning-based approach that uses textual inference judgements between sentences as additional supervision to learn semantic generaliza-  a text x paired with a set of meaning representations z derived from events occurring in a 2-d soccer simulator.", "labels": [], "entities": []}, {"text": "The goal is to learn a latent translation, y, from the text to the correct representation.", "labels": [], "entities": []}, {"text": "tions in a semantic parsing task.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.8057757218678793}]}, {"text": "Our assumption is that differences in sentence realizations provide a strong, albeit indirect, signal about differences in meaning.", "labels": [], "entities": []}, {"text": "When paired with entailment judgements, this evidence can reveal important semantic distinctions (e.g., sense distinctions, modification) that are not captured in target meaning representations.", "labels": [], "entities": []}, {"text": "These judgements can also be used to learn general knowledge about a domain (e.g., meaning postulates or ontological relations).", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel recognizing textual entailment (RTE) inspired inference task for training and evaluating semantic parsers that extends previous approaches.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE) inspired inference", "start_pos": 36, "end_pos": 91, "type": "TASK", "confidence": 0.7356296703219414}]}, {"text": "Our method learns jointly using structured meaning representations (as done in previous approaches) and raw textual inference judgements as the main supervision.", "labels": [], "entities": []}, {"text": "In order to learn and model entailment phenomena, we introduce anew method that integrates natural logic (symbolic) reasoning) directly into a data-driven semantic parsing model.", "labels": [], "entities": []}, {"text": "We perform experiments on the Sportscaster corpus, which we extend by annotating pairs of sentences in the original dataset with inference judgements.", "labels": [], "entities": [{"text": "Sportscaster corpus", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9285088181495667}]}, {"text": "On anew inference task based on this extended dataset, we achieve an accuracy of 73%, which is an improvement of 13 percentage points over a strong baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9997178912162781}]}, {"text": "As a separate result, part of our approach outperforms previously published results (from around 89% accuracy to 96%) on the original Sportscaster semantic parsing task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9821265339851379}, {"text": "Sportscaster semantic parsing task", "start_pos": 134, "end_pos": 168, "type": "TASK", "confidence": 0.8321705460548401}]}, {"text": "Figure 2: Example sentence pairs and semantic representations with textual inference judgements.", "labels": [], "entities": []}, {"text": "Na\u00a8\u0131veNa\u00a8\u0131ve entailments area type of close-world assumption that result from matching semantic representations and assigning an entailment for matches and a contradiction otherwise.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss the Sportscaster dataset and our experimental setup.", "labels": [], "entities": [{"text": "Sportscaster dataset", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.9758512079715729}]}, {"text": "Sportscaster While the domain has a relatively small set of concepts and limited scope, reasoning in this domain still requires a large set of semantic relations and background knowledge.", "labels": [], "entities": [{"text": "Sportscaster", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9319621324539185}]}, {"text": "From this small set of concepts, the inference grammar described in Section 3.2 encodes around 3,000 inference rules.", "labels": [], "entities": []}, {"text": "Since soccer is a topic that most people are familiar with, it is also easy to get non-experts to provide judgements about entailment.", "labels": [], "entities": []}, {"text": "Extended Inference Corpus The extended corpus consists of 461 unaligned pairs of texts from the original Sportscaster corpus annotated with sentence-level entailment judgements.", "labels": [], "entities": [{"text": "Sportscaster corpus", "start_pos": 105, "end_pos": 124, "type": "DATASET", "confidence": 0.921799898147583}]}, {"text": "We annotated 356 pairs using local human judges an average of 2.5 times . Following, we discarded pairs without a majority agreement, which resulted in 306 pairs (or 85% of the initial set).", "labels": [], "entities": []}, {"text": "We also annotated an additional 155 pairs using Amazon Mechanical Turk, which were mitigated by a local annotator.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.9526658058166504}]}, {"text": "In addition to this core set of 461 entailment pairs, we separately experimented with adding unlabeled data (i.e., pairs without inference judgements) and ambiguously labelled data (i.e., pairs with multiple inference judgements) to train our inference grammars (shown in the results as More Data) and test the flexibility of our model.", "labels": [], "entities": []}, {"text": "This included 250 unlabeled pairs taken from the original dataset, as well as 592 (ambiguous) pairs created by deriving new conclusions from the annotated set.", "labels": [], "entities": []}, {"text": "This last group was constructed by exploiting the transitive nature of various inference relations and mapping pairs with matching labels in training to {Entail,Unknown}.", "labels": [], "entities": []}, {"text": "We perform two types of experiments: A semantic parsing experiment (Task 1) to test our approach on the original task of generating Sportscaster representations.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7598188519477844}]}, {"text": "In addition, we introduce an inference experiment (Task 2) to test our approach on the problem of detecting entailments/contradictions between sentences.", "labels": [], "entities": [{"text": "detecting entailments/contradictions between sentences", "start_pos": 98, "end_pos": 152, "type": "TASK", "confidence": 0.8334384063879648}]}, {"text": "For the semantic parsing experiment, we follow the original setup of.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.8189760744571686}]}, {"text": "4-fold cross validation is employed by training on all variations of 3 games and evaluating on a left out game.", "labels": [], "entities": [{"text": "4-fold cross validation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5025893350442251}]}, {"text": "Each representation produced in the evaluation phrase is considered correct if it matches exactly a gold representation.", "labels": [], "entities": []}, {"text": "The second experiment imitates an RTE-style evaluation and tests the quality of the background knowledge being learned using our infer- ence grammars.", "labels": [], "entities": []}, {"text": "Like in the semantic parsing task, we perform cross-validation on the games using both the original data and sentence pairs to jointly train our models, and evaluate on left-out sets of inference pairs.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7862739165623983}]}, {"text": "Each proof generated in the evaluation phrase is considered correct if the resulting inference label matches a gold inference.", "labels": [], "entities": []}, {"text": "We implemented the learning algorithm in Section 3.3 using the k-best algorithm by, with abeam size of 1,000.", "labels": [], "entities": []}, {"text": "The base semantic grammars were each trained for 3 iterations and re-trained using the additional inference grammar rules for 10 iterations.", "labels": [], "entities": []}, {"text": "Two Dirichlet priors were used, \u21b5 1 = 0.05 (for lexical rules) and \u21b5 2 = 0.3 (for non-lexical rules) throughout.", "labels": [], "entities": []}, {"text": "Lexical rule probabilities were initialized using co-occurrence statistics estimated using an IBM Model1 word aligner (uniform initialization otherwise).", "labels": [], "entities": []}, {"text": "5 additional senses were added to the inference grammar for the most frequent events.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the semantic parsing (top) and  inference (bottom) cross validation experiments.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7781056761741638}]}]}