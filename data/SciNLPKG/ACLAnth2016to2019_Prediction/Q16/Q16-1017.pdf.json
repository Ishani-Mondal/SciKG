{"title": [{"text": "Discrete-State Variational Autoencoders for Joint Discovery and Factorization of Relations", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a method for unsupervised open-domain relation discovery.", "labels": [], "entities": [{"text": "open-domain relation discovery", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6104167103767395}]}, {"text": "In contrast to previous (mostly generative and agglomera-tive clustering) approaches, our model relies on rich contextual features and makes minimal independence assumptions.", "labels": [], "entities": [{"text": "generative and agglomera-tive clustering)", "start_pos": 32, "end_pos": 73, "type": "TASK", "confidence": 0.7677322506904602}]}, {"text": "The model is composed of two parts: a feature-rich relation extractor, which predicts a semantic relation between two entities, and a factor-ization model, which reconstructs arguments (i.e., the entities) relying on the predicted relation.", "labels": [], "entities": []}, {"text": "The two components are estimated jointly so as to minimize errors in recovering arguments.", "labels": [], "entities": []}, {"text": "We study factorization models inspired by previous work in relation factoriza-tion and selectional preference modeling.", "labels": [], "entities": [{"text": "selectional preference modeling", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.6825758417447408}]}, {"text": "Our models substantially outperform the genera-tive and agglomerative-clustering counterparts and achieve state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of Relation Extraction (RE) consists of detecting and classifying the semantic relations present in text.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8407248854637146}]}, {"text": "RE has been shown to benefit a wide range of NLP tasks, such as information retrieval (), question answering () and textual entailment ().", "labels": [], "entities": [{"text": "RE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8430799245834351}, {"text": "information retrieval", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7874599993228912}, {"text": "question answering", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8922893702983856}, {"text": "textual entailment", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7221081852912903}]}, {"text": "Supervised methods for RE have been successful when small restricted sets of relations are considered.", "labels": [], "entities": [{"text": "RE", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9913086891174316}]}, {"text": "However, human annotation is expensive and time-consuming, and consequently these approaches do not scale well to the open-domain setting where a large number of relations need to be detected in a heterogeneous text collection (e.g., the entire Web).", "labels": [], "entities": []}, {"text": "Though weakly-supervised approaches, such as distantly supervised methods and bootstrapping (), reduce the amount of necessary supervision, they still require examples for every relation considered.", "labels": [], "entities": []}, {"text": "These limitations led to the emergence of unsupervised approaches for RE.", "labels": [], "entities": [{"text": "RE", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9928355813026428}]}, {"text": "These methods extract surface or syntactic patterns between two entities and either directly use these patterns as substitutes for semantic relations ( or cluster the patterns (sometimes in context-sensitive way) to form relations ().", "labels": [], "entities": []}, {"text": "The existing methods, given their generative (or agglomerative clustering) nature, rely on simpler features than their supervised counterparts and also make strong modeling assumptions (e.g., assuming that arguments are conditionally independent of each other given the relation).", "labels": [], "entities": [{"text": "generative (or agglomerative clustering)", "start_pos": 34, "end_pos": 74, "type": "TASK", "confidence": 0.7567978501319885}]}, {"text": "These shortcomings are likely to harm their performance.", "labels": [], "entities": []}, {"text": "In this work, we tackle the aforementioned challenges and introduce anew model for unsupervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7543209195137024}]}, {"text": "We also describe an efficient estimation algorithm which lets us experiment with large unannotated collections.", "labels": [], "entities": []}, {"text": "Our model is composed of two components: \u2022 an encoding component: a feature-rich relation extractor which predicts a semantic relation between two entities in a specific sentence given contextual features; \u2022 a reconstruction component: a factorization model which reconstructs arguments (i.e., the entities) relying on the predicted relation.", "labels": [], "entities": []}, {"text": "The two components are estimated jointly so as to minimize errors in reconstructing arguments.", "labels": [], "entities": []}, {"text": "While learning to predict left-out arguments, the inference algorithm will search for latent relations that simplify the argument prediction task as much as possible.", "labels": [], "entities": [{"text": "argument prediction", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7387118637561798}]}, {"text": "Roughly, such an objective will favour inducing relations that maximally constrain the set of admissible argument pairs.", "labels": [], "entities": []}, {"text": "Our hypothesis is that relations induced in this way will be interpretable by humans and useful in practical applications.", "labels": [], "entities": []}, {"text": "Why is this hypothesis plausible?", "labels": [], "entities": []}, {"text": "Primarily because humans typically define relations as an abstraction capturing the essence of the underlying situation.", "labels": [], "entities": []}, {"text": "And the underlying situation (rather than surface linguistic details like syntactic functions) is precisely what imposes constraints on admissible argument pairs.", "labels": [], "entities": []}, {"text": "This framework allows us to both exploit rich features (in the encoding component) and capture interdependencies between arguments in a flexible way (both in the reconstruction and encoding components).", "labels": [], "entities": []}, {"text": "The use of a reconstruction-error objective, previously considered primarily in the context of training neural autoencoders, gives us an opportunity to borrow ideas from the well-established area of statistical relational learning, and, more specifically, relation factorization.", "labels": [], "entities": [{"text": "relation factorization", "start_pos": 256, "end_pos": 278, "type": "TASK", "confidence": 0.8727061748504639}]}, {"text": "In this area, tensor and matrix factorization methods have been shown to be effective for inferring missing facts in knowledge bases.", "labels": [], "entities": []}, {"text": "In our work, we also adopt a fairly standard RESCAL factorization) and use it within our reconstruction component.", "labels": [], "entities": [{"text": "RESCAL", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.944969117641449}]}, {"text": "Though there is a clear analogy between statistical relational learning and our setting, there is also a very significant difference.", "labels": [], "entities": []}, {"text": "In contrast to relational learning, rather than factorizing existing relations (an existing 'database'), our method simultaneously discovers the relational schema (i.e., an inventory of relations) and a mapping from text to the relations (i.e., a relation extractor), and it does it in such away as to maximize performance on reconstruction (i.e., inference) tasks.", "labels": [], "entities": []}, {"text": "This analogy also highlights one important property of our framework: unlike generative models, we explicitly force our semantic representations to be useful for at least the most basic form of semantic inference (i.e., inferring an argument based on the relation and another argument).", "labels": [], "entities": []}, {"text": "It is important to note that the model is completely agnostic about the real semantic relation between two arguments, as the relational schema is discovered during learning.", "labels": [], "entities": []}, {"text": "We consider both a factorization method inspired by previous research in knowledge base modeling (as discussed above) and another, even simpler one, based on ideas from previous research on modeling selectional preferences (e.g.,; \u00b4 O S\u00e9aghdha (2010); Van de Cruys (2010)), plus their combination.", "labels": [], "entities": [{"text": "\u00b4 O S\u00e9aghdha (2010); Van de Cruys (2010))", "start_pos": 231, "end_pos": 272, "type": "DATASET", "confidence": 0.8454870994274433}]}, {"text": "Our models are applied to aversion of the New York Times corpus.", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.8799999803304672}]}, {"text": "In order to evaluate our approach, we follow and align named entities in our collection to, a large collaborative knowledge base.", "labels": [], "entities": []}, {"text": "In this way we can evaluate a subset of our induced relations against relations in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.976567804813385}]}, {"text": "Note that Freebase has not been used during learning, making this a fair evaluation scenario for an unsupervised relation induction method.", "labels": [], "entities": [{"text": "relation induction", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7138173282146454}]}, {"text": "We also qualitatively evaluate our model by both considering several examples of induced relations (both appearing and not appearing in Freebase) and visualizing embeddings of named entities induced by our model.", "labels": [], "entities": []}, {"text": "As expected, the choice of a factorization model affects the model performance.", "labels": [], "entities": []}, {"text": "Our best models substantially outperform the state-of-the-art generative Rel-LDA model of: 35.8% F 1 and 29.6% F 1 for our best model and Rel-LDA, respectively.", "labels": [], "entities": [{"text": "F 1", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9897268712520599}, {"text": "F 1", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9888823330402374}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the following section, we formally describe the problem.", "labels": [], "entities": []}, {"text": "In Section 3, we motivate our approach.", "labels": [], "entities": []}, {"text": "In Section 4, we formally describe the method.", "labels": [], "entities": []}, {"text": "In Section 5 we describe our experimental setting and discuss the results.", "labels": [], "entities": []}, {"text": "We give more background on RE, knowledge base completion and autoencoders in Section 6.", "labels": [], "entities": [{"text": "RE", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.7727212905883789}, {"text": "knowledge base completion", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6394936144351959}, {"text": "Section 6", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9051535427570343}]}], "datasetContent": [{"text": "In this section we evaluate how effective our model is in discovering relations between pairs of entities in a sentence.", "labels": [], "entities": []}, {"text": "We consider the unsupervised setting, so we use clustering measures for evaluation.", "labels": [], "entities": []}, {"text": "Since we want to directly compare to Rel-LDA (Yao et al., 2011), we use the transductive set-up: we train our model on the entire training set (with labels removed) and we evaluate the estimated model on a subset of the training set.", "labels": [], "entities": []}, {"text": "Given that we train the relation classifier (i.e., the encoding model), unlike some of the previous approaches, there is nothing in our approach which prevents us from applying it in an inductive scenario (i.e., to unseen data).", "labels": [], "entities": []}, {"text": "Towards the end of this section we also provide qualitative evaluation of the induced relations and entity embeddings.", "labels": [], "entities": []}, {"text": "We tested our model on the New York Times corpus (Sandhaus, 2008) using articles from 2000 to 2007.", "labels": [], "entities": [{"text": "New York Times corpus (Sandhaus, 2008)", "start_pos": 27, "end_pos": 65, "type": "DATASET", "confidence": 0.8995000455114577}]}, {"text": "We use the same filtering and preprocessing steps (POS tagging, NER, and syntactic parsing) as the ones described in.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.6312447041273117}, {"text": "syntactic parsing", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.706668347120285}]}, {"text": "In that way we obtained about 2 million entity pairs (i.e., potential relation realizations).", "labels": [], "entities": []}, {"text": "In order to evaluate our models, we aligned each entity pair with Freebase, and, as in, we discarded unaligned ones from the eval-uation.", "labels": [], "entities": []}, {"text": "We consider Freebase relations as goldstandard clusterings and evaluated induced relations against them.", "labels": [], "entities": []}, {"text": "Note that we use the micro-reading scenario, that is, we predict a relation on the basis of a single occurrence of an entity pair rather than aggregating information across all the occurrences of the pair in the corpus.", "labels": [], "entities": []}, {"text": "Though it is likely to harm our performance when evaluating against Freebase, this is a deliberate choice as we believe extracting relations about less frequent entities (where there is little redundancy in a collection) and modelling content of specific documents is a more challenging and important research direction.", "labels": [], "entities": [{"text": "extracting relations about less frequent entities", "start_pos": 120, "end_pos": 169, "type": "TASK", "confidence": 0.8265166481335958}]}, {"text": "Moreover, feature-rich models are likely to be especially beneficial in these scenarios, as for micro-reading the information extraction systems cannot fallback to easier non-ambiguous contexts.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.7379013001918793}]}, {"text": "We use the B 3 metric () as the scoring function.", "labels": [], "entities": [{"text": "B 3 metric", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9492002725601196}]}, {"text": "B 3 is a standard measure for evaluating precision and recall of clustering tasks (.", "labels": [], "entities": [{"text": "B 3", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8899578154087067}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9980779886245728}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9948235750198364}]}, {"text": "As the final evaluation score we use F 1 , the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F 1", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9917630255222321}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9994476437568665}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9983349442481995}]}], "tableCaptions": [{"text": " Table 1: Average F 1 results (%), and the standard deviation, across 3 runs of different models on the test set.", "labels": [], "entities": [{"text": "Average F 1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8332506616910299}, {"text": "standard deviation", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.9562645554542542}]}]}