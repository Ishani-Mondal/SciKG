{"title": [], "abstractContent": [{"text": "We train one multilingual model for dependency parsing and use it to parse sentences in several languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8447022438049316}]}, {"text": "The parsing model uses (i) multilingual word clusters and em-beddings; (ii) token-level language information ; and (iii) language-specific features (fine-grained POS tags).", "labels": [], "entities": []}, {"text": "This input representation enables the parser not only to parse effectively in multiple languages, but also to generalize across languages based on linguistic uni-versals and typological similarities, making it more effective to learn from limited annotations.", "labels": [], "entities": []}, {"text": "Our parser's performance compares favorably to strong baselines in a range of data scenarios, including when the target language has a large treebank, a small treebank, or no treebank for training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Developing tools for processing many languages has long been an important goal in NLP), 1 but it was only when statistical methods became standard that massively multilingual NLP became economical.", "labels": [], "entities": []}, {"text": "The mainstream approach for multilingual NLP is to design language-specific models.", "labels": [], "entities": []}, {"text": "For each language of interest, the resources necessary for training the model are obtained (or created), and separate parameters are fit for each language separately.", "labels": [], "entities": []}, {"text": "This approach is simple and grants the flexibility of customizing the model and features to the needs of each language, but it is suboptimal for theoretical and practical reasons.", "labels": [], "entities": []}, {"text": "Theoretically, the study of linguistic typology tells us that many languages share morphological, phonological, and syntactic phenomena; therefore, the mainstream approach misses an opportunity to exploit relevant supervision from typologically related languages.", "labels": [], "entities": []}, {"text": "Practically, it is inconvenient to deploy or distribute NLP tools that are customized for many different languages because, for each language of interest, we need to configure, train, tune, monitor, and occasionally update the model.", "labels": [], "entities": []}, {"text": "Furthermore, code-switching or code-mixing (mixing more than one language in the same discourse), which is pervasive in some genres, in particular social media, presents a challenge for monolingually-trained NLP models ().", "labels": [], "entities": []}, {"text": "In parsing, the availability of homogeneous syntactic dependency annotations in many languages has created an opportunity to develop a parser that is capable of parsing sentences in multiple languages, addressing these theoretical and practical concerns.", "labels": [], "entities": [{"text": "parsing", "start_pos": 3, "end_pos": 10, "type": "TASK", "confidence": 0.9794951677322388}]}, {"text": "A multilingual parser can potentially replace an array of language-specific monolingually-trained parsers While our parser can be used to parse input with codeswitching, we have not evaluated this capability due to the lack of appropriate data.", "labels": [], "entities": []}, {"text": "3 Although multilingual dependency treebanks have been available fora decade via the 2006 and 2007 CoNLL shared tasks (, the treebank of each language was annotated independently and with its own annotation conventions.", "labels": [], "entities": []}, {"text": "(for languages with a large treebank).", "labels": [], "entities": []}, {"text": "The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality, but these models may sacrifice accuracy on source languages with a large treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 186, "end_pos": 193, "type": "TASK", "confidence": 0.9585394263267517}, {"text": "accuracy", "start_pos": 234, "end_pos": 242, "type": "METRIC", "confidence": 0.9956924319267273}]}, {"text": "In this paper, we describe a model that works well for both low-resource and high-resource scenarios.", "labels": [], "entities": []}, {"text": "We propose a parsing architecture that takes as input sentences in several languages, optionally predicting the part-of-speech (POS) tags and input language.", "labels": [], "entities": []}, {"text": "The parser is trained on the union of available universal dependency annotations in different languages.", "labels": [], "entities": []}, {"text": "Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (), cross-lingual word clusters), selective sharing (, universal dependency annotations (), advances in neural network architectures, and multilingual word embeddings ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7907772958278656}]}, {"text": "We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank, a small treebank, or no treebank.", "labels": [], "entities": []}, {"text": "Our parser is publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the MALOPA approach in three data scenarios: when the target language has a large treebank, a small treebank ( or no treebank.", "labels": [], "entities": []}, {"text": "For experiments where the target language has a large treebank, we use the standard data splits for German (de), English (en), Spanish (es), French (fr), Italian (it), Portuguese (pt) and Swedish (sv) in the latest release (version 1.2) of Universal Dependencies (), and experiment with both gold and predicted POS tags.", "labels": [], "entities": []}, {"text": "For experiments where the target language has no treebank, we use the standard splits for these languages in the older universal dependency treebanks v2.0 ( ) and use gold POS tags, following the baselines (.", "labels": [], "entities": []}, {"text": "gives the number of sentences and words annotated for each language in both versions.", "labels": [], "entities": []}, {"text": "Ina preprocessing step, we lowercase all tokens and remove multi-word annotations and language-specific dependency relations.", "labels": [], "entities": []}, {"text": "We use the same multilingual Brown clusters and multilingual embeddings of, kindly provided by the authors.", "labels": [], "entities": []}, {"text": "We follow in parameter initialization and optimization.", "labels": [], "entities": [{"text": "parameter initialization", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6630615741014481}]}, {"text": "However, when training the parser on multiple languages We use stochastic gradient updates with an initial learning rate of \u03b70 = 0.1 in epoch #0, update the learning rate in following epochs as \u03b7t = \u03b70/(1 + 0.1t).", "labels": [], "entities": []}, {"text": "We clip the 2 norm of the gradient to avoid \"exploding\" gradients.", "labels": [], "entities": []}, {"text": "Unlabeled attachment score (UAS) on the development set determines early stopping.", "labels": [], "entities": [{"text": "Unlabeled attachment score (UAS)", "start_pos": 0, "end_pos": 32, "type": "METRIC", "confidence": 0.8364091416200002}]}, {"text": "Parameters are initialized with uniform samples in \u00b1 6/(r + c) where rand care the sizes of the previous and following layer in the nueral network.", "labels": [], "entities": []}, {"text": "The standard deviations of the labeled attachment score (LAS) due to random initialization in individual target languages are 0.", "labels": [], "entities": [{"text": "labeled attachment score (LAS)", "start_pos": 31, "end_pos": 61, "type": "METRIC", "confidence": 0.8546144664287567}]}, {"text": "in MALOPA, instead of updating the parameters with the gradient of individual sentences, we use mini-batch updates which include one sentence sampled uniformly (without replacement) from each language's treebank, until all sentences in the smallest treebank are used (which concludes an epoch).", "labels": [], "entities": []}, {"text": "We repeat the same process in following epochs.", "labels": [], "entities": []}, {"text": "We found this to help prevent one source language with a larger treebank (e.g., German) from dominating parameter updates at the expense of other source languages with a smaller treebank (e.g., Swedish).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of sentences (tokens) in each treebank split in Universal Dependency Treebanks (UDT)  version 2.0 and Universal Dependencies (UD) version 1.2 for the languages we experiment with. The last  row gives the number of unique language-specific fine-grained POS tags used in a treebank.", "labels": [], "entities": [{"text": "Universal Dependency Treebanks (UDT)", "start_pos": 65, "end_pos": 101, "type": "DATASET", "confidence": 0.760610376795133}]}, {"text": " Table 3: Dependency parsing: labeled attachment scores (LAS) for monolingually-trained parsers and  MALOPA in the fully supervised scenario where L t = L s . Note that we use the universal dependencies  verson 1.2 which only includes annotations for \u223c13,000 English sentences, which explains the relatively  low scores in English. When we instead use the universal dependency treebanks version 2.0 which includes  annotations for \u223c40,000 English sentences (originally from the English Penn Treebank), we achieve UAS  score 93.0 and LAS score 91.5.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.778195321559906}, {"text": "labeled attachment scores (LAS)", "start_pos": 30, "end_pos": 61, "type": "METRIC", "confidence": 0.7450110117594401}, {"text": "MALOPA", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9027162194252014}, {"text": "Penn Treebank", "start_pos": 486, "end_pos": 499, "type": "DATASET", "confidence": 0.9516129493713379}, {"text": "UAS  score 93.0", "start_pos": 513, "end_pos": 528, "type": "METRIC", "confidence": 0.961941639582316}, {"text": "LAS score 91.5", "start_pos": 533, "end_pos": 547, "type": "METRIC", "confidence": 0.9791277845700582}]}, {"text": " Table 4: Recall of some classes of dependency attachments/relations in German.", "labels": [], "entities": []}, {"text": " Table 5: Effect of automatically predicting language ID and POS tags with MALOPA on LAS scores.", "labels": [], "entities": [{"text": "predicting language ID and POS tags", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.7989031672477722}, {"text": "MALOPA", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.8852840065956116}]}, {"text": " Table 7: Small (3,000 token) target treebank setting:  language-universal dependency parser performance.", "labels": [], "entities": [{"text": "language-universal dependency parser", "start_pos": 56, "end_pos": 92, "type": "TASK", "confidence": 0.5722462634245554}]}]}