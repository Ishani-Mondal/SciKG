{"title": [], "abstractContent": [{"text": "We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features.", "labels": [], "entities": [{"text": "Sparse Non-negative Matrix (SNM) estimation", "start_pos": 11, "end_pos": 54, "type": "TASK", "confidence": 0.5373476488249642}]}, {"text": "We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "SNM language", "start_pos": 12, "end_pos": 24, "type": "TASK", "confidence": 0.8918797969818115}, {"text": "LDC English Gigaword corpus", "start_pos": 99, "end_pos": 126, "type": "DATASET", "confidence": 0.8340102881193161}]}, {"text": "Results show that SNM language models trained with n-gram features area close match for the well-established Kneser-Ney models.", "labels": [], "entities": [{"text": "SNM language", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.8654532134532928}]}, {"text": "The addition of skip-gram features yields a model that is in the same league as the state-of-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark.", "labels": [], "entities": [{"text": "One Billion Word Benchmark", "start_pos": 243, "end_pos": 269, "type": "DATASET", "confidence": 0.5760560259222984}]}, {"text": "On the Gigaword corpus further improvements are observed using features that cross sentence boundaries.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9418478608131409}]}, {"text": "The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data.", "labels": [], "entities": [{"text": "SNM estimation", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9593874216079712}]}], "introductionContent": [{"text": "A statistical language model estimates probability values P (W ) for strings of words W in a vocabulary V whose size can be in the tensor hundreds of thousands and sometimes even millions.", "labels": [], "entities": []}, {"text": "Typically the string Wis broken into sentences, or other segments such as utterances in automatic speech recognition, which are often assumed to be conditionally independent; we will assume that Wis such a segment, or sentence.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.7292471528053284}]}, {"text": "Estimating full sentence language models) is computationally hard if one seeks a properly normalized probability model 1 over strings of words of finite length in V * . A simple and sufficient way to ensure proper normalization of the model is to decompose the sentence probability according to the chain rule and make sure that the end-of-sentence symbol </S> is predicted with non-zero probability in any context.", "labels": [], "entities": []}, {"text": "With W = w N 1 = w 1 , . .", "labels": [], "entities": []}, {"text": ", w N we get: Since the parameter space of P (w k |w k\u22121", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experimental setup used the One Billion Word Benchmark 3 made available by.", "labels": [], "entities": []}, {"text": "It consists of an English training and test set of about 0.8 billion and 159658 tokens, respectively.", "labels": [], "entities": []}, {"text": "The vocabulary contains 793471 words and was constructed by discarding all words with count below 3.", "labels": [], "entities": []}, {"text": "OOV words are mapped to an <UNK> token which is also part of the vocabulary.", "labels": [], "entities": []}, {"text": "The OOV rate of the test set is 0.28%.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9783867299556732}]}, {"text": "All of the described SNM models are initialized with meta-feature weights \u03b8 k = 0 which are updated using AdaGrad with accumulator \u2206 0 = 1 and scaling factor \u03b3 = 0.02 over a single epoch of 30M training examples.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.91973876953125}]}, {"text": "The hash table for the metafeatures was limited to 200M entries as increasing it yielded no significant improvements.", "labels": [], "entities": []}, {"text": "In the first set of experiments, we used all variablelength n-gram features that appeared at least once in the training data up to a given length.", "labels": [], "entities": []}, {"text": "This yields at most n active features: one for each m-gram of length 0 \u2264 m < n where m = 0 corresponds to an empty feature which is always present and produces the unigram distribution.", "labels": [], "entities": []}, {"text": "The number of features is smaller than n when the context is shorter than n \u2212 1 words (near sentence boundaries) and during evaluation where an n-gram that did not occur in the training data is discarded.", "labels": [], "entities": []}, {"text": "When trained using these features, SNMLMs come very close to n-gram models with interpolated Kneser-Ney (KN) smoothing, where no count cut-off was applied and the discount does not change with the order of the model.", "labels": [], "entities": []}, {"text": "shows that Katz smoothing performs considerably worse than both SNM and KN.", "labels": [], "entities": [{"text": "Katz smoothing", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.6378006041049957}]}, {"text": "KN and SNM are not very complementary as linear interpolation with weights optimized on the test data only yields an additional perplexity reduction of about 1%.", "labels": [], "entities": []}, {"text": "The difference between KN and SNM becomes smaller when we increase the size of the context, going from 5% for 5-grams to 3% for 8-grams, which indicates that SNMLMs might be better suited to a large number of features.", "labels": [], "entities": []}, {"text": "Model PPL SNM5-skip (no n-grams) 69.8 + n-grams = SNM5-skip 54.2 + KN5 56.5 SNM5-skip + KN5 53.6: Perplexity (PPL) results comparing two ways of adding n-grams to a 'pure' skip-gram SNM model (no n-grams): joint modeling (SNM5-skip) and linear interpolation with KN5.", "labels": [], "entities": []}, {"text": "The best SNMLM results so far (SNM10-skip) were achieved using 10-grams, together with skip-grams defined by the following feature extractors: This mixture of rich (large context) short-distance and shallow long-distance features enables the model to achieve state-of-the-art results.", "labels": [], "entities": []}, {"text": "compares its perplexity to KN5 as well as to the following language models: \u2022 Stupid Backoff LM (SBO) ( \u2022 Hierarchical Softmax Maximum Entropy LM (HSME)) \u2022 Recurrent Neural Network LM with Maximum Entropy (RNNME) Describing these models however is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "Instead we refer the reader to fora detailed description.", "labels": [], "entities": []}, {"text": "The table also lists the number of model parameters, which in the case of SNMLMs consist of the non-zero entries and precomputed row sums of M.", "labels": [], "entities": []}, {"text": "When we compare the perplexity of SNM10-skip with the state-of-the-art RNNLM with 1024 hidden neurons (RNNME-1024), the difference is only 3%.", "labels": [], "entities": []}, {"text": "Moreover, this small advantage comes at the cost of increased training and evaluation complexity.", "labels": [], "entities": []}, {"text": "Interestingly, when we interpolate the two models, we have an additional gain of 20%, which shows that SNM10-skip and RNNME-1024 are also complementary.", "labels": [], "entities": []}, {"text": "As far as we know, the resulting perplexity of 41.3 is already the best ever reported on this corpus, beating the optimized combination of several models, reported in by 6%.", "labels": [], "entities": []}, {"text": "Finally, interpolation overall models shows that the contribution of other models as well as the additional perplexity reduction of 0.3 is negligible.", "labels": [], "entities": [{"text": "perplexity reduction", "start_pos": 108, "end_pos": 128, "type": "METRIC", "confidence": 0.9370558857917786}]}, {"text": "In this Section we present actual runtimes to give some idea of how the theoretical complexity analysis of Section 4 translates to a practical application.", "labels": [], "entities": []}, {"text": "More specifically, we compare the training runtime (in machine hours) of the best SNM model to the best RNN and n-gram models: \u2022 KN5: 28 machine hours \u2022 SNM5: 115 machine hours \u2022 SNM10-skip: 487 machine hours \u2022 RNNME-1024: 5760 machine hours As these models were trained using different architectures (number of CPUs, type of distributed computing, etc.), a runtime comparison is inherently hard and we would therefore like to stress that these numbers should betaken with a grain of salt.", "labels": [], "entities": []}, {"text": "However, based on the order of magnitude we can clearly conclude that SNM's reduced training complexity shown in Section 4 translates to a substantial reduction in training time compared to RNNs.", "labels": [], "entities": [{"text": "SNM", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.921288788318634}]}, {"text": "Moreover, the large difference between KN5 and SNM5 suggests that our vanilla implementation can be further improved to achieve even larger speed-ups.", "labels": [], "entities": []}, {"text": "In addition to the experiments on the One Billion Word Benchmark, we also conducted experiments on a small subset of the LDC English Gigaword corpus.", "labels": [], "entities": [{"text": "One Billion Word Benchmark", "start_pos": 38, "end_pos": 64, "type": "DATASET", "confidence": 0.5616380944848061}, {"text": "LDC English Gigaword corpus", "start_pos": 121, "end_pos": 148, "type": "DATASET", "confidence": 0.925736591219902}]}, {"text": "This has the advantage that the experiments are more easily reproducible and, since this corpus preserves the original sentence order, it also allows us to investigate SNM's capabilities of modeling phenomena that cross sentence boundaries.", "labels": [], "entities": []}, {"text": "The corpus is the one used in, which we acquired with the help of the authors and is now available at http://www.esat.", "labels": [], "entities": []}, {"text": "kuleuven.be/psi/spraak/downloads/ 4 . It consists of a training set of 44M tokens, a check set of 1.7M tokens and a test set of 13.7M tokens.", "labels": [], "entities": []}, {"text": "The vocabulary contains 56k words which corresponds to an OOV rate of 0.89% and 1.98% for the check and test set, respectively.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9910120666027069}]}, {"text": "OOV words are mapped to an <UNK> token.", "labels": [], "entities": []}, {"text": "The large difference in OOV rate between the check and test set is explained by the fact that the training data and check data are from the same source (Agence France-Presse), whereas the test data is drawn from CNA (Central News Agency of Taiwan) which seems to be out of domain relative to the training data.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9849426448345184}, {"text": "Agence France-Presse)", "start_pos": 153, "end_pos": 174, "type": "DATASET", "confidence": 0.9252583980560303}, {"text": "CNA (Central News Agency of Taiwan)", "start_pos": 212, "end_pos": 247, "type": "DATASET", "confidence": 0.7461999654769897}]}, {"text": "This discrepancy also shows in the perplexity results, presented in.", "labels": [], "entities": []}, {"text": "All of the described SNM models are initialized with meta-feature weights \u03b8 k = 0 which are updated using AdaGrad with accumulator \u2206 0 = 1 and scaling factor \u03b3 = 0.02 over a single epoch of 10M training examples.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9178018569946289}]}, {"text": "The hash table for the metafeatures was limited to 10M entries as increasing it yielded no significant improvements.", "labels": [], "entities": []}, {"text": "With regards to n-gram modeling, the results are analogous to the 1B word experiment: SNM5 is close to KN5; both outperform Katz5 by a large mar-gin.", "labels": [], "entities": []}, {"text": "This is the case for the check set and the test set.", "labels": [], "entities": []}, {"text": "showed that by crossing sentence boundaries, perplexities can be drastically reduced.", "labels": [], "entities": []}, {"text": "Although they did not publish any results on the check set, their mixture of n-gram, syntactic language models and topic models achieved a perplexity of 176 on the test set, a 23% relative reduction compared to KN5.", "labels": [], "entities": []}, {"text": "A similar observation was made for the SNM models by adding a feature extractor (r, s, a) analogous to regular skip-grams, but with snow denoting the number of skipped sentence boundaries </S> instead of words.", "labels": [], "entities": []}, {"text": "Adding skip-</S> features with r + a = 4, 1 \u2264 r \u2264 2 and 1 \u2264 s \u2264 10, yielded an even larger reduction of 26% than the one reported by.", "labels": [], "entities": []}, {"text": "On the check set we observed a 25% reduction.", "labels": [], "entities": []}, {"text": "The RNNME results are achieved with a setup similar to the one in.", "labels": [], "entities": []}, {"text": "The main differences are related to the ME features (3-grams only instead of 10-grams and bag-of-words features) and the number of iterations over the training data (20 epochs instead of 10).", "labels": [], "entities": [{"text": "ME", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9757851362228394}]}, {"text": "These choices are related to the size of the training data.", "labels": [], "entities": []}, {"text": "It can be seen from that the best RNNME model outperforms the best SNM model by 13% on the check set.", "labels": [], "entities": []}, {"text": "The outof-domain test set shows that due to its compactness, RNNME is better suited for LM adaptation.", "labels": [], "entities": [{"text": "outof-domain test set", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7124171455701193}, {"text": "LM adaptation", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.96666619181633}]}], "tableCaptions": [{"text": " Table 1: Perplexity results on the 1B Word Benchmark  for Kneser-Ney (KN), Katz and SNM n-gram models of  different order.", "labels": [], "entities": []}, {"text": " Table 3: Number of parameters and perplexity (PPL) results on the 1B Word Benchmark for the proposed models,  compared to the models in Chelba et al. (2014).", "labels": [], "entities": [{"text": "Number of parameters and perplexity (PPL)", "start_pos": 10, "end_pos": 51, "type": "METRIC", "confidence": 0.7991735488176346}]}, {"text": " Table 4: Perplexity (PPL) results on the 44M corpus. On  the small check set, SNM outperforms a mixture of n- gram, syntactic language models (SLM) and topic models  (PLSA), but RNNME performs best. The out-of-domain  test set shows that due to its compactness, RNNME is  better suited for LM adaptation.", "labels": [], "entities": [{"text": "44M corpus", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.9250875115394592}, {"text": "LM adaptation", "start_pos": 291, "end_pos": 304, "type": "TASK", "confidence": 0.952266663312912}]}]}