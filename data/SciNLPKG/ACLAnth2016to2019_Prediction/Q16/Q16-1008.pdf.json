{"title": [{"text": "Adapting to All Domains at Once: Rewarding Domain Invariance in SMT", "labels": [], "entities": [{"text": "Adapting", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9691250920295715}, {"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9543035626411438}]}], "abstractContent": [{"text": "Existing work on domain adaptation for statistical machine translation has consistently assumed access to a small sample from the test distribution (target domain) at training time.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7271658778190613}, {"text": "statistical machine translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6778874297936758}]}, {"text": "In practice, however, the target domain may not be known at training time or it may change to match user needs.", "labels": [], "entities": []}, {"text": "In such situations, it is natural to push the system to make safer choices, giving higher preference to domain-invariant translations, which work well across domains, over risky domain-specific alternatives.", "labels": [], "entities": []}, {"text": "We encode this intuition by (1) inducing latent subdomains from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain).", "labels": [], "entities": []}, {"text": "We conduct experiments on three language pairs and a number of different domains.", "labels": [], "entities": []}, {"text": "We observe consistent improvements over a baseline which does not explicitly reward domain invariance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Mismatch in phrase translation distributions between test data (target domain) and train data is known to harm performance of statistical translation systems).", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.691208690404892}]}, {"text": "Domain-adaptation methods) aim to specialize a system estimated on out-of-domain training data to a target domain represented by a small data sample.", "labels": [], "entities": []}, {"text": "In practice, however, the target domain may not be known at training time or it may changeover time depending on user needs.", "labels": [], "entities": []}, {"text": "In this work we address exactly the setting where we have a domain-agnostic system but we have no access to any samples from the target domain at training time.", "labels": [], "entities": []}, {"text": "This is an important and challenging setting which, as far as we are aware, has not yet received attention in the literature.", "labels": [], "entities": []}, {"text": "When the target domain is unknown at training time, the system could be trained to make safer choices, preferring translations which are likely to work across different domains.", "labels": [], "entities": []}, {"text": "For example, when translating from English to Russian, the most natural translation for the word 'code' would be highly dependent on the domain (and the corresponding word sense).", "labels": [], "entities": []}, {"text": "The Russian words 'xifr', 'zakon' or 'programma' would perhaps be optimal choices if we consider cryptography, legal and software development domains, respectively.", "labels": [], "entities": []}, {"text": "However, the translation 'kod' is also acceptable across all these domains and, as such, would be a safer choice when the target domain is unknown.", "labels": [], "entities": []}, {"text": "Note that such a translation may not be the most frequent overall and, consequently, might not be proposed by a standard (i.e., domain-agnostic) phrase-based translation system.", "labels": [], "entities": []}, {"text": "In order to encode preference for domaininvariant translations, we introduce a measure which quantifies how likely a phrase (or a phrase-pair) is to be \"domain-invariant\".", "labels": [], "entities": []}, {"text": "We recall that most large parallel corpora are heterogeneous, consisting of diverse language use originating from a variety of unspecified subdomains.", "labels": [], "entities": []}, {"text": "For example, news articles may cover sports, finance, politics, technology and a variety of other news topics.", "labels": [], "entities": []}, {"text": "None of the subdomains may match the target domain particularly well, but they can still reveal how domain-specific a given phrase is.", "labels": [], "entities": []}, {"text": "For example, if we would observe that the word 'code' can be translated as 'kod' across cryptography and legal subdomains observed in training data, we can hypothesize that it may work better on anew unknown domain than 'zakon' which was specific only to a single subdomain (legal).", "labels": [], "entities": []}, {"text": "This would be a suitable decision if the test domain happens to be software development, even though no texts pertaining to this domain were included in the heterogeneous training data.", "labels": [], "entities": []}, {"text": "Importantly, the subdomains are usually not specified in the heterogeneous training data.", "labels": [], "entities": []}, {"text": "Therefore, we treat the subdomains as latent, so we can induce them automatically.", "labels": [], "entities": []}, {"text": "Once induced, we define measures of domain specificity, particularly expressing two generic properties: Phrase domain specificity How specific is a target or a source phrase to some of the induced subdomains?", "labels": [], "entities": []}, {"text": "Phrase pair domain coherence How coherent is a source phrase and a target language translation across the induced subdomains?", "labels": [], "entities": [{"text": "Phrase pair domain coherence", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8504471331834793}]}, {"text": "These features capture two orthogonal aspects of phrase behaviour in heterogeneous corpora, with the rationale that phrase pairs can be weighted along these two dimensions.", "labels": [], "entities": []}, {"text": "Domain-specificity captures the intuition that the more specific a phrase is to certain subdomains, the less applicable it is in general.", "labels": [], "entities": []}, {"text": "Note that specificity is applied not only to target phrases (as 'kod' and 'zakon' in the above example) but also to source phrases.", "labels": [], "entities": []}, {"text": "When applied to a source phrase, it may give a preference towards using shorter phrases as they are inherently less domain specific.", "labels": [], "entities": []}, {"text": "In contrast to phrase domain specificity, phrase pair coherence reflects whether candidate target and source phrases are typically used in the same set of domains.", "labels": [], "entities": []}, {"text": "The intuition here is that the more divergent the distributional behaviour of source and target phrases across subdomains, the less certain we are whether this phrase pair is valid for the unknown target domain.", "labels": [], "entities": []}, {"text": "In other words, a translation rule with source and target phrases having two similar distributions over the latent subdomains is likely safer to use.", "labels": [], "entities": []}, {"text": "Weights for these features, alongside all other standard features, are tuned on a development set.", "labels": [], "entities": []}, {"text": "Importantly, we show that there is no noteworthy benefit from tuning the weights on a sample from the target domain.", "labels": [], "entities": []}, {"text": "It is enough to tune them on a mixed-domain dataset sufficiently different from the training data.", "labels": [], "entities": []}, {"text": "We attribute this attractive property to the fact that our features, unlike the ones typically considered in standard domain-adaptation work, are generic and only affect the amount of risk our system takes.", "labels": [], "entities": []}, {"text": "In contrast, for example, in Eidelman et al.", "labels": [], "entities": []}, {"text": "Clearly, domain adaptation with such rich features, though potentially more powerful, would not be possible without a development set closely matching the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7877368927001953}]}, {"text": "We conduct our experiments on three language pairs and explore adaptation to 9 domain adaptation tasks in total.", "labels": [], "entities": []}, {"text": "We observe significant and consistent performance improvements over the baseline domain-agnostic systems.", "labels": [], "entities": []}, {"text": "This result confirms that our two features, and the latent subdomains they are computed from, are useful also for the very challenging domain adaptation setting considered in this work.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7491928935050964}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Adaptation results when tuning on the in-domain development set. The bold face indicates that the  improvement over the baseline is significant.", "labels": [], "entities": []}, {"text": " Table 4: Adaptation results when tuning on the mixed-domain development set. The bold face indicates  that the improvement over the baseline is significant.", "labels": [], "entities": []}, {"text": " Table 8: Latent Subdomain Analysis (with BLEU score).", "labels": [], "entities": [{"text": "Latent Subdomain Analysis", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6060828963915507}, {"text": "BLEU score", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9772263765335083}]}, {"text": " Table 9: Comparison in latent domain induction  with various algorithms.", "labels": [], "entities": [{"text": "latent domain induction", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6103558937708536}]}, {"text": " Table 10: Combination of all features.", "labels": [], "entities": []}]}