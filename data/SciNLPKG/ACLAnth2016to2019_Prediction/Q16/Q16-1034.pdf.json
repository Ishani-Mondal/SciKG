{"title": [{"text": "Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees", "labels": [], "entities": [{"text": "Infinite-order Language Modelling", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.6135342319806417}]}], "abstractContent": [{"text": "Efficient methods for storing and querying are critical for scaling high-order m-gram language models to large corpora.", "labels": [], "entities": []}, {"text": "We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly.", "labels": [], "entities": []}, {"text": "We present several optimisations which improve query runtimes up to 2500\u00d7, despite only incurring a modest increase in construction time and memory usage.", "labels": [], "entities": []}, {"text": "For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package.", "labels": [], "entities": []}, {"text": "It imposes much lower memory requirements, often by orders of magnitude, and has run-times that are either similar (for training) or comparable (for querying).", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.826568216085434}, {"text": "speech recognition", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7536822259426117}]}, {"text": "Statistical LMs are probabilistic models that assign a probability to a sequence of words w N 1 , indicating how likely the sequence is in the language.", "labels": [], "entities": []}, {"text": "m-gram LMs are popular, and prove to be accurate when estimated using large corpora.", "labels": [], "entities": []}, {"text": "In these LMs, the probabilities of m-grams are often precomputed and stored explicitly.", "labels": [], "entities": []}, {"text": "Although widely successful, current m-gram LM approaches are impractical for learning high-order LMs on large corpora, due to their poor scaling properties in both training and query phases.", "labels": [], "entities": []}, {"text": "Prevailing methods) precompute all m-gram probabilities, and consequently need to store and access as many as a hundred of billions of m-grams fora typical moderate-order LM.", "labels": [], "entities": []}, {"text": "Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables), lossy compression, compact data structures (, and distributed computation (.", "labels": [], "entities": []}, {"text": "Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying.", "labels": [], "entities": []}, {"text": "Exceptions are and, who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly.", "labels": [], "entities": []}, {"text": "In our previous work, we extended this line of research using a Compressed Suffix Tree (CST) ( , which provides a considerably more compact searchable means of storing the corpus than an uncompressed suffix array or suffix tree.", "labels": [], "entities": []}, {"text": "This approach showed favourable scaling properties with m and had only a modest memory requirement.", "labels": [], "entities": []}, {"text": "However, the method only supported Kneser-Ney smoothing, not its modified variant) which overall performs better and has become the de-facto standard.", "labels": [], "entities": []}, {"text": "Additionally, querying was significantly slower than for leading LM toolkits, making the method impractical for widespread use.", "labels": [], "entities": [{"text": "querying", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.9700722098350525}]}, {"text": "In this paper we extend to support modified Kneser-Ney smoothing, and present new optimisation methods for fast construction and querying.", "labels": [], "entities": []}, {"text": "Critical to our approach are: \u2022 Precomputation of several modified counts, which would be very expensive to compute at query time.", "labels": [], "entities": []}, {"text": "To orchestrate this, a subset of the CST nodes is selected based on the cost of computing their modified counts (which relates with the branching factor of a node).", "labels": [], "entities": []}, {"text": "The precomputed counts are then stored in a compressed data structure supporting efficient memory usage and lookup.", "labels": [], "entities": []}, {"text": "\u2022 Re-use of CST nodes within m-gram probability computation as a sentence gets scored leftto-right, thus saving many expensive lookups.", "labels": [], "entities": []}, {"text": "Empirical comparison against our earlier work ( shows the significance of each of these optimisations.", "labels": [], "entities": []}, {"text": "The strengths of our method are apparent when applied to very large training datasets (\u2265 16 GiB) and for high order models, m \u2265 5.", "labels": [], "entities": []}, {"text": "In this setting, while our approach is more memory efficient than the leading KenLM model, both in the construction (training) and querying (testing) phases, we are highly competitive in terms of runtimes of both phases.", "labels": [], "entities": []}, {"text": "When memory is a limiting factor at query time, our approach is orders of magnitude faster than the state of the art.", "labels": [], "entities": []}, {"text": "Moreover, our method allows for efficient querying with an unlimited Markov order, m \u2192 \u221e, without resorting to approximations or heuristics.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our approach we measure memory and time usage, along with the predictive perplexity score of word-level LMs on a number of different corpora varying in size and domain.", "labels": [], "entities": []}, {"text": "For all of our word-level LMs, we use \u00af m, \u02c6 m \u2264 10.", "labels": [], "entities": []}, {"text": "We also demonstrate the positive impact of increasing the set limit on \u00af m, \u02c6 m from 10 to 50 on improving characterlevel LM perplexity.", "labels": [], "entities": []}, {"text": "The SDSL library () is used to implement our data structures.", "labels": [], "entities": []}, {"text": "The benchmarking experiments were run on a single core of a Intel Xeon E5-2687 v3 3.10GHz server with 500GiB of RAM.", "labels": [], "entities": []}, {"text": "In our word-level experiments, we use the German subset of the Europarl () as a small corpus, which is 382 MiB in size measuring the raw uncompressed text.", "labels": [], "entities": [{"text": "German subset of the Europarl ()", "start_pos": 42, "end_pos": 74, "type": "DATASET", "confidence": 0.8944013913472494}]}, {"text": "We also evaluate on much larger corpora, training on 32GiB subsets of the deduplicated English, Spanish, German, and French Common Crawl corpus.", "labels": [], "entities": [{"text": "French Common Crawl corpus", "start_pos": 117, "end_pos": 143, "type": "DATASET", "confidence": 0.6171998381614685}]}, {"text": "As test sets, we used newstest-2014 for all languages except Spanish, for which we used newstest-2013.", "labels": [], "entities": []}, {"text": "11 In our Although the SA can be very large, we need not store it in memory.", "labels": [], "entities": []}, {"text": "The DFS traversal in Algorithm 4 (lines 4-16) means that the calls to SA occur in increasing order of . Hence, we use on-disk storage for the SA with a small memory mapped buffer, thereby incurring a negligible memory overhead.", "labels": [], "entities": []}, {"text": "It is possible to compute the discounts for all patterns of the text using our algorithm with complexity linear in the length of the text.", "labels": [], "entities": []}, {"text": "However, the discounts appear to converge by pattern length \u00af m = 10.", "labels": [], "entities": []}, {"text": "This limit also helps to avoid problems of wild fluctuations in discounts for very long patterns arising from noise for low count events.", "labels": [], "entities": []}, {"text": "benchmarking experiments we used the bottom 1M sentences (not used in training) of the German Comman Crawl corpus.", "labels": [], "entities": [{"text": "German Comman Crawl corpus", "start_pos": 87, "end_pos": 113, "type": "DATASET", "confidence": 0.9364909827709198}]}, {"text": "We used the preprocessing script of, then removed sentences with \u2264 2 words, and replaced rare words 12 c \u2264 9 in the training data with a special token.", "labels": [], "entities": []}, {"text": "In our characterlevel experiments, we used the training and test data of the benchmark 1-billion-words corpus ().", "labels": [], "entities": []}, {"text": "Small data: German Europarl First, we compare the time and memory consumption of both the SRILM and KenLM toolkits, and the CST on the small German corpus.", "labels": [], "entities": [{"text": "German Europarl", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.8458674550056458}, {"text": "SRILM", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.8347639441490173}, {"text": "German corpus", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.7747203707695007}]}, {"text": "shows the memory usage for construction and querying for CST-based methods w/o precomputation is independent of m, but becomes substantially with m for the SRILM and KenLM benchmarks.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.8721532225608826}, {"text": "KenLM benchmarks", "start_pos": 166, "end_pos": 182, "type": "DATASET", "confidence": 0.7163761407136917}]}, {"text": "To make our results comparable to those reported in ( for query time measurements we reported the loading and query time combined.", "labels": [], "entities": []}, {"text": "The construction cost is modest, requiring less memory than the benchmark systems form \u2265 3, and running in a similar time (despite our method supporting queries Running with the full vocabulary increased the memory requirement by 40% for construction and 5% for querying with our model, and 10% and 30%, resp.", "labels": [], "entities": []}, {"text": "Construction times for both approaches were 15% slower, but query runtime was 20% slower for our model versus 80% for KenLM.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.8393165469169617}]}, {"text": "For all timings reported in the paper we manually flushed the system cache between each operation (both for construction size (M) perplexity tokens m = 2 m = 3 m = 5 m = 7 m = 10 m = \u221e  of unlimited size).", "labels": [], "entities": []}, {"text": "Precomputation adds to the construction time, which rose from 173 to 299 seconds, but yielded speed improvements of several orders of magnitude for querying (218k to 98 seconds for 10-gram).", "labels": [], "entities": []}, {"text": "In querying, the CST-precompute method is 2-4\u00d7 slower than both SRILM and KenLM for large m \u2265 5, with the exception of m = 10 where it outperforms SRILM.", "labels": [], "entities": []}, {"text": "A substantial fraction of the query time is loading the structures from disk; when this cost is excluded, our approach is between 8-13\u00d7 slower than the benchmark toolkits.", "labels": [], "entities": []}, {"text": "Note that perplexity computed by the CST closely matched KenLM (differences \u2264 0.1).", "labels": [], "entities": [{"text": "CST", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9001621007919312}, {"text": "KenLM", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.698005199432373}]}, {"text": "reports the perplexity results for training on 32GiB subsets of the English, Spanish, French, and German Common Crawl corpus.", "labels": [], "entities": [{"text": "Common Crawl corpus", "start_pos": 105, "end_pos": 124, "type": "DATASET", "confidence": 0.7286502023537954}]}, {"text": "Note that with such large datasets, perplexity improves with increasing m, with substantial gains available moving above the widely used m = 5.", "labels": [], "entities": []}, {"text": "This highlights the importance of our approach being independent from m, in that we can evaluate for any m, including \u221e, at low cost.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Perplexities on English, French, German new- stests 2014, and Spanish newstest 2013 when trained on  32GiB chunks of English, Spanish, French, and German  Common Crawl corpus.", "labels": [], "entities": []}, {"text": " Table 3: Perplexity of German newstest 2014 with differ- ent datasets (Europarl, News-Commentary, NewsCrawl  2007-2014, CommonCrawl 1-32 GiB chunks) and m.", "labels": [], "entities": [{"text": "German newstest 2014", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.8763942321141561}, {"text": "Europarl", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9668797254562378}, {"text": "NewsCrawl  2007-2014", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.9255443215370178}]}, {"text": " Table 4: Perplexity results for the 1 billion word bench- mark corpus, showing word based and character based  MKN models, for different m. Timings and peak mem- ory usage are reported for construction. The word  model computed discounts and precomputed counts up  to \u00af  m, \u02c6  m = 10, while the character model used thresholds  \u00af  m, \u02c6  m = 50. Timings measured on a single core.", "labels": [], "entities": [{"text": "1 billion word bench- mark corpus", "start_pos": 37, "end_pos": 70, "type": "DATASET", "confidence": 0.6208430273192269}, {"text": "Timings", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9664463996887207}, {"text": "mem- ory usage", "start_pos": 158, "end_pos": 172, "type": "METRIC", "confidence": 0.8466525971889496}]}]}