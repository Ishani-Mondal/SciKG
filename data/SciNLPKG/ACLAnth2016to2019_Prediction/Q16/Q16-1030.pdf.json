{"title": [{"text": "Encoding Prior Knowledge with Eigenword Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Canonical correlation analysis (CCA) is a method for reducing the dimension of data represented using two views.", "labels": [], "entities": [{"text": "Canonical correlation analysis (CCA)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8754580815633138}]}, {"text": "It has been previously used to derive word embeddings, where one view indicates a word, and the other view indicates its context.", "labels": [], "entities": []}, {"text": "We describe away to incorporate prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings.", "labels": [], "entities": []}, {"text": "Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space.", "labels": [], "entities": []}, {"text": "Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing, word or phrase similarity (), dependency parsing (), unsupervised learning () and others.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7464088499546051}, {"text": "word or phrase similarity", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.5736466944217682}, {"text": "dependency parsing", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.8014790117740631}]}, {"text": "Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets.", "labels": [], "entities": []}, {"text": "These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks () or spectral methods.", "labels": [], "entities": []}, {"text": "While these general purpose word embeddings have achieved significant improvement in various tasks in NLP, it has been discovered that further tuning of these continuous word representations for specific tasks improves their performance by a larger margin.", "labels": [], "entities": []}, {"text": "For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses () or they could be refined using semantic lexicons such as WordNet, FrameNet () and the Paraphrase Database ( to improve various similarity tasks (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8064925670623779}, {"text": "WordNet", "start_pos": 196, "end_pos": 203, "type": "DATASET", "confidence": 0.9484943151473999}]}, {"text": "This paper proposes a method to encode prior semantic knowledge in spectral word embeddings.", "labels": [], "entities": []}, {"text": "Spectral learning algorithms are of great interest for their speed, scalability, theoretical guarantees and performance in various NLP applications.", "labels": [], "entities": []}, {"text": "These algorithms are no strangers to word embeddings either.", "labels": [], "entities": []}, {"text": "In latent semantic analysis), word embeddings are learned by performing SVD on the word by document matrix.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7142395873864492}]}, {"text": "Recently, have proposed to use canonical correlation analysis (CCA) as a method to learn lowdimensional real vectors, called Eigenwords.", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 31, "end_pos": 67, "type": "TASK", "confidence": 0.775418202082316}]}, {"text": "Unlike LSA based methods, CCA based methods are scale invariant and can capture multiview information such as the left and right contexts of the words.", "labels": [], "entities": []}, {"text": "As a result, the eigenword embeddings of that were learned using the simple linear methods give accuracies comparable to or better than state of the art when compared with highly nonlinear deep learning based approaches).", "labels": [], "entities": []}, {"text": "The main contribution of this paper is a technique to incorporate prior knowledge into the derivation of canonical correlation analysis.", "labels": [], "entities": [{"text": "canonical correlation analysis", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.6887091994285583}]}, {"text": "In contrast to previous work where prior knowledge is introduced in the off-the-shelf embeddings as a post-processing step, our approach introduces prior knowledge in the CCA derivation itself.", "labels": [], "entities": []}, {"text": "In this way it preserves the theoretical properties of spectral learning algorithms for learning word embeddings.", "labels": [], "entities": []}, {"text": "The prior knowledge is based on lexical resources such as WordNet, FrameNet and the Paraphrase Database.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9735462665557861}]}, {"text": "Our derivation of CCA to incorporate prior knowledge is not limited to eigenwords and can be used with CCA for other problems.", "labels": [], "entities": []}, {"text": "It follows a similar idea to the one proposed by for improving the visualization of principal vectors with principal component analysis (PCA).", "labels": [], "entities": []}, {"text": "Our derivation represents the solution to CCA as that of an optimization problem which maximizes the distance between the two view projections of training examples, while weighting these distances using the external source of prior knowledge.", "labels": [], "entities": []}, {"text": "As such, our approach applies to other uses of CCA in the NLP literature, such as the one of, who used CCA for transliteration, or the one of, who used CCA for semantically representing visual attributes.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experiments.", "labels": [], "entities": []}, {"text": "Training Data We used three datasets, WIKI1, WIKI2 and WIKI5, all based on the first 1, 2 and 5 billion words from Wikipedia respectively.", "labels": [], "entities": [{"text": "Training Data", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8337071537971497}, {"text": "WIKI1", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.9561268091201782}, {"text": "WIKI2", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.8718955516815186}, {"text": "WIKI5", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.917369544506073}]}, {"text": "Each dataset is broken into chunks of length 13 (window sizes of 6), corresponding to a document.", "labels": [], "entities": []}, {"text": "The above Laplacian L is calculated within each document separately.", "labels": [], "entities": [{"text": "Laplacian L", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.5124984979629517}]}, {"text": "This means that \u2212L ij is 1 only if i and j denote two words that appear in the same document.", "labels": [], "entities": []}, {"text": "This is done to make the calculations computationally feasible.", "labels": [], "entities": []}, {"text": "We calculate word embeddings for the topmost frequent 200K words.", "labels": [], "entities": []}, {"text": "Prior Knowledge Resources We consider three sources of prior knowledge: WordNet, the Paraphrase Database of, abbreviated as PPDB, and FrameNet ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9639977216720581}]}, {"text": "Since FrameNet and WordNet index words in their base form, we use WordNet's stemmer to identify the base form for the text in our corpora whenever we calculate the Laplacian graph.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9345177412033081}, {"text": "WordNet's stemmer", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9204753637313843}]}, {"text": "For WordNet, we have an edge in the graph if one word is a synonym, hypernym or hyponym of the other.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9347443580627441}]}, {"text": "For PPDB, we have an edge if one word is a paraphrase of the other, according to the database.", "labels": [], "entities": []}, {"text": "For FrameNet, we connect two words in the graph if they appear in the same frame.", "labels": [], "entities": []}, {"text": "System Implementation We modified the implementation of the SWELL Java package 4 of.", "labels": [], "entities": []}, {"text": "Specifically, we needed to modify the loop that iterates over words in each document to a nested loop that iterates over pairs of words, in order to compute a sum of the form ij X ri L ij Y js . 5 use window size k = 2, which we retain in our experiments.", "labels": [], "entities": []}, {"text": "We evaluated the quality of our eigenword embeddings on three different tasks: word similarity, geographic analogies and NP bracketing.", "labels": [], "entities": [{"text": "NP bracketing", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.8155481219291687}]}, {"text": "Word Similarity For the word similarity task we experimented with 11 different widely used benchmarks.", "labels": [], "entities": [{"text": "Word Similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.610744833946228}, {"text": "word similarity task", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8242173989613851}]}, {"text": "The WS-353-ALL dataset) consists of 353 pairs of English words with their human similarity ratings.", "labels": [], "entities": [{"text": "WS-353-ALL dataset", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8944635093212128}]}, {"text": "Later, re-annotated WS-353-ALL for similarity (WS-353-SIM) and relatedness (WS-353-REL) with specific distinctions between them.", "labels": [], "entities": [{"text": "WS-353-ALL", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.8229618072509766}, {"text": "similarity", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9753088355064392}]}, {"text": "The SimLex-999 dataset () was built to measure how well models capture similarity, rather than relatedness or association.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9314856231212616}]}, {"text": "The MEN-TR-3000 dataset () consists of 3000 word pairs sampled from words that occur at least 700 times in a large web corpus.", "labels": [], "entities": [{"text": "MEN-TR-3000 dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9039442837238312}]}, {"text": "The datasets,) and MTurk-771 (, were scored by Amazon Mechanical Turk workers for relatedness of English word pairs.) and Verb-143 () datasets were developed for verb similarity predictions.", "labels": [], "entities": [{"text": "MTurk-771", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.8308171033859253}, {"text": "Verb-143", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.7909046411514282}, {"text": "verb similarity predictions", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.7813164591789246}]}, {"text": "The last two datasets, and RG-65 consist of 30 and 65 noun pairs respectively.", "labels": [], "entities": [{"text": "RG-65", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.7270577549934387}]}, {"text": "For each dataset, we calculate the cosine similarity between the vectors of word pairs and measure Spearman's rank correlation coefficient between the scores produced by the embeddings and human ratings.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 99, "end_pos": 138, "type": "METRIC", "confidence": 0.6681387186050415}]}, {"text": "We report the average of the correlations on all 11 datasets.", "labels": [], "entities": [{"text": "correlations", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.9511707425117493}]}, {"text": "Each word similarity task in the above list represents a different aspect of word similarity, and as such, averaging the results points to the quality of the word embeddings on several tasks.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7020555436611176}]}, {"text": "We later analyze specific datasets.", "labels": [], "entities": []}, {"text": "NP Bracketing Here the goal is to identify the correct bracketing of a three-word noun (.", "labels": [], "entities": [{"text": "NP Bracketing", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.81856769323349}]}, {"text": "For example, the bracketing of annual (price growth) is \"right,\" while the bracketing of (entry level) machine is \"left.\"", "labels": [], "entities": []}, {"text": "Similarly to , we concatenate the word vectors of the three words, and use this vector for binary classification into left or right.", "labels": [], "entities": []}, {"text": "Since most of the datasets that we evaluate on in this paper are not standardly separated into development and test sets, we report all results we calculated (with respect to hyperparameter differences) and do not select just a subset of the results.", "labels": [], "entities": []}, {"text": "Preliminary Experiments In our first set of experiments, we vary the dimension of the word embedding vectors.", "labels": [], "entities": []}, {"text": "We try m \u2208 {50, 100, 200, 300}.", "labels": [], "entities": []}, {"text": "Our experiments showed that the results consistently improve when the dimension increases for all the different datasets.", "labels": [], "entities": [{"text": "dimension", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9669271111488342}]}, {"text": "For example, form = 50 and WIKI1, we get an average of 46.4 on the word similarity tasks, 50.1 form = 100, 53.4 form = 200 and 54.2 form = 300.", "labels": [], "entities": [{"text": "WIKI1", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.7101746201515198}, {"text": "word similarity tasks", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.7597549259662628}]}, {"text": "The more data are available, the more likely larger dimension will improve the quality of the word embeddings.", "labels": [], "entities": []}, {"text": "Indeed, for WIKI5, we get an average of 49.4, 54.9, 57.0 and 59.5 for each of the dimensions.", "labels": [], "entities": [{"text": "WIKI5", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9527021050453186}]}, {"text": "The improvements with respect to the dimension are consistent across all of our results, so we fix mat 300.", "labels": [], "entities": []}, {"text": "We also noticed a consistent improvement inaccuracy when using more data from Wikipedia.", "labels": [], "entities": []}, {"text": "For example, form = 300, using WIKI1 gives an average of 54.1, while using WIKI2 gives an average of 54.9 and finally, using WIKI5 gives an average of 59.5.", "labels": [], "entities": [{"text": "form", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9470693469047546}, {"text": "WIKI1", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9803111553192139}, {"text": "WIKI2", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.976116418838501}, {"text": "WIKI5", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.979649007320404}]}, {"text": "We fix the dataset we use to be WIKI5.", "labels": [], "entities": [{"text": "WIKI5", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9663949608802795}]}, {"text": "Results describes the results from our first set of experiments.", "labels": [], "entities": []}, {"text": "(Note that the table is divided into 9 distinct blocks, labeled A through I.)", "labels": [], "entities": []}, {"text": "In general, adding prior knowledge to eigenword embeddings does improve the quality of word vectors for the word similarity, geographic analogies and NP bracketing tasks on several occasions (blocks D-F compared to last row in blocks A-C).", "labels": [], "entities": [{"text": "NP bracketing", "start_pos": 150, "end_pos": 163, "type": "TASK", "confidence": 0.848796010017395}]}, {"text": "For example, our eigenword vectors encoded with prior knowledge (CCAPrior) consistently perform better than the eigenword vectors that do not have any prior knowledge for the word similarity task (59.5, Eigen in the first row under NPK column, versus block D).", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 175, "end_pos": 195, "type": "TASK", "confidence": 0.7910253008206686}]}, {"text": "The only exceptions are for \u03b1 = 0.1 with WordNet (59.1), for \u03b1 = 0.7 with PPDB (59.3) and for \u03b1 = 0.9 with FrameNet (58.9), where \u03b1 denotes the smoothing factor.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9719027280807495}, {"text": "PPDB", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9129359722137451}]}, {"text": "In several cases, running the retrofitting algorithm of  on top of our word embeddings helps further, as if \"adding prior knowledge twice is better than once.\"", "labels": [], "entities": []}, {"text": "Results for these word embeddings (CCAPrior+RF) are shown in.", "labels": [], "entities": []}, {"text": "Adding retrofitting to our encoding of prior knowl-edge often performs better for word similarity and NP bracketing tasks (block D versus G and block F versus I).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7145270705223083}, {"text": "NP bracketing", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.7824365496635437}]}, {"text": "Interestingly, CCAPrior+RF embeddings also often perform better than eigenword vectors (Eigen) of when retrofitted using the method of . For example, in the word similarity task, eigenwords retrofitted with WordNet get an accuracy of 62.2 whereas encoding prior knowledge using both CCA and retrofitting gets a maximum accuracy of 63.3.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.8294343749682108}, {"text": "accuracy", "start_pos": 222, "end_pos": 230, "type": "METRIC", "confidence": 0.9976511597633362}, {"text": "accuracy", "start_pos": 319, "end_pos": 327, "type": "METRIC", "confidence": 0.9910188913345337}]}, {"text": "We seethe same pattern for PPDB, with 63.6 for \"Eigen\" and 64.9 for \"CCAPrior+RF\".", "labels": [], "entities": [{"text": "PPDB", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8408488035202026}]}, {"text": "We hypothesize that the reason for these changes is that the two methods for encoding prior knowledge maximize different objective functions.", "labels": [], "entities": []}, {"text": "The performance with FrameNet is weaker, in some cases leading to worse performance (e.g., with Glove and SG vectors).", "labels": [], "entities": []}, {"text": "We believe that FrameNet does not perform as well as the other lexicons because it groups words based on very abstract concepts; often words with seemingly distantly related meanings (e.g., push and growth) can evoke the same frame.", "labels": [], "entities": []}, {"text": "This also supports the findings of , who noticed that the use of FrameNet as a prior knowledge resource for improving the quality of word embeddings is not as helpful as other resources such as WordNet and PPDB.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 194, "end_pos": 201, "type": "DATASET", "confidence": 0.9466875791549683}]}, {"text": "We note that CCA works especially well for the geographic analogies dataset.", "labels": [], "entities": []}, {"text": "The quality of eigenword embeddings (and the other embeddings) degrades when we encode prior knowledge using the method of . Our method improves the quality of eigenword embeddings.", "labels": [], "entities": []}, {"text": "Global Picture of the Results When comparing retrofitting to CCA with prior knowledge, there is a noticable difference.", "labels": [], "entities": []}, {"text": "Retrofitting performs well or badly, depending on the dataset, while the results with CCA are more stable.", "labels": [], "entities": []}, {"text": "We attribute this to the difference between how our algorithm and retrofitting work.", "labels": [], "entities": []}, {"text": "Retrofitting makes a direct use of the source of prior knowledge, by adding a regularization term that enforces words which are similar according to the prior knowledge to be closer in the embedding space.", "labels": [], "entities": []}, {"text": "Our algorithm, on the other hand, makes a more indirect use of the source of prior knowledge, by changing the co-occurence matrix on which we do singular value decomposition.", "labels": [], "entities": [{"text": "singular value decomposition", "start_pos": 145, "end_pos": 173, "type": "TASK", "confidence": 0.6205726067225138}]}, {"text": "Specifically, we believe that our algorithm is more stable to cases in which words for the task at hand are unknown words with respect to the source of prior knowledge.", "labels": [], "entities": []}, {"text": "This is demonstrated with the geographical analogies task: in that case, retrofitting lowers the results inmost cases.", "labels": [], "entities": []}, {"text": "The city and country names do not appear in the sources of prior knowledge we used.", "labels": [], "entities": []}, {"text": "Further Analysis We further inspected the results on the word similarity tasks for the RG-65 and WS-353-ALL datasets.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.733627438545227}, {"text": "RG-65", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.9357903599739075}, {"text": "WS-353-ALL datasets", "start_pos": 97, "end_pos": 116, "type": "DATASET", "confidence": 0.8768392205238342}]}, {"text": "Our goal was to find cases in which either CCA embeddings by themselves outperform other types of embeddings or that encoding prior knowledge into CCA the way we describe significantly improves the results.", "labels": [], "entities": []}, {"text": "For the WS-353-ALL dataset, the eigenword embeddings get a correlation of 69.6.", "labels": [], "entities": [{"text": "WS-353-ALL dataset", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.961976557970047}]}, {"text": "The next best performing word embeddings are the multilingual word embeddings (68.0) and skip-gram (58.3).", "labels": [], "entities": []}, {"text": "Interestingly enough, the multilingual word embeddings also use CCA to project words into a lowdimensional space using a linear transformation, suggesting that linear projections area good fit for the WS-353-ALL dataset.", "labels": [], "entities": [{"text": "WS-353-ALL dataset", "start_pos": 201, "end_pos": 219, "type": "DATASET", "confidence": 0.9654180705547333}]}, {"text": "The dataset itself includes pairs of common words with a corresponding similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.9437626302242279}]}, {"text": "The words that appear in the dataset are actually expected to occur in similar contexts, a property that CCA directly encodes when deriving word embeddings.", "labels": [], "entities": []}, {"text": "The best performance on the RG-65 dataset is with the Glove word embeddings (76.6).", "labels": [], "entities": [{"text": "RG-65 dataset", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9450322389602661}]}, {"text": "CCA embeddings give an accuracy of 69.7 on that dataset.", "labels": [], "entities": [{"text": "CCA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8356874585151672}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993658661842346}]}, {"text": "However, with this dataset, we observe significant improvement when encoding prior knowledge using our method.", "labels": [], "entities": []}, {"text": "For example, using WordNet with this dataset improves the results by 4.2 points (73.9).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9683116674423218}]}, {"text": "Using the method of  (with WordNet) on top of our CCA word embeddings improves the results even further by 8.7 points (78.4).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9619669914245605}]}, {"text": "The Role of Prior Knowledge We also designed an experiment to test whether using distributional information is necessary for having well-performing word embeddings, or whether it is sufficient to rely on the prior knowledge resource.", "labels": [], "entities": []}, {"text": "In order to test this, we created a sparse matrix that corresponds to the graph based on the external resource graph.", "labels": [], "entities": []}, {"text": "We then followup with singular value decomposition on  that graph, and get embeddings of size 300.", "labels": [], "entities": []}, {"text": "gives the results when using these embeddings.", "labels": [], "entities": []}, {"text": "We see that the results are consistently lower than the results that appear in, implying that the use of prior knowledge comes hand in hand with the use of distributional information.", "labels": [], "entities": []}, {"text": "When using the retrofitting method by Faruqui et al. on top of these word embeddings, the results barely improved.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks  (A-C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for  WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the  word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer", "labels": [], "entities": [{"text": "NP bracketing", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.8501796722412109}, {"text": "WordNet", "start_pos": 230, "end_pos": 237, "type": "DATASET", "confidence": 0.9171945452690125}, {"text": "FN", "start_pos": 255, "end_pos": 257, "type": "METRIC", "confidence": 0.9744884371757507}]}, {"text": " Table 2: Results on word similarity dataset (average  over 11 datasets) and NP bracketing. The word embed- dings are derived by using SVD on the similarity graph  extracted from the prior knowledge source (WordNet,  PPDB and FrameNet).", "labels": [], "entities": [{"text": "NP bracketing", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.8215666115283966}, {"text": "WordNet", "start_pos": 207, "end_pos": 214, "type": "DATASET", "confidence": 0.9733596444129944}]}]}