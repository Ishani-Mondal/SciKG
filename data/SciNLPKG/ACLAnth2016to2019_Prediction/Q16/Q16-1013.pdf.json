{"title": [{"text": "Reassessing the Goals of Grammatical Error Correction: Fluency Instead of Grammaticality", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.7762730717658997}]}], "abstractContent": [{"text": "The field of grammatical error correction (GEC) has grown substantially in recent years, with research directed at both evaluation met-rics and improved system performance against those metrics.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.789406547943751}]}, {"text": "One unvisited assumption, however, is the reliance of GEC evaluation on error-coded corpora, which contain specific labeled corrections.", "labels": [], "entities": []}, {"text": "We examine current practices and show that GEC's reliance on such corpora unnaturally constrains annotation and automatic evaluation, resulting in (a) sentences that do not sound acceptable to native speakers and (b) system rankings that do not correlate with human judgments.", "labels": [], "entities": []}, {"text": "In light of this, we propose an alternate approach that jettisons costly error coding in favor of unan-notated, whole-sentence rewrites.", "labels": [], "entities": []}, {"text": "We compare the performance of existing metrics over different gold-standard annotations, and show that automatic evaluation with our new annotation scheme has very strong correlation with expert rankings (\u03c1 = 0.82).", "labels": [], "entities": []}, {"text": "As a result, we advocate fora fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency.", "labels": [], "entities": [{"text": "GEC", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9228160977363586}]}], "introductionContent": [{"text": "What is the purpose of grammatical error correction (GEC)?", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.7504794001579285}]}, {"text": "One response is that GEC aims to help people become better writers by correcting grammatical mistakes in their writing.", "labels": [], "entities": [{"text": "GEC", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.7551864385604858}]}, {"text": "In the NLP community, the original scope of GEC was correcting targeted error types with the goal of providing feedback to non-native writers).", "labels": [], "entities": []}, {"text": "As systems improved and more advanced methods were applied to the task, the definition evolved to wholesentence correction, or correcting all errors of every error type (.", "labels": [], "entities": [{"text": "wholesentence correction", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.7694827318191528}]}, {"text": "With this pivot, we urge the community to revisit the original question.", "labels": [], "entities": []}, {"text": "It is often the case that writing exhibits problems that are difficult to ascribe to specific grammatical categories.", "labels": [], "entities": []}, {"text": "Consider the following example: Original: From this scope , social media has shorten our distance . Corrected: From this scope , social media has shortened our distance . If the goal is to correct verb errors, the grammatical mistake in the original sentence has been addressed and we can move on.", "labels": [], "entities": []}, {"text": "However, when we aim to correct the sentence as a whole, a more vexing problem remains.", "labels": [], "entities": []}, {"text": "The more prominent error has to do with how unnaturally this sentence reads.", "labels": [], "entities": []}, {"text": "The meanings of words and phrases like scope and the corrected shortened our distance are clear, but this is not how a native English speaker would use them.", "labels": [], "entities": [{"text": "scope", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.905879020690918}, {"text": "corrected shortened our distance", "start_pos": 53, "end_pos": 85, "type": "METRIC", "confidence": 0.791933611035347}]}, {"text": "A more fluent version of this sentence would be the following: Fluent: From this perspective , social media has shortened the distance between us . This issue argues fora broader definition of grammaticality that we will term native-language fluency, or simply fluency.", "labels": [], "entities": []}, {"text": "One can argue that traditional understanding of grammar and grammar correction encompasses the idea of native-language fluency.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7455776929855347}]}, {"text": "However, the metrics commonly used in evaluating GEC undermine these arguments.", "labels": [], "entities": [{"text": "GEC", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7840407490730286}]}, {"text": "The performance of GEC systems is typically evaluated us-ing metrics that compute corrections against errorcoded corpora, which impose a taxonomy of types of grammatical errors.", "labels": [], "entities": []}, {"text": "Assigning these codes can be difficult, as evidenced by the low agreement found between annotators of these corpora.", "labels": [], "entities": [{"text": "agreement", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9756121635437012}]}, {"text": "It is also quite expensive.", "labels": [], "entities": []}, {"text": "But most importantly, as we will show in this paper, annotating for explicit error codes places a downward pressure on annotators to find and fix concrete, easily-identifiable grammatical errors (such as wrong verb tense) in lieu of addressing the native fluency of the text.", "labels": [], "entities": []}, {"text": "A related problem is the presence of multiple evaluation metrics computed over error-annotated corpora.", "labels": [], "entities": []}, {"text": "Recent work has shown that metrics like M 2 and I-measure, both of which require errorcoded corpora, produce dramatically different results when used to score system output and produce a ranking of systems in conventional competitions.", "labels": [], "entities": []}, {"text": "In light of all of this, we suggest that the GEC task has overlooked a fundamental question: What are the best practices for corpus annotation and system evaluation?", "labels": [], "entities": [{"text": "GEC task", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.6779845654964447}]}, {"text": "This work attempts to answer this question.", "labels": [], "entities": []}, {"text": "We show that native speakers prefer text that exhibits fluent sentences over ones that have only minimal grammatical corrections.", "labels": [], "entities": []}, {"text": "We explore different methods for corpus annotation (with and without error codes, written by experts and non-experts) and different evaluation metrics to determine which configuration of annotated corpus and metric has the strongest correlation with the human ranking.", "labels": [], "entities": []}, {"text": "In so doing, we establish a reliable and replicable evaluation procedure to help further the advancement of GEC methods.", "labels": [], "entities": [{"text": "GEC", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9233295917510986}]}, {"text": "1 To date, this is the only work to undertake a comprehensive empirical study of annotation and evaluation.", "labels": [], "entities": []}, {"text": "As we will show, the two areas are intimately related.", "labels": [], "entities": []}, {"text": "Fundamentally, this work reframes grammatical error correction as a fluency task.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.611991693576177}]}, {"text": "Our proposed evaluation framework produces system rankings with strong to very strong correlations with human judgments (Spearman's \u03c1 = 0.82, Pearson's r = 0.73), using a variation of the GLEU metric (Napoles et al., 2015) 2 and two sets of \"fluent\" sen-tence rewrites as a gold standard, which are simpler and cheaper to collect than previous annotations.", "labels": [], "entities": [{"text": "Pearson's r = 0.73)", "start_pos": 142, "end_pos": 161, "type": "METRIC", "confidence": 0.8877139886220297}, {"text": "GLEU metric", "start_pos": 188, "end_pos": 199, "type": "METRIC", "confidence": 0.9106739163398743}]}], "datasetContent": [{"text": "Three evaluation metrics have been proposed for GEC: MaxMatch (M 2 ) (Dahlmeier and Ng, 2012), I-measure, and GLEU ().", "labels": [], "entities": [{"text": "GEC", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8470134735107422}, {"text": "MaxMatch (M 2 )", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.8650618553161621}, {"text": "GLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9985253214836121}]}, {"text": "The first two compare the changes made in the output to error-coded spans of the reference corrections.", "labels": [], "entities": []}, {"text": "M 2 was the metric used for the).", "labels": [], "entities": []}, {"text": "It captures wordand phrase-level edits by building an edit lattice and calculating an F-score over the lattice.", "labels": [], "entities": [{"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9842674136161804}]}, {"text": "Felice and Briscoe (2015) note problems with M 2 : specifically, it does not distinguish between a \"do-nothing baseline\" and systems that only propose wrong corrections; also, phrase-level edits can be easily gamed because the lattice treats the deletion of along phrase as a single edit.", "labels": [], "entities": [{"text": "M 2", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9135430157184601}]}, {"text": "To address these issues, they propose I-measure, which generates a token-level alignment between the source sentence, system output, and gold-standard sentences, and then computes accuracy based on the alignment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9976553916931152}]}, {"text": "Unlike these approaches, GLEU does not use error-coded references).", "labels": [], "entities": [{"text": "GLEU", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.5955039262771606}]}, {"text": "Based on BLEU (), it computes n-gram precision of the system output against reference sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9973733425140381}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9511508941650391}]}, {"text": "GLEU additionally penalizes text in the output that was unchanged from the source but changed in the reference sentences.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8797007203102112}]}, {"text": "Recent work by and evaluated these metrics against human evaluations obtained using methods borrowed from the Workshop on Statistical Machine Translation ().", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.7524349888165792}]}, {"text": "Both papers found a moderate to strong correlation with human judgments for GLEU and M 2 , and a slightly negative correlation for I-measure.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9465233087539673}]}, {"text": "Importantly, however, none of these metrics achieved as a high correlation with the human oracle ranking as desired in a fully reliable metric.", "labels": [], "entities": []}, {"text": "In Section 4, we examine the available metrics over different types of reference sets to identify an evaluation setup nearly as reliable as human experts.", "labels": [], "entities": []}, {"text": "Automatic metrics are only a proxy for human judgments, which are crucial to truthfully ascertain the quality of systems.", "labels": [], "entities": []}, {"text": "Even the best result in Section 4.2, which is state of the art and has very strong rank correlation (\u03c1 = 0.819) with the expert ranking, makes dramatic errors in the system ranking.", "labels": [], "entities": []}, {"text": "Given the inherent imperfection of automatic evaluation (and possible over-optimization to the NU-CLE data set), we recommend that human evaluation be produced alongside metric scores whenever possible.", "labels": [], "entities": [{"text": "NU-CLE data set", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.9731618563334147}]}, {"text": "However, human judgments can be expensive to obtain.", "labels": [], "entities": []}, {"text": "Crowdsourcing may address this problem and has been shown to yield reasonably good judgments for several error types at a relatively low cost ).", "labels": [], "entities": []}, {"text": "Therefore, we apply crowdsourcing to sentence-level grammaticality judgments, by replicating previous experiments that reported expert rankings of system output () using nonexperts on MTurk.", "labels": [], "entities": [{"text": "sentence-level grammaticality judgments", "start_pos": 37, "end_pos": 76, "type": "TASK", "confidence": 0.6433890064557394}, {"text": "MTurk", "start_pos": 184, "end_pos": 189, "type": "DATASET", "confidence": 0.9273757338523865}]}, {"text": "Using the same data set as those experiments and the work described in this paper, we asked screened participants 13 on MTurk to rank five randomly selected systems and NUCLE corrections from best to worst, with ties allowed.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.8784827589988708}]}, {"text": "294 sentences were randomly selected for evaluation from the NUCLE subsection used in, and the output for each sentence was ranked by two different participants.", "labels": [], "entities": [{"text": "NUCLE subsection", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9684804975986481}]}, {"text": "The 588 system rankings yield 26,265 pairwise judgments, from which we inferred the absolute system ranking using TrueSkill.", "labels": [], "entities": [{"text": "TrueSkill", "start_pos": 114, "end_pos": 123, "type": "DATASET", "confidence": 0.9112781882286072}]}], "tableCaptions": [{"text": " Table 4: A comparison of annotations across different  annotators (E for expert, N for non-expert). Where there  were more than two annotators, statistics are over the full  pairwise set. Identical refers to the percentage of sen- tences where both annotators made the same correction  and sTER is the mean sTER between the annotators' cor- rections.", "labels": [], "entities": [{"text": "Identical", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.987129271030426}]}, {"text": " Table 5: Human ranking of the new annotations by gram- maticality. Lines between systems indicate clusters ac- cording to bootstrap resampling at p \u2264 0.05. Systems in  the same cluster are considered to be tied.", "labels": [], "entities": []}, {"text": " Table 6: Correlation between the human ranking and  metric scores over different reference sets. The first line  of each cell is Spearman's \u03c1 and the second line is Pear- son's r. The strongest correlations for each metric are  starred, and the overall strongest correlations are in bold.", "labels": [], "entities": []}, {"text": " Table 7: Inter-annotator agreement of pairwise system  judgments within non-experts, experts and between them.  We show Cohen's \u03ba and quadratic-weighted \u03ba. 15", "labels": [], "entities": []}]}