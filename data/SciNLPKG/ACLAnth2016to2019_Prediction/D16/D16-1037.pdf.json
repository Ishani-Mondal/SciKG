{"title": [], "abstractContent": [{"text": "Implicit discourse relation recognition is a crucial component for automatic discourse-level analysis and nature language understanding.", "labels": [], "entities": [{"text": "Implicit discourse relation recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7812486886978149}, {"text": "automatic discourse-level analysis", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.6104709406693777}, {"text": "nature language understanding", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.7462899883588155}]}, {"text": "Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations.", "labels": [], "entities": []}, {"text": "In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer.", "labels": [], "entities": [{"text": "variational neural discourse relation recognizer", "start_pos": 67, "end_pos": 115, "type": "TASK", "confidence": 0.6666866898536682}]}, {"text": "We refer to this model as VarNDRR.", "labels": [], "entities": [{"text": "VarNDRR", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7496204972267151}]}, {"text": "VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse.", "labels": [], "entities": []}, {"text": "In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a repa-rameterized variational lower bound.", "labels": [], "entities": []}, {"text": "This allows VarNDRR to be trained with standard stochastic gradient methods.", "labels": [], "entities": []}, {"text": "Experiments on the benchmark data set show that VarNDRR can achieve comparable results against state-of-the-art baselines without using any manual features.", "labels": [], "entities": [{"text": "benchmark data set", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.8223803440729777}, {"text": "VarNDRR", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.46528512239456177}]}], "introductionContent": [{"text": "Discourse relation characterizes the internal structure and logical relation of a coherent text.", "labels": [], "entities": []}, {"text": "Automatically identifying these relations not only plays an important role in discourse comprehension and generation, but also obtains wide applications in many * Corresponding author other relevant natural language processing tasks, such as text summarization (), conversation (, question answering ( and information extraction ().", "labels": [], "entities": [{"text": "text summarization", "start_pos": 242, "end_pos": 260, "type": "TASK", "confidence": 0.7361574769020081}, {"text": "question answering", "start_pos": 281, "end_pos": 299, "type": "TASK", "confidence": 0.8614490032196045}, {"text": "information extraction", "start_pos": 306, "end_pos": 328, "type": "TASK", "confidence": 0.7792751789093018}]}, {"text": "Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: The company was disappointed by the ruling.", "labels": [], "entities": []}, {"text": "because The obligation is totally unwarranted.", "labels": [], "entities": []}, {"text": "(adapted from wsj 0294) With the discourse connective because, these two sentences display an explicit discourse relation CONTINGENCY which can be inferred easily.", "labels": [], "entities": [{"text": "CONTINGENCY", "start_pos": 122, "end_pos": 133, "type": "METRIC", "confidence": 0.9853206276893616}]}, {"text": "Once this discourse connective is removed, however, the discourse relation becomes implicit and difficult to be recognized.", "labels": [], "entities": []}, {"text": "This is because almost no surface information in these two sentences can signal this relation.", "labels": [], "entities": []}, {"text": "For successful recognition of this relation, in the contrary, we need to understand the deep semantic correlation between disappointed and obligation in the two sentences above.", "labels": [], "entities": []}, {"text": "Although explicit discourse relation recognition (DRR) has made great progress (, implicit DRR still remains a serious challenge due to the difficulty in semantic analysis.", "labels": [], "entities": [{"text": "explicit discourse relation recognition (DRR)", "start_pos": 9, "end_pos": 54, "type": "TASK", "confidence": 0.7604653707572392}, {"text": "semantic analysis", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.727445125579834}]}, {"text": "Conventional approaches to implicit DRR often treat the relation recognition as a classification problem, where discourse arguments and relations are regarded as the inputs and outputs respectively.", "labels": [], "entities": [{"text": "implicit DRR", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.6980632841587067}, {"text": "relation recognition", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7368547916412354}]}, {"text": "Generally, these methods first generate a representation fora discourse, denoted as x 1 (e.g., manual fea- tures in SVM-based recognition) or sentence embeddings in neural networks-based recognition), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x).", "labels": [], "entities": [{"text": "SVM-based recognition", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.7935084104537964}]}, {"text": "In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x.", "labels": [], "entities": []}, {"text": "Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data.", "labels": [], "entities": []}, {"text": "Instead, we assume that there is a latent continuous variable z from an underlying semantic space.", "labels": [], "entities": []}, {"text": "It is this latent variable that generates both discourse arguments and the corresponding relation, i.e. p(x, y|z).", "labels": [], "entities": []}, {"text": "The latent variable enables us to jointly model discourse arguments and their relations, rather than conditionally model yon x.", "labels": [], "entities": []}, {"text": "However, the incorporation of the latent variable makes the modeling difficult due to the intractable computation with respect to the posterior distribution.", "labels": [], "entities": []}, {"text": "Inspired by  as well as  who introduce a variational neural inference model to the intractable posterior via optimizing a reparameterized variational lower bound, we propose a variational neural discourse relation recognizer (VarNDRR) with a latent continuous variable for implicit DRR in this paper.", "labels": [], "entities": [{"text": "variational neural discourse relation recognizer", "start_pos": 176, "end_pos": 224, "type": "TASK", "confidence": 0.6283186376094818}]}, {"text": "The key idea behind VarNDRR is that although the posterior distribution is intractable, we can approximate it via a deep neural network.", "labels": [], "entities": []}, {"text": "illustrates the treat them as univariate variables inmost cases.", "labels": [], "entities": []}, {"text": "Additionally, we use bold symbols to denote variables, and plain symbols to denote values.", "labels": [], "entities": []}, {"text": "Specifically, there are two essential components: \u2022 neural discourse recognizer As a discourse x and its corresponding relation y are independent with each other given the latent variable z (as shown by the solid lines), we can formulate the generation of x and y from z in the equation p \u03b8 (x, y|z) = p \u03b8 (x|z)p \u03b8 (y|z).", "labels": [], "entities": [{"text": "neural discourse recognizer", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6991399725278219}]}, {"text": "These two conditional probabilities on the right hand side are modeled via deep neural networks (see section 3.1).", "labels": [], "entities": []}, {"text": "\u2022 neural latent approximator VarNDRR assumes that the latent variable can be inferred from discourse arguments x and relations y (as shown by the dash lines).", "labels": [], "entities": [{"text": "neural latent approximator VarNDRR", "start_pos": 2, "end_pos": 36, "type": "TASK", "confidence": 0.5556719601154327}]}, {"text": "In order to infer the latent variable, we employ a deep neural network to approximate the posterior q \u03c6 (z|x, y) as well as the prior q \u03c6 (z|x) (see section 3.2), which makes the inference procedure efficient.", "labels": [], "entities": []}, {"text": "We further employ a reparameterization technique to sample z from q \u03c6 (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3).", "labels": [], "entities": []}, {"text": "The main contributions of our work lie in two aspects.", "labels": [], "entities": []}, {"text": "1) We exploit a generative graphic model for implicit DRR.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this has never been investigated before.", "labels": [], "entities": []}, {"text": "2) We develop a neural recognizer and two neural approximators specifically for implicit DRR, which enables both the recognition and inference to be efficient.", "labels": [], "entities": []}, {"text": "We conduct a series of experiments for English implicit DRR on the PDTB-style corpus to evaluate the effectiveness of our proposed VarNDRR model.", "labels": [], "entities": [{"text": "English implicit DRR", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.5824977258841196}, {"text": "PDTB-style corpus", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9628801643848419}]}, {"text": "Experiment results show that our variational model achieves comparable results against several strong baselines in term of F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9876088500022888}]}, {"text": "Extensive analysis on the variational lower bound further reveals that our model can indeed fit the data set with respect to discourse arguments and relations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.", "labels": [], "entities": [{"text": "VarNDRR", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.7573142051696777}]}, {"text": "We used the largest hand-annotated discourse corpus PDTB 2.0   al., 2013; Zhang et al., 2015), we used sections 2-20 as our training set, sections 21-22 as the test set.", "labels": [], "entities": [{"text": "hand-annotated discourse corpus PDTB 2.0   al.", "start_pos": 20, "end_pos": 66, "type": "DATASET", "confidence": 0.7178294410308202}]}, {"text": "Sections 0-1 were used as the development set for hyperparameter optimization.", "labels": [], "entities": [{"text": "hyperparameter optimization", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.8872840106487274}]}, {"text": "In PDTB, discourse relations are annotated in a predicate-argument view.", "labels": [], "entities": []}, {"text": "Each discourse connective is treated as a predicate that takes two text spans as its arguments.", "labels": [], "entities": []}, {"text": "The discourse relation tags in PDTB are arranged in a three-level hierarchy, where the top level consists of four major semantic classes: TEMPORAL (TEM), CONTINGENCY (CON), EX-PANSION (EXP) and COMPARISON (COM).", "labels": [], "entities": [{"text": "TEMPORAL (TEM)", "start_pos": 138, "end_pos": 152, "type": "METRIC", "confidence": 0.8744309544563293}]}, {"text": "Because the top-level relations are general enough to be annotated with a high inter-annotator agreement and are common to most theories of discourse, in our experiments we only use this level of annotations.", "labels": [], "entities": []}, {"text": "We formulated the task as four separate oneagainst-all binary classification problems: each top level class vs. the other three discourse relation classes.", "labels": [], "entities": []}, {"text": "We also balanced the training set by resampling training instances in each class until the number of positive and negative instances are equal.", "labels": [], "entities": []}, {"text": "In contrast, all instances in the test and development set are kept in nature.", "labels": [], "entities": []}, {"text": "The statistics of various data sets is listed in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of implicit discourse relations for the  training (Train), development (Dev) and test (Test) sets in  PDTB.", "labels": [], "entities": []}, {"text": " Table 2: Classification results of different models on the implicit DRR task. Acc=Accuracy, P=Precision, R=Recall,  and F1=F1 score.", "labels": [], "entities": [{"text": "Acc", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9993556141853333}, {"text": "Accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8890336155891418}, {"text": "Precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9278432726860046}, {"text": "Recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.7798746824264526}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9994059801101685}, {"text": "F1 score", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9427335262298584}]}]}