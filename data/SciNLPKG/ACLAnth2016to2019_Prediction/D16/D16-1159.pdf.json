{"title": [{"text": "Does String-Based Neural MT Learn Source Syntax?", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate whether a neural, encoder-decoder translation system learns syntactic information on the source side as a by-product of training.", "labels": [], "entities": []}, {"text": "We propose two methods to detect whether the encoder has learned local and global source syntax.", "labels": [], "entities": []}, {"text": "A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing.", "labels": [], "entities": []}], "introductionContent": [{"text": "The sequence to sequence model (seq2seq) has been successfully applied to neural machine translation (NMT) and can match or surpass MT state-of-art.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.8242280781269073}]}, {"text": "Nonneural machine translation systems consist chiefly of phrase-based systems ( and syntax-based systems (), the latter of which adds syntactic information to source side (tree-to-string), target side (string-to-tree) or both sides (tree-to-tree).", "labels": [], "entities": [{"text": "Nonneural machine translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6972675720850626}]}, {"text": "As the seq2seq model first encodes the source sentence into a high-dimensional vector, then decodes into a target sentence, it is hard to understand and interpret what is going on inside such a procedure.", "labels": [], "entities": []}, {"text": "Considering the evolution of non-neural translation systems, it is natural to ask: 1.", "labels": [], "entities": []}, {"text": "Does the encoder learn syntactic information about the source sentence?", "labels": [], "entities": []}, {"text": "2. What kind of syntactic information is learned, and how much?", "labels": [], "entities": []}, {"text": "3. Is it useful to augment the encoder with additional syntactic information?", "labels": [], "entities": []}, {"text": "In this work, we focus on the first two questions and propose two methods: \u2022 We create various syntactic labels of the source sentence and try to predict these syntactic labels with logistic regression, using the learned sentence encoding vectors (for sentence-level labels) or learned word-by-word hidden vectors (for word-level label).", "labels": [], "entities": []}, {"text": "We find that the encoder captures both global and local syntactic information of the source sentence, and different information tends to be stored at different layers.", "labels": [], "entities": []}, {"text": "\u2022 We extract the whole constituency tree of source sentence from the NMT encoding vectors using a retrained linearized-tree decoder.", "labels": [], "entities": []}, {"text": "A deep analysis on these parse trees indicates that much syntactic information is learned, while various types of syntactic information are still missing.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train two NMT models, English-French (E2F) and English-German (E2G).", "labels": [], "entities": []}, {"text": "To answer whether these translation models' encoders to learn store syntactic information, and how much, we employ two benchmark models: \u2022 An upper-bound model, in which the encoder learns quite a lot of syntactic information.", "labels": [], "entities": []}, {"text": "For the upper bound, we train a neural parser that learns to \"translate\" an English sentence to its linearized constitutional tree (E2P), following.", "labels": [], "entities": []}, {"text": "\u2022 An lower-bound model, in which the encoder learns much less syntactic information.", "labels": [], "entities": []}, {"text": "For the lower bound, we train two sentence autoencoders: one translates an English sentence to itself (E2E), while the other translates a permuted English sentence to itself (PE2PE).", "labels": [], "entities": []}, {"text": "We already had an indication above (Section 2) that a copying model does not necessarily need to remember a sentence's syntactic structure.", "labels": [], "entities": []}, {"text": "shows sample inputs and outputs of the E2E, PE2PE, E2F, E2G, and E2P models.", "labels": [], "entities": []}, {"text": "We use English-French and English-German data from WMT2014 ().", "labels": [], "entities": [{"text": "WMT2014", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.6157275438308716}]}, {"text": "We take 4M English sentences from the English-German data to train E2E and PE2PE.", "labels": [], "entities": []}, {"text": "For the neural parser (E2P), we construct the training corpus following the recipe of: Model settings and test-set BLEU-n4r1 scores ().", "labels": [], "entities": [{"text": "BLEU-n4r1 scores", "start_pos": 115, "end_pos": 131, "type": "METRIC", "confidence": 0.9683153927326202}]}, {"text": "() and the English Web Treebank (. In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sentences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser 1).", "labels": [], "entities": [{"text": "English Web Treebank", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.8873277505238851}]}, {"text": "We call these 8,162K pairs the CJ corpus.", "labels": [], "entities": [{"text": "CJ corpus", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.9518783092498779}]}, {"text": "We use WSJ Section 22 as our development set and section 23 as the test set, where we obtain an F1-score of 89.6, competitive with the previously-published 90.5.", "labels": [], "entities": [{"text": "WSJ Section 22", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9358557860056559}, {"text": "F1-score", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9996045231819153}]}, {"text": "For all experiments 2 , we use a two-layer encoder-decoder with long short-term memory (LSTM) units  For auto-encoders and translation models, we train 8 epochs.", "labels": [], "entities": []}, {"text": "The learning rate is initially set as 0.35 and starts to halve after 6 epochs.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8950560092926025}]}, {"text": "For E2P model, we train 15 epochs.", "labels": [], "entities": []}, {"text": "The learning rate is initialized as 0.35 and starts to decay by 0.7 once the perplexity on a development set starts to increase.", "labels": [], "entities": []}, {"text": "All parameters are re-scaled when the global norm is larger than 5.", "labels": [], "entities": []}, {"text": "All models are non-attentional, because we want the encoding vector to summarize the whole source sentence.", "labels": [], "entities": []}, {"text": "shows the settings of each model and reports the BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.998902440071106}]}, {"text": "We now turn to whether NMT systems capture deeper syntactic structure as a by-product of learning to translate from English to another language.", "labels": [], "entities": []}, {"text": "We do this by predicting full parse trees from the information stored in encoding vectors.", "labels": [], "entities": []}, {"text": "Since this is a structured prediction problem, we can no longer use logistic regression.", "labels": [], "entities": []}, {"text": "Instead, we extract a constituency parse tree from the encoding vector of a model E2X by using anew neural parser E2X2P with the following steps: 1.", "labels": [], "entities": [{"text": "constituency parse", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6929450035095215}]}, {"text": "Take the E2X encoder as the encoder of the new model E2X2P.", "labels": [], "entities": [{"text": "E2X encoder", "start_pos": 9, "end_pos": 20, "type": "DATASET", "confidence": 0.9491600096225739}]}, {"text": "2. Initialize the E2X2P decoder parameters with a uniform distribution.", "labels": [], "entities": []}, {"text": "3. Fine-tune the E2X2P decoder (while keeping its encoder parameters fixed), using the CJ corpus, the same corpus used to train E2P . shows how we construct model E2F2P from model E2F.", "labels": [], "entities": [{"text": "CJ corpus", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.9212926030158997}]}, {"text": "For fine-tuning, we use the same dropout rate and learning rate updating configuration for E2P as described in Section 4.", "labels": [], "entities": []}, {"text": "We train four new neural parsers using the encoders of the two auto-encoders and the two NMT models respectively.", "labels": [], "entities": []}, {"text": "We use three tools to evaluate and analyze: 1.", "labels": [], "entities": []}, {"text": "The EVALB tool 3 to calculate the labeled bracketing F1-score.", "labels": [], "entities": [{"text": "EVALB", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.5884289741516113}, {"text": "F1-score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8910862803459167}]}, {"text": "2. The zxx package 4 to calculate Tree edit distance (TED) ().", "labels": [], "entities": [{"text": "Tree edit distance (TED)", "start_pos": 34, "end_pos": 58, "type": "METRIC", "confidence": 0.6902331237991651}]}, {"text": "3. The Berkeley Parser Analyser) to analyze parsing error types.", "labels": [], "entities": [{"text": "parsing error types", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.871701697508494}]}, {"text": "The linearized parse trees generated by these neural parsers are not always well-formed.", "labels": [], "entities": []}, {"text": "They can be split into the following categories: \u2022 Malformed trees: The linearized sequence cannot be converted back into a tree, due to missing or mismatched brackets.", "labels": [], "entities": []}, {"text": "\u2022 Well-formed trees: The sequence can be converted back into a tree.", "labels": [], "entities": []}, {"text": "Tree edit distance can be calculated on this category.", "labels": [], "entities": []}, {"text": "-Wrong length trees: The number of tree leaves does not match the number of source-sentence tokens.", "labels": [], "entities": []}, {"text": "-Correct length trees: The number of tree leaves does match the number of sourcesentence tokens.", "labels": [], "entities": [{"text": "Correct length trees", "start_pos": 1, "end_pos": 21, "type": "METRIC", "confidence": 0.9169129133224487}]}, {"text": "Before we move to results, we emphasize the following points: First, compared to the linear classifier used in Section 5, the retrained decoder for predicting a linearized parse tree is a highly non-linear method.", "labels": [], "entities": [{"text": "predicting a linearized parse tree", "start_pos": 148, "end_pos": 182, "type": "TASK", "confidence": 0.8015470147132874}]}, {"text": "The syntactic prediction/parsing performance will increase due to such non-linearity.", "labels": [], "entities": [{"text": "syntactic prediction/parsing", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.781345397233963}]}, {"text": "Thus, we do not make conclusions based only on absolute performance values, but also on a comparison against the designed baseline models.", "labels": [], "entities": []}, {"text": "An improvement over the lower bound models indicates that the encoder learns syntactic information, whereas a decline from the upper bound model shows that the encoder loses certain syntactic information.", "labels": [], "entities": []}, {"text": "Second, the NMT's encoder maps a plain English sentence into a high-dimensional vector, and our goal is to test whether the projected vectors form a more syntactically-related manifold in the highdimensional space.", "labels": [], "entities": []}, {"text": "In practice, one could also predict parse structure for the E2E in two steps: (1) use E2E's decoder to recover the original English sentence, and (2) parse that sentence with the CJ parser.", "labels": [], "entities": [{"text": "parse structure", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.8929045498371124}, {"text": "E2E", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9295686483383179}]}, {"text": "But in this way, the manifold structure in the highdimensional space is destroyed during the mapping.", "labels": [], "entities": []}, {"text": "reports perplexity on training and development sets, the labeled F1-score on WSJ Section 23, and the Tree Edit Distance (TED) of various systems.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.98381108045578}, {"text": "WSJ Section 23", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9600329399108887}, {"text": "Tree Edit Distance (TED)", "start_pos": 101, "end_pos": 125, "type": "METRIC", "confidence": 0.7833742300669352}]}], "tableCaptions": [{"text": " Table 1: Voice (active/passive) prediction accuracy using the", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9481326341629028}]}, {"text": " Table 2: Model settings and test-set BLEU-n4r1 scores (", "labels": [], "entities": [{"text": "BLEU-n4r1", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9963569045066833}]}, {"text": " Table 4: Corpus statistics for five syntactic labels.", "labels": [], "entities": []}, {"text": " Table 5: Perplexity, labeled F1-score, and Tree Edit Distance (TED) of various systems. Labeled F1-scores are calculated on", "labels": [], "entities": [{"text": "F1-score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9716379642486572}, {"text": "Tree Edit Distance (TED)", "start_pos": 44, "end_pos": 68, "type": "METRIC", "confidence": 0.7895762374003729}]}, {"text": " Table 6: Labeled F1-scores and POS tagging accuracy on the", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8658909797668457}, {"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7792280912399292}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9586999416351318}]}]}