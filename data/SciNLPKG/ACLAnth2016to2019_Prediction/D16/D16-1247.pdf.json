{"title": [{"text": "Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation", "labels": [], "entities": [{"text": "Insertion Position Selection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7725310325622559}, {"text": "Dependency Tree-to-Tree Machine Translation", "start_pos": 65, "end_pos": 108, "type": "TASK", "confidence": 0.5790137723088264}]}], "abstractContent": [{"text": "Dependency tree-to-tree translation models are powerful because they can naturally handle long range reorderings which is important for distant language pairs.", "labels": [], "entities": [{"text": "Dependency tree-to-tree translation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6625287334124247}]}, {"text": "The translation process is easy if it can be accomplished only by replacing non-terminals in translation rules with other rules.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9835159182548523}]}, {"text": "However it is sometimes necessary to adjoin translation rules.", "labels": [], "entities": []}, {"text": "Flexible non-terminals have been proposed as a promising solution for this problem.", "labels": [], "entities": []}, {"text": "A flexible non-terminal provides several insertion position candidates for the rules to be adjoined , but it increases the computational cost of decoding.", "labels": [], "entities": []}, {"text": "In this paper we propose a neu-ral network based insertion position selection model to reduce the computational cost by selecting the appropriate insertion positions.", "labels": [], "entities": [{"text": "insertion position selection", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6649169921875}]}, {"text": "The experimental results show the proposed model can select the appropriate insertion position with a high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9965657591819763}]}, {"text": "It reduces the decoding time and improves the translation quality owing to reduced search space.", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9603778123855591}]}], "introductionContent": [{"text": "Tree-to-tree machine translation models currently receive limited attention.", "labels": [], "entities": [{"text": "Tree-to-tree machine translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5891535381476084}]}, {"text": "However, we believe that using target-side syntax is important to achieve highquality translations between distant language pairs which require long range reorderings.", "labels": [], "entities": []}, {"text": "Especially, using dependency trees on both source and target sides is promising for this purpose).", "labels": [], "entities": []}, {"text": "Tree-based translation models naturally realize word reorderings using the non-terminals or anchors for the attachment in the translation rules: therefore they do not need a reordering model which string-based models require.", "labels": [], "entities": []}, {"text": "For example, suppose we have a translation rule with the word alignment shown in, it is easy to translate anew input sentence which has \" (library)\" instead of \" (park)\" because we can accomplish it by simply substituting \"library\" for the target word \"park\" without considering the reordering.", "labels": [], "entities": []}, {"text": "In this case, the source word \"\" and target word \"park\" work as the non-terminals.", "labels": [], "entities": []}, {"text": "On the other hand, it is problematic when we need to adjoin a subtree which is not presented in training sentences, which we call floating subtree in this paper.", "labels": [], "entities": []}, {"text": "The floating subtrees are not necessarily adjuncts, but any words or phrases.", "labels": [], "entities": []}, {"text": "For example, suppose the Japanese input sentence in has \" (suddenly)\", but the training corpus provides only a translation rule without the word.", "labels": [], "entities": []}, {"text": "In this case we cannot directly use the rule for the translation because it does not know whereto insert the translation of the floating word in the output.", "labels": [], "entities": []}, {"text": "As another example, there is no context information available for the children of the OOV word in the input sentence, so we need some special process to translate them.", "labels": [], "entities": []}, {"text": "Previous work deals with this problem by using glue rules) or limiting the dependency structures to be well-formed.", "labels": [], "entities": []}, {"text": "introduces the concept of flexible non-terminals.", "labels": [], "entities": []}, {"text": "It provides multiple possible insertion positions for the floating subtree rather than fixed insertion positions.", "labels": [], "entities": []}, {"text": "A possible insertion position must satisfy the following conditions: \u2022 it must be a child of the aligned word of the parent of the floating subtree \u2022 it must not violate the projectivity of the dependency tree For example, possible insertion positions for the floating word \"\" are shown in gray arrows in.", "labels": [], "entities": []}, {"text": "Since \"\" is a child of \"\", and the translation of \"\" is \"called\", insertion positions must be a child of \"called\".", "labels": [], "entities": []}, {"text": "Also, insertion positions do not violate the projectivity of the target tree.", "labels": [], "entities": []}, {"text": "Flexible non-terminals are analogous to the auxiliary tree of the tree adjoining grammars (TAG), which is successfully adopted in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 130, "end_pos": 149, "type": "TASK", "confidence": 0.7411676943302155}]}, {"text": "The difference is that TAG is defined on the constituency trees rather than the dependency trees.", "labels": [], "entities": [{"text": "TAG", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.8419466018676758}]}, {"text": "Flexible non-terminals are powerful to handle floating subtrees and it achieve better translation quality.", "labels": [], "entities": []}, {"text": "However the computational cost of decoding becomes high even though they are compactly represented in the lattice form).", "labels": [], "entities": []}, {"text": "In our experiments, using flexible nonterminals causes the decoding to be 3 to 6 times slower than when they are not used.", "labels": [], "entities": []}, {"text": "Flexible nonterminals increase the number of translation rules because the insertion positions are selected during the decoding.", "labels": [], "entities": []}, {"text": "However, we think it is possible to restrict possible insertion positions or even select only one insertion position by looking at the tree structures on both sides.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to select the appropriate insertion position before decoding.", "labels": [], "entities": []}, {"text": "This cannot only reduce the decoding time but also improve the translation quality because of reduced search space.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted two kinds of experiments: the insertion position selection and translation.", "labels": [], "entities": [{"text": "insertion position selection", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.8064172069231669}, {"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9519315361976624}]}, {"text": "We used ASPEC (  as the dataset and the numbers of the sentences of the corpus are shown in.", "labels": [], "entities": [{"text": "ASPEC", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.6986356377601624}]}, {"text": "The Japanese morphological analyzer (  and dependency parser (  are used for Japanese Ja \u2192 En En \u2192 Ja Ja \u2192 Zh Zh \u2192 Ja  sentences.", "labels": [], "entities": [{"text": "Japanese Ja \u2192 En En \u2192 Ja Ja \u2192 Zh Zh \u2192 Ja  sentences", "start_pos": 77, "end_pos": 128, "type": "TASK", "confidence": 0.6704277587788445}]}, {"text": "English sentences are first parsed by nlparser) and then converted into word dependency trees using Collins' head percolation table.", "labels": [], "entities": []}, {"text": "We used Chinese word segmenter) and dependency parser SKP () for Chinese sentences.", "labels": [], "entities": [{"text": "dependency parser SKP", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.742421418428421}]}, {"text": "The supervised word alignment Nile () was used.", "labels": [], "entities": []}, {"text": "We used a state-of-the-art dependency tree-to-tree decoder () with the default settings.", "labels": [], "entities": []}, {"text": "The neural network is constructed and trained using the Chainer ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The statistics of the data and results of the insertion", "labels": [], "entities": [{"text": "insertion", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.7861855626106262}]}, {"text": " Table 3: Detailed Ja \u2192 En insertion position selection experimental result.", "labels": [], "entities": [{"text": "Detailed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8305100798606873}, {"text": "Ja \u2192 En insertion position selection", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.5236874520778656}]}, {"text": " Table 4: The results of the translation experiments.  \u2020 means the Proposed method achieved significantly better score than the", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9757148623466492}]}]}