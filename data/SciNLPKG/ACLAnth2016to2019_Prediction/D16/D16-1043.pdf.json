{"title": [{"text": "Comparing Data Sources and Architectures for Deep Visual Representation Learning in Semantics", "labels": [], "entities": []}], "abstractContent": [{"text": "Multi-modal distributional models learn grounded representations for improved performance in semantics.", "labels": [], "entities": []}, {"text": "Deep visual representations, learned using convolutional neural networks, have been shown to achieve particularly high performance.", "labels": [], "entities": []}, {"text": "In this study, we systematically compare deep visual representation learning techniques, experimenting with three well-known network architectures.", "labels": [], "entities": []}, {"text": "In addition, we explore the various data sources that can be used for retrieving relevant images, showing that images from search engines perform as well as, or better than, those from manually crafted resources such as ImageNet.", "labels": [], "entities": []}, {"text": "Furthermore, we explore the optimal number of images and the multilingual applicability of multi-modal semantics.", "labels": [], "entities": []}, {"text": "We hope that these findings can serve as a guide for future research in the field.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-modal distributional semantics addresses the fact that text-based semantic models, which represent word meanings as a distribution over other words, suffer from the grounding problem.", "labels": [], "entities": []}, {"text": "Recent work has shown that this theoretical motivation can be successfully exploited for practical gain.", "labels": [], "entities": []}, {"text": "Indeed, multi-modal representation learning leads to improvements over language-only models in a range of tasks, including modelling semantic similarity and relatedness, improving lexical entailment (), predicting compositionality, bilingual lexicon induction), selectional preference prediction (, linguistic ambiguity resolution (, visual information retrieval () and metaphor identification ().", "labels": [], "entities": [{"text": "predicting compositionality", "start_pos": 203, "end_pos": 230, "type": "TASK", "confidence": 0.9347163140773773}, {"text": "bilingual lexicon induction", "start_pos": 232, "end_pos": 259, "type": "TASK", "confidence": 0.6052049398422241}, {"text": "selectional preference prediction", "start_pos": 262, "end_pos": 295, "type": "TASK", "confidence": 0.7230491240819296}, {"text": "linguistic ambiguity resolution", "start_pos": 299, "end_pos": 330, "type": "TASK", "confidence": 0.6556427776813507}, {"text": "visual information retrieval", "start_pos": 334, "end_pos": 362, "type": "TASK", "confidence": 0.6750075817108154}, {"text": "metaphor identification", "start_pos": 370, "end_pos": 393, "type": "TASK", "confidence": 0.8673414587974548}]}, {"text": "Most multi-modal semantic models tend to rely on raw images as the source of perceptual input.", "labels": [], "entities": []}, {"text": "Many data sources have been tried, ranging from image search engines to photo sharing websites to manually crafted resources.", "labels": [], "entities": []}, {"text": "Images are retrieved fora given target word if they are ranked highly, have been tagged, or are otherwise associated with the target word(s) in the data source.", "labels": [], "entities": []}, {"text": "Traditionally, representations for images were learned through bag-of-visual words, using SIFT-based local feature descriptors.", "labels": [], "entities": []}, {"text": "showed that transferring representations from deep convolutional neural networks (ConvNets) yield much better performance than bag-of-visual-words in multi-modal semantics.", "labels": [], "entities": []}, {"text": "ConvNets () have become very popular in recent years: they are now the dominant approach for almost all recognition and detection tasks in the computer vision community (), approaching or even exceeding human performance in some cases.", "labels": [], "entities": [{"text": "recognition and detection tasks", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.8928103297948837}]}, {"text": "The work by Alex , which won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) () in 2012, has played an important role in bringing convolutional networks (back) to prominence.", "labels": [], "entities": [{"text": "ImageNet Large Scale Visual Recognition Challenge (ILSVRC)", "start_pos": 33, "end_pos": 91, "type": "TASK", "confidence": 0.673044890165329}]}, {"text": "A similar network was used by to obtain high quality image embeddings for semantics.", "labels": [], "entities": []}, {"text": "This work aims to provide a systematic comparison of such deep visual representation learning techniques and data sources; i.e. we aim to answer the following open questions in multi-modal semantics: \u2022 Does the improved performance over bagof-visual-words extend to different convolutional network architectures, or is it specific to Krizhevsky's AlexNet?", "labels": [], "entities": []}, {"text": "Do others work even better?", "labels": [], "entities": []}, {"text": "\u2022 How important is the source of images?", "labels": [], "entities": []}, {"text": "Is there a difference between search engines and manually annotated data sources?", "labels": [], "entities": []}, {"text": "Does the number of images obtained for each word matter?", "labels": [], "entities": []}, {"text": "\u2022 Do these findings extend to different languages beyond English?", "labels": [], "entities": []}, {"text": "We evaluate semantic representation quality through examining how well a system's similarity scores correlate with human similarity and relatedness judgments.", "labels": [], "entities": []}, {"text": "We examine both the visual representations themselves as well as the multi-modal representations that fuse visual representations with linguistic input, in this case using middle fusion (i.e., concatenation).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this work is the first to systematically compare these aspects of visual representation learning.", "labels": [], "entities": [{"text": "visual representation learning", "start_pos": 96, "end_pos": 126, "type": "TASK", "confidence": 0.7080399592717489}]}], "datasetContent": [{"text": "Representation quality in semantics is usually evaluated using intrinsic datasets of human similarity and relatedness judgments.", "labels": [], "entities": []}, {"text": "Model performance is assessed through the Spearman \u03c1 s rank correlation between the system's similarity scores fora given pair of words, together with human judgments.", "labels": [], "entities": [{"text": "Spearman \u03c1 s rank correlation", "start_pos": 42, "end_pos": 71, "type": "METRIC", "confidence": 0.7164320647716522}]}, {"text": "Here, we evaluate on two well-known similarity and relatedness judgment datasets: MEN (Bruni et al., 2012) and SimLex-999 (.", "labels": [], "entities": [{"text": "MEN", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.5043988823890686}, {"text": "SimLex-999", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.8496743440628052}]}, {"text": "MEN focuses explicitly on relatedness (i.e. coffee-tea and coffee-mug get high scores, while bakery-zebra gets a low score), while SimLex-999 focuses on what it calls \"genuine\" similarity (i.e., coffee-tea gets a high score, while both coffee-mug and bakery-zebra get low scores).", "labels": [], "entities": [{"text": "MEN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9336246252059937}]}, {"text": "They are standard evaluations for evaluating representational quality in semantics.", "labels": [], "entities": []}, {"text": "In each experiment, we examine performance of the visual representations compared to text-based representations, as well as performance of the multimodal representation that fuses the two.", "labels": [], "entities": []}, {"text": "In this  case, we apply mid-level fusion, concatenating the L2-normalized representations ().", "labels": [], "entities": []}, {"text": "Middle fusion is a popular technique in multi-modal semantics that has several benefits: 1) it allows for drawing from different data sources for each modality, that is, it does not require joint data; 2) concatenation is less susceptible to noise, since it preserves the information in the individual modalities; and 3) it is straightforward to apply and computationally inexpensive.", "labels": [], "entities": [{"text": "Middle fusion", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7402115315198898}]}, {"text": "Linguistic representations are 300-dimensional and are obtained by applying skipgram with negative sampling () to a recent dump of Wikipedia.", "labels": [], "entities": [{"text": "skipgram", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.8434628844261169}]}, {"text": "The normalization step that is performed before applying fusion ensures that both modalities contribute equally to the overall multi-modal representation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Network architectures. Layer counts only include layers with parameters.", "labels": [], "entities": []}, {"text": " Table 2: Sources of image data.", "labels": [], "entities": []}, {"text": " Table 4: Performance on maximally covered datasets.", "labels": [], "entities": []}, {"text": " Table 5: Performance on common coverage subsets of the datasets (MEN* and SimLex*).", "labels": [], "entities": []}, {"text": " Table 6: Performance on English and Italian Sim- Lex, either in the multi-lingual setting (M) or the  cross-lingual settting (C) where we first map to En- glish.", "labels": [], "entities": [{"text": "English and Italian Sim- Lex", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.6887044459581375}]}]}