{"title": [{"text": "Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets", "labels": [], "entities": [{"text": "Lifelong Relaxation Labeling", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.7058681150277456}]}], "abstractContent": [{"text": "It is well-known that opinions have targets.", "labels": [], "entities": []}, {"text": "Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use.", "labels": [], "entities": [{"text": "Extracting", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.96112060546875}, {"text": "opinion mining", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.8163142204284668}]}, {"text": "So far many algorithms have been proposed to extract opinion targets.", "labels": [], "entities": []}, {"text": "However, an opinion target can bean entity or an aspect (part or attribute) of an entity.", "labels": [], "entities": []}, {"text": "An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity.", "labels": [], "entities": []}, {"text": "Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions.", "labels": [], "entities": []}, {"text": "This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling.", "labels": [], "entities": []}, {"text": "Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly.", "labels": [], "entities": []}], "introductionContent": [{"text": "A core problem of opinion mining or sentiment analysis is to identify each opinion/sentiment target and to classify the opinion/sentiment polarity on the target (.", "labels": [], "entities": [{"text": "opinion mining or sentiment analysis", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7411070108413697}]}, {"text": "For example, in a review sentence fora car, one wrote \"Although the engine is slightly weak, this car is great.\"", "labels": [], "entities": []}, {"text": "The person is positive (opinion polarity) about the car (opinion target) as a whole, but slightly negative (opinion polarity) about the car's engine (opinion target).", "labels": [], "entities": []}, {"text": "Past research has proposed many techniques to extract opinion targets (we will just call them targets hereafter for simplicity) and also to classify sentiment polarities on the targets.", "labels": [], "entities": []}, {"text": "However, a target can bean entity or an aspect (part or attribute) of an entity.", "labels": [], "entities": []}, {"text": "\"Engine\" in the above sentence is just one aspect of the car, while \"this car\" refers to the whole car.", "labels": [], "entities": []}, {"text": "Note that in, an entity is called a general aspect.", "labels": [], "entities": []}, {"text": "For effective opinion mining, we need to classify whether a target is an entity or an aspect because they refer to very different things.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.8227359354496002}]}, {"text": "One can be positive about the whole entity (car) but negative about some aspects of it (e.g., engine) and vice versa.", "labels": [], "entities": []}, {"text": "This paper aims to perform the target classification task, which, to our knowledge, has not been attempted before.", "labels": [], "entities": [{"text": "target classification task", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.8420209685961405}]}, {"text": "Although in supervised extraction one can annotate entities and aspects with separate labels in the training data to build a model to extract them separately, in this paper our goal is to help unsupervised target extraction methods to classify targets.", "labels": [], "entities": [{"text": "supervised extraction", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7271206378936768}]}, {"text": "Unsupervised target extraction methods are often preferred because they save the time-consuming data labeling or annotation step for each domain.", "labels": [], "entities": []}, {"text": "Problem Statement: Given a set of opinion targets T = {t 1 , . .", "labels": [], "entities": []}, {"text": ", tn } extracted from an opinion corpus d, we want to classify each target ti \u2208 T into one of the three classes, entity, aspect, or NIL, which are called class labels.", "labels": [], "entities": []}, {"text": "NIL means that the target is neither an entity nor an aspect and is used because target extraction algorithms can make mistakes.", "labels": [], "entities": []}, {"text": "This paper does not propose anew target extraction algorithm.", "labels": [], "entities": [{"text": "target extraction", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7354543209075928}]}, {"text": "We use an existing unsupervised method, called Double Propagation (DP) (), for extraction.", "labels": [], "entities": [{"text": "Double Propagation (DP)", "start_pos": 47, "end_pos": 70, "type": "METRIC", "confidence": 0.8696587920188904}]}, {"text": "We only focus on target classification after the targets have been extracted.", "labels": [], "entities": [{"text": "target classification", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.8178119361400604}]}, {"text": "Note that an entity here can be a named entity, a prod-uct category, or an abstract product (e.g., \"this machine\" and \"this product\").", "labels": [], "entities": []}, {"text": "An named entity can be the name of a brand, a model, or a manufacturer.", "labels": [], "entities": []}, {"text": "An aspect is apart or attribute of an entity, e.g., \"battery\" and \"price\" of the entity \"camera\".", "labels": [], "entities": []}, {"text": "Since our entities not just include the traditional named entities (e.g., \"Microsoft\" and \"Google\") but also other expressions that refer to such entities, traditional named entity recognition algorithms are not sufficient.", "labels": [], "entities": []}, {"text": "Pronouns such as \"it,\" \"they,\" etc., are not considered in this paper as co-reference resolution is out of the scope of this work.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7160247266292572}]}, {"text": "We solve this problem in an unsupervised manner so that there is no need for labor-intensive manual labeling of the training data.", "labels": [], "entities": []}, {"text": "One key observation of the problem is that although entities and aspects are different, they are closely related because aspects are parts or attributes of entities and they often have syntactic relationships in a sentence, e.g., \"This phone's screen is super.\"", "labels": [], "entities": []}, {"text": "Thus it is natural to solve the problem using a relational learning method.", "labels": [], "entities": []}, {"text": "We employ the graph labeling algorithm, Relaxation Labeling (RL), which performs unsupervised belief propagation on a graph.", "labels": [], "entities": [{"text": "Relaxation Labeling (RL)", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6428766787052155}]}, {"text": "In our case, each target extracted from the given corpus d forms a graph node and each relation identified ind between two targets forms an edge.", "labels": [], "entities": []}, {"text": "With some initial probability assignments, RL can assign each target node the most probable class label.", "labels": [], "entities": []}, {"text": "Although some other graph labeling methods can be applied as well, the key issue here is that just using a propagation method in isolation is far from sufficient due to lack of information from the given corpus, which we detail in Section 5.", "labels": [], "entities": [{"text": "graph labeling", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.689133882522583}]}, {"text": "We then employ Lifelong Machine Learning (LML)) to make a major improvement.", "labels": [], "entities": []}, {"text": "LML works as follows: The learner has performed a number learning tasks in the past and has retained the knowledge gained so far.", "labels": [], "entities": []}, {"text": "In the new/current task, it makes use of the past knowledge to help current learning and problem solving.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.8707735538482666}]}, {"text": "Since RL is unsupervised, we can assume that the system has performed the same task on reviews of a large number of products/domains (or corpora).", "labels": [], "entities": [{"text": "RL", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.8526945114135742}]}, {"text": "It has also saved all the graphs and classification results from those past domains in a Knowledge Base (KB).", "labels": [], "entities": []}, {"text": "It then exploits this past knowledge to help classification in the current task/domain.", "labels": [], "entities": [{"text": "classification", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.9679234027862549}]}, {"text": "We call this combined approach of relaxation labeling and LML Lifelong-RL.", "labels": [], "entities": [{"text": "relaxation labeling", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.8627736568450928}]}, {"text": "The approach is effective because there is a significant amount of sharing of targets and target relations across domains.", "labels": [], "entities": []}, {"text": "LML is different from the classic learning paradigm (supervised or unsupervised) because classic learning has no memory.", "labels": [], "entities": []}, {"text": "It basically runs a learning algorithm on a given data in isolation without considering any past learned knowledge.", "labels": [], "entities": []}, {"text": "LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning.", "labels": [], "entities": [{"text": "LML", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7926344871520996}]}, {"text": "Our experimental results show that the proposed Lifelong-RL system is highly promising.", "labels": [], "entities": []}, {"text": "The paradigm of LML helps improve the classification results greatly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate the proposed method and compare with baselines.", "labels": [], "entities": []}, {"text": "We use the DP method for target extraction ().", "labels": [], "entities": [{"text": "target extraction", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7943746149539948}]}, {"text": "This method uses dependency relations between opinion words and targets to extract targets using seed opinion words.", "labels": [], "entities": []}, {"text": "Since our paper does not focus on extraction, interested readers can refer to (Qiu et al.,) for details.", "labels": [], "entities": []}, {"text": "Evaluation Datasets: We use two sets of datasets.", "labels": [], "entities": [{"text": "Evaluation Datasets", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.6160181313753128}]}, {"text": "The first set consists of eight annotated review datasets.", "labels": [], "entities": []}, {"text": "We use each of them as the new domain data in LML to compute precision, recall, F1 scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.998798131942749}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9991211295127869}, {"text": "F1 scores", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9790486097335815}]}, {"text": "Five of them are from (, and the remaining three are from ().", "labels": [], "entities": []}, {"text": "They have been used for target extraction, and thus have annotated targets, but no annotation on whether a: Annotation details of the benchmark datasets.", "labels": [], "entities": [{"text": "target extraction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8295252919197083}, {"text": "Annotation", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9849360585212708}]}, {"text": "target is an entity or aspect.", "labels": [], "entities": []}, {"text": "We made this annotation, which is straightforward.", "labels": [], "entities": []}, {"text": "We used two annotators to annotate the datasets.", "labels": [], "entities": []}, {"text": "The Cohen's kappa is 0.84.", "labels": [], "entities": []}, {"text": "Through discussion, the annotators got complete agreement.", "labels": [], "entities": []}, {"text": "Details of the datasets are listed in.", "labels": [], "entities": []}, {"text": "Each cell is the number of distinct terms.", "labels": [], "entities": []}, {"text": "These datasets are not very large but they are realistic because many products do not have a large number of reviews.", "labels": [], "entities": []}, {"text": "The second set consists of unlabeled review datasets from 100 diverse products or domains.", "labels": [], "entities": []}, {"text": "Each domain has 1000 reviews.", "labels": [], "entities": []}, {"text": "They are treated as past domain data in LML since they are not annotated and thus cannot be used for computing evaluation measures.", "labels": [], "entities": []}, {"text": "Evaluating Measures: We mainly use precision P, recall R, and F 1 -score F 1 as evaluation measures.", "labels": [], "entities": [{"text": "Evaluating Measures", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7040499448776245}, {"text": "precision P", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9343921840190887}, {"text": "recall R", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9741207659244537}, {"text": "F 1 -score F 1", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.956016739209493}]}, {"text": "We take multiple occurrences of the same target as one count, and only evaluate entities and aspects.", "labels": [], "entities": []}, {"text": "We will also give the accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9997218251228333}]}, {"text": "Compared Methods: We compare the following methods, including our proposed method, Lifelong-RL.", "labels": [], "entities": []}, {"text": "NER+TM: NER is Named Entity Recognition.", "labels": [], "entities": [{"text": "TM", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.47806861996650696}, {"text": "Named Entity Recognition", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6412399808565775}]}, {"text": "We can regard the extracted terms from a NER system as entities and the rest of the targets as aspects.", "labels": [], "entities": []}, {"text": "However, a NER system cannot identify entities such as \"this car\" from \"this car is great.\"", "labels": [], "entities": []}, {"text": "Its result is rather poor.", "labels": [], "entities": []}, {"text": "But our type modifier (TM) does that, i.e., if an opinion target appears after \"this\" or \"these\" in at least two sentences, TM labels the target as an entity; otherwise an aspect.", "labels": [], "entities": []}, {"text": "However, TM cannot extract named entities.", "labels": [], "entities": [{"text": "TM", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9652876257896423}]}, {"text": "Its result is also rather poor.", "labels": [], "entities": []}, {"text": "We thus combine the two methods to give NER+TM as they complement each other very well.", "labels": [], "entities": [{"text": "NER+TM", "start_pos": 40, "end_pos": 46, "type": "TASK", "confidence": 0.44826336701711017}]}, {"text": "To make NER more powerful, we use two NER systems: Stanford-NER 1 () and UIUC-NER 2.", "labels": [], "entities": [{"text": "NER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9071594476699829}]}, {"text": "NER+TM treats the extracted entities by the three systems as entities and the rest of the targets as aspects.", "labels": [], "entities": [{"text": "NER+TM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8106110692024231}]}, {"text": "NER+TM+DICT: We run NER+TM on the 100 datasets for LML to get a list of entities, which we call the dictionary (DICT).", "labels": [], "entities": []}, {"text": "For anew task, if any target word is in the list, it is treated as an entity; otherwise an aspect.", "labels": [], "entities": []}, {"text": "RL: This is the base method described in Section 3.", "labels": [], "entities": [{"text": "RL", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8484389781951904}]}, {"text": "It performs relaxation labeling (RL) without the help of LML.", "labels": [], "entities": [{"text": "relaxation labeling (RL)", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.8761175990104675}]}, {"text": "Lifelong-RL-1: This performs LML with RL but the current task only uses the relations in the KB from previous tasks (Section 5.1).", "labels": [], "entities": []}, {"text": "Lifelong-RL: This is our proposed final method.", "labels": [], "entities": []}, {"text": "It improves Lifelong-RL-1 by further incorporating target labels in the KB from previous tasks (Section 5.2).", "labels": [], "entities": [{"text": "Lifelong-RL-1", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.798109233379364}]}, {"text": "Parameter Settings: RL has 2 initial label distributions Pm E and Pm A and 3 conditional label distributions P mc , Pm E|A and Pm A|E . Like other belief propagation algorithms, these probabilities need to beset empirically, as shown in.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7159677445888519}]}, {"text": "The parameter \u03b1 is set to 1.", "labels": [], "entities": []}, {"text": "Our LML method has one parameter \u03bb for Lifelong-RL.", "labels": [], "entities": []}, {"text": "We set it to 0.1.: Comparative results on Entity and Aspect in precision, recall and F 1 score: NER+TM+DICT's results are very poor and not included (see Section 6.2) for the average results.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.999313473701477}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9991771578788757}, {"text": "F 1 score", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9882951180140177}, {"text": "NER+TM+DICT", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.5253836572170257}]}], "tableCaptions": [{"text": " Table 1: Annotation details of the benchmark datasets.", "labels": [], "entities": []}, {"text": " Table 3: Comparative results on Entity and Aspect in precision, recall and F 1 score: NER+TM+DICT's  results are very poor and not included (see Section 6.2) for the average results.", "labels": [], "entities": [{"text": "Entity", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9191603064537048}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9994750618934631}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.999422550201416}, {"text": "F 1 score", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9890229304631551}, {"text": "NER+TM+DICT", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.6529149770736694}]}, {"text": " Table 1. When an entity term is  wrongly classified as an aspect, it has much less im- pact on the aspect result than on the entity result.", "labels": [], "entities": [{"text": "im- pact", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.8232122461001078}]}]}