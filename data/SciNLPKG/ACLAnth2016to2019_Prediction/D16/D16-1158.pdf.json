{"title": [{"text": "Length bias in Encoder Decoder Models and a Case for Global Conditioning", "labels": [], "entities": [{"text": "Length bias", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8587580323219299}, {"text": "Global Conditioning", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7567475736141205}]}], "abstractContent": [{"text": "Encoder-decoder networks are popular for modeling sequences probabilistically in many applications.", "labels": [], "entities": []}, {"text": "These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables.", "labels": [], "entities": []}, {"text": "However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size.", "labels": [], "entities": []}, {"text": "In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model.", "labels": [], "entities": []}, {"text": "The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences.", "labels": [], "entities": [{"text": "predicting short sequences", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.8570715188980103}]}, {"text": "For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models.", "labels": [], "entities": []}, {"text": "From a practical point of view, our proposed model also eliminates the need fora beam-search during inference, which reduces to an efficient dot-product based search in a vector-space.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we investigate the use of neural networks for modeling the conditional distribution Pr(y|x) over sequences y of discrete tokens in response to a complex input x, which can be another * Work done while visiting Google Research on a leave from IIT sequence or an image.", "labels": [], "entities": []}, {"text": "Such models have applications in machine translation (), image captioning (, response generation in emails (, and conversations).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8033961951732635}, {"text": "image captioning", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7640417814254761}, {"text": "response generation in emails", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.799343042075634}]}, {"text": "The most popular neural network for probabilistic modeling of sequences in the above applications is the encoder-decoder (ED) network.", "labels": [], "entities": []}, {"text": "A ED network first encodes an input x into a vector which is then used to initialize a recurrent neural network (RNN) for decoding the output y.", "labels": [], "entities": []}, {"text": "The decoder RNN factorizes Pr(y|x) using the chain rule as j Pr(y j |y 1 , . .", "labels": [], "entities": [{"text": "Pr", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.8168289065361023}]}, {"text": ", y j\u22121 , x) where y 1 , . .", "labels": [], "entities": []}, {"text": ", y n denote the tokens in y.", "labels": [], "entities": []}, {"text": "This factorization does not entail any conditional independence assumption among the {y j } variables.", "labels": [], "entities": []}, {"text": "This is unlike earlier sequence models like CRFs () and MeMMs () that typically assume that a token is independent of all other tokens given its adjacent tokens.", "labels": [], "entities": [{"text": "MeMMs", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.8710231781005859}]}, {"text": "Modern-day RNNs like LSTMs promise to capture non-adjacent and long-term dependencies by summarizing the set of previous tokens in a continuous, high-dimensional state vector.", "labels": [], "entities": []}, {"text": "Within the limits of parameter capacity allocated to the model, the ED, by virtue of exactly factorizing the token sequence, is consistent.", "labels": [], "entities": [{"text": "ED", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.8877701759338379}]}, {"text": "However, when we created and deployed an ED model fora chat suggestion task we observed several counter-intuitive patterns in its predicted outputs.", "labels": [], "entities": []}, {"text": "Even after training the model over billions of examples, the predictions were systematically biased towards short sequences.", "labels": [], "entities": []}, {"text": "Such bias has also been seen in translation ( ).", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9847254157066345}]}, {"text": "Another curious phenomenon was that the accuracy of the predictions sometimes dropped with increasing beam-size, more than could be explained by statistical variations of a well-calibrated model (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994707703590393}]}, {"text": "In this paper we expose a margin discrepancy in the training loss of encoder-decoder models to explain the above problems in its predictions.", "labels": [], "entities": []}, {"text": "We show that the training loss of ED network often underestimates the margin of separating a correct sequence from an incorrect shorter sequence.", "labels": [], "entities": [{"text": "margin", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9831215143203735}]}, {"text": "The discrepancy gets more severe as the length of the correct sequence increases.", "labels": [], "entities": []}, {"text": "That is, even after the training loss converges to a small value, full inference on the training data can incur errors causing the model to be underfitted for long sequences in spite of low training cost.", "labels": [], "entities": []}, {"text": "We call this the length bias problem.", "labels": [], "entities": []}, {"text": "We propose an alternative model that avoids the margin discrepancy by globally conditioning the P (y|x) distribution.", "labels": [], "entities": []}, {"text": "Our model is applicable in the many practical tasks where the space of allowed outputs is closed.", "labels": [], "entities": []}, {"text": "For example, the responses generated by the smart reply feature of Inbox is restricted to lie within a hand-screened whitelist of responses W \u2282 Y (, and the same holds fora recent conversation assistant feature of Google's Allo.", "labels": [], "entities": [{"text": "Inbox", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9495006203651428}]}, {"text": "Our model uses a second RNN encoder to represent the output as another fixed length vector.", "labels": [], "entities": []}, {"text": "We show that our proposed encoderencoder model produces better calibrated whole sequence probabilities and alleviates the length-bias problem of ED models on two conversation tasks.", "labels": [], "entities": []}, {"text": "A second advantage of our model is that inference is significantly faster than ED models and is guaranteed to find the globally optimal solution.", "labels": [], "entities": []}, {"text": "In contrast, inference in ED models requires an expensive beamsearch which is both slow and is not guaranteed to find the optimal sequence.", "labels": [], "entities": []}], "datasetContent": [{"text": "We contrast the quality of the ED and encoderencoder models on two conversational datasets: Open Subtitles and Reddit Comments.", "labels": [], "entities": []}, {"text": "The Open Subtitles dataset consists of transcriptions of spoken dialog in movies and television shows (.", "labels": [], "entities": [{"text": "Open Subtitles dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.6172727147738138}]}, {"text": "We restrict our modeling only to the English subtitles, of which results in 319 million utternaces.", "labels": [], "entities": []}, {"text": "Each utterance is tokenized into word and punctuation tokens, with the start and end marked by the BOS and EOS tokens.", "labels": [], "entities": [{"text": "BOS", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9927908182144165}, {"text": "EOS", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.6647241711616516}]}, {"text": "We randomly split out 90% of the utterances into the training set, placing the rest into the validation set.", "labels": [], "entities": []}, {"text": "As the speaker information is not present in this data set, we treat each utterance as a label sequence, with the preceding utterances as context.", "labels": [], "entities": []}, {"text": "The Reddit Comments dataset is constructed from publicly available user comments on submissions on the Reddit website.", "labels": [], "entities": [{"text": "Reddit Comments dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.872052013874054}]}, {"text": "Each submission is associated with a list of directed comment trees.", "labels": [], "entities": []}, {"text": "In total, there are 41 million submissions and 501 million comments.", "labels": [], "entities": []}, {"text": "We tokenize the individual comments in the same way as we have done with the utternaces in the Open Subtitles dataset.", "labels": [], "entities": [{"text": "Open Subtitles dataset", "start_pos": 95, "end_pos": 117, "type": "DATASET", "confidence": 0.7900503675142924}]}, {"text": "We randomly split 90% of the submissions and the associated comments into the training set, and the rest into the validation set.", "labels": [], "entities": []}, {"text": "We use each comment (except the ones with no parent comments) as a label sequence, with the context sequence composed of its ancestor comments.", "labels": [], "entities": []}], "tableCaptions": []}