{"title": [{"text": "Universal Decompositional Semantics on Universal Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a framework for augmenting data sets from the Universal Dependencies project with Universal Decompositional Semantics.", "labels": [], "entities": []}, {"text": "Where the Universal Dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard, our extension has similar aims for semantic annotation.", "labels": [], "entities": []}, {"text": "We describe results from annotating the English Universal Dependencies treebank, dealing with word senses, semantic roles, and event properties.", "labels": [], "entities": [{"text": "English Universal Dependencies treebank", "start_pos": 40, "end_pos": 79, "type": "DATASET", "confidence": 0.7448292672634125}]}], "introductionContent": [{"text": "This paper describes the Universal Decompositional Semantics (Decomp) project, which aims to augments Universal Dependencies (UD) data sets with robust, scalable semantic annotations based in linguistic theory.", "labels": [], "entities": []}, {"text": "The UD project 1 aims to provide (i) a syntactic dependency annotation standard that can be used consistently across many languages and (ii) a collection of corpora that use that standard.", "labels": [], "entities": []}, {"text": "Decomp provides complementary semantic annotations that scale across different types of semantic information and different languages and can integrate seamlessly with any UD-annotated corpus.", "labels": [], "entities": []}, {"text": "Decomp has two mutually supportive tenetssemantic decomposition and simplicity.", "labels": [], "entities": []}, {"text": "As we discuss further in the next section, these tenets allow us to collect annotations from everyday speakers of a language that are rooted in basic, commonsensical   aspects of meaning and that can be straightforwardly explained and generally agreed upon in context.", "labels": [], "entities": []}, {"text": "In this paper, we describe Decomp protocols for three domains-semantic role decomposition, event decomposition, and word sense decompositionand we present annotation results on top of the English UD v1.2 (EUD1.2) treebank.", "labels": [], "entities": [{"text": "event decomposition", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7727886736392975}, {"text": "word sense decompositionand", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.7007001439730326}, {"text": "English UD v1.2 (EUD1.2) treebank", "start_pos": 188, "end_pos": 221, "type": "DATASET", "confidence": 0.8676967961447579}]}, {"text": "We begin in \u00a72 by connecting Decomp with previous work on decomposition in linguistic theory.", "labels": [], "entities": []}, {"text": "In \u00a73, we present PredPatt, which is a software package for preprocessing UD annotated corpora for input into Decomp protocols.", "labels": [], "entities": [{"text": "PredPatt", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.5368756055831909}]}, {"text": "In \u00a74, we present a major revision to semantic role decomposition protocol (SPR1).", "labels": [], "entities": [{"text": "semantic role decomposition protocol (SPR1)", "start_pos": 38, "end_pos": 81, "type": "TASK", "confidence": 0.7130772003105709}]}, {"text": "Our revision, SPR2, brings SPR1 into full alignment with Decomp while adding various new features.", "labels": [], "entities": []}, {"text": "In \u00a75, we present Decomp-aligned annotation of event properties, focusing specifically on event realis.", "labels": [], "entities": []}, {"text": "Finally, in \u00a76, we describe a Decomp-aligned word sense decomposition protocol and associated set of annotations.", "labels": [], "entities": [{"text": "Decomp-aligned word sense decomposition", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.5634975135326385}]}], "datasetContent": [{"text": "In this section, we present three pilot experiments conducted on a subset of EUD1.2 and aimed at validating the updated protocol in preparation for deployment of the full task.", "labels": [], "entities": [{"text": "EUD1.2", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9751883149147034}]}, {"text": "In the first, we use the SPR1 protocol to obtain judgments on a small sample of EUD1.2 sentences from the same trusted annotator that produced SPR1.", "labels": [], "entities": []}, {"text": "In the second, we open the same task to multiple annotators.", "labels": [], "entities": []}, {"text": "And in the third, we deploy our updated SPR2 protocol on the in (i) as likely to have been used in carrying out the advising.", "labels": [], "entities": []}, {"text": "same subset of EUD1.2-again, open to multiple annotators.", "labels": [], "entities": [{"text": "EUD1.2-again", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.9680699706077576}]}, {"text": "We use these three pilots to evaluate interannotator agreement within and across protocols (where possible) and to construct a pool of trusted annotators to work on the full annotation task.", "labels": [], "entities": []}, {"text": "Item selection For each pilot, the same set of sentences were used.", "labels": [], "entities": [{"text": "Item selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6670004725456238}]}, {"text": "These sentences were selected based on properties of both the predicate and its corresponding arguments in each sentence.", "labels": [], "entities": []}, {"text": "The pilot experiments were limited to the same 10 verbs (want, put, think, see, know, look, say, take, tell, give) that were considered in Reisinger et al.'s pilot.", "labels": [], "entities": []}, {"text": "As in, tokens were excluded with verbs that occur in certain syntactic environments that interfere with property judgments.", "labels": [], "entities": []}, {"text": "We used the same filters described by those authors, modified for UD.", "labels": [], "entities": [{"text": "UD", "start_pos": 66, "end_pos": 68, "type": "DATASET", "confidence": 0.7300193309783936}]}, {"text": "Additionally, verbs occurring as the second item in a conjunction were removed, as EUD1.2 does not have sufficient annotation to identify all arguments of such verbs from the syntax.", "labels": [], "entities": [{"text": "EUD1.2", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9316982626914978}]}, {"text": "Verbal arguments were defined as the subtrees governed by a verb via a core grammatical relation (nsubj, nsubpass, dobj, and iobj).", "labels": [], "entities": []}, {"text": "In addition, occurrences of the pronoun it in subject position were excluded because of inconsistencies in the annotation of expletive subjects in EUD1.2.", "labels": [], "entities": [{"text": "EUD1.2", "start_pos": 147, "end_pos": 153, "type": "DATASET", "confidence": 0.9525842070579529}]}, {"text": "Pilots 1 & 2: SPR1 protocol Pilot 1, designed to compare SPR1 directly to SPR2, used the same protocol described in and was deployed on 99 argument tokens selected based on the method above.", "labels": [], "entities": []}, {"text": "8 To ensure that the only difference between SPR1 and this pilot was which sentences were annotated, we obtained the AMT identifier for the SPR1 annotator from Reisinger et al.", "labels": [], "entities": [{"text": "AMT", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.6330087184906006}]}, {"text": "Thus, the only annotator in this pilot was the same one that produced all the annotations for the SPR1 dataset.", "labels": [], "entities": [{"text": "SPR1 dataset", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9086199998855591}]}, {"text": "The data from this pilot cannot be compared to the SPR1 dataset on a token level, since the items do not come from the same dataset.", "labels": [], "entities": [{"text": "SPR1 dataset", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9129113852977753}]}, {"text": "But these data can be compared to the SPR1 dataset on a type level by averaging responses to particular questions asked about particular argument positions (e.g., nsubj, dobj, etc.) fora particular predicate (e.g., want, put, etc.) and then comparing the correlation between these averages.", "labels": [], "entities": [{"text": "SPR1 dataset", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.7291323393583298}]}, {"text": "The average type-level correlation between the average bypredicate, by-argument relation ratings in the SPR1 dataset and those in the current pilot was high for all verb-argument pairs (Spearman \u03c1=0.82).", "labels": [], "entities": [{"text": "SPR1 dataset", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.8420409262180328}, {"text": "Spearman \u03c1", "start_pos": 186, "end_pos": 196, "type": "METRIC", "confidence": 0.9845719039440155}]}, {"text": "Pilot 2 uses the same materials and protocol as Pilot 1.", "labels": [], "entities": [{"text": "Pilot", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9162337183952332}]}, {"text": "The only difference between the two is that this pilot was open to multiple annotators.", "labels": [], "entities": []}, {"text": "A total of 33 annotators participated, one of whom was the same annotator that produced all the annotations for the SPR1 dataset and participated in Pilot 1.", "labels": [], "entities": [{"text": "SPR1 dataset", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.9307348728179932}]}, {"text": "For each argument token, we collected five judgments per property question.", "labels": [], "entities": []}, {"text": "Interannotator agreement was calculated by argument token for the likelihood responses using pairwise Spearman rank correlations.", "labels": [], "entities": []}, {"text": "The mean \u03c1 across all annotator pairs and argument tokens was 0.562 (95% CI=[0.549, 0.574]) and, due to heavy left skew, the median was 0.618 (95% CI=).", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 9, "end_pos": 10, "type": "METRIC", "confidence": 0.8322026133537292}]}, {"text": "This agreement is relatively high, suggesting that different annotators tend to agree on the relative likelihood of a property applying to an argument.", "labels": [], "entities": []}, {"text": "Since the SPR1 and Pilot 1 annotator was among this group, we can also assess the extent to which the Pilot 1 annotator is consistent with other annotators.", "labels": [], "entities": [{"text": "SPR1", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8246676921844482}]}, {"text": "Comparing this annotator to every other annotator that annotated the same argument token, the mean \u03c1 was 0.499 (95% CI=[0.451, 0.546]), and the median was 0.565 (95% CI=).", "labels": [], "entities": []}, {"text": "This suggests that, on average, the other annotators are even more consistent with each other than they are with the original SPR1 annotator, vindicating the use of multiple annotators.", "labels": [], "entities": []}, {"text": "Pilot 3: SPR2 protocol Pilot 3 uses the same materials as Pilots 1 and 2 but introduces the SPR2 protocol laid out above.", "labels": [], "entities": []}, {"text": "A total of 57 annotators participated in this pilot.", "labels": [], "entities": []}, {"text": "For each argument token, we again collected five judgments per property.", "labels": [], "entities": []}, {"text": "Interannotator agreement was calculated by argument token for the likelihood responses using pairwise Spearman rank correlations.", "labels": [], "entities": []}, {"text": "The mean \u03c1 across all annotator pairs and argument tokens was 0.622 (95% CI=[0.610, 0.634]) and, again due to heavy left skew, the median was 0.677 (95% CI=).", "labels": [], "entities": []}, {"text": "This higher agreement compared to Pilot 2 likely arises due to the fact that we have fewer questions in the SPR2 protocol and suggests that we suc-ceeded in removing noisy questions without adding questions that were similarly noisy.", "labels": [], "entities": []}, {"text": "Since we use the same materials as Pilots 1 and 2, we can also compare the SPR1 and SPR2 protocols on the subset of questions they share.", "labels": [], "entities": []}, {"text": "We find similar mean agreement, at 0.593 (95% CI=[0.580, 0.607]), and median agreement, at 0.665 (95% CI=[0.652, 0.672]), to that we found within the Pilots 2 and 3 results.", "labels": [], "entities": [{"text": "agreement", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.5082707405090332}, {"text": "median agreement", "start_pos": 70, "end_pos": 86, "type": "METRIC", "confidence": 0.7233776450157166}]}, {"text": "This suggests that the addition and subtraction of questions does not substantially alter annotators' judgments on the questions that both protocols share.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of manual evaluation of PredPatt on UD", "labels": [], "entities": [{"text": "UD", "start_pos": 54, "end_pos": 56, "type": "DATASET", "confidence": 0.6827470064163208}]}]}