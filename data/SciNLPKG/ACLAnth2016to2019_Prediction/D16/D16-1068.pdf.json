{"title": [{"text": "Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing", "labels": [], "entities": [{"text": "Graph-based Non-Projective Dependency Parsing", "start_pos": 31, "end_pos": 76, "type": "TASK", "confidence": 0.5662148147821426}]}], "abstractContent": [{"text": "Exact inference in high-order graph-based non-projective dependency parsing is intractable.", "labels": [], "entities": [{"text": "high-order graph-based non-projective dependency parsing", "start_pos": 19, "end_pos": 75, "type": "TASK", "confidence": 0.7840598821640015}]}, {"text": "Hence, sophisticated approximation techniques based on algorithms such as belief propagation and dual decomposition have been employed.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.8232970833778381}, {"text": "dual decomposition", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.9408184587955475}]}, {"text": "In contrast, we propose a simple greedy search approximation for this problem which is very intuitive and easy to implement.", "labels": [], "entities": []}, {"text": "We implement the algorithm within the second-order TurboParser and experiment with the datasets of the CoNLL 2006 and 2007 shared task on multilingual dependency parsing.", "labels": [], "entities": [{"text": "CoNLL 2006 and 2007 shared task", "start_pos": 103, "end_pos": 134, "type": "DATASET", "confidence": 0.9165792564551035}, {"text": "multilingual dependency parsing", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.5770170787970225}]}, {"text": "Our algorithm improves the run time of the parser by a factor of 1.43 while losing 1% in UAS on average across languages.", "labels": [], "entities": [{"text": "UAS", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.5806792378425598}]}, {"text": "Moreover , an ensemble method exploiting the joint power of the parsers, achieves an average UAS 0.27% higher than the TurboParser.", "labels": [], "entities": [{"text": "UAS", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9990054965019226}]}], "introductionContent": [{"text": "Dependency parsing is instrumental in NLP applications, with recent examples in information extraction (, word embeddings (, and opinion mining ().", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8730409145355225}, {"text": "information extraction", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.8134775161743164}, {"text": "opinion mining", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.7668489813804626}]}, {"text": "The two main approaches for this task are graph based) and transition based.", "labels": [], "entities": []}, {"text": "The graph based approach aims to optimize a global objective function.", "labels": [], "entities": []}, {"text": "While exact polynomial inference algorithms exist for projective parsing, inter alia), high order non-projective parsing is NP-hard).", "labels": [], "entities": [{"text": "projective parsing", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.6765937507152557}]}, {"text": "The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition, LP relaxations (, belief propagation () and sampling ().", "labels": [], "entities": [{"text": "dual decomposition", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7803937196731567}, {"text": "belief propagation", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7282213121652603}]}, {"text": "The transition based approach (, inter alia), and the easy first approach which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global objective function.", "labels": [], "entities": []}, {"text": "Yet, their sequential greedy solvers are fast and accurate in practice.", "labels": [], "entities": [{"text": "sequential greedy solvers", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.6703308721383413}]}, {"text": "We propose a greedy search algorithm for highorder, non-projective graph-based dependency parsing.", "labels": [], "entities": [{"text": "non-projective graph-based dependency parsing", "start_pos": 52, "end_pos": 97, "type": "TASK", "confidence": 0.8124110996723175}]}, {"text": "Our algorithm is a simple iterative graph-based method that does not rely on advanced optimization techniques.", "labels": [], "entities": []}, {"text": "Moreover, we factorize the graph-based objective into a sum of terms and show that our basic greedy algorithm relaxes the global objective by sequentially optimizing these terms instead of globally optimizing their sum.", "labels": [], "entities": []}, {"text": "Unlike previous greedy approaches to dependency parsing, transition based and non-directional, our algorithm does not require a specialized feature set or a training method that specializes in local decisions.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8329485058784485}]}, {"text": "In contrast, it supports global parameter training based on the comparison between an induced tree and the gold tree.", "labels": [], "entities": [{"text": "global parameter training", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.6724564035733541}]}, {"text": "Hence, it can be integrated into any graph-based parser.", "labels": [], "entities": []}, {"text": "We first present a basic greedy algorithm that relaxes the global graph-based objective (Section 3).", "labels": [], "entities": []}, {"text": "However, as this simple algorithm does not provide a realistic estimation of the impact of an arc selection on uncompleted high-order structures in the partial parse forest, it is not competitive with state of the art approximations.", "labels": [], "entities": []}, {"text": "We hence present an advanced version of our algorithm with an improved arc score formulation and show that this simple algorithm provides high quality solutions to the graph-based inference problem (Section 4).", "labels": [], "entities": [{"text": "arc score formulation", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.8964849909146627}]}, {"text": "Particularly, we implement the algorithm within the TurboParser () and experiment (Sections 8 and 9) with the datasets of the CoNLL 2006-2007 shared tasks on multilingual dependency parsing).", "labels": [], "entities": [{"text": "CoNLL 2006-2007 shared tasks", "start_pos": 126, "end_pos": 154, "type": "DATASET", "confidence": 0.9191635400056839}, {"text": "multilingual dependency parsing", "start_pos": 158, "end_pos": 189, "type": "TASK", "confidence": 0.5667966604232788}]}, {"text": "On average across languages our parser achieves UAS scores of 87.78% and 89.25% for first and second order parsing respectively, compared to respective UAS of 87.98% and 90.26% achieved by the original TurboParser.", "labels": [], "entities": [{"text": "UAS", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9852482080459595}, {"text": "UAS", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9960428476333618}]}, {"text": "We further implement (Section 6) an ensemble method that integrates information from the output tree of the original TurboParser and the arc weights learned by our variant of the parser into our search algorithm to generate anew tree.", "labels": [], "entities": []}, {"text": "This yields an improvement: average UAS of 88.03% and 90.53% for first and second parsing, respectively.", "labels": [], "entities": [{"text": "UAS", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9938173890113831}, {"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.8912490606307983}]}, {"text": "Despite being greedy, the theoretical runtime complexity of our advanced algorithm is not better than the best previously proposed approximations for our problem (O(n k+1 ), for n word sentences and k order parsing, Section 5).", "labels": [], "entities": [{"text": "O", "start_pos": 163, "end_pos": 164, "type": "METRIC", "confidence": 0.9201136231422424}]}, {"text": "In experiments, our algorithms improve the runtime of the TurboParser by a factor of up to 2.41.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is hence in providing a simple, intuitive and easy to implement solution fora longstanding problem that has been addressed in past with advanced optimization techniques.", "labels": [], "entities": []}, {"text": "Besides the intellectual contribution, we believe this will make high-order graph-based dependency parsing accessible to a much broader research and engineering community as it substantially relaxes the coding and algorithmic proficiency required for the implementation and understanding of parsing algorithms.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7252607047557831}, {"text": "parsing algorithms", "start_pos": 291, "end_pos": 309, "type": "TASK", "confidence": 0.8497822880744934}]}], "datasetContent": [{"text": "We implemented our algorithms within the TurboParser ( . That is, every other aspect of the parser -feature set, pruning algorithm, cost-augmented MIRA training () etc., is kept fixed but our algorithms replace the inference algorithms: Chu-Liu-Edmonds (), first order) and dual-decomposition (higher order).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9379103183746338}]}, {"text": "We implemented two variants, for algorithm 1 and 2 respectively, and compare their results to those of the original TurboParser.", "labels": [], "entities": []}, {"text": "We experiment with the datasets of the CoNLL 2006 and 2007 shared task on multilingual dependency parsing (, fora total of 17 languages.", "labels": [], "entities": [{"text": "CoNLL 2006 and 2007 shared task", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.8918163081010183}, {"text": "multilingual dependency parsing", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.5819152394930521}]}, {"text": "When a language is represented in both sets, we used the 2006 version.", "labels": [], "entities": []}, {"text": "We followed the standard train/test split of these datasets and, for the 8 languages with a training set of at least 10000 sentences, we randomly sampled 1000 sentences from the training set to serve as a development set.", "labels": [], "entities": []}, {"text": "For these languages, we first trained the parser on the training set and then used the development set for hyperparameter tuning (|B|, s, \u03b1, \u03b2, and \u03b3 1 , . .", "labels": [], "entities": []}, {"text": ", \u03b3 k fork order parsing).", "labels": [], "entities": []}, {"text": "We employ four evaluation measures, where every measure is computed per language, and we report the average across all languages: (1) Unlabeled Attachment Score (UAS); (2) Undirected UAS (U-UAS) -for error analysis purposes; (3) Shared arcs (SARC) -the percentage of arcs shared by the predictions of each of our algorithms and of the original TurboParser; and (4) Tokens per second (TPS) -for ensemble models this measure includes the TurboParser's inference time.", "labels": [], "entities": [{"text": "Unlabeled Attachment Score (UAS)", "start_pos": 134, "end_pos": 166, "type": "METRIC", "confidence": 0.8062176406383514}, {"text": "Shared arcs (SARC)", "start_pos": 229, "end_pos": 247, "type": "METRIC", "confidence": 0.8141077160835266}, {"text": "Tokens per second (TPS)", "start_pos": 365, "end_pos": 388, "type": "METRIC", "confidence": 0.6935707529385885}]}, {"text": "We also report a gold(x,y) = (a,b) measure: where a is the percentage of gold standard arcs included in trees produced by algorithm x but not by y, and b is the corresponding number for y and x.", "labels": [], "entities": []}, {"text": "The original TurboParser is trained on the training set of each language and tested on its test set, without any further division of the training data to training and development sets.", "labels": [], "entities": []}, {"text": "Run times where computed on an Intel(R) Xeon(R) CPU E5-2697 v3@2.60GHz machine with 20GB RAM memory.", "labels": [], "entities": []}, {"text": "Fully Supervised Training In this setup we only consider the 8 languages with a development set.", "labels": [], "entities": []}, {"text": "For each language, the parser is trained on the training set and then the hyperparameters are tuned.", "labels": [], "entities": []}, {"text": "First we set the beam size (|B|) and number of improvement iterations (s) to 0, and tune the other hyperparameters on the language-specific development set.", "labels": [], "entities": []}, {"text": "Then, we tune |B| and s, using the optimal parameters of the first step, on the English dev. set.", "labels": [], "entities": [{"text": "English dev. set", "start_pos": 80, "end_pos": 96, "type": "DATASET", "confidence": 0.9535815864801407}]}, {"text": "Minimally Supervised Training Here we consider all 17 languages.", "labels": [], "entities": []}, {"text": "For each language we randomly sampled 20 training sets of 500 sentences from the original training set, trained a parser on each set and tested on the original test set.", "labels": [], "entities": []}, {"text": "Results for each language were calculated as the average over these 20 folds.", "labels": [], "entities": []}, {"text": "The hyper parameters for all languages were tuned once on the English development set to the values that yielded the best average results across the 20 training samples.", "labels": [], "entities": [{"text": "English development set", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.7094030678272247}]}], "tableCaptions": [{"text": " Table 1: Results for the fully supervised (top table) and minimally supervised (bottom table) setups. The left column", "labels": [], "entities": []}, {"text": " Table 2: Per language UAS for the fully supervised setup. Model names are as in Table 1, 'e' stands for ensemble.", "labels": [], "entities": []}]}