{"title": [{"text": "Deep Reinforcement Learning for Mention-Ranking Coreference Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8987149298191071}]}, {"text": "In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics.", "labels": [], "entities": [{"text": "coreference evaluation", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.9016085565090179}]}, {"text": "We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective.", "labels": [], "entities": [{"text": "REINFORCE policy gradient", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6697483559449514}]}, {"text": "We find the latter to be more effective, resulting in a significant improvement over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task.", "labels": [], "entities": [{"text": "CoNLL 2012 Shared Task", "start_pos": 157, "end_pos": 179, "type": "DATASET", "confidence": 0.8559319972991943}]}], "introductionContent": [{"text": "Coreference resolution systems typically operate by making sequences of local decisions (e.g., adding a coreference link between two mentions).", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.933233767747879}]}, {"text": "However, most measures of coreference resolution performance do not decompose over local decisions, which means the utility of a particular decision is not known until all other decisions have been made.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9553713798522949}]}, {"text": "Due to this difficulty, coreference systems are usually trained with loss functions that heuristically define the goodness of a particular coreference decision.", "labels": [], "entities": []}, {"text": "These losses contain hyperparameters that are carefully selected to ensure the model performs well according to coreference evaluation metrics.", "labels": [], "entities": []}, {"text": "This complicates training, especially across different languages and datasets where systems may work best with different settings of the hyperparameters.", "labels": [], "entities": []}, {"text": "To address this, we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics.", "labels": [], "entities": []}, {"text": "In particular, we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss's slack rescaling.", "labels": [], "entities": []}, {"text": "We also test the REINFORCE policy gradient algorithm.", "labels": [], "entities": [{"text": "REINFORCE policy gradient", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.7354381680488586}]}, {"text": "Our model is a neural mention-ranking model.", "labels": [], "entities": []}, {"text": "Mention-ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters.", "labels": [], "entities": []}, {"text": "Hence they operate in a simple setting where coreference decisions are made independently.", "labels": [], "entities": []}, {"text": "Although they are less expressive than entity-centric approaches to coreference (e.g.,, mention-ranking models are fast, scalable, and simple to train, causing them to be the dominant approach to coreference in recent years (.", "labels": [], "entities": []}, {"text": "Having independent actions is particularly useful when applying reinforcement learning because it means a particular action's effect on the final reward can be computed efficiently.", "labels": [], "entities": []}, {"text": "We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task.", "labels": [], "entities": [{"text": "CoNLL 2012 Shared Task", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.9179427027702332}]}, {"text": "The REINFORCE algorithm is competitive with a heuristic loss function while the reward-rescaled objective significantly outperforms both . We attribute this to reward rescaling being well suited fora ranking task due to its max-margin loss as well as benefiting from directly optimizing for coreference metrics.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9688100218772888}]}, {"text": "Error analysis shows that using the reward-rescaling loss results in a similar number of mistakes as the heuristic loss, but the mistakes tend to be less severe.", "labels": [], "entities": []}], "datasetContent": [{"text": "We (), dropout (Hinton et al., 2012) with a rate of 0.5, and pretraining with the all pairs classification and top pairs classification tasks.", "labels": [], "entities": [{"text": "pretraining", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9515297412872314}]}, {"text": "However, we improve on the previous system by using using better mention detection, more effective hyperparameters, and more epochs of training.", "labels": [], "entities": [{"text": "mention detection", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.71971495449543}]}], "tableCaptions": [{"text": " Table 1: Comparison of the methods together with other state-of-the-art approaches on the test sets.", "labels": [], "entities": []}, {"text": " Table 3: Examples of classes of mention on which the reward-rescaling loss significantly improves upon the heuristic loss due to  its reward-based cost function. Reported numbers are from the English CoNLL 2012 test set.", "labels": [], "entities": [{"text": "English CoNLL 2012 test set", "start_pos": 193, "end_pos": 220, "type": "DATASET", "confidence": 0.9182673573493958}]}, {"text": " Table 2: Number of \"false new,\" \"false anaphoric,\" and  \"wrong link\" errors produced by the models on the English  CoNLL 2012 test set.", "labels": [], "entities": [{"text": "wrong link\" errors produced", "start_pos": 58, "end_pos": 85, "type": "METRIC", "confidence": 0.7391089677810669}, {"text": "English  CoNLL 2012 test set", "start_pos": 107, "end_pos": 135, "type": "DATASET", "confidence": 0.9621622085571289}]}]}