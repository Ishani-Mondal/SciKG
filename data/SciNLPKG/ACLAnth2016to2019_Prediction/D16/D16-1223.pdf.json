{"title": [{"text": "Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling", "labels": [], "entities": [{"text": "Semantic Slot Filling", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.6700660685698191}]}], "abstractContent": [{"text": "Recurrent Neural Network (RNN) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used for sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.6142668128013611}]}, {"text": "Explicitly modeling output label dependencies on top of RNN/LSTM is a widely-studied and effective extension.", "labels": [], "entities": []}, {"text": "We propose another extension to incorporate the global information spanning over the whole input sequence.", "labels": [], "entities": []}, {"text": "The proposed method, encoder-labeler LSTM, first encodes the whole input sequence into a fixed length vector with the encoder LSTM, and then uses this encoded vector as the initial state of another LSTM for sequence labeling.", "labels": [], "entities": []}, {"text": "With this method, we can predict the label sequence while taking the whole input sequence information into consideration.", "labels": [], "entities": []}, {"text": "In the experiments of a slot filling task, which is an essential component of natural language understanding , with using the standard ATIS corpus , we achieved the state-of-the-art F 1-score of 95.66%.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8769154151280721}, {"text": "natural language understanding", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6485822002092997}, {"text": "ATIS corpus", "start_pos": 135, "end_pos": 146, "type": "DATASET", "confidence": 0.9548269510269165}, {"text": "F 1-score", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9905754029750824}]}], "introductionContent": [{"text": "Natural language understanding (NLU) is an essential component of natural human computer interaction and typically consists of identifying the intent of the users (intent classification) and extracting the associated semantic slots (slot filling).", "labels": [], "entities": [{"text": "Natural language understanding (NLU)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7894820868968964}]}, {"text": "We focus on the latter semantic slot filling task in this paper.", "labels": [], "entities": [{"text": "semantic slot filling", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.6427459220091502}]}, {"text": "Slot filling can be framed as a sequential labeling problem in which the most probable semantic slot labels are estimated for each word of the given word sequence.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9668473899364471}]}, {"text": "Slot filling is a traditional task and tremendous efforts have been done, especially since the 1980s when the Defense Advanced Research Program Agency (DARPA) Airline Travel Information System (ATIS) projects started.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.984709769487381}, {"text": "Defense Advanced Research Program Agency (DARPA) Airline Travel Information System (ATIS)", "start_pos": 110, "end_pos": 199, "type": "TASK", "confidence": 0.6355741699536641}]}, {"text": "Following the success of deep learning, Recurrent Neural Network (RNN)) and one of its specific architectures, Long Short-Term Memory (LSTM), have been widely used since they can capture temporal dependencies (.", "labels": [], "entities": []}, {"text": "The RNN/LSTM-based slot filling has been extended to be combined with explicit modeling of label dependencies (;.", "labels": [], "entities": [{"text": "RNN/LSTM-based slot filling", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.5072060227394104}]}, {"text": "In this paper, we extend the LSTM-based slot filling to consider sentence-level information.", "labels": [], "entities": [{"text": "LSTM-based slot filling", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7897894779841105}]}, {"text": "In the field of machine translation, an encoder-decoder LSTM has been gaining attention , where the encoder LSTM encodes the global information spanning over the whole input sentence in its last hidden state.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7839672267436981}]}, {"text": "Inspired by this idea, we propose an encoder-labeler LSTM that leverages the encoder LSTM for slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.8012669682502747}]}, {"text": "First, we encode the input sentence into a fixed length vector by the encoder LSTM.", "labels": [], "entities": []}, {"text": "Then, we predict the slot label sequence by the labeler LSTM whose hidden state is initialized with the encoded vector by the encoder LSTM.", "labels": [], "entities": []}, {"text": "With this encoder-labeler LSTM, we can predict the label sequence while taking the sentence-level information into consideration.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are twofolds: 1.", "labels": [], "entities": []}, {"text": "Proposed an encoder-labeler LSTM to leverage sentence-level information for slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.8450523018836975}]}, {"text": "2. Achieved the state-of-the-art F 1 -score of 95.66% in the slot filling task of the standard ATIS corpus.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9858430176973343}, {"text": "slot filling task", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8488633235295614}, {"text": "ATIS corpus", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.9578958451747894}]}], "datasetContent": [{"text": "We report two sets of experiments.", "labels": [], "entities": []}, {"text": "First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9629952907562256}]}, {"text": "Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case.", "labels": [], "entities": []}, {"text": "We used the ATIS corpus, which has been widely used as the benchmark for NLU.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.9547452628612518}, {"text": "NLU", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8783900141716003}]}, {"text": "shows an example sentence and its semantic slot labels in In-Out-Begin (IOB) representation.", "labels": [], "entities": []}, {"text": "The slot filling task was to predict the slot label sequences from input word sequences.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8888181149959564}]}, {"text": "The performance was measured by the F 1 -score: where precision is the ratio of the correct labels in the system's output and recall is the ratio of the correct labels in the ground truth of the evaluation data.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9856278896331787}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9993715882301331}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.99943608045578}]}, {"text": "The ATIS corpus contains the training data of 4,978 sentences and evaluation data of 893 sentences.", "labels": [], "entities": [{"text": "ATIS corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9517946243286133}]}, {"text": "The unique number of slot labels is 127 and the vocabulary size is 572.", "labels": [], "entities": []}, {"text": "In the following experiments, we randomly selected 80% of the original training data to train the model and used the remaining 20% as the heldout data.", "labels": [], "entities": []}, {"text": "We reported the F 1 -score on the evaluation data with hyper-parameters that achieved the best F 1 -score on the heldout data.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9861269742250443}, {"text": "F 1 -score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9893535375595093}]}, {"text": "For training, we randomly initialized parameters in accordance with the normalized initialization (.", "labels": [], "entities": []}, {"text": "We used ADAM for learning rate control () and dropout for generalization with a dropout rate of 0.5 ().", "labels": [], "entities": [{"text": "ADAM", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.7826640605926514}]}, {"text": "We prepared a large-scale data set by merging the MIT Restaurant Corpus and MIT Movie Cor- There are other published results that achieved better F1-scores by using other information on top of word features.", "labels": [], "entities": [{"text": "MIT Restaurant Corpus", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.9206666549046835}, {"text": "MIT Movie Cor", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.931286613146464}, {"text": "F1-scores", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9984257221221924}]}, {"text": "achieved 96.16% F1-score by using the named entity (NE) database when estimating word embeddings. and used NE features in addition to word features and obtained improvement with both the RNN and LSTM upto 96.60% F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993964433670044}, {"text": "F1-score", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9978925585746765}]}, {"text": "also used NE features and reported F1-score of 96.29% with RNN and 96.46% with Recurrent CRF.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9997658133506775}]}, {"text": "pus () with the ATIS corpus.", "labels": [], "entities": [{"text": "pus", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9032621383666992}, {"text": "ATIS corpus", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.9807896912097931}]}, {"text": "Since users of the NLU system may provide queries without explicitly specifying their domain, building one NLU model for multiple domains is necessary.", "labels": [], "entities": []}, {"text": "The merged data set contains 30,229 training and 6,810 evaluation sentences.", "labels": [], "entities": []}, {"text": "The unique number of slot labels is 191 and the vocabulary size is 16,049.", "labels": [], "entities": []}, {"text": "With this merged data set, we compared the labeler LSTM(W) and the proposed encoder-labeler LSTM(W) according to the experimental procedure explained in Section 3.1.2.", "labels": [], "entities": []}, {"text": "The labeler LSTM(W) achieved the F 1 -score of 72.80% and the encoder-labeler LSTM(W) improved it to 74.41%, which confirmed the effect of the proposed method in large and realistic data set 6 .", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9930558204650879}]}], "tableCaptions": [{"text": " Table 1: Experimental results on ATIS slot filling task. Left-", "labels": [], "entities": [{"text": "ATIS slot filling task", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7343407198786736}]}, {"text": " Table 2: Comparison with published results on ATIS slot filling", "labels": [], "entities": [{"text": "ATIS slot", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.7379849553108215}]}]}