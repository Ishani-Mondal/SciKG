{"title": [{"text": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution.", "labels": [], "entities": []}, {"text": "We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences.", "labels": [], "entities": []}, {"text": "In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary.", "labels": [], "entities": []}, {"text": "In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data.", "labels": [], "entities": []}, {"text": "Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.", "labels": [], "entities": [{"text": "semi-supervised compression", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7600840330123901}]}], "introductionContent": [{"text": "The recurrent sequence-to-sequence paradigm for natural language generation) has achieved remarkable recent success and is now the approach of choice for applications such as machine translation (, caption generation () and speech recognition (.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6764895021915436}, {"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.8348818719387054}, {"text": "caption generation", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.8666490018367767}, {"text": "speech recognition", "start_pos": 224, "end_pos": 242, "type": "TASK", "confidence": 0.8218772113323212}]}, {"text": "While these models have developed sophisticated conditioning mechanisms, e.g. attention, fundamentally they are discriminative models trained only to approximate the conditional output distribution of strings.", "labels": [], "entities": []}, {"text": "In this paper we explore modelling the joint distribution of string pairs using a deep generative model and employing a discrete variational autoencoder (VAE) for inference ().", "labels": [], "entities": [{"text": "discrete variational autoencoder (VAE)", "start_pos": 120, "end_pos": 158, "type": "METRIC", "confidence": 0.7249414424101511}]}, {"text": "We evaluate our generative approach on the task of sentence compression.", "labels": [], "entities": [{"text": "generative", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.9651613235473633}, {"text": "sentence compression", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7224980890750885}]}, {"text": "This approach provides both alternative supervised objective functions and the opportunity to perform semi-supervised learning by exploiting the VAEs ability to marginalise the latent compressed text for unlabelled data.", "labels": [], "entities": []}, {"text": "Auto-encoders ( area typical neural network architecture for learning compact data representations, with the general aim of performing dimensionality reduction on embeddings).", "labels": [], "entities": []}, {"text": "In this paper, rather than seeking to embed inputs as points in a vector space, we describe them with explicit natural language sentences.", "labels": [], "entities": []}, {"text": "This approach is a natural fit for summarisation tasks such as sentence compression.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9861228466033936}, {"text": "sentence compression", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7253378331661224}]}, {"text": "According to this, we propose a generative auto-encoding sentence compression (ASC) model, where we introduce a latent language model to provide the variablelength compact summary.", "labels": [], "entities": [{"text": "generative auto-encoding sentence compression (ASC)", "start_pos": 32, "end_pos": 83, "type": "TASK", "confidence": 0.8650703685624259}]}, {"text": "The objective is to perform Bayesian inference for the posterior distribution of summaries conditioned on the observed utterances.", "labels": [], "entities": []}, {"text": "Hence, in the framework of VAE, we construct an inference network as the variational approximation of the posterior, which generates compression samples to optimise the variational lower bound.", "labels": [], "entities": [{"text": "VAE", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.5240267515182495}]}, {"text": "The most common family of variational autoencoders relies on the reparameterisation trick, which is not applicable for our discrete latent language model.", "labels": [], "entities": []}, {"text": "Instead, we employ the REINFORCE algorithm   to mitigate the problem of high variance during sampling-based variational inference.", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9939666986465454}]}, {"text": "Nevertheless, when directly applying the RNN encoder-decoder to model the variational distribution it is very difficult to generate reasonable compression samples in the early stages of training, since each hidden state of the sequence would have |V | possible words to be sampled from.", "labels": [], "entities": []}, {"text": "To combat this we employ pointer networks ( to construct the variational distribution.", "labels": [], "entities": []}, {"text": "This biases the latent space to sequences composed of words only appearing in the source sentence (i.e. the size of softmax output for each state becomes the length of current source sentence), which amounts to applying an extractive compression model for the variational approximation.", "labels": [], "entities": []}, {"text": "In order to further boost the performance on sentence compression, we employ a supervised forcedattention sentence compression model (FSC) trained on labelled data to teach the ASC model to generate compression sentences.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.781355619430542}, {"text": "forcedattention sentence compression model (FSC)", "start_pos": 90, "end_pos": 138, "type": "METRIC", "confidence": 0.6085587910243443}]}, {"text": "The FSC model shares the pointer network of the ASC model and combines a softmax output layer over the whole vocabulary.", "labels": [], "entities": [{"text": "FSC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.68282151222229}]}, {"text": "Therefore, while training on the sentencecompression pairs, it is able to balance copying a word from the source sentence with generating it from the background distribution.", "labels": [], "entities": []}, {"text": "More importantly, by jointly training on the labelled and unlabelled datasets, this shared pointer network enables the model to work in a semi-supervised scenario.", "labels": [], "entities": []}, {"text": "In this case, the FSC teaches the ASC to generate reasonable samples, while the pointer network trained on a large unlabelled data set helps the FSC model to perform better abstractive summarisation.", "labels": [], "entities": [{"text": "FSC", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.898663341999054}, {"text": "FSC", "start_pos": 145, "end_pos": 148, "type": "DATASET", "confidence": 0.880368709564209}]}, {"text": "In Section 6, we evaluate the proposed model by jointly training the generative (ASC) and discriminative (FSC) models on the standard Gigaword sentence compression task with varying amounts of labelled and unlabelled data.", "labels": [], "entities": [{"text": "Gigaword sentence compression task", "start_pos": 134, "end_pos": 168, "type": "TASK", "confidence": 0.7050236538052559}]}, {"text": "The results demonstrate that by introducing a latent language variable we are able to match the previous benchmakers with small amount of the supervised data.", "labels": [], "entities": []}, {"text": "When we employ our mixed discriminative and generative objective with all of the supervised data the model significantly outperforms all previously published results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed models on the standard Gigaword 3 sentence compression dataset.", "labels": [], "entities": [{"text": "Gigaword 3 sentence compression dataset", "start_pos": 48, "end_pos": 87, "type": "DATASET", "confidence": 0.7935616612434387}]}, {"text": "This dataset was generated by pairing the headline of each article with its first sentence to create a source-compression pair.", "labels": [], "entities": []}, {"text": "provided scripts 4 to filter out outliers, resulting in roughly 3.8M training pairs, a 400K validation set, and a 400K test set.", "labels": [], "entities": []}, {"text": "In the following experiments all models are trained on the training set with different data sizes and tested on a 2K subset, which is identical to the test set used by and.", "labels": [], "entities": []}, {"text": "We decode the sentences by k = 5 Beam search and test with full-length Rouge score.", "labels": [], "entities": [{"text": "Rouge score", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9025108814239502}]}, {"text": "For the ASC and FSC models, we use 256 for the dimension of both hidden units and lookup tables.", "labels": [], "entities": [{"text": "FSC", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8022431135177612}]}, {"text": "In the ASC model, we apply a 3-layer bidirectional RNN with skip connections as the encoder, a 3-layer RNN pointer network with skip connections as the compressor, and a 1-layer vanilla RNN with soft attention as the decoder.", "labels": [], "entities": []}, {"text": "The language model prior is trained on the article sentences of the full training set using a 3-layer vanilla RNN with 0.5 dropout.", "labels": [], "entities": []}, {"text": "To lower the computational cost, we apply different vocabulary sizes for encoder and compressor) which corresponds to the settings of.", "labels": [], "entities": []}, {"text": "Specifically, the vocabulary of the decoder is filtered by taking the most frequent 10,000 words from the vocabulary of the encoder, where the rest of the words are tagged as '<unk>'.", "labels": [], "entities": []}, {"text": "In further consideration of efficiency, we use only one sample for the gradient estimator.", "labels": [], "entities": []}, {"text": "We optimise the model by) with a 0.0002 learning rate and 64 sentences per batch.", "labels": [], "entities": []}, {"text": "The model converges in 5 epochs.", "labels": [], "entities": []}, {"text": "Except for the pretrained language model, we do not use dropout or embedding initialisation for ASC and FSC models.", "labels": [], "entities": [{"text": "FSC", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.5824146270751953}]}], "tableCaptions": [{"text": " Table 1: Extractive Summarisation Performance. (1) The extractive summaries of these models are decoded  by the pointer network (i.e the shared component of the ASC and FSC models). (2) R-1, R-2 and R-L  represent the Rouge-1, Rouge-2 and Rouge-L score respectively.", "labels": [], "entities": [{"text": "Extractive Summarisation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8419128358364105}, {"text": "FSC", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.60919189453125}]}, {"text": " Table 2: Abstractive Summarisation Performance. The abstractive summaries of these models are decoded by  the combined pointer network (i.e. the shared pointer network together with the softmax output layer over the  full vocabulary).", "labels": [], "entities": [{"text": "Abstractive Summarisation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6533785760402679}]}, {"text": " Table 3: Comparison on validation perplexity. BoW,  TDNN and NABS are the baseline neural compres- sion models with different encoders in Rush et al.  (2015)", "labels": [], "entities": [{"text": "BoW", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9092685580253601}, {"text": "NABS", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.695923924446106}]}, {"text": " Table 4: Comparison on test Rouge scores", "labels": [], "entities": [{"text": "test Rouge scores", "start_pos": 24, "end_pos": 41, "type": "METRIC", "confidence": 0.6182803312937418}]}]}