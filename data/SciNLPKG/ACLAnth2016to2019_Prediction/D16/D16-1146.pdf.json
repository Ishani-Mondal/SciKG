{"title": [{"text": "Lifted Rule Injection for Relation Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.8980280458927155}]}, {"text": "Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models.", "labels": [], "entities": []}, {"text": "A recent approach reg-ularizes relation and entity representations by propositionalization of first-order logic rules.", "labels": [], "entities": []}, {"text": "However, propositionalization does not scale beyond domains with only few entities and rules.", "labels": [], "entities": []}, {"text": "In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction.", "labels": [], "entities": [{"text": "automated knowledge base construction", "start_pos": 124, "end_pos": 161, "type": "TASK", "confidence": 0.596597857773304}]}, {"text": "We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 163, "end_pos": 170, "type": "DATASET", "confidence": 0.9556429386138916}]}, {"text": "Surprisingly , we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization.", "labels": [], "entities": []}, {"text": "By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9893703460693359}]}], "introductionContent": [{"text": "Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (.", "labels": [], "entities": [{"text": "knowledge base construction tasks", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.705323688685894}]}, {"text": "Although these models are able to learn robust representations from large amounts of data, they often lack commonsense knowledge.", "labels": [], "entities": []}, {"text": "Such knowledge is rarely explicitly stated in texts but can be found in resources like PPDB ( or WordNet.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.9368106126785278}, {"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9722805619239807}]}, {"text": "Combining neural methods with symbolic commonsense knowledge, for instance in the form of implication rules, is in the focus of current research.", "labels": [], "entities": []}, {"text": "A recent approach) regularizes entity-tuple and relation embeddings via first-order logic rules.", "labels": [], "entities": []}, {"text": "To this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule.", "labels": [], "entities": []}, {"text": "This approach does not scale beyond only a few entity-tuples and rules.", "labels": [], "entities": []}, {"text": "For example, propositionalizing the rule \u2200x : isMan(x) \u21d2 isMortal(x) would result in a very large number of loss terms on a large database.", "labels": [], "entities": []}, {"text": "In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts.", "labels": [], "entities": []}, {"text": "This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entity pairs.", "labels": [], "entities": []}, {"text": "It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space.", "labels": [], "entities": []}, {"text": "In the example given above, if we require that every component of the vector representation of isMan is smaller than the corresponding component of relation isMortal, then we can show that the rule holds for any nonnegative representation of an entity-tuple.", "labels": [], "entities": []}, {"text": "Hence our method avoids the need for separate loss terms for every ground atom resulting from propositionalizing rules.", "labels": [], "entities": []}, {"text": "In statistical relational learning this type of approach is often referred to as lifted inference or learning because it deals with groups of random variables at a first-order level.", "labels": [], "entities": [{"text": "lifted inference", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.9370686411857605}]}, {"text": "In this sense our approach is a lifted form of rule injection.", "labels": [], "entities": [{"text": "rule injection", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.8210873901844025}]}, {"text": "This allows for imposing large numbers of rules while learning distributed representations of relations and entity-tuples.", "labels": [], "entities": []}, {"text": "Besides drastically lower computation time, an important advantage of our method over is that when these constraints are satisfied, the injected rules always hold, even for unseen but inferred facts.", "labels": [], "entities": []}, {"text": "While the method presented here only deals with implications and not general first-order rules, it does not rely on the assumption of independence between relations, and is hence more generally applicable.", "labels": [], "entities": []}, {"text": "Our contributions are fourfold: (i) we develop a very efficient way of regularizing relation representations to incorporate first-order logic implications ( \u00a73), (ii) we reveal that, against expectation, mapping entity-tuple embeddings to non-negative space does not hurt but instead improves the generalization ability of our model ( \u00a75.1) (iii) we show improvements on a knowledge base completion task by injecting mined commonsense rules from WordNet ( \u00a75.3), and finally (iv) we give a qualitative analysis of the results, demonstrating that implication constraints are indeed satisfied in an asymmetric way and result in a substantially increased structuring of the relation embedding space ( \u00a75.6).", "labels": [], "entities": [{"text": "knowledge base completion task", "start_pos": 373, "end_pos": 403, "type": "TASK", "confidence": 0.7046363428235054}, {"text": "WordNet", "start_pos": 446, "end_pos": 453, "type": "DATASET", "confidence": 0.9537645578384399}]}], "datasetContent": [{"text": "We now present our experimental results.", "labels": [], "entities": []}, {"text": "We start by describing the experimental setup and hyperparameters.", "labels": [], "entities": []}, {"text": "Before turning to the injection of rules, we compare model F with model FS, and show that restricting the tuple embedding space has a regularization effect, rather than limiting the expressiveness of the model ( \u00a75.1 leads to an improved precision ( \u00a75.3).", "labels": [], "entities": [{"text": "precision", "start_pos": 238, "end_pos": 247, "type": "METRIC", "confidence": 0.997911274433136}]}, {"text": "We proceed with a visual illustration of the relation embeddings with and without injected rules ( \u00a75.4), provide details on time efficiency of the lifted rule injection method ( \u00a75.5), and show that it correctly captures the asymmetry of implication rules ( \u00a75.6).", "labels": [], "entities": []}, {"text": "All models were implemented in TensorFlow (.", "labels": [], "entities": []}, {"text": "We use the hyperparameters of, with k = 100 hidden dimensions and a weight of \u03b1 = 0.01 for the L 2 regularization loss.", "labels": [], "entities": []}, {"text": "We use ADAM) for optimization with an initial learning rate of 0.005 and a mini-batch size of 8192.", "labels": [], "entities": []}, {"text": "The embeddings are initialized by sampling uniformly from [\u22120.1, 0.1] and we use\u02dc\u03b2use\u02dc use\u02dc\u03b2 = 0.1 for the implication loss throughout our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Weighted mean average precision for our  reimplementation of the matrix factorization model  (F) compared to restricting the entity-pair space (FS)  and injecting WordNet rules (FSL). Model F results  by Riedel et al. (2013) are denoted as R13-F.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.65707927942276}]}, {"text": " Table 3: Average of \u03c3(r  q t) over all inferred facts r q , t p for tuples t p from training items for relation r p ,  and vice versa, for Wordnet implications r p \u21d2 r q , and model FSL (injected rules) vs. model FS (no rules).", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9935023188591003}, {"text": "FSL", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.9725043773651123}, {"text": "FS", "start_pos": 214, "end_pos": 216, "type": "METRIC", "confidence": 0.8855854272842407}]}]}