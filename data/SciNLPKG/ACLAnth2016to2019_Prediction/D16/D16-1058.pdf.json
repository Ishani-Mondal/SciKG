{"title": [{"text": "Attention-based LSTM for Aspect-level Sentiment Classification", "labels": [], "entities": [{"text": "Aspect-level Sentiment Classification", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.7883723974227905}]}], "abstractContent": [{"text": "Aspect-level sentiment classification is a fine-grained task in sentiment analysis.", "labels": [], "entities": [{"text": "Aspect-level sentiment classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9108218550682068}, {"text": "sentiment analysis", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9535776078701019}]}, {"text": "Since it provides more complete and in-depth results, aspect-level sentiment analysis has received much attention these years.", "labels": [], "entities": [{"text": "aspect-level sentiment analysis", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.8901524742444357}]}, {"text": "In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect.", "labels": [], "entities": []}, {"text": "For instance, \"The appetizers are ok, but the service is slow.\", for aspect taste, the polarity is positive while for service, the polarity is negative.", "labels": [], "entities": []}, {"text": "Therefore, it is worthwhile to explore the connection between an aspect and the content of a sentence.", "labels": [], "entities": []}, {"text": "To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification.", "labels": [], "entities": [{"text": "aspect-level sentiment classification", "start_pos": 78, "end_pos": 115, "type": "TASK", "confidence": 0.7581674655278524}]}, {"text": "The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input.", "labels": [], "entities": []}, {"text": "We experiment on the SemEval 2014 dataset and results show that our model achieves state-of-the-art performance on aspect-level sentiment classification.", "labels": [], "entities": [{"text": "SemEval 2014 dataset", "start_pos": 21, "end_pos": 41, "type": "DATASET", "confidence": 0.7437238196531931}, {"text": "aspect-level sentiment classification", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.6236557364463806}]}], "introductionContent": [{"text": "Sentiment analysis, also known as opinion mining, is a key NLP task that receives much attention these years.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9430807530879974}, {"text": "opinion mining", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8226710259914398}]}, {"text": "Aspect-level sentiment analysis is a fine-grained task that can provide complete and in-depth results.", "labels": [], "entities": [{"text": "Aspect-level sentiment analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9210346937179565}]}, {"text": "In this paper, we deal with aspect-level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect.", "labels": [], "entities": [{"text": "aspect-level sentiment classification", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.7024116317431132}]}, {"text": "For example, the sentiment polarity of \"Staffs are not that friendly, but the taste covers all.\" will be positive if the aspect is food but negative when considering the aspect service.", "labels": [], "entities": []}, {"text": "Polarity could be opposite when different aspects are considered.", "labels": [], "entities": []}, {"text": "Neural networks have achieved state-of-the-art performance in a variety of NLP tasks such as machine translation (, paraphrase identification (), question answering ( and text summarization (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7830132842063904}, {"text": "paraphrase identification", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.8563728928565979}, {"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.8877856731414795}, {"text": "text summarization", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.726309210062027}]}, {"text": "However, neural network models are still in infancy to deal with aspectlevel sentiment classification.", "labels": [], "entities": [{"text": "aspectlevel sentiment classification", "start_pos": 65, "end_pos": 101, "type": "TASK", "confidence": 0.7842111190160116}]}, {"text": "In some works, target dependent sentiment classification can be benefited from taking into account target information, such as in Target-Dependent LSTM (TD-LSTM) and Target-Connection LSTM (TC-LSTM)).", "labels": [], "entities": [{"text": "target dependent sentiment classification", "start_pos": 15, "end_pos": 56, "type": "TASK", "confidence": 0.6861777976155281}]}, {"text": "However, those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect-level classification.", "labels": [], "entities": [{"text": "aspect-level classification", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.8120006322860718}]}, {"text": "Attention has become an effective mechanism to obtain superior results, as demonstrated in image recognition (), machine translation (), reasoning about entailment () and sentence summarization (.", "labels": [], "entities": [{"text": "image recognition", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7887039482593536}, {"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.781074583530426}, {"text": "reasoning about entailment", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.8262609442075094}, {"text": "sentence summarization", "start_pos": 171, "end_pos": 193, "type": "TASK", "confidence": 0.7192426919937134}]}, {"text": "Even more, neural attention can improve the ability to read comprehension (.", "labels": [], "entities": []}, {"text": "In this paper, we propose an attention mechanism to enforce the model to attend to the important part of a sentence, in response to a specific aspect.", "labels": [], "entities": []}, {"text": "We design an aspect-tosentence attention mechanism that can concentrate on the key part of a sentence given the aspect.", "labels": [], "entities": []}, {"text": "We explore the potential correlation of aspect and sentiment polarity in aspect-level sentiment classification.", "labels": [], "entities": [{"text": "aspect-level sentiment classification", "start_pos": 73, "end_pos": 110, "type": "TASK", "confidence": 0.7162119646867117}]}, {"text": "In order to capture important information in response to a given aspect, we design an attentionbased LSTM.", "labels": [], "entities": []}, {"text": "We evaluate our approach on a benchmark dataset (, which contains restaurants and laptops data.", "labels": [], "entities": []}, {"text": "The main contributions of our work can be summarized as follows: \u2022 We propose attention-based Long Short-Term memory for aspect-level sentiment classification.", "labels": [], "entities": [{"text": "aspect-level sentiment classification", "start_pos": 121, "end_pos": 158, "type": "TASK", "confidence": 0.760680099328359}]}, {"text": "The models are able to attend different parts of a sentence when different aspects are concerned.", "labels": [], "entities": []}, {"text": "Results show that the attention mechanism is effective.", "labels": [], "entities": []}, {"text": "\u2022 Since aspect plays a key role in this task, we propose two ways to take into account aspect information during attention: one way is to concatenate the aspect vector into the sentence hidden representations for computing attention weights, and another way is to additionally append the aspect vector into the input word vectors.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results indicate that our approach can improve the performance compared with several baselines, and further examples demonstrate the attention mechanism works well for aspect-level sentiment classification.", "labels": [], "entities": [{"text": "aspect-level sentiment classification", "start_pos": 183, "end_pos": 220, "type": "TASK", "confidence": 0.7769376834233602}]}, {"text": "The rest of our paper is structured as follows: Section 2 discusses related works, Section 3 gives a detailed description of our attention-based proposals, Section 4 presents extensive experiments to justify the effectiveness of our proposals, and Section 5 summarizes this work and the future direction.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the proposed model to aspect-level sentiment classification.", "labels": [], "entities": [{"text": "aspect-level sentiment classification", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7312367161115011}]}, {"text": "In our experiments, all word vectors are initialized by Glove 1).", "labels": [], "entities": [{"text": "Glove", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9727821350097656}]}, {"text": "The word embedding vectors are pre-trained on an unlabeled corpus whose size is about 840 billion.", "labels": [], "entities": []}, {"text": "The other parameters are initialized by sampling from a uniform distribution U (\u2212\u03f5, \u03f5).", "labels": [], "entities": []}, {"text": "The dimension of word vectors, aspect embeddings and the size of hidden layer are 300.", "labels": [], "entities": []}, {"text": "The length of attention weights is the same as the length of sentence.", "labels": [], "entities": []}, {"text": "Theano () is used for implementing our neural network models.", "labels": [], "entities": []}, {"text": "We trained all models with a batch size of 25 examples, and a momentum of 0.9, L 2 -regularization weight of 0.001 and initial learning rate of 0.01 for AdaGrad.", "labels": [], "entities": [{"text": "L 2 -regularization weight", "start_pos": 79, "end_pos": 105, "type": "METRIC", "confidence": 0.9475298285484314}, {"text": "initial learning rate", "start_pos": 119, "end_pos": 140, "type": "METRIC", "confidence": 0.8620966275533041}, {"text": "AdaGrad", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.8651865124702454}]}, {"text": "We experiment on the dataset of SemEval 2014 Task 4 2 ().", "labels": [], "entities": [{"text": "SemEval 2014 Task 4", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7170964628458023}]}, {"text": "The dataset consists of customers reviews.", "labels": [], "entities": []}, {"text": "Each review contains a list of aspects and corresponding polarities.", "labels": [], "entities": []}, {"text": "Our aim is to identify the aspect polarity of a sentence with the corresponding aspect.", "labels": [], "entities": []}, {"text": "The statistics is presented in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2 Asp.  Positive  Negative  Neural  Train Test Train Test Train Test  Fo.  867 302 209  69  90  31  Pr.  179  51  115  28  10  1  Se.  324 101 218  63  20  3  Am.  263  76  98  21  23  8  An.  546 127 199  41  357  51  Total 2179 657 839 222 500  94", "labels": [], "entities": [{"text": "Positive  Negative  Neural  Train Test Train Test Train Test  Fo.  867 302 209", "start_pos": 15, "end_pos": 93, "type": "DATASET", "confidence": 0.8014693813664573}]}, {"text": " Table 2: Accuracy on aspect level polarity classification about", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99803227186203}, {"text": "aspect level polarity classification", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.6116997003555298}]}, {"text": " Table 3: Accuracy on aspect term polarity classification about", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99604731798172}, {"text": "aspect term polarity classification", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.7691173404455185}]}, {"text": " Table 4: Accuracy on aspect term polarity classification about", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9962180256843567}, {"text": "aspect term polarity classification", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.7688292562961578}]}]}