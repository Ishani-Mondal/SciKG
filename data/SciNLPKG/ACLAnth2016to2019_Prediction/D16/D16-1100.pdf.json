{"title": [], "abstractContent": [{"text": "This paper considers the problem of learning Chinese word embeddings.", "labels": [], "entities": []}, {"text": "In contrast to En-glish, a Chinese word is usually composed of characters, and most of the characters themselves can be further divided into components such as radicals.", "labels": [], "entities": []}, {"text": "While characters and radicals contain rich information and are capable of indicating semantic meanings of words, they have not been fully exploited by existing word embedding methods.", "labels": [], "entities": []}, {"text": "In this work, we propose multi-granularity embedding (MGE) for Chi-nese words.", "labels": [], "entities": []}, {"text": "The key idea is to make full use of such word-character-radical composition, and enrich word embeddings by further incorporating finer-grained semantics from characters and radicals.", "labels": [], "entities": []}, {"text": "Quantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning.", "labels": [], "entities": [{"text": "MGE", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.7444679141044617}, {"text": "word similarity computation", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7601439356803894}, {"text": "analogical reasoning", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.7905377745628357}]}, {"text": "Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embedding, also known as distributed word representation, is to represent each word as a realvalued low-dimensional vector, through which the semantic meaning of the word can be encoded.", "labels": [], "entities": [{"text": "distributed word representation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.6679387589295706}]}, {"text": "Recent years have witnessed tremendous success of word embedding in various NLP tasks ().", "labels": [], "entities": []}, {"text": "The basic idea behind is to learn the distributed representation of a word using its context.", "labels": [], "entities": []}, {"text": "Among existing approaches, the continuous bag-of-words model (CBOW) and Skip-Gram model are simple and effective, capable of learning word embeddings efficiently from large-scale text corpora ().", "labels": [], "entities": []}, {"text": "* Corresponding author: Peng Li.", "labels": [], "entities": []}, {"text": "Besides the success in English, word embedding has also been demonstrated to be extremely useful for Chinese language processing ().", "labels": [], "entities": [{"text": "Chinese language processing", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6929740508397421}]}, {"text": "The work on Chinese generally follows the same idea as on English, i.e., to learn the embedding of a word on the basis of its context.", "labels": [], "entities": []}, {"text": "However, in contrast to English where words are usually taken as basic semantic units, Chinese words may have a complicated composition structure of their semantic meanings.", "labels": [], "entities": []}, {"text": "More specifically, a Chinese word is often composed of several characters, and most of the characters themselves can be further divided into components such as radicals (\u90e8\u9996).", "labels": [], "entities": []}, {"text": "1 Both characters and radicals may suggest the semantic meaning of a word, regardless of its context.", "labels": [], "entities": []}, {"text": "For example, the Chinese word \"\u5403\u996d (have a meal)\" consists of two characters \" \u5403 (eat)\" and \"\u996d (meal)\", where \" \u5403 (eat)\" has the radical of \"\u53e3 (mouth)\", and \"\u996d (meal)\" the radical of \" \u9963 (food)\".", "labels": [], "entities": []}, {"text": "The semantic meaning of \"\u5403\u996d\" can be revealed by the constituent characters as well as their radicals.", "labels": [], "entities": []}, {"text": "Despite being the linguistic nature of Chinese and containing rich semantic information, such wordcharacter-radical composition has not been fully exploited by existing approaches.", "labels": [], "entities": []}, {"text": "introduced a character-enhanced word embedding model (CWE), which learns embeddings jointly for words and characters but ignores radicals. and  utilized radical information to learn better character embeddings.", "labels": [], "entities": []}, {"text": "Similarly, split characters into small components based on the Wubi method, 2 and took into account those components during the learning process.", "labels": [], "entities": []}, {"text": "In their work, however, embeddings are learned only for characters.", "labels": [], "entities": []}, {"text": "For a word, the embedding is generated by simply combining the embeddings of the constituent characters.", "labels": [], "entities": []}, {"text": "Since not all Chinese word- s are semantically compositional (e.g., transliterated words such as \"\u82cf\u6253 (soda)\"), embeddings obtained in this way maybe of low quality for these words.", "labels": [], "entities": []}, {"text": "In this paper, aiming at making full use of the semantic composition in Chinese, we propose multigranularity embedding (MGE) which learns embeddings jointly for words, characters, and radicals.", "labels": [], "entities": []}, {"text": "The framework of MGE is sketched in.", "labels": [], "entities": [{"text": "MGE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.7431231141090393}]}, {"text": "Given a word, we learn its embedding on the basis of 1) the context words (blue bars in the figure), 2) their constituent characters (green bars), and 3) the radicals found in the target word (orange bars).", "labels": [], "entities": []}, {"text": "Compared to utilizing context words alone, MGE enriches the embeddings by further incorporating finer-grained semantics from characters and radicals.", "labels": [], "entities": [{"text": "MGE", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9256501197814941}]}, {"text": "Similar ideas of adaptively using multiple levels of embeddings have also been investigated in English recently (.", "labels": [], "entities": []}, {"text": "We evaluate MGE with the benchmark tasks of word similarity computation and analogical reasoning, and demonstrate its superiority over state-ofthe-art metods.", "labels": [], "entities": [{"text": "MGE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7385128736495972}, {"text": "word similarity computation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8220522801081339}, {"text": "analogical reasoning", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7014061659574509}]}, {"text": "A qualitative analysis further shows the capability of MGE to identify finer-grained semantic meanings of words.", "labels": [], "entities": [{"text": "MGE", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9181757569313049}]}], "datasetContent": [{"text": "We evaluate MGE with the tasks of word similarity computation and analogical reasoning.", "labels": [], "entities": [{"text": "MGE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7873220443725586}, {"text": "word similarity computation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7993119955062866}, {"text": "analogical reasoning", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.8692141473293304}]}, {"text": "We select the Chinese Wikipedia Dump 4 for embedding learning.", "labels": [], "entities": [{"text": "Chinese Wikipedia Dump 4", "start_pos": 14, "end_pos": 38, "type": "DATASET", "confidence": 0.923004075884819}]}, {"text": "In preprocessing, we use the THU-LAC tool to segment the corpus.", "labels": [], "entities": []}, {"text": "Pure digit words, non-Chinese words, and words whose frequencies are less than 5 in the corpus are removed.", "labels": [], "entities": []}, {"text": "We further crawl from an online Chinese dictionary and build a character-radical index with 20,847 characters and 269 radicals.", "labels": [], "entities": []}, {"text": "We use this index to detect the radical of each character in the corpus.", "labels": [], "entities": []}, {"text": "As such, we get a training set with 72,602,549 words, 277,200 unique words, 8,410 unique characters, and 256 unique radicals.", "labels": [], "entities": []}, {"text": "Finally, we use THULAC to perform Chinese POS tagging on the training set and identify all entity names.", "labels": [], "entities": [{"text": "THULAC", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.5284847021102905}, {"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.6663163751363754}]}, {"text": "For these entity names, neither characters nor radicals are considered during learning.", "labels": [], "entities": []}, {"text": "Actually,  categorized non-compositional Chinese words into three groups, i.e., transliterated words, single-morpheme multi-character words, and entity names.", "labels": [], "entities": []}, {"text": "In their work, they used a humanannotated corpus, manually determining each word to be split or not.", "labels": [], "entities": []}, {"text": "Since human annotation could be time-consuming and labor intensive, we just consider automatically identified entity names.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on word similarity computation.", "labels": [], "entities": [{"text": "word similarity computation", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8530350724856058}]}]}