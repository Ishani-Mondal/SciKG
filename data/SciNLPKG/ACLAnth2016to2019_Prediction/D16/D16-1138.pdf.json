{"title": [{"text": "Online Segment to Segment Neural Transduction", "labels": [], "entities": [{"text": "Segment to Segment Neural Transduction", "start_pos": 7, "end_pos": 45, "type": "TASK", "confidence": 0.5766384124755859}]}], "abstractContent": [{"text": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read.", "labels": [], "entities": []}, {"text": "By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginaliza-tion of the latent segmentation during training , and during decoding beam search is employed to find the best alignment path together with the predicted output sequence.", "labels": [], "entities": []}, {"text": "Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output.", "labels": [], "entities": []}, {"text": "It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation.", "labels": [], "entities": []}, {"text": "Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.", "labels": [], "entities": [{"text": "abstractive sentence summarization", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.6015415589014689}]}], "introductionContent": [{"text": "The problem of mapping from one sequence to another is an importance challenge of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.6480678717295328}]}, {"text": "Common applications include machine translation and abstractive sentence summarisation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8307837545871735}, {"text": "abstractive sentence summarisation", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.6427396635214487}]}, {"text": "Traditionally this type of problem has been tackled by a combination of hand-crafted features, alignment models, segmentation heuristics, and language models, all of which are tuned separately.", "labels": [], "entities": []}, {"text": "The recently introduced encoder-decoder paradigm has proved very successful for machine translation, where an input sequence is encoded into a fixed-length vector and an output sequence is then decoded from said vector).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8097521662712097}]}, {"text": "This architecture is appealing, as it makes it possible to tackle the problem of sequenceto-sequence mapping by training a large neural network in an end-to-end fashion.", "labels": [], "entities": [{"text": "sequenceto-sequence mapping", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.7598451673984528}]}, {"text": "However it is difficult fora fixed-length vector to memorize all the necessary information of an input sequence, especially for long sequences.", "labels": [], "entities": []}, {"text": "Often a very large encoding needs to be employed in order to capture the longest sequences, which invariably wastes capacity and computation for short sequences.", "labels": [], "entities": []}, {"text": "While the attention mechanism of goes someway to address this issue, it still requires the full input to be seen before any output can be produced.", "labels": [], "entities": []}, {"text": "In this paper we propose an architecture to tackle the limitations of the vanilla encoder-decoder model, a segment to segment neural transduction model (SSNT) that learns to generate and align simultaneously.", "labels": [], "entities": []}, {"text": "Our model is inspired by the HMM word alignment model proposed for statistical machine translation (;); we impose a monotone restriction on the alignments but incorporate recurrent dependencies on the input which enable rich locally non-monotone alignments to be captured.", "labels": [], "entities": [{"text": "HMM word alignment", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7006608049074808}, {"text": "statistical machine translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.677019457022349}]}, {"text": "This is similar to the sequence transduction model of, but we propose alignment distributions which are parameterised separately, making the model more flexible and allowing online inference.", "labels": [], "entities": []}, {"text": "Our model introduces a latent segmentation which determines correspondences between tokens of the input sequence and those of the output sequence.", "labels": [], "entities": []}, {"text": "The aligned hidden states of the encoder and decoder are used to predict the next output token and to calculate the transition probability of the alignment.", "labels": [], "entities": []}, {"text": "We carefully design the input and output RNNs such that they independently update their respective hidden states.", "labels": [], "entities": []}, {"text": "This enables us to derive an exact dynamic programme to marginalize out the hidden segmentation during training and an efficient beam search to generate online the best alignment path together with the output sequence during decoding.", "labels": [], "entities": []}, {"text": "Unlike previous recurrent segmentation models that only capture dependencies in the input (), our segmentation model is able to capture unbounded dependencies in both the input and output sequences while still permitting polynomial inference.", "labels": [], "entities": []}, {"text": "While attentive models treat the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out.", "labels": [], "entities": []}, {"text": "Our model is general and could be incorporated into any RNN-based encoder-decoder architecture, such as Neural Turing Machines (), memory networks ( or stackbased networks (, enabling such models to process data online.", "labels": [], "entities": []}, {"text": "We conduct experiments on two different transduction tasks, abstractive sentence summarisation (sequence to sequence mapping at word level) and morphological inflection generation (sequence to sequence mapping at character level).", "labels": [], "entities": [{"text": "abstractive sentence summarisation", "start_pos": 60, "end_pos": 94, "type": "TASK", "confidence": 0.6507170498371124}, {"text": "morphological inflection generation", "start_pos": 144, "end_pos": 179, "type": "TASK", "confidence": 0.6722349226474762}]}, {"text": "We evaluate our proposed algorithms in both the online setting, where the input is encoded with a unidirectional LSTM, and where the whole input is available such that it can be encoded with a bidirectional network.", "labels": [], "entities": []}, {"text": "The experimental results demonstrate the effectiveness of SSNT -it consistently output performs the baseline encoder-decoder approach while requiring significantly smaller hidden layers, thus showing that the segmentation model is able to learn to break one large transduction task into a series of smaller encodings and decodings.", "labels": [], "entities": [{"text": "SSNT", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.9772177338600159}]}, {"text": "When bidirectional encodings are used the segmentation model outperforms an attention-based benchmark.", "labels": [], "entities": []}, {"text": "Quali- tative analysis shows that the alignments found by our model are highly intuitive and demonstrates that the model learns to read ahead the required number of tokens before producing output.", "labels": [], "entities": [{"text": "Quali- tative", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6174759169419607}]}], "datasetContent": [{"text": "We evaluate the effectiveness of our model on two representative natural language processing tasks, sentence compression and morphological inflection.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7705853879451752}]}, {"text": "The primary aim of this evaluation is to assess whether our proposed architecture is able to outperform the baseline encoder-decoder model by overcoming its encoding bottleneck.", "labels": [], "entities": []}, {"text": "We further benchmark our results against an attention model in order to determine whether our alternative alignment strategy is able to provide similar benefits while processing the input online.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE F1 scores on the sentence sum- marisation test set. Seq2seq refers to the vanilla  encoder-decoder and attention denotes the attention- based model. SSNT denotes our model with align- ment transition probability modelled as geometric  distribution. SSNT+ refers to our model with tran- sition probability modelled using neural networks.  The prefixes uni-and bi-denote using unidirectional  and bidirectional encoder LSTMs, respectively.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9401017427444458}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.6277446150779724}, {"text": "sentence sum- marisation test set", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6295278519392014}]}, {"text": " Table 2: Perplexity on the validation set with 172k  sentence-summary pairs.", "labels": [], "entities": []}, {"text": " Table 3: Average accuracy over all the morpho- logical inflection datasets. The baseline results for  Seq2Seq variants are taken from (Faruqui et al.,  2016).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9949663281440735}]}, {"text": " Table 4: Comparison of the performance of our  model (biSSNT+) against the previous state-of-the- art on each morphological inflection dataset.", "labels": [], "entities": []}]}