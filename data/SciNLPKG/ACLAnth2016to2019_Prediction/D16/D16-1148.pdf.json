{"title": [{"text": "Analyzing Framing through the Casts of Characters in the News", "labels": [], "entities": [{"text": "Analyzing Framing through the Casts of Characters in the News", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8453872859477997}]}], "abstractContent": [{"text": "We present an unsupervised model for the discovery and clustering of latent \"personas\" (characterizations of entities).", "labels": [], "entities": [{"text": "discovery and clustering of latent \"personas\" (characterizations of entities)", "start_pos": 41, "end_pos": 118, "type": "TASK", "confidence": 0.7047694119123312}]}, {"text": "Our model simultaneously clusters documents featuring similar collections of personas.", "labels": [], "entities": []}, {"text": "We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus.", "labels": [], "entities": [{"text": "Media Frames Corpus", "start_pos": 156, "end_pos": 175, "type": "DATASET", "confidence": 0.8000292579332987}]}, {"text": "We also introduce automated model selection as a fair and robust form of feature evaluation.", "labels": [], "entities": [{"text": "automated model selection", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6336199243863424}]}], "introductionContent": [{"text": "Social science tells us that communication almost inescapably involves framing-choosing \"a few elements of perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation\".", "labels": [], "entities": []}, {"text": "Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end.", "labels": [], "entities": []}, {"text": "Framing is associated with several phenomena to which NLP has been applied, including ideology (), sentiment (Pang and, and stance (.", "labels": [], "entities": [{"text": "Framing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9404346942901611}]}, {"text": "Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issueswithout necessarily tying these to the states or intentions of authors-and the effects that such patterns may have on public opinion and policy.", "labels": [], "entities": []}, {"text": "We also note that NLP has often been used in large-scale studies of news and its relation to other social phenomena ().", "labels": [], "entities": []}, {"text": "Can framing be automatically recognized?", "labels": [], "entities": []}, {"text": "If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of everyday readers.", "labels": [], "entities": []}, {"text": "Several recent studies have begun to explore unsupervised framing analysis of political text using autoregressive and hierarchical topic models (, but most of these conceptualize framing along a single dimension.", "labels": [], "entities": [{"text": "framing analysis of political text", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.8488939046859741}]}, {"text": "Rather than trying to place individual articles on a continuum from liberal to conservative or positive to negative, we are interested in discovering broad-based patterns in the ways in which the media communicate about issues.", "labels": [], "entities": []}, {"text": "Here, our focus is on the narratives found in news stories, specifically the participants in those stories.", "labels": [], "entities": []}, {"text": "Insofar as journalists make use of archetypal narratives (e.g., the struggle of an individual against a more powerful adversary), we expect to see recurring representations of characters in these narratives ().", "labels": [], "entities": []}, {"text": "A classic example is the contrast between \"worthy\" and \"unworthy\" victims).", "labels": [], "entities": []}, {"text": "More recently, Glenn Greenwald has pointed out how he was repeatedly characterized as an activist or blogger, rather than a journalist during his reporting on the NSA).", "labels": [], "entities": []}, {"text": "Our model builds on the \"Dirichlet persona model\" (DPM) introduced by for the unsupervised discovery of what they called \"personas\" in short film summaries (e.g., the \"dark hero\").", "labels": [], "entities": []}, {"text": "As in the DPM, we operationalize personas as mixture of textually-expressed characteristics: what they do, what is done to them, and their descriptive attributes.", "labels": [], "entities": []}, {"text": "We begin by providing a description of our full model, after which we highlight the differences from the DPM.", "labels": [], "entities": [{"text": "DPM", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.8509908318519592}]}, {"text": "This paper's main contributions are: \u2022 We strengthen the DPM's assumptions about the combinations of personas found in documents, applying a Dirichlet process prior to infer patterns of coocurrence ( \u00a73).", "labels": [], "entities": []}, {"text": "The result is a clustering of documents based on the collections of personas they use, discovered simultaneously with those personas.", "labels": [], "entities": []}, {"text": "\u2022 Going beyond named characters, we allow Bamman-style personas to account for entities like institutions, objects, and concepts ( \u00a75).", "labels": [], "entities": []}, {"text": "\u2022 We find that our model produces interpretable clusters that provide insight into our corpus of immigration news articles ( \u00a76).", "labels": [], "entities": []}, {"text": "\u2022 We propose anew kind of evaluation based on Bayesian optimization.", "labels": [], "entities": []}, {"text": "Given a supervised learning problem, we treat the inclusion of a candidate feature set (here, personas) as a hyperparameter to be optimized alongside other hyperparameters ( \u00a77).", "labels": [], "entities": []}, {"text": "\u2022 In the case of U.S. news stories about immigration, we find that personas are, in many cases, helpful for automatically inferring the coarsegrained framing and tone employed in apiece of text, as defined in the Media Frames Corpus () ( \u00a77).", "labels": [], "entities": [{"text": "Media Frames Corpus", "start_pos": 213, "end_pos": 232, "type": "DATASET", "confidence": 0.8484712640444437}]}], "datasetContent": [{"text": "The Media Frames Corpus (MFC; consists of annotations for approximately 4,200 articles about immigration taken from 13 U.S. newspapers over the years 1980-2012.", "labels": [], "entities": [{"text": "Media Frames Corpus (MFC;", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.8212121427059174}]}, {"text": "The annotations for these articles are in terms of a set of 15 generalpurpose \"framing dimensions\" (such as Politics and Legality), developed to be broadly applicable to a variety of issues, and to be recognizable in text (by trained annotators).", "labels": [], "entities": [{"text": "Politics and Legality)", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.646967351436615}]}, {"text": "Each article has been annotated with a \"primary frame\" (the overall dominant aspect of immigration being emphasized), as well as an overall \"tone\" (pro, neutral, or anti), which is the extent to which a pro-immigration advocate would like to seethe article in print, without implying any any stance taken by the author.", "labels": [], "entities": []}, {"text": "The MFC contains at least two independent annotations for each article; agreement on the primary frame and tone was established through discussion in cases of initial disagreement.", "labels": [], "entities": []}, {"text": "A complete list of these framing dimensions is given in the supplementary material.", "labels": [], "entities": [{"text": "framing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9453955292701721}]}, {"text": "In order to train our model on a larger collection of articles, we use the original corpus of articles from which the annotated articles in the MFC were drawn.", "labels": [], "entities": []}, {"text": "This produces a corpus of approximately 37,000 articles about immigration; we train the persona model on this larger dataset, only using the smaller set for evaluation on a secondary task.", "labels": [], "entities": []}, {"text": "Note that the MFC annotations are not used by our model; rather, we hypothesize that the personas it discovers may serve as features to help predict framing-this serves as one of our evaluations ( \u00a77).", "labels": [], "entities": []}, {"text": "We evaluate personas as features for automatic analysis of framing and tone, as defined in the MFC ( \u00a74).", "labels": [], "entities": [{"text": "automatic analysis of framing and tone", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.6918812890847524}, {"text": "MFC", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.7995209097862244}]}, {"text": "Specifically, we build multi-class text classifiers (separately) for the primary frame and the tone of a news article, for which there are 15 and 3 classes, respectively.", "labels": [], "entities": []}, {"text": "Because there are only a few thousand annotated articles, we applied 10-fold cross-validation to estimate performance.", "labels": [], "entities": []}, {"text": "Features are derived from our model by considering each persona and each story cluster as a potential feature.", "labels": [], "entities": []}, {"text": "A document's feature values for story types are the proportion of samples in which it was assigned to each cluster.", "labels": [], "entities": []}, {"text": "Persona feature values are similarly derived by the proportion of samples in which each entity was assigned to each persona, with the persona values for each entity in each document summed into a single set of persona values per: Evaluation using a direct comparison to a simple baseline.", "labels": [], "entities": []}, {"text": "Each model uses the union of listed features.", "labels": [], "entities": []}, {"text": "(W = unigrams and bigrams, P 1 = personas from DPM, P 2 = personas from our model, S = story clusters; MF = always predict most frequent class.)", "labels": [], "entities": [{"text": "MF", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.8828497529029846}]}, {"text": "* indicates a statistically significant difference compared to the (W) baseline (p<0.05). document.", "labels": [], "entities": []}, {"text": "We did not use the topics (z) discovered by our model as features.", "labels": [], "entities": []}, {"text": "For the first experiment, we train independent multiclass logistic regression classifiers for predicting primary frame and tone.", "labels": [], "entities": []}, {"text": "We consider adding persona and/or story cluster features to baseline classifiers based only on unigrams and bigrams with binarized counts, a simple but robust baseline (.", "labels": [], "entities": []}, {"text": "In all cases, we use L 1 regularization and use 5-fold cross validation within each split's training set to determine the strength of regularization.", "labels": [], "entities": []}, {"text": "We then repeat this for each of the 10 folds, thereby producing one prediction (of primary frame and tone) for every annotated article.", "labels": [], "entities": []}, {"text": "The results of this experiment are given in; for predicting the primary frame, classifiers that used persona and/or story cluster features achieve higher accuracy than the bag-of-words baseline (W); the classifier using personas from our model but not story clusters is significantly better than the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9979538917541504}]}, {"text": "The enhanced models are also more compact, on average, using fewer effective features.", "labels": [], "entities": []}, {"text": "A benefit to predicting tone is also observed, but it did not reach statistical significance.", "labels": [], "entities": [{"text": "predicting tone", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.9130044877529144}]}, {"text": "Although bag-of-n-grams models are known to be a strong baseline for text classification, researchers familiar with the extensive catalogue of features of-fered by NLP will potentially see them as a straw man.", "labels": [], "entities": [{"text": "text classification", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8549012243747711}]}, {"text": "We propose anew and more rigorous method of comparison, in which a wide range of features are offered to an automatic model selection algorithm for each of the prediction tasks, with the features to be evaluated withheld from the baseline.", "labels": [], "entities": []}, {"text": "Because no single combination of features and regularization strength is best for all situations, it is an empirical question which features are best for each task.", "labels": [], "entities": []}, {"text": "We therefore make use of Bayesian optimization (Bayesopt) to make as many modeling decisions as possible).", "labels": [], "entities": []}, {"text": "In particular, let F be the set of features that might be used as input to any text classification algorithm.", "labels": [], "entities": [{"text": "F", "start_pos": 19, "end_pos": 20, "type": "METRIC", "confidence": 0.9355831742286682}, {"text": "text classification algorithm", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.8018503983815511}]}, {"text": "Let f be anew feature that is being proposed.", "labels": [], "entities": []}, {"text": "Allow the inclusion or exclusion of each feature in the feature set to be a hyperparameter to be optimized, along with any additional decisions such as input transformations (e.g., lowercasing), and feature transformations (e.g., normalization).", "labels": [], "entities": []}, {"text": "Using an automatic model selection algorithm such as Bayesian optimization, allow the performance on the validation set to guide choices about all of these hyperparameters on each iteration, and setup two independent experiments.", "labels": [], "entities": []}, {"text": "For the first condition, A 1 , allow the algorithm access to all features in F . For the second, A 2 , allow the algorithm access to all features in F \u222a f . After R iterations of each, choose the best model or the best set of models from each of A 1 and A 2 (M 1 and M 2 , respectively), based on performance on the validation set.", "labels": [], "entities": []}, {"text": "Finally, compare the selected models in terms of performance on the test set (using an appropriate metric such as F 1 ), and examine the features included in each of the best models.", "labels": [], "entities": [{"text": "F 1", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9877728819847107}]}, {"text": "If f is a helpful feature, we should expect to see that, a) , and b), f is included in the best model(s) found by A 2 . If but f is not included in the best models from A 2 , this suggests that the performance improvement may simply be a matter of chance, and there is no evidence that f is helpful.", "labels": [], "entities": []}, {"text": "By contrast, if f is included in the best models, but is not significantly better than F 1 (M 1 ), this suggests that f is offering some value, perhaps in a more compressed form of the useful signal from other features, but does not actually offer better per-: Mean accuracy of the best three iterations from Bayesian optimization (chosen based on validation accuracy).", "labels": [], "entities": [{"text": "Mean", "start_pos": 261, "end_pos": 265, "type": "METRIC", "confidence": 0.9893584251403809}, {"text": "accuracy", "start_pos": 266, "end_pos": 274, "type": "METRIC", "confidence": 0.6017794609069824}]}, {"text": "(B = features from many NLP tools, P 1 =personas from the DPM, P 2 = personas from our model, S=story clusters.) formance.", "labels": [], "entities": []}, {"text": "For this experiment, we use the tree-structured Parzen estimator for Bayesian optimization, with L 1 -regularized logistic regression as the underlying classifier, and set R = 40.", "labels": [], "entities": [{"text": "Bayesian optimization", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.6534583419561386}]}, {"text": "In addition to the entities and story clusters identified by these models, we allow these classifiers access to a large set of features, including unigrams, bigrams, parts of speech, named entities, dependency tuples, ordinal sentiment values), multi-word expressions, supersense tags (, Brown clusters (, frame semantic features (), and topics produced by standard LDA (.", "labels": [], "entities": []}, {"text": "The inclusion or exclusion of each feature is determined automatically on each iteration, along with feature transformations (removal of rare words, lowercasing, and binary or normalized counts).", "labels": [], "entities": []}, {"text": "The baseline, denoted \"B,\" offers all features except personas and story clusters to Bayesopt; we consider adding DPM personas, our model's personas, and our model's personas and story clusters.", "labels": [], "entities": []}, {"text": "shows test-set accuracy for each setup, averaged across the three best models returned by Bayesopt.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9955224990844727}]}, {"text": "Using this more rigorous form of evaluation, approximately the same accuracy is obtained in all experimental conditions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9993839263916016}]}, {"text": "However, we can still gain insight into which features are useful by examining those selected by the best models in each condition.", "labels": [], "entities": []}, {"text": "For primary frame prediction, both personas and story clusters are included by the best models in every case where they have been offered as possible features, as are unigrams, dependency tuples, and semantic frames.", "labels": [], "entities": [{"text": "primary frame prediction", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6421202222506205}]}, {"text": "Other commonly-selected features include bigrams and part of speech tags.", "labels": [], "entities": []}, {"text": "For predicting tone, personas are only included by half of the best models, with the most common features being unigrams, bigrams, semantic frames, and Brown clusters.", "labels": [], "entities": [{"text": "predicting tone", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9411062300205231}]}, {"text": "As expected, the best models in each condition obtain better performance than the models from experiment 1, thanks to the inclusion of additional features and transformations.", "labels": [], "entities": []}, {"text": "This secondary evaluation suggests that for this task, persona features are useful in predicting the primary frame, but are unable to offer improved performance over existing features, such as semantic frames.", "labels": [], "entities": []}, {"text": "However, the fact that that both personas and story clusters are included by all the best models for predicting the primary frame suggests that they are competitive with other features, and perhaps offer useful information in a more compact form.", "labels": [], "entities": []}, {"text": "Prior to exposure to any output of our model, one of the co-authors on this paper (Gross, who has expertise in both framing and the immigration issue) prepared a list of personas he expected to frequently occur in American news coverage of immigration.", "labels": [], "entities": []}, {"text": "Given the example of the \"skilled immigrant,\" he listed 22 additional named personas, along with a few examples of things they do, things done to them, and attributes.", "labels": [], "entities": []}, {"text": "The list he prepared includes several different characterizations of immigrants (low-skilled, unauthorized, legal, citizen children, undocumented children, refugees, naturalized citizens), non-immigrant personas (U.S. workers, smugglers, politicians, officials, border patrol, vigilantes), related pairs (pro / anti advocacy groups, employers / guest workers, criminals / victims), and a few more conceptual entities (the border, bills, executive actions).", "labels": [], "entities": []}, {"text": "Of these, almost all are arguably represented in the personas we have discovered.", "labels": [], "entities": []}, {"text": "However, there is rarely a perfect one-to-one mapping: predefined personas are sometimes merged (e.g., \"the border\" and \"border patrols\") or split (e.g., legislation, employers, and various categories of immigrants).", "labels": [], "entities": []}, {"text": "Personas which don't emerge from our model include smugglers, guest workers, vigilantes, and victims of immigrant criminals.", "labels": [], "entities": [{"text": "smugglers", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.9730409979820251}]}, {"text": "On the other hand, our model proposes far more non-person entities, such as ID cards, courts, companies, jobs, and programs.", "labels": [], "entities": []}, {"text": "These partial matchings between predefined personas and the results of our model are generally identifiable by comparing the names given to the predefined personas to the the most commonly occurring mention words and attributes of our discovered personas.", "labels": [], "entities": []}, {"text": "The attributes and action words given to the predefined personas are harder to evaluate, as many of them are rare (e.g. politicians \"vacillate\") or compound phrases (e.g. low-skilled immigrants \"do jobs Americans won't do\") that tend to miss the more obvious properties captured by our model.", "labels": [], "entities": []}, {"text": "For example, the employer persona captured by our model engages in actions like hire, employ, and pay.", "labels": [], "entities": []}, {"text": "By contrast, the terms given for the predefined \"business owners\" persona are \"lobby\" and \"rely on immigrant labor.\"", "labels": [], "entities": []}, {"text": "Our unsupervised discovery of this persona can clearly be matched to the predefined persona in this case, but doesn't provide such fine-grained insight into how they might be characterized.", "labels": [], "entities": []}, {"text": "The best match between predefined and discovered personas is the U.S.-Mexican border.", "labels": [], "entities": [{"text": "U.S.-Mexican border", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.6318413615226746}]}, {"text": "Of the words given for the predefined persona, almost all are more frequently associated with border than with any other discovered persona (\"Mexican-U.S.,\" \"lawless,\" \"porous,\" \"unprotected,\" \"guarded,\" and \"militarized\").", "labels": [], "entities": []}, {"text": "The most commonly associated words discovered by our model that are missing from the predefined description include crossed, secured, southern, and closed.", "labels": [], "entities": []}, {"text": "While this qualitative evaluation helps to demonstrate the face validity of our model, it would be better to have a more comprehensive set of predefined personas, based on input from additional experts.", "labels": [], "entities": []}, {"text": "Moreover, it also illustrates the challenge of trying to match the output of an unsupervised model to expected results.", "labels": [], "entities": []}, {"text": "Not only is some merging and splitting of categories inevitable, there was a mismatch in this casein the types of entities to be described (people as opposed to more abstract entities), and the ways of describing them (rare but specific words as opposed to more generic but potentially obvious terms).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Truncated distribution over personas for the two  clusters depicted in Figure 3. IDs index into Table 1.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation using a direct comparison to a sim- ple baseline. Each model uses the union of listed features.", "labels": [], "entities": []}, {"text": " Table 4: Mean accuracy of the best three iterations from  Bayesian optimization (chosen based on validation accu- racy). (B = features from many NLP tools, P 1 =personas  from the DPM, P 2 = personas from our model, S=story  clusters.)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9382076859474182}]}]}