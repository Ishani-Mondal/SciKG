{"title": [], "abstractContent": [{"text": "In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles.", "labels": [], "entities": []}, {"text": "The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling.", "labels": [], "entities": []}, {"text": "We introduce two algorithms to infer a tree fora given sentence.", "labels": [], "entities": []}, {"text": "The first one is based on Gibbs sampling.", "labels": [], "entities": []}, {"text": "It is fast, but does not guarantee to find the most probable tree.", "labels": [], "entities": []}, {"text": "The second one is based on dynamic programming.", "labels": [], "entities": []}, {"text": "It is slower, but guarantees to find the most probable tree.", "labels": [], "entities": []}, {"text": "We provide comparison of both algorithms.", "labels": [], "entities": []}, {"text": "We combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation.", "labels": [], "entities": []}, {"text": "Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling is one of the core disciplines in natural language processing (NLP).", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7506926655769348}, {"text": "natural language processing (NLP)", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.8197294970353445}]}, {"text": "Automatic speech recognition, machine translation, optical character recognition, and other tasks strongly depend on the language model (LM).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7076662927865982}, {"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8164774477481842}, {"text": "optical character recognition", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.72725510597229}]}, {"text": "An improvement in language modeling often leads to better performance of the whole task.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7830284535884857}]}, {"text": "The goal of language modeling is to determine the joint probability of a sentence.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7205073982477188}]}, {"text": "Currently, the dominant approach is n-gram language modeling, which decomposes the joint probability into the product of conditional probabilities by using the chain rule.", "labels": [], "entities": [{"text": "n-gram language modeling", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6863005757331848}]}, {"text": "In traditional n-gram LMs the words are represented as distinct symbols.", "labels": [], "entities": []}, {"text": "This leads to an enormous number of word combinations.", "labels": [], "entities": []}, {"text": "In the last years many researchers have tried to capture words contextual meaning and incorporate it into the LMs.", "labels": [], "entities": []}, {"text": "Word sequences that have never been seen before receive high probability when they are made of words that are semantically similar to words forming sentences seen in training data.", "labels": [], "entities": [{"text": "probability", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9543836116790771}]}, {"text": "This ability can increase the LM performance because it reduces the data sparsity problem.", "labels": [], "entities": [{"text": "LM", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9705552458763123}]}, {"text": "In NLP a very common paradigm for word meaning representation is the use of the Distributional hypothesis.", "labels": [], "entities": [{"text": "word meaning representation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.8002100189526876}]}, {"text": "It suggests that two words are expected to be semantically similar if they occur in similar contexts (they are similarly distributed in the text).", "labels": [], "entities": []}, {"text": "Models based on this assumption are denoted as distributional semantic models (DSMs).", "labels": [], "entities": []}, {"text": "Recently, semantically motivated LMs have begun to surpass the ordinary n-gram LMs.", "labels": [], "entities": []}, {"text": "The most commonly used architectures are neural network LMs () and class-based LMs.", "labels": [], "entities": []}, {"text": "Classbased LMs are more related to this work thus we investigate them deeper.", "labels": [], "entities": []}, {"text": "introduced class-based LMs of English.", "labels": [], "entities": []}, {"text": "Their unsupervised algorithm searches classes consisting of words that are most probable in the given context (one word window in both directions).", "labels": [], "entities": []}, {"text": "However, the computational complexity of this algorithm is very high.", "labels": [], "entities": []}, {"text": "This approach was later extended by to improve the complexity and to work with wider context.", "labels": [], "entities": []}, {"text": "used the same idea and introduced Latent Words Language Model (LWLM), where word classes are latent variables in a graphical model.", "labels": [], "entities": []}, {"text": "They apply Gibbs sampling or the expectation maximization algorithm to discover the word classes that are most probable in the context of surrounding word classes.", "labels": [], "entities": []}, {"text": "A similar approach was presented in, where the word clusters derived from various semantic spaces were used to improve LMs.", "labels": [], "entities": []}, {"text": "In above mentioned approaches, the meaning of a word is inferred from the surrounding words independently of their relation.", "labels": [], "entities": []}, {"text": "An alternative approach is to derive contexts based on the syntactic relations the word participates in.", "labels": [], "entities": []}, {"text": "Such syntactic contexts are automatically produced by dependency parse-trees.", "labels": [], "entities": []}, {"text": "Resulting word representations are usually less topical and exhibit more functional similarity (they are more syntactically oriented) as shown in.", "labels": [], "entities": []}, {"text": "Dependency-based methods for syntactic parsing have become increasingly popular in NLP in the last years (.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7843893468379974}]}, {"text": "showed that these methods are promising direction of improving LMs.", "labels": [], "entities": [{"text": "LMs", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9706103801727295}]}, {"text": "Recently, unsupervised algorithms for dependency parsing appeared in offering new possibilities even for poorly-resourced languages.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.8430480360984802}]}, {"text": "In this work we introduce anew DSM that uses tree-based context to create word roles.", "labels": [], "entities": []}, {"text": "The word role contains the words that are similarly distributed over similar tree-based contexts.", "labels": [], "entities": []}, {"text": "The word role encodes the semantic and syntactic properties of a word.", "labels": [], "entities": []}, {"text": "We do not rely on parse trees as a prior knowledge, but we jointly learn the tree structures and word roles.", "labels": [], "entities": []}, {"text": "Our model is a soft clustering, i.e. one word maybe present in several roles.", "labels": [], "entities": []}, {"text": "Thus it is theoretically able to capture the word polysemy.", "labels": [], "entities": []}, {"text": "The learned structure is used as a LM, where each word role is conditioned on its parent role.", "labels": [], "entities": []}, {"text": "We present the unsupervised algorithm that discovers the tree structures only from the distribution of words in a training corpus (i.e. no labeled data or external sources of information are needed).", "labels": [], "entities": []}, {"text": "In our work we were inspired by class-based LMs, unsupervised dependency parsing, and tree-based DSMs (.", "labels": [], "entities": [{"text": "unsupervised dependency parsing", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.694450835386912}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start with the definition of our model (Section 2).", "labels": [], "entities": []}, {"text": "The process of learning the hidden sentence structures is explained in Section 3.", "labels": [], "entities": []}, {"text": "We introduce two algorithms for searching the most probable tree fora given sentence (Section 4).", "labels": [], "entities": []}, {"text": "The experimental results on English and Czech corpora are presented in Section 6.", "labels": [], "entities": []}, {"text": "We conclude in Section 7 and offer some directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present experiments with LTLM on two languages, English (EN) and Czech (CS).", "labels": [], "entities": []}, {"text": "As a training corpus we use CzEng 1.0 (Bojar et al., 2012) of the sentence-parallel Czech-English corpus.", "labels": [], "entities": []}, {"text": "We choose this corpus because it contains multiple domains, it is of reasonable length, and it is parallel so we can easily provide comparison between both languages.", "labels": [], "entities": []}, {"text": "The corpus is divided into 100 similarly-sized sections.", "labels": [], "entities": []}, {"text": "We use parts 0-97 for training, the part 98 as a development set, and the last part 99 for testing.", "labels": [], "entities": []}, {"text": "We have removed all sentences longer than 30 words.", "labels": [], "entities": []}, {"text": "The reason was that the complexity of the learning phase and the process of searching most probable trees depends on the length of sentences.", "labels": [], "entities": []}, {"text": "It has led to removing approximately a quarter of all sentences.", "labels": [], "entities": []}, {"text": "The corpus is available in a tokenized form so the only preprocessing step we use is lowercasing.", "labels": [], "entities": []}, {"text": "We keep the vocabulary of 100,000 most frequent words in the corpus for both languages.", "labels": [], "entities": []}, {"text": "The less frequent words were replaced by the symbol <unk>.", "labels": [], "entities": []}, {"text": "Statistics for the final corpora are shown in.", "labels": [], "entities": []}, {"text": "We measure the quality of LTLM by perplexity that is the standard measure used for LMs.", "labels": [], "entities": []}, {"text": "Perplexity is a measure of uncertainty.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9618637561798096}]}, {"text": "The lower perplexity means the better predictive ability of the LM.: Corpora statistics.", "labels": [], "entities": []}, {"text": "OOV rate denotes the out-of-vocabulary rate.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.977048933506012}]}, {"text": "During the process of parameter estimation we measure the perplexity of joint probability of sentences and their trees defined as PPX(P (w, G)) = N 1 P (w,G) , where N is the number of all words in the training data w.", "labels": [], "entities": []}, {"text": "As we describe in Section 3, there are two approaches for the parameter estimation of LTLM.", "labels": [], "entities": []}, {"text": "During our experiments, we found that the perposition strategy of training has the ability to converge faster, but to a worse solution compared to the per-sentence strategy which converges slower, but to a better solution.", "labels": [], "entities": []}, {"text": "We train LTLM by 500 iterations of the perposition sampling followed by another 500 iterations of the per-sentence sampling.", "labels": [], "entities": []}, {"text": "This proves to be effi-   We present the models with 10, 20, 50, 100, 200, 500, and 1000 roles.", "labels": [], "entities": []}, {"text": "The higher role cardinality models were not possible to create because of the very high computational requirements.", "labels": [], "entities": []}, {"text": "Similarly to the training of LTLM, the non-deterministic inference uses 100 iterations of per-position sampling followed by 100 iterations of per-sentence sampling.", "labels": [], "entities": []}, {"text": "In the following experiments we measure how well LTLM generalizes the learned patterns, i.e. how well it works on the previously unseen data.", "labels": [], "entities": []}, {"text": "Again, we measure the perplexity, but of probability P (w) for mutual comparison with different LMs that are based on different architectures (PPX(P (w)) = N 1 P (w) ).", "labels": [], "entities": []}, {"text": "To show the strengths of LTLM we compare it with several state-of-the-art LMs.", "labels": [], "entities": []}, {"text": "We experiment with Modified Kneser-Ney (MKN) interpolation: Perplexity results on the test data for LTLMs and STLMs with different number of roles.", "labels": [], "entities": []}, {"text": "Deterministic inference is denoted as det. and non-deterministic inference as non-det.", "labels": [], "entities": []}, {"text": "We use the same architecture as for LTLM and experiment with two approaches to represent the roles.", "labels": [], "entities": []}, {"text": "Firstly, the roles are given by the part-of-speech tag (denoted as PoS STLM).", "labels": [], "entities": []}, {"text": "No training is required, all information come from CzEng corpus.", "labels": [], "entities": [{"text": "CzEng corpus", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9455280900001526}]}, {"text": "Secondly, we learn the roles using the same algorithm as for LTLM.", "labels": [], "entities": []}, {"text": "The only difference is that the trees are kept unchanged.", "labels": [], "entities": []}, {"text": "Note that both deterministic and non-deterministic inference perform almost the same in this model so we do not distinguish between them.", "labels": [], "entities": []}, {"text": "We combine baseline 4-gram MKN model with other models via linear combination (in the tables denoted by the symbol +) that is simple but very efficient technique to combine LMs.", "labels": [], "entities": []}, {"text": "Final probability is then expressed as In the case of MKN the probability P MKN is the probability of a word w s,i conditioned by 3 previous words with MKN smoothing.", "labels": [], "entities": []}, {"text": "For LTLM or STLM this probability is defined as We use the expectation maximization algorithm ( for the maximum likelihood estimate of \u03bb parameter on the development part of the corpus.", "labels": [], "entities": []}, {"text": "The influence of the number of roles on the perplexity is shown in  From the tables we can see several important findings.", "labels": [], "entities": []}, {"text": "Standalone LTLM performs worse than MKN on both languages, however their combination leads to dramatic improvements compared with other LMs.", "labels": [], "entities": []}, {"text": "Best results are achieved by 4-gram MKN interpolated with 1000 roles LTLM and the deterministic inference.", "labels": [], "entities": []}, {"text": "The perplexity was improved by approximately 46% on English and 49% on Czech compared with standalone MKN.", "labels": [], "entities": []}, {"text": "The deterministic inference outperformed the nondeterministic one in all cases.", "labels": [], "entities": []}, {"text": "LTLM also signifi-: Ten most probable word substitutions on each position in the sentence \"Everything has beauty, but not everyone sees it.\" produced by 1000 roles LTLM with the deterministic inference.", "labels": [], "entities": [{"text": "LTLM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8477367162704468}]}, {"text": "cantly outperformed STLM where the syntactic dependency trees were provided as a prior knowledge.", "labels": [], "entities": [{"text": "STLM", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.7342118620872498}]}, {"text": "The joint learning of syntax and semantics of a sentence proved to be more suitable for predicting the words.", "labels": [], "entities": []}, {"text": "An in-depth analysis of semantic and syntactic properties of LTLM is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "For better insight into the behavior of LTLM, we show the most probable word substitutions for one selected sentence (see).", "labels": [], "entities": []}, {"text": "We can see that the original words are often on the front positions.", "labels": [], "entities": []}, {"text": "Also it seems that LTLM is more syntactically oriented, which confirms claims from (, but to draw such conclusions a deeper analysis is required.", "labels": [], "entities": []}, {"text": "The properties of the model strongly depends on the number of distinct roles.", "labels": [], "entities": []}, {"text": "We experimented with maximally 1000 roles.", "labels": [], "entities": []}, {"text": "To catch the meaning of various words in natural language, more roles maybe needed.", "labels": [], "entities": [{"text": "catch the meaning of various words in natural language", "start_pos": 3, "end_pos": 57, "type": "TASK", "confidence": 0.8810018632147048}]}, {"text": "However, with our current implementation, it was intractable to train LTLM with more roles in a reasonable time.", "labels": [], "entities": []}, {"text": "Training 1000 roles LTLM took up to two weeks on a powerful computational unit.", "labels": [], "entities": [{"text": "LTLM", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.8210980892181396}]}], "tableCaptions": [{"text": " Table 1: Corpora statistics. OOV rate denotes the  out-of-vocabulary rate.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9882046580314636}]}, {"text": " Table 2: Perplexity results on the test data. The  numbers in brackets are the relative improvements  compared with standalone 4-gram MKN LM.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.953088104724884}]}, {"text": " Table 3: Perplexity results on the test data for LTLMs and STLMs with different number of roles. Deter- ministic inference is denoted as det. and non-deterministic inference as non-det.", "labels": [], "entities": [{"text": "Deter- ministic inference", "start_pos": 98, "end_pos": 123, "type": "TASK", "confidence": 0.6742721870541573}]}]}