{"title": [{"text": "Analyzing Linguistic Knowledge in Sequential Model of Sentence", "labels": [], "entities": [{"text": "Analyzing Linguistic Knowledge", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8198078672091166}]}], "abstractContent": [{"text": "Sentence modelling is a fundamental topic in computational linguistics.", "labels": [], "entities": [{"text": "Sentence modelling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.948963463306427}, {"text": "computational linguistics", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.7395942211151123}]}, {"text": "Recently, deep learning-based sequential models of sentence , such as recurrent neural network, have proved to be effective in dealing with the non-sequential properties of human language.", "labels": [], "entities": []}, {"text": "However, little is known about how a recurrent neural network captures linguistic knowledge.", "labels": [], "entities": []}, {"text": "Here we propose to correlate the neuron activation pattern of a LSTM language model with rich language features at sequential, lexical and compositional level.", "labels": [], "entities": []}, {"text": "Qualitative visualization as well as quantitative analysis under multilingual perspective reveals the effectiveness of gate neurons and indicates that LSTM learns to allow different neurons selectively respond to linguistic knowledge at different levels.", "labels": [], "entities": []}, {"text": "Cross-language evidence shows that the model captures different aspects of linguistic properties for different languages due to the variance of syntactic complexity.", "labels": [], "entities": []}, {"text": "Additionally, we analyze the influence of modelling strategy on linguistic knowledge encoded implicitly in different sequential models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence modelling is a central and fundamental topic in the study of language generation and comprehension.", "labels": [], "entities": [{"text": "Sentence modelling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9473788738250732}, {"text": "language generation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.726816862821579}]}, {"text": "With the application of popular deep learning methods, researchers have found that recurrent neural network can successfully model the non-sequential linguistic properties with sequential * Corresponding author.", "labels": [], "entities": []}, {"text": "However, due to the complexity of the neural networks and the lack of effective analytic methodology, little is known about how a sequential model of sentence, such as recurrent neural network, captures linguistic knowledge.", "labels": [], "entities": []}, {"text": "This makes it hard to understand the underlying mechanism as well as the model's strength and weakness.", "labels": [], "entities": []}, {"text": "Previous work () has attempted to visualize neural models in NLP, but only focus on analyzing the hidden layer and sentiment representation rather than grammar knowledge.", "labels": [], "entities": [{"text": "sentiment representation", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.7499901950359344}]}, {"text": "Currently, there have been a few attempts at understanding what is embedded in the word vectors or building linguistically interpretable embeddings.", "labels": [], "entities": []}, {"text": "Few works focus on investigating the linguistic knowledge encoded in a sequential neural network model of a sentence, not to mention the comparison of model behaviours from a crosslanguage perspective.", "labels": [], "entities": []}, {"text": "Our work, therefore, aims to shedding new insights into the following topics: a) How well does a sequential neural model (e.g. language model) encodes linguistic knowledge of different levels?", "labels": [], "entities": []}, {"text": "b) How does modelling strategy (e.g. the optimization objective) influence the neuron's ability of capturing linguistic knowledge?", "labels": [], "entities": []}, {"text": "c) Does the sequential model behave similarly towards typologically diverse languages?", "labels": [], "entities": []}, {"text": "To tackle the questions above, we propose to visualize and analyze the neuron activation pattern: List of the linguistic features to be correlated with model neuron behaviours.", "labels": [], "entities": []}, {"text": "so as to understand how a sequential neural model of sentence encodes linguistic properties of different level.", "labels": [], "entities": []}, {"text": "By training vanilla LSTM language models with multilingual data and correlating the model neuron's activation with various linguistic features, we not only qualitatively show the activation pattern of a certain model neuron, but also quantify the selectivity of the neuron towards input language data or certain linguistic properties.", "labels": [], "entities": []}, {"text": "correlates brain activities with linguistic stimuli under a popular brain-mapping paradigm.", "labels": [], "entities": []}, {"text": "Since brain is a 'black box', researchers want to decode what is represented in a certain neuronal cluster of the brain at a certain time step.", "labels": [], "entities": []}, {"text": "Here we propose that this paradigm can be applied to similar 'black-box' model, such as the neural network.", "labels": [], "entities": []}, {"text": "This is what we calla 'brain' metaphor of the artificial model, as is visualized in.", "labels": [], "entities": []}, {"text": "We treat the neural network as a simplified 'brain'.", "labels": [], "entities": []}, {"text": "We correlate the neuron behaviours with the input stimuli and design experiments to map the neuron activation to an explicit linguistic feature.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison of model components' corre- lation with tree structure stastistics.", "labels": [], "entities": []}]}