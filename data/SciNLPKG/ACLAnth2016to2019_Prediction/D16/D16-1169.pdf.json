{"title": [{"text": "Context-Sensitive Lexicon Features for Neural Sentiment Analysis", "labels": [], "entities": [{"text": "Neural Sentiment Analysis", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.8392025828361511}]}], "abstractContent": [{"text": "Sentiment lexicons have been leveraged as a useful source of features for sentiment analysis models, leading to the state-of-the-art accuracies.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.9524291157722473}]}, {"text": "On the other hand, most existing methods use sentiment lexicons without considering context, typically taking the count, sum of strength, or maximum sentiment scores over the whole input.", "labels": [], "entities": []}, {"text": "We propose a context-sensitive lexicon-based method based on a simple weighted-sum model, using a recurrent neural network to learn the sentiments strength, intensification and negation of lexicon sentiments in composing the sentiment value of sentences.", "labels": [], "entities": []}, {"text": "Results show that our model cannot only learn such operation details, but also give significant improvements over state-of-the-art recurrent neural network baselines without lexical features, achieving the best results on a Twitter benchmark.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment lexicons () have been a useful resource for opinion mining (.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.8783063590526581}]}, {"text": "Containing sentiment attributes of words such as polarities and strengths, they can serve to provide a word-level foundation for analyzing the sentiment of sentences and documents.", "labels": [], "entities": [{"text": "analyzing the sentiment of sentences and documents", "start_pos": 129, "end_pos": 179, "type": "TASK", "confidence": 0.7411731481552124}]}, {"text": "We investigate an effective way to use sentiment lexicon features.", "labels": [], "entities": []}, {"text": "A traditional way of deciding the sentiment of a document is to use the sum of sentiment values of  all words in the document that exist in a sentiment lexicon).", "labels": [], "entities": [{"text": "deciding the sentiment of a document", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.8055343230565389}]}, {"text": "This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks ( , and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance () and product reviews ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8954637348651886}]}, {"text": "More sophisticated sentence-level features such as the counts of positive and negative words, their total strength, and the maximum strength, etc, have also been exploited ().", "labels": [], "entities": []}, {"text": "Such lexicon features have been shown highly effective, leading to the best accuracies in the SemEval shared task).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9738077521324158}, {"text": "SemEval shared task", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7943163911501566}]}, {"text": "On the other hand, they are typically based on bag-of-word models, hence suffering two limitations.", "labels": [], "entities": []}, {"text": "First, they do not explicitly handle semantic compositionality, some examples of which are shown in.", "labels": [], "entities": [{"text": "semantic compositionality", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7060530036687851}]}, {"text": "The composition effects can exhibit intricacies such as negation over intensification (e.g. not very good), shifting (e.g. not terrific) vs flip-ping negation (e.g. not acceptable), content word negation (e.g. removes my doubts) and unbounded dependencies (e.g. No body gives a good performance).", "labels": [], "entities": [{"text": "content word negation", "start_pos": 182, "end_pos": 203, "type": "TASK", "confidence": 0.6017993489901224}]}, {"text": "Second, they cannot effectively deal with word sense variations).", "labels": [], "entities": []}, {"text": "show challenges in modeling the correlation between contextdependent posterior word sentiments and their context independent priors.", "labels": [], "entities": []}, {"text": "For example, the sentiment value of \"cold\" varies between \"cold beer\", \"cold pizza\" and \"cold person\" due to sense and context differences.", "labels": [], "entities": []}, {"text": "Such variations raise difficulties fora sentiment classifier with bag-of-word nature, since they can depend on semantic information overlong phrases or the full sentence.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.786386251449585}]}, {"text": "We investigate a method that can potentially address the above issues, by using a recurrent neural network to capture context-dependent semantic composition effects over sentences.", "labels": [], "entities": []}, {"text": "Shown in, the model is conceptually simple, using a weighted sum of lexicon sentiments and a sentence-level bias to estimate the sentiment value of a sentence.", "labels": [], "entities": []}, {"text": "The key idea is to use a bi-directional long-short-term-memory (LSTM)) model to capture global syntactic dependencies and semantic information, based on which the weight of each sentiment word together with a sentence-level sentiment bias score are predicted.", "labels": [], "entities": []}, {"text": "Such weights are context-sensitive, and can express flipping negation by having negative values.", "labels": [], "entities": [{"text": "flipping negation", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.8957171738147736}]}, {"text": "The advantages of the recurrent network model over existing semantic-composition-aware discrete models such as include its capability of representing non-local and subtle semantic features without suffering from the challenge of designing sparse manual features.", "labels": [], "entities": []}, {"text": "On the other hand, compared with neural network models, which recently give the state-of-the-art accuracies (, our model has the advantage of leveraging sentiment lexicons as a useful resource.", "labels": [], "entities": []}, {"text": "To our knowledge, we are the first to integrate the operation into sentiment lexicons and a deep neural model for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.9592199623584747}]}, {"text": "The conceptually simple model gives strong empirical performances.", "labels": [], "entities": []}, {"text": "Results on standard sentiment benchmarks show that our method gives competitive: Overall model structure.", "labels": [], "entities": []}, {"text": "The sentiment score of the sentence \"not a bad movie at all\" is a weighted sum of the scores of sentiment words \"not\", \"bad\" and a sentence-level bias score b. score(not) and score(bad) are prior scores obtained from sentiment lexicons.", "labels": [], "entities": []}, {"text": "\u03b31 and \u03b33 are context-sensitive weights for sentiment words \"not\" and \"bad\", respectively.", "labels": [], "entities": []}, {"text": "accuracies to the state-of-the-art models in the literature.", "labels": [], "entities": []}, {"text": "As a by-product, the model can also correctly identify the compositional changes on the sentiment values of each word given a sentential context.", "labels": [], "entities": []}, {"text": "Our code is released at https://github.com/zeeeyang/lexicon rnn.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the Twitter dataset.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9295998215675354}]}, {"text": " Table 2: Statistics of SST.", "labels": [], "entities": [{"text": "SST", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9421090483665466}]}, {"text": " Table 3: Document distribution of the mixed domain dataset.", "labels": [], "entities": []}, {"text": " Table 4: Statistics of sentiment lexicons.", "labels": [], "entities": []}, {"text": " Table 5: Results on the Twitter development set.", "labels": [], "entities": [{"text": "Twitter development set", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.8082244396209717}]}, {"text": " Table 6: Results on the Twitter test set.", "labels": [], "entities": [{"text": "Twitter test set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8470921516418457}]}, {"text": " Table 8: Cross-domain sentiment analysis. Training domain is movie review.", "labels": [], "entities": [{"text": "Cross-domain sentiment analysis", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8203478654225668}, {"text": "movie review", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.7133796215057373}]}]}