{"title": [{"text": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "labels": [], "entities": [{"text": "Multimodal Compact Bilinear Pooling", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5499144718050957}, {"text": "Question Answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7136117666959763}]}], "abstractContent": [{"text": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years.", "labels": [], "entities": [{"text": "Modeling textual or visual information", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7961440086364746}]}, {"text": "However, tasks such as visual question answering require combining these vector representations with each other.", "labels": [], "entities": [{"text": "question answering", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.696229949593544}]}, {"text": "Approaches to multimodal pooling include element-wise product or sum, as well as con-catenation of the visual and textual representations.", "labels": [], "entities": []}, {"text": "We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors.", "labels": [], "entities": []}, {"text": "As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multi-modal features.", "labels": [], "entities": []}, {"text": "We extensively evaluate MCB on the visual question answering and grounding tasks.", "labels": [], "entities": [{"text": "question answering", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7496773898601532}]}, {"text": "We consistently show the benefit of MCB over ablations without MCB.", "labels": [], "entities": []}, {"text": "For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.6916565597057343}]}, {"text": "This model out-performs the state-of-the-art on the Visual7W dataset and the VQA challenge.", "labels": [], "entities": [{"text": "Visual7W dataset", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.9745738804340363}, {"text": "VQA challenge", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.8421767055988312}]}], "introductionContent": [{"text": "Representation learning for text and images has been extensively studied in recent years.", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9673583209514618}]}, {"text": "Recurrent neural networks (RNNs) are often used to represent sentences or phrases   2015), and convolutional neural networks (CNNs) have shown to work best to represent images).", "labels": [], "entities": []}, {"text": "For tasks such as visual question answering (VQA) and visual grounding, most approaches require joining the representation of both modalities.", "labels": [], "entities": [{"text": "question answering (VQA)", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8440711438655853}]}, {"text": "For combining the two vector representations (multimodal pooling), current approaches in VQA or grounding rely on concatenating vectors or applying element-wise sum or product.", "labels": [], "entities": []}, {"text": "While this generates a joint representation, it might not be expressive enough to fully capture the complex associations between the two different modalities.", "labels": [], "entities": []}, {"text": "In this paper, we propose to rely on Multimodal Compact Bilinear pooling (MCB) to get a joint representation.", "labels": [], "entities": []}, {"text": "Bilinear pooling computes the outer product between two vectors, which allows, in contrast to element-wise product, a multiplicative interaction between all elements of both vectors.", "labels": [], "entities": []}, {"text": "Bilinear pooling models) have recently been shown to be beneficial for fine-grained classification for vision only tasks (.", "labels": [], "entities": []}, {"text": "However, given their high dimensionality (n 2 ), bilinear pooling has so far not been widely used.", "labels": [], "entities": []}, {"text": "In this paper, we adopt the idea from  which shows how to efficiently compress bilinear pooling fora single modality.", "labels": [], "entities": []}, {"text": "In this work, we discuss and extensively evaluate the extension to the multimodal case for text and visual modalities.", "labels": [], "entities": []}, {"text": "As shown in, Multimodal Compact Bilinear pooling (MCB) is approximated by randomly projecting the image and text representations to a higher dimensional space (using Count Sketch () and then convolving both vectors efficiently by using element-wise product in Fast Fourier Transform (FFT) space.", "labels": [], "entities": [{"text": "Multimodal Compact Bilinear pooling (MCB)", "start_pos": 13, "end_pos": 54, "type": "TASK", "confidence": 0.6943891942501068}]}, {"text": "We use MCB to predict answers for the VQA task and locations for the visual grounding task.", "labels": [], "entities": []}, {"text": "For open-ended question answering, we present an architecture for VQA which uses MCB twice, once to predict spatial attention and the second time to predict the answer.", "labels": [], "entities": [{"text": "open-ended question answering", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.6328492164611816}]}, {"text": "For multiple-choice question answering we introduce a third MCB to relate the encoded answer to the question-image space.", "labels": [], "entities": [{"text": "multiple-choice question answering", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6139126420021057}]}, {"text": "Additionally, we discuss the benefit of attention maps and additional training data for the VQA task.", "labels": [], "entities": [{"text": "VQA task", "start_pos": 92, "end_pos": 100, "type": "TASK", "confidence": 0.4881232678890228}]}, {"text": "To summarize, MCB is evaluated on two tasks, four datasets, and with a diverse set of ablations and comparisons to the state-of-the-art.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the benefit of MCB with a diverse set of ablations on two visual question answering datasets.", "labels": [], "entities": [{"text": "MCB", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8220800757408142}]}, {"text": "what, where, when, who, why, and how).", "labels": [], "entities": []}, {"text": "Compared to the VQA dataset, Visual Genome represents a more balanced distribution of the 6W question types.", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.9669656753540039}]}, {"text": "Moreover, the average question and answer lengths for Visual Genome are larger than the VQA dataset.", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9742188155651093}]}, {"text": "To leverage the Visual Genome dataset as additional training data, we remove all the unnecessary words such as \"a\", \"the\", and \"it is\" from the answers to decrease the length of the answers and extract QA pairs whose answers are single-worded.", "labels": [], "entities": [{"text": "Visual Genome dataset", "start_pos": 16, "end_pos": 37, "type": "DATASET", "confidence": 0.806972881158193}]}, {"text": "The extracted data is filtered again based on the answer vocabulary space created from the VQA dataset, leaving us with additional 1M image-QA triplets.", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9777109026908875}]}, {"text": "The  In all experiments we use Adam solver) with learning rate = 0.0001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.9700297713279724}]}, {"text": "The embedding size is 500 both for visual and language embeddings.", "labels": [], "entities": []}, {"text": "We used = 2048 in the MCB pooling, which we found to work best for the visual grounding task.", "labels": [], "entities": [{"text": "MCB pooling", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.9029115438461304}]}, {"text": "The accuracy is measured as percentage of query phrases which have been localized correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999508261680603}]}, {"text": "The phrase is localized correctly if the predicted bounding box overlaps with the ground-truth bounding box by more than 50% intersection over union (IOU).", "labels": [], "entities": []}, {"text": "summarize our results in the visual grounding task.", "labels": [], "entities": [{"text": "visual grounding task", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7286587258179983}]}, {"text": "We present multiple ablations of our proposed architecture.", "labels": [], "entities": []}, {"text": "First, we replace the MCB with simple concatenation of the embedded visual feature and the embedded phrase, resulting in 46.5% on the Flickr30k Entities and 25.48% on the ReferItGame datasets.", "labels": [], "entities": [{"text": "Flickr30k Entities", "start_pos": 134, "end_pos": 152, "type": "DATASET", "confidence": 0.9524258375167847}, {"text": "ReferItGame datasets", "start_pos": 171, "end_pos": 191, "type": "DATASET", "confidence": 0.9613425731658936}]}, {"text": "The results can be improved by replacing the concatenation with the element-wise product of both embedded features (47.41% and 27.80%).", "labels": [], "entities": []}, {"text": "We can further slightly increase the performance by introducing additional 2048-D convolution after the element-wise product (47.86% and 27.98%).", "labels": [], "entities": []}, {"text": "However, even with fewer parameters, our MCB pooling significantly improves over this baseline on both datasets, reaching state-of-the-art accuracy of 48.69% on Flickr30k Entities and 28.91% on ReferItGame dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9994882345199585}, {"text": "Flickr30k Entities", "start_pos": 161, "end_pos": 179, "type": "DATASET", "confidence": 0.9263007342815399}, {"text": "ReferItGame dataset", "start_pos": 194, "end_pos": 213, "type": "DATASET", "confidence": 0.9630207717418671}]}, {"text": "We evaluate our visual grounding approach on two datasets.", "labels": [], "entities": []}, {"text": "al., 2013) object proposals and the Fast R-CNN) fine-tuned VGG16 features).", "labels": [], "entities": []}, {"text": "The second dataset is ReferItGame (, which contains 20K images from IAPR TC-12 dataset) with segmented regions from SAIAPR-12 dataset (", "labels": [], "entities": [{"text": "IAPR TC-12 dataset", "start_pos": 68, "end_pos": 86, "type": "DATASET", "confidence": 0.9381479024887085}, {"text": "SAIAPR-12 dataset", "start_pos": 116, "end_pos": 133, "type": "DATASET", "confidence": 0.9410139620304108}]}], "tableCaptions": [{"text": " Table 1: Comparison of multimodal pooling methods.  Models are trained on the VQA train split and tested  on test-dev.", "labels": [], "entities": [{"text": "VQA train split", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.9732004006703695}]}, {"text": " Table 2: Accuracies for different values of d, the  dimension of the compact bilinear feature. Models  are trained on the VQA train split and tested on test- dev. Details in Sec. 4.3.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9963856935501099}, {"text": "VQA train split", "start_pos": 123, "end_pos": 138, "type": "DATASET", "confidence": 0.9639188249905905}]}, {"text": " Table 3: Multiple-choice QA tasks accuracy (%) on  Visual7W test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9889426231384277}, {"text": "Visual7W test set", "start_pos": 52, "end_pos": 69, "type": "DATASET", "confidence": 0.9814242323239645}]}, {"text": " Table 1. We see that  MCB pooling outperforms all non-bilinear pooling  methods, such as eltwise sum, concatenation, and  eltwise product.  One could argue that the compact bilinear method  simply has more parameters than the non-bilinear  pooling methods, which contributes to its perfor- mance. We compensated for this by stacking fully  connected layers (with 4096 units per layer, ReLU  activation, and dropout) after the non-bilinear pool- ing methods to increase their number of parameters.  However, even with similar parameter budgets, non- bilinear methods could not achieve the same accuracy  as the MCB method. For example, the \"Concatena- tion + FC + FC\" pooling method has approximately  4096 2 + 4096 2 + 4096 \u00d7 3000 \u2248 46 million pa- rameters, which matches the 48 million parameters  available in MCB with d = 16000. However, the per- formance of the \"Concatenation + FC + FC\" method  is only 57.10% compared to MCB's 59.83%.", "labels": [], "entities": [{"text": "ReLU  activation", "start_pos": 386, "end_pos": 402, "type": "METRIC", "confidence": 0.8984954357147217}, {"text": "accuracy", "start_pos": 594, "end_pos": 602, "type": "METRIC", "confidence": 0.998162567615509}]}, {"text": " Table 4: Open-ended and multiple-choice (MC) results on VQA test set (trained on train+val set) compared  with state-of-the-art: accuracy in %. See Sec. 4.4.", "labels": [], "entities": [{"text": "VQA test set", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9153105815251669}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9994086027145386}]}, {"text": " Table 5: Grounding accuracy on Flickr30k Entities  dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9801027178764343}, {"text": "Flickr30k Entities  dataset", "start_pos": 32, "end_pos": 59, "type": "DATASET", "confidence": 0.9476627906163534}]}, {"text": " Table 6: Grounding accuracy on ReferItGame  dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9716947674751282}, {"text": "ReferItGame  dataset", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.8309413194656372}]}]}