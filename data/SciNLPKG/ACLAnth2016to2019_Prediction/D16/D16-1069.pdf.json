{"title": [{"text": "Generating Abbreviations for Chinese Named Entities Using Recurrent Neural Network with Dynamic Dictionary", "labels": [], "entities": []}], "abstractContent": [{"text": "Chinese named entities occur frequently informal and informal environments.", "labels": [], "entities": []}, {"text": "Various approaches have been formalized the problem as a sequence labelling task and utilize a character-based methodology, in which character is treated as the basic classification unit.", "labels": [], "entities": [{"text": "sequence labelling task", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.7376159032185873}]}, {"text": "One of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of Chinese.", "labels": [], "entities": []}, {"text": "To address this problem, we propose a novel neural network architecture to perform task.", "labels": [], "entities": []}, {"text": "It combines recurrent neural network (RNN) with an architecture determining whether a given sequence of characters can be a word or not.", "labels": [], "entities": []}, {"text": "For demonstrating the effectiveness of the proposed method, we evaluate it on Chinese named entity generation and opinion target extraction tasks.", "labels": [], "entities": [{"text": "Chinese named entity generation", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.48131774365901947}, {"text": "opinion target extraction", "start_pos": 114, "end_pos": 139, "type": "TASK", "confidence": 0.6349536180496216}]}, {"text": "Experimental results show that the proposed method can achieve better performance than state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abbreviations of Chinese named entities are frequently used on different kinds of environments.", "labels": [], "entities": []}, {"text": "Along with the development of social media, this kinds of circumstance occurs more frequently.", "labels": [], "entities": []}, {"text": "Unlike western languages such as English, Chinese does not insert spaces between words or word forms that undergo morphological alternations.", "labels": [], "entities": []}, {"text": "Hence, most of the Chinese natural language processing methods assume a Chinese word segmenter is used in a pre-processing step to produce word-segmented Chinese sentences as input.", "labels": [], "entities": []}, {"text": "However, if the Chinese word segmenter produces erroneous output, the quality of these methods will be degraded as a direct result.", "labels": [], "entities": [{"text": "Chinese word segmenter", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.648272693157196}]}, {"text": "Moreover, since the word segmenter may split the targets into two individual words, many methods adopted character-based methodologies, such as methods for named entity recognition (), aspect-based opinion mining (, and soon.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.7485067546367645}, {"text": "named entity recognition", "start_pos": 156, "end_pos": 180, "type": "TASK", "confidence": 0.6257820725440979}, {"text": "aspect-based opinion mining", "start_pos": 185, "end_pos": 212, "type": "TASK", "confidence": 0.5780371526877085}]}, {"text": "Through character-based methodology, most of the previous abbreviation generation approaches have been formalized as sequence labelling problem.", "labels": [], "entities": [{"text": "abbreviation generation", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.803320437669754}]}, {"text": "Chinese characters are treated as the basic classification unit and are classified one by one.", "labels": [], "entities": []}, {"text": "In these methods, dictionaries play important effect in constructing features and avoiding meaningless outputs.", "labels": [], "entities": []}, {"text": "Various previous works have demonstrated the significant positive effectiveness of the external dictionary (.", "labels": [], "entities": []}, {"text": "However, because these external dictionaries are usually static and preconstructed, one of the main drawbacks of these methods is that the words which are not included in the dictionaries cannot be well processed.", "labels": [], "entities": []}, {"text": "This issue has also been mentioned by numerous previous works ().", "labels": [], "entities": []}, {"text": "Hence, understanding how Chinese words are constructed can benefit a variety of Chinese NLP tasks to avoid meaningless output.", "labels": [], "entities": []}, {"text": "For example, to generate the abbreviation fora named entity, we can use a binary classifier to determine whether a character should be removed or retained.", "labels": [], "entities": []}, {"text": "Both \"\u56fd \u822a\" and \"\u4e2d\u56fd\u56fd\u822a\" are appropriate abbreviations for \"\u4e2d \u56fd \u56fd \u9645 \u822a \u7a7a \u516c \u53f8(Air China)\".", "labels": [], "entities": [{"text": "Air China)\"", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.871492842833201}]}, {"text": "However \"\u56fd \u822a \u53f8\" is not a Chinese word and cannot be understood by humans.", "labels": [], "entities": []}, {"text": "Thus we are motivated to study the task of \"dynamic dictionary\" and integrating it with sequence labelling model to perform the abbreviation generation task.", "labels": [], "entities": [{"text": "abbreviation generation", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.8602340221405029}]}, {"text": "Dynamic dictionary denotes a binary classification problem which tries to determine whether or not a given sequence of characters is a word.", "labels": [], "entities": []}, {"text": "Although human can use implicit knowledge to easily recognize whether an unseen text segment is a word or not at first glance, the task is not as easy as it may seem.", "labels": [], "entities": []}, {"text": "First, Chinese has a different morphological system from English.", "labels": [], "entities": []}, {"text": "Each Chinese character represents both a syllable and a morpheme.", "labels": [], "entities": []}, {"text": "Hence, Chinese script is sometimes described as being morphosyllabic.", "labels": [], "entities": []}, {"text": "Second, there are many homophones in Chinese.", "labels": [], "entities": []}, {"text": "This means that characters that have very different written forms may sound identical.", "labels": [], "entities": []}, {"text": "Third, there area huge number of Chinese words.", "labels": [], "entities": []}, {"text": "Without taking the implicit knowledge of morphology into consideration, an arbitrary sequence of characters can be used as a name.", "labels": [], "entities": []}, {"text": "In Mandarin, there are approximately 7,000 characters in daily use.", "labels": [], "entities": []}, {"text": "Hence, determining whether a given sequence of characters is a word or not is an challenging task.", "labels": [], "entities": []}, {"text": "Since the length of Chinese words is variable, in this paper, we propose a modified recurrent architecture to model the dynamic dictionary construction task.", "labels": [], "entities": [{"text": "dynamic dictionary construction task", "start_pos": 120, "end_pos": 156, "type": "TASK", "confidence": 0.6896311268210411}]}, {"text": "For processing sequence labelling tasks, we also combine the proposed method with RNN.", "labels": [], "entities": [{"text": "processing sequence labelling tasks", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.7545177638530731}]}, {"text": "Since the proposed dynamic dictionary model can be pre-trained independently with extensive domain independent dictionaries, the combined model can be easily used in different domains.", "labels": [], "entities": []}, {"text": "The proposed model can take advantage of both the sequencelevel discrimination ability of RNN and the ability of external dictionary.", "labels": [], "entities": [{"text": "sequencelevel discrimination", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6800679266452789}]}, {"text": "The main contributions of this work can be summarized as follows: \u2022 We define the dynamic dictionary problem and construct a large dataset, which consists of more than 20 million words for training and evaluation.", "labels": [], "entities": []}, {"text": "\u2022 We integrate RNN with a deep feedforward network based dynamic dictionary learning method for processing Chinese NLP tasks which are formalized as sequence labelling tasks.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results demonstrate that the accuracy of the proposed method can achieve better results than current state-of-the-arts methods on two different tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9981984496116638}]}], "datasetContent": [{"text": "To demonstrate the effectiveness of the proposed method, we first compared the proposed RNN-based dynamic dictionary construction method against several baseline methods on the task.", "labels": [], "entities": [{"text": "RNN-based dynamic dictionary construction", "start_pos": 88, "end_pos": 129, "type": "TASK", "confidence": 0.6087870895862579}]}, {"text": "Then, we evaluated the performance of the proposed method on two Chinese natural language processing tasks: Chinese word segmentation, and opinion target extraction.", "labels": [], "entities": [{"text": "Chinese natural language processing", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.5944345742464066}, {"text": "Chinese word segmentation", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.6330858170986176}, {"text": "opinion target extraction", "start_pos": 139, "end_pos": 164, "type": "TASK", "confidence": 0.6400815943876902}]}, {"text": "To generate the distributed representations for Chinese characters, we use the method similar to Skipngram (, which has been successfully employed in comparable tasks.", "labels": [], "entities": [{"text": "Skipngram", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.9292762279510498}]}, {"text": "However, in this work, characters were considered the basic units of data, and the toolkit was provided by the authors . We used Sogou news corpus (SogouCA 2 ), which consists of news articles belonging to 18 different domains published from June 2012 to July 2012, as the training data to optimize the distributed representations of Chinese characters.", "labels": [], "entities": [{"text": "Sogou news corpus (SogouCA 2 )", "start_pos": 129, "end_pos": 159, "type": "DATASET", "confidence": 0.9368116429873875}]}, {"text": "After several experiments on development, we decided to set the dimension of the character embedding to 200.", "labels": [], "entities": []}, {"text": "Through several evaluations on the validation set, in both RNN-RAD and RAD, the hidden layer size is set to 50.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of different methods on the dynamic", "labels": [], "entities": []}, {"text": " Table 4: Statistics of the dataset used for the opinion target", "labels": [], "entities": []}, {"text": " Table 5: Results of different methods on the opinion target", "labels": [], "entities": []}]}