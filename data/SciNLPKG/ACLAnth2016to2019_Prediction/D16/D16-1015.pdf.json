{"title": [{"text": "Improving Semantic Parsing via Answer Type Inference", "labels": [], "entities": [{"text": "Improving Semantic Parsing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8857011198997498}, {"text": "Answer Type Inference", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.810731828212738}]}], "abstractContent": [{"text": "In this work, we show the possibility of inferring the answer type before solving a factoid question and leveraging the type information to improve semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 148, "end_pos": 164, "type": "TASK", "confidence": 0.7652570009231567}]}, {"text": "By replacing the topic entity in a question with its type, we are able to generate an abstract form of the question , whose answer corresponds to the answer type of the original question.", "labels": [], "entities": []}, {"text": "A bidirectional LSTM model is built to train over the abstract form of questions and infer their answer types.", "labels": [], "entities": []}, {"text": "It is also observed that if we convert a question into a statement form, our LSTM model achieves better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9969846606254578}]}, {"text": "Using the predicted type information to rerank the logical forms returned by AgendaIL, one of the leading semantic parsers, we are able to improve the F1-score from 49.7% to 52.6% on the WE-BQUESTIONS data.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9994474053382874}, {"text": "WE-BQUESTIONS data", "start_pos": 187, "end_pos": 205, "type": "DATASET", "confidence": 0.8503152430057526}]}], "introductionContent": [{"text": "Large scale knowledge bases (KB) like Freebase (,, and YAGO () that store the world's factual information in a structured fashion have become substantial resources for people to solve questions.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9538114070892334}]}, {"text": "KB-based factoid question answering (KB-QA) that attempts to find exact answers to natural language questions has gained much attention recently.", "labels": [], "entities": [{"text": "KB-based factoid question answering (KB-QA)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6819596375737872}]}, {"text": "KB-QA is a challenging task due to the representation variety between natural language and structural knowledge in KBs.", "labels": [], "entities": []}, {"text": "As one of the promising KB-QA techniques, semantic parsing maps a natural language question into its semantic representation (e.g., logical forms).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7612660825252533}]}], "datasetContent": [{"text": "In this section, we describe the datasets, model training, and experimental results.", "labels": [], "entities": []}, {"text": "To evaluate our method, we use the WebQuestions dataset), which contains 5,810 questions crawled via Google Suggest API.", "labels": [], "entities": [{"text": "WebQuestions dataset", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.949573427438736}]}, {"text": "The answers to these questions are annotated from Freebase using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9634850025177002}, {"text": "Amazon Mechanical Turk", "start_pos": 65, "end_pos": 87, "type": "DATASET", "confidence": 0.9119552771250407}]}, {"text": "The data is split into training and test sets of size 3,778 and 2,032 questions, respectively.", "labels": [], "entities": []}, {"text": "This dataset has been popularly used in question answering and semantic parsing.", "labels": [], "entities": [{"text": "question answering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.892535388469696}, {"text": "semantic parsing", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.718533918261528}]}, {"text": "The SimpleQuestions (Bordes et al., 2015) contains 108,442 questions written in natural language by English-speaking human annotators.", "labels": [], "entities": [{"text": "SimpleQuestions (Bordes et al., 2015)", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.8082004636526108}]}, {"text": "This dataset is a collection of question/Freebase-fact pairs rather than question/answer pairs.", "labels": [], "entities": []}, {"text": "The data 4 is split and provided as training(75,910), test, and validation(10,845) sets.", "labels": [], "entities": []}, {"text": "Each question is mapped to the subject, relation, and object of the corresponding Freebase fact.", "labels": [], "entities": []}, {"text": "This dataset is only used for training the question abstraction model.", "labels": [], "entities": [{"text": "question abstraction", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7562239170074463}]}, {"text": "Since WebQuestions only provides question-answer pairs along with annotated topic entities, we need to figure out the type information, which can be used as training data.", "labels": [], "entities": []}, {"text": "We obtain simulated types as follows: We retrieve 1-hop and 2-hop predicates r from/to annotated topic entity e in Freebase.", "labels": [], "entities": []}, {"text": "For each relation r, we query (e, r, ?) and (?, r, e) against Freebase and retrieve the candidate answers r a . The F 1 value of each candidate answer r a is computed with respect to the annotated answer.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9795678853988647}, {"text": "F 1 value", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.967937688032786}]}, {"text": "The subject and object types of the relation r with the highest F 1 value is selected as the simulated type for the topic entity and the answer.", "labels": [], "entities": []}, {"text": "When there are multiple such relations, we obtain multiple simulated types for topic entity and answer, one from each relation.", "labels": [], "entities": []}, {"text": "We treat each of them as correct with equal probability.", "labels": [], "entities": []}, {"text": "Candidate Logical Forms for Evaluation.", "labels": [], "entities": []}, {"text": "To obtain candidate logical forms, we train AgendaIL ( on WebQuestions with beam size 200 using the publicly available code 5 by the authors.", "labels": [], "entities": []}, {"text": "We report average F 1 score of the reranked logical forms using the predicted answer types as the main evaluation metric.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9710584084192911}]}, {"text": "It is a common performance measure in question answering as questions might have multiple answers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.8190232217311859}]}, {"text": "We use 50 dimensional word embeddings, which are initialized by the 50 dimensional pre-trained word vectors 6 from GloVe (), and updated in the training process.", "labels": [], "entities": []}, {"text": "Hyperparameters are tuned on the development set.", "labels": [], "entities": []}, {"text": "The size of the LSTM hidden layer is set at 50.", "labels": [], "entities": []}, {"text": "We use RMSProp) with a learning rate of 0.005 and mini-batch size of 32 for the optimization.", "labels": [], "entities": []}, {"text": "We use a dropout layer with probability 0.5 for regularization.", "labels": [], "entities": []}, {"text": "We implemented the LSTM networks using Theano.", "labels": [], "entities": [{"text": "Theano", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9810954928398132}]}, {"text": "We use Stanford NER tagger () to identify topic entity span for both training and test data.", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.6171614229679108}]}, {"text": "For entity linking, annotated mention span is mapped to a ranked list of candidate Freebase entities using Freebase Search API for the test data.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7379471361637115}]}, {"text": "For the training data, we use the gold Freebase topic entity linkings of each question provided by WebQuestions, coming from its question generation process.", "labels": [], "entities": [{"text": "question generation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.683845192193985}]}, {"text": "We first pre-train the LSTM model described in Section 3.2 on the SimpleQuestions dataset.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.9250568151473999}]}, {"text": "Then, we update the pretrained model on the training portion of WebQuestions data where the simulated topic entity types are used as true labels.", "labels": [], "entities": [{"text": "WebQuestions data", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9377206861972809}]}, {"text": "We use the detected topic entity mentions to obtain candidate matching entities in the KB using Freebase Search API.", "labels": [], "entities": [{"text": "KB", "start_pos": 87, "end_pos": 89, "type": "DATASET", "confidence": 0.964423656463623}]}, {"text": "We use top-  3 entities returned for the pruning step of Question Abstraction on the test examples.", "labels": [], "entities": []}, {"text": "We train Answer Type Prediction model using the simulated topic entity and answer types for each question.", "labels": [], "entities": [{"text": "Answer Type Prediction", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.7794605493545532}]}, {"text": "We perform the answer type prediction on test data using the predicted topic entity type.", "labels": [], "entities": [{"text": "answer type prediction", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7760693033536276}]}], "tableCaptions": [{"text": " Table 3: Comparison of our reranking-by-type system with sev-", "labels": [], "entities": []}, {"text": " Table 6: Ablation analysis of modules of our method.", "labels": [], "entities": []}]}