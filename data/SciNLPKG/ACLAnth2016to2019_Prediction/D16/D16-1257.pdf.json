{"title": [], "abstractContent": [{"text": "We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve anew state of the art for constituency Penn Treebank parsing-93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.731362909078598}, {"text": "Penn Treebank parsing-93.8 F 1", "start_pos": 169, "end_pos": 199, "type": "DATASET", "confidence": 0.7839552164077759}]}, {"text": "When trees are converted to Stan-ford dependencies, UAS and LAS are 95.9% and 94.1%.", "labels": [], "entities": [{"text": "UAS", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9689005017280579}, {"text": "LAS", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9926156997680664}]}], "introductionContent": [{"text": "Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.", "labels": [], "entities": [{"text": "deep learning syntactic parsing", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6055203378200531}, {"text": "92.4 F 1", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.7562117576599121}, {"text": "Penn Treebank constituency parsing", "start_pos": 112, "end_pos": 146, "type": "DATASET", "confidence": 0.911677360534668}, {"text": "F 1", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9003720879554749}]}, {"text": "In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.6929413080215454}]}, {"text": "Section 2 looks more closely at three of the most relevant previous papers.", "labels": [], "entities": []}, {"text": "We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5).", "labels": [], "entities": []}, {"text": "There is a one-to-one mapping between a tree and its sequential form.", "labels": [], "entities": []}, {"text": "(Part-of-speech tags are not used.)", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe datasets we use for evaluation, detail training and development processes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The performance of LSTM-LM (G) with  varying n-best parses on the dev set. Oracle refers  to Charniak parser's oracle F 1 . Final and Exact re- port LSTM-LM (G)'s F 1 and exact match percent- age respectively. To simulate an optimal scenario,  we include gold trees to 50-best trees and rerank  them with LSTM-LM (G) (51 o ).", "labels": [], "entities": [{"text": "F 1", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.9743669629096985}, {"text": "exact match percent- age", "start_pos": 181, "end_pos": 205, "type": "METRIC", "confidence": 0.9310429453849792}]}, {"text": " Table 2: F 1 of models trained on WSJ. Base refers  to performance of a single base parser and Final that  of a final parser.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9807835221290588}, {"text": "WSJ", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.6517190933227539}, {"text": "Base", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9846324920654297}]}, {"text": " Table 3: Evaluation of models trained on the WSJ and additional resources. Note that the numbers of", "labels": [], "entities": [{"text": "WSJ", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.8372243046760559}]}]}