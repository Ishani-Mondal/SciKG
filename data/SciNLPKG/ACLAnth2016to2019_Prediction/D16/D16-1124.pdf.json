{"title": [{"text": "Generalizing and Hybridizing Count-based and Neural Language Models", "labels": [], "entities": [{"text": "Generalizing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9643955826759338}]}], "abstractContent": [{"text": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols.", "labels": [], "entities": []}, {"text": "Currently two major paradigms for language model-ing exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance.", "labels": [], "entities": []}, {"text": "We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions.", "labels": [], "entities": []}, {"text": "This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) are statistical models that, given a sentence w I 1 := w 1 , . .", "labels": [], "entities": []}, {"text": ", w I , calculate its probability P (w I 1 ).", "labels": [], "entities": []}, {"text": "LMs are widely used in applications such as machine translation and speech recognition, and because of their broad applicability they have also been widely studied in the literature.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8282392919063568}, {"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7985670864582062}]}, {"text": "The most traditional and broadly used language modeling paradigm is that of count-based LMs, usually smoothed n-grams.", "labels": [], "entities": []}, {"text": "Recently, there has been a focus on LMs based on neural networks (, which have shown impressive improvements in performance over count-based LMs.", "labels": [], "entities": []}, {"text": "On the other hand, these neural LMs also come at the cost of increased computational complexity at both training and test time, and even the largest reported neural LMs () are trained on a fraction of the data of their count-based counterparts (.", "labels": [], "entities": []}, {"text": "In this paper we focus on a class of LMs, which we will call mixture of distributions LMs (MODLMs; \u00a72).", "labels": [], "entities": []}, {"text": "Specifically, we define MODLMs as all LMs that take the following form, calculating the probabilities of the next word in a sentence w i given preceding context c according to a mixture of several component probability distributions P k (w i |c): Here, \u03bb k (c) is a function that defines the mixture weights, with the constraint that K k=1 \u03bb k (c) = 1 for all c.", "labels": [], "entities": []}, {"text": "This form is not new in itself, and widely used both in the calculation of smoothing coefficients for n-gram LMs (, and interpolation of LMs of various varieties.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is to demonstrate that depending on our definition of c, \u03bb k (c), and P k (w i |c), Eq.", "labels": [], "entities": []}, {"text": "1 can be used to describe not only n-gram models, but also feed-forward () and recurrent () neural network LMs ( \u00a73).", "labels": [], "entities": []}, {"text": "This observation is useful theoretically, as it provides a single mathematical framework that encompasses several widely used classes of LMs.", "labels": [], "entities": []}, {"text": "It is also useful practically, in that this new view of these traditional models allows us to create new models that combine the desirable features of n-gram and neural models, such as: neurally interpolated n-gram LMs ( \u00a74.1), which learn the interpolation weights of n-gram models using neural networks, and neural/n-gram hybrid LMs ( \u00a74.2), which add a count-based n-gram component to neural models, allowing for flexibility to add large-scale external data sources to neural LMs.", "labels": [], "entities": []}, {"text": "We discuss learning methods for these models ( \u00a75) including a novel method of randomly dropping out more easy-to-learn distributions to prevent the parameters from falling into sub-optimal local minima.", "labels": [], "entities": []}, {"text": "Experiments on language modeling benchmarks ( \u00a76) find that these models outperform baselines in terms of performance and convergence speed.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we perform experiments to evaluate the neurally interpolated n-grams ( \u00a76.2) and neural/n-gram hybrids ( \u00a76.3), the ability of our models to take advantage of information from large data sets ( \u00a76.4), and the relative performance compared (details in Tab. 1).", "labels": [], "entities": [{"text": "Tab. 1", "start_pos": 268, "end_pos": 274, "type": "DATASET", "confidence": 0.9349649846553802}]}, {"text": "The PTB corpus uses the standard vocabulary of 10k words, and for the ASPEC corpus we use a vocabulary of the 20k most frequent words.", "labels": [], "entities": [{"text": "PTB corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9856358468532562}, {"text": "ASPEC corpus", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.8619735240936279}]}, {"text": "Our implementation is included as supplementary material.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sizes for the PTB and ASPEC corpora.", "labels": [], "entities": [{"text": "PTB and ASPEC corpora", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.7351559996604919}]}, {"text": " Table 2: PTB/ASPEC perplexities for traditional  heuristic (HEUR) and proposed neural net (FF or  LSTM) interpolation methods using ML or KN dis- tributions, and count (C) or count+word representa- tion (CR) features.", "labels": [], "entities": [{"text": "PTB", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8248706459999084}]}, {"text": " Table 3: PTB/ASPEC perplexities for traditional  KN (1) and LSTM LMs (2), neurally interpolated n- grams", "labels": [], "entities": [{"text": "PTB", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7187906503677368}]}, {"text": " Table 4: PTB perplexity for interpolation between  neural (\u03b4) LMs and count-based models.", "labels": [], "entities": [{"text": "PTB", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5554038286209106}]}]}