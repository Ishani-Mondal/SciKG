{"title": [{"text": "Morphological Priors for Probabilistic Neural Word Embeddings", "labels": [], "entities": [{"text": "Morphological Priors", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7355220913887024}, {"text": "Probabilistic Neural Word Embeddings", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.5910681113600731}]}], "abstractContent": [{"text": "Word embeddings allow natural language processing systems to share statistical information across related words.", "labels": [], "entities": []}, {"text": "These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words.", "labels": [], "entities": []}, {"text": "We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features.", "labels": [], "entities": []}, {"text": "Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework , in which the word embedding is a latent variable.", "labels": [], "entities": []}, {"text": "The morphological information provides a prior distribution on the latent word em-beddings, which in turn condition a likelihood function over an observed corpus.", "labels": [], "entities": []}, {"text": "This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.", "labels": [], "entities": [{"text": "word similarity evaluations", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.7608466943105062}, {"text": "part-of-speech tagging", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.7312814593315125}]}], "introductionContent": [{"text": "Word embeddings have been shown to improve many natural language processing applications, from language models () to information extraction, and from parsing () to machine translation ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7962824106216431}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7816630899906158}]}, {"text": "Word embeddings leverage a classical idea in natural language processing: use distributional statistics from large amounts of unlabeled data to learn representations that allow sharing * The first two authors contributed equally.", "labels": [], "entities": []}, {"text": "Code is available at https://github.com/rguthrie3/ MorphologicalPriorsForWordEmbeddings.", "labels": [], "entities": []}, {"text": "While this approach is undeniably effective, the long-tail nature of linguistic data ensures that there will always be words that are not observed in even the largest corpus.", "labels": [], "entities": []}, {"text": "There will be many other words which are observed only a handful of times, making the distributional statistics too sparse to accurately estimate the 100-or 1000-dimensional dense vectors that are typically used for word embeddings.", "labels": [], "entities": []}, {"text": "These problems are particularly acute in morphologically rich languages like German and Turkish, where each word may have dozens of possible inflections.", "labels": [], "entities": []}, {"text": "Recent work has proposed to address this issue by replacing word-level embeddings with embeddings based on subword units: morphemes () or individual characters ().", "labels": [], "entities": []}, {"text": "Such models leverage the fact that word meaning is often compositional, arising from subword components.", "labels": [], "entities": []}, {"text": "By learning representations of subword units, it is possible to generalize to rare and unseen words.", "labels": [], "entities": []}, {"text": "But while morphology and orthography are sometimes a signal of semantics, there are also many cases similar spellings do not imply similar meanings: better-batter, melon-felon, dessert-desert, etc.", "labels": [], "entities": []}, {"text": "If each word's embedding is constrained to be a deterministic function of its characters, as in prior work, then it will be difficult to learn appropriately distinct embeddings for such pairs.", "labels": [], "entities": []}, {"text": "Automated morphological analysis maybe incorrect: for example, really maybe segmented into re+ally, incorrectly suggesting a similarity to revise and review.", "labels": [], "entities": []}, {"text": "Even correct morphological segmentation maybe misleading.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.683876171708107}]}, {"text": "Consider 490 that incredible and inflammable share a prefix in-, which exerts the opposite effect in these two cases.", "labels": [], "entities": []}, {"text": "Overall, a word's observed internal structure gives evidence about its meaning, but it must be possible to override this evidence when the distributional facts point in another direction.", "labels": [], "entities": []}, {"text": "We formalize this idea using the machinery of probabilistic graphical models.", "labels": [], "entities": []}, {"text": "We treat word embeddings as latent variables, which are conditioned on a prior distribution that is based on word morphology.", "labels": [], "entities": []}, {"text": "We then maximize a variational approximation to the expected likelihood of an observed corpus of text, fitting variational parameters over latent binary word embeddings.", "labels": [], "entities": []}, {"text": "For common words, the expected word embeddings are largely determined by the expected corpus likelihood, and thus, by the distributional statistics.", "labels": [], "entities": []}, {"text": "For rare words, the prior plays a larger role.", "labels": [], "entities": []}, {"text": "Since the prior distribution is a function of the morphology, it is possible to impute embeddings for unseen words after training the model.", "labels": [], "entities": []}, {"text": "We model word embeddings as latent binary vectors.", "labels": [], "entities": []}, {"text": "This choice is based on linguistic theories of lexical semantics and morphology.", "labels": [], "entities": []}, {"text": "Morphemes are viewed as adding morphosyntactic features to words: for example, in English, un-adds a negation feature (unbelievable), -s adds a plural feature, and -ed adds a past tense feature.", "labels": [], "entities": []}, {"text": "Similarly, the lexicon is often viewed as organized in terms of features: for example, the word bachelor carries the features HUMAN, MALE, and UNMAR-RIED (.", "labels": [], "entities": [{"text": "HUMAN", "start_pos": 126, "end_pos": 131, "type": "METRIC", "confidence": 0.849226713180542}, {"text": "MALE", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9629048109054565}]}, {"text": "Each word's semantic role within a sentence can also be characterized in terms of binary features.", "labels": [], "entities": []}, {"text": "Our approach is more amenable to such theoretical models than traditional distributed word embeddings.", "labels": [], "entities": []}, {"text": "However, we can also work with the expected word embeddings, which are vectors of probabilities, and can therefore be expected to hold the advantages of dense distributed representations).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation compares the following embeddings: We train the popular word2vec CBOW (continuous bag of words) model (, using the gensim implementation.", "labels": [], "entities": []}, {"text": "SUMEMBED We compare against the baseline described in \u00a7 3.3, which can be viewed as a reimplementation of the compositional model of.", "labels": [], "entities": []}, {"text": "VAREMBED For our model, we take the expected embeddings E q, and then pass them through an inverse sigmoid function to obtain values over the entire real line.", "labels": [], "entities": [{"text": "VAREMBED", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9901295304298401}]}], "tableCaptions": [{"text": " Table 1: Word similarity evaluation results, as measured by Spearmann's \u03c1 \u00d7 100. WORD2VEC cannot be evaluated on all words,", "labels": [], "entities": [{"text": "Word similarity evaluation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6699127654234568}, {"text": "WORD2VEC", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9111595749855042}]}, {"text": " Table 2: Alignment with lexical semantic features, as measured", "labels": [], "entities": []}, {"text": " Table 3.  Both  morphologically-informed  embeddings  are  significantly better to WORD2VEC (p < .01,  two-tailed binomial test), but the difference between  SUMEMBED and VAREMBED is not significant", "labels": [], "entities": [{"text": "WORD2VEC", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.6753118634223938}, {"text": "VAREMBED", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9904768466949463}]}, {"text": " Table 3: Part-of-speech tagging accuracies", "labels": [], "entities": [{"text": "Part-of-speech tagging accuracies", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7332866191864014}]}]}