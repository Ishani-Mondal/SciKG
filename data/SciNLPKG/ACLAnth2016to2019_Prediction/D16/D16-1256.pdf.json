{"title": [], "abstractContent": [{"text": "Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9145168960094452}]}, {"text": "In many cases, however, proper morphological analysis requires hierarchical structure-especially in the case of derivational morphology.", "labels": [], "entities": []}, {"text": "In this work, we introduce a discrimina-tive joint model of morphological segmenta-tion along with the orthographic changes that occur during word formation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.6979497075080872}]}, {"text": "To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model.", "labels": [], "entities": [{"text": "discriminative segmentation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7202581465244293}]}, {"text": "Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.", "labels": [], "entities": []}], "introductionContent": [{"text": "In NLP, supervised morphological segmentation has typically been viewed as either a sequence labeling or a segmentation task.", "labels": [], "entities": [{"text": "supervised morphological segmentation", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.696222980817159}]}, {"text": "In contrast, we consider a hierarchical approach, employing a context-free grammar (CFG).", "labels": [], "entities": []}, {"text": "CFGs provide a richer model of morphology: they capture (i) the intuition that words themselves have internal constituents, which belong to different categories, as well as (ii) the order in which affixes are attached.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8798888921737671}]}, {"text": "Moreover, many morphological processes, e.g., compounding and reduplication, are best modeled as hierarchical; thus, context-free models are expressively more appropriate.", "labels": [], "entities": [{"text": "compounding", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9681957960128784}]}, {"text": "The purpose of morphological segmentation is to decompose words into smaller units, known as morphemes, which are typically taken to be the smallest meaning bearing units in language.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7139557600021362}]}, {"text": "This work concerns itself with modeling hierarchical structure over these morphemes.", "labels": [], "entities": []}, {"text": "Note a simple flat morphological segmentation can also be straightforwardly derived from the CFG parse tree.", "labels": [], "entities": [{"text": "CFG parse tree", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.9243865013122559}]}, {"text": "Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (), keyword spotting), machine translation () and parsing.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.6108579337596893}, {"text": "keyword spotting", "start_pos": 106, "end_pos": 122, "type": "TASK", "confidence": 0.7626532316207886}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8307395875453949}]}, {"text": "In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation.", "labels": [], "entities": []}, {"text": "For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le.", "labels": [], "entities": []}, {"text": "We make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone.", "labels": [], "entities": [{"text": "CFG backbone", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.930443674325943}]}, {"text": "We experimentally show that this model outperforms a semi-Markov model on flat segmentation.", "labels": [], "entities": []}, {"text": "(ii) We release the first morphology treebank, consisting of 7454 English word types, each annotated with a full constituency parse.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run a simple experiment to show the empirical utility of parsing words-we compare a WCFGbased canonical segmenter with the semi-Markov segmenter introduced in.", "labels": [], "entities": [{"text": "parsing words-we", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8892517387866974}]}, {"text": "We divide the corpus into 10 distinct train/dev/test splits with 5454 words for train and 1000 for each of dev and test.", "labels": [], "entities": []}, {"text": "We report three evaluation metrics: full form accuracy, morpheme F 1 (Van den Bosch and Daelemans, 1999) and average edit distance to the gold segmentation with boundaries marked by a distinguished symbol.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8981166481971741}, {"text": "morpheme F 1", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.8555370569229126}, {"text": "average edit distance", "start_pos": 109, "end_pos": 130, "type": "METRIC", "confidence": 0.8384637435277303}]}, {"text": "For the WCFG model, we also report constituent F 1 -typical for sentential constituency parsing-as a baseline for future systems.", "labels": [], "entities": [{"text": "constituent F 1", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.7029079894224802}, {"text": "sentential constituency parsing-as", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.8050355315208435}]}, {"text": "This F 1 measures how well we predict the whole tree (not just a segmentation).", "labels": [], "entities": [{"text": "F 1", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.9824864864349365}]}, {"text": "For all models, we use L 2 regularization and run 100 epochs of ADAGRAD () with early stopping.", "labels": [], "entities": [{"text": "ADAGRAD", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9929861426353455}]}, {"text": "We tune the regularization coefficient by grid search considering \u03bb \u2208 {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.", "labels": [], "entities": []}, {"text": "The hierarchical WCFG model outperforms the flat semi-Markov model on all metrics on the segmentation task.", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.9155682325363159}]}, {"text": "This shows that modeling structure among the morphemes, indeed, does help segmentation.", "labels": [], "entities": []}, {"text": "The largest improvements are found under the morpheme F 1 metric (\u2248 6.5 points).", "labels": [], "entities": [{"text": "morpheme F 1 metric", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.7253037095069885}]}, {"text": "In contrast, accuracy improves by < 1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996347427368164}]}, {"text": "Edit distance is in between with an improvement of 0.2 characters.", "labels": [], "entities": [{"text": "Edit distance", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9263086318969727}]}, {"text": "Accuracy, in general, is an all or nothing metric since it requires getting every canonical segment correct.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9905866980552673}]}, {"text": "Morpheme F 1 , on the other hand, gives us partial credit.", "labels": [], "entities": [{"text": "Morpheme F 1", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8707903424898783}]}, {"text": "Thus, what this shows us is that the WCFG gets a lot more of the morphemes in the held-out set correct, even if it only gets a few complete forms correct.", "labels": [], "entities": []}, {"text": "We provide additional results evaluating the entire tree with constituency F 1 as a future baseline.", "labels": [], "entities": []}], "tableCaptions": []}