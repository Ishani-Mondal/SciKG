{"title": [{"text": "Bilingually-constrained Synthetic Data for Implicit Discourse Relation Recognition", "labels": [], "entities": [{"text": "Implicit Discourse Relation Recognition", "start_pos": 43, "end_pos": 82, "type": "TASK", "confidence": 0.748711496591568}]}], "abstractContent": [{"text": "To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition.", "labels": [], "entities": [{"text": "discourse relation recognition", "start_pos": 122, "end_pos": 152, "type": "TASK", "confidence": 0.5974161028862}]}, {"text": "These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages.", "labels": [], "entities": []}, {"text": "Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chi-nese CDTB data sets.", "labels": [], "entities": [{"text": "English PDTB and Chi-nese CDTB data sets", "start_pos": 125, "end_pos": 165, "type": "DATASET", "confidence": 0.8519233039447239}]}], "introductionContent": [{"text": "Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.9376875460147858}, {"text": "machine translation", "start_pos": 205, "end_pos": 224, "type": "TASK", "confidence": 0.8152060806751251}]}, {"text": "Implicit discourse relation recognition (DRR imp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but).", "labels": [], "entities": [{"text": "Implicit discourse relation recognition (DRR imp )", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8449327275156975}]}, {"text": "Most work resorts to large amounts of manually designed features, or distributed features learned via neural network models.", "labels": [], "entities": []}, {"text": "The above methods usually suffer from limited labeled data.", "labels": [], "entities": []}, {"text": "attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data.", "labels": [], "entities": []}, {"text": "These data are usually called as syn- * Corresponding author.", "labels": [], "entities": []}, {"text": "thetic implicit data (hereafter SynData).", "labels": [], "entities": []}, {"text": "However, argue that SynData has two drawbacks: 1) meaning shifts in some cases when removing connectives, and 2) a different word distribution with the real implicit data.", "labels": [], "entities": []}, {"text": "They also show that using SynData directly degrades the performance.", "labels": [], "entities": []}, {"text": "Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation society reckon existence youth problems, but many young people think themselves no problems.", "labels": [], "entities": []}, {"text": "taking two arguments (Arg1 and Arg2).", "labels": [], "entities": [{"text": "Arg1", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9890651702880859}, {"text": "Arg2", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9691298604011536}]}, {"text": "Different from previous work, we propose to construct bilingually-constrained synthetic implicit data (called BiSynData) for DRR imp , which can alleviate the drawbacks of SynData.", "labels": [], "entities": []}, {"text": "Our method is inspired by the findings that a discourse instance expressed implicitly in one language maybe expressed explicitly in another.", "labels": [], "entities": []}, {"text": "For example, Zhou and Xue", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on both the English PDTB and Chinese CDTB data sets.", "labels": [], "entities": [{"text": "English PDTB and Chinese CDTB data sets", "start_pos": 35, "end_pos": 74, "type": "DATASET", "confidence": 0.8202274867466518}]}, {"text": "We tokenize English data and segment Chinese data using the Stanford CoreNLP toolkit.", "labels": [], "entities": [{"text": "tokenize English", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.8736824095249176}, {"text": "Stanford CoreNLP toolkit", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.9507697423299154}]}, {"text": "shows the results of MT N combining our BiSynData (denoted as MT N bi ) on the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9569381475448608}]}, {"text": "ST N means we train MT N with only the main task.", "labels": [], "entities": [{"text": "ST N", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8837628364562988}, {"text": "MT N", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.9092833399772644}]}, {"text": "On the macro F 1 , MT N bi gains an improvement of 4.17% over ST N . The improvement is significant under one-tailed t-test (p<0.05).", "labels": [], "entities": []}, {"text": "A closer look into the results shows that MT N bi performs better across all relations, on the precision, recall and F 1 score, except a little drop on the recall of Cont.", "labels": [], "entities": [{"text": "MT N", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.7064405083656311}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9996802806854248}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9985131621360779}, {"text": "F 1 score", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9910764296849569}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9990735054016113}]}, {"text": "The reason for the recall drop of Cont is not clear.", "labels": [], "entities": [{"text": "recall drop of Cont", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.8340084478259087}]}, {"text": "The greatest improvement is observed on Comp, up to 6.36% F 1 score.", "labels": [], "entities": [{"text": "Comp", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8026781678199768}, {"text": "F 1 score", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9902944564819336}]}, {"text": "The possible reason is that only while is ambiguous about Comp and T emp, while as, when and since are all ambiguous about T emp and Cont, among top 10 connectives in our BiSynData.", "labels": [], "entities": [{"text": "BiSynData", "start_pos": 171, "end_pos": 180, "type": "DATASET", "confidence": 0.8665115237236023}]}, {"text": "Meanwhile the amount of labeled data for Comp is relatively small.", "labels": [], "entities": []}, {"text": "Overall, using BiSynData under our multi-task model achieves significant improvements on the English DRR imp . We believe the reasons for the improvements are twofold: 1) the added synthetic English instances from our BiSynData can alleviate the meaning shift problem, and 2) a multi-task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.", "labels": [], "entities": [{"text": "meaning shift", "start_pos": 246, "end_pos": 259, "type": "TASK", "confidence": 0.7232786864042282}]}, {"text": "Considering some of the English connectives (e.g., while) are highly ambiguous, we compare our method with ones that uses only unambiguous connectives.", "labels": [], "entities": []}, {"text": "Specifically, we first discard as, when, while and since in top 20 connectives, and get 22,999 synthetic instances.", "labels": [], "entities": []}, {"text": "Then, we leverage these instances in two different ways: 1) using them in our multi-task model as above, and 2) using them as additional training data directly after mapping unambiguous connectives into relations.", "labels": [], "entities": []}, {"text": "Both methods using only unambiguous connectives do not achieve better performance.", "labels": [], "entities": []}, {"text": "One possible reason is that these synthetic instances become more unbalanced after discarding ones with ambiguous connectives.", "labels": [], "entities": []}, {"text": "We also compare MT N bi with recent systems using additional training data.", "labels": [], "entities": [{"text": "MT N", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.8163845241069794}]}, {"text": "select explicit instances that are similar to the implicit ones via connective classification, to enrich the training data.", "labels": [], "entities": [{"text": "connective classification", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.7727918326854706}]}, {"text": "use a multi-task model with three auxiliary tasks: 1) conn: connective classification on explicit instances, 2) exp: relation classification on the labeled explicit instances in the PDTB, and 3) rst: relation classification on the labeled RST corpus (William and Thompson,  1988), which defines different discourse relations with that in the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.9273680448532104}, {"text": "RST corpus (William and Thompson,  1988)", "start_pos": 239, "end_pos": 279, "type": "DATASET", "confidence": 0.8122561507754855}]}, {"text": "The results are shown in Table 3.", "labels": [], "entities": []}, {"text": "Although achieve the stateof-the-art performance (Line 5), they use two additional labeled corpora.", "labels": [], "entities": []}, {"text": "We can find that MT N bi (Line 6) yields better results than those systems incorporating SynData (Line 1, 2 and 3), or even the labeled RST (Line 4).", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.8053608536720276}]}, {"text": "These results confirm that BiSynData can indeed alleviate the disadvantages of SynData effectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top 10 most frequent connectives in our BiSynData.", "labels": [], "entities": [{"text": "BiSynData", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.8500195741653442}]}, {"text": " Table 2: Results of 4-way classification on the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.529975950717926}]}, {"text": " Table 3: Comparison with recent systems on the PDTB. conn+", "labels": [], "entities": [{"text": "PDTB. conn+", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.9749311357736588}]}, {"text": " Table 4: Results of 3-way classification on the CDTB.", "labels": [], "entities": [{"text": "the CDTB", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.8888514339923859}]}, {"text": " Table 5: M T N with different auxiliary tasks on the CDTB.", "labels": [], "entities": [{"text": "CDTB", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.978692352771759}]}]}