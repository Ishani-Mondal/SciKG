{"title": [{"text": "Event participant modelling with neural networks", "labels": [], "entities": [{"text": "Event participant modelling", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6437475383281708}]}], "abstractContent": [{"text": "A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal.", "labels": [], "entities": []}, {"text": "As shown in, e.g., Bicknell et al.", "labels": [], "entities": []}, {"text": "(2010), event-level knowledge does affect human expectations for verbal arguments.", "labels": [], "entities": []}, {"text": "For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos.", "labels": [], "entities": []}, {"text": "Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations.", "labels": [], "entities": []}, {"text": "Furthermore, such a model can be used to provide a probability distribution over fillers fora thematic role which is not mentioned in the text at all.", "labels": [], "entities": []}, {"text": "To this end, we train two neural network models (an incremental one and a non-incremental one) on large amounts of automatically role-labelled text.", "labels": [], "entities": []}, {"text": "Our models are probabilistic and can handle several roles at once, which also enables them to learn interactions between different role fillers.", "labels": [], "entities": []}, {"text": "Evaluation shows a drastic improvement over current state-of-the-art systems on modelling human thematic fit judgements , and we demonstrate via a sentence similarity task that the system learns highly useful embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our goals in this paper are to learn a representation of events and their thematic roles based on large quantities of automatically role-labelled text and to be able to calculate probability distributions over the possible role fillers of specific missing roles.", "labels": [], "entities": []}, {"text": "In this sense, the task is closely related to work on selectional preference acquisition (Van de Cruys, 2014).", "labels": [], "entities": [{"text": "selectional preference acquisition", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.7620011766751608}]}, {"text": "We focus hereon the roles agent, patient, location, time, manner and the predicate itself.", "labels": [], "entities": []}, {"text": "The model we develop is trained to represent the eventrelevant context and hence systematically captures long-range dependencies.", "labels": [], "entities": []}, {"text": "This has been previously shown to be beneficial also for more general language modelling tasks (e.g.,).", "labels": [], "entities": [{"text": "general language modelling tasks", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.724540077149868}]}, {"text": "This type of modelling is potentially relevant to a wide range of tasks, for instance for performing thematic fit judgment tasks, detecting anomalous events (, or predicting event structure that is not explicitly present in the text.", "labels": [], "entities": []}, {"text": "The latter could be useful for inferring missing information in entailment tasks or improving identification of thematic roles outside the sentence containing the predicate.", "labels": [], "entities": [{"text": "identification of thematic roles outside the sentence containing the predicate", "start_pos": 94, "end_pos": 172, "type": "TASK", "confidence": 0.7539964079856872}]}, {"text": "Potential applications also include predicate prediction based on arguments and roles, which has been noted to be relevant for simultaneous machine translation fora verb-final to a verb-medial source language).", "labels": [], "entities": [{"text": "predicate prediction", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.8875329196453094}]}, {"text": "Within cognitive modelling, our model could help to more accurately estimate semantic surprisal for broadcoverage texts, when used in combination with an incremental role labeller (e.g.,, or to provide surprisal estimates for content words as a control variable for psycholinguistic experimental materials.", "labels": [], "entities": []}, {"text": "In this work, we focus on the predictability of verbs and nouns, and we suggest that the predictability of these words depends to a large extent on the relationship of these words to other nouns and verbs, especially those connected via the same event.", "labels": [], "entities": []}, {"text": "We choose a neural network (NN) model because we found that results from existing related models, e.g. Baroni and Lenci's Distributional Memory, depend heavily on how exactly the distributional space is defined, while having no principled way of optimizing the space.", "labels": [], "entities": []}, {"text": "A crucial advantage of a neural network-based approach is thus that the model can be trained to optimize the distributional representation for the task.", "labels": [], "entities": []}, {"text": "Our model is trained specifically to predict missing semantic role-fillers based on the predicate and other available role-fillers of that predicate.", "labels": [], "entities": []}, {"text": "The model can also predict the predicate based on the semantic roles and their fillers.", "labels": [], "entities": []}, {"text": "In our model, there is no difference in how the semantic roles or the predicate are treated.", "labels": [], "entities": []}, {"text": "Thus, when we refer hereto roles, we usually mean both semantic roles and the predicate, unless otherwise explicitly stated.", "labels": [], "entities": []}, {"text": "Our model is compositional in that it has access to several role-fillers (including the verb) at the same time, and can thus represent interdependencies between participants of an event and predict from a combined representation.", "labels": [], "entities": []}, {"text": "Consider, for example, the predicate serve, whose likely patients include e.g., drinks.", "labels": [], "entities": []}, {"text": "If we had the agent robber, we would like to be able to predict a patient like sentence, in the sense of \"the robber will serve his sentence.", "labels": [], "entities": []}, {"text": "\" This task is related to modelling thematic fit.", "labels": [], "entities": []}, {"text": "In this paper, we evaluate our model on a variety of thematic fit rating datasets as well as on a sentence similarity dataset that tests for successful compositionality in our model's representations.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: \u2022 We compare two novel NN models for generating a probability distribution over selectional preferences given one or more roles and fillers.", "labels": [], "entities": []}, {"text": "\u2022 We show that our technique outperforms state of the art thematic fit models on many datasets.", "labels": [], "entities": []}, {"text": "\u2022 We show that the embeddings thus obtained are effective in measuring sentence similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to see whether our model accurately represents events and their typical thematic role fillers, we evaluate our model on a range of existing datasets containing human thematic fit ratings.", "labels": [], "entities": []}, {"text": "This evaluation also allows us to compare our model to existing models that have been used on this task.) model, further developed and evaluated in and the neural network predict model described in.", "labels": [], "entities": []}, {"text": "NN RF is the non-incremental model presented in this article.", "labels": [], "entities": [{"text": "NN RF", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.5855264216661453}]}, {"text": "Our model maps ARG2 in Pado to OTHER role.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9419509172439575}]}, {"text": "Significances were calculated using paired two-tailed significance tests for correlations.", "labels": [], "entities": [{"text": "Significances", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9449880719184875}]}, {"text": "NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for Pado and Ferretti instruments.", "labels": [], "entities": [{"text": "RF", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.46678903698921204}, {"text": "Greenberg and Ferretti location datasets", "start_pos": 68, "end_pos": 108, "type": "DATASET", "confidence": 0.6985405325889588}, {"text": "BL2010", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.8574163913726807}, {"text": "GSD2015", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.6043702960014343}, {"text": "McRae", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.9477248191833496}]}, {"text": "To show that our model learns to represent input words and their roles in a useful way that reflects the meaning and interactions between inputs, we evaluate our non-incremental model on a sentence similarity task from.", "labels": [], "entities": [{"text": "sentence similarity task", "start_pos": 189, "end_pos": 213, "type": "TASK", "confidence": 0.7655987540880839}]}, {"text": "We assign similarity scores to sentence pairs by computing representations for each sentence by tak- ing the hidden layer state (Equation 8) of the nonincremental model given the words in the sentence and their corresponding roles.", "labels": [], "entities": []}, {"text": "Sentence similarity is then rated with the cosine similarity between the representations of the two sentences.", "labels": [], "entities": []}, {"text": "Spearman's rank correlation between the cosine similarities produced by our model and human ratings are shown in.", "labels": [], "entities": []}, {"text": "Our model achieves much higher correlation with human ratings than the best result reported by, showing our model's ability to compose meaningful representations of multiple input words and their roles.", "labels": [], "entities": []}, {"text": "We also compare our model with another NN word representation model baseline that does not embed role information; by this comparison, we can determine the size of the improvement brought by our role-specific embeddings.", "labels": [], "entities": []}, {"text": "The baseline sentence representations are constructed by elementwise addition of pre-trained word2vec (Mikolov et al., 2013) word embeddings . Scores are again computed by using cosine similarity.", "labels": [], "entities": []}, {"text": "The large gap between our model's and word2vec baseline's performance illustrates the importance of embedding role information in word representations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexities on dev/test dataset.", "labels": [], "entities": []}, {"text": " Table 2: Thematic fit evaluation scores, consisting of Spearman's \u03c1 correlations between average human judgements and model", "labels": [], "entities": [{"text": "Spearman's \u03c1 correlations", "start_pos": 56, "end_pos": 81, "type": "METRIC", "confidence": 0.6125008314847946}]}, {"text": " Table 3: Per role thematic-fit evaluation scores in terms of", "labels": [], "entities": []}, {"text": " Table 4: Accuracies on the Bicknell evaluation task.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.997864305973053}, {"text": "Bicknell evaluation task", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6075327396392822}]}, {"text": " Table 5: Sentence similarity evaluation scores on GS2013", "labels": [], "entities": [{"text": "Sentence similarity evaluation", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.9215949575106303}, {"text": "GS2013", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.8156745433807373}]}]}