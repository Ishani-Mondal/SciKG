{"title": [{"text": "An Unsupervised Probability Model for Speech-to-Translation Alignment of Low-Resource Languages", "labels": [], "entities": [{"text": "Speech-to-Translation Alignment of Low-Resource Languages", "start_pos": 38, "end_pos": 95, "type": "TASK", "confidence": 0.8295409142971039}]}], "abstractContent": [{"text": "For many low-resource languages, spoken language resources are more likely to be annotated with translations than with transcriptions.", "labels": [], "entities": []}, {"text": "Translated speech data is potentially valuable for documenting endangered languages or for training speech translation systems.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7899234890937805}]}, {"text": "A first step towards making use of such data would be to automatically align spoken words with their translations.", "labels": [], "entities": []}, {"text": "We present a model that combines Dyer et al.'s reparam-eterization of IBM Model 2 (fast_align) and k-means clustering using Dynamic Time Warping as a distance measure.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9067155122756958}]}, {"text": "The two components are trained jointly using expectation-maximization.", "labels": [], "entities": []}, {"text": "In an extremely low-resource scenario, our model performs significantly better than both a neural model and a strong baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "For many low-resource languages, speech data is easier to obtain than textual data.", "labels": [], "entities": []}, {"text": "And because speech transcription is a costly and slow process, speech is more likely to be annotated with translations than with transcriptions.", "labels": [], "entities": [{"text": "speech transcription", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7168077528476715}]}, {"text": "This translated speech is a potentially valuable source of information -for example, for documenting endangered languages or for training speech translation systems.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7823885381221771}]}, {"text": "In language documentation, data is usable only if it is interpretable.", "labels": [], "entities": []}, {"text": "To make a collection of speech data usable for future studies of the language, something resembling interlinear glossed text (transcription, morphological analysis, word glosses, free translation) would be needed at minimum.", "labels": [], "entities": []}, {"text": "New technologies are being developed to facilitate collection of translations (, and there already exist recent examples of parallel speech collection efforts focused on endangered languages ( . As for the other annotation layers, one might hope that a first pass could be done automatically.", "labels": [], "entities": [{"text": "collection of translations", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.8464029828707377}]}, {"text": "A first step towards this goal would be to automatically align spoken words with their translations, capturing information similar to that captured byword glosses.", "labels": [], "entities": []}, {"text": "In machine translation, statistical models have traditionally required alignments between the source and target languages as the first step of training.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7679802179336548}]}, {"text": "Therefore, producing alignments between speech and text would be a natural first step towards MT systems operating directly on speech.", "labels": [], "entities": [{"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9926665425300598}]}, {"text": "We present a model that, in order to learn such alignments, adapts and combines two components: Dyer et al.'s reparameterization of IBM Model 2 (), more commonly known as fast_align, and k-means clustering using Dynamic Time as a distance measure.", "labels": [], "entities": []}, {"text": "The two components are trained jointly using expectation-maximization.", "labels": [], "entities": []}, {"text": "We experiment on two language pairs.", "labels": [], "entities": []}, {"text": "One is Spanish-English, using the CALLHOME and Fisher corpora.", "labels": [], "entities": [{"text": "CALLHOME", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.5666404962539673}]}, {"text": "The other is Griko-Italian; Griko is an endangered language for which we created (and make freely available) gold-standard translations and word alignments (.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.6869398951530457}]}, {"text": "In all cases, our model outperforms both a naive but strong baseline and a neural model ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on two language pairs, Spanish-English and Griko-Italian, against two baseline methods, a naive baseline, and the model of.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our model achieves higher precision and F-score than", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9997344613075256}, {"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9990556836128235}]}, {"text": " Table 2: Model performance (F-score) is generally consistent", "labels": [], "entities": [{"text": "F-score)", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9837081134319305}]}]}