{"title": [{"text": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "labels": [], "entities": [{"text": "Dialogue Response", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7864289283752441}]}], "abstractContent": [{"text": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available.", "labels": [], "entities": [{"text": "dialogue response generation", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.8391622304916382}]}, {"text": "Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response.", "labels": [], "entities": [{"text": "response generation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.9189387857913971}, {"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7176845520734787}]}, {"text": "We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain.", "labels": [], "entities": []}, {"text": "We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important aspect of dialogue response generation systems, which are trained to produce a reasonable utterance given a conversational context, is how to evaluate the quality of the generated response.", "labels": [], "entities": [{"text": "dialogue response generation", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.8528788685798645}]}, {"text": "Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (, which are relevant when the dialogue is task-focused.", "labels": [], "entities": []}, {"text": "We call models optimized for such supervised objectives supervised dialogue models, while those that do not are unsupervised dialogue models.", "labels": [], "entities": []}, {"text": "This paper focuses on unsupervised dialogue response generation models, such as chatbots.", "labels": [], "entities": [{"text": "dialogue response generation", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6889047026634216}]}, {"text": "These * Denotes equal contribution.", "labels": [], "entities": []}, {"text": "models are receiving increased attention, particularly using end-to-end training with neural networks (.", "labels": [], "entities": []}, {"text": "This avoids the need to collect supervised labels on a large scale, which can be prohibitively expensive.", "labels": [], "entities": []}, {"text": "However, automatically evaluating the quality of these models remains an open question.", "labels": [], "entities": []}, {"text": "Automatic evaluation metrics would help accelerate the deployment of unsupervised response generation systems.", "labels": [], "entities": [{"text": "response generation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7002418637275696}]}, {"text": "Faced with similar challenges, other natural language tasks have successfully developed automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "For example, BLEU) and METEOR () are now standard for evaluating machine translation models, and ROUGE) is often used for automatic summarization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9988705515861511}, {"text": "METEOR", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9697985649108887}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7305805087089539}, {"text": "ROUGE", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9954338669776917}, {"text": "summarization", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.8686268925666809}]}, {"text": "These metrics have recently been adopted by dialogue researchers.", "labels": [], "entities": []}, {"text": "However these metrics assume that valid responses have significant word overlap with the ground truth responses.", "labels": [], "entities": []}, {"text": "This is a strong assumption for dialogue systems, where there is significant diversity in the space of valid responses to a given context.", "labels": [], "entities": []}, {"text": "This is illustrated in, where two reasonable responses are proposed to the context, but these responses do not share any words in common and do not have the same semantic meaning.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the correlation between the scores from several automatic evaluation metrics and human judgements of dialogue response quality, fora variety of response generation models.", "labels": [], "entities": []}, {"text": "We consider both statistical word-overlap similar- ity metrics such as BLEU, METEOR, and ROUGE, and word embedding metrics derived from word embedding models such as.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9986234903335571}, {"text": "METEOR", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9861966967582703}, {"text": "ROUGE", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9934648871421814}]}, {"text": "We find that all metrics show either weak or no correlation with human judgements, despite the fact that word overlap metrics have been used extensively in the literature for evaluating dialogue response models (see above, and).", "labels": [], "entities": []}, {"text": "In particular, we show that these metrics have only a small positive correlation on the chitchat oriented Twitter dataset, and no correlation at all on the technical Ubuntu Dialogue Corpus.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 106, "end_pos": 121, "type": "DATASET", "confidence": 0.6918813139200211}, {"text": "Ubuntu Dialogue Corpus", "start_pos": 166, "end_pos": 188, "type": "DATASET", "confidence": 0.8773459196090698}]}, {"text": "For the word embedding metrics, we show that this is true even though all metrics are able to significantly distinguish between baseline and state-of-the-art models across multiple datasets.", "labels": [], "entities": []}, {"text": "We further highlight the shortcomings of these metrics using: a) a statistical analysis of our survey's results; b) a qualitative analysis of examples from our data; and c) an exploration of the sensitivity of the metrics.", "labels": [], "entities": []}, {"text": "Our results indicate that a shift must be made in the research community away from these metrics, and highlight the need fora new metric that correlates more strongly with human judgement.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given a dialogue context and a proposed response, our goal is to automatically evaluate how appropriate the proposed response is to the conversation.", "labels": [], "entities": []}, {"text": "We focus on metrics that compare it to the ground truth response of the conversation.", "labels": [], "entities": []}, {"text": "In particular, we investigate two approaches: word based similarity metrics and word-embedding based similarity metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Correlation between each metric and human judgements for each response. Correlations shown in  the human row result from randomly dividing human judges into two groups.", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9443938732147217}]}, {"text": " Table 4: Correlation between BLEU metric and  human judgements after removing stopwords and  punctuation for the Twitter dataset.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.9479052424430847}, {"text": "Twitter dataset", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.8698249161243439}]}, {"text": " Table 5: Effect of differences in response length  for the Twitter dataset, \u2206w = absolute difference in  #words between a ground truth response and pro- posed response", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.8581910729408264}]}]}