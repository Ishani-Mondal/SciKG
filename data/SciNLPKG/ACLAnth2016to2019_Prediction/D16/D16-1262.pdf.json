{"title": [{"text": "Global Neural CCG Parsing with Optimality Guarantees", "labels": [], "entities": [{"text": "Global Neural CCG Parsing", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6439180970191956}]}], "abstractContent": [{"text": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding.", "labels": [], "entities": [{"text": "global recursive neural parsing", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.580169714987278}]}, {"text": "To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees.", "labels": [], "entities": []}, {"text": "Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser.", "labels": [], "entities": []}, {"text": "We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena.", "labels": [], "entities": []}, {"text": "The global model is trained with a novel objective that encourages the parser to search both efficiently and accurately.", "labels": [], "entities": []}, {"text": "The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.7149282991886139}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9940424561500549}, {"text": "F1", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9955282807350159}]}, {"text": "The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures.", "labels": [], "entities": []}, {"text": "However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in away that allows effective dynamic programming.", "labels": [], "entities": []}, {"text": "Existing work has therefore used greedy inference techniques such as beam search or reranking (.", "labels": [], "entities": [{"text": "beam search", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.8481851816177368}]}, {"text": "We introduce the first global recursive neural parsing approach with optimality guarantees for decoding and use it to build a state-of-the-art CCG parser.", "labels": [], "entities": [{"text": "global recursive neural parsing", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.5844103544950485}]}, {"text": "To enable learning of global representations, we modify the parser to search directly in the space of all possible parse trees with no dynamic programming.", "labels": [], "entities": []}, {"text": "Optimality guarantees come from A * search, which provides a certificate of optimality if run to completion with a heuristic that is abound on the future cost.", "labels": [], "entities": []}, {"text": "Generalizing A * to global models is challenging; these models also break the locality assumptions used to efficiently compute existing A * heuristics ().", "labels": [], "entities": []}, {"text": "Rather than directly replacing local models, we show that they can simply be augmented by adding a score from a global model that is constrained to be non-positive and has a trivial upper bound of zero.", "labels": [], "entities": []}, {"text": "The global model, in effect, only needs to model the remaining non-local phenomena.", "labels": [], "entities": []}, {"text": "In our experiments, we use a recent factored A * CCG parser () for the local model, and we train a Tree-LSTM () to model global structure.", "labels": [], "entities": []}, {"text": "Finding a model that achieves these A * guarantees in practice is a challenging learning problem.", "labels": [], "entities": [{"text": "A * guarantees", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.9606685241063436}]}, {"text": "Traditional structured prediction objectives focus on ensuring that the gold parse has the highest score).", "labels": [], "entities": []}, {"text": "This condition is insufficient in our case, since it does not guarantee that the search will terminate in subexponential time.", "labels": [], "entities": []}, {"text": "We instead introduce anew objective that optimizes efficiency as well as accuracy.", "labels": [], "entities": [{"text": "efficiency", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9689721465110779}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.998492956161499}]}, {"text": "Our loss function is defined over states of the A * search agenda, and it penalizes the model whenever the top agenda item is not apart of the gold parse.", "labels": [], "entities": []}, {"text": "Minimizing this loss encourages the model to return the correct parse as quickly as possible.", "labels": [], "entities": []}, {"text": "The combination of global representations and optimal decoding enables our parser to achieve state-of-the-art accuracy for Combinatory Categorial Grammar (CCG) parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.999190628528595}, {"text": "Combinatory Categorial Grammar (CCG) parsing", "start_pos": 123, "end_pos": 167, "type": "TASK", "confidence": 0.710883515221732}]}, {"text": "Despite being intractable in the worst case, the parser in practice is highly efficient.", "labels": [], "entities": []}, {"text": "It finds optimal parses for 99.9% of held out sentences while exploring just 190 subtrees on average-allowing it to outperform beam search in both speed and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9945419430732727}]}], "datasetContent": [{"text": "For the local model, we use the supertag-factored model of.", "labels": [], "entities": []}, {"text": "Here, s local (e) corresponds to a supertag score if a HEAD(e) is a leaf and zero otherwise.", "labels": [], "entities": []}, {"text": "The outside score heuristic is computed by summing the maximum supertag score for every word outside of each span.", "labels": [], "entities": []}, {"text": "In the reported results, we back off to the supertag-factored model after the forest size exceeds 500,000, the agenda size exceeds 2 million, or we build more than 200,000 recursive units in the neural network.: Labeled F1 for CCGbank dependencies on the CCGbank development and test set for our system Global A * and the baselines.", "labels": [], "entities": [{"text": "F1", "start_pos": 220, "end_pos": 222, "type": "METRIC", "confidence": 0.9841200709342957}, {"text": "CCGbank development and test set", "start_pos": 255, "end_pos": 287, "type": "DATASET", "confidence": 0.7960556387901306}]}, {"text": "Our full system is trained with all-violations updates.", "labels": [], "entities": []}, {"text": "During training, we lower the forest size limit to 2000 to reduce training times.", "labels": [], "entities": []}, {"text": "The model is trained for 30 epochs using ADAM), and we use early stopping based on development F1.", "labels": [], "entities": []}, {"text": "The LSTM cells and hidden states have 64 dimensions.", "labels": [], "entities": []}, {"text": "We initialize word representations with pre-trained 50-dimensional embeddings from.", "labels": [], "entities": []}, {"text": "Embeddings for categories have 16 dimensions and are randomly initialized.", "labels": [], "entities": []}, {"text": "We also apply dropout with a probability of 0.4 at the word embedding layer during training.", "labels": [], "entities": []}, {"text": "Since the structure of the neural network is dynamically determined, we do not use mini-batches.", "labels": [], "entities": []}, {"text": "The neural networks are implemented using the CNN library, 1 and the CCG parser is implemented using the EasySRL library.", "labels": [], "entities": [{"text": "CNN library", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9010988175868988}, {"text": "EasySRL library", "start_pos": 105, "end_pos": 120, "type": "DATASET", "confidence": 0.9491092264652252}]}, {"text": "The code is available online.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Labeled F1 for CCGbank dependencies on the CCG-", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9232523441314697}]}, {"text": " Table 4: Parsing results trained with different update methods.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9553178548812866}]}, {"text": " Table 5: Comparison of various decoders using the same model", "labels": [], "entities": []}]}