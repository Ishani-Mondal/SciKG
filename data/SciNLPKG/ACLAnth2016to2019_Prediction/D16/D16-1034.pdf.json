{"title": [{"text": "PaCCSS-IT: A Parallel Corpus of Complex-Simple Sentences for Automatic Text Simplification", "labels": [], "entities": [{"text": "PaCCSS-IT", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9431804418563843}, {"text": "Automatic Text Simplification", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.5860207279523214}]}], "abstractContent": [{"text": "In this paper we present PaCCSS-IT, a Parallel Corpus of Complex-Simple Sentences for ITalian.", "labels": [], "entities": [{"text": "PaCCSS-IT", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.8565607666969299}]}, {"text": "To build the resource we develop anew method for automatically acquiring a corpus of complex-simple paired sentences able to intercept structural transformations and particularly suitable for text simplification.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.7557888031005859}]}, {"text": "The method requires a wide amount of texts that can be easily extracted from the web making it suitable also for less-resourced languages.", "labels": [], "entities": []}, {"text": "We test it on the Italian language making available the biggest Italian corpus for automatic text simplification.", "labels": [], "entities": [{"text": "automatic text simplification", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.5763384203116099}]}], "introductionContent": [{"text": "The availability of monolingual parallel corpora is a prerequisite for research on automatic text simplification (ATS), i.e. the task of reducing sentence complexity by preserving the original meaning.", "labels": [], "entities": [{"text": "text simplification (ATS)", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.8339068710803985}]}, {"text": "This has been recently shown for different languages, e.g. English (),), French (), Portuguese (,), Italian (.", "labels": [], "entities": []}, {"text": "While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus ( and, more recently, the Newsela corpus (, for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application of data-driven techniques to automatically induce simplification operations.", "labels": [], "entities": [{"text": "Newsela corpus", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.9776656031608582}]}, {"text": "This is true for the language we are considering, i.e. Italian, where the only documented corpus for text simplification contains approximately 1,000 aligned original and manually simplified sentences (.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.6810538619756699}]}, {"text": "In this paper we present PaCCSS-IT, a Parallel Corpus of Complex-Simple Aligned Sentences for ITalian.", "labels": [], "entities": [{"text": "PaCCSS-IT", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.8428078293800354}]}, {"text": "To build the resource we developed anew approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems.", "labels": [], "entities": []}, {"text": "The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection () and evaluation), question answering, textual entailment, machine translation), short answer scoring (), domain adaptation of dependency parsing.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7437660992145538}, {"text": "paraphrase detection", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.7477960288524628}, {"text": "question answering", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.8728601932525635}, {"text": "machine translation", "start_pos": 217, "end_pos": 236, "type": "TASK", "confidence": 0.7048275470733643}, {"text": "short answer scoring", "start_pos": 239, "end_pos": 259, "type": "TASK", "confidence": 0.6007192432880402}, {"text": "domain adaptation", "start_pos": 264, "end_pos": 281, "type": "TASK", "confidence": 0.7832321524620056}, {"text": "dependency parsing", "start_pos": 285, "end_pos": 303, "type": "TASK", "confidence": 0.7445197999477386}]}, {"text": "Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence () or methods taking into account also the order in which information is presented ().", "labels": [], "entities": [{"text": "TF/IDF score", "start_pos": 254, "end_pos": 266, "type": "METRIC", "confidence": 0.8704131245613098}]}, {"text": "Differently from these methods, our approach contains two important novelties: the typology of the starting data and consequently the methodology developed to build the complex-simple aligned corpus.", "labels": [], "entities": []}, {"text": "To overcome the scarcity of large parallel corpora of complex and simple texts in lessresourced languages like Italian, we started from a wide amount of texts that can be easily extracted from the web for all languages.", "labels": [], "entities": []}, {"text": "This makes our method less expensive since it does not need a manually created corpus of aligned documents.", "labels": [], "entities": []}, {"text": "The proposed alignment method has been strongly shaped by the perspective from which we investigate text simplification, i.e. syntactic rather than lexical simplification.", "labels": [], "entities": []}, {"text": "While lexical simplification aims at the substitution of complex words by simpler synonyms, syntactic simplification attempts to reduce complexity at grammatical level.", "labels": [], "entities": []}, {"text": "As shown by comparative analyses of monolingual parallel corpora in many languages, syntactic simplification concerns transformations affecting e.g. verbal features, the order of phrases or the deletion of redundant or unnecessary words (.", "labels": [], "entities": [{"text": "syntactic simplification", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.8484525978565216}]}, {"text": "Following this second perspective we define a method for bootstrapping and pairing sentences that intercepts simplification operations at morpho-syntactic and syntactic level typically used by human experts when simplify real texts.", "labels": [], "entities": [{"text": "bootstrapping and pairing sentences", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.6632259711623192}]}, {"text": "Section 2 illustrates the approach to automatically acquire the corpus of complex-simple aligned sentences.", "labels": [], "entities": []}, {"text": "In Section 3, the approach is tested and tuned on a development corpus.", "labels": [], "entities": []}, {"text": "In Section 4, our approach is applied on a large corpus thus obtaining the final corpus of paired sentences, named PaCCSS-IT.", "labels": [], "entities": [{"text": "PaCCSS-IT", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.9477825164794922}]}, {"text": "In this last section, we also provide a global evaluation of the whole process and a qualitative analysis of the linguistic phenomena related to sentence complexity that we intercepted.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to tune and evaluate each step of the proposed approach, we tested it on PAIS`APAIS` PAIS`A.", "labels": [], "entities": [{"text": "PAIS`APAIS` PAIS`A", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.6848077774047852}]}, {"text": "We first pruned from the corpus the sentences with a number of tokens <5 and >40.", "labels": [], "entities": []}, {"text": "The resulting sentences were then grouped with respect to their shared POS (i.e. nouns, verbs, numerals, personal pronouns and negative adverbs) and paired using cosine similarity.", "labels": [], "entities": []}, {"text": "We obtained 256,383 clusters containing at least two sentences.", "labels": [], "entities": []}, {"text": "In order to discard different and equal or quasi-equal sentences we empirically set two cosine pruning thresholds: we discarded pairs with cosine below 0.4 since they were too lexically different and above 0.93 since they were too identical.", "labels": [], "entities": []}, {"text": "To build the training set for the supervised step we selected a subset of pairs resulting from the unsupervised step at different cosine similarity scores.", "labels": [], "entities": []}, {"text": "This subset was manually reviewed by two nativespeaker linguists with a background in text simplification.", "labels": [], "entities": []}, {"text": "Specifically, they reviewed a subset of 10,543 pairs at different cosine similarity scores, i.e. those comprised between 0.92 and 0.83.", "labels": [], "entities": [{"text": "cosine similarity scores", "start_pos": 66, "end_pos": 90, "type": "METRIC", "confidence": 0.7420577804247538}]}, {"text": "In order to evaluate sentence similarity at lower values we also selected cosine scores 0.71 and 0.70.", "labels": [], "entities": []}, {"text": "In the end, we obtained 4,327 correct pairs (i.e. about 41% of the whole set of candidate sentence pairs) distributed as in.", "labels": [], "entities": []}, {"text": "This manually revised set of pairs was then used to test the classifier in two different experimental scenarios.", "labels": [], "entities": []}, {"text": "In the first one, named Known Cosine, KC, we tested the classifier in a five-fold cross validation process where pairs of sentences belonging to all the cosine scores were contained in each training and test set.", "labels": [], "entities": []}, {"text": "In the second experiment, named Unknown Cosine, UC, the manually-revised corpus was differently split.", "labels": [], "entities": []}, {"text": "In this case, the test set was composed by pairs of sentences with a cosine similarity score not contained in the training set and con- sequently twelve classification runs were performed.", "labels": [], "entities": []}, {"text": "In order to assess the discriminating power of the linguistic features used in the classification, we carried out an Information Gain analysis.", "labels": [], "entities": []}, {"text": "This analysis showed the effectiveness of all the selected features in both experiments (i.e. KC, UC).", "labels": [], "entities": []}, {"text": "In particular, we observed that the best ranked features are the morpho-syntactic and syntactic ones.", "labels": [], "entities": []}, {"text": "This might suggest that our classification approach is intercepting pairs of sentences undergoing different typologies of structural transformations involving e.g. the use of verbal features or the order of phrases.", "labels": [], "entities": []}, {"text": "Sentence length and lexical features play a lower discriminative role with respect to the grammatical features; this follows from the constraints we put on the unsupervised sentence pairing process.", "labels": [], "entities": [{"text": "sentence pairing process", "start_pos": 173, "end_pos": 197, "type": "TASK", "confidence": 0.7840370138486227}]}, {"text": "As it can be expected, the best ranked features are those providing information about the overlapping characteristics of the paired sentences. and 2 report the results for each cosine threshold considered in the manual revision of the two experiments respectively in terms of i) Accuracy in the classification of the correct and incorrect alignments, and of ii) Precision, Recall of the classification of the correct alignments.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 279, "end_pos": 287, "type": "METRIC", "confidence": 0.997477114200592}, {"text": "Precision", "start_pos": 362, "end_pos": 371, "type": "METRIC", "confidence": 0.9981337189674377}, {"text": "Recall", "start_pos": 373, "end_pos": 379, "type": "METRIC", "confidence": 0.9965736865997314}]}, {"text": "As it can be noted in, in both experiments the classifier is able to outperform the process of sentence pairing based only on the cosine (i.e. line Cosine, that represents the unsupervised step of the pairing process).", "labels": [], "entities": [{"text": "sentence pairing", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7264512926340103}]}, {"text": "As we can expect, Precision, Recall and Accuracy of the KC experiment are higher than the classification results obtained in the UC.", "labels": [], "entities": [{"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9996899366378784}, {"text": "Recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9993276596069336}, {"text": "Accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9992690682411194}]}, {"text": "The latter represents a more challenging experimental scenario where the classifier is tested on a cosine threshold unseen in training.", "labels": [], "entities": []}, {"text": "The overall results for the KC and the UC experiments are respectively 73.95% and 58.71% in terms of Precision, 70.3% and 68.1% in terms of Recall; and respectively 77.64% and 67.2% in terms of Accuracy.", "labels": [], "entities": [{"text": "Precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9984221458435059}, {"text": "Recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9991274476051331}, {"text": "Accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9991822838783264}]}, {"text": "These results are significantly higher when compared with the accuracy of 41% reported for the unsupervised alignment (i.e. line Cosine).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9994687438011169}]}, {"text": "Interestingly, in the KC experiment, Precision and Recall lines are close and they remain stable with respect to all cosines even if the distribution of correct pairs varies in the different cosine values.", "labels": [], "entities": [{"text": "Precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9980600476264954}, {"text": "Recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9303338527679443}]}, {"text": "shows the accuracy of our classifier at different confidence thresholds (i.e. the probability assigned by the classifier for the correct alignments) for both the KC and UC experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994699358940125}]}, {"text": "Note that for each confidence intervals we have a different number of total pairs, and of gold-correct alignments and gold-incorrect alignments.", "labels": [], "entities": []}, {"text": "As expected, the performance grows as the confidence grows.", "labels": [], "entities": []}, {"text": "Interestingly, in the KC scenario, the classifier reached up to 90% of accuracy in discriminating the correct from the incorrect alignments when the classifier has a confidence score \u22650.90.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9995194673538208}]}, {"text": "These results look very promising if we consider that 30% of the pairs of the whole test set classified as correct alignments is comprised in the subset for which the classifier is more confident.", "labels": [], "entities": []}, {"text": "This is also the case of the UC experiment, where, even if with lower accuracies, more than 56% of the correct alignments occurs when the classifier has a confidence score \u22650.90.", "labels": [], "entities": []}, {"text": "We carried out a last evaluation to estimate the classifier performance in the UC scenario for low cosine ranges not comprised in the manually revised portion of the corpus (from 0.45 to 0.75, excluding cosines 0.70 and 0.71).", "labels": [], "entities": []}, {"text": "We considered only the pairs classified as correct with a confidence score of \u226585%.", "labels": [], "entities": []}, {"text": "As expected, the system performance grows as the cosine grows: only few correct pairs occur at cosine <0.60, at cosine 0.60-0.65 the classifier assigns the correct class 237 times with an accuracy of 62.97%, at 0.65-0.69 330 times with 71.82% and at 0.72-0.75 256 times with an accuracy of 87.89%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9956477284431458}, {"text": "accuracy", "start_pos": 278, "end_pos": 286, "type": "METRIC", "confidence": 0.9981616139411926}]}, {"text": "According to these evaluations, we extracted from PAIS\u00b4APAIS\u00b4 PAIS\u00b4A those pairs with a confidence score \u2265 85% and cosine similarity between 0.6 and 0.93, resulting in a collection of about 20,000 pairs.", "labels": [], "entities": [{"text": "PAIS\u00b4APAIS\u00b4", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.8563651442527771}, {"text": "PAIS\u00b4A", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.5084223747253418}]}, {"text": "In the last step, the sentences in each pair were ranked according to the readability score automatically assigned by READ-IT making a collection of complex-simple aligned sentences.", "labels": [], "entities": [{"text": "READ-IT", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.6021506786346436}]}, {"text": "However, the average difference of the readability score between the complex and simple sentences is only 0.13, making this collection not so useful for ATS.", "labels": [], "entities": [{"text": "ATS", "start_pos": 153, "end_pos": 156, "type": "TASK", "confidence": 0.9643704295158386}]}, {"text": "For this reason, we selected only pairs with a difference of readability score higher than a significant threshold set at 0.2.", "labels": [], "entities": []}, {"text": "We defined this threshold on the basis of previous empirical experiments carried out using READ-IT on different typologies of texts.", "labels": [], "entities": []}, {"text": "Lower variations of READ-IT score are scarcely perceived by human subjects.", "labels": [], "entities": [{"text": "READ-IT score", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.9797026813030243}]}, {"text": "Since the construction of this resource has been specifically designed to develop ATS systems for human target, this READ-IT variation is a fundamental parameter of PaCCSS-IT.", "labels": [], "entities": [{"text": "READ-IT", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9403087496757507}, {"text": "PaCCSS-IT", "start_pos": 165, "end_pos": 174, "type": "DATASET", "confidence": 0.9266611337661743}]}, {"text": "We thus obtained about 4,450 pairs.", "labels": [], "entities": []}, {"text": "The evaluation process was intended to calculate the accuracy of i) the automatic classification process in predicting correct sentence alignments and ii) the automatic readability ranking of each pair.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9994893074035645}, {"text": "predicting correct sentence alignments", "start_pos": 108, "end_pos": 146, "type": "TASK", "confidence": 0.8480437099933624}]}, {"text": "The alignment evaluation was carried out by two trained linguists who manually revised 40 pairs of randomly selected sentences for each cosine score (1,088 paired sentences).", "labels": [], "entities": []}, {"text": "It resulted that 85% of pairs were correctly classified (i.e. 921 pairs) and precision increases as cosine grows (from 73.2% at cosine 0.65-0.69 to 90.8% at cosine 0.90-0.92).", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9994537234306335}]}, {"text": "The subset of 921 pairs correctly classified was further investigated with respect to the readability level automatically assigned.", "labels": [], "entities": []}, {"text": "To this aim we elicited human judgements through the crowdsourcing platform CrowdFlower . We collected judgements from 7 workers that were asked to rate for each pair which of the two individual sentences was simpler.", "labels": [], "entities": []}, {"text": "We considered the majority label to be true label for each pair.", "labels": [], "entities": []}, {"text": "Comparing the score obtained by our system with the human judgements we obtained an accuracy of 74%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.999762237071991}]}, {"text": "Restricting the evaluation only to pairs with the same label assigned by at least five out seven annotators (i.e. 79% of the whole pairs), the system achieved an accuracy of 78%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9996730089187622}]}], "tableCaptions": [{"text": " Table 1: Absolute number and % distribution of correct ex-", "labels": [], "entities": [{"text": "Absolute number", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.930196613073349}]}, {"text": " Table 2: Distribution of a subset of linguistic features with sta-", "labels": [], "entities": []}]}