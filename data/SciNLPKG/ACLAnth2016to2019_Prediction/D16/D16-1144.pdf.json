{"title": [{"text": "AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding", "labels": [], "entities": [{"text": "AFET", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8223667740821838}, {"text": "Automatic Fine-Grained Entity Typing", "start_pos": 6, "end_pos": 42, "type": "TASK", "confidence": 0.5819914489984512}]}], "abstractContent": [{"text": "Distant supervision has been widely used in current systems of fine-grained entity typing to automatically assign categories (en-tity types) to entity mentions.", "labels": [], "entities": []}, {"text": "However, the types so obtained from knowledge bases are often incorrect for the entity mention's local context.", "labels": [], "entities": []}, {"text": "This paper proposes a novel embedding method to separately model \"clean\" and \"noisy\" mentions, and incorporates the given type hierarchy to induce loss functions.", "labels": [], "entities": []}, {"text": "We formulate a joint optimization problem to learn embeddings for mentions and type-paths, and develop an iterative algorithm to solve the problem.", "labels": [], "entities": []}, {"text": "Experiments on three public datasets demonstrate the effectiveness and robustness of the proposed method, with an average 15% improvement inaccuracy over the next best compared method 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Assigning types (e.g., person, organization) to mentions of entities in context is an important task in natural language processing (NLP).", "labels": [], "entities": [{"text": "Assigning types (e.g., person, organization) to mentions of entities in context", "start_pos": 0, "end_pos": 79, "type": "TASK", "confidence": 0.7046385129292806}, {"text": "natural language processing (NLP)", "start_pos": 104, "end_pos": 137, "type": "TASK", "confidence": 0.7346937557061514}]}, {"text": "The extracted entity type information can serve as primitives for relation extraction) and event extraction, and assists a wide range of downstream applications including knowledge base (KB) completion), question answering () and entity recommendation ().", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7671922445297241}, {"text": "event extraction", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.7333660423755646}, {"text": "knowledge base (KB) completion", "start_pos": 171, "end_pos": 201, "type": "TASK", "confidence": 0.6243409266074499}, {"text": "question answering", "start_pos": 204, "end_pos": 222, "type": "TASK", "confidence": 0.8968266844749451}, {"text": "entity recommendation", "start_pos": 230, "end_pos": 251, "type": "TASK", "confidence": 0.7667413055896759}]}, {"text": "While Current systems may detect Arnold Schwarzenegger in sentences S1-S3 and assign the same types to all (listed within braces), when only some types are correct for context (blue labels within braces).", "labels": [], "entities": []}, {"text": "traditional named entity recognition systems focus on a small set of coarse types (typically fewer than 10), recent studies () work on a much larger set of fine-grained types (usually over 100) which form a tree-structured hierarchy (see the blue region of).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.6714714268843333}]}, {"text": "Fine-grained typing allows one mention to have multiple types, which together constitute a type-path (not necessarily ending in a leaf node) in the given type hierarchy, depending on the local context (e.g., sentence).", "labels": [], "entities": []}, {"text": "Consider the example in, \"Arnold Schwarzenegger\" could be labeled as {person, businessman} in S3 (investment).", "labels": [], "entities": []}, {"text": "But he could also be labeled as {person, politician} in S1 or {person, artist, actor} in S2.", "labels": [], "entities": []}, {"text": "Such fine-grained type representation provides more informative features for other NLP tasks.", "labels": [], "entities": []}, {"text": "For exam-ple, since relation and event extraction pipelines rely on entity recognizer to identify possible arguments in a sentence, fine-grained argument types help distinguish hundreds or thousands of different relations and events.", "labels": [], "entities": [{"text": "relation and event extraction", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.6633763611316681}]}, {"text": "Traditional named entity recognition systems adopt manually annotated corpora as training data (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.7320810556411743}]}, {"text": "But the process of manually labeling a training set with large numbers of fine-grained types is too expensive and errorprone (hard for annotators to distinguish over 100 types consistently).", "labels": [], "entities": [{"text": "errorprone", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9595582485198975}]}, {"text": "Current fine-grained typing systems annotate training corpora automatically using knowledge bases (i.e., distant supervision) (.", "labels": [], "entities": []}, {"text": "A typical workflow of distant supervision is as follows (see): (1) identify entity mentions in the documents; (2) link mentions to entities in KB; and (3) assign, to the candidate typeset of each mention, all KB types of its KB-linked entity.", "labels": [], "entities": []}, {"text": "However, existing distant supervision methods encounter the following limitations when doing automatic fine-grained typing.", "labels": [], "entities": []}, {"text": "Current practice of distant supervision may introduce label noise to training data since it fails to take a mention's local contexts into account when assigning type labels (e.g., see).", "labels": [], "entities": []}, {"text": "Many previous studies ignore the label noises which appear in a majority of training mentions (see 1, row (1)), and assume all types obtained by distant supervision are \"correct\").", "labels": [], "entities": []}, {"text": "The noisy labels may mislead the trained models and cause negative effect.", "labels": [], "entities": []}, {"text": "A few systems try to denoise the training corpora using simple pruning heuristics such as deleting mentions with conflicting types ( ).", "labels": [], "entities": []}, {"text": "However, such strategies significantly reduce the size of training set (Table 1, rows (2a-c)) and lead to performance degradation (later shown in our experiments).", "labels": [], "entities": []}, {"text": "The larger the target typeset, the more severe the loss.", "labels": [], "entities": []}, {"text": "Most existing methods) treat every type label in a training mention's candidate typeset equally and independently when learning the classifiers but ignore the fact that types in the given hierarchy are semantically correlated (e.g., actor is more relevant to singer than to politician).", "labels": [], "entities": []}, {"text": "As a consequence, the learned classifiers may bias  toward popular types but perform poorly on infrequent types since training data on infrequent types is scarce.", "labels": [], "entities": []}, {"text": "Intuitively, one should pose smaller penalty on types which are semantically more relevant to the true types.", "labels": [], "entities": []}, {"text": "For example, in singer should receive a smaller penalty than politician does, by knowing that actor is a true type for \"Arnold Schwarzenegger\" in S2.", "labels": [], "entities": []}, {"text": "This provides classifiers with additional information to distinguish between two types, especially those infrequent ones.", "labels": [], "entities": []}, {"text": "In this paper, we approach the problem of automatic fine-grained entity typing as follows: (1) Use different objectives to model training mentions with correct type labels and mentions with noisy labels, respectively.", "labels": [], "entities": [{"text": "automatic fine-grained entity typing", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.5899015516042709}]}, {"text": "(2) Design a novel partial-label loss to model true types within the noisy candidate typeset which requires only the \"best\" candidate type to be relevant to the training mention, and progressively estimate the best type by leveraging various text features extracted for the mention.", "labels": [], "entities": []}, {"text": "(3) Derive type correlation based on two signals: (i) the given type hierarchy, and (ii) the shared entities between two types in KB, and incorporate the correlation so induced by enforcing adaptive margins between different types for mentions in the training set.", "labels": [], "entities": []}, {"text": "To integrate these ideas, we develop a novel embedding-based framework called AFET.", "labels": [], "entities": []}, {"text": "First, it uses distant supervision to obtain candidate types for each mention, and extract a variety of text features from the mentions themselves and their local contexts.", "labels": [], "entities": []}, {"text": "Mentions are partitioned into a \"clean\" set and a \"noisy\" set based on the given type hierarchy.", "labels": [], "entities": []}, {"text": "Second, we embed mentions and types jointly into a low-dimensional space, where, in that space, objects (i.e., features and types) that are semantically close to each other also have similar representations.", "labels": [], "entities": []}, {"text": "In the proposed objective, an adaptive margin-based rank loss is pro-posed to model the set of clean mentions to capture type correlation, and a partial-label rank loss is formulated to model the \"best\" candidate type for each noisy mention.", "labels": [], "entities": []}, {"text": "Finally, with the learned embeddings (i.e., mapping matrices), one can predict the typepath for each mention in the test set in a top-down manner, using its text features.", "labels": [], "entities": []}, {"text": "The major contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "We propose an automatic fine-grained entity typing framework, which reduces label noise introduced by distant supervision and incorporates type correlation in a principle way.", "labels": [], "entities": [{"text": "type correlation", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.8361211121082306}]}, {"text": "2. A novel optimization problem is formulated to jointly embed entity mentions and types to the same space.", "labels": [], "entities": []}, {"text": "It models noisy typeset with a partial-label rank loss and type correlation with adaptive-margin rank loss.", "labels": [], "entities": []}, {"text": "3. We develop an iterative algorithm for solving the joint optimization problem efficiently.", "labels": [], "entities": [{"text": "joint optimization problem", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7761909862359365}]}, {"text": "4. Experiments with three public datasets demonstrate that AFET achieves significant improvement over the state of the art.", "labels": [], "entities": [{"text": "AFET", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.5650635361671448}]}], "datasetContent": [{"text": "For the Wiki and OntoNotes datasets, we used the provided test set.", "labels": [], "entities": [{"text": "Wiki", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9282060861587524}, {"text": "OntoNotes datasets", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.9249248206615448}]}, {"text": "Since BBN corpus is fully annotated, we followed a 80/20 ratio to partition it into training/test sets.", "labels": [], "entities": [{"text": "BBN corpus", "start_pos": 6, "end_pos": 16, "type": "DATASET", "confidence": 0.9416207373142242}]}, {"text": "We report Accuracy (Strict-F1), Micro-averaged F1 (Mi-F1) and Macro-averaged F1 (Ma-F1) scores commonly used in the fine-grained type problem ().", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983859062194824}, {"text": "Micro-averaged F1 (Mi-F1)", "start_pos": 32, "end_pos": 57, "type": "METRIC", "confidence": 0.8910451889038086}, {"text": "Macro-averaged F1 (Ma-F1) scores", "start_pos": 62, "end_pos": 94, "type": "METRIC", "confidence": 0.8098050008217493}]}, {"text": "Since we use the gold mention set for testing, the Accuracy (Acc) we reported is the same as the Strict F1.", "labels": [], "entities": [{"text": "Accuracy (Acc)", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.9202377945184708}, {"text": "Strict F1", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.8261117041110992}]}, {"text": "We compared the proposed method (AFET) and its variant with state-of-the-art typing methods, embedding methods and partial-label learning methods 4 :  We compare AFET and its variant: (1) AFET: complete model with KB-induced type correlation; (2) AFET-CoH: with hierarchy-induced correlation (i.e., shortest path distance); (3) AFET-NoCo: without type correlation (i.e., all margin are \"1\") in the objective O; and (4) AFET-NoPa: without label partial loss in the objective O. shows the results of AFET and its variants.", "labels": [], "entities": [{"text": "AFET", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.7797135710716248}, {"text": "AFET-NoCo", "start_pos": 328, "end_pos": 337, "type": "DATASET", "confidence": 0.6827155947685242}, {"text": "AFET-NoPa", "start_pos": 419, "end_pos": 428, "type": "METRIC", "confidence": 0.8718551993370056}]}, {"text": "Comparison with the other typing methods.", "labels": [], "entities": []}, {"text": "AFET outperforms both FIGER and HYENA systems, demonstrating the predictive power of the learned embeddings, and the effectiveness of modeling type correlation information and noisy candidate types.", "labels": [], "entities": [{"text": "AFET", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6048079133033752}, {"text": "FIGER", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.7008002996444702}]}, {"text": "We also observe that pruning methods do not always improve the performance, since they aggressively filter out rare types in the corpus, which may lead to low Recall.", "labels": [], "entities": [{"text": "Recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9104102849960327}]}, {"text": "ClusType is not as good as FIGER and HYENA because it is intended for coarse types and only utilizes relation phrases.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.4762243926525116}]}], "tableCaptions": [{"text": " Table 1: A study of label noise. (1): %mentions with  multiple sibling types (e.g., actor, singer); (2a)-(2c):  %mentions deleted by the three pruning heuristics (2014)  (see Sec. 4), for three experiment datasets and New York  Times annotation corpus (2014).", "labels": [], "entities": [{"text": "New York  Times annotation corpus", "start_pos": 219, "end_pos": 252, "type": "DATASET", "confidence": 0.7892570972442627}]}, {"text": " Table 2: Text features used in this paper. \"Turing Machine\" is used as an example mention from \"The band's former drummer Jerry Fuchs-who", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the datasets.", "labels": [], "entities": []}, {"text": " Table 4: Study of typing performance on the three datasets.", "labels": [], "entities": []}, {"text": " Table 6: Example output of AFET and other methods on  frequent/infrequent type from OntoNotes dataset.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.9289751350879669}]}]}