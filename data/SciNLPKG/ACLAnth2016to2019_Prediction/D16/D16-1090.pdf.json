{"title": [{"text": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions", "labels": [], "entities": [{"text": "VQA", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.6875178217887878}, {"text": "Identifying Non-Visual And False-Premise Questions", "start_pos": 27, "end_pos": 77, "type": "TASK", "confidence": 0.7549988746643066}]}], "abstractContent": [{"text": "Visual Question Answering (VQA) is the task of answering natural-language questions about images.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7470889737208685}, {"text": "answering natural-language questions about images", "start_pos": 47, "end_pos": 96, "type": "TASK", "confidence": 0.6690893530845642}]}, {"text": "We introduce the novel problem of determining the relevance of questions to images in VQA.", "labels": [], "entities": []}, {"text": "Current VQA models do not reason about whether a question is even related to the given image (e.g., What is the capital of Argentina?) or if it requires information from external resources to answer correctly.", "labels": [], "entities": []}, {"text": "This can break the continuity of a dialogue in human-machine interaction.", "labels": [], "entities": []}, {"text": "Our approaches for determining relevance are composed of two stages.", "labels": [], "entities": []}, {"text": "Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not.", "labels": [], "entities": []}, {"text": "Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outper-form strong baselines on both relevance tasks.", "labels": [], "entities": [{"text": "VQA model uncertainty", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.49816153446833294}]}, {"text": "We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent , reasonable, and human-like.", "labels": [], "entities": []}], "introductionContent": [{"text": "Visual Question Answering (VQA) is the task of predicting a suitable answer given an image and a question about it.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7468247910340627}]}, {"text": "VQA models (e.g.,) are typically discriminative models that take in image and question representations and output one of a set of possible answers.", "labels": [], "entities": []}, {"text": "Our work is motivated by the following key observation -all current VQA systems always output an answer regardless of whether the input question makes any sense for the given image or not.", "labels": [], "entities": []}, {"text": "Non-Visual Visual True-Premise Who is the president of the USA?", "labels": [], "entities": [{"text": "USA", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9275271892547607}]}, {"text": "What is the girl wearing?", "labels": [], "entities": []}, {"text": "What is the cat wearing?", "labels": [], "entities": []}, {"text": "Visual False-Premise shows examples of relevant and irrelevant questions.", "labels": [], "entities": []}, {"text": "When VQA systems are fed irrelevant questions as input, they understandably produce nonsensical answers (Q: \"What is the capital of Argentina?\" A: \"fire hydrant\").", "labels": [], "entities": []}, {"text": "Humans, on the other hand, are unlikely to provide such nonsensical answers and will instead answer that this is irrelevant or use another knowledge source to answer correctly, when possible.", "labels": [], "entities": []}, {"text": "We argue that this implicit assumption by all VQA systems -that an input question is always relevant for the input image -is simply untenable as VQA systems move beyond standard academic datasets to interacting with real users, who maybe unfamiliar, or malicious.", "labels": [], "entities": []}, {"text": "The goal of this work is to make VQA systems more human-like by providing them the capability to identify relevant questions.", "labels": [], "entities": []}, {"text": "While existing work has reasoned about crossmodal similarity, being able to identify whether a question is relevant to a given image is a novel problem with real-world applications.", "labels": [], "entities": []}, {"text": "In human-robot interaction, being able to identify questions that are dissociated from the perception data available is important.", "labels": [], "entities": []}, {"text": "The robot must decide whether to process the scene it perceives or query external world knowledge resources to provide a response.", "labels": [], "entities": []}, {"text": "As shown in, we study three types of question-image pairs: Non-Visual.", "labels": [], "entities": []}, {"text": "These questions are not questions about images at all -they do not require information from any image to be answered (e.g., \"What is the capital of Argentina?\").", "labels": [], "entities": []}, {"text": "While visual, these questions do not apply to the given image.", "labels": [], "entities": []}, {"text": "For instance, the question \"What is the girl wearing?\" makes sense only for images that contain a girl in them.", "labels": [], "entities": []}, {"text": "These questions are relevant to (i.e., have a premise which is true) the image at hand.", "labels": [], "entities": []}, {"text": "We introduce datasets and train models to recognize both non-visual and false-premise questionimage (QI) pairs in the context of VQA.", "labels": [], "entities": []}, {"text": "First, we identify whether a question is visual or non-visual; if visual, we identify whether the question has a truepremise for the given image.", "labels": [], "entities": []}, {"text": "For visual vs. nonvisual question detection, we use a Long Short-Term Memory (LSTM) recurrent neural network (RNN) trained on part of speech (POS) tags to capture visual-specific linguistic structure.", "labels": [], "entities": [{"text": "question detection", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7157223522663116}]}, {"text": "For true vs. falsepremise question detection, we present one set of approaches that use the uncertainty of a VQA model, and another set that use pre-trained captioning models to generate relevant captions (or questions) for the given image and then compare them to the given question to determine relevance.", "labels": [], "entities": [{"text": "true vs. falsepremise question detection", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.6457139670848846}]}, {"text": "Our proposed models achieve accuracies of 92% for detecting non-visual, and 74% for detecting false-premise questions, which significantly outperform strong baselines.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.980743944644928}]}, {"text": "We also show through human studies that a VQA system that reasons about question relevance is picked significantly more often as being more intelligent, human-like and reasonable than a baseline VQA system which does not.", "labels": [], "entities": [{"text": "question relevance", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7037219256162643}]}, {"text": "Our code and datasets are publicly available on the authors' webpages.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results for both experiments are presented in Table 1.", "labels": [], "entities": []}, {"text": "We present results averaged over 40 random train/test splits.", "labels": [], "entities": []}, {"text": "RULE-BASED and Q-GEN SCORE were not averaged because they are deterministic.", "labels": [], "entities": [{"text": "RULE-BASED", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9825934767723083}, {"text": "Q-GEN SCORE", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.8484790325164795}]}, {"text": "We use a random set of 100,000 questions from the VNQ dataset for training, and the remaining 31,464 for testing.", "labels": [], "entities": [{"text": "VNQ dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9913499653339386}]}, {"text": "We see that LSTM performs 16.59% (21.92% relative) better than RULE-BASED.", "labels": [], "entities": [{"text": "RULE-BASED", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.8934901356697083}]}, {"text": "We use a random set of 7,195 (67%) QI pairs from the VTFQ dataset to train and the remaining 3,597 (33%) to test.", "labels": [], "entities": [{"text": "VTFQ dataset", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9721730947494507}]}, {"text": "While the VQA model uncertainty based approaches (ENTROPY, VQA-MLP) perform reasonably well (with the MLP helping over raw entropy), the learned similarity approaches perform much better (10.39% gain in normalized accuracy).", "labels": [], "entities": [{"text": "ENTROPY", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.8944205045700073}, {"text": "VQA-MLP", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9279453158378601}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.8383207321166992}]}, {"text": "High uncertainty of the model may suggest that a similar QI pair was not seen during training; however, that does not seem to translate to detecting irrelevance.", "labels": [], "entities": []}, {"text": "The language generation models (Q-C SIM, Q-Q' SIM) seem to work significantly better at modeling the semantic interaction between the question and the image.", "labels": [], "entities": []}, {"text": "The generative approach (Q-GEN SCORE) is outperformed by the discriminative approaches (VQA-MLP, Q-C SIM, Q-Q' SIM) that are trained explicitly for the task at hand.", "labels": [], "entities": [{"text": "VQA-MLP", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.717745304107666}]}, {"text": "We show qualitative examples of Q-Q' SIM for true-vs. false-premise detection in.", "labels": [], "entities": [{"text": "true-vs. false-premise detection", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6787464221318563}]}, {"text": "We also perform human studies where we compare two agents: (1) AGENT-BASELINE-always answers every question.", "labels": [], "entities": [{"text": "AGENT-BASELINE-always", "start_pos": 63, "end_pos": 84, "type": "METRIC", "confidence": 0.998890221118927}]}, {"text": "(2) AGENT-OURS-reasons about question relevance before responding.", "labels": [], "entities": [{"text": "AGENT-OURS-reasons", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9835561513900757}]}, {"text": "If question is classified as visual true-premise, AGENT-OURS answers the question using the same VQA model as AGENT-BASELINE (using ( ).", "labels": [], "entities": []}, {"text": "Otherwise, it responds with a prompt indicating that the question does not seem meaningful for the image.", "labels": [], "entities": []}, {"text": "A total of 120 questions (18.33% relevant, 81.67% irrelevant, mimicking the distribution of the VTFQ dataset) were used.", "labels": [], "entities": [{"text": "VTFQ dataset", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.9775214791297913}]}, {"text": "Of the relevant questions, 54% were answered correctly by the VQA model.", "labels": [], "entities": [{"text": "VQA", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8274226188659668}]}, {"text": "Human subjects on AMT were shown the response of both agents and asked to pick the agent that sounded more intelligent, more reasonable, and more human-like after every observed QI pair.", "labels": [], "entities": []}, {"text": "Each QI pair was assessed by 5 different subjects.", "labels": [], "entities": []}, {"text": "Not all pairs were rated by the same 5 subjects.", "labels": [], "entities": []}, {"text": "In total, 28 unique AMT workers participated in the study.", "labels": [], "entities": [{"text": "AMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9788468480110168}]}, {"text": "AGENT-OURS was picked 65.8% of the time as the winner, AGENT-BASELINE was picked only 1.6% of the time, and both considered equally (un)reasonable in the remaining cases.", "labels": [], "entities": [{"text": "AGENT-OURS", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.5635923147201538}, {"text": "AGENT-BASELINE", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.901299774646759}]}, {"text": "We also measure the percentage of times each robot gets picked Q\":\"Is\"the\"event\"indoor\"or\"outdoor?", "labels": [], "entities": []}, {"text": "Q'#:#What\"is\"the\"elephant\"doing?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Normalized accuracy results (averaged over 40 random train/test splits) for visual vs. non-visual detection and  true-vs. false-premise detection. RULE-BASED and Q-GEN SCORE were not averaged because they are deterministic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9691051244735718}, {"text": "RULE-BASED", "start_pos": 157, "end_pos": 167, "type": "METRIC", "confidence": 0.9937441945075989}]}, {"text": " Table 2: Percentage of times each robot gets picked by  AMT workers as being more intelligent, more reasonable,  and more human-like for true-premise, false-premise, and  non-visual questions.", "labels": [], "entities": []}]}