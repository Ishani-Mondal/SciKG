{"title": [{"text": "Addressee and Response Selection for Multi-Party Conversation", "labels": [], "entities": [{"text": "Multi-Party Conversation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.7327627539634705}]}], "abstractContent": [{"text": "To create conversational systems working in actual situations, it is crucial to assume that they interact with multiple agents.", "labels": [], "entities": []}, {"text": "In this work, we tackle addressee and response selection for multi-party conversation, in which systems are expected to select whom they address as well as what they say.", "labels": [], "entities": [{"text": "addressee and response selection", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.6212266460061073}]}, {"text": "The key challenge of this task is to jointly model who is talking about what in a previous context.", "labels": [], "entities": []}, {"text": "For the joint modeling, we propose two model-ing frameworks: 1) static modeling and 2) dynamic modeling.", "labels": [], "entities": []}, {"text": "To show benchmark results of our frameworks, we created a multi-party conversation corpus.", "labels": [], "entities": []}, {"text": "Our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Short text conversation (STC) has been gaining popularity: given an input message, predict an appropriate response in a single-round, two-party conversation (.", "labels": [], "entities": [{"text": "Short text conversation (STC)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.703397274017334}]}, {"text": "Modeling STC is simpler than modeling a complete conversation, but instantly helps applications such as chat-bots and automatic short-message replies).", "labels": [], "entities": [{"text": "Modeling STC", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.767096608877182}]}, {"text": "Beyond two-party conversations, there is also a need for modeling multi-party conversation, a form of conversation with several interlocutors conversing with each other.", "labels": [], "entities": []}, {"text": "For example, in the Ubuntu Internet Relay Chat (IRC), sev- eral users cooperate to find a solution fora technical issue contributed by another user.", "labels": [], "entities": [{"text": "Ubuntu Internet Relay Chat (IRC)", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.6744264875139508}]}, {"text": "Each agent might have one part of the solution, and these pieces have to be combined through conversation in order to come up with the whole solution.", "labels": [], "entities": []}, {"text": "A unique issue of such multi-party conversations is addressing, a behavior whereby interlocutors indicate to whom they are speaking.", "labels": [], "entities": []}, {"text": "In faceto-face communication, the basic clue for specifying addressees is turning one's face toward the addressee.", "labels": [], "entities": []}, {"text": "In contrast, in voice-only or textbased communication, the explicit declaration of addressee's names is more common.", "labels": [], "entities": []}, {"text": "In this work, we tackle addressee and response selection for multi-party conversation: given a context, predict an addressee and response.", "labels": [], "entities": [{"text": "addressee and response selection", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.5916930064558983}]}, {"text": "As Figure 1 shows, a system is required to select an addressee from the agents appearing in the previous context and a response from a fixed set of candidate responses (Section 3).", "labels": [], "entities": []}, {"text": "The key challenge for predicting appropriate addressees and responses is to jointly capture who is talking about what at each time step in a context.", "labels": [], "entities": [{"text": "predicting appropriate addressees and responses", "start_pos": 22, "end_pos": 69, "type": "TASK", "confidence": 0.8513797640800476}]}, {"text": "For jointly modeling the speaker-utterance information, we present two modeling frameworks: 1) static modeling and 2) dynamic modeling (Section 5).", "labels": [], "entities": []}, {"text": "While speakers are represented as fixed vectors in the static modeling, they are represented as hidden state vectors that dynamically change with time steps in the dynamic modeling.", "labels": [], "entities": []}, {"text": "In practice, our models trained for the task can be applied to retrieval-based conversation systems, which retrieves candidate responses from a large-scale repository with the matching model and returns the highest scoring one with the ranking model (.", "labels": [], "entities": []}, {"text": "Our trained models work as the ranking model and allow the conversation system to produce addressees as well as responses.", "labels": [], "entities": []}, {"text": "To evaluate the trained models, we provide a corpus and dataset.", "labels": [], "entities": []}, {"text": "By exploiting Ubuntu IRC Logs 1 , we build a large-scale multi-party conversation corpus, and create a dataset from it (Section 6).", "labels": [], "entities": []}, {"text": "Our experiments on the dataset show the models instantiated by the static and dynamic modeling outperform a strong baseline.", "labels": [], "entities": []}, {"text": "In particular, the model based on the dynamic modeling robustly predicts appropriate addressees and responses even if the number of interlocutors in a conversation increases.", "labels": [], "entities": []}, {"text": "We make three contributions in this work: 1.", "labels": [], "entities": []}, {"text": "We formalize the task of addressee and response selection for multi-party conversation.", "labels": [], "entities": [{"text": "addressee and response selection", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.6116405576467514}]}, {"text": "2. We present modeling frameworks and the performance benchmarks for the task.", "labels": [], "entities": []}, {"text": "3. We build a large-scale multi-party conversation corpus and dataset for the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our goal is to provide a multi-party conversation corpus/dataset that can be used over a wide range of conversation research, such as turn-taking modeling) and disentanglement modeling, as well as for the ARS task.", "labels": [], "entities": [{"text": "turn-taking modeling", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.7779880166053772}, {"text": "disentanglement modeling", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.9646526575088501}, {"text": "ARS task", "start_pos": 205, "end_pos": 213, "type": "TASK", "confidence": 0.8872250914573669}]}, {"text": "shows the flow of the corpus and dataset creation process.", "labels": [], "entities": []}, {"text": "We firstly crawl Ubuntu IRC Logs and preprocess the obtained logs.", "labels": [], "entities": [{"text": "Ubuntu IRC Logs", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.903572698434194}]}, {"text": "Then, from the logs, we extract and add addressee information to the corpus.", "labels": [], "entities": []}, {"text": "In the final step, we set candidate responses and labels as the dataset.", "labels": [], "entities": []}, {"text": "shows the statistics of the corpus and dataset.", "labels": [], "entities": []}, {"text": "By exploiting the corpus, we create a dataset for the ARS task.", "labels": [], "entities": [{"text": "ARS task", "start_pos": 54, "end_pos": 62, "type": "TASK", "confidence": 0.9009622931480408}]}, {"text": "If the line of the corpus includes an addressee ID, we regard it as a sample for the task.", "labels": [], "entities": []}, {"text": "As the ground truth addressees and responses, we straightforwardly use the obtained addressee IDs and the preprocessed utterances.", "labels": [], "entities": []}, {"text": "As false responses, we sample utterances elsewhere within a document.", "labels": [], "entities": []}, {"text": "This document-within sampling method makes the response selection task more difficult than the random sampling method . One reason for this is that common or similar topics in a document are often discussed and the used words tend to be similar, which makes the wordbased features for the task less effective.", "labels": [], "entities": [{"text": "response selection task", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7966320912043253}]}, {"text": "We partitioned the dataset randomly into a training set (90%), a development set (5%) and a test set (5%).", "labels": [], "entities": []}, {"text": "We provide performance benchmarks of our learning architectures on the addressee and response selection (ARS) task for multi-party conversation.", "labels": [], "entities": [{"text": "addressee and response selection (ARS) task", "start_pos": 71, "end_pos": 114, "type": "TASK", "confidence": 0.7493598386645317}]}, {"text": "We use the created dataset for the experiments.", "labels": [], "entities": []}, {"text": "The number of candidate responses RES-CAND (|R|) is set to 2 or 10.", "labels": [], "entities": [{"text": "RES-CAND", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9891678094863892}]}, {"text": "We evaluate performance by accuracies on three aspects: addressee-response pair selection (ADR-RES), addressee selection (ADR), and response selection (RES).", "labels": [], "entities": [{"text": "response selection (RES)", "start_pos": 132, "end_pos": 156, "type": "METRIC", "confidence": 0.6472419202327728}]}, {"text": "In the addressee-response pair selection, we regard the answer as correct if both the addressee and the response are correctly selected.", "labels": [], "entities": []}, {"text": "In the addressee/response selection, we regard the answer as correct if the addressee/response is correctly selected.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the corpus and dataset. \"Docs\" is docu-", "labels": [], "entities": []}, {"text": " Table 3: Benchmark results: accuracies on addressee-response selection (ADR-RES), addressee selection (ADR), and response", "labels": [], "entities": [{"text": "accuracies", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9851087927818298}]}, {"text": " Table 4: Performance comparison for different numbers of agents appearing in the context. The numbers are accuracies on the test", "labels": [], "entities": []}]}