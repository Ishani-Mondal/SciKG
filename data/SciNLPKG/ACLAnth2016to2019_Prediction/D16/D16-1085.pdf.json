{"title": [{"text": "Modeling Skip-Grams for Event Detection with Convolutional Neural Networks", "labels": [], "entities": [{"text": "Event Detection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7067344188690186}]}], "abstractContent": [{"text": "Convolutional neural networks (CNN) have achieved the top performance for event detection due to their capacity to induce the underlying structures of the k-grams in the sentences.", "labels": [], "entities": [{"text": "event detection", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7690905928611755}]}, {"text": "However, the current CNN-based event detectors only model the consecutive k-grams and ignore the non-consecutive k-grams that might involve important structures for event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.76871657371521}]}, {"text": "In this work, we propose to improve the current CNN models for ED by introducing the non-consecutive convolu-tion.", "labels": [], "entities": []}, {"text": "Our systematic evaluation on both the general setting and the domain adaptation setting demonstrates the effectiveness of the non-consecutive CNN model, leading to the significant performance improvement over the current state-of-the-art systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of event detection (ED) is to locate event triggers of some specified types in text.", "labels": [], "entities": [{"text": "event detection (ED)", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8659215569496155}]}, {"text": "Triggers are generally single verbs or nominalizations that evoke the events of interest.", "labels": [], "entities": [{"text": "Triggers", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.951701283454895}]}, {"text": "This is an important and challenging task of information extraction in natural language processing (NLP), as the same event might appear in various expressions, and an expression might express different events depending on contexts.", "labels": [], "entities": [{"text": "information extraction in natural language processing (NLP)", "start_pos": 45, "end_pos": 104, "type": "TASK", "confidence": 0.7544601029819913}]}, {"text": "The current state-of-the-art systems for ED have involved the application of convolutional neural networks (CNN)) that automatically learn effective feature representations for ED from sentences.", "labels": [], "entities": [{"text": "ED", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9493167400360107}]}, {"text": "This has overcome the two fundamental limitations of the traditional feature-based methods for ED: (i) the complicated feature engineering for rich feature sets and (ii) the error propagation from the NLP toolkits and resources (i.e, parsers, part of speech taggers etc) that generate such features.", "labels": [], "entities": [{"text": "ED", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9296888709068298}]}, {"text": "The prior CNN models for ED are characterized by the temporal convolution operators that linearly map the vectors for the k-grams in the sentences into the feature space.", "labels": [], "entities": []}, {"text": "Such k-gram vectors are obtained by concatenating the vectors of the k consecutive words in the sentences.", "labels": [], "entities": []}, {"text": "In other words, the previous CNN models for ED only focus on modeling the consecutive k-grams.", "labels": [], "entities": []}, {"text": "Unfortunately, such consecutive mechanism is unable to capture the long-range and non-consecutive dependencies that are necessary to the prediction of trigger words.", "labels": [], "entities": [{"text": "prediction of trigger words", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.850092202425003}]}, {"text": "For instance, consider the following sentence with the trigger word \"leave\" from the ACE 2005 corpus: The mystery is that she took the job in the first place or didn't leave earlier.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9692749381065369}]}, {"text": "The correct event type for the trigger word \"leave\" in this case is \"End-Org\".", "labels": [], "entities": [{"text": "End-Org", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9450483322143555}]}, {"text": "However, the previous CNN models might not be able to detect \"leave\" as an event trigger or incorrectly predict its type as \"Movement\".", "labels": [], "entities": []}, {"text": "This is caused by their reliance on the consecutive local k-grams such as \"leave earlier\".", "labels": [], "entities": []}, {"text": "Consequently, we need to resort to the nonconsecutive pattern \"job leave\" to correctly determine the event type of \"leave\" in this case.", "labels": [], "entities": []}, {"text": "Guided by this intuition, we propose to improve the previous CNN models for ED by operating the convolution on all possible non-consecutive k-grams in the sentences.", "labels": [], "entities": []}, {"text": "We aggregate the resulting convolution scores via the max-pooling function to unveil the most important non-consecutive k-grams for ED.", "labels": [], "entities": []}, {"text": "The aggregation overall the possible nonconsecutive k-grams is made efficient with dynamic programming.", "labels": [], "entities": []}, {"text": "Note that our work is related to () who employ the non-consecutive convolution for the sentence and news classification problems.", "labels": [], "entities": [{"text": "news classification", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.6413974016904831}]}, {"text": "Our work is different from () in that we model the relative distances of words to the trigger candidates in the sentences via position embeddings, while ( use the absolute distances between words in the k-grams to compute the decay weights for aggregation.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work on non-consecutive CNN for ED.", "labels": [], "entities": [{"text": "ED", "start_pos": 80, "end_pos": 82, "type": "DATASET", "confidence": 0.6239151954650879}]}, {"text": "We systematically evaluate the proposed model in the general setting as well as the domain adaptation setting.", "labels": [], "entities": []}, {"text": "The experiment results demonstrate that our model significantly outperforms the current state-ofthe-art models in such settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the same parameters and resources as) to ensure the compatible comparison.", "labels": [], "entities": []}, {"text": "Specifically, we employ the window sizes in the set {2, 3, 4, 5} for the convolution operation with 150 filters for each window size.", "labels": [], "entities": []}, {"text": "The window size of the trigger candidate is 31 while the dimensionality of the position embeddings and entity type embeddings is 50.", "labels": [], "entities": []}, {"text": "We use word2vec from (Mikolov et al., 2013b) as the pretrained word embeddings.", "labels": [], "entities": []}, {"text": "The other parameters include the dropout rate \u21e2 = 0.5, the mini-batch size = 50, the predefined threshold for the l 2 norms = 3.", "labels": [], "entities": [{"text": "dropout rate \u21e2", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.8981700340906779}]}, {"text": "Following the previous studies (, we evaluate the models on the ACE 2005 corpus with 33 event subtypes.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9824986855189005}]}, {"text": "In order to make it compatible, we use the same test set with 40 newswire articles, the same development set with 30 other documents and the same training set with the remaining 529 documents.", "labels": [], "entities": []}, {"text": "All the data preprocessing and evaluation criteria follow those in (Nguyen and Grishman, 2015b).", "labels": [], "entities": []}, {"text": "Previous studies have shown that the NLP models would suffer from a significant performance loss when domains shift).", "labels": [], "entities": []}, {"text": "In particular, if a model is trained on some source domain and applied to a different domain (the target domain), its performance would degrade significantly.", "labels": [], "entities": []}, {"text": "The domain adaptation (DA) studies aim to overcome this issue by developing robust techniques across domains.", "labels": [], "entities": [{"text": "domain adaptation (DA)", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8469986140727996}]}, {"text": "The best reported system in the DA setting for ED is, which demonstrated that the CNN model outperformed the feature-based models in the cross-domain setting.", "labels": [], "entities": [{"text": "ED", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.8771253228187561}]}, {"text": "In this section, we compare NC-CNN with the CNN model in) (as well as the other models above) in the DA setting to further investigate their effectiveness.", "labels": [], "entities": []}, {"text": "This section also uses the ACE 2005 dataset but focuses more on the difference between domains.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.9841060837109884}]}, {"text": "The ACE 2005 corpus includes 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl).", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9658476909001669}]}, {"text": "Following), we use news (the union of bn and nw) as the source domain and bc, cts, wl and un as four different target domains . We take half of bc as the development set and use the remaining data for testing.", "labels": [], "entities": []}, {"text": "Our data split is the same as that in reports the performance of the systems with 5-fold cross validation.", "labels": [], "entities": []}, {"text": "Note that we focus on the systems exploiting only the sentence level information in this section.", "labels": [], "entities": []}, {"text": "For each system, we train a model on the training data of the source domain and evaluate this model on the test set of the source domain (in-domain performance) as well as on the four target domains bc, cts, wl and un.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance with Gold-Standard Entity Men-", "labels": [], "entities": []}, {"text": " Table 2: Performance on the source domain and on the target domains. Cells marked with  \u2020designates that NC-CNN", "labels": [], "entities": [{"text": "NC-CNN", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.9005534052848816}]}]}