{"title": [{"text": "Transferring User Interests Across Websites with Unstructured Text for Cold-Start Recommendation", "labels": [], "entities": [{"text": "Transferring User Interests", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8680857022603353}, {"text": "Cold-Start Recommendation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.699919193983078}]}], "abstractContent": [{"text": "In this work, we investigate the possibility of cross-website transfer learning for tackling the cold-start problem.", "labels": [], "entities": [{"text": "cross-website transfer learning", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.8146374622980753}]}, {"text": "To address the cold-start issues commonly present in a collabora-tive ltering (CF) system, most existing cross-domain CF models require auxiliary rating data from another domain; nevertheless, under the cross-website scenario, such data is often unobtainable.", "labels": [], "entities": []}, {"text": "Therefore, we propose the nearest-neighbor transfer matrix factorization (NT-MF) model, where a topic model is applied to the unstructured user-generated content in the source domain, and the similarity between users in the latent topic space is utilized to guide the target-domain CF model.", "labels": [], "entities": []}, {"text": "Specically, the latent factors of the nearest-neighbors are regarded as a set of pseudo observations , which can be used to estimate the unknown parameters in the model.", "labels": [], "entities": []}, {"text": "Improvement over previous methods, especially for the cold-start users, is demonstrated with experiments on a real-world cross-website dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "While collaborative ltering (CF) approaches are one of the most successful methods for building recommender systems, their performance deteriorates dramatically under cold-start situations.", "labels": [], "entities": [{"text": "collaborative ltering (CF)", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.6726048171520234}]}, {"text": "That is, low prediction accuracy is observed for users/items with very few ratings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.8920716047286987}]}, {"text": "Content-based recommender systems may also suffer from the cold-start problem.", "labels": [], "entities": []}, {"text": "For instance, content-based nearest-neighbor models ( might not be as effective if some users contain too few information to generate a meaningful set of neighbors.", "labels": [], "entities": []}, {"text": "Two types of solutions have been proposed to address the cold-start problem.", "labels": [], "entities": []}, {"text": "The rst is to create hybrid recommendation models that impose a content-based model on a CF model to enrich the information for users/items with sparse rating proles.", "labels": [], "entities": []}, {"text": "The second is to transfer the information from auxiliary domains as a remedy to the cold-start individuals.", "labels": [], "entities": []}, {"text": "This paper aims at bringing a marriage between these two types of strategies.", "labels": [], "entities": []}, {"text": "Although transfer learning gradually gains popularity in handling the cold-start issue (), most of them assume a homogeneous model where observations in both domains are of the same type.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.9545339941978455}]}, {"text": "That is, to transfer knowledge to a ratingbased/text-based recommender system, the source system must also be rating-based/text-based.", "labels": [], "entities": []}, {"text": "Some earlier works even require the ratings from both domains to be in the same format (, or assume specic structured text, such as userprovided tags.", "labels": [], "entities": []}, {"text": "In this work, by contrast, no source-domain ratings are available and unstructured user-generated content is treated as the auxiliary data.", "labels": [], "entities": []}, {"text": "We propose a heterogeneous transfer learning framework to utilize unstructured auxiliary text fora better target-domain CF model.", "labels": [], "entities": []}, {"text": "As there is no single service satisfying all social needs, users nowadays hold multiple accounts across many websites.", "labels": [], "entities": []}, {"text": "Furthermore, the account linking mechanism is often available on these websites.", "labels": [], "entities": []}, {"text": "This allows a precise mapping between the accounts of the same user to be built.", "labels": [], "entities": []}, {"text": "One major application of our approach is to improve the recom-mendation quality in the target service using auxiliary data obtained from another seemingly irrelevant service.", "labels": [], "entities": []}, {"text": "For instance, consider anew user on YouTube.", "labels": [], "entities": []}, {"text": "The initial recommended videos for this user is likely to be irrelevant as there is very few information available.", "labels": [], "entities": []}, {"text": "However, with the account linking mechanism, YouTube accounts can be linked to Twitter accounts with a simple click.", "labels": [], "entities": []}, {"text": "Our goal is to utilize the content generated by this user on Twitter, despite the possibility that the content is irrelevant to their preference on video browsing, to produce a better video recommendation list on YouTube.", "labels": [], "entities": []}, {"text": "Seemingly intuitive, there exist some difculties in such cross-website transfer learning approach.", "labels": [], "entities": [{"text": "cross-website transfer learning", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.7304802139600118}]}, {"text": "The biggest challenge lies in the fact that most users do not use multiple services (e.g. social media sites) for the same purpose.", "labels": [], "entities": []}, {"text": "Usually a user registers for multiple services because each of them serves its own purpose.", "labels": [], "entities": []}, {"text": "As a result, we cannot assume the existence of direct mentions about target-domain items in the source-domain text data.", "labels": [], "entities": []}, {"text": "For example, a regular YouTube viewer does not necessarily tweet about the videos he/she has viewed.", "labels": [], "entities": []}, {"text": "Thus simple methods such as keyword matching are likely to fail.", "labels": [], "entities": [{"text": "keyword matching", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.8076430559158325}]}, {"text": "The same reasoning also implies that, when transferring knowledge across websites or services, the assumption of a shared rating format or structured text is overly optimistic.", "labels": [], "entities": []}, {"text": "Even websites aiming for the same purpose often violate this assumption, let alone websites of different types.", "labels": [], "entities": []}, {"text": "Therefore, we expect that the source and target services contain heterogeneous information (e.g. content vs. rating).", "labels": [], "entities": []}, {"text": "In our model, we make a less strong assumption: regardless of the type of information available in each domain, the users that are similar in one domain should have similar taste in the other domain.", "labels": [], "entities": []}, {"text": "Thus, instead of directly transfer the content material from source to target domain, we transfer the similarity between users, and use it as a guide to improve the CF model in the target domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use YouTube video recommendation to test the usefulness of NT-MF under the cold-start scenario.", "labels": [], "entities": [{"text": "NT-MF", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.8119619488716125}]}, {"text": "The NT-MF model used in this section follows the optimization procedure derived in Section 3.3.", "labels": [], "entities": []}, {"text": "To construct a dataset containing both the users' rating history and textual information, we begin with the user prole pages on Google+.", "labels": [], "entities": []}, {"text": "A large proportion of Google+ users provide links to their prole pages from other social network services (e.g. Twitter).", "labels": [], "entities": []}, {"text": "More importantly, if a user owns a YouTube account, a link to the user's YouTube channel will be automatically added to his Google+ prole.", "labels": [], "entities": [{"text": "Google+ prole", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.7843819459279379}]}, {"text": "This makes a fully aligned dataset available.", "labels": [], "entities": []}, {"text": "Users' Twitter accounts are obtained via their Google+ prole page, and the concatenation of tweets is regarded as the auxiliary text data.", "labels": [], "entities": []}, {"text": "It has been shown that by concatenating the tweets, more representative user topic vectors can be obtained.", "labels": [], "entities": []}, {"text": "We refer to this text data as the Twitter corpus.", "labels": [], "entities": [{"text": "Twitter corpus", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.7542060613632202}]}, {"text": "Videos in a user's \u0093liked\u0094 or \u0093favorite\u0094 playlists are considered to have a rating r ij = 1.", "labels": [], "entities": []}, {"text": "Other videos are assigned r ij = 0.", "labels": [], "entities": []}, {"text": "In other words, we are dealing with a one-class collaborative ltering (OCCF) problem.", "labels": [], "entities": []}, {"text": "We adopt the same strategy as in () to deal with OCCF.", "labels": [], "entities": [{"text": "OCCF", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.7680994272232056}]}, {"text": "First, all ratings are assumed to be observed, i.e. \u03b3 ij = 1 for all user-item pairs.", "labels": [], "entities": []}, {"text": "Next, a condence parameter c ij is introduced to reduce the inuence of the huge number of zeroes during model optimization.", "labels": [], "entities": []}, {"text": "The condence parameter takes place of the original rating precision parameter \u03bb 0 and is dened in () as c ij = a if r ij = 1 and c ij = b otherwise (a > b > 0).", "labels": [], "entities": [{"text": "rating precision parameter \u03bb 0", "start_pos": 51, "end_pos": 81, "type": "METRIC", "confidence": 0.78758345246315}]}, {"text": "All the derivations in the previous sections follow intuitively.", "labels": [], "entities": []}, {"text": "The titles of the liked videos are concatenated and treated as the text data in the target domain (which we refer to as the YouTube corpus).", "labels": [], "entities": [{"text": "YouTube corpus", "start_pos": 124, "end_pos": 138, "type": "DATASET", "confidence": 0.8521902859210968}]}, {"text": "As for the vocabulary, stopwords are rst removed, and then 5000 words are selected from the YouTube corpus based on their TF-IDF scores).", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.8504953980445862}]}, {"text": "On average, each user's Twitter text data contains 5149 words and 1193 distinct terms, and each user's YouTube text data contains 158 words and 116 distinct terms.", "labels": [], "entities": [{"text": "YouTube text data", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.7191211382548014}]}, {"text": "These statistics are in accordance with our assumption that text data in the source domain is abundant comparing to that in the target domain.", "labels": [], "entities": []}, {"text": "To validate the prediction result, each user has at least 10 liked videos.", "labels": [], "entities": []}, {"text": "Videos with less than 5 likes are removed from the dataset.", "labels": [], "entities": []}, {"text": "After data cleansing, there are 7328 users and 18691 videos in the dataset.", "labels": [], "entities": [{"text": "data cleansing", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7270060777664185}]}, {"text": "The maximum number of likes received by a video is 98, and the average is 19.1.", "labels": [], "entities": []}, {"text": "Among all videos, 92% of them are liked by less than 40 users.", "labels": [], "entities": []}, {"text": "The maximum number of likes given by a user is 908, and the average is 48.8.", "labels": [], "entities": []}, {"text": "Among all users, 89% of them have liked less than 100 videos.", "labels": [], "entities": []}, {"text": "The sparsity (ratio of zeroes to the total number of entries) of the rating matrix is 99.74%, which illustrates the difculty of this recommendation task.", "labels": [], "entities": [{"text": "sparsity", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9947721362113953}]}, {"text": "We choose the area under ROC curve (AUC) as the evaluation metric.", "labels": [], "entities": [{"text": "ROC curve (AUC)", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9163146257400513}]}, {"text": "AUC is often used to compare models when there is severe class imbalance, which is the casein our OCCF problem since we regard all zeroes as observed.", "labels": [], "entities": []}, {"text": "All reported results are the average of 5 random data splits.", "labels": [], "entities": []}, {"text": "Similar to the experiments performed in (, we test the performance of each model under two different scenarios.", "labels": [], "entities": []}, {"text": "The rst one is the task of in-matrix prediction.", "labels": [], "entities": []}, {"text": "In this task, the likes received by each video are partitioned into three sets, namely the training, validation and testing sets.", "labels": [], "entities": []}, {"text": "The ratio of data partition is 3:1:1.", "labels": [], "entities": []}, {"text": "There are no coldstart users for the in-matrix prediction.", "labels": [], "entities": []}, {"text": "The second task is the out-of-matrix prediction, where the users are partitioned into three sets with the same 3:1:1 ratio.", "labels": [], "entities": [{"text": "out-of-matrix prediction", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6230960190296173}]}, {"text": "To make the two tasks comparable, we randomly split the data until the number of observations in each of the three sets is closed to that of the in-matrix task.", "labels": [], "entities": []}, {"text": "Users in the testing set are all cold-start users.", "labels": [], "entities": []}, {"text": "The only data we have when making prediction on the cold-start users is the auxiliary text data.", "labels": [], "entities": []}], "tableCaptions": []}