{"title": [{"text": "Training with Exploration Improves a Greedy Stack LSTM Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "We adapt the greedy stack LSTM dependency parser of Dyer et al.", "labels": [], "entities": []}, {"text": "(2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history.", "labels": [], "entities": []}, {"text": "This form of training, which accounts for model predictions at training time, improves parsing accuracies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9640445113182068}]}, {"text": "We discuss some modifications needed in order to get training with exploration to work well fora probabilistic neu-ral network dependency parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure.", "labels": [], "entities": [{"text": "Natural language parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.644160121679306}, {"text": "transitionbased parsing", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.6943918764591217}]}, {"text": "The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state and predicts the next action to take conditioned on the state.", "labels": [], "entities": []}, {"text": "The state is of unbounded size.", "labels": [], "entities": []}, {"text": "presented a parser in which the parser's unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks.", "labels": [], "entities": []}, {"text": "Coupled with a recursive tree composition function, the feature representation is able to capture information from the entirety of the state, without resorting to locality assumptions that were common inmost other transition-based parsers.", "labels": [], "entities": []}, {"text": "The use of a novel stack LSTM data structure allows the parser to maintain a constant time per-state update, and retain an overall linear parsing time.", "labels": [], "entities": []}, {"text": "The Dyer et al. parser was trained to maximize the likelihood of gold-standard transition sequences, given words.", "labels": [], "entities": []}, {"text": "At test time, the parser makes greedy decisions according to the learned model.", "labels": [], "entities": []}, {"text": "Although this setup obtains very good performance, the training and testing conditions are mismatched in the following way: at training time the historical context of an action is always derived from the gold standard (i.e., perfectly correct past actions), but attest time, it will be a model prediction.", "labels": [], "entities": []}, {"text": "In this work, we adapt the training criterion so as to explore parser states drawn not only from the training data, but also from the model as it is being learned.", "labels": [], "entities": []}, {"text": "To do so, we use the method of to dynamically chose an optimal (relative to the final attachment accuracy) action given an imperfect history.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.5979827046394348}]}, {"text": "By interpolating between algorithm states sampled from the model and those sampled from the training data, more robust predictions attest time can be made.", "labels": [], "entities": []}, {"text": "We show that the technique can be used to improve the strong parser of Dyer et al.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the same settings of and Dyer et al we report results 4 in the English PTB and Chinese CTB-5.", "labels": [], "entities": [{"text": "English PTB", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8408154845237732}, {"text": "Chinese CTB-5", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.7901087999343872}]}, {"text": "The score achieved by the dynamic oracle for English is 93.56 UAS.", "labels": [], "entities": [{"text": "UAS", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.6404625177383423}]}, {"text": "This is remarkable given that the parser uses a completely greedy search procedure.", "labels": [], "entities": []}, {"text": "Moreover, the Chinese score establishes the state-of-the-art, using the same settings as  The error-exploring dynamic-oracle training always improves over static oracle training controlling for the transition system, but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle.", "labels": [], "entities": [{"text": "Chinese score", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.883182555437088}]}, {"text": "Flattening the sampling distribution (\u03b1 = 0.75) is especially beneficial when training with pretrained word embeddings.", "labels": [], "entities": []}, {"text": "In order to be able to compare with similar greedy parsers ( we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task", "start_pos": 138, "end_pos": 160, "type": "DATASET", "confidence": 0.9110467582941055}]}, {"text": "Since some of the treebanks contain nonprojective sentences and archybrid does not allow nonprojective trees, we use the pseudo-projective approach).", "labels": [], "entities": []}, {"text": "We used predicted part-of-speech tags provided by the CoNLL 2009 shared task organizers.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task organizers", "start_pos": 54, "end_pos": 87, "type": "DATASET", "confidence": 0.948244321346283}]}, {"text": "We also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as; for English and Chinese we used the same pretrained word embeddings as in, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3.", "labels": [], "entities": [{"text": "WMT 2015 dataset", "start_pos": 269, "end_pos": 285, "type": "DATASET", "confidence": 0.9233793417612711}]}], "tableCaptions": [{"text": " Table 1 shows  the results of the parser in its different configura- tions. The table also shows the best result obtained  with the static oracle (obtained by rerunning Dyer et  al. parser) for the sake of comparison between static  and dynamic training strategies.", "labels": [], "entities": []}, {"text": " Table 2: Dependency parsing results. The dynamic oracle uses \u03b1 = 0.75 (selected on English; see Table 1). PP refers to pseudo-", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8102535009384155}]}]}