{"title": [{"text": "Long Short-Term Memory-Networks for Machine Reading", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7884785234928131}]}], "abstractContent": [{"text": "In this paper we address the question of how to render sequence-level networks better at handling structured input.", "labels": [], "entities": []}, {"text": "We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention.", "labels": [], "entities": [{"text": "machine reading simulator", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.7561797698338827}]}, {"text": "The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell.", "labels": [], "entities": []}, {"text": "This enables adaptive memory usage during recurrence with neural attention, offering away to weakly induce relations among tokens.", "labels": [], "entities": []}, {"text": "The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture.", "labels": [], "entities": []}, {"text": "Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.695819690823555}, {"text": "sentiment analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9708489179611206}, {"text": "natural language inference", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.6673070391019186}]}], "introductionContent": [{"text": "How can a sequence-level network induce relations which are presumed latent during text processing?", "labels": [], "entities": []}, {"text": "How can a recurrent network attentively memorize longer sequences in away that humans do?", "labels": [], "entities": []}, {"text": "In this paper we design a machine reader that automatically learns to understand text.", "labels": [], "entities": []}, {"text": "The term machine reading is related to a wide range of tasks from answering reading comprehension questions, to fact and relation extraction ), ontology learning, and textual entailment).", "labels": [], "entities": [{"text": "machine reading", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7374578267335892}, {"text": "fact and relation extraction", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.5940849855542183}, {"text": "ontology learning", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.8195163309574127}, {"text": "textual entailment", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.713673934340477}]}, {"text": "Rather than focusing on a specific task, we develop a general-purpose reading simulator, drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the meaning of utterances on a word-by-word basis.", "labels": [], "entities": []}, {"text": "In order to understand texts, our machine reader should provide facilities for extracting and representing meaning from natural language text, storing meanings internally, and working with stored meanings to derive further consequences.", "labels": [], "entities": [{"text": "extracting and representing meaning from natural language text", "start_pos": 79, "end_pos": 141, "type": "TASK", "confidence": 0.7040842175483704}]}, {"text": "Ideally, such a system should be robust, open-domain, and degrade gracefully in the presence of semantic representations which maybe incomplete, inaccurate, or incomprehensible.", "labels": [], "entities": []}, {"text": "It would also be desirable to simulate the behavior of English speakers who process text sequentially, from left to right, fixating nearly every word while they read and creating partial representations for sentence prefixes.", "labels": [], "entities": []}, {"text": "Language modeling tools such as recurrent neural networks (RNN) bode well with human reading behavior (.", "labels": [], "entities": []}, {"text": "RNNs treat each sentence as a sequence of words and recursively compose each word with its previous memory, until the meaning of the whole sentence has been derived.", "labels": [], "entities": []}, {"text": "In practice, however, sequence-level networks are met with at least three challenges.", "labels": [], "entities": []}, {"text": "The first one concerns model training problems associated with vanishing and exploding gradients, which can be partially ameliorated with gated activation functions, such as the Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and gradient clipping (Pascanu et al.,: Illustration of our model while reading the sentence The FBI is chasing a criminal on the run.", "labels": [], "entities": [{"text": "Long ShortTerm Memory (LSTM)", "start_pos": 178, "end_pos": 206, "type": "METRIC", "confidence": 0.8334989845752716}]}, {"text": "Color red represents the current word being fixated, blue represents memories.", "labels": [], "entities": []}, {"text": "Shading indicates the degree of memory activation.", "labels": [], "entities": [{"text": "Shading", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9427633285522461}]}, {"text": "ficiently large memory capacity is required to store past information.", "labels": [], "entities": []}, {"text": "As a result, the network generalizes poorly to long sequences while wasting memory on shorter ones.", "labels": [], "entities": []}, {"text": "Finally, it should be acknowledged that sequence-level networks lack a mechanism for handling the structure of the input.", "labels": [], "entities": []}, {"text": "This imposes an inductive bias which is at odds with the fact that language has inherent structure.", "labels": [], "entities": []}, {"text": "In this paper, we develop a text processing system which addresses these limitations while maintaining the incremental, generative property of a recurrent language model.", "labels": [], "entities": []}, {"text": "Recent attempts to render neural networks more structure aware have seen the incorporation of external memories in the context of recurrent neural networks (.", "labels": [], "entities": []}, {"text": "The idea is to use multiple memory slots outside the recurrence to piece-wise store representations of the input; read and write operations for each slot can be modeled as an attention mechanism with a recurrent controller.", "labels": [], "entities": []}, {"text": "We also leverage memory and attention to empower a recurrent network with stronger memorization capability and more importantly the ability to discover relations among tokens.", "labels": [], "entities": []}, {"text": "This is realized by inserting a memory network module in the update of a recurrent network together with attention for memory addressing.", "labels": [], "entities": []}, {"text": "The attention acts as a weak inductive module discovering relations between input tokens, and is trained without direct supervision.", "labels": [], "entities": []}, {"text": "As a point of departure from previous work, the memory network we employ is internal to the recurrence, thus strengthening the interaction of the two and leading to a representation learner which is able to reason over shallow structures.", "labels": [], "entities": []}, {"text": "The resulting model, which we term Long Short-Term Memory-Network (LSTMN), is a reading simulator that can be used for sequence processing tasks.", "labels": [], "entities": [{"text": "sequence processing tasks", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.7996052702267965}]}, {"text": "illustrates the reading behavior of the LSTMN.", "labels": [], "entities": []}, {"text": "The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed.", "labels": [], "entities": []}, {"text": "As a result, the model induces undirected relations among tokens as an intermediate step of learning representations.", "labels": [], "entities": []}, {"text": "We validate the performance of the LSTMN in language modeling, sentiment analysis, and natural language inference.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7246217876672745}, {"text": "sentiment analysis", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.9678827226161957}, {"text": "natural language inference", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.629600872596105}]}, {"text": "In all cases, we train LSTMN models end-to-end with task-specific supervision signals, achieving performance comparable or better to state-of-the-art models and superior to vanilla LSTMs.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experiments for evaluating the performance of the LSTMN machine reader.", "labels": [], "entities": [{"text": "LSTMN machine reader", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.49042247732480365}]}, {"text": "We start with language modeling as it is a natural testbed for our model.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7224871218204498}]}, {"text": "We then assess the model's ability to extract meaning representations for generic sentence classification tasks such as sentiment analysis.", "labels": [], "entities": [{"text": "generic sentence classification", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7390965223312378}, {"text": "sentiment analysis", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.9176444411277771}]}, {"text": "Finally, we examine whether the LSTMN can recognize the semantic relationship between two sentences by applying it to a natural language inference task.", "labels": [], "entities": []}, {"text": "Our code is available at https://github.com/cheng6076/ SNLI-attention.: Language model perplexity on the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.9957420825958252}]}, {"text": "The size of memory is 300 for all models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language model perplexity on the Penn  Treebank. The size of memory is 300 for all models.", "labels": [], "entities": [{"text": "Penn  Treebank", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.9964779317378998}]}, {"text": " Table 2: Model accuracy (%) on the Sentiment Tree- bank (test set). The memory size of LSTMN models  is set to 168 to be compatible with previously pub- lished LSTM variants (Tai et al., 2015).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9839363098144531}, {"text": "Sentiment Tree- bank (test set", "start_pos": 36, "end_pos": 66, "type": "DATASET", "confidence": 0.8511160952704293}]}, {"text": " Table 3: Parameter counts |q| M , size of hidden  unit h, and model accuracy (%) on the natural lan- guage inference task.", "labels": [], "entities": [{"text": "Parameter counts", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.968269556760788}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9697590470314026}]}]}