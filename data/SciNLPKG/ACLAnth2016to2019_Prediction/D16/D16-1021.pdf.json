{"title": [{"text": "Aspect Level Sentiment Classification with Deep Memory Network", "labels": [], "entities": [{"text": "Aspect Level Sentiment Classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7974127978086472}]}], "abstractContent": [{"text": "We introduce a deep memory network for aspect level sentiment classification.", "labels": [], "entities": [{"text": "aspect level sentiment classification", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.7588721513748169}]}, {"text": "Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect.", "labels": [], "entities": []}, {"text": "Such importance degree and text representation are calculated with multiple computational layers, each of which is a neu-ral attention model over an external memory.", "labels": [], "entities": [{"text": "text representation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.6839888244867325}]}, {"text": "Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system , and substantially better than LSTM and attention-based LSTM architectures.", "labels": [], "entities": []}, {"text": "On both datasets we show that multiple computational layers could improve the performance.", "labels": [], "entities": []}, {"text": "Moreover , our approach is also fast.", "labels": [], "entities": []}, {"text": "The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Aspect level sentiment classification is a fundamental task in the field of sentiment analysis).", "labels": [], "entities": [{"text": "Aspect level sentiment classification", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8565440773963928}, {"text": "sentiment analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.9381084144115448}]}, {"text": "Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect.", "labels": [], "entities": []}, {"text": "For example, in sentence \"great food but the service was dreadful!\", the sentiment polarity of aspect \"food\" is positive while the polarity of aspect \"service\" is * Corresponding author. negative.", "labels": [], "entities": []}, {"text": "Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.8005064725875854}]}, {"text": "Representative approaches in literature include feature based Support Vector Machine () and neural network models ().", "labels": [], "entities": []}, {"text": "Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.", "labels": [], "entities": []}, {"text": "Despite these advantages, conventional neural models like long short-term memory (LSTM) () capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect.", "labels": [], "entities": []}, {"text": "We believe that only some subset of context words are needed to infer the sentiment towards an aspect.", "labels": [], "entities": []}, {"text": "For example, in sentence \"great food but the service was dreadful!\", \"dreadful\" is an important clue for the aspect \"service\" but \"great\" is not needed.", "labels": [], "entities": []}, {"text": "Standard LST-M works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word.", "labels": [], "entities": []}, {"text": "A desirable solution should be capable of explicitly capturing the importance of context words and using that information to buildup features for the sentence after given an aspect word.", "labels": [], "entities": []}, {"text": "Furthermore, a human asked to do this task will selectively focus on parts of the contexts, and acquire information where it is needed to buildup an internal representation towards an aspect in his/her mind.", "labels": [], "entities": []}, {"text": "In pursuit of this goal, we develop deep memory network for aspect level sentiment classification, which is inspired by the recent success of computational models with attention mechanism and explicit memory (.", "labels": [], "entities": [{"text": "aspect level sentiment classification", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.6835833266377449}]}, {"text": "Our approach is data-driven, computationally efficient and does not rely on syntactic parser or sentiment lexicon.", "labels": [], "entities": []}, {"text": "The approach consists of multiple computational layers with shared parameters.", "labels": [], "entities": []}, {"text": "Each layer is a content-and location-based attention model, which first learns the importance/weight of each context word and then utilizes this information to calculate continuous text representation.", "labels": [], "entities": []}, {"text": "The text representation in the last layer is regarded as the feature for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.9742829203605652}]}, {"text": "As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 171, "end_pos": 195, "type": "TASK", "confidence": 0.8520009815692902}]}, {"text": "We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 ().", "labels": [], "entities": [{"text": "laptop and restaurant datasets from SemEval 2014", "start_pos": 34, "end_pos": 82, "type": "DATASET", "confidence": 0.6619289474827903}]}, {"text": "Experimental results show that our approach performs comparable to atop system using feature-based SVM ().", "labels": [], "entities": []}, {"text": "On both datasets, our approach outperforms both LST-M and attention-based LSTM models () in terms of classification accuracy and running speed.", "labels": [], "entities": [{"text": "classification", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.9354049563407898}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9328340291976929}]}, {"text": "Lastly, we show that using multiple computational layers over external memory could achieve improved performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe experimental settings and report empirical results in this section.", "labels": [], "entities": []}, {"text": "We conduct experiments on two datasets from SemEval 2014 (), one from laptop domain and another from restaurant domain.", "labels": [], "entities": [{"text": "SemEval 2014", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.7475693225860596}]}, {"text": "Statistics of the datasets are given in.", "labels": [], "entities": []}, {"text": "It is worth noting that the original dataset contains the fourth category -conflict, which means that a sentence expresses both positive and negative opinion towards an aspect.", "labels": [], "entities": []}, {"text": "We remove conflict category as the number of instances is very tiny, incorporating which Available at: http://nlp.stanford.edu/projects/glove/.", "labels": [], "entities": []}, {"text": "will make the dataset extremely unbalanced.", "labels": [], "entities": []}, {"text": "Evaluation metric is classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.9588261842727661}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8949626088142395}]}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Classification accuracy of different methods on laptop", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9412381649017334}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9791688323020935}]}, {"text": " Table 3: Runtime (seconds) of each training epoch on the", "labels": [], "entities": [{"text": "Runtime", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9975704550743103}]}, {"text": " Table 4: Examples of attention weights in different hops for aspect level sentiment classification. The model only uses content", "labels": [], "entities": [{"text": "aspect level sentiment classification", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.7194877490401268}]}, {"text": " Table 5: Examples of attention weights in different hops for aspect level sentiment classification. The model also takes into account", "labels": [], "entities": [{"text": "aspect level sentiment classification", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.7139705568552017}]}]}