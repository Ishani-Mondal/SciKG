{"title": [{"text": "Attention-based LSTM Network for Cross-Lingual Sentiment Classification", "labels": [], "entities": [{"text": "Cross-Lingual Sentiment Classification", "start_pos": 33, "end_pos": 71, "type": "TASK", "confidence": 0.8097837169965109}]}], "abstractContent": [{"text": "Most of the state-of-the-art sentiment classification methods are based on supervised learning algorithms which require large amounts of manually labeled data.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9067572057247162}]}, {"text": "However, the labeled resources are usually imbalanced in different languages.", "labels": [], "entities": []}, {"text": "Cross-lingual sentiment classification tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages.", "labels": [], "entities": [{"text": "Cross-lingual sentiment classification", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7658644715944926}]}, {"text": "In this study, we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages.", "labels": [], "entities": []}, {"text": "In each language, we use Long Short Term Memory (LSTM) network to model the documents, which has been proved to be very effective for word sequences.", "labels": [], "entities": []}, {"text": "Meanwhile, we propose a hierarchical attention mechanism for the bilingual LSTM network.", "labels": [], "entities": []}, {"text": "The sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive.", "labels": [], "entities": []}, {"text": "The proposed model achieves good results on a benchmark dataset using English as the source language and Chinese as the target language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most of the sentiment analysis research focuses on sentiment classification which aims to determine whether the users attitude is positive, neutral or negative.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9495158195495605}, {"text": "sentiment classification", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.9197835028171539}]}, {"text": "There are two classes of mainstreaming sentiment classification algorithms: unsupervised methods which usually require a sentiment lexicon) and supervised methods () which require manually labeled data.", "labels": [], "entities": [{"text": "mainstreaming sentiment classification", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.8177472949028015}]}, {"text": "However, both of these sentiment resources are unbalanced in different languages.", "labels": [], "entities": []}, {"text": "The sentiment lexicon or labeled data are rich in several languages such as English and are poor in others.", "labels": [], "entities": []}, {"text": "Manually building these resources for all the languages will be expensive and time-consuming.", "labels": [], "entities": []}, {"text": "Cross-lingual sentiment classification tackles the problem by trying to adapt the resources in one language to other languages.", "labels": [], "entities": [{"text": "Cross-lingual sentiment classification", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7974110245704651}]}, {"text": "It can also be regarded as a special kind of cross-lingual text classification task.", "labels": [], "entities": [{"text": "cross-lingual text classification task", "start_pos": 45, "end_pos": 83, "type": "TASK", "confidence": 0.6971872895956039}]}, {"text": "Recently, there have been several bilingual representation learning methods such as () for cross-lingual sentiment or text classification which achieve promising results.", "labels": [], "entities": [{"text": "cross-lingual sentiment", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7948350608348846}, {"text": "text classification", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.763567715883255}]}, {"text": "They try to learn a joint embedding space for different languages such that the training data in the source language can be directly applied to the test data in the target language.", "labels": [], "entities": []}, {"text": "However, most of the studies only use simple functions, e.g. arithmetic average, to synthesize representations for larger text sequences.", "labels": [], "entities": []}, {"text": "Some of them use more complicated compositional models such as the bi-gram non-linearity model in () which also fail to capture the long distance dependencies in texts.", "labels": [], "entities": []}, {"text": "In this study, we propose an attention-based bilingual LSTM network for cross-lingual sentiment classification.", "labels": [], "entities": [{"text": "cross-lingual sentiment classification", "start_pos": 72, "end_pos": 110, "type": "TASK", "confidence": 0.8537895282109579}]}, {"text": "LSTMs have been proved to be very effective to model word sequences and are powerful to learn on data with long range temporal dependencies.", "labels": [], "entities": []}, {"text": "After translating the training data into the target language using machine translation tools, we use the bidirectional LSTM network to model the documents in both of the source and the target languages.", "labels": [], "entities": []}, {"text": "The LSTMs show strong ability to capture the compositional semantics for the bilingual texts in our experiments.", "labels": [], "entities": []}, {"text": "For the traditional LSTM network, each word in the input document is treated with equal importance, which is reasonable for traditional text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.7942150235176086}]}, {"text": "In this paper, we propose a hierarchical attention mechanism which enables our model to focus on certain part of the input document.", "labels": [], "entities": []}, {"text": "The motivation mainly comes from the following three observations: 1) the machine translation tool that we use to translate the documents will always introduce much noise for sentiment classification.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7086046487092972}, {"text": "sentiment classification", "start_pos": 175, "end_pos": 199, "type": "TASK", "confidence": 0.9480337798595428}]}, {"text": "We hope that the attention mechanism can help to filter out these noises.", "labels": [], "entities": []}, {"text": "2) In each individual language, the sentiment of a document is usually decided by a relative small part of it.", "labels": [], "entities": []}, {"text": "Ina long review document, the user might discuss both the advantages and disadvantages of a product.", "labels": [], "entities": []}, {"text": "The sentiment will be confusing if we consider each sentence of the same contribution.", "labels": [], "entities": []}, {"text": "For example, in the first review of, the first sentence reveals a negative sentiment towards the movie but the second one reveals a positive sentiment.", "labels": [], "entities": []}, {"text": "As human readers, we can understand that the review is expressing a positive overall sentiment but it is hard for the sequence modeling algorithms including LSTM to capture.", "labels": [], "entities": []}, {"text": "3) At the sentence level, it is important to focus on the sentiment signals such as the sentiment words.", "labels": [], "entities": []}, {"text": "They are usually very decisive to determine the polarity even fora very long sentence, e.g. \"easy\" and \"nice\" in the second example of \"I felt it could have been a lot better with a little less comedy and a little more drama to get the point across.", "labels": [], "entities": []}, {"text": "However, its still a must see for any Jim Carrey fan.", "labels": [], "entities": []}, {"text": "\" \"It is easy to read, it is easy to look things up in and provides a nice section on the treatments.\"", "labels": [], "entities": []}, {"text": "In sum, the main contributions of this study are summarized as follows: 1) We propose a bilingual LSTM network for cross-lingual sentiment classification.", "labels": [], "entities": [{"text": "cross-lingual sentiment classification", "start_pos": 115, "end_pos": 153, "type": "TASK", "confidence": 0.7799728910128275}]}, {"text": "Compared to the previous methods which only use weighted or arithmetic average of word embeddings to represent the document, LSTMs have obvious advantage to model the compositional semantics and to capture the long distance dependencies between words for bilingual texts.", "labels": [], "entities": []}, {"text": "2) We propose a hierarchical bilingual attention mechanism for our model.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first attention-based model designed for cross-lingual sentiment analysis.", "labels": [], "entities": [{"text": "cross-lingual sentiment analysis", "start_pos": 83, "end_pos": 115, "type": "TASK", "confidence": 0.833413819471995}]}, {"text": "3) The proposed framework achieves good results on a benchmark dataset from a cross-language sentiment classification evaluation.", "labels": [], "entities": [{"text": "cross-language sentiment classification evaluation", "start_pos": 78, "end_pos": 128, "type": "TASK", "confidence": 0.7387708202004433}]}, {"text": "It outperforms the best team in the evaluation as well as several strong baseline methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the dataset from the cross-language sentiment classification evaluation of NLP&CC", "labels": [], "entities": [{"text": "cross-language sentiment classification", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.6347992320855459}, {"text": "NLP&CC", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.853122353553772}]}], "tableCaptions": [{"text": " Table 2: Cross-lingual sentiment prediction accuracy of our", "labels": [], "entities": [{"text": "Cross-lingual sentiment prediction", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.815404216448466}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9898539185523987}]}, {"text": " Table 3: Comparison of different attention mechanisms", "labels": [], "entities": []}, {"text": " Table 1. We show the visualized word attention", "labels": [], "entities": []}, {"text": " Table 4: Performance of our model with four different word", "labels": [], "entities": []}]}