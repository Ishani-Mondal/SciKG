{"title": [{"text": "Modeling Human Reading with Neural Attention", "labels": [], "entities": [{"text": "Modeling Human Reading", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8578973015149435}]}], "abstractContent": [{"text": "When humans read text, they fixate some words and skip others.", "labels": [], "entities": []}, {"text": "However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g., using surprisal).", "labels": [], "entities": [{"text": "skipping behavior", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8919536769390106}]}, {"text": "In this paper , we propose a novel approach that models both skipping and reading, using an unsuper-vised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning.", "labels": [], "entities": []}, {"text": "Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible).", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9975466132164001}]}, {"text": "We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.", "labels": [], "entities": [{"text": "Dundee eye-tracking corpus", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.9052124818166097}]}], "introductionContent": [{"text": "Humans read text by making a sequence of fixations and saccades.", "labels": [], "entities": []}, {"text": "During a fixation, the eyes land on a word and remain fairly static for 200-250 ms.", "labels": [], "entities": []}, {"text": "Saccades are the rapid jumps that occur between fixations, typically lasting 20-40 ms and spanning 7-9 characters.", "labels": [], "entities": []}, {"text": "Readers, however, do not simply fixate one word after another; some saccades go in reverse direction, and some words are fixated more than once or skipped altogether.", "labels": [], "entities": []}, {"text": "A range of computational models have been developed to account for human eye-movements in reading, including models of saccade generation in cognitive psychology, such as EZ-Reader (), SWIFT (), or the Bayesian Model of.", "labels": [], "entities": [{"text": "saccade generation", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7294866591691971}]}, {"text": "More recent approaches use machine learning models trained on eye-tracking data to predict human reading patterns.", "labels": [], "entities": []}, {"text": "Both types of models involve theoretical assumptions about human eye-movements, or at least require the selection of relevant eye-movement features.", "labels": [], "entities": []}, {"text": "Model parameters have to be estimated in a supervised way from eye-tracking corpora.", "labels": [], "entities": []}, {"text": "Unsupervised approaches, that do not involve training the model on eye-tracking data, have also been proposed.", "labels": [], "entities": []}, {"text": "A key example is surprisal, which measures the predictability of a word in context, defined as the negative logarithm of the conditional probability of the current word given the preceding words.", "labels": [], "entities": [{"text": "surprisal", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9263902306556702}]}, {"text": "Surprisal is computed by a language model, which can take the form of a probabilistic grammar, an n-gram model, or a recurrent neural network.", "labels": [], "entities": []}, {"text": "While surprisal has been shown to correlate with word-by-word reading times, it cannot explain other aspects of human reading, such as reverse saccades, re-fixations, or skipping.", "labels": [], "entities": []}, {"text": "Skipping is a particularly intriguing phenomenon: about 40% of all words are skipped (in the Dundee corpus, see below), without apparent detriment to text understanding.", "labels": [], "entities": [{"text": "Skipping", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9234998226165771}, {"text": "Dundee corpus", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9883200228214264}, {"text": "text understanding", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.754536360502243}]}, {"text": "In this paper, we propose a novel model architecture that is able to explain which words are skipped and which ones are fixated, while also predicting reading times for fixated words.", "labels": [], "entities": []}, {"text": "Our approach is completely unsupervised and requires only unlabeled text for training.", "labels": [], "entities": []}, {"text": "Compared to language as a whole, reading is a recent innovation in evolutionary terms, and people learning to read do not have access to competent readers' eye-movement patterns as training data.", "labels": [], "entities": []}, {"text": "This suggests that human eye-movement patterns emerge from general principles of language processing that are independent of reading.", "labels": [], "entities": []}, {"text": "Our starting point is the Tradeoff Hypothesis: Human reading optimizes a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible).", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9978623986244202}]}, {"text": "Based on the Tradeoff Hypothesis, we expect that humans only fixate words to the extent necessary for language understanding, while skipping words whose contribution to the overall meaning can be inferred from context.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7084911316633224}]}, {"text": "In order to test these assumptions, this paper investigates the following questions: 1.", "labels": [], "entities": []}, {"text": "Can the Tradeoff Hypothesis be implemented in an unsupervised model that predicts skipping and reading times in quantitative terms?", "labels": [], "entities": []}, {"text": "In particular, can we compute surprisal based only on the words that are actually fixated?", "labels": [], "entities": []}, {"text": "2. Can the Tradeoff Hypothesis explain known qualitative features of human fixation patterns?", "labels": [], "entities": []}, {"text": "These include dependence on word frequency, word length, predictability in context, a contrast between content and function words, and the statistical dependence of the current fixation on previous fixations.", "labels": [], "entities": []}, {"text": "To investigate these questions, we develop a generic architecture that combines neural language modeling with recent ideas on integrating recurrent neural networks with mechanisms of attention, which have shown promise both in NLP and in computer vision.", "labels": [], "entities": []}, {"text": "We train our model end-to-end on a large text corpus to optimize a tradeoff between minimizing input reconstruction error and minimizing the number of words fixated.", "labels": [], "entities": [{"text": "minimizing input reconstruction", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.6276378631591797}]}, {"text": "We evaluate the model's reading behavior against a corpus of human eye-tracking data.", "labels": [], "entities": []}, {"text": "Apart from the unlabeled training corpus and the generic architecture, no further assumptions about language structure are made -in particular, no lexicon or grammar or otherwise labeled data is required.", "labels": [], "entities": []}, {"text": "Our unsupervised model is able to predict human skips and fixations with an accuracy of 63.7%.", "labels": [], "entities": [{"text": "predict human skips and fixations", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.6607666671276092}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9995381832122803}]}, {"text": "This compares to a baseline of 52.6% and a supervised accuracy of 69.9%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9168015122413635}]}, {"text": "For fixated words, the model significantly predicts human reading times in a linear mixed effects analysis.", "labels": [], "entities": []}, {"text": "The performance of our model is comparable to surprisal, even though it only fixates 60.4% of all input words.", "labels": [], "entities": []}, {"text": "Furthermore, we show that known qualitative features of human fixation sequences emerge in our model without additional assumptions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Evaluation of fixation sequence predictions against hu-", "labels": [], "entities": []}, {"text": " Table 3: Linear mixed effects models for first pass duration.", "labels": [], "entities": [{"text": "first pass duration", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.5913642048835754}]}, {"text": " Table 4: Actual and simulated fixation probabilities (in %) by", "labels": [], "entities": []}, {"text": " Table 5: Correlations between human and NEAT fixation prob-", "labels": [], "entities": [{"text": "NEAT fixation prob", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8918935656547546}]}]}