{"title": [{"text": "Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter", "labels": [], "entities": [{"text": "Keyphrase Extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6463034451007843}]}], "abstractContent": [{"text": "Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas.", "labels": [], "entities": []}, {"text": "The task of automatically extracting them have received considerable attention in recent decades.", "labels": [], "entities": [{"text": "automatically extracting them", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.8398238817850748}]}, {"text": "Different from previous studies, which are usually focused on automatically extracting keyphrases from documents or articles, in this study, we considered the problem of automatically extracting keyphrases from tweets.", "labels": [], "entities": []}, {"text": "Because of the length limitations of Twitter-like sites, the performances of existing methods usually drop sharply.", "labels": [], "entities": []}, {"text": "We proposed a novel deep recurrent neural network (RNN) model to combine keywords and context information to perform this problem.", "labels": [], "entities": []}, {"text": "To evaluate the proposed method, we also constructed a large-scale dataset collected from Twitter.", "labels": [], "entities": []}, {"text": "The experimental results showed that the proposed method performs significantly better than previous methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Keyphrases are usually the selected phrases that can capture the main topics described in a given document).", "labels": [], "entities": []}, {"text": "They can provide users with highly condensed and valuable information, and there area wide variety of sources for keyphrases, including web pages, research articles, books, and even movies.", "labels": [], "entities": []}, {"text": "In contrast to keywords, keyphrases usually contain two or more words.", "labels": [], "entities": []}, {"text": "Normally, the meaning representations of these phrases are more precise than those of single words.", "labels": [], "entities": []}, {"text": "Moreover, along with the increasing development of the internet, this kind of summarization has received continuous consideration in recent years from both the academic and entiprise communities).", "labels": [], "entities": []}, {"text": "Because of the enormous usefulness of keyphrases, various studies have been conducted on the automatic extraction of keyphrases using different methods, including rich linguistic features, supervised classification-based methods;), ranking-based methods, and clustering-based methods).", "labels": [], "entities": []}, {"text": "These methods usually focus on extracting keyphrases from a single document or multiple documents.", "labels": [], "entities": []}, {"text": "Typically, a large number of words exist in even a document of moderate length, where a few hundred words or more is common.", "labels": [], "entities": []}, {"text": "Hence, statistical and linguistic features can be considered to determine the importance of phrases.", "labels": [], "entities": []}, {"text": "In addition to the previously mentioned methods, a few researchers have studied the problem of extracting keyphrases from collections of tweets (.", "labels": [], "entities": [{"text": "extracting keyphrases from collections of tweets", "start_pos": 95, "end_pos": 143, "type": "TASK", "confidence": 0.7809742093086243}]}, {"text": "In contrast to traditional web applications, Twitter-like services usually limit the content length to 140 characters.", "labels": [], "entities": []}, {"text": "In (), the contextsensitive topical PageRank method was proposed to extract keyphrases by topic from a collection of tweets.", "labels": [], "entities": []}, {"text": "NE-Rank was also proposed to rank keywords for the purpose of extracting topical keyphrases ().", "labels": [], "entities": []}, {"text": "Because multiple tweets are usually organized by topic, many document-level approaches can also be adopted to achieve the task.", "labels": [], "entities": []}, {"text": "In contrast with the previous methods, focused on the task of extracting keywords from single tweets.", "labels": [], "entities": []}, {"text": "They used several unsupervised methods and word embeddings to construct features.", "labels": [], "entities": []}, {"text": "However, the proposed method worked on the word level.", "labels": [], "entities": []}, {"text": "In this study, we investigated the problem of automatically extracting keyphrases from single tweets.", "labels": [], "entities": [{"text": "automatically extracting keyphrases from single tweets", "start_pos": 46, "end_pos": 100, "type": "TASK", "confidence": 0.7793865005175272}]}, {"text": "Compared to the problem of identifying keyphrases from documents containing hundreds of words, the problem of extracting keyphrases from a single short text is generally more difficult.", "labels": [], "entities": []}, {"text": "Many linguistic and statistical features (e.g., the number of word occurrences) cannot be determined and used.", "labels": [], "entities": []}, {"text": "Moreover, the standard steps of keyphrase extraction usually include keyword ranking, candidate keyphrase generation, and keyphrase ranking.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8179335594177246}, {"text": "keyword ranking", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7368751466274261}, {"text": "candidate keyphrase generation", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.6160462896029154}, {"text": "keyphrase ranking", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.8414758145809174}]}, {"text": "Previous works usually used separate methods to handle these steps.", "labels": [], "entities": []}, {"text": "Hence, the error of each step is propagated, which may highly impact the final performance.", "labels": [], "entities": []}, {"text": "Another challenge of keyphrase extraction on Twitter is the lack of training and evaluation data.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.850722998380661}]}, {"text": "Manual labelling is a time-consuming procedure.", "labels": [], "entities": []}, {"text": "The labelling consistency of different labellers cannot be easily controlled.", "labels": [], "entities": []}, {"text": "To meet these challenges, in this paper, we propose a novel deep recurrent neural network (RNN) model for the joint processing of the keyword ranking, keyphrase generation, and keyphrase ranking steps.", "labels": [], "entities": [{"text": "keyword ranking", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.688169538974762}, {"text": "keyphrase generation", "start_pos": 151, "end_pos": 171, "type": "TASK", "confidence": 0.7794415354728699}, {"text": "keyphrase ranking", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.825617790222168}]}, {"text": "The proposed RNN model contains two hidden layers.", "labels": [], "entities": []}, {"text": "In the first hidden layer, we capture the keyword information.", "labels": [], "entities": []}, {"text": "Then, in the second hidden layer, we extract the keyphrases based on the keyword information using a sequence labelling method.", "labels": [], "entities": []}, {"text": "In order to train and evaluate the proposed method, we also proposed a novel method to construct a dataset that contained a large number of tweets with golden standard keyphrases.", "labels": [], "entities": []}, {"text": "The proposed dataset construction method was based on the hashtag definitions in Twitter and how these were used in specific tweets.", "labels": [], "entities": []}, {"text": "The main contributions of this work can be summarized as follows: \u2022 We proposed a two-hidden-layer RNN-based method to jointly model the keyword ranking, keyphrase generation, and keyphrase ranking steps.", "labels": [], "entities": [{"text": "keyword ranking", "start_pos": 137, "end_pos": 152, "type": "TASK", "confidence": 0.7277756035327911}, {"text": "keyphrase generation", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.8026956617832184}, {"text": "keyphrase ranking", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.8045082092285156}]}, {"text": "\u2022 To train and evaluate the proposed method, we proposed a novel method for constructing a large dataset, which consisted of more than one million words.", "labels": [], "entities": []}, {"text": "\u2022 Experimental results demonstrated that the proposed method could achieve better results than the current state-of-the-art methods for these tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "To perform an experiment on extracting keyphrases, we used 70% as a training set, 10% as a development set, and 20% as a testing set.", "labels": [], "entities": []}, {"text": "For evaluation metrics, we used the precision (P), recall (R), and F1-score (F1) to evaluate the performance.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9606837928295135}, {"text": "recall (R)", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9659394919872284}, {"text": "F1-score (F1)", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.9309098869562149}]}, {"text": "The precision was calculated based on the percentage of keyphrases truly identified among the keyphrases labeled by the system.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994046688079834}]}, {"text": "Recall was calculated based on the keyphrases truly identified among the golden standard keyphrases.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9837017059326172}]}, {"text": "In the experiments, we use word embeddings as input to the neural network.", "labels": [], "entities": []}, {"text": "The word embeddings we used in this work were pre-trained vectors trained on part of a Google News dataset (about 100 billion words).", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.8020952939987183}]}, {"text": "A skip-gram model () was used to generate these 300-dimensional vectors for 3 million words and phrases.", "labels": [], "entities": []}, {"text": "We used the word embeddings to initialize our word weight matrix.", "labels": [], "entities": []}, {"text": "The matrix was updated in the training process.", "labels": [], "entities": []}, {"text": "The default parameters of our model are as follows: The window size is 3, number of neurons in the hidden layer is 300, and \u03b1 is 0.5, which were chosen based on the performance using the valid set.", "labels": [], "entities": []}, {"text": "To further analyze the keyword extraction results on Twitter, we compared AKET and our method.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7998405694961548}, {"text": "AKET", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.7758114337921143}]}, {"text": "In, we can see that except for the recall, AKET is a little better than our method, but our method performed significantly better than AKET in the precision and F-score.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9995299577713013}, {"text": "AKET", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.7421178817749023}, {"text": "AKET", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.548635721206665}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9998180270195007}, {"text": "F-score", "start_pos": 161, "end_pos": 168, "type": "METRIC", "confidence": 0.9975101947784424}]}, {"text": "This indicates that our PR F1 AKET 20.68% 87.56% 33.46% Joint-layer RNN 87.45% 85.38% 86.40%: Keyword Extraction on Twitter model indeed has better performance in keyword finding.", "labels": [], "entities": [{"text": "PR F1 AKET", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.5525674223899841}, {"text": "Keyword Extraction", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.8109558522701263}, {"text": "keyword finding", "start_pos": 163, "end_pos": 178, "type": "TASK", "confidence": 0.7991621196269989}]}, {"text": "In summary, the experimental results conclusively demonstrated that the proposed joint-layer RNN method is superior to the state-of-the-art methods when measured using commonly accepted performance metrics on Twitter.", "labels": [], "entities": []}, {"text": "To analysis the sensitivity of the hyper-parameters of the joint-layer RNN, we conducted several empirical experiments on the dataset.", "labels": [], "entities": []}, {"text": "shows the performances of the jointlayer RNN with different numbers of neurons in the hidden layers.", "labels": [], "entities": []}, {"text": "To simplify, we made hidden layer 1 and hidden layer 2 have the same number of neurons.", "labels": [], "entities": []}, {"text": "In the figure, the x-axis denotes the number of neurons, and the y-axis denotes the precision, recall, and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.999760091304779}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9991170763969421}, {"text": "F-score", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9984991550445557}]}, {"text": "The data used for constructing the test set were the same as we used in the previous section.", "labels": [], "entities": []}, {"text": "From the figure, we can observe that the number of neurons in the hidden layers do not highly affect the final performance.", "labels": [], "entities": []}, {"text": "Three performance indicators of the joint-layer RNN change stably with different numbers of neurons.", "labels": [], "entities": []}, {"text": "shows the performances of the joint-layer RNN with different window sizes.", "labels": [], "entities": []}, {"text": "In the figure, the x-axis denotes the different window size, and the yaxis denotes the precision, recall, and F-score.", "labels": [], "entities": [{"text": "yaxis", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9598850011825562}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9997413754463196}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9989981055259705}, {"text": "F-score", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9978221654891968}]}, {"text": "From the figure, we observe that when the window size is one, the three performance indicators of jointlayer RNN perform badly.", "labels": [], "entities": []}, {"text": "Then, as the window size increases, the three performance indicators change stably.", "labels": [], "entities": []}, {"text": "The main reason may possibly be that when the window size is one, the model just uses the current word information.", "labels": [], "entities": []}, {"text": "When the window size increases, the model uses the context information of the current word but the most important context information is nearby the current word.", "labels": [], "entities": []}, {"text": "shows the performances of the joint-layer RNN with different \u03b1 values.", "labels": [], "entities": []}, {"text": "In the figure, the xaxis denotes the value of \u03b1 used for training, and the y-axis denotes the precision, recall, and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9997550845146179}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9994926452636719}, {"text": "F-score", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.9984291195869446}]}, {"text": "We can see that the best performance is obtained when \u03b1 is around 0.5.", "labels": [], "entities": []}, {"text": "This indicates that our model emphasizes the combination of keyword finding and keyphrase extraction.", "labels": [], "entities": [{"text": "keyword finding", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.804411381483078}, {"text": "keyphrase extraction", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.8029952347278595}]}, {"text": "lists the effects of word embedding.", "labels": [], "entities": []}, {"text": "We can see that the performance when updating the word embedding is better than when not updating, and the performance of word embedding is a little better than random word embedding.", "labels": [], "entities": []}, {"text": "The main reason is that the vocabulary size is 147,377, but the number of words from tweets that exist in the word embedding trained on the Google News dataset is just 35,133.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 140, "end_pos": 159, "type": "DATASET", "confidence": 0.8457180460294088}]}, {"text": "This means that 76.2% of the words are missing.", "labels": [], "entities": []}, {"text": "This also confirms that the proposed jointlayer RNN is more suitable for keyphrase extraction on Twitter.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7775065898895264}]}, {"text": "shows the performances of the joint-layer RNN with different percentages of training data.", "labels": [], "entities": []}, {"text": "In the figure, the x-axis denotes the percentages of data used for training, and the y-axis denotes the precision, recall, and F-score.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.999749481678009}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9992020726203918}, {"text": "F-score", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.9985692501068115}]}, {"text": "From the we observe that as the amount of training data increases, the three performance indicators of the joint-layer RNN consequently improve.", "labels": [], "entities": []}, {"text": "When the percentage of training data is greater than 60% of the whole dataset, the performance indicators slowly increase.", "labels": [], "entities": []}, {"text": "The main reason may possibly be that the number concepts included in these data sets are small.", "labels": [], "entities": []}, {"text": "However, on the other hand, we can say that the proposed joint-layer RNN method can achieve acceptable results with a few ground truths.", "labels": [], "entities": []}, {"text": "Hence, it can be easily adopted for other data sets.", "labels": [], "entities": []}, {"text": "Since the keyphrase extraction training process is solved using an iterative procedure, we also evaluated its convergence property.", "labels": [], "entities": [{"text": "keyphrase extraction training", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.9072394768397013}]}, {"text": "shows the precision, recall, and F-score performances of the joint-layer RNN.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9996668100357056}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992401599884033}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9963899254798889}]}, {"text": "In the figure, the x-axis denotes the number of epochs for optimizing the model, and the y-axis denotes the precision, recall, and Fscore.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9997332692146301}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.999404788017273}, {"text": "Fscore", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9993622899055481}]}, {"text": "From the figure, we observe that the jointlayer RNN can coverage with less than six iterations.", "labels": [], "entities": []}, {"text": "This means that the joint-layer RNN can achieve a stable and superior performance under a wide range of parameter values.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical information of dataset. W , T , \u00af  Nw, and \u00af  Nt  are the vocabulary of words, number of tweets with hashtags,", "labels": [], "entities": []}, {"text": " Table 2: Keyphrase Extraction on Twitter", "labels": [], "entities": [{"text": "Keyphrase Extraction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6766188740730286}]}, {"text": " Table 4: Effects of embedding on performance. WEU, WENU,", "labels": [], "entities": [{"text": "WEU", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8707456588745117}, {"text": "WENU", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.516452968120575}]}]}