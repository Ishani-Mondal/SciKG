{"title": [{"text": "Building compositional semantics and higher-order inference system fora wide-coverage Japanese CCG parser", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a system that compo-sitionally maps outputs of a wide-coverage Japanese CCG parser onto semantic representations and performs automated inference in higher-order logic.", "labels": [], "entities": []}, {"text": "The system is evaluated on a textual entailment dataset.", "labels": [], "entities": []}, {"text": "It is shown that the system solves inference problems that focus on a variety of complex linguistic phenomena , including those that are difficult to represent in the standard first-order logic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Logic-based semantic representations have played an important role in the study of semantic parsing and inference.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7161794304847717}]}, {"text": "For English, several methods have been proposed to map outputs of parsers based on syntactic theories like CCG) ontological formulas.", "labels": [], "entities": []}, {"text": "Output formulas have been used in various tasks, including Question Answering ( and Recognizing Textual Entailment (RTE)).", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7616551518440247}, {"text": "Recognizing Textual Entailment (RTE))", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.7440623690684637}]}, {"text": "Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (.", "labels": [], "entities": [{"text": "Syntactic and semantic parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6391782984137535}, {"text": "chunk-based dependency parsing", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.6044005354245504}, {"text": "semantic role labelling", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.626690944035848}]}, {"text": "Recently, the method of inducing wide-coverage CCG resources for English) has been applied to Japanese and a robust CCG parser based on it has been developed (.", "labels": [], "entities": []}, {"text": "However, building a method to map CCG trees in Japanese ontological formulas is not a trivial task, mainly due to the differences in syntactic structures between English and Japanese (Section 3).", "labels": [], "entities": []}, {"text": "There are two primary contributions of this paper.", "labels": [], "entities": []}, {"text": "First, based on an in-depth analysis of the syntax-semantics interface in Japanese, we present the first system that compositionally derives semantic representations fora wide-coverage Japanese CCG parser.", "labels": [], "entities": []}, {"text": "Output representations are formulas in higher-order logic (HOL) combined with NeoDavidsonian Event Semantics.", "labels": [], "entities": []}, {"text": "Second, we demonstrate the capacity of HOL for textual entailment.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7689111828804016}]}, {"text": "We evaluate the system on a Japanese textual entailment dataset (), a dataset constructed in a similar way to the FraCaS dataset for English.", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.9705738127231598}]}, {"text": "Although it is usually thought that HOL is unfeasible for practical applications, the results show that the entire system is able to perform efficient logical inference on complex linguistic phenomena such as generalized quantifiers and intensional modifiers -phenomena that pose challenges to the standard first-order-logic-based approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system 3 on Japanese Semantics test suite (JSeM)), a Japanese dataset for textual entailment designed in a similar way to the FraCaS dataset for English.", "labels": [], "entities": [{"text": "Japanese Semantics test suite (JSeM))", "start_pos": 28, "end_pos": 65, "type": "DATASET", "confidence": 0.7769498143877301}, {"text": "textual entailment", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.6976603269577026}, {"text": "FraCaS dataset", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.9599681198596954}]}, {"text": "These datasets focus on the types of logical inferences that do not require world knowledge.", "labels": [], "entities": []}, {"text": "JSeM has Japanese translations of FraCaS problems and an extended set of problems focusing on Japanese syntax and semantics.", "labels": [], "entities": [{"text": "JSeM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9575600624084473}]}, {"text": "Each problem has one or more premises, followed by a hypothesis.", "labels": [], "entities": []}, {"text": "There are three types of answer: yes (entailment), no (contradiction), and unknown (neutral).", "labels": [], "entities": []}, {"text": "Each problem is annotated with the types of inference (logical entailment, presupposition, etc.) and of linguistic phenomena.", "labels": [], "entities": []}, {"text": "We evaluate the system on 523 problems in the dataset.", "labels": [], "entities": []}, {"text": "We focus on problems tagged with one of the five phenomena: generalized quantifier, plu-   ral, adjective, verb, and attitude.", "labels": [], "entities": []}, {"text": "We use problems whose inference type is logical entailment, excluding anaphora and presupposition.", "labels": [], "entities": []}, {"text": "We use Kuromoji 5 for morphological analysis.", "labels": [], "entities": [{"text": "Kuromoji 5", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.8327301442623138}, {"text": "morphological analysis", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9105104207992554}]}, {"text": "To focus on the evaluation of semantic parsing and inference, we use gold syntactic parses, which show an upper bound on the performance of the semantic component.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7212094664573669}]}, {"text": "Gold syntactic parses are manually selected from n-best outputs of the CCG parser.", "labels": [], "entities": []}, {"text": "For the higher-order inference system, we use the axioms presented in adapted with the necessary modification for our event semantics.", "labels": [], "entities": []}, {"text": "Given premises P 1 , ...", "labels": [], "entities": []}, {"text": ", P n and a hypothesis H, the system outputs yes (P 1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 P n \u2192 H is proved), no (P 1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 P n \u2192 \u00acH is proved), or unknown (neither is proved in a fixed proof-search space).", "labels": [], "entities": []}, {"text": "We set a 30 seconds timeout for each inference run; the system outputs unknown after it.", "labels": [], "entities": []}, {"text": "The current semantic lexicon has 36 templates and 113 lexical entries.", "labels": [], "entities": []}, {"text": "The system with gold syntactic parses achieved 86% accuracy on the total 523 problems, with high precision and reasonable speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9995594620704651}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9990348815917969}, {"text": "speed", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9816732406616211}]}, {"text": "The accuracy dropped to 70% when ablating HOL axioms (  on the corresponding sections of FraCaS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997047781944275}, {"text": "FraCaS", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.9762448072433472}]}, {"text": "possible, our system with gold parses outperforms it for all sections.", "labels": [], "entities": []}, {"text": "Out of the 523 problems, 417 are Japanese translations of the FraCaS problems.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.9567497968673706}]}, {"text": "shows a comparison between the performance of our system on this subset of the JSeM problems and the performance of the RTE system for English in  on the corresponding problems in the FraCaS dataset.", "labels": [], "entities": [{"text": "FraCaS dataset", "start_pos": 184, "end_pos": 198, "type": "DATASET", "confidence": 0.9911069869995117}]}, {"text": "used system parses of the English C&C parser.", "labels": [], "entities": [{"text": "English C&C parser", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.8826964378356934}]}, {"text": "The total accuracy of our system is comparable to that of . Most errors we found are due to syntactic parse errors caused by the CCG parser, where no correct syntactic parses were found in n-best responses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995049238204956}]}, {"text": "Comparison between gold parses and system parses shows that correct syntactic disambiguation improves performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy on each section of JSeM.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988269209861755}, {"text": "JSeM", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8492215871810913}]}, {"text": " Table 3: Accuracy, precision, recall, and average proof time.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992015957832336}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9996371269226074}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9996767044067383}, {"text": "average proof time", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.7826235989729563}]}]}