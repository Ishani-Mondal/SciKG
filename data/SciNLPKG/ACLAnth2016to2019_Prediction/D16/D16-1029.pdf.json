{"title": [{"text": "Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatically solving algebra word problems has raised considerable interest recently.", "labels": [], "entities": [{"text": "Automatically solving algebra word problems", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7778015375137329}]}, {"text": "Existing state-of-the-art approaches mainly rely on learning from human annotated equations.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that it is possible to efficiently mine algebra problems and their numerical solutions with little to no manual effort.", "labels": [], "entities": []}, {"text": "To leverage the mined dataset, we propose a novel structured-output learning algorithm that aims to learn from both explicit (e.g., equations) and implicit (e.g., solutions) supervision signals jointly.", "labels": [], "entities": []}, {"text": "Enabled by this new algorithm, our model gains 4.6% absolute improvement inaccuracy on the ALG-514 benchmark compared to the one without using implicit supervision.", "labels": [], "entities": [{"text": "ALG-514 benchmark", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.7942975759506226}]}, {"text": "The final model also outperforms the current state-of-the-art approach by 3%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Algebra word problems express mathematical relationships via narratives set in a real-world scenario, such as the one below: Maria is now four times as old as Kate.", "labels": [], "entities": []}, {"text": "Four years ago, Maria was six times as old as Kate.", "labels": [], "entities": []}, {"text": "The desired output is an equation system which expresses the mathematical relationship symbolically: m = 4 \u00d7 n and m \u2212 4 = 6 \u00d7 (n \u2212 4) where m and n represent the age of Maria and Kate, respectively.", "labels": [], "entities": []}, {"text": "The solution (i.e., m = 40, n = 10) can be found by a mathematical engine given the equation systems.", "labels": [], "entities": []}, {"text": "Building efficient automatic algebra word problem solvers have clear values for online education scenarios.", "labels": [], "entities": [{"text": "automatic algebra word problem solvers", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.6579893112182618}]}, {"text": "The challenge itself also provides a good test bed for evaluating an intelligent agent that understands natural languages, a direction advocated by artificial intelligence researchers).", "labels": [], "entities": []}, {"text": "One key challenge of solving algebra word problems is the lack of fully annotated data (i.e., the annotated equation system associated with each problem).", "labels": [], "entities": []}, {"text": "In contrast to annotating problems with binary or categorical labels, manually solving algebra word problems to provide correct equations is time consuming.", "labels": [], "entities": []}, {"text": "As a result, existing benchmark datasets are small, limiting the performance of supervised learning approaches.", "labels": [], "entities": []}, {"text": "However, thousands of algebra word problems have been posted and discussed in online forums, where the solutions can be easily mined, despite the fact that some of them could be incorrect.", "labels": [], "entities": []}, {"text": "It is thus interesting to ask whether a better algebra problem solver can be learned by leveraging these noisy and implicit supervision signals, namely the solutions.", "labels": [], "entities": [{"text": "algebra problem solver", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.6793256998062134}]}, {"text": "In this work, we address the technical difficulty of leveraging implicit supervision in learning an algebra word problem solver.", "labels": [], "entities": [{"text": "algebra word problem solver", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6341090723872185}]}, {"text": "We argue that the effective strategy is to learn from both explicit and implicit supervision signals jointly.", "labels": [], "entities": []}, {"text": "In particular, we design a novel online learning algorithm based on structured-output Perceptron.", "labels": [], "entities": []}, {"text": "By taking both kinds of training signals together as input, the algorithm iteratively improves the model, while at the same time it uses the intermediate model to find candidate equation systems for problems with only numerical solutions.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel learning algorithm (Section 3 and 4) that jointly learns from both explicit and implicit supervision.", "labels": [], "entities": []}, {"text": "Under different settings, the proposed algorithm outperforms the existing supervised and weakly supervised algorithms (Section 6) for algebra word problems.", "labels": [], "entities": []}, {"text": "\u2022 We mine the problem-solution pairs for algebra word problems from an online forum and show that we can effectively obtain the implicit supervision with little to no manual effort (Section 5).", "labels": [], "entities": []}, {"text": "1 \u2022 By leveraging both implicit and explicit supervision signals, our final solver outperforms the state-of-the-art system by 3% on ALG-514, a popular benchmark data set proposed by ).", "labels": [], "entities": [{"text": "solver", "start_pos": 76, "end_pos": 82, "type": "TASK", "confidence": 0.9628497362136841}]}], "datasetContent": [{"text": "In this section, we demonstrate the effectiveness of the proposed approach and empirically verify the design choices of the algorithm.", "labels": [], "entities": []}, {"text": "We show that our joint learning approach leverages mined implicit supervision effectively, improving system performance without using additional manual annotations (Section 6.1).", "labels": [], "entities": []}, {"text": "We also compare our approach to existing methods under different supervision settings (Section 6.2)., which covers more than 200 templates and contains 1,000 algebra word problems.", "labels": [], "entities": []}, {"text": "The data is split into training, development, and test sets, with 600/200/200 examples, respectively.", "labels": [], "entities": []}, {"text": "The SOL-2K dataset contains the word problemsolution pairs we mined from online forum (see Section 5).", "labels": [], "entities": [{"text": "SOL-2K dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8321005403995514}]}, {"text": "Unlike ALG-514 and DRAW-1K, there are no annotated equation systems in this dataset, and only the solutions are available.", "labels": [], "entities": [{"text": "DRAW-1K", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9543689489364624}]}, {"text": "Also, no preprocessing or cleaning is performed, so the problem descriptions might contain some irrelevant phrases such as \"please help me\".", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9553062319755554}]}, {"text": "Since all the datasets are generated from online forums, we carefully examined and removed problems from SOL-2K that are identical to problems in ALG-514 and DRAW-1K, to ensure fairness.", "labels": [], "entities": [{"text": "DRAW-1K", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.7890079021453857}]}, {"text": "We set the number of iterations to 15 and the learning rate \u03b7 to be 1.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.9435241421063741}]}, {"text": "For all experiments, we report solution accuracy (whether the solution was correct).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9558120369911194}]}, {"text": "Following, we ignore the ordering of answers when calculating the solution accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9893620610237122}]}, {"text": "We report the 5-fold cross validation accuracy on ALG-514 in order to have a fair comparison with previous work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9222984910011292}, {"text": "ALG-514", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8257858753204346}]}, {"text": "For DRAW-1K, we report the results on the test set.", "labels": [], "entities": [{"text": "DRAW-1K", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.889549195766449}]}, {"text": "In all the experiments, we only use the templates that appear in the corresponding explicit supervision.", "labels": [], "entities": []}, {"text": "Following, we do not model the alignments between noun phrases and variables.", "labels": [], "entities": []}, {"text": "We use a similar set of features introduced in (, except that our solver does not use rich NLP features from dependency parsing or coreference-resolution systems.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7592224776744843}]}, {"text": "We follow) and set the beam-size K to 10, unless stated otherwise.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The statistics of the data sets.", "labels": [], "entities": []}, {"text": " Table 3: The solution accuracies of different protocols on  ALG-514 and DRAW-1K.", "labels": [], "entities": [{"text": "DRAW-1K", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9052786231040955}]}]}