{"title": [{"text": "What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation", "labels": [], "entities": []}], "abstractContent": [{"text": "This article tackles anew challenging task in computational argumentation.", "labels": [], "entities": []}, {"text": "Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one.", "labels": [], "entities": []}, {"text": "We approach this task in a fully empirical manner by annotating 26k explanations written in natural language.", "labels": [], "entities": []}, {"text": "These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws.", "labels": [], "entities": []}, {"text": "We create anew crowd-sourced corpus containing 9,111 argument pairs, multi-labeled with 17 classes, which was cleaned and curated by employing several strict quality measures.", "labels": [], "entities": []}, {"text": "We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of flaws in less convincing arguments.", "labels": [], "entities": []}, {"text": "Our experiments with feature-rich SVM learners and Bidirec-tional LSTM neural networks with convolu-tion and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task.", "labels": [], "entities": [{"text": "Web argument convincingness", "start_pos": 179, "end_pos": 206, "type": "TASK", "confidence": 0.5679297347863516}]}, {"text": "We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community.", "labels": [], "entities": [{"text": "UKPConvArg2", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9856036901473999}]}], "introductionContent": [{"text": "People engage in argumentation in various contexts, both online and in the real life.", "labels": [], "entities": []}, {"text": "Existing definitions of argumentation do not solely focus on giving reasons and laying out a logical framework of premises and conclusions, but also highlight its social purpose which is to convince or to persuade.", "labels": [], "entities": []}, {"text": "Assessing the quality and strength of perceived arguments therefore plays an inherent role in argumentative discourse.", "labels": [], "entities": []}, {"text": "Despite strong theoretical foundations and plethora of normative theories, such as Walton's schemes and their critical questions), an ideal model of critical discussion in the pragma-dialectic view, or research into fallacies, assessing qualitative criteria of everyday argumentation represents a challenge for argumentation scholars and practitioners.", "labels": [], "entities": []}, {"text": "Addressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation.", "labels": [], "entities": []}, {"text": "Scoring strength of persuasive essays, exploring interaction in persuasive dialogues on Reddit (, or detecting convincing arguments are among recent attempts to tackle the quality of argumentation.", "labels": [], "entities": []}, {"text": "However, these approaches are holistic and do not necessarily explain why a given argument is strong or convincing.", "labels": [], "entities": []}, {"text": "We asked the following research questions.", "labels": [], "entities": []}, {"text": "First, can we assess what makes an argument convincing in a purely empirical fashion as opposite to theoretical normative approaches?", "labels": [], "entities": []}, {"text": "Second, to what extent can the problem be tackled by computational models?", "labels": [], "entities": []}, {"text": "To address these questions, we exploit our recently introduced UKPConvArg1 corpus.", "labels": [], "entities": [{"text": "UKPConvArg1 corpus", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.9933917820453644}]}, {"text": "This data set consists of 11,650 argument pairs -two arguments with the Prompt: Should physical education be mandatory in schools?", "labels": [], "entities": []}], "datasetContent": [{"text": "We propose two experiments, both performed in 16-fold cross-domain validation.", "labels": [], "entities": []}, {"text": "In each fold, argument pairs from 15 debates are used and the remaining one is used for testing.", "labels": [], "entities": []}, {"text": "In both experiments, it is assumed that the more convincing argument in a pair is known and we concatenate (using a particular delimiter) both arguments such that the more convincing argument comes first.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of annotated labels per argument pairs.", "labels": [], "entities": []}, {"text": " Table 2: Results of multi-label classification from Experiment", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.7909866869449615}]}, {"text": " Table 3: Gold data distribution for the second experiment. Ar-", "labels": [], "entities": [{"text": "Ar-", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9188797175884247}]}, {"text": " Table 4: Results for experiment 2. P = precision, R = recall, M-F1 = macro F1, C.I. = confidence interval at 0.95. Both BLSTM", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9972295165061951}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9748400449752808}, {"text": "macro F1", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.6482154726982117}, {"text": "confidence interval", "start_pos": 87, "end_pos": 106, "type": "METRIC", "confidence": 0.9668157696723938}, {"text": "BLSTM", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.6789969801902771}]}]}