{"title": [{"text": "Creating a Large Benchmark for Open Information Extraction", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.630175362030665}]}], "abstractContent": [{"text": "Open information extraction (Open IE) was presented as an unrestricted variant of traditional information extraction.", "labels": [], "entities": [{"text": "Open information extraction (Open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8036350224699292}, {"text": "information extraction", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7574312388896942}]}, {"text": "It has been gaining substantial attention, manifested by a large number of automatic Open IE extractors and downstream applications.", "labels": [], "entities": [{"text": "Open IE extractors", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.6615483562151591}]}, {"text": "In spite of this broad attention, the Open IE task definition has been lacking-there are no formal guidelines and no large scale gold standard annotation.", "labels": [], "entities": [{"text": "Open IE task", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.5473858614762624}]}, {"text": "Subsequently, the various implementations of Open IE resorted to small scale post-hoc evaluations, inhibiting an objective and reproducible cross-system comparison.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.5362640619277954}]}, {"text": "In this work, we develop a methodology that leverages the recent QA-SRL annotation to create a first independent and large scale Open IE annotation , 1 and use it to automatically compare the most prominent Open IE systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open Information Extraction (Open IE) was originally formulated as a function from a document to a set of tuples indicating a semantic relation between a predicate phrase and its arguments (.", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7886892557144165}]}, {"text": "further defined that an Open IE extractor should \"produce one triple for every relation stated explicitly in the text, but is not required to infer implicit facts\".", "labels": [], "entities": []}, {"text": "For example, given the sentence \"John managed to open the door\" an Open IE extractor should produce the tuple (John; managed to open; the door) but is not required to produce the extraction (John; opened; the door).", "labels": [], "entities": []}, {"text": "Following this initial presentation of the task, Open IE has gained substantial and consistent attention.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.7265483140945435}]}, {"text": "Many automatic extractors were created (e.g.,) and were put to use in various downstream applications.", "labels": [], "entities": []}, {"text": "In spite of this wide attention, Open IE's formal definition is lacking.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.5579916685819626}]}, {"text": "There are no clear guidelines as to what constitutes a valid proposition to be extracted, and subsequently there is no large scale benchmark annotation.", "labels": [], "entities": []}, {"text": "Open IE evaluations therefore usually consist of a post-hoc manual evaluation of a small output sample.", "labels": [], "entities": [{"text": "IE evaluations", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.8042083382606506}]}, {"text": "This evaluation practice lacks in several respects: (1) Most works provide a precision oriented metric, whereas recall is often not measured, (2) the numbers are not comparable across systems, as they use different guidelines and datasets, and (3) the experiments are hard to replicate.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9939160943031311}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9979604482650757}]}, {"text": "In this work, we aim to contribute to the standardization of Open IE evaluation by providing a large gold benchmark corpus.", "labels": [], "entities": [{"text": "standardization", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.9786020517349243}, {"text": "Open IE evaluation", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.6858186523119608}]}, {"text": "For that end, we first identify consensual guiding principles across prominent Open IE systems, resulting in a clearer formulation of the Open IE task.", "labels": [], "entities": [{"text": "Open IE task", "start_pos": 138, "end_pos": 150, "type": "TASK", "confidence": 0.48302628596623737}]}, {"text": "Following, we find that the recent formulation of QA-SRL () in fact subsumes these requirements for Open IE.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.5703787207603455}]}, {"text": "This enables us to automatically convert the annotations of QA-SRL to a high-quality Open IE corpus of more than 10K extractions, 13 times larger than the previous largest Open IE annotation.", "labels": [], "entities": []}, {"text": "Finally, we automatically evaluate the performance of various Open IE systems against our corpus, using a soft matching criterion.", "labels": [], "entities": []}, {"text": "This is the first time such a comparative evaluation is performed on a large scale gold corpus.", "labels": [], "entities": []}, {"text": "Future Open IE systems (and its applicative users) can use this large benchmark, along with the automatic evaluation measure, to easily compare their performance against previous baselines, alleviating the current need for ad-hoc evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we illustrate the utility of our new corpus by testing the performance of 6 prominent Open IE systems: OpenIE-4, ClausIE, OLLIE, PropS, Stanford, and ReVerb (see Section 2).", "labels": [], "entities": [{"text": "OpenIE-4", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9557672739028931}, {"text": "ClausIE", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.8765270709991455}, {"text": "OLLIE", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.911394476890564}, {"text": "PropS", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.9010403156280518}]}, {"text": "In order to evaluate these systems in terms of precision and recall, we need to match between their automated extractions and the benchmark extractions.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9994704127311707}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.999049961566925}]}, {"text": "To allow some flexibility (e.g., omissions of prepositions or auxiliaries), we follow ( and match an automated extraction with a gold proposition if both agree on the grammatical head of all of their elements (predicate and arguments).", "labels": [], "entities": []}, {"text": "We then analyze the recall and precision of Open IE systems on different confidence thresholds.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9994770884513855}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9989474415779114}]}, {"text": "Furthermore, we calculate the area under the PR curve for each of the different corpora) and the explicit yield per system.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first objective comparative evaluation of prominent Open-IE systems, over a large and independently created dataset.", "labels": [], "entities": []}, {"text": "This comparison gives rise to several observations; which can be useful for future research and for choosing a preferred system fora particular application setting, such as:: Area Under the PR Curve (AUC) measure for the evaluated systems.", "labels": [], "entities": [{"text": "Area Under the PR Curve (AUC) measure", "start_pos": 175, "end_pos": 212, "type": "METRIC", "confidence": 0.5807245042588975}]}, {"text": "1. Open IE-4 achieves best precision above 3% recall (\u011b 78.67) and best AUC score (54.02), 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.999170184135437}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9890128374099731}, {"text": "\u011b 78.67)", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9380892515182495}, {"text": "AUC score", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.980837881565094}]}, {"text": "ClausIE is best at recall (81.38), and 3.", "labels": [], "entities": [{"text": "ClausIE", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8514801859855652}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9998020529747009}]}, {"text": "Stanford Open IE assigns confidence of 1 to 94% of its extractions, explaining its low precision.", "labels": [], "entities": [{"text": "Stanford Open IE", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8664838671684265}, {"text": "confidence", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9692868590354919}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9978792667388916}]}], "tableCaptions": [{"text": " Table 1: The post-hoc evaluation metrics taken by the different systems described in Section 2. In contrast,  Stanford Open IE and PropS took an extrinsic evaluation approach.", "labels": [], "entities": [{"text": "Stanford Open IE", "start_pos": 111, "end_pos": 127, "type": "DATASET", "confidence": 0.7600656549135844}, {"text": "PropS", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.9261666536331177}]}, {"text": " Table 3: The yield of the different Open IE systems.", "labels": [], "entities": []}]}