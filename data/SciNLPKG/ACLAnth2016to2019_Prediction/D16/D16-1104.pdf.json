{"title": [{"text": "Are Word Embedding-based Features Useful for Sarcasm Detection?", "labels": [], "entities": [{"text": "Sarcasm Detection", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.9302883148193359}]}], "abstractContent": [{"text": "This paper makes a simple increment to state-of-the-art in sarcasm detection research.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.984096348285675}]}, {"text": "Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm.", "labels": [], "entities": []}, {"text": "We explore if prior work can be enhanced using semantic similarity/discordance between word embed-dings.", "labels": [], "entities": []}, {"text": "We augment word embedding-based features to four feature sets reported in the past.", "labels": [], "entities": []}, {"text": "We also experiment with four types of word embeddings.", "labels": [], "entities": []}, {"text": "We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.9187888205051422}]}, {"text": "For example, this augmentation results in an improvement in F-score of around 4% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used.", "labels": [], "entities": [{"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9992332458496094}]}, {"text": "Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9165818095207214}, {"text": "sarcasm detection", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.9107376635074615}]}], "introductionContent": [{"text": "Sarcasm is a form of verbal irony that is intended to express contempt or ridicule.", "labels": [], "entities": []}, {"text": "Linguistic studies show that the notion of context incongruity is at the heart of sarcasm (.", "labels": [], "entities": []}, {"text": "A popular trend in automatic sarcasm detection is semi-supervised extraction of patterns that capture the underlying context incongruity (.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7911112606525421}]}, {"text": "However, techniques to extract these patterns rely on sentiment-bearing words and may not capture nuanced forms of sarcasm.", "labels": [], "entities": []}, {"text": "Consider the sentence 'With a sense of humor like that, you could make a living as a garbageman anywhere in the country.", "labels": [], "entities": []}, {"text": "' The speaker makes a subtle, contemptuous remark about the All examples in this paper are actual instances from our dataset.", "labels": [], "entities": []}, {"text": "sense of humor of the listener.", "labels": [], "entities": []}, {"text": "However, absence of sentiment words makes the sarcasm in this sentence difficult to capture as features fora classifier.", "labels": [], "entities": []}, {"text": "In this paper, we explore use of word embeddings to capture context incongruity in the absence of sentiment words.", "labels": [], "entities": []}, {"text": "The intuition is that word vector-based similarity/discordance is indicative of semantic similarity which in turn is a handle for context incongruity.", "labels": [], "entities": []}, {"text": "In the case of the 'sense of humor' example above, the words 'sense of humor' and 'garbage man' are semantically dissimilar and their presence together in the sentence provides a clue to sarcasm.", "labels": [], "entities": []}, {"text": "Hence, our set of features based on word embeddings aim to capture such semantic similarity/discordance.", "labels": [], "entities": []}, {"text": "Since such semantic similarity is but one of the components of context incongruity and since existing feature sets rely on sentiment-based features to capture context incongruity, it is imperative that the two be combined for sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 226, "end_pos": 243, "type": "TASK", "confidence": 0.9061629772186279}]}, {"text": "Thus, our paper deals with the question: Can word embedding-based features when augmented to features reported in prior work improve the performance of sarcasm detection?", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.9096929728984833}]}, {"text": "To the best of our knowledge, this is the first attempt that uses word embedding-based features to detect sarcasm.", "labels": [], "entities": []}, {"text": "In this respect, the paper makes a simple increment to state-of-the-art but opens up anew direction in sarcasm detection research.", "labels": [], "entities": [{"text": "sarcasm detection research", "start_pos": 103, "end_pos": 129, "type": "TASK", "confidence": 0.9460906783739725}]}, {"text": "We establish our hypothesis in case of four past works and four types of word embeddings, to show that the benefit of using word embedding-based features holds across multiple feature sets and word embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We create a dataset consisting of quotes on GoodReads 4 . GoodReads describes itself as 'the world's largest site for readers and book recommendations.'", "labels": [], "entities": [{"text": "GoodReads 4", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9546608030796051}, {"text": "GoodReads", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9538384675979614}]}, {"text": "The website also allows users to post quotes from books.", "labels": [], "entities": []}, {"text": "These quotes are snippets from books labeled by the user with tags of their choice.", "labels": [], "entities": []}, {"text": "We download quotes with the tag 'sarcastic' as sarcastic quotes, and the ones with 'philosophy' as nonsarcastic quotes.", "labels": [], "entities": []}, {"text": "Our labels are based on these tags given by users.", "labels": [], "entities": []}, {"text": "We ensure that no quote has both these tags.", "labels": [], "entities": []}, {"text": "This results in a dataset of 3629 quotes out of which 759 are labeled as sarcastic.", "labels": [], "entities": []}, {"text": "This skew is similar to skews observed in datasets on which sarcasm detection experiments have been reported in the past ().", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7482225000858307}]}, {"text": "We report five-fold cross-validation results on the above dataset.", "labels": [], "entities": []}, {"text": "We use SV M perf by Joachims (2006) with c as 20, was 3, and loss function as F-score optimization.", "labels": [], "entities": [{"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9613404870033264}]}, {"text": "This allows SVM to be learned while optimizing the F-score.", "labels": [], "entities": [{"text": "SVM", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9208480715751648}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9826174378395081}]}, {"text": "As described above, we compare features given in prior work alongside the augmented versions.", "labels": [], "entities": []}, {"text": "This means that for each of the four papers, we experiment with four configurations: 1.", "labels": [], "entities": []}, {"text": "Features given in paper X  We experiment with four types of word embeddings: 1.", "labels": [], "entities": []}, {"text": "LSA: This approach was reported in.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7072036862373352}]}, {"text": "We use pre-trained word embeddings based on LSA 5 . The vocabulary size is 100,000.", "labels": [], "entities": [{"text": "LSA 5", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.8693846464157104}]}, {"text": "2. GloVe: We use pre-trained vectors avaiable from the GloVe project . The vocabulary size in this case is 2,195,904.", "labels": [], "entities": []}, {"text": "3. Dependency Weights: We use pre-trained vectors weighted using dependency distance, as given in).", "labels": [], "entities": []}, {"text": "To interact with the first three pre-trained vectors, we use scikit library (Pedregosa et al., 2011)., and compare them with augmented versions in. shows results for four kinds of word embeddings.", "labels": [], "entities": []}, {"text": "All entries in the tables are higher than the simple unigrams baseline, i.e., F-score for each of the four is higher than unigrams -highlighting that these are better features for sarcasm detection than simple unigrams.", "labels": [], "entities": [{"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9982470273971558}, {"text": "sarcasm detection", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.8739611804485321}]}, {"text": "Values in bold indicate the best F-score fora given prior work-embedding type combination.", "labels": [], "entities": [{"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9977871179580688}]}, {"text": "In case of for Word2Vec, the overall improvement in F-score is 4%.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.933630645275116}, {"text": "F-score", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.999437153339386}]}, {"text": "Precision increases by 8% while recall remains nearly unchanged.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9910750389099121}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9997469782829285}]}, {"text": "For features given in, there is a negligible degradation of 0.91% when word embedding-based features based on Word2Vec are used.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.9374526739120483}]}, {"text": "For for Word2Vec, we observe an improvement in F-score from 76.61% to 78.09%.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9440143704414368}, {"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9989759922027588}]}, {"text": "Precision remains nearly unchanged while recall increases.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9811878800392151}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9997946619987488}]}, {"text": "In case of  and Word2Vec, we observe a slight improvement of 0.20% when unweighted (S) features are used.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.9558059573173523}]}, {"text": "This shows that word embedding-based features are useful, across four past works for Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9259819984436035}]}, {"text": "also shows that the improvement holds across the four word embedding types as well.", "labels": [], "entities": []}, {"text": "The maximum improvement is observed in case of.", "labels": [], "entities": []}, {"text": "It is around 4% in case of LSA, 5% in case of GloVe, 6% in case of Dependency weight-based and 4% in case of Word2Vec.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.5568706393241882}, {"text": "Word2Vec", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.9694122076034546}]}, {"text": "These improvements are not directly comparable because the four embeddings have different vocabularies (since they are trained on different datasets) and vocabulary sizes, their results cannot be directly compared.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Similarity scores for all pairs of content words in 'A woman", "labels": [], "entities": [{"text": "Similarity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8649349808692932}]}, {"text": " Table 4: Average gain in F-Scores obtained by using intersection of the", "labels": [], "entities": [{"text": "Average gain", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9611171782016754}, {"text": "F-Scores", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9761552810668945}]}, {"text": " Table 5: Average gain in F-scores for the four types of word embed-", "labels": [], "entities": [{"text": "Average gain", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9271847307682037}, {"text": "F-scores", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9380940794944763}]}]}