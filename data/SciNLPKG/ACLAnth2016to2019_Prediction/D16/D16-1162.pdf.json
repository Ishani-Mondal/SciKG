{"title": [{"text": "Incorporating Discrete Translation Lexicons into Neural Machine Translation", "labels": [], "entities": [{"text": "Incorporating Discrete Translation Lexicons into Neural Machine Translation", "start_pos": 0, "end_pos": 75, "type": "TASK", "confidence": 0.7232009768486023}]}], "abstractContent": [{"text": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8245474398136139}]}, {"text": "We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words.", "labels": [], "entities": []}, {"text": "We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on.", "labels": [], "entities": []}, {"text": "We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation.", "labels": [], "entities": []}, {"text": "Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9981959462165833}, {"text": "NIST score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.8357791900634766}]}], "introductionContent": [{"text": "Neural machine translation, ) is a variant of statistical machine translation (SMT;), using neural networks.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.705161432425181}, {"text": "statistical machine translation (SMT", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.7284955143928528}]}, {"text": "NMT has recently gained popularity due to its ability to model the translation process end-to-end using a single probabilistic model, and for its state-of-the-art performance on several language pairs.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8232195377349854}]}, {"text": "One feature of NMT systems is that they treat each word in the vocabulary as a vector of Tools to replicate our experiments can be found at http://isw3.naist.jp/~philip-a/emnlp2016/index.html", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe experiments we use to evaluate our proposed methods.", "labels": [], "entities": []}, {"text": "To test whether the proposed method is useful on larger data sets, we also performed follow-up experiments on the larger Japanese-English ASPEC dataset () that consist of 2 million training examples, 63 million tokens, and 81,000 vocabulary size.", "labels": [], "entities": [{"text": "Japanese-English ASPEC dataset", "start_pos": 121, "end_pos": 151, "type": "DATASET", "confidence": 0.6248525778452555}]}, {"text": "We gained an improvement in BLEU score from 20.82 using the attn baseline to 22.66 using the auto-bias proposed method.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9730774164199829}]}, {"text": "This experiment shows that our method scales to larger datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracies for the baseline attentional NMT (attn) and the proposed bias-based method using  the automatic (auto-bias) or hybrid (hyb-bias) dictionaries. Bold indicates a gain over the attn  baseline,  \u2020 indicates a significant increase at p < 0.05, and  *  indicates p < 0.10. Traditional phrase-based  (pbmt) and hierarchical phrase based (hiero) systems are shown for reference.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9857161641120911}]}, {"text": " Table 4: A comparison of the bias and linear  lexicon integration methods on the automatic, man- ual, and hybrid lexicons. The first line without lexi- con is the traditional attentional NMT.", "labels": [], "entities": []}]}