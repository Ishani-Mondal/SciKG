{"title": [{"text": "LAMB: A Good Shepherd of Morphologically Rich Languages", "labels": [], "entities": [{"text": "LAMB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8999900221824646}]}], "abstractContent": [{"text": "This paper introduces STEM and LAMB, em-beddings trained for stems and lemmata instead of for surface forms.", "labels": [], "entities": [{"text": "STEM", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.6830811500549316}, {"text": "LAMB", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.8064113855361938}]}, {"text": "For morphologically rich languages, they perform significantly better than standard embeddings on word similarity and polarity evaluations.", "labels": [], "entities": []}, {"text": "On anew WordNet-based evaluation, STEM and LAMB are up to 50% better than standard em-beddings.", "labels": [], "entities": [{"text": "STEM", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.4679182469844818}, {"text": "LAMB", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9350805878639221}]}, {"text": "We show that both embeddings have high quality even for small dimension-ality and training corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite their power and prevalence, embeddings, i.e., (low-dimensional) word representations in vector space, have serious practical problems.", "labels": [], "entities": []}, {"text": "First, large text corpora are necessary to train high-quality embeddings.", "labels": [], "entities": []}, {"text": "Such corpora are not available for underresourced languages.", "labels": [], "entities": []}, {"text": "Second, morphologically rich languages (MRLs) area challenge for standard embedding models because many inflectional forms are rare or absent even in a large corpus.", "labels": [], "entities": []}, {"text": "For example, a Spanish verb has more than 50 forms, many of which are rarely used.", "labels": [], "entities": []}, {"text": "This leads to missing or low quality embeddings for such inflectional forms, even for otherwise frequent verbs, i.e., sparsity is a problem.", "labels": [], "entities": []}, {"text": "Therefore, we propose to compute normalized embeddings instead of embeddings for surface/inflectional forms (referred to as forms throughout the rest of the paper): STem EMbeddings (STEM) for word stems and LemmA eMBeddings (LAMB) for lemmata.", "labels": [], "entities": []}, {"text": "Stemming is a heuristic approach to reducing form-related sparsity issues.", "labels": [], "entities": []}, {"text": "Based on simple rules, forms are converted into their stem.", "labels": [], "entities": []}, {"text": "1 However, often the forms of one word are converted into several different stems.", "labels": [], "entities": []}, {"text": "For example, present indicative forms of the German verb \"brechen\" (to break) are mapped to four different stems (\"brech\", \"brich\", \"bricht\", \"brecht\").", "labels": [], "entities": []}, {"text": "A more principled solution is lemmatization.", "labels": [], "entities": []}, {"text": "Lemmatization unites many individual forms, many of which are rare, in one equivalence class, represented by a single lemma.", "labels": [], "entities": []}, {"text": "Stems and equivalence classes are more frequent than each individual form.", "labels": [], "entities": []}, {"text": "As we will show, this successfully addresses the sparsity issue.", "labels": [], "entities": []}, {"text": "Both methods can learn high-quality semantic representations for rare forms and thus are most beneficial for MRLs as we show below.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 109, "end_pos": 113, "type": "TASK", "confidence": 0.9664245843887329}]}, {"text": "Moreover, less training data is required to train lemma embeddings of the same quality as form embeddings.", "labels": [], "entities": []}, {"text": "Alternatively, we can train lemma embeddings that have the same quality but fewer dimensions than form embeddings, resulting in more efficient applications.", "labels": [], "entities": []}, {"text": "If an application such as parsing requires inflectional information, then stem and lemma embeddings may not be a good choice since they do not contain such information.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9626204967498779}]}, {"text": "However, NLP applications such as similarity benchmarks (e.g.,) and (as we show below) polarity classification are semantic and are largely independent of inflectional morphology.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.7352478057146072}]}, {"text": "Our contributions are the following.", "labels": [], "entities": []}, {"text": "(i) We introduce the normalized embeddings STEM and LAMB and show their usefulness on different tasks for five languages.", "labels": [], "entities": [{"text": "LAMB", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.6698240637779236}]}, {"text": "This paper is the first study that comprehensively compares stem/lemma-based with formbased embeddings for MRLs.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 107, "end_pos": 111, "type": "TASK", "confidence": 0.863030195236206}]}, {"text": "(ii) We show the advantage of normalization on word similarity benchmarks.", "labels": [], "entities": []}, {"text": "Normalized embeddings yield better performance for MRL languages on most datasets (6 out of 7 datasets for German and 2 out of 2 datasets for Spanish).", "labels": [], "entities": [{"text": "MRL languages", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8860900104045868}]}, {"text": "(iii) We propose anew intrinsic relatedness evaluation based on WordNet graphs and publish datasets for five languages.", "labels": [], "entities": [{"text": "WordNet graphs", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.948276698589325}]}, {"text": "On this new evaluation, LAMB outperforms form-based baselines by a big margin.", "labels": [], "entities": []}, {"text": "(iv) STEM and LAMB outperform baselines on polarity classification for Czech and English.", "labels": [], "entities": [{"text": "STEM", "start_pos": 5, "end_pos": 9, "type": "TASK", "confidence": 0.5743905305862427}, {"text": "polarity classification", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6821285635232925}]}, {"text": "(v) We show that LAMB embeddings are efficient in that they are high-quality for small training corpora and small dimensionalities.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experiment evaluates how well STEM/LAMB embeddings predict human word similarity judgments.", "labels": [], "entities": [{"text": "STEM/LAMB embeddings", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7902261912822723}, {"text": "human word similarity judgments", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6693479642271996}]}, {"text": "Given a pair of words (m, n) with a human-generated similarity value and a set of embeddings E we compute their similarity as cosine similarity.", "labels": [], "entities": []}, {"text": "For form embeddings E F , we directly use the embeddings of the word pairs' forms (E Fm and E F n ) and compute their similarity.", "labels": [], "entities": []}, {"text": "For STEM we use E S stem(w) , where stem(w) is the stem of w.", "labels": [], "entities": [{"text": "STEM", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9636432528495789}]}, {"text": "For LAMB we use E L lemma(w) , where lemma(w) is the lemma of w; we randomly select one of w's lemmata if there are several.", "labels": [], "entities": [{"text": "LAMB", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.762420117855072}]}, {"text": "We conduct experiments on English (en), German (de) and Spanish (es).", "labels": [], "entities": []}, {"text": "For good performance, high-quality embeddings trained on large corpora are required.", "labels": [], "entities": []}, {"text": "Hence, the training corpora for German and Spanish are web corpora taken from COW14.", "labels": [], "entities": [{"text": "COW14", "start_pos": 78, "end_pos": 83, "type": "DATASET", "confidence": 0.9854298830032349}]}, {"text": "Preprocessing includes removal of XML, conversion of HTML characters, lowercasing, stemming using SNOWBALL and lemmatization using LEM-MING.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9364240169525146}]}, {"text": "We use the entire Spanish corpus (3.7 billion tokens), but cut the German corpus to approximately 8 billion tokens to be comparable to.", "labels": [], "entities": [{"text": "Spanish corpus", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.7829230427742004}, {"text": "German corpus", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9143833816051483}]}, {"text": "We train CBOW models () for forms, stems and lemmata using WORD2VEC 4 with the following settings: 400 dimensions, symmetric context of size 2 (no dynamic window), 1 training iteration, negative sampling with 15 samples, a learning rate of 0.025, min-imum count of words of 50, and a sampling parameter of 10 \u22125 . CBOW is chosen, because it trains much faster than skip-gram, which is beneficial on these large corpora.", "labels": [], "entities": []}, {"text": "Since the morphology of English is rather simple we do not expect STEM and LAMB to reach or even surpass highly optimized systems on any word similarity dataset (e.g.,).", "labels": [], "entities": []}, {"text": "Therefore, for practical reasons we use a smaller training corpus, namely the preprocessed and tokenized Wikipedia dataset of . Embeddings are trained with the same settings (using 5 iterations instead of only 1, due to the smaller size of the corpus: 1.8 billion tokens).", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.9130222797393799}]}, {"text": "We also report the Spearman correlation on the vocabulary intersection, i.e., only those word pairs that are covered by the vocabularies of all models.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 19, "end_pos": 39, "type": "METRIC", "confidence": 0.7449265122413635}]}, {"text": "Although English has a simple morphology, LAMB improves over form performance on MEN and SL.", "labels": [], "entities": [{"text": "LAMB", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.7597087025642395}]}, {"text": "A tie is achieved on RW.", "labels": [], "entities": [{"text": "tie", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9530236124992371}, {"text": "RW", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.8008791208267212}]}, {"text": "These are the three largest English datasets, giving a more reliable result.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.8431959748268127}]}, {"text": "Both models perform comparably on WS.", "labels": [], "entities": []}, {"text": "Here, STEM is ahead by 1 point.", "labels": [], "entities": [{"text": "STEM", "start_pos": 6, "end_pos": 10, "type": "TASK", "confidence": 0.7247334718704224}]}, {"text": "Forms are better on the small datasets MC and RG, where a single word pair can have a large influence on the result.", "labels": [], "entities": []}, {"text": "Additionally, these are datasets with high frequency forms, where form embeddings can be well trained.", "labels": [], "entities": []}, {"text": "Because of the simple morphology of English, STEM/LAMB do not outperform forms or only by a small margin and thus they cannot compete with highly optimized state-of-the-art systems.", "labels": [], "entities": [{"text": "STEM/LAMB", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.8364946842193604}]}, {"text": "On German, both STEM and LAMB perform better on all datasets except WS.", "labels": [], "entities": [{"text": "German", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.9550262689590454}, {"text": "LAMB", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8784021735191345}]}, {"text": "We set the new stateof-the-art of 0.79 on Gur350 (compared to 0.77 (Szarvas et al., 2011)) and 0.39 on ZG (compared to 0.25 (Botha and Blunsom, 2014)); 0.83 on Gur65 (compared to 0.79) is the best performance of a system that does not need additional knowledge bases (cf.,).", "labels": [], "entities": [{"text": "Gur350", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.8376988172531128}, {"text": "Gur65", "start_pos": 160, "end_pos": 165, "type": "DATASET", "confidence": 0.8972532153129578}]}, {"text": "LAMB's results on Spanish are equally good.", "labels": [], "entities": [{"text": "LAMB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8838712573051453}]}, {"text": "0.82 on MC and 0.58 on WS are again the best per-formances of a system not requiring an additional knowledge base (cf.).", "labels": [], "entities": [{"text": "MC", "start_pos": 8, "end_pos": 10, "type": "DATASET", "confidence": 0.799568772315979}, {"text": "WS", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.7772100567817688}]}, {"text": "The best performance before was 0.64 for MC and 0.50 for WS).", "labels": [], "entities": [{"text": "WS", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.4020063281059265}]}, {"text": "STEM cannot improve over form embeddings, showing the difficulty of Spanish morphology.", "labels": [], "entities": [{"text": "STEM", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8075201511383057}]}], "tableCaptions": [{"text": " Table 1: Word similarity results. The left part shows dataset information. The right part shows Spearman correlation (\u03c1) for the", "labels": [], "entities": [{"text": "Spearman correlation (\u03c1)", "start_pos": 97, "end_pos": 121, "type": "METRIC", "confidence": 0.9496399402618408}]}, {"text": " Table 2: Number of lemmata in WordNet datasets", "labels": [], "entities": [{"text": "WordNet datasets", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9565044045448303}]}, {"text": " Table 3: Word relation results. MRR per language and POS type for all models. unfiltered is the unfiltered nearest neighbor search", "labels": [], "entities": [{"text": "MRR", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9973971843719482}]}, {"text": " Table 4: Polarity classification datasets", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7648120224475861}]}, {"text": " Table 5: Polarity classification results. Bold is best per lan-", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.761474221944809}]}]}