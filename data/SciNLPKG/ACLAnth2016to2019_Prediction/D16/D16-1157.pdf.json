{"title": [{"text": "CHARAGRAM: Embedding Words and Sentences via Character n-grams", "labels": [], "entities": []}], "abstractContent": [{"text": "We present CHARAGRAM embeddings, a simple approach for learning character-based compositional models to embed textual sequences.", "labels": [], "entities": []}, {"text": "A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding.", "labels": [], "entities": []}, {"text": "We use three tasks for evaluation: word similarity , sentence similarity, and part-of-speech tagging.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.5983652472496033}, {"text": "part-of-speech tagging", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7388512492179871}]}, {"text": "We demonstrate that CHARAGRAM embeddings outperform more complex archi-tectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing textual sequences such as words and sentences is a fundamental component of natural language understanding systems.", "labels": [], "entities": [{"text": "Representing textual sequences such as words and sentences", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7905638217926025}, {"text": "natural language understanding", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.6460417111714681}]}, {"text": "Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging to functions with rich recursive structure.", "labels": [], "entities": []}, {"text": "Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (.", "labels": [], "entities": []}, {"text": "Some prior work has found benefit from using character-based compositional models that encode arbitrary character sequences into vectors.", "labels": [], "entities": []}, {"text": "Examples include recurrent neural networks (RNNs) and convolutional neural networks (CNNs) on character sequences, showing improvements for several NLP tasks (.", "labels": [], "entities": []}, {"text": "By sharing subword information across words, character models have the potential to better represent rare words and morphological variants.", "labels": [], "entities": []}, {"text": "Our approach, CHARAGRAM, uses a much simpler functional architecture.", "labels": [], "entities": []}, {"text": "We represent a character sequence by a vector containing counts of character n-grams, inspired by.", "labels": [], "entities": []}, {"text": "This vector is embedded into a low-dimensional space using a single nonlinear transformation.", "labels": [], "entities": []}, {"text": "This can be interpreted as learning embeddings of character n-grams, which are learned so as to produce effective sequence embeddings when a summation is performed over the character n-grams in the sequence.", "labels": [], "entities": []}, {"text": "We consider three evaluations: word similarity, sentence similarity, and part-of-speech tagging.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6075559854507446}, {"text": "part-of-speech tagging", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.7387595176696777}]}, {"text": "On multiple word similarity datasets, CHARAGRAM outperforms RNNs and CNNs, achieving state-ofthe-art performance on SimLex-999 (.", "labels": [], "entities": []}, {"text": "When evaluated on a large suite of sentencelevel semantic textual similarity tasks, CHARA-GRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM-PHRASE embeddings of.", "labels": [], "entities": [{"text": "sentencelevel semantic textual similarity tasks", "start_pos": 35, "end_pos": 82, "type": "TASK", "confidence": 0.64075465798378}]}, {"text": "We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of.", "labels": [], "entities": [{"text": "English part-of-speech (POS) tagging", "start_pos": 17, "end_pos": 53, "type": "TASK", "confidence": 0.5522208511829376}]}, {"text": "The three architectures reach similar performance, though CHARAGRAM converges fastest to high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9970535039901733}]}, {"text": "We perform extensive analysis of our CHARA-GRAM embeddings.", "labels": [], "entities": []}, {"text": "We find large gains in performance on rare words, showing the empirical benefit of subword modeling.", "labels": [], "entities": []}, {"text": "We also compare performance across different character n-gram vocabulary sizes, finding that the semantic tasks benefit far more from large vocabularies than the syntactic task.", "labels": [], "entities": []}, {"text": "However, even for challenging semantic similarity tasks, we still see strong performance with only a few thousand character n-grams.", "labels": [], "entities": []}, {"text": "Nearest neighbors show that CHARAGRAM embeddings simultaneously address differences due to spelling variation, morphology, and word choice.", "labels": [], "entities": []}, {"text": "Inspection of embeddings of particular character ngrams reveals etymological links; e.g., die is close to mort.", "labels": [], "entities": []}, {"text": "We release our resources to the community in the hope that CHARAGRAM can provide a strong baseline for subword-aware text representation.", "labels": [], "entities": [{"text": "subword-aware text representation", "start_pos": 103, "end_pos": 136, "type": "TASK", "confidence": 0.6199662486712137}]}], "datasetContent": [{"text": "We perform three sets of experiments.", "labels": [], "entities": []}, {"text": "The goal of the first two (Section 4.1) is to produce embeddings for textual sequences such that the embeddings for paraphrases have high cosine similarity.", "labels": [], "entities": []}, {"text": "Our third evaluation (Section 4.2) is a classification task, and follows the setup of the English part-of-speech tagging experiment from Ling et al.", "labels": [], "entities": [{"text": "English part-of-speech tagging", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.5538589557011923}]}, {"text": "For word similarity, we focus on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) () and SimLex-999 (SL999) ().", "labels": [], "entities": [{"text": "word similarity", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7760038673877716}, {"text": "WordSim-353 (WS353)", "start_pos": 127, "end_pos": 146, "type": "DATASET", "confidence": 0.8859124928712845}]}, {"text": "We also evaluate our best model on the Stanford Rare Word Similarity Dataset (.", "labels": [], "entities": [{"text": "Stanford Rare Word Similarity Dataset", "start_pos": 39, "end_pos": 76, "type": "DATASET", "confidence": 0.8096489429473877}]}, {"text": "For sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7524529993534088}, {"text": "SemEval semantic textual similarity (STS) task from 2012 to 2015", "start_pos": 123, "end_pos": 187, "type": "TASK", "confidence": 0.8517149239778519}]}, {"text": "We also evaluate on the SemEval 2015 Twitter task ( ) and the SemEval 2014 SICK Semantic Relatedness task).", "labels": [], "entities": [{"text": "SemEval 2015 Twitter task", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.5323561877012253}, {"text": "SemEval 2014 SICK Semantic Relatedness task", "start_pos": 62, "end_pos": 105, "type": "TASK", "confidence": 0.6373327573140463}]}, {"text": "Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent.", "labels": [], "entities": [{"text": "STS tasks", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.8731101155281067}]}, {"text": "Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others.", "labels": [], "entities": [{"text": "machine translation outputs, web forums, news headlines, image and video captions", "start_pos": 112, "end_pos": 193, "type": "TASK", "confidence": 0.6313141648258481}]}, {"text": "Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks.", "labels": [], "entities": []}, {"text": "Further details are provided in the official task descriptions ().", "labels": [], "entities": []}, {"text": "Training and Tuning For hyperparameter tuning, we used one epoch on the lexical section of PPDB XXL, which consists of 770,007 word pairs.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7797398567199707}, {"text": "PPDB XXL", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.8076032996177673}]}, {"text": "We  used either WS353 or SL999 for model selection (reported below).", "labels": [], "entities": [{"text": "WS353", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.9161916971206665}]}, {"text": "We then took the selected hyperparameters and trained for 50 epochs to ensure that all models had a chance to converge.", "labels": [], "entities": []}, {"text": "Full details of our tuning procedure are provided in the supplementary material.", "labels": [], "entities": []}, {"text": "In short, we tuned all models thoroughly, tuning the activation functions for CHARAGRAM and charCNN, as well as the regularization strength, mini-batch size, and sampling type for all models.", "labels": [], "entities": [{"text": "CHARAGRAM", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.8591269850730896}]}, {"text": "For charCNN, we experimented with two filter sets: one uses 175 filters for each ngram size \u2208 {2, 3, 4}, and the other uses the set of filters from, consisting of 25 filters of size 1, 50 of size 2, 75 of size 3, 100 of size 4, 125 of size 5, and 150 of size 6.", "labels": [], "entities": []}, {"text": "We also experimented with using dropout () on the inputs to the final layer of charCNN in place of L 2 regularization, as well as removing the last feedforward layer.", "labels": [], "entities": []}, {"text": "Neither variation significantly improved performance on our suite of tasks for word or sentence similarity.", "labels": [], "entities": [{"text": "word or sentence similarity", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6053862571716309}]}, {"text": "However, using more filters does improve performance, apparently linearly with the square of the number of filters.", "labels": [], "entities": []}, {"text": "Training and Tuning We did initial training of our models using one pass through PPDB XL, which consists of 3,033,753 unique phrase pairs.", "labels": [], "entities": []}, {"text": "Following, we use the annotated phrase pairs developed by as our validation set, using Spearman's \u03c1 to rank the models.", "labels": [], "entities": []}, {"text": "We then take the highest performing models and train on the 9,123,575 unique phrase pairs in the phrasal section of PPDB XXL for 10 epochs.", "labels": [], "entities": [{"text": "PPDB XXL", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.933773934841156}]}, {"text": "For all experiments, we fix the mini-batch size to 100, the margin \u03b4 to 0.4, and use MAX sampling (see supplementary material).", "labels": [], "entities": [{"text": "margin \u03b4", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9367092251777649}, {"text": "MAX sampling", "start_pos": 85, "end_pos": 97, "type": "METRIC", "confidence": 0.8251223862171173}]}, {"text": "For CHARA-GRAM, V contains all 122,610 character n-grams (n \u2208 {2, 3, 4}) in the PPDB XXL phrasal section.", "labels": [], "entities": []}, {"text": "Other tuning settings are the same as Section 4.1.3.", "labels": [], "entities": []}, {"text": "For another baseline, we train the PARAGRAM-PHRASE model of, tuning its regularization strength over {10 \u22125 , 10 \u22126 , 10 \u22127 , 10 \u22128 }.", "labels": [], "entities": [{"text": "PARAGRAM-PHRASE", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.943552553653717}]}, {"text": "The PARAGRAM-PHRASE model simply uses word averaging as its composition function, but outperforms many more complex models.", "labels": [], "entities": [{"text": "PARAGRAM-PHRASE", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.838381826877594}, {"text": "word averaging", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7576858401298523}]}, {"text": "In this section, we refer to our model as CHARAGRAM-PHRASE because the input is a character sequence containing multiple words rather than only a single word as in Section 4.1.3.", "labels": [], "entities": []}, {"text": "Since the vocabulary V is defined by the training data sequences, the CHARAGRAM-PHRASE model includes character n-grams that span multiple words, permitting it to capture some aspects of word order and word co-occurrence, which the PARAGRAM-PHRASE model is unable to do.", "labels": [], "entities": []}, {"text": "We encountered difficulties training the char-LSTM and charCNN models for this task.", "labels": [], "entities": []}, {"text": "We tried several strategies to improve their chance at convergence, including clipping gradients, increasing training data, and experimenting with different optimizers and learning rates.", "labels": [], "entities": []}, {"text": "We found success by using the original (confidence-based) ordering of the PPDB phrase pairs for the initial epoch of learning, then shuffling them for subsequent epochs.", "labels": [], "entities": []}, {"text": "This is similar to curriculum learning ().", "labels": [], "entities": []}, {"text": "The higher-confidence phrase pairs tend to be shorter and have many overlapping words, possibly making them easier to learn from.", "labels": [], "entities": []}, {"text": "Results An abbreviated version of the sentence similarity results is shown in; the supplementary material contains the full results.", "labels": [], "entities": []}, {"text": "For comparison, we report performance for the median (50%), third quartile (75%), and top-performing (Max) systems from the shared tasks.", "labels": [], "entities": []}, {"text": "We observe strong performance for the CHARAGRAM-PHRASE model.", "labels": [], "entities": []}, {"text": "It always does better than the char-CNN and charLSTM models, and outperforms the PARAGRAM-PHRASE model on 15 of the 22 tasks.", "labels": [], "entities": [{"text": "PARAGRAM-PHRASE", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.9187203645706177}]}, {"text": "Furthermore, CHARAGRAM-PHRASE matches or exceeds the top-performing task-tuned systems on 5 tasks, and is within 0.003 on 2 more.", "labels": [], "entities": [{"text": "CHARAGRAM-PHRASE", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.8000397682189941}]}, {"text": "The charLSTM and charCNN models are significantly worse, with the charCNN being the better of the two and beating PARAGRAM-PHRASE on 4 of the tasks.", "labels": [], "entities": [{"text": "PARAGRAM-PHRASE", "start_pos": 114, "end_pos": 129, "type": "METRIC", "confidence": 0.7040051817893982}]}, {"text": "We emphasize that there are many other models that could be compared to, such as an LSTM over word embeddings.", "labels": [], "entities": []}, {"text": "This and many other models were explored by   function, was among their best-performing models.", "labels": [], "entities": []}, {"text": "We used this model in our experiments as a stronglyperforming representative of their results.", "labels": [], "entities": []}, {"text": "Lastly, we note other recent work that considers a similar transfer learning setting.", "labels": [], "entities": []}, {"text": "The FastSent model () uses the 2014 STS task in its evaluation and reports an average Pearson's r of 61.3.", "labels": [], "entities": [{"text": "2014 STS task", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.8096837004025778}, {"text": "Pearson's r", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9775814215342203}]}, {"text": "On the same data, the C-PHRASE model () has an average Pearson's r of 65.7.", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9822230140368143}]}, {"text": "Both results are lower than the 74.7 achieved by CHARAGRAM-PHRASE on this dataset.", "labels": [], "entities": []}, {"text": "We now consider part-of-speech (POS) tagging, since it has been used as a testbed for evaluating architectures for character-level word representations.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6129182934761047}]}, {"text": "It also differs from semantic similarity, allowing us to evaluate our architectures on a syntactic task.", "labels": [], "entities": []}, {"text": "We replicate the POS tagging experimental setup of.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.7374271750450134}]}, {"text": "Their model uses a bidirectional LSTM over character embeddings to represent words.", "labels": [], "entities": []}, {"text": "They then use the resulting word representations in another bidirectional LSTM that predicts the tag for each word.", "labels": [], "entities": []}, {"text": "We replace their character bidirectional LSTM with our three architectures: char-CNN, charLSTM, and CHARAGRAM.", "labels": [], "entities": []}, {"text": "We use the Wall Street Journal portion of the Penn Treebank, using Sections 1-18 for training, 19-21 for tuning, and 22-24 for testing.", "labels": [], "entities": [{"text": "Wall Street Journal portion of the Penn Treebank", "start_pos": 11, "end_pos": 59, "type": "DATASET", "confidence": 0.9554543793201447}]}, {"text": "We set the dimensionality of the character embeddings to 50 and that of the (induced) word representations to 150.", "labels": [], "entities": []}, {"text": "For optimization, we use stochastic gradient descent with a mini-batch size of 100 sentences.", "labels": [], "entities": []}, {"text": "The learning rate and momentum are set to 0.2 and 0.95 respectively.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9126415848731995}, {"text": "momentum", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9368451237678528}]}, {"text": "We train the models for 50 epochs, again to ensure that all models have an opportunity to converge.", "labels": [], "entities": []}, {"text": "The other settings for our models are mostly the same as for the word and sentence experiments (Section 4.1).", "labels": [], "entities": []}, {"text": "We again use character n-grams with n \u2208 {2, 3, 4}, tuning over whether to include all 54,893 in the training data or only those that occur more than once.", "labels": [], "entities": []}, {"text": "However, there are two minor differences from the previous sections.", "labels": [], "entities": []}, {"text": "First, we add a single binary feature to indicate if the token contains a capital letter.", "labels": [], "entities": []}, {"text": "Second, our tuning considers rectified linear units as the activation function for the CHARAGRAM and charCNN architectures.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Performance is similar across models.", "labels": [], "entities": []}, {"text": "We found that adding a second fully-connected 150 dimensional layer to the CHARAGRAM model improved results slightly.", "labels": [], "entities": []}, {"text": "The default setting for our CHARAGRAM and CHARAGRAM-PHRASE models is to use all character bigram, trigrams, and 4-grams that occur in the training data at least C times, tuning C over the set {1, 2}.", "labels": [], "entities": []}, {"text": "This results in a large number of parameters, which could be seen as an unfair advantage over the comparatively smaller charCNN and char-LSTM similarity models, which have up to 881,025: Results of using different numbers and different combinations of character n-grams. and 763,200 parameters respectively (including 134 character embeddings for each).", "labels": [], "entities": []}, {"text": "However, fora given sequence, very few parameters in the CHARAGRAM model are actually used.", "labels": [], "entities": []}, {"text": "For charCNN and charLSTM, by contrast, all parameters are used except character embeddings for characters not present in the sequence.", "labels": [], "entities": []}, {"text": "For a 100-character sequence, the 300-dimensional CHARA-GRAM model uses approximately 90,000 parameters, about one-tenth of those used by charCNN and charLSTM for the same sequence.", "labels": [], "entities": []}, {"text": "We performed a series of experiments to investigate how the CHARAGRAM and CHARAGRAM-PHRASE models perform with different numbers and lengths of character n-grams.", "labels": [], "entities": []}, {"text": "For a given k, we took the k most frequent character n-grams for each value of n in use.", "labels": [], "entities": []}, {"text": "We experimented with k values in {100, 1000, 50000}.", "labels": [], "entities": []}, {"text": "If there were fewer thank unique character n-grams fora given n, we used all of them.", "labels": [], "entities": []}, {"text": "For these experiments, we did very little tuning, setting the regularization strength to 0 and only tuning the activation function.", "labels": [], "entities": []}, {"text": "For word similarity, we report performance on SL999 after 5 training epochs on the lexical section of PPDB XXL.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7833264172077179}, {"text": "PPDB XXL", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.8560431897640228}]}, {"text": "For sentence similarity, we report the average Pearson's rover all 22 datasets after 5 training epochs on the phrasal section of PPDB XL.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7365407347679138}, {"text": "Pearson's rover", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9286346236864725}, {"text": "PPDB XL", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.9133076667785645}]}, {"text": "For tagging, we report accuracy on the tuning set after 50 training epochs.", "labels": [], "entities": [{"text": "tagging", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9784777164459229}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9996359348297119}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "When using extremely small models with only 100 n-grams of each order, we still see relatively strong performance on tagging.", "labels": [], "entities": []}, {"text": "However, the similarity tasks require far more n-grams to yield strong performance.", "labels": [], "entities": []}, {"text": "Using 1000 n-grams clearly outperforms 100, and 50,000 n-grams performs best.", "labels": [], "entities": []}, {"text": "We also found that models converged more quickly on tagging than on the similarity tasks.", "labels": [], "entities": []}, {"text": "We suspect this is due to differences in task complexity.", "labels": [], "entities": []}, {"text": "In tagging, the model does not need to learn all facets of each word's semantics; it only needs to map a word to its syntactic categories.", "labels": [], "entities": []}, {"text": "Therefore, simple surface-level features like affixes can help tremendously.", "labels": [], "entities": []}, {"text": "However, learning representations that reflect detailed differences in word meaning is a more fine-grained endeavor and this is presumably why larger models are needed and convergence is slower.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word similarity results (Spearman's \u03c1 \u00d7 100). The", "labels": [], "entities": []}, {"text": " Table 2: Spearman's \u03c1 \u00d7 100 on SL999. CHARAGRAM (large)", "labels": [], "entities": [{"text": "CHARAGRAM", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9649109840393066}]}, {"text": " Table 3: Results on SemEval textual similarity datasets (Pearson's r \u00d7 100). The highest score in each row is in boldface (omitting", "labels": [], "entities": [{"text": "SemEval textual similarity", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.816800038019816}, {"text": "Pearson's r \u00d7 100", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.906395673751831}]}, {"text": " Table 4: Results on part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.744129091501236}]}, {"text": " Table 5: Results of using different numbers and different com-", "labels": [], "entities": []}, {"text": " Table 6: Performance (Pearson's r \u00d7 100) as a function of", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9747207760810852}, {"text": "Pearson's r \u00d7 100)", "start_pos": 23, "end_pos": 41, "type": "METRIC", "confidence": 0.7294942140579224}]}, {"text": " Table 7: Performance (Pearson's r \u00d7 100) as a function of the", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9707561731338501}, {"text": "Pearson's r \u00d7 100)", "start_pos": 23, "end_pos": 41, "type": "METRIC", "confidence": 0.729141354560852}]}, {"text": " Table 9: Nearest neighbors of CHARAGRAM-PHRASE embeddings. Above the double horizontal line are nearest neighbors of", "labels": [], "entities": []}, {"text": " Table 9. In the second, we col- lected nearest neighbors of words that were in our  training data, shown in the lower part of", "labels": [], "entities": []}, {"text": " Table 10. They ap- pear to be grouped into themes, such as death (row", "labels": [], "entities": []}]}