{"title": [{"text": "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce two first-order graph-based dependency parsers achieving anew state of the art.", "labels": [], "entities": []}, {"text": "The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition-based parsers with different random initializations.", "labels": [], "entities": []}, {"text": "We cast this approach as minimum Bayes risk decoding (under the Hamming cost) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity.", "labels": [], "entities": []}, {"text": "The second parser is a \"distillation\" of the ensemble into a single model.", "labels": [], "entities": []}, {"text": "We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment , thereby avoiding the intractable cross-entropy computations required by applying standard distillation objectives to problems with structured outputs.", "labels": [], "entities": [{"text": "distillation parser", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8331364691257477}]}, {"text": "The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network dependency parsers achieve state of the art performance (, but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values.", "labels": [], "entities": [{"text": "Neural network dependency parsers", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6211831271648407}]}, {"text": "For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (.", "labels": [], "entities": []}, {"text": "In \u00a73, we apply this idea to build a firstorder graph-based (FOG) ensemble parser) that seeks consensus among 20 randomly-initialized stack LSTM parsers , achieving nearly the best-reported performance on the standard Penn Treebank Stanford dependencies task (94.51 UAS, 92.70 LAS).", "labels": [], "entities": [{"text": "Penn Treebank Stanford dependencies task", "start_pos": 218, "end_pos": 258, "type": "DATASET", "confidence": 0.9388647317886353}, {"text": "UAS", "start_pos": 266, "end_pos": 269, "type": "METRIC", "confidence": 0.9204097390174866}, {"text": "LAS", "start_pos": 277, "end_pos": 280, "type": "METRIC", "confidence": 0.9393347501754761}]}, {"text": "We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference.", "labels": [], "entities": []}, {"text": "We propose that disagreements among the ensemble's members maybe taken as a signal that an attachment decision is difficult or ambiguous.", "labels": [], "entities": []}, {"text": "Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding consensus.", "labels": [], "entities": [{"text": "Ensemble parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7825276255607605}]}, {"text": "We address this issue in \u00a75 by distilling the ensemble into a single FOG parser with discriminative training by defining anew cost function, inspired by the notion of \"soft targets\" ( . The essential idea is to derive the cost of each possible attachment from the ensemble's division of votes, and use this cost in discriminative learning.", "labels": [], "entities": []}, {"text": "The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7201763838529587}]}, {"text": "The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers, and (iii) many higher-order graph-based parsers (.", "labels": [], "entities": []}, {"text": "It represents anew state of the art for graphbased dependency parsing for English, Chinese, and German.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6592552661895752}]}, {"text": "The code to reproduce our results is publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (), and German CoNLL 2009 tasks.", "labels": [], "entities": [{"text": "English PTB-SD version 3.3.0", "start_pos": 26, "end_pos": 54, "type": "DATASET", "confidence": 0.8376964181661606}, {"text": "Penn Chinese Treebank", "start_pos": 56, "end_pos": 77, "type": "DATASET", "confidence": 0.962306022644043}, {"text": "German CoNLL 2009 tasks", "start_pos": 86, "end_pos": 109, "type": "DATASET", "confidence": 0.8373610228300095}]}, {"text": "We used the standard splits for all languages.", "labels": [], "entities": []}, {"text": "Like Chen and Manning (2014) and , we use predicted tags with the Stanford tagger ( for English and gold tags for Chinese.", "labels": [], "entities": []}, {"text": "For German we use the predicted tags provided by the CoNLL 2009 shared task organizers.", "labels": [], "entities": [{"text": "CoNLL 2009 shared task organizers", "start_pos": 53, "end_pos": 86, "type": "DATASET", "confidence": 0.9422465562820435}]}, {"text": "All models were augmented with pretrained structured-skipgram (  embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.8953144550323486}, {"text": "WMT 2010 monolingual data", "start_pos": 181, "end_pos": 206, "type": "DATASET", "confidence": 0.7565693035721779}]}, {"text": "The hyperparameters for neural FOG are summarized in.", "labels": [], "entities": [{"text": "FOG", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.563687264919281}]}, {"text": "For the Adam optimizer we use the default settings in the CNN neural network library.", "labels": [], "entities": [{"text": "CNN neural network library", "start_pos": 58, "end_pos": 84, "type": "DATASET", "confidence": 0.8196556270122528}]}, {"text": "8 Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set.", "labels": [], "entities": []}, {"text": "To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncertainty under the ensemble.", "labels": [], "entities": []}, {"text": "To obtain the ensemble uncertainty on each language, we use 21 base models for English (see footnote 4), 17 for Chinese, and 11 for German.", "labels": [], "entities": []}, {"text": "One potential drawback of using a quadratic or cubic time parser to distill an ensemble of linear-time transition-based models is speed.", "labels": [], "entities": []}, {"text": "Our FOG model is implemented using the same CNN library as the stack LSTM transition-based parser.", "labels": [], "entities": []}, {"text": "On the same single-thread CPU hardware, the distilled MST parser   is only three times faster at 60 sentences per second.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.7948746085166931}]}, {"text": "Running an ensemble of 20 stack LSTMs is at least 20 times slower (without multi-threading), not including consensus parsing.", "labels": [], "entities": [{"text": "consensus parsing", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7275355756282806}]}, {"text": "In the end, the distilled parser is more than ten times faster than the ensemble pipeline.", "labels": [], "entities": []}, {"text": "All scores are shown in.", "labels": [], "entities": []}, {"text": "First, consider the neural FOG parser trained with Hamming cost (C H in the second-to-last row).", "labels": [], "entities": [{"text": "FOG parser", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.6820702850818634}]}, {"text": "This is a very strong benchmark, outperforming many higherorder graph-based and neural network models on all three datasets.", "labels": [], "entities": []}, {"text": "Nonetheless, training the same model with distillation cost gives consistent improvements for all languages.", "labels": [], "entities": []}, {"text": "For English, we see that this model comes close to the slower ensemble it was trained to simulate.", "labels": [], "entities": []}, {"text": "For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre for LAS.", "labels": [], "entities": []}, {"text": "Effects of Pre-trained Word Embedding.", "labels": [], "entities": []}, {"text": "As an ablation study, we ran experiments on English without pre-trained word embedding, both with the Hamming and distillation costs.", "labels": [], "entities": [{"text": "Hamming", "start_pos": 102, "end_pos": 109, "type": "TASK", "confidence": 0.5757954716682434}]}, {"text": "The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS, compared to 93.6 UAS and 91.1 LAS for the model with distillation cost.", "labels": [], "entities": [{"text": "Hamming", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.8036342263221741}, {"text": "UAS", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.997250497341156}, {"text": "LAS", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9524849057197571}, {"text": "UAS", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9918036460876465}, {"text": "LAS", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.90870600938797}]}, {"text": "This result further showcases the consistent improvements from using the distillation cost across different settings and languages.", "labels": [], "entities": []}, {"text": "We conclude that \"soft targets\" derived from ensemble uncertainty offer useful guidance, through the distillation cost function and discriminative training of a graph-based parser.", "labels": [], "entities": []}, {"text": "Here we consid-: Dependency parsing performance on English, Chinese, and German tasks.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.8556920289993286}]}, {"text": "The \"P?\" column indicates the use of pretrained word embeddings.", "labels": [], "entities": []}, {"text": "Reranking/blend indicates that the reranker score is interpolated with the base model's score.", "labels": [], "entities": [{"text": "Reranking", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9467127919197083}, {"text": "blend", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.5215790867805481}]}, {"text": "Note that previous works might use different predicted tags for English.", "labels": [], "entities": []}, {"text": "We report accuracy without punctuation for English and Chinese, and with punctuation for German, using the standard evaluation script in each case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994969367980957}]}, {"text": "We only consider systems that do not use additional training data.", "labels": [], "entities": []}, {"text": "The best overall results are indicated with bold (this was achieved by the ensemble of greedy stack LSTMs in Chinese and German), while the best non-ensemble model is denoted with an underline.", "labels": [], "entities": []}, {"text": "The \u2020 sign indicates the use of predicted tags for Chinese in the original publication, although we report accuracy using gold Chinese tags based on private correspondence with the authors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9994168281555176}]}, {"text": "ered a FOG parser, though future work might investigate any parser amenable to training to minimize a cost-aware loss like the structured hinge.", "labels": [], "entities": [{"text": "FOG parser", "start_pos": 7, "end_pos": 17, "type": "TASK", "confidence": 0.5038497745990753}]}], "tableCaptions": [{"text": " Table 1: PTB-SD task: ensembles improve over a strong  greedy baseline. UEM indicates unlabeled exact match.", "labels": [], "entities": [{"text": "PTB-SD", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6087694764137268}, {"text": "UEM", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9892773628234863}]}, {"text": " Table 2: An ambiguous sentence from the training set and  the posteriors 4 of various possible parents for including.  The last two columns are, respectively, the contributions  to the distillation cost C D (explained in  \u00a75.1, Eq. 5) and  the standard Hamming cost C H . The most probable head  under the ensemble is changes, which is also the correct  answer.", "labels": [], "entities": []}, {"text": " Table 3: Example of soft targets (taken from our 20- model ensemble's uncertainty on the sentence) and hard  targets (taken from the gold standard) for possible parents  of with. The soft target corresponds with the posterior  (second column) in", "labels": [], "entities": []}, {"text": " Table 4. For the  Adam optimizer we use the default settings in the  CNN neural network library. 8 Since the ensemble  is used to obtain the uncertainty on the training set,  it is imperative that the stack LSTMs do not overfit  the training set. To address this issue, we performed  five-way jackknifing of the training data for each  stack LSTM model to obtain the training data uncer- tainty under the ensemble. To obtain the ensemble  uncertainty on each language, we use 21 base mod- els for English (see footnote 4), 17 for Chinese, and  11 for German.", "labels": [], "entities": [{"text": "CNN neural network library", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.8547261208295822}]}, {"text": " Table 5: Dependency parsing performance on English, Chinese, and German tasks. The \"P?\" column indicates the use  of pretrained word embeddings. Reranking/blend indicates that the reranker score is interpolated with the base model's  score. Note that previous works might use different predicted tags for English. We report accuracy without punctuation  for English and Chinese, and with punctuation for German, using the standard evaluation script in each case. We only  consider systems that do not use additional training data. The best overall results are indicated with bold (this was  achieved by the ensemble of greedy stack LSTMs in Chinese and German), while the best non-ensemble model is  denoted with an underline. The  \u2020 sign indicates the use of predicted tags for Chinese in the original publication,  although we report accuracy using gold Chinese tags based on private correspondence with the authors.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8163976967334747}, {"text": "Reranking", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9874774217605591}, {"text": "accuracy", "start_pos": 325, "end_pos": 333, "type": "METRIC", "confidence": 0.9986829161643982}, {"text": "accuracy", "start_pos": 837, "end_pos": 845, "type": "METRIC", "confidence": 0.9983271956443787}]}]}