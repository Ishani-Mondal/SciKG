{"title": [], "abstractContent": [{"text": "Word embedding has been widely studied and proven helpful in solving many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.7194286361336708}]}, {"text": "However, the ambiguity of natural language is always a problem on learning high quality word embed-dings.", "labels": [], "entities": []}, {"text": "A possible solution is sense embedding which trains embedding for each sense of words instead of each word.", "labels": [], "entities": []}, {"text": "Some recent work on sense embedding uses context clustering methods to determine the senses of words, which is heuristic in nature.", "labels": [], "entities": [{"text": "sense embedding", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7335411608219147}]}, {"text": "Other work creates a probabilistic model and performs word sense disambiguation and sense embedding iteratively.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.6574448347091675}]}, {"text": "However, most of the previous work has the problems of learning sense embeddings based on imperfect word embeddings as well as ignoring the dependency between sense choices of neighboring words.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel probabilistic model for sense embedding that is not based on problematic word embedding of polysemous words and takes into account the dependency between sense choices.", "labels": [], "entities": []}, {"text": "Based on our model, we derive a dynamic programming inference algorithm and an Expectation-Maximization style unsupervised learning algorithm.", "labels": [], "entities": [{"text": "Expectation-Maximization", "start_pos": 79, "end_pos": 103, "type": "METRIC", "confidence": 0.9289026856422424}]}, {"text": "The empirical studies show that our model outperforms the state-of-the-art model on a word sense induction task by a 13% relative gain.", "labels": [], "entities": [{"text": "word sense induction task", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.850716769695282}]}], "introductionContent": [{"text": "Distributed representation of words (aka word embedding) aims to learn continuous-valued vectors to * The second author was supported by the National Natural Science Foundation of China (61503248).", "labels": [], "entities": [{"text": "Distributed representation of words", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8675576001405716}]}, {"text": "represent words based on their context in a large corpus.", "labels": [], "entities": []}, {"text": "They can serve as input features for algorithms of natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.7305428726332528}]}, {"text": "High quality word embeddings have been proven helpful in many NLP tasks.", "labels": [], "entities": []}, {"text": "Recently, with the development of deep learning, many novel neural network architectures are proposed for training high quality word embeddings ().", "labels": [], "entities": []}, {"text": "However, since natural language is intrinsically ambiguous, learning one vector for each word may not coverall the senses of the word.", "labels": [], "entities": []}, {"text": "In the case of a multi-sense word, the learned vector will be around the average of all the senses of the word in the embedding space, and therefore may not be a good representation of any of the senses.", "labels": [], "entities": []}, {"text": "A possible solution is sense embedding which trains a vector for each sense of a word.", "labels": [], "entities": []}, {"text": "There are two key steps in training sense embeddings.", "labels": [], "entities": []}, {"text": "First, we need to perform word sense disambiguation (WSD) or word sense induction (WSI) to determine the senses of words in the training corpus.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.7340458432833353}, {"text": "word sense induction (WSI)", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6060142616430918}]}, {"text": "Then, we need to train embedding vectors for word senses according to their contexts.", "labels": [], "entities": []}, {"text": "Early work on sense embedding proposes context clustering methods which determine the sense of a word by clustering aggregated embeddings of words in its context.", "labels": [], "entities": [{"text": "context clustering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7152028232812881}]}, {"text": "This kind of methods is heuristic in nature and relies on external knowledge from lexicon like WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9658482074737549}]}, {"text": "Recently, sense embedding methods based on complete probabilistic models and well-defined learning objective functions () become more popular.", "labels": [], "entities": []}, {"text": "These methods regard the choice of senses of the words in a sentence as hidden variables.", "labels": [], "entities": []}, {"text": "Learning is therefore done with expectationmaximization style algorithms, which alternate between inferring word sense choices in the training corpus and learning sense embeddings.", "labels": [], "entities": []}, {"text": "A common problem with these methods is that they model the sense embedding of each center word dependent on the word embeddings of its context words.", "labels": [], "entities": []}, {"text": "As we previously explained, word embedding of a polysemous word is not a good representation and may negatively influence the quality of inference and learning.", "labels": [], "entities": []}, {"text": "Furthermore, these methods choose the sense of each word in a sentence independently, ignoring the dependency that may exist between the sense choices of neighboring words.", "labels": [], "entities": []}, {"text": "We argue that such dependency is important in word sense disambiguation and therefore helpful in learning sense embeddings.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6529285808404287}]}, {"text": "For example, consider the sentence \"He cashed a check at the bank\".", "labels": [], "entities": []}, {"text": "Both \"check\" and \"bank\" are ambiguous here.", "labels": [], "entities": []}, {"text": "Although the two words hint at banking related senses, the hint is not decisive (as an alternative interpretation, they may represent a check mark at a river bank).", "labels": [], "entities": []}, {"text": "Fortunately, \"cashed\" is not ambiguous and it can help disambiguate \"check\".", "labels": [], "entities": []}, {"text": "However, if we consider a small context window in sense embedding, then \"cashed\" cannot directly help disambiguate \"bank\".", "labels": [], "entities": []}, {"text": "We need to rely on the dependency between the sense choices of \"check\" and \"bank\" to disambiguate \"bank\".", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel probabilistic model for sense embedding that takes into account the dependency between sense choices of neighboring words.", "labels": [], "entities": []}, {"text": "We do not learn any word embeddings in our model and hence avoid the problem with embedding polysemous words discussed above.", "labels": [], "entities": []}, {"text": "Our model has a similar structure to a high-order hidden Markov model.", "labels": [], "entities": []}, {"text": "It contains a sequence of observable words and latent senses and models the dependency between each word-sense pair and between neighboring senses in the sequence.", "labels": [], "entities": []}, {"text": "The energy of neighboring senses can be modeled using existing word embedding approaches such as CBOW and Skipgram ().", "labels": [], "entities": [{"text": "CBOW", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8277224898338318}]}, {"text": "Given the model and a sentence, we can perform exact inference using dynamic programming and get the optimal sense sequence of the sentence.", "labels": [], "entities": []}, {"text": "Our model can be learned from an unannotated corpus by optimizing a max-margin objective using an algorithm similar to hard-EM.", "labels": [], "entities": []}, {"text": "Our main contributions are the following: 1.", "labels": [], "entities": []}, {"text": "We propose a complete probabilistic model for sense embedding.", "labels": [], "entities": [{"text": "sense embedding", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.8244548141956329}]}, {"text": "Unlike previous work, we model the dependency between sense choices of neighboring words and do not learn sense embeddings dependent on problematic word embeddings of polysemous words.", "labels": [], "entities": []}, {"text": "2. Based on our proposed model, we derive an exact inference algorithm and a max-margin learning algorithm which do not rely on external knowledge from any knowledge base or lexicon (except that we determine the numbers of senses of polysemous words according to an existing sense inventory).", "labels": [], "entities": []}, {"text": "3. The performance of our model on contextual word similarity task is competitive with previous work and we obtain a 13% relative gain compared with previous state-of-the-art methods on the word sense induction task of SemEval-2013.", "labels": [], "entities": [{"text": "contextual word similarity task", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6715075224637985}, {"text": "word sense induction task", "start_pos": 190, "end_pos": 215, "type": "TASK", "confidence": 0.8217925429344177}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We introduce related work in section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes our models and algorithms in detail.", "labels": [], "entities": []}, {"text": "We present our experiments and results in section 4.", "labels": [], "entities": []}, {"text": "In section 5, a conclusion is given.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents our experiments and results.", "labels": [], "entities": []}, {"text": "First, we describe our experimental setup including the training corpus and the model configuration.", "labels": [], "entities": []}, {"text": "Then, we perform a qualitative evaluation on our model by presenting the nearest neighbors of senses of some polysemous words.", "labels": [], "entities": []}, {"text": "Finally, we introduce two different tasks and show the experimental results on these tasks respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman's rank correlation results on the SCWS", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.6492123454809189}, {"text": "SCWS", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.7994934916496277}]}, {"text": " Table 3: Results of single-sense instances on task 13 of", "labels": [], "entities": []}]}