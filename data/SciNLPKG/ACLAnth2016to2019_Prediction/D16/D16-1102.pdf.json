{"title": [{"text": "Towards Semi-Automatic Generation of Proposition Banks for Low-Resource Languages", "labels": [], "entities": [{"text": "Semi-Automatic Generation of Proposition Banks", "start_pos": 8, "end_pos": 54, "type": "TASK", "confidence": 0.7919138133525848}]}], "abstractContent": [{"text": "Annotation projection based on parallel corpora has shown great promise in inexpensively creating Proposition Banks for languages for which high-quality parallel corpora and syntactic parsers are available.", "labels": [], "entities": []}, {"text": "In this paper, we present an experimental study where we apply this approach to three languages that lack such resources: Tamil, Bengali and Malay-alam.", "labels": [], "entities": []}, {"text": "We find an average quality difference of 6 to 20 absolute F-measure points visa -vis high-resource languages, which indicates that annotation projection alone is insufficient in low-resource scenarios.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9473149180412292}]}, {"text": "Based on these results , we explore the possibility of using annotation projection as a starting point for inexpensive data curation involving both experts and non-experts.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7079408466815948}]}, {"text": "We give an outline of what such a process may look like and present an initial study to discuss its potential and challenges .", "labels": [], "entities": []}], "introductionContent": [{"text": "Creating syntactically and semantically annotated NLP resources for low-resource languages is known to be immensely costly.", "labels": [], "entities": []}, {"text": "For instance, the Proposition) was created by annotating predicate-argument structures in the Penn Treebank () with shallow semantic labels: frame labels for verbal predicates and role labels for arguments.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9955312311649323}]}, {"text": "Similarly, the SALSA () resource added FrameNet-style annotations to the TIGER Treebank (), the Chinese Propbank) is built on the Chinese Treebank (), and so forth.", "labels": [], "entities": [{"text": "TIGER Treebank", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.8499356508255005}, {"text": "Chinese Propbank", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.9357903599739075}, {"text": "Chinese Treebank", "start_pos": 130, "end_pos": 146, "type": "DATASET", "confidence": 0.9691044390201569}]}, {"text": "Since each such layer of annotation typically requires years of manual work, the accumulated costs can be prohibitive for low-resource languages.", "labels": [], "entities": []}, {"text": "Recent work on annotation projection offers away to inexpensively label a target language corpus with linguistic annotation.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8427777290344238}]}, {"text": "This only requires a word-aligned parallel corpus of labeled English sentences and their translations in the target language.", "labels": [], "entities": []}, {"text": "English labels are then automatically projected onto the aligned target language words.", "labels": [], "entities": []}, {"text": "Refer to for an example.", "labels": [], "entities": []}, {"text": "However, previous work that investigated Propbank annotation projection has focused only on languages for which treebanks -and therefore syntactic parsers -already exist.", "labels": [], "entities": [{"text": "Propbank annotation projection", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.7222140431404114}]}, {"text": "Since syntactic information is typically used to increase projection accuracy, we must expect this approach to work less well for low-resource languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.8871752023696899}]}, {"text": "In addition, low-resource languages have fewer sources of high- quality parallel data available, further complicating annotation projection.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 118, "end_pos": 139, "type": "TASK", "confidence": 0.737697422504425}]}, {"text": "In this paper, we present a study in which we apply annotation projection to three lowresource languages in order to quantify the difference in precision and recall vis-a-vis high-resource languages.", "labels": [], "entities": [{"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9988036155700684}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9899789094924927}]}, {"text": "Our study finds overall F1-measure of generated Proposition Banks to be significantly below state-of-the-art results, leading us to conclude that annotation projection may at best be a starting point for the generation of semantic resources for low-resource languages.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9982927441596985}, {"text": "annotation projection", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.7096642404794693}]}, {"text": "To explore this idea, we outline a potential semi-automatic process in which we use crowdsourced data curation and limited expert involvement to confirm and correct automatically projected labels.", "labels": [], "entities": []}, {"text": "Based on this initial study, we discuss the potential and challenges of the proposed approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report our initial investigations over the following questions: (1) What are the differences in annotation projection quality between low-and highresource languages?; and (2) Can non-experts be leveraged to at least partially curate projected labels?", "labels": [], "entities": []}, {"text": "We evaluate three low-resource languages, namely Bengali, an Indo-Aryan language, as well as Tamil and Malayalam, two South Dravidian languages.", "labels": [], "entities": []}, {"text": "Between them, they are estimated to have more than 300 million first language speakers, yet there are few NLP resources available.", "labels": [], "entities": []}, {"text": "We use two parallel corpora (see): OPENSUBTITLES2016 (Tiedemann, 2012), a corpus automatically generated from movie subtitles, and SPOKENTUTORIALS, a corpus of technicaldomain tutorial translations.", "labels": [], "entities": [{"text": "OPENSUBTITLES2016", "start_pos": 35, "end_pos": 52, "type": "METRIC", "confidence": 0.9517731666564941}]}, {"text": "For the purpose of comparison to previous work on high-resource languages, we replicate 1 Common problems for non-experts that we observe in our initial experiments involve ambiguities caused by implicit or causal role-predicate relationships, as well as figurative usage and hypotheticals.", "labels": [], "entities": []}, {"text": "earlier evaluation practice and English preprocessing steps.", "labels": [], "entities": []}, {"text": "After projection, we randomly select 100 sentences for each target language and pass them to a curation step by 2 nonexperts.", "labels": [], "entities": []}, {"text": "We then measure the inter-annotator agreement and the quality of the generated Proposition Banks in terms of predicate precision 2 and argument F1-score before and after crowdsourced curation 3 .", "labels": [], "entities": [{"text": "predicate precision 2", "start_pos": 109, "end_pos": 130, "type": "METRIC", "confidence": 0.7661278446515402}, {"text": "F1-score", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.7502628564834595}]}], "tableCaptions": [{"text": " Table 1: Parallel data sets and number of parallel sentences", "labels": [], "entities": []}, {"text": " Table 2: Estimated precision and recall for Tamil, Bengali and", "labels": [], "entities": [{"text": "Estimated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9969423413276672}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.955884575843811}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9995655417442322}]}, {"text": " Table 2. For  comparison, we include evaluation results reported  for three high-resource languages: German and Chi- nese, representing average high-resource results, as  well as Hindi, a below-average outlier. We make the  following observations:  Lower annotation projection quality. We find that  the F1-scores of Bengali, Malayalam and Tamil are \u0986\u09b0 \u098f\u0995\u099f\u09c1 \u09bf\u09b9\u0982\ufffd , \ufffd\u09af\u09ae\u09a8\u099f\u09be \u0986\u099c \u0986\u09bf\u09ac\ufffd\u09be\u09b0 \u0995\u09b0\u09b2\u09be\u09ae", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 305, "end_pos": 314, "type": "METRIC", "confidence": 0.9979574680328369}, {"text": "\u0986\u09b0", "start_pos": 351, "end_pos": 353, "type": "METRIC", "confidence": 0.9888060092926025}]}, {"text": " Table 3: Number of labeled sentences, semantic labels and", "labels": [], "entities": []}]}