{"title": [{"text": "Learning Connective-based Word Representations for Implicit Discourse Relation Identification", "labels": [], "entities": [{"text": "Implicit Discourse Relation Identification", "start_pos": 51, "end_pos": 93, "type": "TASK", "confidence": 0.7356053441762924}]}], "abstractContent": [{"text": "We introduce a simple semi-supervised approach to improve implicit discourse relation identification.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 58, "end_pos": 100, "type": "TASK", "confidence": 0.6027129217982292}]}, {"text": "This approach harnesses large amounts of automatically extracted discourse connectives along with their arguments to construct new distributional word representations.", "labels": [], "entities": []}, {"text": "Specifically, we represent words in the space of discourse connectives as away to directly encode their rhetorical function.", "labels": [], "entities": []}, {"text": "Experiments on the Penn Discourse Treebank demonstrate the effectiveness of these task-tailored representations in predicting implicit discourse relations.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.9743790825208029}, {"text": "predicting implicit discourse relations", "start_pos": 115, "end_pos": 154, "type": "TASK", "confidence": 0.810824915766716}]}, {"text": "Our results indeed show that, despite their simplicity, these connective-based representations outperform various off-the-shelf word embeddings, and achieve state-of-the-art performance on this problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "A natural distinction is often made between explicit and implicit discourse relations depending on whether they are lexicalized by a connective or not, respectively.", "labels": [], "entities": []}, {"text": "To illustrate, the Contrast relation in example (1a) is triggered by the connective but, while it is not overtly marked in example (1b).", "labels": [], "entities": []}, {"text": "Given the lack of strong explicit cues, the identification of implicit relations is a much more challenging and still open problem.", "labels": [], "entities": []}, {"text": "The typically low performance scores for this task also hinder the development of text-level discourse parsers (: implicit discourse relations The difficulty of this task lies in its dependence on a wide variety of linguistic factors, ranging from syntax, lexical semantics and also world knowledge.", "labels": [], "entities": [{"text": "text-level discourse parsers", "start_pos": 82, "end_pos": 110, "type": "TASK", "confidence": 0.6399406592051188}]}, {"text": "In order to deal with this issue, a common approach is to exploit hand-crafted resources to design features capturing lexical, temporal, modal, or syntactic information (.", "labels": [], "entities": []}, {"text": "By contrast, more recent work show that using simple low-dimensional word-based representations, either cluster-based or distributed (aka word embeddings), yield comparable or better performance, while dispensing with feature engineering.", "labels": [], "entities": []}, {"text": "While standard low-dimensional word representations appear to encode relevant linguistic information, they have not been built with the specific rhetorical task in mind.", "labels": [], "entities": []}, {"text": "A natural question is therefore whether one could improve implicit discourse relation identification by using word representations that are more directly related to the task.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 58, "end_pos": 100, "type": "TASK", "confidence": 0.62564467638731}]}, {"text": "The 203 problem of learning good representation for discourse has been recently tackled by on the problem of text-level discourse parsing.", "labels": [], "entities": [{"text": "text-level discourse parsing", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.6153101325035095}]}, {"text": "Their approach uses two recursive neural networks to jointly learn the task and a transformation of the discourse segments to be attached.", "labels": [], "entities": []}, {"text": "While this type of joint learning yields encouraging results, it is also computationally intensive, requiring long training times, and could be limited by the relatively small amount of manually annotated data available.", "labels": [], "entities": []}, {"text": "In this paper, we explore the possibility of learning a distributional word representation adapted to the task by selecting relevant rhetorical contexts, in this case discourse connectives, extracted from large amounts of automatically detected connectives along with their arguments.", "labels": [], "entities": []}, {"text": "Informally, the assumption is that the estimated word-connective cooccurrence statistics will in effect give us an important insight to the rhetorical function of different words.", "labels": [], "entities": []}, {"text": "The learning phase in this case is extremely simple, as it amounts to merely estimating cooccurrence frequencies, potentially combined with a reweighting scheme, between each word appearing in a discourse segment and its co-occurring connective.", "labels": [], "entities": []}, {"text": "To assess the usefulness of these connectivebased representations, 2 we compare them with pretrained word representations, like Brown clusters and other word embeddings, on the task of implicit discourse relation identification.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 185, "end_pos": 227, "type": "TASK", "confidence": 0.6029649004340172}]}, {"text": "Our experiments on the Penn Discourse Treebank (PDTB) show that these new representations deliver improvements over systems using these generic representations and yield state-of-the-art results, and this without the use of other hand-crafted features, thus also alleviating the need for external linguistic resources (like lexical databases).", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 23, "end_pos": 53, "type": "DATASET", "confidence": 0.9522547125816345}]}, {"text": "Thus, our approach could be easily extended to resource-poor languages as long as connectives can be reliably identified on raw texts.", "labels": [], "entities": []}, {"text": "Section 2 summarizes related work.", "labels": [], "entities": []}, {"text": "In Section 3, we detail our connective-based distributional word representation approach.", "labels": [], "entities": [{"text": "connective-based distributional word representation", "start_pos": 28, "end_pos": 79, "type": "TASK", "confidence": 0.5590912252664566}]}, {"text": "Section 4 presents the automatic annotation of the explicit examples used to build the word representation.", "labels": [], "entities": []}, {"text": "In Section 5, we describe our comparative experiments on the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8564051389694214}]}], "datasetContent": [{"text": "Our experiments investigate the relevance of our connective-based representations for implicit discourse relation identification, recast here as multiclass classification problem.", "labels": [], "entities": [{"text": "implicit discourse relation identification", "start_pos": 86, "end_pos": 128, "type": "TASK", "confidence": 0.6087784394621849}, {"text": "multiclass classification", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.7100578546524048}]}, {"text": "That is, we aim at evaluating the usefulness of having a word representation linked to the task, compared to using generic  word representations (either one-hot, cluster-based or distributed), and whether they encode all the information relevant to the task, thus comparing systems with or without additional hand-crafted features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Illustrative example of association measures between connectives and words.", "labels": [], "entities": []}, {"text": " Table 2: Lexicon coverage for Brown clusters (Brown et al.,", "labels": [], "entities": []}, {"text": " Table 3: Number of examples in train, dev, test.", "labels": [], "entities": []}, {"text": " Table 4: Results for multiclass experiments. R&X 15 are the", "labels": [], "entities": []}, {"text": " Table 5: Scores per relation for multiclass experiments, \"R&X", "labels": [], "entities": []}]}