{"title": [{"text": "Multi-view Response Selection for Human-Computer Conversation", "labels": [], "entities": [{"text": "Human-Computer Conversation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7136310935020447}]}], "abstractContent": [{"text": "In this paper, we study the task of response selection for multi-turn human-computer conversation.", "labels": [], "entities": [{"text": "response selection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8996068239212036}]}, {"text": "Previous approaches take word as a unit and view context and response as sequences of words.", "labels": [], "entities": []}, {"text": "This kind of approaches do not explicitly take each utterance as a unit, therefore it is difficult to catch utterance-level discourse information and dependencies.", "labels": [], "entities": []}, {"text": "In this paper, we propose a multi-view response selection model that integrates information from two different views, i.e., word sequence view and utterance sequence view.", "labels": [], "entities": []}, {"text": "We jointly model the two views via deep neu-ral networks.", "labels": [], "entities": []}, {"text": "Experimental results on a public corpus for context-sensitive response selection demonstrate the effectiveness of the proposed multi-view model, which significantly outper-forms other single-view baselines.", "labels": [], "entities": [{"text": "context-sensitive response selection", "start_pos": 44, "end_pos": 80, "type": "TASK", "confidence": 0.5777102410793304}]}], "introductionContent": [{"text": "Selecting a potential response from a set of candidates is an important and challenging task for open-domain human-computer conversation, especially for the retrieval-based human-computer conversation.", "labels": [], "entities": []}, {"text": "In general, a set of candidate responses from the indexed conversation corpus are retrieved, and then the best one is selected from the candidates as the system's response ().", "labels": [], "entities": []}, {"text": "Previous Deep Neural Network (DNN) based approaches to response selection represent context and response as two embeddings.", "labels": [], "entities": [{"text": "response selection", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.77311572432518}]}, {"text": "The response is selected based on the similarity of these two embeddings ().", "labels": [], "entities": []}, {"text": "In * These two authors contributed equally these work, context and response are taken as two separate word sequences without considering the relationship among utterances in the context and response.", "labels": [], "entities": []}, {"text": "The response selection in these models is largely influenced by word-level information.", "labels": [], "entities": []}, {"text": "We called this kind of models as word sequence model in this paper.", "labels": [], "entities": []}, {"text": "Besides word-level dependencies, utterance-level semantic and discourse information are also very important to catch the conversation topics to ensure coherence (.", "labels": [], "entities": []}, {"text": "For example an utterance can bean affirmation, negation or deduction to the previous utterances, or starts anew topic for discussion.", "labels": [], "entities": []}, {"text": "This kind of utterance-level information is generally ignored in word sequence model, which maybe helpful for selecting the next response.", "labels": [], "entities": []}, {"text": "Therefore, it is necessary to take each utterance as a unit and model the context and response from the view of utterance sequence.", "labels": [], "entities": []}, {"text": "This paper proposes a multi-view response selection model, which integrates information from both word sequence view and utterance sequence view.", "labels": [], "entities": [{"text": "multi-view response selection", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.5818230907122294}]}, {"text": "Our assumption is that each view can represent relationships between context and response from a particular aspect, and features extracted from the word sequence and the utterance sequence provide complementary information for response selection.", "labels": [], "entities": [{"text": "response selection", "start_pos": 227, "end_pos": 245, "type": "TASK", "confidence": 0.7012597322463989}]}, {"text": "An effective integration of these two views is expected to improve the model performance.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to improve the response selection for multi-turn human-computer conversation in a multi-view manner.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the multi-view response selection model on a public corpus containing about one million context-response-label triples.", "labels": [], "entities": []}, {"text": "This corpus was extracted from an online chatting room for Ubuntu troubleshooting, which is called the Ubuntu Corpus in this paper ( . Experimental results show that the proposed multiview response selection model significantly outperforms the current best single-view models for multiturn human-computer conversation.", "labels": [], "entities": [{"text": "Ubuntu Corpus", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9791945815086365}, {"text": "multiturn human-computer conversation", "start_pos": 280, "end_pos": 317, "type": "TASK", "confidence": 0.7580042481422424}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly introduce related works.", "labels": [], "entities": []}, {"text": "Then we move onto a detailed description of our model in Section 3.", "labels": [], "entities": []}, {"text": "Experimental results are described in Section 4.", "labels": [], "entities": []}, {"text": "Analysis of our models is shown in Section 5.", "labels": [], "entities": []}, {"text": "We conclude the paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model is evaluated on the public Ubuntu Corpus ( , designed for response selection study of multi-turn human-computer conversation ( ).", "labels": [], "entities": [{"text": "Ubuntu Corpus", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.969117134809494}]}, {"text": "The dataset contains 0.93 million human-human dialogues crawled from an Internet chatting room for Ubuntu troubleshooting.", "labels": [], "entities": [{"text": "Ubuntu troubleshooting", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.49190889298915863}]}, {"text": "Around 1 million context-response-labeled triples, namely < c, r, l >, are generated for training after preprocessing 2 , where the original context and the corresponding response are taken as the positive instances while the random utterances in the data set taken as the negative instances, and the number of positive instance and negative instance in training set is balanced.", "labels": [], "entities": []}, {"text": "The validation set and testing set are constructed in a similar way to the training set, with one notable difference that for each context and the corresponding positive response, 9 negative responses are randomly selected for further evaluation.", "labels": [], "entities": []}, {"text": "Following the work of , the evaluation metric is 1 in m Recall@k (denoted 1 in m R@k), where a response selection model is designed to select k most likely responses among m candidates, and it gets the score \"1\" if the correct response is in the k selected ones.", "labels": [], "entities": [{"text": "Recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9354401230812073}]}, {"text": "This metric can be seen as an adaptation of the precision and recall metrics previously applied to dialogue datasets ().", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.998620867729187}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9707834720611572}]}, {"text": "It is worth noticing that 1 in 2 R@1 equals to precision and recall in binary classification.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9995220899581909}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9989736080169678}]}, {"text": "We summarize the experiment result in.", "labels": [], "entities": []}, {"text": "As shown in, all DNN-based models achieve significant improvements compared to Randomguess and TF-IDF, which implies the effectiveness of DNN models in the task of response selection.", "labels": [], "entities": [{"text": "response selection", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.7915447950363159}]}, {"text": "The word sequence models implemented with GRU and LSTM achieve similar performance.", "labels": [], "entities": []}, {"text": "The utterance sequence model significantly outperforms word sequence models for 1 in 10 R@1.", "labels": [], "entities": []}, {"text": "Multiview model significantly outperforms all the other models, especially for 1 in 10 R@1, which is more difficult and closer to the real world scenario than other metrics.", "labels": [], "entities": []}, {"text": "The experimental result demonstrates the effectiveness of multi-view model and proves that word sequence view and utterance sequence view can bring complementary information for each other.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance comparison between our models and baseline models. In the table, Word-seq-LSTM is the experiment", "labels": [], "entities": [{"text": "Word-seq-LSTM", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.8785061240196228}]}]}