{"title": [{"text": "Semantic Parsing to Probabilistic Programs for Situated Question Answering", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8803320825099945}, {"text": "Question Answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.678670346736908}]}], "abstractContent": [{"text": "Situated question answering is the problem of answering questions about an environment such as an image or diagram.", "labels": [], "entities": [{"text": "Situated question answering", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7434250116348267}, {"text": "answering questions about an environment such as an image or diagram", "start_pos": 46, "end_pos": 114, "type": "TASK", "confidence": 0.656059286811135}]}, {"text": "This problem requires jointly interpreting a question and an environment using background knowledge to select the correct answer.", "labels": [], "entities": []}, {"text": "We present Parsing to Probabilistic Programs (P 3), a novel situated question answering model that can use background knowledge and global features of the question/environment interpretation while retaining efficient approximate inference.", "labels": [], "entities": [{"text": "question answering", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.750487744808197}]}, {"text": "Our key insight is to treat semantic parses as prob-abilistic programs that execute nondetermin-istically and whose possible executions represent environmental uncertainty.", "labels": [], "entities": []}, {"text": "We evaluate our approach on anew, publicly-released data set of 5000 science diagram questions, outper-forming several competitive classical and neu-ral baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Situated question answering is a challenging problem that requires reasoning about uncertain interpretations of both a question and an environment together with background knowledge to determine the answer.", "labels": [], "entities": [{"text": "Situated question answering", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7322381436824799}]}, {"text": "To illustrate these challenges, consider the 8th grade science diagram questions in, which are motivated by the Aristo project.", "labels": [], "entities": [{"text": "8th grade science diagram questions", "start_pos": 45, "end_pos": 80, "type": "DATASET", "confidence": 0.6881878256797791}]}, {"text": "These questions require both computer vision to interpret the diagram and compositional question understanding.", "labels": [], "entities": [{"text": "compositional question understanding", "start_pos": 74, "end_pos": 110, "type": "TASK", "confidence": 0.6488538483778635}]}, {"text": "These components, being imperfect, introduce uncertainty that must be jointly reasoned about to avoid implausible interpretations.", "labels": [], "entities": []}, {"text": "These uncertain interpretations must further  be combined with background knowledge, such as the definition of a \"predator,\" to determine the correct answer.", "labels": [], "entities": []}, {"text": "The challenges of situated question answering have not been completely addressed by prior work.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7692199051380157}]}, {"text": "Early \"possible worlds\" models () were capable of compositional question understanding and using background knowledge, but did not jointly reason about environ-ment/question uncertainty.", "labels": [], "entities": [{"text": "compositional question understanding", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.6252445677916209}]}, {"text": "These models also used unscalable inference algorithms for reasoning about the environment, despite the lack of joint reasoning.", "labels": [], "entities": []}, {"text": "More recent neural models () are incapable of using background knowledge and it remains unclear to what extent these models can represent compositionality in language.", "labels": [], "entities": []}, {"text": "We present Parsing to Probabilistic Programs (P 3 ), a novel approach to situated question answering that addresses these challenges.", "labels": [], "entities": [{"text": "situated question answering", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.6066965560118357}]}, {"text": "It is motivated by two observations: (1) situated question answering can be formulated as semantic parsing with an execution model that is a learned function of the environment, and (2) probabilistic programming is a natural and powerful method for specifying the space of permissible execution models and learning over it.", "labels": [], "entities": [{"text": "situated question answering", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6413794457912445}, {"text": "semantic parsing", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7550115883350372}]}, {"text": "In P 3 , we define a domain theory for the task as a probabilistic program, then train a joint loglinear model to semantically parse questions to logical forms in this theory and execute them in an environment.", "labels": [], "entities": []}, {"text": "Importantly, the model includes global features over parsing and execution that enable it to avoid unlikely joint configurations.", "labels": [], "entities": []}, {"text": "P 3 leverages semantic parsing to represent compositionality in language and probabilistic programming to specify background knowledge and perform linear-time approximate inference over the environment.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.7549233138561249}]}, {"text": "We present an experimental evaluation of P 3 on anew data set of 5000 food web diagram questions ().", "labels": [], "entities": []}, {"text": "We compare our approach to several baselines, including possible worlds and neural network approaches, finding that P 3 outperforms both.", "labels": [], "entities": []}, {"text": "An ablation study demonstrates that global features help the model achieve high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9962267875671387}]}, {"text": "We also demonstrate that P 3 improves accuracy on a previously published data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9992671608924866}]}, {"text": "Finally, we have released our data and code to facilitate further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation compares P 3 to both possible worlds and neural network approaches on our data set of food web diagram questions.", "labels": [], "entities": []}, {"text": "An ablation study demonstrates that both sets of global features improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9971844553947449}]}, {"text": "Finally, we demonstrate P 3 's generality by applying it to a previously-published data set, obtaining state-of-the-art results.", "labels": [], "entities": []}, {"text": "Code, data and supplementary material for this paper are available at: http://www.allenai.", "labels": [], "entities": []}, {"text": "org/paper-appendix/emnlp2016-p3  Our final experiment applies P 3 to the SCENE data set of.", "labels": [], "entities": [{"text": "SCENE data set", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.8146018981933594}]}, {"text": "In this data set, the input is a natural language expression, such as \"blue mug to the left of the monitor,\" and the output is the set of objects in an image that the expression denotes.", "labels": [], "entities": []}, {"text": "The images are annotated with a bounding box for each candidate object.", "labels": [], "entities": []}, {"text": "The data set includes a domain theory that was automatically generated by creating a category and/or relation per word based on its part of speech.", "labels": [], "entities": []}, {"text": "It also includes a CCG lexicon and image features.", "labels": [], "entities": []}, {"text": "We use these resources, adding predicate and denotation features..5 compares P 3 to prior work on SCENE.", "labels": [], "entities": []}, {"text": "The evaluation metric is exact match accuracy between the predicted and labeled sets of objects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.5498639345169067}]}, {"text": "We consider three supervision conditions: QA trains with question/answer pairs, QA+E further includes labeled environments, and QA+E+LF further includes labeled logical forms.", "labels": [], "entities": []}, {"text": "We trained P 3 in the first two conditions, while prior work trained in the first and third conditions.", "labels": [], "entities": []}, {"text": "KK2013 is a possible worlds model with a max-margin training objective.", "labels": [], "entities": [{"text": "KK2013", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9536733627319336}]}, {"text": "P 3 slightly outperforms in the QA condition and P 3", "labels": [], "entities": [{"text": "QA", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.7792273163795471}]}], "tableCaptions": [{"text": " Table 1: Accuracy of P 3 and several baselines on the FOOD-", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988409876823425}, {"text": "FOOD", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.6857087016105652}]}, {"text": " Table 2: Test set accuracy of P 3 removing LSTM answer se- lection (Section 4.4), denotation features and predicate features  (Section 4.3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9842932820320129}]}, {"text": " Table 3: Accuracy of P 3 when trained and evaluated with la- beled logical forms, food webs, or both.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981973767280579}]}, {"text": " Table 4: Accuracy on the SCENE data set. KK2013 results are  from Krishnamurthy and Kollar (2013).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984703660011292}, {"text": "SCENE data set", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.7914160490036011}, {"text": "KK2013", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.7488820552825928}]}]}