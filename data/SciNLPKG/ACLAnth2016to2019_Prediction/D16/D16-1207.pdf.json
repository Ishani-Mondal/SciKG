{"title": [], "abstractContent": [{"text": "Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks.", "labels": [], "entities": []}, {"text": "We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust.", "labels": [], "entities": []}, {"text": "Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep learning has achieved state-of-the-art results across a range of computer vision (, speech recognition () and natural language processing tasks (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7098961919546127}]}, {"text": "However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (.", "labels": [], "entities": []}, {"text": "argued that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization.", "labels": [], "entities": []}, {"text": "provided a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and also showed linear models are usually not robust to adversarial noise.", "labels": [], "entities": []}, {"text": "In this work, we present a regularization method which makes deep learning models more robust to noise, inspired by.", "labels": [], "entities": []}, {"text": "The intuition behind the approach is to stabilize predictions by minimizing the ability of features to perturb predictions, based on high-order derivatives.", "labels": [], "entities": []}, {"text": "introduced contractive auto-encoders based on similar ideas, using the Frobenius norm of the Jacobian matrix as a penalty term to extract robust features.", "labels": [], "entities": []}, {"text": "Further, introduced deep contractive networks, generalizing this idea to a feed-forward neural network.", "labels": [], "entities": []}, {"text": "Also related, Martens (2010) investigated a second-order optimization method based on Hessian-free approach for training deep auto-encoders.", "labels": [], "entities": []}, {"text": "Where our proposed approach differs is that we train models using first-order derivatives of the training loss as part of a regularization term, necessitating second-order derivatives for computing the gradient.", "labels": [], "entities": []}, {"text": "We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets) with a convolutional neural network model.", "labels": [], "entities": []}, {"text": "In this, we show that our method is superior to dropout) and a baseline method using MAP training.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment on the following datasets, 2 following Kim (2014): \u2022", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy (%) with increasing word-level dropout across the four datasets. For each dataset, we apply four levels of noise", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981557726860046}]}, {"text": " Table 2: Accuracy under cross-domain evaluation; the best", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9978660941123962}]}]}