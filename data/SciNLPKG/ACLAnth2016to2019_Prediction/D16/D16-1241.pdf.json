{"title": [{"text": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "labels": [], "entities": []}], "abstractContent": [{"text": "We have constructed anew \"Who-did-What\" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gi-gaword newswire corpus.", "labels": [], "entities": [{"text": "LDC English Gi-gaword newswire corpus", "start_pos": 156, "end_pos": 193, "type": "DATASET", "confidence": 0.875969409942627}]}, {"text": "The WDW dataset has a variety of novel features.", "labels": [], "entities": [{"text": "WDW dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9251398146152496}]}, {"text": "First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation.", "labels": [], "entities": [{"text": "Daily Mail datasets", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.8401898145675659}, {"text": "question formation", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8465050756931305}]}, {"text": "Instead, each problem is formed from two independent articles-an article given as the passage to be read and a separate article on the same events used to form the question.", "labels": [], "entities": []}, {"text": "Second , we avoid anonymization-each choice is a person named entity.", "labels": [], "entities": []}, {"text": "Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans.", "labels": [], "entities": []}, {"text": "We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.", "labels": [], "entities": [{"text": "WDW dataset", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9562428891658783}]}], "introductionContent": [{"text": "Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (.", "labels": [], "entities": [{"text": "general knowledge question answering", "start_pos": 39, "end_pos": 75, "type": "TASK", "confidence": 0.698705866932869}]}, {"text": "Reading comprehension is more difficult than knowledge-based or IR-based question answering in two ways.", "labels": [], "entities": [{"text": "Reading comprehension", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8765108287334442}, {"text": "IR-based question answering", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.7819466392199198}]}, {"text": "First, reading comprehension systems must infer answers from a given unstructured passage rather than structured knowledge sources such as Freebase ( or the Google Knowledge Graph.", "labels": [], "entities": []}, {"text": "Second, machine comprehension systems cannot exploit the large level of redundancy present on the web to find statements that provide a strong syntactic match to the question ().", "labels": [], "entities": []}, {"text": "In contrast, a machine comprehension system must use the single phrasing in the given passage, which maybe a poor syntactic match to the question.", "labels": [], "entities": []}, {"text": "In this paper, we describe the construction of anew reading comprehension dataset that we refer to as \"Who-did-What\".", "labels": [], "entities": []}, {"text": "Two typical examples are shown in.", "labels": [], "entities": []}, {"text": "The process of forming a problem starts with the selection of a question article from the English Gigaword corpus.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 90, "end_pos": 113, "type": "DATASET", "confidence": 0.8403504689534506}]}, {"text": "The question is formed by deleting a person named entity from the first sentence of the question article.", "labels": [], "entities": []}, {"text": "An information retrieval system is then used to select a passage with high overlap with the first sentence of the question article, and an answer choice list is generated from the person named entities in the passage.", "labels": [], "entities": []}, {"text": "Our dataset differs from the CNN and Daily Mail comprehension tasks () in that it forms questions from two distinct articles rather than summary points.", "labels": [], "entities": [{"text": "CNN and Daily Mail comprehension tasks", "start_pos": 29, "end_pos": 67, "type": "DATASET", "confidence": 0.7525232136249542}]}, {"text": "This allows problems to be derived from document collections that do not contain manually-written summaries.", "labels": [], "entities": []}, {"text": "This also reduces the syntactic similarity between the question and the relevant sentences in the passage, increasing the need for deeper semantic analysis.", "labels": [], "entities": []}, {"text": "To make the dataset more challenging we selectively remove problems so as to suppress four simple Passage: Britain's decision on Thursday to drop extradition proceedings against Gen. Augusto Pinochet and allow him to return to Chile is understandably frustrating ...", "labels": [], "entities": []}, {"text": "Jack Straw, the home secretary, said the 84-year-old former dictator's ability to understand the charges against him and to direct his defense had been seriously impaired by a series of strokes.", "labels": [], "entities": []}, {"text": "Chile's president-elect, Ricardo Lagos, has wisely pledged to let justice run its course.", "labels": [], "entities": []}, {"text": "But the outgoing government of President Eduardo Frei is pushing a constitutional reform that would allow Pinochet to step down from the Senate and retain parliamentary immunity from prosecution.", "labels": [], "entities": []}, {"text": "Question: Sources close to the presidential palace said that Fujimori declined at the last moment to leave the country and instead he will send a high level delegation to the ceremony, at which Chilean President Eduardo Frei will pass the mandate to XXX.", "labels": [], "entities": []}, {"text": "baselines -selecting the most mentioned person, the first mentioned person, and two language model baselines.", "labels": [], "entities": []}, {"text": "This is also intended to produce problems requiring deeper semantic analysis.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7341882586479187}]}, {"text": "The resulting dataset yields a larger gap between human and machine performance than existing ones.", "labels": [], "entities": []}, {"text": "Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN () and 82% for the CBT named entities task (.", "labels": [], "entities": [{"text": "CNN", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.9221811294555664}, {"text": "CBT named entities task", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.519499272108078}]}, {"text": "In spite of this higher level of human performance, various existing readers perform significantly worse on our dataset than they do on the CNN dataset.", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.9601236879825592}]}, {"text": "For example, the Attentive Reader () achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader () achieves 70% on CNN but only 59% on Whodid-What.", "labels": [], "entities": [{"text": "CNN", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9564240574836731}, {"text": "CNN", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.9349091649055481}, {"text": "Whodid-What", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.9664013385772705}]}, {"text": "In summary, we believe that our Who-did-What dataset is more challenging, and requires deeper semantic analysis, than existing datasets.", "labels": [], "entities": [{"text": "Who-did-What dataset", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.7481256723403931}]}], "datasetContent": [{"text": "We now describe the construction of our Who-didWhat dataset in more detail.", "labels": [], "entities": [{"text": "Who-didWhat dataset", "start_pos": 40, "end_pos": 59, "type": "DATASET", "confidence": 0.9050045311450958}]}, {"text": "To generate a problem we first generate the question by selecting a random article -the \"question article\" -from the Gigaword corpus and taking the first sentence of that article -the \"question sentence\" -as the source of the cloze question.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.95284104347229}]}, {"text": "The hope is that the first sentence of an article contains prominent people and events which are likely to be discussed in other independent articles.", "labels": [], "entities": []}, {"text": "To convert the question sentence to a cloze question, we first extract named entities using the Stanford NER system () and parse the sentence using the Stanford PCFG parser (.", "labels": [], "entities": [{"text": "Stanford NER system", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.9139295021692911}, {"text": "Stanford PCFG parser", "start_pos": 152, "end_pos": 172, "type": "DATASET", "confidence": 0.8970503409703573}]}, {"text": "The person named entities are candidates for deletion to create a cloze problem.", "labels": [], "entities": []}, {"text": "For each person named entity we then identify a noun phrase in the automatic parse that is headed by that person.", "labels": [], "entities": []}, {"text": "For example, if the question sentence is \"President Obama met yesterday with Apple Founder Steve Jobs\" we identify the two person noun phrases \"President Obama\" and \"Apple Founder Steve Jobs\".", "labels": [], "entities": []}, {"text": "When a person named entity is selected for deletion, the entire noun phrase is deleted.", "labels": [], "entities": []}, {"text": "For example, when deleting the second named entity, we get \"President Obama met yesterday with XXX\" rather than \"President Obama met yesterday with Apple founder XXX\".", "labels": [], "entities": []}, {"text": "This increases the difficulty of the problems because systems cannot rely on descriptors and other local contextual cues.", "labels": [], "entities": []}, {"text": "About 700,000 question sentences are generated from Gigaword articles (8% of the total number of articles).", "labels": [], "entities": []}, {"text": "Once a cloze question has been formed we select an appropriate article as a passage.", "labels": [], "entities": []}, {"text": "The article should be independent of the question article but should discuss the people and events mentioned in the question sentence.", "labels": [], "entities": []}, {"text": "To find a passage we search the Gigaword dataset using the Apache Lucene information retrieval system, using the question sentence as the query.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9690259397029877}]}, {"text": "The named entity to be deleted is included in the query and required to be included in the returned article.", "labels": [], "entities": []}, {"text": "We also restrict the search to articles published within two weeks of the date of the question article.", "labels": [], "entities": []}, {"text": "Articles containing sentences too similar to the question in word overlap and phrase matching near the blanked phrase are removed.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.641149640083313}, {"text": "phrase matching", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.6999491155147552}]}, {"text": "We select the best matching article satisfying our constraints.", "labels": [], "entities": []}, {"text": "If no such article can be found, we abort the process and move onto anew question.", "labels": [], "entities": []}, {"text": "Given a question and a passage we next form the list of choices.", "labels": [], "entities": []}, {"text": "We collect all person named entities in the passage except unblanked person named entities in the question.", "labels": [], "entities": []}, {"text": "Choices that are subsets of longer choices are eliminated.", "labels": [], "entities": []}, {"text": "For example the choice \"Obama\" would be eliminated if the list also contains \"Barack Obama\".", "labels": [], "entities": []}, {"text": "We also discard ambiguous cases where apart of a blanked NE appears in multiple candidate answers, e.g., if a passage has \"Bill Clinton\" and \"Hillary Clinton\" and the blanked phrase is \"Clinton\".", "labels": [], "entities": []}, {"text": "We found this simple coreference rule to work well in practice since news articles usually employ full names for initial mentions of persons.", "labels": [], "entities": []}, {"text": "If the resulting choice list contains fewer than two or more than five choices, the process is aborted and we move onto anew question.", "labels": [], "entities": []}, {"text": "After forming an initial set of problems we then remove \"duplicated\" problems.", "labels": [], "entities": []}, {"text": "Duplication arises because Gigaword contains many copies of the same article or articles where one is clearly an edited version of another.", "labels": [], "entities": []}, {"text": "Our duplication-removal process ensures that no two problems have very similar questions.", "labels": [], "entities": []}, {"text": "Here, similarity is defined as the ratio of the size of the bag of words intersection to the size of the smaller bag.", "labels": [], "entities": [{"text": "similarity", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9869322180747986}]}, {"text": "In order to focus our dataset on the most interesting problems, we remove some problems to suppress the performance of the following simple baselines: \u2022 First person in passage: Select the person that appears first in the passage.", "labels": [], "entities": []}, {"text": "\u2022 Most frequent person: Select the most frequent person in the passage.", "labels": [], "entities": []}, {"text": "\u2022 n-gram: Select the most likely answer to fill the blank under a 5-gram language model trained on Gigaword minus articles which are too similar to one of the questions in word overlap and phrase matching.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 172, "end_pos": 184, "type": "TASK", "confidence": 0.7348630130290985}, {"text": "phrase matching", "start_pos": 189, "end_pos": 204, "type": "TASK", "confidence": 0.7681411504745483}]}, {"text": "\u2022 Unigram: Select the most frequent last name using the unigram counts from the 5-gram model.", "labels": [], "entities": []}, {"text": "To minimize the number of questions removed we solve an optimization problem defined by limiting the performance of each baseline to a specified target value while removing as few problems as possible, i.e., max subject to where T (C) is the subset of the questions solved by the subset C of the suppressed baselines, \u03b1(C) is a keeping rate for question set T (C), Ci = 1 indicates the i-th baseline is in the subset, |b| is the number of baselines, N is a total number of questions, and k is the upper bound for the baselines after suppression.", "labels": [], "entities": []}, {"text": "We choose k to yield random performance for the baselines.", "labels": [], "entities": []}, {"text": "The performance of the baselines before and after suppression is shown in shows statistics of our dataset after suppression.", "labels": [], "entities": []}, {"text": "We split the final dataset into train, validation, and test by taking the validation and test to be a random split of the most recent 20,000 problems as measured by question article date.", "labels": [], "entities": []}, {"text": "In this way there is very little overlap in semantic subject matter between the training set and either validation or test.", "labels": [], "entities": []}, {"text": "We also provide a larger \"relaxed\" training set formed by applying less baseline suppression (a larger value of kin the optimization).", "labels": [], "entities": []}, {"text": "The relaxed training set then has a slightly different distribution from the train, validation, and test sets which are all fully suppressed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of suppressed baselines.  *  Random per-", "labels": [], "entities": [{"text": "Random per-", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9225982626279196}]}, {"text": " Table 4: System performance on test set. Human performance", "labels": [], "entities": []}]}