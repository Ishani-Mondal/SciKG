{"title": [{"text": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7349861860275269}, {"text": "Automatic Grammatical Error Correction", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.7069122642278671}]}], "abstractContent": [{"text": "In this work, we study parameter tuning towards the M 2 metric, the standard metric for automatic grammar error correction (GEC) tasks.", "labels": [], "entities": [{"text": "automatic grammar error correction (GEC) tasks", "start_pos": 88, "end_pos": 134, "type": "TASK", "confidence": 0.7180375196039677}]}, {"text": "After implementing M 2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task.", "labels": [], "entities": []}, {"text": "We notice erratic behavior when optimizing sparse feature weights with M 2 and offer partial solutions.", "labels": [], "entities": []}, {"text": "We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning out-performs all previously published results for the CoNLL-2014 test set by a large margin (46.37% M 2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9147874116897583}, {"text": "CoNLL-2014 test set", "start_pos": 139, "end_pos": 158, "type": "DATASET", "confidence": 0.9054312109947205}, {"text": "M 2", "start_pos": 185, "end_pos": 188, "type": "METRIC", "confidence": 0.967058926820755}]}, {"text": "Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M 2 .", "labels": [], "entities": [{"text": "M 2", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9891574680805206}]}], "introductionContent": [{"text": "Statistical machine translation (SMT), especially the phrase-based variant, is well established in the field of automatic grammatical error correction (GEC) and systems that are either pure SMT or incorporate SMT as system components occupied top positions in GEC shared tasks for different languages.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8339535295963287}, {"text": "automatic grammatical error correction (GEC)", "start_pos": 112, "end_pos": 156, "type": "TASK", "confidence": 0.7436833594526563}]}, {"text": "With the recent paradigm shift in machine translation towards neural translation models, neural encoder-decoder models are expected to appear in the field of GEC as well, and first published results () already look promising.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7373222708702087}, {"text": "neural translation", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7623665630817413}, {"text": "GEC", "start_pos": 158, "end_pos": 161, "type": "TASK", "confidence": 0.8414551615715027}]}, {"text": "As it is the casein classical bilingual machine translation research, these models should be compared against strong SMT baselines.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7234008014202118}, {"text": "SMT", "start_pos": 117, "end_pos": 120, "type": "TASK", "confidence": 0.9803391098976135}]}, {"text": "Similarly, system combinations of SMT with classifier-based approaches) suffer from unnecessarily weak MT base systems which make it hard to assess how large the contribution of the classifier pipelines really is.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9794619679450989}]}, {"text": "In this work we provide these baselines.", "labels": [], "entities": []}, {"text": "During our experiments, we find that a bare-bones phrase-based system outperforms the best published results on the CoNLL-2014 test set by a significant margin only due to a task-specific parameter tuning when being trained on the same data as previous systems.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 116, "end_pos": 135, "type": "DATASET", "confidence": 0.9379370212554932}]}, {"text": "When we further investigate the influence of well-known SMT-specific features and introduce new features adapted to the problem of GEC, our final systems outperform the best reported results by 8% M 2 , moving the state-of-the-art results for the CoNLL-2014 test set from 41.75% M 2 to 49.49%.", "labels": [], "entities": [{"text": "SMT-specific", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.9818936586380005}, {"text": "GEC", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.7621852159500122}, {"text": "CoNLL-2014 test set", "start_pos": 247, "end_pos": 266, "type": "DATASET", "confidence": 0.9454669753710429}]}, {"text": "The paper is organized as follows: section 2 describes previous work, the CoNLL-2014 shared tasks on GEC and follow-up papers.", "labels": [], "entities": [{"text": "GEC", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.7492440938949585}]}, {"text": "Our main contributions are presented in sections 3 and 4 where we investigate the interaction of parameter tuning towards the M 2 metric with task-specific dense and sparse features.", "labels": [], "entities": []}, {"text": "Especially tuning for sparse features is more challenging than initially expected, but we describe optimizer hyper-parameters that make sparse feature tuning with M 2 feasible.", "labels": [], "entities": []}, {"text": "Section 5 reports on the effects of adding a web-scale n-gram language model to our models.", "labels": [], "entities": []}, {"text": "Scripts and models used in this paper are available from https://github.com/grammatical/ baselines-emnlp2016 to facilitate reproducibility of our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system is based on the phrase-based part of the statistical machine translation system Moses (.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.6344190637270609}]}, {"text": "Only plain text data is used for language model and translation model training.", "labels": [], "entities": [{"text": "translation model training", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.9324698448181152}]}, {"text": "External linguistic knowledge is introduced during parameter tuning as the tuning metric relies on the error annotation present in NUCLE.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7498829066753387}, {"text": "NUCLE", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.9297440052032471}]}, {"text": "The translation model is built with the standard Moses training script, word-alignment models are produced with MGIZA++ (, we restrict the word alignment training to 5 iterations of Model 1 and 5 iterations of the HMM-Model.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9674698710441589}, {"text": "MGIZA", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.6418066620826721}, {"text": "word alignment", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.7679460346698761}]}, {"text": "No reordering models are used, the distortion limit is set to 0, effectively prohibiting any reordering.", "labels": [], "entities": []}, {"text": "All systems use one 5-gram language model that has been estimated from the target side of the parallel data available for translation model training.", "labels": [], "entities": [{"text": "translation model training", "start_pos": 122, "end_pos": 148, "type": "TASK", "confidence": 0.9089853763580322}]}, {"text": "Another 5-gram language model trained on Wikipedia in the restricted setting or on Common Crawl in the unrestricted case.", "labels": [], "entities": []}, {"text": "Systems are retuned when new features of any type are added.", "labels": [], "entities": []}, {"text": "We first successfully reproduce results from  for BLEU-based tuning on the CoNLL-2013 test set as the development set  Even with BLEU-based tuning, we can see significant improvements when replacing Levenshtein distance with the finer-grained edit operations, and another performance jump with additional stateful features.", "labels": [], "entities": [{"text": "BLEU-based", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9728133082389832}, {"text": "CoNLL-2013 test set", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.9877435366312662}, {"text": "BLEU-based", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9597387313842773}]}, {"text": "The value range of the different tuning runs for the last feature set includes the currently bestperforming system (Xie et al. with 40.56%), but the result for the averaged centroid are inferior.", "labels": [], "entities": []}, {"text": "Tuning directly with M 2 and averaging weights across five iterations, yields between 40.66% M 2 fora vanilla Moses system and 42.32% fora system with all described dense features.", "labels": [], "entities": []}, {"text": "Results seen to be more stable.", "labels": [], "entities": []}, {"text": "Averaging weight vectors across runs to produce the final vector seems like a fair bet.", "labels": [], "entities": []}, {"text": "Performance with the averaged weight vectors is either similar to or better than the average number for five runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word-based Levenshtein distance (LD) fea- ture and separated edit operations (D = deletions, I  = insertions, S = substitutions)", "labels": [], "entities": [{"text": "Levenshtein distance (LD) fea- ture", "start_pos": 21, "end_pos": 56, "type": "METRIC", "confidence": 0.8741101622581482}]}, {"text": " Table 2: Parallel (above line) and monolingual train- ing data.", "labels": [], "entities": []}, {"text": " Table 3: Tuning with different optimizers with dense  features only, results are given for the CoNLL-2013  and CoNLL-2014 test set", "labels": [], "entities": [{"text": "CoNLL-2013", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9377166628837585}, {"text": "CoNLL-2014 test set", "start_pos": 112, "end_pos": 131, "type": "DATASET", "confidence": 0.9113554159800211}]}, {"text": " Table 4: Best results in restricted setting with added unrestricted language model for original (2014) and  extended (2014-10) CoNLL test set (trained with public data only).", "labels": [], "entities": [{"text": "CoNLL test set", "start_pos": 128, "end_pos": 142, "type": "DATASET", "confidence": 0.9281824827194214}]}, {"text": " Table 5: Previous best systems trained with non- public (np) error-corrected data for comparison with  Rozovskaya and Roth (2016) denoted as R&R.", "labels": [], "entities": []}]}