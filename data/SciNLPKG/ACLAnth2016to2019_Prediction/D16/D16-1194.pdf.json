{"title": [{"text": "Non-uniform Language Detection in Technical Writing", "labels": [], "entities": [{"text": "Non-uniform Language Detection in Technical Writing", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6816234340270361}]}], "abstractContent": [{"text": "Technical writing in professional environments , such as user manual authoring, requires the use of uniform language.", "labels": [], "entities": [{"text": "user manual authoring", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.6064813733100891}]}, {"text": "Non-uniform language detection is a novel task, which aims to guarantee the consistency for technical writing by detecting sentences in a document that are intended to have the same meaning within a similar context but use different words or writing style.", "labels": [], "entities": [{"text": "Non-uniform language detection", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6978476842244467}, {"text": "consistency", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9791734218597412}]}, {"text": "This paper proposes an approach that utilizes text similarity algorithms at lexical, syntactic, semantic and pragmatic levels.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7102028280496597}]}, {"text": "Different features are extracted and integrated by applying a machine learning classification method.", "labels": [], "entities": []}, {"text": "We tested our method using smartphone user manuals, and compared its performance against the state-of-the-art methods in a related area.", "labels": [], "entities": []}, {"text": "The experiments demonstrate that our approach achieves the upper bound performance for this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Technical writing, such as creating device operation manuals and user guide handbooks, is a special writing task that requires accurate text to describe a certain product or operation.", "labels": [], "entities": [{"text": "Technical writing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7149873971939087}]}, {"text": "To avoid ambiguity and bring accurate and straightforward understanding to readers, technical writing requires consistency in the use of terminology and uniform language.", "labels": [], "entities": []}, {"text": "There are always demands from modern industries to improve the quality of technical documents in cost-efficient ways.", "labels": [], "entities": []}, {"text": "Non-uniform Language Detection (NLD) aims to avoid inner-inconsistency and ambiguity of technical content by identifying non-uniform sentences.", "labels": [], "entities": [{"text": "Non-uniform Language Detection (NLD)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8206287026405334}]}, {"text": "Such sentences are intended to have the same meaning or usage within a similar context but use different words or writing style.", "labels": [], "entities": []}, {"text": "However, even though non-uniform sentences tend to have similar wording, similar sentence pairs do not necessarily indicate a non-uniform language instance.", "labels": [], "entities": []}, {"text": "For example, here are four similar sentence pairs cited from the iPhone user manual, where only two pairs are true non-uniform language instances: (1) tap the screen to show the controls.", "labels": [], "entities": []}, {"text": "tap the screen to display the controls.", "labels": [], "entities": []}, {"text": "(2) tap the screen to show the controls.", "labels": [], "entities": []}, {"text": "tap the screen to display the controls.", "labels": [], "entities": []}, {"text": "(3) if the photo hasn't been downloaded yet, tap the download notice first.", "labels": [], "entities": []}, {"text": "if the video hasn't been downloaded yet, tap the download notice first.", "labels": [], "entities": []}, {"text": "(4) you can also turn blue tooth on or off in control center.", "labels": [], "entities": []}, {"text": "you can also turn wi-fi and blue tooth on or off in control center.", "labels": [], "entities": []}, {"text": "As we can see above, the pattern of difference within each sentence pair could be between one word and one word, or one word and multiple words, or one sentence having extra words or phrases that the other sentence does not have.", "labels": [], "entities": []}, {"text": "Each pattern could be a true or false non-uniform language instance depending on the content and context.", "labels": [], "entities": []}, {"text": "The word 'show' and 'display' are synonyms in Example (1).", "labels": [], "entities": [{"text": "Example", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9038628935813904}]}, {"text": "Both sentences convey the same meaning, so they are an instance of non-uniform language.", "labels": [], "entities": []}, {"text": "In Example (2), even though 'enter' and 'write' are not synonyms, since the two sentences describe the same operation, they should be considered as non-uniform language as well.", "labels": [], "entities": []}, {"text": "In Example (3), even though the only different words between the sentences, 'photo' and 'video', are both media contents, because they are different objects, they should not be regarded as non-uniform language.", "labels": [], "entities": []}, {"text": "In Example (4), it is a false candidate because each sentence mentions different functions.", "labels": [], "entities": []}, {"text": "However, the two sentences are unequal in length, thus it is hard to know what the extra phrase 'wi-fi and' should be compared against.", "labels": [], "entities": []}, {"text": "Therefore, it is challenging to distinguish true and false occurrences of non-uniform cases based on text similarity algorithms only, and finer grained analyses need to be applied.", "labels": [], "entities": []}, {"text": "To address the problem of NLD, this paper proposes a methodology for detecting non-uniform language within a technical document at the sentence level.", "labels": [], "entities": []}, {"text": "A schematic diagram of our approach is shown in.", "labels": [], "entities": []}, {"text": "It is worth to mention that NLD is similar to Plagiarism Detection and Paraphrase Detection (PD) as all these tasks aim to capture similar sentences with the same meaning (.", "labels": [], "entities": [{"text": "Plagiarism Detection and Paraphrase Detection (PD)", "start_pos": 46, "end_pos": 96, "type": "TASK", "confidence": 0.7422460876405239}]}, {"text": "However, the goal of authors in plagiarism and paraphrasing is to change as many words as possible to increase the differences between texts, whereas in technical writing, the authors try to avoid such differences, but they do not always succeed and thus NLD solutions are needed.", "labels": [], "entities": []}, {"text": "Cases of plagiarism and paraphrasing with high lexical differences will be typically classified as NLD negative, and cases with low lexical differences will be typically classified as NLD positive.", "labels": [], "entities": []}, {"text": "While true positive cases for both NLD and PD can exist, there are not likely to happen in practice since textual differences in PD tend to be much higher than in NLD.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the dataset, experimental work and results, including results using other baseline methods for comparative purposes.", "labels": [], "entities": []}, {"text": "We downloaded smartphone user manuals of iPhone (Apple Inc., 2015), LG (LG, 2009) and Samsung, which are available online as three raw datasets.", "labels": [], "entities": [{"text": "LG (LG, 2009)", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9066742161909739}]}, {"text": "Then, we performed Stage 1 three times on the three different datasets, and identified 325 candidate sentence pairs (650 sentences) as part of Stage 1, which is considered as our candidate dataset.", "labels": [], "entities": []}, {"text": "Before applying the sentence analysis and classification stages, each candidate sentence pair in the dataset was labeled by three different annotators as true or false.", "labels": [], "entities": [{"text": "sentence analysis", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7261965572834015}]}, {"text": "Then the ground truth for each instance is generated by annotators' voting.", "labels": [], "entities": []}, {"text": "The annotators worked separately to label the sentence pairs.", "labels": [], "entities": []}, {"text": "Cases of disagreement were sent again to the annotators to double-check their judgement.", "labels": [], "entities": []}, {"text": "Some statistics from the manuals are shown in.", "labels": [], "entities": []}, {"text": "To prepare for the SVM based classification stage, we split the dataset into a training set DS train , and a testing set DS test . Considering that the data distribution is nearly balanced in terms of true and false instances, DS train was formed by randomly selecting 200 instances from the dataset and the remaining 125 instances were used for DS test .  The performance of each annotator against the majority voting is evaluated in terms of Precision, Recall, Accuracy, and F-measure.", "labels": [], "entities": [{"text": "SVM based classification", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.8543572028477987}, {"text": "Precision", "start_pos": 444, "end_pos": 453, "type": "METRIC", "confidence": 0.9995579123497009}, {"text": "Recall", "start_pos": 455, "end_pos": 461, "type": "METRIC", "confidence": 0.9963692426681519}, {"text": "Accuracy", "start_pos": 463, "end_pos": 471, "type": "METRIC", "confidence": 0.995810866355896}, {"text": "F-measure", "start_pos": 477, "end_pos": 486, "type": "METRIC", "confidence": 0.9992105960845947}]}, {"text": "These results along with the number of true/ false, positive/ negative cases for each annotator are presented in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Experiment data distribution", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of annotators performance", "labels": [], "entities": []}, {"text": " Table 6: Evaluation of NLDS", "labels": [], "entities": [{"text": "NLDS", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.5356756448745728}]}]}