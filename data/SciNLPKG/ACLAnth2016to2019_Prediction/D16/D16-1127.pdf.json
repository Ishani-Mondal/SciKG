{"title": [{"text": "Deep Reinforcement Learning for Dialogue Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted , predicting utterances one at a time while ignoring their influence on future outcomes.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8146950006484985}]}, {"text": "Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning.", "labels": [], "entities": []}, {"text": "In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chat-bot dialogue.", "labels": [], "entities": []}, {"text": "The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: infor-mativity, coherence, and ease of answering (re-lated to forward-looking function).", "labels": [], "entities": [{"text": "ease", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9889670014381409}]}, {"text": "We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation.", "labels": [], "entities": []}, {"text": "This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural response generation () is of growing interest.", "labels": [], "entities": [{"text": "Neural response generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9183047016461691}]}, {"text": "The LSTM sequence-to-sequence (SEQ2SEQ) model) is one type of neural generation model that maximizes the probability of generating a response given the previous dialogue turn.", "labels": [], "entities": []}, {"text": "This approach enables the incorporation of rich context when mapping between consecutive dialogue turns ( ) in away not possible, for example, with MT-based dialogue models.", "labels": [], "entities": [{"text": "MT-based dialogue", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.8914640545845032}]}, {"text": "Despite the success of SEQ2SEQ models in dialogue generation, two problems emerge: First, SEQ2SEQ models are trained by predicting the next dialogue turn in a given conversational context using the maximum-likelihood estimation (MLE) objective function.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8461567461490631}]}, {"text": "However, it is not clear how well MLE approximates the real-world goal of chatbot development: teaching a machine to converse with humans, while providing interesting, diverse, and informative feedback that keeps users engaged.", "labels": [], "entities": [{"text": "MLE", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9036843180656433}]}, {"text": "One concrete example is that SEQ2SEQ models tend to generate highly generic responses such as \"I don't know\" regardless of the input ().", "labels": [], "entities": []}, {"text": "This can be ascribed to the high frequency of generic responses found in the training set and their compatibility with a diverse range of conversational contexts.", "labels": [], "entities": []}, {"text": "Yet \"I don't know\" is apparently not a good action to take, since it closes the conversation down.", "labels": [], "entities": []}, {"text": "Another common problem, illustrated in the two sample conversations on the left of Table 1, is that the system becomes stuck in an infinite loop of repetitive responses.", "labels": [], "entities": []}, {"text": "This is due to MLE-based SEQ2SEQ models' inability to account for repetition.", "labels": [], "entities": [{"text": "repetition", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9727082848548889}]}, {"text": "In example 2 (bottom left), the dialogue falls into an infinite loop after three turns, with both agents generating dull, generic utterances like i don't know what you are talking about and you don't know what you are saying.", "labels": [], "entities": []}, {"text": "Looking at the entire conversation, utterance (4) turns out to be a bad action to take because it offers noway of continuing the conversation.", "labels": [], "entities": []}, {"text": "1: Left Column: Dialogue simulation between two agents using a 4-layer LSTM encoder-decoder trained on the OpenSubtitles dataset.", "labels": [], "entities": [{"text": "OpenSubtitles dataset", "start_pos": 107, "end_pos": 128, "type": "DATASET", "confidence": 0.9397734999656677}]}, {"text": "The first turn (index 1) is input by the authors.", "labels": [], "entities": []}, {"text": "Then the two agents take turns conversing, taking as input the other agent's prior generated turn.", "labels": [], "entities": []}, {"text": "The output is generated using the mutual information model () in which an N-best list is first obtained using beam search based on p(t|s) and reranked by linearly combining the backward probability p(s|t), where t and s respectively denote targets and sources.", "labels": [], "entities": []}, {"text": "Right Column: Dialogue simulated using the proposed reinforcement learning model.", "labels": [], "entities": []}, {"text": "The new model has more forward-looking utterances (questions like \"Why are you asking?\" and offers like \"I'll come with you\") and lasts longer before it falls into conversational black holes.", "labels": [], "entities": []}, {"text": "These challenges suggest we need a conversation framework that has the ability to (1) integrate developer-defined rewards that better mimic the true goal of chatbot development and (2) model the longterm influence of a generated response in an ongoing dialogue.", "labels": [], "entities": []}, {"text": "To achieve these goals, we draw on the insights of reinforcement learning, which have been widely applied in MDP and POMDP dialogue systems (see Related Work section for details).", "labels": [], "entities": []}, {"text": "We introduce a neural reinforcement learning (RL) generation method, which can optimize long-term rewards designed by system developers.", "labels": [], "entities": [{"text": "neural reinforcement learning (RL) generation", "start_pos": 15, "end_pos": 60, "type": "TASK", "confidence": 0.7427450929369245}]}, {"text": "Our model uses the encoderdecoder architecture as its backbone, and simulates conversation between two virtual agents to explore the space of possible actions while learning to maximize expected reward.", "labels": [], "entities": []}, {"text": "We define simple heuristic approximations to rewards that characterize good conversations: good conversations are forward-looking ( or interactive (a turn suggests a following turn), informative, and coherent.", "labels": [], "entities": []}, {"text": "The parameters of an encoder-decoder RNN define a policy over an infinite action space consisting of all possible utterances.", "labels": [], "entities": []}, {"text": "The agent learns a policy by optimizing the long-term developer-defined reward from ongoing dialogue simulations using policy gradient methods, rather than the MLE objective defined in standard SEQ2SEQ models.", "labels": [], "entities": []}, {"text": "Our model thus integrates the power of SEQ2SEQ systems to learn compositional semantic meanings of utterances with the strengths of reinforcement learning in optimizing for long-term goals across a conversation.", "labels": [], "entities": []}, {"text": "Experimental results (sampled results at the right panel) demonstrate that our approach fosters a more sustained dialogue and manages to produce more interactive responses than standard SEQ2SEQ models trained using the MLE objective.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe experimental results along with qualitative analysis.", "labels": [], "entities": []}, {"text": "We evaluate dialogue generation systems using both human judgments and two automatic metrics: conversation length (number of turns in the entire session) and diversity.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8350339233875275}]}, {"text": "The dialogue simulation requires high-quality initial inputs fed to the agent.", "labels": [], "entities": []}, {"text": "For example, an initial input of \"why ?\" is undesirable since it is unclear how the dialogue could proceed.", "labels": [], "entities": []}, {"text": "We take a subset of 10 million messages from the OpenSubtitles dataset and extract 0.8 million sequences with the lowest likelihood of generating the response \"i don't know what you are taking about\" to ensure initial inputs are easy to respond to.", "labels": [], "entities": [{"text": "OpenSubtitles dataset", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.9637613594532013}]}, {"text": "Evaluating dialogue systems is difficult.", "labels": [], "entities": [{"text": "Evaluating dialogue", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9083935916423798}]}, {"text": "Metrics such as BLEU () and perplexity have been widely used for dialogue quality evaluation (), but it is widely debated how well these automatic metrics are correlated with true response quality ( . Since the goal of the proposed system is not to predict the highest probability response, but rather the long-term success of the dialogue, we do not employ BLEU or perplexity for evaluation 2 .  We explore three settings for human evaluation: the first setting is similar to what was described in, where we employ crowdsourced judges to evaluate a random sample of 500 items.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9992625117301941}, {"text": "BLEU", "start_pos": 358, "end_pos": 362, "type": "METRIC", "confidence": 0.9978777170181274}]}, {"text": "We present both an input message and the generated outputs to 3 judges and ask them to decide which of the two outputs is better (denoted as singleturn general quality).", "labels": [], "entities": []}, {"text": "Identical strings are assigned the same score.", "labels": [], "entities": []}, {"text": "We measure the improvement achieved by the RL model over the mutual information model by the mean difference in scores between the models.", "labels": [], "entities": []}, {"text": "For the second setting, judges are again presented with input messages and system outputs, but are asked to decide which of the two outputs is easier to respond to (denoted as single-turn ease to answer).", "labels": [], "entities": []}, {"text": "Again we evaluate a random sample of 500 items, each being assigned to 3 judges.", "labels": [], "entities": []}, {"text": "For the third setting, judges are presented with simulated conversations between the two agents (denoted as multi-turn general quality).", "labels": [], "entities": []}, {"text": "Each conversation consists of 5 turns.", "labels": [], "entities": []}, {"text": "We evaluate 200 simulated conversations, each being assigned to 3 judges, who are asked to decide which of the simulated conversations is of higher quality.", "labels": [], "entities": []}, {"text": "Results for human evaluation are shown in.", "labels": [], "entities": []}, {"text": "The proposed RL system does not introduce a significant boost in single-turn response quality (winning 40 percent of time and losing 36 percent of time).", "labels": [], "entities": [{"text": "RL", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9567524194717407}]}, {"text": "This is inline with our expectations, as the RL model is not optimized to predict the next utterance, but rather to increase long-term reward.", "labels": [], "entities": []}, {"text": "The RL system produces responses that are significantly easier to answer than does the mutual information system, as demonstrated by the single-turn ease to answer setting (winning 52 percent of time and losing 23 percent of time), and also significantly higher quality multi-turn dialogues, as demonstrated by the multiturn general quality setting (winning 72 percent of time).", "labels": [], "entities": [{"text": "ease", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9464957118034363}]}, {"text": "Qualitative Analysis and Discussion We show a random sample of generated responses in and simulated conversations in    responses than the other baselines.", "labels": [], "entities": [{"text": "Qualitative Analysis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7881496548652649}]}, {"text": "We also find that the RL model has a tendency to end a sentence with another question and hand the conversation over to the user.", "labels": [], "entities": []}, {"text": "From, we observe that the RL model manages to produce more interactive and sustained conversations than the mutual information model.", "labels": [], "entities": []}, {"text": "During error analysis, we found that although we penalize repetitive utterances in consecutive turns, the dialogue sometimes enters a cycle with length greater than one, as shown in.", "labels": [], "entities": []}, {"text": "This can be ascribed to the limited amount of conversational history we consider.", "labels": [], "entities": []}, {"text": "Another issue observed is that the model sometimes starts a less relevant topic during the conversation.", "labels": [], "entities": []}, {"text": "There is a tradeoff between relevance and less repetitiveness, as manifested in the reward function we define in Eq 4.", "labels": [], "entities": []}, {"text": "The fundamental problem, of course, is that the manually defined reward function can't possibly cover the crucial aspects that define an ideal conversation.", "labels": [], "entities": []}, {"text": "While the heuristic rewards that we defined are amenable to automatic calculation, and do capture  some aspects of what makes a good conversation, ideally the system would instead receive real rewards from humans.", "labels": [], "entities": []}, {"text": "Another problem with the current model is that we can only afford to explore a very small number of candidates and simulated turns since the number of cases to consider grow exponentially.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Sampled responses generated from the mutual information models and the proposed RL model.", "labels": [], "entities": []}, {"text": " Table 5: RL gains over the mutual information sys- tem based on pairwise human judgments.", "labels": [], "entities": [{"text": "RL", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.7428879737854004}]}]}