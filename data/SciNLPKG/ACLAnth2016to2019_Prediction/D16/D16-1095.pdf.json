{"title": [{"text": "A General Regularization Framework for Domain Adaptation", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a domain adaptation framework, and formally prove that it generalizes the feature augmentation technique in (Daum\u00e9 III, 2007) and the multi-task regularization framework in (Evgeniou and Pontil, 2004).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7390190660953522}]}, {"text": "We show that our framework is strictly more general than these approaches and allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid negative transfer between distant ones.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain adaptation (DA) is an important problem that has received substantial attention in natural language processing.", "labels": [], "entities": [{"text": "Domain adaptation (DA)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8639755070209503}]}, {"text": "In this paper, we propose a novel regularization framework which allows DA practitioners to tune hyper-parameters to encourage transfer between close domains, and avoid negative transfer () between distant ones.", "labels": [], "entities": []}, {"text": "In our framework, model parameters in multiple domains are learned jointly and constrained to remain close to one another.", "labels": [], "entities": []}, {"text": "In the transfer learning taxonomy (, our framework falls under the parameter-transfer category for multi-task inductive learning.", "labels": [], "entities": []}, {"text": "We show that our framework generalizes the frustratingly easy domain adaptation (FEDA) in,, and the regularised multi-task learning of.", "labels": [], "entities": [{"text": "FEDA", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.7103795409202576}]}, {"text": "At the same time, it provides us with hyper-parameters to control the amount of transfer between domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we apply our framework to both structured and un-structured tasks.", "labels": [], "entities": []}, {"text": "For structured prediction, we use the named-entity recognition (NER) ACE-2005 dataset with 7 classes and 6 domains.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8231892585754395}, {"text": "named-entity recognition (NER)", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.8199327945709228}, {"text": "ACE-2005 dataset", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.8589979112148285}]}, {"text": "We apply the linear chain CRF (), and show results using standard and softmaxmargin CRF (SM-CRF) (, with features consisting of word shape features, neighboring words, previous prediction and prefixes/suffixes.", "labels": [], "entities": []}, {"text": "The second task is sentiment classification on the Amazon review data set) from 4 domains, labeled positive or negative.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.9664519727230072}, {"text": "Amazon review data set", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.9419207125902176}]}, {"text": "We apply logistic regression (LR) and SVM using unigram and bigram features.", "labels": [], "entities": []}, {"text": "All the models used in this section are implemented on top of a common framework, which was also used to implement various structured prediction models previously.", "labels": [], "entities": []}, {"text": "For each task we compare: TGT Trained only on the specific domain data, ALL Trained on the data from all domains, 1 They proved in Lemma 2.1 in their paper a similar relationship to Equation 4, but their proof assumes a SVM framework, and that \u03bb1=\u03bb2=.", "labels": [], "entities": [{"text": "TGT", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.8059455752372742}, {"text": "ALL", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9912080764770508}]}, {"text": "=\u03bbN .   AUG The FEDA approach, and RF Our proposed regularization framework.", "labels": [], "entities": [{"text": "AUG", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9952782392501831}, {"text": "FEDA", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.7897347211837769}, {"text": "RF", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9655800461769104}, {"text": "regularization", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.9679570198059082}]}, {"text": "We use a 40/30/30 train-development-test split and report the results on the test set.", "labels": [], "entities": []}, {"text": "The regularization parameters were tuned on the development set over a logarithmic scale between 10 \u22123 to 10 3 . For our framework, we used random search to tune the parameters, since an exhaustive search is too expensive (21 parameters for 6 domains).", "labels": [], "entities": []}, {"text": "We choose the within-domain \u03b7 0,i to be close to those used for the ALL and AUG model, while choosing the other \u03b7 j,k to be 1-2 orders of magnitude higher.", "labels": [], "entities": [{"text": "AUG", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.6070643067359924}]}, {"text": "A good model could quickly be found that generally beats the baselines on the development set and also generalizes well to the test set.", "labels": [], "entities": []}, {"text": "We show the results for NER in and the sentiment task in.", "labels": [], "entities": [{"text": "NER", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8147290349006653}]}], "tableCaptions": [{"text": " Table 1: F-score on the ACE NER task. The domains are  broadcast conversations (bc), broadcast news (bn), conversa- tional telephone speech (cts), newswire (nw), usenet (un) and  weblog (wl). The macro-average (avg) over the 6 domains is  also shown in the table.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9988287091255188}, {"text": "ACE NER task", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.4878689448038737}, {"text": "macro-average (avg)", "start_pos": 197, "end_pos": 216, "type": "METRIC", "confidence": 0.7233724594116211}]}, {"text": " Table 2: Accuracies on the sentiment classification task. The  domains are books (book), dvds (dvd), electronics (elec.) and  kitchen (kit.). The macro-average (avg) over the four domains  are also shown in the table.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.9434509873390198}, {"text": "macro-average (avg)", "start_pos": 147, "end_pos": 166, "type": "METRIC", "confidence": 0.677006408572197}]}]}