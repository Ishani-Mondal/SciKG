{"title": [], "abstractContent": [{"text": "This paper demonstrates that it is possible fora parser to improve its performance with a human in the loop, by posing simple questions to non-experts.", "labels": [], "entities": []}, {"text": "For example, given the first sentence of this abstract, if the parser is uncertain about the subject of the verb \"pose,\" it could generate the question What would pose something? with candidate answers this paper and a parser.", "labels": [], "entities": []}, {"text": "Any fluent speaker can answer this question, and the correct answer resolves the original uncertainty.", "labels": [], "entities": []}, {"text": "We apply the approach to a CCG parser, converting uncertain attachment decisions into natural language questions about the arguments of verbs.", "labels": [], "entities": []}, {"text": "Experiments show that crowd workers can answer these questions quickly, accurately and cheaply.", "labels": [], "entities": []}, {"text": "Our human-in-the-loop parser improves on the state of the art with less than 2 questions per sentence on average, with again of 1.7 F1 on the 10% of sentences whose parses are changed.", "labels": [], "entities": []}], "introductionContent": [{"text": "The size of labelled datasets has long been recognized as a bottleneck in the performance of natural language processing systems.", "labels": [], "entities": []}, {"text": "Such datasets are expensive to create, requiring expert linguists and extensive annotation guidelines.", "labels": [], "entities": []}, {"text": "Even relatively large datasets, such as the Penn Treebank, are much smaller than required-as demonstrated by improvements from semi-supervised learning.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9943910837173462}]}, {"text": "We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Temple also said Sea Containers' plan raises numerous legal, regulatory, financial and fairness issues, but didn't elaborate.", "labels": [], "entities": []}, {"text": "Q: What didn't elaborate?", "labels": [], "entities": []}, {"text": "**** Temple * Sea Containers' plan None of the above.", "labels": [], "entities": []}, {"text": "non-experts improve parsing accuracy by answering questions automatically generated from the parser's output.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9825417995452881}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8988500237464905}]}, {"text": "We develop the approach for CCG parsing, leveraging the link between CCG syntax and semantics to convert uncertain attachment decisions into natural language questions.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.8588444590568542}]}, {"text": "The answers are used as soft constraints when re-parsing the sentence.", "labels": [], "entities": []}, {"text": "Previous work used crowdsourcing for less structured tasks such as named entity recognition and prepositional phrase attachment (.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.621564527352651}, {"text": "prepositional phrase attachment", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.5961142381032308}]}, {"text": "Our work is most related to that of, which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets.", "labels": [], "entities": []}, {"text": "improve a parser by creating paraphrases of sentences, and then parsing the sentence and its paraphrase jointly.", "labels": [], "entities": []}, {"text": "Instead of using paraphrases, we build on the approach of QA-SRL (, which shows that untrained crowd workers can annotate predicate-argument structures by writing question-answer pairs.", "labels": [], "entities": []}, {"text": "Our experiments for newswire and biomedical text demonstrate improvements to parsing accuracy of 1.7 F1 on the sentences changed by re-parsing, while asking only less than 2 questions per sentence.", "labels": [], "entities": [{"text": "parsing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9501924514770508}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9679675102233887}, {"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9616963267326355}]}, {"text": "The annotations we collected 1 area representationindependent resource that could be used to develop new models or human-in-the-loop algorithms for related tasks, including semantic role labeling and syntactic parsing with other formalisms.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.6858727137247721}, {"text": "syntactic parsing", "start_pos": 200, "end_pos": 217, "type": "TASK", "confidence": 0.7140093594789505}]}], "datasetContent": [{"text": "5 annotators answered each query; on CCGbank we required 85% accuracy on test questions and on Bioinfer we set the threshold at 80% because of the difficulty of the sentences.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9898101091384888}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9991897940635681}, {"text": "Bioinfer", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.9279129505157471}]}, {"text": "Annotators unanimously chose the same set of answers for over 40% of the queries; an absolute majority is achieved for over 90% of the queries.", "labels": [], "entities": []}, {"text": "Qualitative Analysis shows example queries from the CCGbank development set.", "labels": [], "entities": [{"text": "CCGbank development set", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.9799894690513611}]}, {"text": "Examples 1 and 2 show that workers could annotate longrange dependencies and scoping decisions, which are challenging for existing parsers.", "labels": [], "entities": []}, {"text": "However, there are some cases where annotators disagree with the gold syntax, mostly involving semantic phenomena which are not reflected in the syntactic structure.", "labels": [], "entities": []}, {"text": "Many cases involve coreference, where annotators often prefer a proper noun referent over a pronoun or indefinite (see Examples 4 and 5), even if it is not the syntactic argument of the verb.", "labels": [], "entities": [{"text": "coreference", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.960962176322937}]}, {"text": "Example 6 shows a complex control structure, where the gold CCGbank syntax does not recover the true agent of build.", "labels": [], "entities": []}, {"text": "CCGbank also does not distinguish between subject and object control.", "labels": [], "entities": [{"text": "CCGbank", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9255202412605286}]}, {"text": "For these cases, our method could be used to extend existing treebanks.", "labels": [], "entities": []}, {"text": "Another common error case involved partitives and related constructions, where the correct attachment is subtle-as reflected by the annotators' split decision in Example 7.", "labels": [], "entities": []}, {"text": "Question Quality shows the percentage of questions that are answered with None of the above (written N/A below) by at most k annotators.", "labels": [], "entities": []}, {"text": "On all domains, about 80% of the queries are considered answerable by all 5 annotators.", "labels": [], "entities": []}, {"text": "To have a better understanding of the quality of automatically generated questions, we did a manual analysis on 50 questions for sentences from the CCGbank development set that are marked N/A by more than one annotator.", "labels": [], "entities": [{"text": "CCGbank development set", "start_pos": 148, "end_pos": 171, "type": "DATASET", "confidence": 0.9668717781702677}]}, {"text": "Among the 50 questions, 31 of them are either generated from an incorrect supertag or unanswerable given the candidates.", "labels": [], "entities": []}, {"text": "So the N/A answer k-N/A CCG-Dev CCG-Test Bioinfer 0 77.6% 81.6% 79.3% \u2264 1 89.6% 92.6% 89.1% \u2264 2 93.8% 96.1% 92.8%: The percentage of queries with at most k annotators choosing the None of the above (N/A) option.", "labels": [], "entities": []}, {"text": "can provide useful signal that the parses that generated the question are likely incorrect.", "labels": [], "entities": []}, {"text": "Common mistakes in question generation include: bad argument span in a copula question (4 questions), bad modality/negation (3 questions), and missing argument or particle (5 questions).", "labels": [], "entities": [{"text": "question generation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7333710789680481}]}, {"text": "Example 8 in shows an example of a nonsensical question.", "labels": [], "entities": []}, {"text": "While the parses agreed with the gold category S\\NP, the question they generated omitted the negation and the verb phrase that was elided in the original sentence.", "labels": [], "entities": []}, {"text": "In this case, 3 out of 5 annotators were able to answer with the correct dependency, but such mistakes can make re-parsing more challenging.", "labels": [], "entities": []}, {"text": "Cost and Speed We paid 6 cents for each answer.", "labels": [], "entities": [{"text": "Cost", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9848694801330566}, {"text": "Speed", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.8759081363677979}]}, {"text": "With 5 judgments per query, 20% test questions, and CrowdFlower's 20% service fee, the average cost per query was about 46 cents.", "labels": [], "entities": []}, {"text": "On average, we collected about 1000 judgments per hour, so we were able to annotate all the queries generated from the CCGbank test set within 15 hours.", "labels": [], "entities": [{"text": "CCGbank test set", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.9779623746871948}]}], "tableCaptions": [{"text": " Table 3: Sentence coverage, number of queries annotated, and", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.619495153427124}]}, {"text": " Table 4: The percentage of queries with at least k annotators", "labels": [], "entities": []}, {"text": " Table 6: CCG parsing accuracy with human in the loop (HITL)", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6469108760356903}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.976473331451416}]}, {"text": " Table 7: Improvements of CCG parsing accuracy on changed", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.6603145748376846}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9704450964927673}]}]}