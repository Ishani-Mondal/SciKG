{"title": [], "abstractContent": [{"text": "Unsupervised dependency parsing aims to learn a dependency grammar from text annotated with only POS tags.", "labels": [], "entities": [{"text": "Unsupervised dependency parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5952500899632772}]}, {"text": "Various features and inductive biases are often used to incorporate prior knowledge into learning.", "labels": [], "entities": []}, {"text": "One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags.", "labels": [], "entities": []}, {"text": "Previous work employed manually designed features or special prior distributions to encode such information.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to unsupervised dependency parsing that uses a neural model to predict grammar rule probabilities based on distributed representation of POS tags.", "labels": [], "entities": [{"text": "unsupervised dependency parsing", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6660404205322266}]}, {"text": "The distributed representation is automatically learned from data and captures the correlations between POS tags.", "labels": [], "entities": []}, {"text": "Our experiments show that our approach outperforms previous approaches utilizing POS correlations and is competitive with recent state-of-the-art approaches on nine different languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised structured prediction from data is an important problem in natural language processing, with applications in grammar induction, POS tag induction, word alignment and soon.", "labels": [], "entities": [{"text": "Unsupervised structured prediction from data", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7150124847888947}, {"text": "grammar induction", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7406820207834244}, {"text": "POS tag induction", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.8259651263554891}, {"text": "word alignment", "start_pos": 160, "end_pos": 174, "type": "TASK", "confidence": 0.8421411514282227}]}, {"text": "Because the training data is unannotated in unsupervised structured prediction, learning is very hard.", "labels": [], "entities": []}, {"text": "In this paper, we focus on unsupervised dependency parsing, which aims to identify the dependency trees of sentences in an unsupervised manner.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.6873820275068283}]}, {"text": "Previous work on unsupervised dependency parsing is mainly based on the dependency model with valence (DMV) () and its extension.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7254006862640381}]}, {"text": "To effectively learn the DMV model for better parsing accuracy, a variety of inductive biases and handcrafted features have been proposed to incorporate prior information into learning.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9741292595863342}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8758208155632019}]}, {"text": "One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags.", "labels": [], "entities": []}, {"text": "employed special prior distributions to encourage learning of correlations between POS tags.", "labels": [], "entities": []}, {"text": "encoded the relations between POS tags using manually designed features.", "labels": [], "entities": []}, {"text": "In this work, we propose a neural based approach to unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6883355230093002}]}, {"text": "We incorporate a neural model into the DMV model to predict grammar rule probabilities based on distributed representation of POS tags.", "labels": [], "entities": []}, {"text": "We learn the neural network parameters as well as the distributed representations from data using the expectationmaximization algorithm.", "labels": [], "entities": []}, {"text": "The correlations between POS tags are automatically captured in the learned POS embeddings and contribute to the improvement of parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 128, "end_pos": 135, "type": "TASK", "confidence": 0.97462397813797}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8784179091453552}]}, {"text": "In particular, probabilities of grammar rules involving correlated POS tags are automatically smoothed in our approach without the need for manual features or additional smoothing procedures.", "labels": [], "entities": []}, {"text": "Our experiments show that on the Wall Street Journal corpus our approach outperforms the previous approaches that also utilize POS tag correla-tions, and achieves a comparable result with recent state-of-the-art grammar induction systems.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 33, "end_pos": 59, "type": "DATASET", "confidence": 0.9811834096908569}]}, {"text": "On the datasets of eight additional languages, our approach is able to achieve better performance than the baseline methods without any parameter tuning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparisons of Approaches based on POS Correla-", "labels": [], "entities": []}, {"text": " Table 3: DDA results (on sentences no longer than 10) on eight additional languages. Our neural based approaches are compared", "labels": [], "entities": []}, {"text": " Table 5: Comparison between activation functions.", "labels": [], "entities": []}, {"text": " Table 6: Comparison between using two separate networks and", "labels": [], "entities": []}, {"text": " Table 4: DDA results (on all the sentences) on eight additional languages. Our neural based approaches are compared with", "labels": [], "entities": []}]}