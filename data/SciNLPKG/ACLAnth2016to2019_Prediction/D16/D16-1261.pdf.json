{"title": [{"text": "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning", "labels": [], "entities": [{"text": "Improving Information Extraction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9095290104548136}]}], "abstractContent": [{"text": "Most successful information extraction systems operate with access to a large collection of documents.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.833454966545105}]}, {"text": "In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9440920352935791}]}, {"text": "This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected.", "labels": [], "entities": []}, {"text": "We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on con-textual information.", "labels": [], "entities": []}, {"text": "We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9799773693084717}]}, {"text": "Our experiments on two databases-of shooting incidents, and food adulteration cases-demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many realistic domains, information extraction (IE) systems require exceedingly large amounts of annotated data to deliver high performance.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.856284910440445}]}, {"text": "Increases in training data size enable models to handle robustly the multitude of linguistic expressions that convey the same semantic relation.", "labels": [], "entities": []}, {"text": "Consider, for instance, an IE system that aims to identify entities such as the perpetrator and the number of vic- Code is available at http://people.csail.mit.", "labels": [], "entities": []}, {"text": "edu/karthikn/rl-ie/ ShooterName: Scott Westerhuis NumKilled: 6 A couple and four children found dead in their burning South Dakota home had been shot in an apparent murder-suicide, officials said Monday.", "labels": [], "entities": []}, {"text": "Scott Westerhuis's cause of death was \"shotgun wound with manner of death as suspected suicide,\" it added in a statement.", "labels": [], "entities": []}, {"text": "tims in a shooting incident).", "labels": [], "entities": []}, {"text": "The document does not explicitly mention the shooter (Scott Westerhuis), but instead refers to him as a suicide victim.", "labels": [], "entities": []}, {"text": "Extraction of the number of fatally shot victims is similarly difficult, as the system needs to infer that \"A couple and four children\" means six people.", "labels": [], "entities": [{"text": "Extraction", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9436436295509338}]}, {"text": "Even a large annotated training set may not provide sufficient coverage to capture such challenging cases.", "labels": [], "entities": []}, {"text": "In this paper, we explore an alternative approach for boosting extraction accuracy, when a large training corpus is not available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9584146738052368}]}, {"text": "Instead, the proposed method utilizes external information sources to resolve ambiguities inherent in text interpretation.", "labels": [], "entities": [{"text": "text interpretation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7190828025341034}]}, {"text": "Specifically, our strategy is to find other documents that contain the information sought, expressed in a form that a basic extractor can \"understand\".", "labels": [], "entities": []}, {"text": "For instance, shows two other articles describing the same event, wherein the entities of interest The six members of a South Dakota family found dead in the ruins of their burned home were fatally shot, with one death believed to be a suicide, authorities said Monday.", "labels": [], "entities": []}, {"text": "AG Jackley says all evidence supports the story he told based on preliminary findings back in September: Scott Westerhuis shot his wife and children with a shotgun, lit his house on fire with an accelerant, then shot himself with his shotgun.", "labels": [], "entities": [{"text": "AG Jackley", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9315412044525146}]}, {"text": "-the number of people killed and the name of the shooter -are expressed explicitly.", "labels": [], "entities": []}, {"text": "Processing such stereotypical phrasing is easier for most extraction systems, compared to analyzing the original source document.", "labels": [], "entities": []}, {"text": "This approach is particularly suitable for extracting information from news where atypical event is covered by multiple news outlets.", "labels": [], "entities": [{"text": "extracting information from news", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.8466276973485947}]}, {"text": "The challenges, however, lie in (1) performing event coreference (i.e. retrieving suitable articles describing the same incident) and (2) reconciling the entities extracted from these different documents.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7074980437755585}]}, {"text": "Querying the web (using the source article's title for instance) often retrieves documents about other incidents with a tangential relation to the original story.", "labels": [], "entities": []}, {"text": "For example, the query \"4 adults, 1 teenager shot in west Baltimore 3 april 2015\" yields only two relevant articles among the top twenty results on Bing search, while returning other shooting events at the same location.", "labels": [], "entities": [{"text": "Bing search", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.9075031578540802}]}, {"text": "Moreover, the values extracted from these different sources require resolution since some of them might be inaccurate.", "labels": [], "entities": []}, {"text": "One solution to this problem would be to perform a single search to retrieve articles on the same event and then reconcile values extracted from them (say, using a meta-classifier).", "labels": [], "entities": []}, {"text": "However, if the confidence of the new set of values is still low, we might wish to perform further queries.", "labels": [], "entities": []}, {"text": "Thus, the problem is inherently sequential, requiring alternating phases of querying to retrieve articles and integrating the extracted values.", "labels": [], "entities": []}, {"text": "We address these challenges using a Reinforcement Learning (RL) approach that combines query formulation, extraction from new sources, and value reconciliation.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6771287739276886}, {"text": "query formulation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7758454978466034}, {"text": "value reconciliation", "start_pos": 139, "end_pos": 159, "type": "TASK", "confidence": 0.7461884319782257}]}, {"text": "To effectively select among possible actions, our state representation encodes information about the current and new entity values along with the similarity between the source article and the newly retrieved document.", "labels": [], "entities": []}, {"text": "The model learns to select good actions for both article retrieval and value reconciliation in order to optimize the reward function, which reflects extraction accuracy and includes penalties for extra moves.", "labels": [], "entities": [{"text": "article retrieval", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8338038921356201}, {"text": "value reconciliation", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.725363165140152}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9848662614822388}]}, {"text": "We train the RL agent using a Deep Q-Network (DQN) ( that is used to predict both querying and reconciliation choices simultaneously.", "labels": [], "entities": []}, {"text": "While we use a maximum entropy model as the base extractor, this framework can be inherently applied to other extraction algorithms.", "labels": [], "entities": []}, {"text": "We evaluate our system on two datasets where available training data is inherently limited.", "labels": [], "entities": []}, {"text": "The first dataset is constructed from a publicly available database of mass shootings in the United States.", "labels": [], "entities": []}, {"text": "The database is populated by volunteers and includes the source articles.", "labels": [], "entities": []}, {"text": "The second dataset is derived from a FoodShield database of illegal food adulterations.", "labels": [], "entities": [{"text": "FoodShield database of illegal food adulterations", "start_pos": 37, "end_pos": 86, "type": "DATASET", "confidence": 0.9316032528877258}]}, {"text": "Our experiments demonstrate that the final RL model outperforms basic extractors as well as a meta-classifier baseline in both domains.", "labels": [], "entities": []}, {"text": "For instance, in the Shootings domain, the average accuracy improvement over the meta-classifier is 7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9996122717857361}]}], "datasetContent": [{"text": "Data We perform experiments on two different datasets.", "labels": [], "entities": []}, {"text": "For the first set, we collected data from the Gun Violence archive,: Stats for Shootings and Adulteration datasets the incident took place.", "labels": [], "entities": [{"text": "Gun Violence archive", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.8934564193089803}, {"text": "Adulteration datasets", "start_pos": 93, "end_pos": 114, "type": "DATASET", "confidence": 0.701616108417511}]}, {"text": "We consider these as the entities of interest, to be extracted from the articles.", "labels": [], "entities": []}, {"text": "The second dataset we use is the Foodshield EMA database 10 documenting adulteration incidents since 1980.", "labels": [], "entities": [{"text": "Foodshield EMA database", "start_pos": 33, "end_pos": 56, "type": "DATASET", "confidence": 0.9444929162661234}]}, {"text": "This data contains annotations for (1) the affected food product, (2) the adulterant and (3) the location of the incident.", "labels": [], "entities": []}, {"text": "Both datasets are classic examples where the number of recorded incidents is insufficient for large-scale IE systems to leverage.", "labels": [], "entities": [{"text": "IE", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9785903096199036}]}, {"text": "For each source article in the above databases, we download extra articles (top 20 links) using the Bing Search API 11 with different automatically generated queries.", "labels": [], "entities": []}, {"text": "We use only the source articles from the train portion to learn the parameters of the base extractor.", "labels": [], "entities": []}, {"text": "The entire train set with downloaded articles is used to train the DQN agent and the metaclassifier baseline (described below).", "labels": [], "entities": [{"text": "DQN", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8838599920272827}]}, {"text": "All parameters are tuned on the dev set.", "labels": [], "entities": []}, {"text": "For the final results, we train the models on the combined train and dev sets and use the entire test set (source + downloaded articles) to evaluate.", "labels": [], "entities": []}, {"text": "Extraction model We use a maximum entropy classifier as the base extraction system, since it provides flexibility to capture various local context features and has been shown to perform well for information extraction ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 195, "end_pos": 217, "type": "TASK", "confidence": 0.8735522925853729}]}, {"text": "The classifier is used to tag each word in a document as one of the entity types or not (e.g. {Shooter-Name, NumKilled, NumWounded, City, Other} in the Shootings domain).", "labels": [], "entities": []}, {"text": "Then, for each tag except Other, we choose the mode of the values to obtain the set of entity extractions from the article.", "labels": [], "entities": []}, {"text": "Features used in the classifier are provided in the Supplementary material.", "labels": [], "entities": [{"text": "Supplementary material", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.906977504491806}]}, {"text": "The features and context window c = 4 of neighboring words are tuned to maximize performance on a dev set.", "labels": [], "entities": []}, {"text": "We also experimented with a conditional random field (CRF) (with the same features) for the sequence tagging ( but obtained worse empirical performance (see Section 6).", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7309150099754333}]}, {"text": "The parameters of the base extraction model are not changed during training of the RL model.", "labels": [], "entities": []}, {"text": "We evaluate the extracted entity values against the gold annotations and report the corpus-level average accuracy on each entity type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.969794511795044}]}, {"text": "For entities like ShooterName, the annotations (and the news articles) often contain multiple names (first and last) in various combinations, so we consider retrieving either name as a successful extraction.", "labels": [], "entities": []}, {"text": "For all other entities, we look for exact matches.", "labels": [], "entities": []}, {"text": "Baselines We explore 4 types of baselines: Basic extractors: We use the CRF and the Maxent classifier mentioned previously.", "labels": [], "entities": []}, {"text": "Aggregation systems: We examine two systems that perform different types of value reconciliation.", "labels": [], "entities": [{"text": "value reconciliation", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7421403527259827}]}, {"text": "The first model (Confidence) chooses entity values with the highest confidence score assigned by the base extractor.", "labels": [], "entities": []}, {"text": "The second system (Majority) takes a majority vote overall values extracted from these articles.", "labels": [], "entities": []}, {"text": "Both methods filter new entity values using a threshold \u03c4 on the cosine similarity over the tf-idf representations of the source and new articles.", "labels": [], "entities": []}, {"text": "Meta-classifer: To demonstrate the importance of modeling the problem in the RL framework, we consider a meta-classifier baseline.", "labels": [], "entities": []}, {"text": "The classifier operates over the same input state space and produces the same set of reconciliation decisions {d} as the DQN.", "labels": [], "entities": []}, {"text": "For training, we use the original source article for each event along with a related downloaded article to compute the state.", "labels": [], "entities": []}, {"text": "If the downloaded article has the correct value and the original one does not, we label it as a positive example for that entity class.", "labels": [], "entities": []}, {"text": "If multiple such entity classes exist, we create several training instances with appropriate labels, and if none exist, we use the label corresponding to the reject all action.", "labels": [], "entities": []}, {"text": "For each test event, the classifier is used to provide decisions for all the downloaded articles and the final extraction is performed by aggregating the value predictions using the Confidence-based scheme described above.", "labels": [], "entities": []}, {"text": "Oracle: Finally, we also have an ORACLE score which is computed assuming perfect reconciliation and querying decisions on top of the Maxent base extractor.", "labels": [], "entities": [{"text": "ORACLE", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9984861016273499}]}, {"text": "This helps us analyze the contribution of the RL system in isolation of the inherent limitations of the base extractor.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy of various baselines (italics), our system (DQN) and the Oracle on Shootings and Adulteration  datasets. Agg. refers to aggregation baselines. Bold indicates best system scores.  *  statistical significance of p <  0.0005 vs basic Maxent extractor using the Student-t test. Numbers in parentheses indicate the optimal threshold (\u03c4 )  for the aggregation baselines. Confidence-based reconciliation was used for RL-Query.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9705255031585693}, {"text": "Agg", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.998115062713623}, {"text": "Confidence-based reconciliation", "start_pos": 384, "end_pos": 415, "type": "TASK", "confidence": 0.6823219060897827}]}, {"text": " Table 4: Sample outputs (along with corresponding article snippets) on the Shootings domain showing correct predic- tions from RL-Extract where the basic extractor (Maxent) fails.", "labels": [], "entities": [{"text": "Shootings domain", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.7591097354888916}]}, {"text": " Table 5: Effect of using different reconciliation schemes, context-vectors, and rewards in our RL framework (Shoot- ings domain). The last row is the overall best scheme (deviations from this are in italics). Context refers to the  type of word counts used in the state vector to represent entity context. Rewards are either per step or per episode.  (S: ShooterName, K: NumKilled, W: NumWounded, C: City, Steps: Average number of steps per episode)", "labels": [], "entities": []}]}