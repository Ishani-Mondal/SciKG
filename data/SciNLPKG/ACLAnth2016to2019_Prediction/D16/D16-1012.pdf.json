{"title": [{"text": "Deep Multi-Task Learning with Shared Memory", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural network based models have achieved impressive results on various specific tasks.", "labels": [], "entities": []}, {"text": "However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data.", "labels": [], "entities": []}, {"text": "In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks.", "labels": [], "entities": []}, {"text": "More specifically, we augment neural model with an external memory, which is shared by several tasks.", "labels": [], "entities": []}, {"text": "Experiments on two groups of text classification tasks show that our proposed archi-tectures can improve the performance of a task with the help of other related tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.8422588308652242}]}], "introductionContent": [{"text": "Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification), semantic matching (), parser) and machine translation ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7582333087921143}, {"text": "machine translation", "start_pos": 217, "end_pos": 236, "type": "TASK", "confidence": 0.7996943891048431}]}, {"text": "Usually, due to the large number of parameters these neural models need a large-scale corpus.", "labels": [], "entities": []}, {"text": "It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge.", "labels": [], "entities": []}, {"text": "To overcome this problem, these models often involve an unsupervised pre-training phase.", "labels": [], "entities": []}, {"text": "The final model is fine-tuned on specific task with respect * Corresponding author.", "labels": [], "entities": []}, {"text": "to a supervised training criterion.", "labels": [], "entities": []}, {"text": "However, most pre-training methods are based on unsupervised objectives, which is effective to improve the final performance, but it does not directly optimize the desired task.", "labels": [], "entities": []}, {"text": "Multi-task learning is an approach to learn multiple related tasks simultaneously to significantly improve performance relative to learning each task independently.", "labels": [], "entities": [{"text": "Multi-task learning", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7746281921863556}]}, {"text": "Inspired by the success of multi-task learning, several neural network based models) are proposed for NLP tasks, which utilized multi-task learning to jointly learn several tasks with the aim of mutual benefit.", "labels": [], "entities": []}, {"text": "The characteristic of these multi-task architectures is they share some lower layers to determine common features.", "labels": [], "entities": []}, {"text": "After the shared layers, the remaining layers are split into multiple specific tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose two deep architectures of sharing information among several tasks in multitask learning framework.", "labels": [], "entities": []}, {"text": "All the related tasks are integrated into a single system which is trained jointly.", "labels": [], "entities": []}, {"text": "More specifically, inspired by Neural Turing Machine (NTM) () and memory network (, we equip taskspecific long short-term memory (LSTM) neural network) with an external shared memory.", "labels": [], "entities": []}, {"text": "The external memory has capability to store long term information and knowledge shared by several related tasks.", "labels": [], "entities": []}, {"text": "Different with NTM, we use a deep fusion strategy to integrate the information from the external memory into taskspecific LSTM, in which a fusion gate controls the information flowing flexibly and enables the model to selectively utilize the shared information.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our architectures on two groups of text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.8213105400403341}]}, {"text": "Experimental results show that jointly learning of multiple related tasks can improve the performance of each task relative to learning them independently.", "labels": [], "entities": []}, {"text": "Our contributions are of three-folds: \u2022 We proposed a generic multi-task framework, in which different tasks can share information by an external memory and communicate by a reading/writing mechanism.", "labels": [], "entities": []}, {"text": "Two proposed models are complementary to prior multi-task neural networks.", "labels": [], "entities": []}, {"text": "\u2022 Different with Neural Turing Machine and memory network, we introduce a deep fusion mechanism between internal and external memories, which helps the LSTM units keep them interacting closely without being conflated.", "labels": [], "entities": []}, {"text": "\u2022 As a by-product, the fusion gate enables us to better understand how the external shared memory helps specific task.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we investigate the empirical performances of our proposed architectures on two multitask datasets.", "labels": [], "entities": []}, {"text": "Each dataset contains several related tasks.", "labels": [], "entities": []}, {"text": "The used multi-task datasets are briefly described as follows.", "labels": [], "entities": []}, {"text": "The detailed statistics are listed in.", "labels": [], "entities": []}, {"text": "Movie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews.", "labels": [], "entities": []}, {"text": "\u2022 SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank 1).", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank 1", "start_pos": 51, "end_pos": 80, "type": "DATASET", "confidence": 0.9004094898700714}]}, {"text": "\u2022 SST-2 The movie reviews with binary classes.", "labels": [], "entities": [{"text": "SST-2", "start_pos": 2, "end_pos": 7, "type": "TASK", "confidence": 0.8553392291069031}]}, {"text": "It is also from the Stanford Sentiment Treebank.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 20, "end_pos": 47, "type": "DATASET", "confidence": 0.9393683075904846}]}, {"text": "\u2022 SUBJ The movie reviews with labels of subjective or objective (: Accuracies of our models on movie reviews tasks against state-of-the-art neural models.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9746944308280945}]}, {"text": "The last column gives the improvements relative to LSTM and ME-LSTM respectively.", "labels": [], "entities": [{"text": "ME-LSTM", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.8629357814788818}]}, {"text": "NBOW: Sums up the word vectors and applies a non-linearity followed by a softmax classification layer.", "labels": [], "entities": [{"text": "NBOW", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8421041369438171}]}, {"text": "Product Reviews This dataset 3 , constructed by, contains Amazon product reviews from four different domains: Books, DVDs, Electronics and Kitchen appliances.", "labels": [], "entities": []}, {"text": "The goal in each domain is to classify a product review as either positive or negative.", "labels": [], "entities": []}, {"text": "The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 20% and 10% respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of two multi-task datasets. Each dataset consists of four related tasks.", "labels": [], "entities": []}, {"text": " Table 3: Accuracies of our models on movie reviews tasks against state-of-the-art neural models. The last  column gives the improvements relative to LSTM and ME-LSTM respectively. NBOW: Sums up the word  vectors and applies a non-linearity followed by a softmax classification layer.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.982455313205719}]}, {"text": " Table 4: Accuracies of our models on product reviews dataset. The last column gives the improvement  relative to LSTM and ME-LSTM respectively.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9949434399604797}, {"text": "product reviews dataset", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.6755693157513937}, {"text": "ME-LSTM", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9082768559455872}]}]}