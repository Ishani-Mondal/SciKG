{"title": [{"text": "Memory-enhanced Decoder for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7770192623138428}]}], "abstractContent": [{"text": "We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN.", "labels": [], "entities": []}, {"text": "This memory-enhanced RNN de-coder is called MEMDEC.", "labels": [], "entities": [{"text": "MEMDEC", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.8548726439476013}]}, {"text": "At each time during decoding, MEMDEC will read from this memory and write to this memory once, both with content-based addressing.", "labels": [], "entities": []}, {"text": "Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with predetermined size designed to better capture the information important for the decoding process at each time step.", "labels": [], "entities": []}, {"text": "Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses, yielding the best performance achieved with the same training set.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.5662464946508408}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9990296363830566}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9982165694236755}]}], "introductionContent": [{"text": "The introduction of external memory has greatly expanded the representational capability of neural network-based model on modeling sequences(), by providing flexible ways of storing and accessing information.", "labels": [], "entities": []}, {"text": "More specifically, in neural machine translation, one great improvement came from using an array of vectors to represent the source in a sentencelevel memory and dynamically accessing relevant segments of them (alignment) () through content-based addressing ().", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.698509136835734}]}, {"text": "The success of RNNsearch demonstrated the advantage of saving the entire sentence of arbitrary length in an unbounded memory for operations of next stage (e.g., decoding).", "labels": [], "entities": []}, {"text": "In this paper, we show that an external memory can be used to facilitate the decoding/generation process thorough a memory-enhanced RNN decoder, called MEMDEC.", "labels": [], "entities": []}, {"text": "The memory in MEMDEC is a direct extension to the state in the decoding, therefore functionally closer to the memory cell in LSTM).", "labels": [], "entities": []}, {"text": "It takes the form of a matrix with pre-determined size, each column (\"a memory cell\") can be accessed by the decoding RNN with content-based addressing for both reading and writing during the decoding process.", "labels": [], "entities": []}, {"text": "This memory is designed to provide a more flexible way to select, represent and synthesize the information of source sentence and previously generated words of target relevant to the decoding.", "labels": [], "entities": []}, {"text": "This is in contrast to the set of hidden states of the entire source sentence (which can viewed as another form of memory) in () for attentive read, but can be combined with it to greatly improve the performance of neural machine translator.", "labels": [], "entities": []}, {"text": "We apply our model on EnglishChinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data Our contributions are mainly two-folds \u2022 we propose a memory-enhanced decoder for neural machine translator which naturally extends the RNN with vector state.", "labels": [], "entities": [{"text": "EnglishChinese translation tasks", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7798568705717722}, {"text": "neural machine translator", "start_pos": 233, "end_pos": 258, "type": "TASK", "confidence": 0.7020724018414816}]}, {"text": "\u2022 our empirical study on Chinese-English translation tasks show the efficacy of the proposed model.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7226204574108124}]}], "datasetContent": [{"text": "We test the memory-enhanced decoder to task of Chinese-to-English translation, where MEMDEC is put on the top of encoder same as in ().", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6209776848554611}, {"text": "MEMDEC", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.8470234870910645}]}, {"text": "Our training data for the translation task consists of  Hyper parameters In training of the neural networks, we limit the source and target vocabularies to the most frequent 30K words in both Chinese and English, covering approximately 97.7% and 99.3% of the two corpora respectively.", "labels": [], "entities": [{"text": "translation task", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9108816981315613}]}, {"text": "The dimensions of word embedding is 512 and the size of the hidden layer is 1024.", "labels": [], "entities": []}, {"text": "The dimemsion of each cell in MB is set to 1024 and the number of cells n is set to 8.", "labels": [], "entities": [{"text": "dimemsion", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9817125201225281}]}, {"text": "Training details We initialize the recurrent weight matrices as random orthogonal matrices.", "labels": [], "entities": []}, {"text": "All the bias vectors were initialize to zero.", "labels": [], "entities": []}, {"text": "For other parameters, we initialize them by sampling each element from the Gaussian distribution of mean 0 and variance 0.01 2 . Parameter optimization is performed using stochastic gradient descent.", "labels": [], "entities": [{"text": "Parameter optimization", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.8495876789093018}]}, {"text": "Adadelta) is used to automatically adapt the learning rate of each parameter ( = 10 \u22126 and \u03c1 = 0.95).", "labels": [], "entities": []}, {"text": "To avoid gradients explosion, the gradients of the cost function which had 2 norm larger than a predefined threshold 1.0 was normalized to the threshold (.", "labels": [], "entities": []}, {"text": "Each SGD is of a minibatch of 80 sentences.", "labels": [], "entities": []}, {"text": "We train our NMT model with the sentences of length up to 50 words in training data, while for moses system we use the full training data.", "labels": [], "entities": []}, {"text": "Memory Initialization Each memory cell is initialized with the source sentence hidden state computed as LDC2004T08 and LDC2005T06.", "labels": [], "entities": [{"text": "Memory Initialization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.697031632065773}]}, {"text": "where W INI \u2208 R m\u00d72\u00b7m ; \u03c3 is tanh function.", "labels": [], "entities": [{"text": "INI", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9331414103507996}]}, {"text": "m makes a nonlinear transformation of the source sentence information.", "labels": [], "entities": []}, {"text": "\u03bd i is a random vector sampled from N (0, 0.1).", "labels": [], "entities": []}, {"text": "Dropout we also use dropout for our NMT baseline model and MEMDEC to avoid overfitting ().", "labels": [], "entities": [{"text": "NMT baseline model", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.8946497241655985}, {"text": "MEMDEC", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.7204967737197876}]}, {"text": "The key idea is to randomly drop units (along with their connections) from the neural network during training.", "labels": [], "entities": []}, {"text": "This prevents units from co-adapting too much.", "labels": [], "entities": []}, {"text": "In the simplest case, each unit is omitted with a fixed probability p, namely dropout rate.", "labels": [], "entities": []}, {"text": "In our experiments, dropout was applied only on the output layer and the dropout rate is set to 0.5.", "labels": [], "entities": []}, {"text": "We also try other strategy such as dropout at word embeddings or RNN hidden states but fail to get further improvements.", "labels": [], "entities": []}, {"text": "Pre-training For MEMDEC, the objective function is a highly non-convex function of the parameters with more complicated landscape than that for decoder without external memory, rendering direct optimization overall the parameters rather difficult.", "labels": [], "entities": []}, {"text": "Inspired by the effort on easing the training of very deep architectures), we propose a simple pre-training strategyFirst we train a regular attention-based NMT model without external memory.", "labels": [], "entities": []}, {"text": "Then we use the trained NMT model to initialize the parameters of encoder and parameters of MEMDEC, except those related to memory-state After that, we fine-tune all the parameters of NMT with MEMDEC decoder, including the parameters initialized with pre-training and those associated with accessing memory-state.", "labels": [], "entities": [{"text": "MEMDEC", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8526015877723694}]}], "tableCaptions": [{"text": " Table 1: Case-insensitive BLEU scores on Chinese-English translation. Moses is the state-of-the-art phrase-based statistical", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9399365782737732}]}, {"text": " Table 2: MEMDEC performances of different memory size.", "labels": [], "entities": []}, {"text": " Table 1. Clearly MEMDEC leads to remark- able improvement over Moses (+5.28 BLEU) and  Groundhog (+4.78 BLEU). The feedback atten- tion gains +1.06 BLEU score on top of Ground- hog on average, while together with dropout adds  another +0.83 BLEU score, which constitute the  1.89 BLEU gain of RNNsearch over Ground- hog. Compared to RNNsearch MEMDEC is  +2.89 BLEU score higher, showing the model- ing power gained from the external memory. Fi-nally, we also compare MEMDEC with the state- of-the-art attention-based NMT with COVERAGE  mechanism(Tu et al., 2016), which is about 2  BLEU over than the published result after adding  fast attention and dropout. In this comparison  MEMDEC wins with big margin (+1.46 BLEU  score).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9773280620574951}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9639160633087158}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9767512083053589}, {"text": "BLEU", "start_pos": 281, "end_pos": 285, "type": "METRIC", "confidence": 0.9684251546859741}, {"text": "BLEU", "start_pos": 361, "end_pos": 365, "type": "METRIC", "confidence": 0.978646993637085}, {"text": "BLEU", "start_pos": 716, "end_pos": 720, "type": "METRIC", "confidence": 0.9788684248924255}]}]}