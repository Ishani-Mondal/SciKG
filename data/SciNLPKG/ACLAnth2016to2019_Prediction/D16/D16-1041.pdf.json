{"title": [{"text": "Supervised Distributional Hypernym Discovery via Domain Adaptation", "labels": [], "entities": [{"text": "Distributional Hypernym Discovery", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.8640516996383667}]}], "abstractContent": [{"text": "Lexical taxonomies are graph-like hierarchical structures that provide a formal representation of knowledge.", "labels": [], "entities": []}, {"text": "Most knowledge graphs to date rely on is-a (hypernymic) relations as the backbone of their semantic structure.", "labels": [], "entities": []}, {"text": "In this paper, we propose a supervised distributional framework for hypernym discovery which operates at the sense level, enabling large-scale automatic acquisition of disambiguated tax-onomies.", "labels": [], "entities": [{"text": "hypernym discovery", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8581848740577698}]}, {"text": "By exploiting semantic regularities between hyponyms and hypernyms in embed-dings spaces, and integrating a domain clustering algorithm, our model becomes sensitive to the target data.", "labels": [], "entities": []}, {"text": "We evaluate several configurations of our approach, training with information derived from a manually created knowledge base, along with hypernymic relations obtained from Open Information Extraction systems.", "labels": [], "entities": []}, {"text": "The integration of both sources of knowledge yields the best overall results according to both automatic and manual evaluation on ten different domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical taxonomies (taxonomies henceforth) are graph-like hierarchical structures where terms are nodes, and are typically organized over a predefined merging or splitting criterion (.", "labels": [], "entities": []}, {"text": "By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition ( and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential.", "labels": [], "entities": []}, {"text": "In fact, taxonomies have enabled a remarkable number of novel NLP techniques, e.g. the contribution of WordNet to lexical semantics) as well as various tasks, from word sense disambiguation () to information retrieval), question answering () and textual entailment ().", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 164, "end_pos": 189, "type": "TASK", "confidence": 0.6672279636065165}, {"text": "information retrieval", "start_pos": 196, "end_pos": 217, "type": "TASK", "confidence": 0.7349444925785065}, {"text": "question answering", "start_pos": 220, "end_pos": 238, "type": "TASK", "confidence": 0.9047347903251648}, {"text": "textual entailment", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.7129359543323517}]}, {"text": "To date, the application of taxonomies in NLP has consisted mainly of, on one hand, formally representing a domain of knowledge (e.g. Food), and, on the other hand, constituting the semantic backbone of large-scale knowledge repositories such as ontologies or Knowledge Bases (KBs).", "labels": [], "entities": []}, {"text": "In domain knowledge formalization, prominent work has made use of the web (, lexico-syntactic patterns), syntactic evidence (Luu), graph-based algorithms) or popularity of web sources.", "labels": [], "entities": [{"text": "domain knowledge formalization", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.6231913864612579}]}, {"text": "As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions.", "labels": [], "entities": []}, {"text": "Notable approaches of this kind include,, and the Wikipedia Bitaxonomy ().", "labels": [], "entities": [{"text": "Wikipedia Bitaxonomy", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.9172660708427429}]}, {"text": "In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as NELL () rely crucially on taxonomized concepts and their relations within their learning process.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7043119519948959}]}, {"text": "Taxonomy learning is roughly based on a twostep process, namely is-a (hypernymic) relation de-tection, and graph induction.", "labels": [], "entities": [{"text": "Taxonomy learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8427417278289795}, {"text": "graph induction", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.6816508620977402}]}, {"text": "The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics.", "labels": [], "entities": [{"text": "hypernym detection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8424000144004822}, {"text": "taxonomy learning", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.8358674049377441}]}, {"text": "It has been addressed by means of pattern-based methods 1, clustering ( and graph-based approaches (.", "labels": [], "entities": []}, {"text": "Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings).", "labels": [], "entities": []}, {"text": "In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relations.", "labels": [], "entities": []}, {"text": "These pairs maybe represented either as a concatenation of both vectors (), difference), dot-product (), or including additional linguistic information for LSTMbased learning.", "labels": [], "entities": []}, {"text": "In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings, which can be easily applied to the construction of lexical taxonomies.", "labels": [], "entities": [{"text": "TAXOEMBED 2", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9526678919792175}, {"text": "hypernym detection", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7220218628644943}]}, {"text": "It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces () and, unlike previous approaches, leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge.", "labels": [], "entities": []}, {"text": "Our best configuration (ranking first in two thirds of the experiments conducted) considers two training sources: (1) Manually curated pairs from Wikidata; and (2) Hypernymy relations from a KB which integrates several Open Information Extraction (OIE) systems).", "labels": [], "entities": []}, {"text": "Since our method uses a very large semantic network as reference sense inventory, we are able to perform jointly hypernym extraction and disambiguation, from which The terminology is not entirely unified in this respect.", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7103146910667419}]}, {"text": "In addition to pattern-based), other terms like path-based or rule-based) are also used.", "labels": [], "entities": []}, {"text": "2 Data and source code available from the following link: www.taln.upf.edu/taxoembed.", "labels": [], "entities": []}, {"text": "expanding existing ontologies becomes a trivial task.", "labels": [], "entities": []}, {"text": "Compared to word-level taxonomy learning, TAXO-EMBED results in more refined and unambiguous hypernymic relations at the sense level, with a direct application in tasks such as semantic search.", "labels": [], "entities": [{"text": "semantic search", "start_pos": 177, "end_pos": 192, "type": "TASK", "confidence": 0.7110875248908997}]}, {"text": "Evaluation (both manual and automatic) shows that we can effectively replicate the Wikidata is-a branch, and capture previously unseen relations in other reference taxonomies (YAGO or WIBI).", "labels": [], "entities": []}], "datasetContent": [{"text": "The performance of TAXOEMBED is evaluated by conducting several experiments, both automatic and manual.", "labels": [], "entities": [{"text": "TAXOEMBED", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8081445097923279}]}, {"text": "Specifically, we assess its ability to return valid hypernyms fora given unseen term with a held-out evaluation dataset of 250 Wikidata termhypernym pairs (Section 5.1).", "labels": [], "entities": []}, {"text": "In addition, we assess the extent to which TAXOEMBED is able to correctly identify hypernyms outside of Wikidata (Section 5.2).", "labels": [], "entities": []}, {"text": "For this experiment we use two configurations of TAXOEMBED: the first one includes 25k domainwise expanded training pairs (TaxE 25k ), whereas the second one adds 1k pairs from KB-U (TaxE 25k+K d ).", "labels": [], "entities": [{"text": "TAXOEMBED", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.8094125986099243}]}, {"text": "We randomly extract 200 test BabelNet synsets (20 per domain) whose hypernyms are missing in Wikidata.", "labels": [], "entities": []}, {"text": "We compare against a number of taxonomy learning and Information Extraction systems, namely,) and DefIE).", "labels": [], "entities": []}, {"text": "Yago and WiBi are used as upper bounds due to the nature of their hypernymic relations.", "labels": [], "entities": []}, {"text": "They include a great number of manually-encoded taxonomies (e.g. exploiting WordNet and Wikipedia categories).", "labels": [], "entities": []}, {"text": "Yago derives its taxonomic relations from an automatic mapping between WordNet and Wikipedia categories.", "labels": [], "entities": [{"text": "Yago", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8701860904693604}]}, {"text": "WiBi, on the other hand, exploits, among a number of different Wikipedia-specific heuristics, categories and the syntactic structure of the introductory sentence of Wikipedia pages.", "labels": [], "entities": [{"text": "WiBi", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8627272248268127}]}, {"text": "Finally, DefIE is an automaic OIE system relying on the syntactic structure of pre-disambiguated definitions . Three annotators manually evaluated the validity of the hypernyms extracted by each system (one per test instance).", "labels": [], "entities": [{"text": "DefIE", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.8877073526382446}]}, {"text": "shows the results of TAXOEMBED and all comparison systems.", "labels": [], "entities": [{"text": "TAXOEMBED", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.4751458466053009}]}, {"text": "As expected, Yago and WiBi achieve the best overall results.", "labels": [], "entities": [{"text": "WiBi", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.9134095311164856}]}, {"text": "However, TAXOEM-BED, based solely on distributional information, performed competitively in detecting new hypernyms when compared to DefIE, improving its recall inmost domains, and even surpassing Yago in technical areas like biology or health.", "labels": [], "entities": [{"text": "TAXOEM-BED", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.7715365290641785}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9975519776344299}]}, {"text": "However, our model does not perform particularly well on media and physics.", "labels": [], "entities": []}, {"text": "In most domains our model is able to discover novel hypernym relations that are not captured by any other system (e.g. therapy for radiation treatment planning in the health domain or decoration for molding in the art domain) .  In this experiment we evaluate the performance of TAXOEMBED on instances not included in Wikidata.", "labels": [], "entities": [{"text": "TAXOEMBED", "start_pos": 279, "end_pos": 288, "type": "METRIC", "confidence": 0.9151585698127747}]}, {"text": "We describe the experimental setting in Section 5.2.1 and present the results in Section 5.2.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the performance of TAXOEMBED using different training data samples.", "labels": [], "entities": [{"text": "TAXOEMBED", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.6258441805839539}]}, {"text": " Table 2: Precision, recall and F-Measure between TAXOEMBED, two taxonomy learning systems (Yago and  WiBi), and a pattern-based approach that performs hypernym extraction (DefIE).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9820227026939392}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9986228942871094}, {"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9958708882331848}, {"text": "TAXOEMBED", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.7908120155334473}, {"text": "hypernym extraction", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7454498708248138}]}]}