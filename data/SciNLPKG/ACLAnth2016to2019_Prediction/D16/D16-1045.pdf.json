{"title": [{"text": "The Structured Weighted Violations Perceptron Algorithm", "labels": [], "entities": [{"text": "Structured Weighted Violations Perceptron Algorithm", "start_pos": 4, "end_pos": 55, "type": "TASK", "confidence": 0.7148415684700012}]}], "abstractContent": [{"text": "We present the Structured Weighted Violations Perceptron (SWVP) algorithm, anew struc-tured prediction algorithm that generalizes the Collins Structured Perceptron (CSP, (Collins, 2002)).", "labels": [], "entities": [{"text": "Collins, 2002))", "start_pos": 171, "end_pos": 186, "type": "DATASET", "confidence": 0.844869390130043}]}, {"text": "Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels.", "labels": [], "entities": []}, {"text": "We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case.", "labels": [], "entities": [{"text": "convergence", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.9526904225349426}, {"text": "SWVP", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.853774905204773}, {"text": "mistake", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9903862476348877}]}, {"text": "In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case.", "labels": [], "entities": []}, {"text": "SWVP also provides encouraging initial dependency parsing results.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6693862527608871}]}], "introductionContent": [{"text": "The structured perceptron (), henceforth denoted CSP) is a prominent training algorithm for structured prediction models in NLP, due to its effective parameter estimation and simple implementation.", "labels": [], "entities": []}, {"text": "It has been utilized in numerous NLP applications including word segmentation and POS tagging, dependency parsing (, semantic parsing and information extraction (, if to name just a few.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8211908638477325}, {"text": "POS tagging", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.7587856650352478}, {"text": "dependency parsing", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8618105947971344}, {"text": "semantic parsing", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7329900562763214}, {"text": "information extraction", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.8242246806621552}]}, {"text": "Like some training algorithms in structured prediction (e.g. structured SVM (), MIRA and), CSP considers in its update rule the difference between complete predicted and gold standard labels (Sec. 2).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.7470360398292542}]}, {"text": "Unlike others (e.g. factored MIRA () and dual-loss based methods () it does not exploit the structure of the predicted label.", "labels": [], "entities": [{"text": "factored MIRA", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.3430159240961075}]}, {"text": "This may result invaluable information being lost.", "labels": [], "entities": []}, {"text": "Consider, for example, the gold and predicted dependency trees of.", "labels": [], "entities": []}, {"text": "The substantial difference between the trees maybe mostly due to the difference in roots (are and worse, respectively).", "labels": [], "entities": []}, {"text": "Parameter update w.r.t this mistake may thus be more useful than an update w.r.t the complete trees.", "labels": [], "entities": []}, {"text": "In this work we present anew perceptron algorithm with an update rule that exploits the structure of a predicted label when it differs from the gold label (Section 3).", "labels": [], "entities": []}, {"text": "Our algorithm is called The Structured Weighted Violations Perceptron (SWVP) as its update rule is based on a weighted sum of updates w.r.t violating assignments and non-violating assignments: assignments to the input example, derived from the predicted label, that score higher (for violations) and lower (for non-violations) than the gold standard label according to the current model.", "labels": [], "entities": [{"text": "Structured Weighted Violations Perceptron (SWVP)", "start_pos": 28, "end_pos": 76, "type": "TASK", "confidence": 0.7784040910857064}]}, {"text": "Our concept of violating assignment is based on that presented a variant of the CSP algorithm where the argmax inference problem is replaced with a violation finding function.", "labels": [], "entities": [{"text": "violating assignment", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9712043702602386}]}, {"text": "Their update rule, however, is identical to that of the CSP algorithm.", "labels": [], "entities": [{"text": "update", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9392176866531372}]}, {"text": "Importantly, although CSP and the above variant do not exploit the internal structure of the predicted label, they are special cases of SWVP.", "labels": [], "entities": []}, {"text": "In Section 4 we prove that fora linearly separable training set, SWVP converges to a linear separator of the data under certain conditions on the parameters of the algorithm, that are respected by the CSP special case.", "labels": [], "entities": []}, {"text": "We further prove mistake and generalization bounds for SWVP, and show that in the general case the SWVP bounds are tighter than the CSP's.", "labels": [], "entities": [{"text": "mistake", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9483654499053955}]}, {"text": "In Section 5 we show that SWVP allows aggressive updates, that exploit only violating assignments derived from the predicted label, and more balanced updates, that exploit both violating and non-violating assignments.", "labels": [], "entities": []}, {"text": "In experiments with synthetic data generated by an HMM, we demonstrate that various SWVP variants substantially outperform CSP training.", "labels": [], "entities": []}, {"text": "We also provide initial encouraging dependency parsing results, indicating the potential of SWVP for real world NLP applications.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.6462656408548355}]}], "datasetContent": [{"text": "Synthetic Data We experiment with synthetic data generated by a linear-chain, first-order Hidden Markov Model).", "labels": [], "entities": []}, {"text": "Our learning algorithm is a liner-chain conditional random field (CRF, ()): is a normalization factor) with binary indicator features {x i , y i , y i\u22121 , (x i , y i ), (y i , y i\u22121 ), (x i , y i , y i\u22121 )} for the triplet (y i , y i\u22121 , x).", "labels": [], "entities": []}, {"text": "A dataset is generated by iteratively sampling K items, each is sampled as follows.", "labels": [], "entities": []}, {"text": "We first sample a hidden state, y 1 , from a uniform prior distribution.", "labels": [], "entities": []}, {"text": "Then, iteratively, for i = 1, 2, . .", "labels": [], "entities": []}, {"text": ", L x we sample an observed state from the emission probability and (for i < L x ) a hidden state from the transition probability.", "labels": [], "entities": []}, {"text": "We experimented in 3 setups.", "labels": [], "entities": []}, {"text": "In each setup we generated 10 datasets that were subsequently divided to a 7000 items training set, a 2000 items development set and a 1000 items test set.", "labels": [], "entities": []}, {"text": "In all datasets, for each item, we set L x = 8.", "labels": [], "entities": []}, {"text": "We experiment in three conditions: (1) simple(++), learnable(+++), (2) simple(++), learnable(++) and (3) simple(+), learnable(+).", "labels": [], "entities": []}, {"text": "For each dataset (3 setups, 10 datasets per setup) we train variants of the SWVP algorithm differing in the \u03b3 selection strategy (WM or WMR, Section 5), being aggressive (A) or passive (B), and in their \u03b2 parameter (\u03b2 = {0.5, 1, . .", "labels": [], "entities": []}, {"text": "Training is done on the training subset and the best performing variant on the development subset is applied to the test subset.", "labels": [], "entities": []}, {"text": "For CSP no development set is employed as there is no hyper-parameter to tune.", "labels": [], "entities": []}, {"text": "We report averaged accuracy (fraction of observed states for which the model successfully predicts the hidden state value) across the test sets, together with the standard deviation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9836249351501465}]}, {"text": "Dependency Parsing We also report initial dependency parsing results.", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7850059866905212}, {"text": "dependency parsing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6089340299367905}]}, {"text": "We implemented our algorithms within the TurboParser.", "labels": [], "entities": [{"text": "TurboParser", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.9285942912101746}]}, {"text": "That is, every other aspect of the parser: feature set, probabilistic pruning algorithm, inference algorithm etc., is kept fixed but training is performed with SWVP.", "labels": [], "entities": []}, {"text": "We compare our results to the parser performance with CSP training (which comes with the standard implementation of the parser).", "labels": [], "entities": []}, {"text": "We experiment with the datasets of the CoNLL 2007 shared task on multilingual dependency parsing (, fora total of 9 languages.", "labels": [], "entities": [{"text": "CoNLL 2007 shared task", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.8457925021648407}, {"text": "multilingual dependency parsing", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.577868233124415}]}, {"text": "We followed the standard train/test split of these dataset.", "labels": [], "entities": []}, {"text": "For SWVP, we randomly sampled 1000 sentences from each training set to serve as development sets and tuned the parameters as in the synthetic data experiments.", "labels": [], "entities": [{"text": "SWVP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.922290563583374}]}, {"text": "CSP is trained on the training set and applied to the test set without any development set involved.", "labels": [], "entities": []}, {"text": "We report the Unlabeled Attachment Score (UAS) for each language and model.", "labels": [], "entities": [{"text": "Unlabeled Attachment Score (UAS)", "start_pos": 14, "end_pos": 46, "type": "METRIC", "confidence": 0.8626544376214346}]}], "tableCaptions": [{"text": " Table 1: Overall Synthetic Data Results. A-and B-denote an aggressive and a balanced approaches, respectively. Acc. (std) is", "labels": [], "entities": [{"text": "Acc", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9993385672569275}]}, {"text": " Table 2: First and second order dependency parsing UAS results for CSP trained models, as well as for models trained with SWVP", "labels": [], "entities": [{"text": "dependency parsing UAS", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7408056259155273}, {"text": "SWVP", "start_pos": 123, "end_pos": 127, "type": "TASK", "confidence": 0.4834361970424652}]}]}