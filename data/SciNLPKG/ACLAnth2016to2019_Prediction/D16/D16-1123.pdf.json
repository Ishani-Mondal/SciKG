{"title": [], "abstractContent": [{"text": "Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks.", "labels": [], "entities": [{"text": "Computer Vision tasks", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8437523444493612}]}, {"text": "Their application to language has received much less attention, and it has mainly focused on static classification tasks, such as sentence classification for Sentiment Analysis or relation extraction.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.7564750015735626}, {"text": "Sentiment Analysis", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9065357744693756}, {"text": "relation extraction", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.8409758806228638}]}, {"text": "In this work, we study the application of CNNs to language modeling, a dynamic, sequential prediction task that needs models to capture local as well as long-range dependency information.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7160857170820236}]}, {"text": "First, we show that CNNs achieve 11-26% better absolute performance than feed-forward neu-ral language models, demonstrating their potential for language representation even in sequential tasks.", "labels": [], "entities": [{"text": "language representation", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.7185997813940048}]}, {"text": "As for recurrent models, our model outperforms RNNs but is below state of the art LSTM models.", "labels": [], "entities": []}, {"text": "Second, we gain some understanding of the behavior of the model, showing that CNNs in language act as feature detectors at a high level of abstraction, like in Computer Vision, and that the model can profitably use information from as far as 16 words before the target.", "labels": [], "entities": []}], "introductionContent": [{"text": "Convolutional Neural Networks (CNNs) are the family of neural network models that feature a type of layer known as the convolutional layer.", "labels": [], "entities": []}, {"text": "This layer can extract features by convolving a learnable filter (or kernel) along different positions of a vectorial input.", "labels": [], "entities": []}, {"text": "CNNs have been successfully applied in Computer Vision in many different tasks, including object recognition, scene parsing, and action recognition (), but they have received less attention in NLP.", "labels": [], "entities": [{"text": "Computer Vision", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8096449673175812}, {"text": "object recognition", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7786337435245514}, {"text": "scene parsing", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.7166232913732529}, {"text": "action recognition", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.770745575428009}]}, {"text": "They have been somewhat explored in static classification tasks where the model is provided with a full linguistic unit as input (e.g. a sentence) and classes are treated as independent of each other.", "labels": [], "entities": [{"text": "static classification tasks", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.7284409205118815}]}, {"text": "Examples of this are sentence or document classification for tasks such as Sentiment Analysis or Topic Categorization (, sentence matching (, and relation extraction).", "labels": [], "entities": [{"text": "sentence or document classification", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.6164049804210663}, {"text": "Sentiment Analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.9370346069335938}, {"text": "sentence matching", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7423682659864426}, {"text": "relation extraction", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7429488301277161}]}, {"text": "However, their application to sequential prediction tasks, where the input is construed to be part of a sequence (for example, language modeling or POS tagging), has been rather limited (with exceptions, such as).", "labels": [], "entities": [{"text": "sequential prediction tasks", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.8599170446395874}, {"text": "language modeling", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7219399064779282}, {"text": "POS tagging", "start_pos": 148, "end_pos": 159, "type": "TASK", "confidence": 0.7084043323993683}]}, {"text": "The main contribution of this paper is a systematic evaluation of CNNs in the context of a prominent sequential prediction task, namely, language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7541501522064209}]}, {"text": "Statistical language models area crucial component in many NLP applications, such as Automatic Speech Recognition, Machine Translation, and Information Retrieval.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6542887588342031}, {"text": "Machine Translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8496125340461731}, {"text": "Information Retrieval", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.8273875117301941}]}, {"text": "Here, we study the problem under the standard formulation of learning to predict the upcoming token given its previous context.", "labels": [], "entities": []}, {"text": "One successful approach to this problem relies on counting the number of occurrences of n-grams while using smoothing and back-off techniques to estimate the probability of an upcoming word.", "labels": [], "entities": []}, {"text": "However, since each individual word is treated independently of the others, n-gram models fail to capture semantic relations between words.", "labels": [], "entities": []}, {"text": "In contrast, neural network language models () learn to predict the up-coming word given the previous context while embedding the vocabulary in a continuous space that can represent the similarity structure between words.", "labels": [], "entities": []}, {"text": "Both feed-forward and recurrent neural networks () have been shown to outperform n-gram models in various setups ().", "labels": [], "entities": []}, {"text": "These two types of neural networks make different architectural decisions.", "labels": [], "entities": []}, {"text": "Recurrent networks take one token at a time together with a hidden \"memory\" vector as input and produce a prediction and an updated hidden vector for the next time step.", "labels": [], "entities": []}, {"text": "In contrast, feed-forward language models take as input the last n tokens, where n is a fixed window size, and use them jointly to predict the upcoming word.", "labels": [], "entities": []}, {"text": "In this paper we define and explore CNN-based language models and compare them with both feedforward and recurrent neural networks.", "labels": [], "entities": []}, {"text": "Our results show a 11-26% perplexity reduction of the CNN with respect to the feed-forward language model, comparable or higher performance compared to similarly-sized recurrent models, and lower performance with respect to larger, state-of-the-art recurrent language models (LSTMs as trained in).", "labels": [], "entities": []}, {"text": "Our second contribution is an analysis of the kind of information learned by the CNN, showing that the network learns to extract a combination of grammatical, semantic, and topical information from tokens of all across the input window, even those that are the farthest from the target.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on three English corpora of different sizes and genres, the first two of which have been used for language modeling evaluation before.", "labels": [], "entities": [{"text": "language modeling evaluation", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.7576319972674052}]}, {"text": "The Penn Treebank contains one million words of newspaper text with 10K words in the vocabulary.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9934085309505463}]}, {"text": "We reuse the preprocessing and training/test/validation division from.", "labels": [], "entities": []}, {"text": "Europarl-NC is a 64-million word corpus that was developed fora Machine Translation shared task (, combining Europarl data (from parliamentary debates in the European Union) and News Commentary data.", "labels": [], "entities": [{"text": "Europarl-NC", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9711319804191589}, {"text": "Machine Translation shared task", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.814953163266182}, {"text": "Europarl data", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9163545966148376}, {"text": "News Commentary data", "start_pos": 178, "end_pos": 198, "type": "DATASET", "confidence": 0.8119194308916727}]}, {"text": "We preprocessed the corpus with tokenization and true-casing tools from the Moses toolkit (.", "labels": [], "entities": []}, {"text": "The vocabulary is composed of words that occur at least 3 times in the training set and contains approximately 60K words.", "labels": [], "entities": []}, {"text": "We use the validation and test set of the MT shared task.", "labels": [], "entities": [{"text": "MT shared task", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.905758003393809}]}, {"text": "Finally, we took a subset of the ukWaC corpus, which was constructed by crawling UK websites ().", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9898381233215332}]}, {"text": "The training subset contains 200 million words and the vocabulary consists of the 200K words that appear more than 5 times in the training subset.", "labels": [], "entities": []}, {"text": "The validation and test sets are different subsets of the ukWaC corpus, both containing 120K words.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9922972023487091}]}, {"text": "We preprocessed the data similarly to what we did for Europarl-NC.", "labels": [], "entities": [{"text": "Europarl-NC", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9877088069915771}]}, {"text": "We train our models using Stochastic Gradient Descent (SGD), which is relatively simple to tune compared to other optimization methods that involve additional hyper parameters (such as alpha in RMSprop) while being still fast and effective.", "labels": [], "entities": []}, {"text": "SGD is commonly used in similar work).", "labels": [], "entities": []}, {"text": "The learning rate is kept fixed during a single epoch, but we reduce it by a fixed proportion every time the validation perplexity increases by the end of the epoch.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9106720387935638}, {"text": "validation", "start_pos": 109, "end_pos": 119, "type": "TASK", "confidence": 0.952509880065918}]}, {"text": "The values for learning rate, learning rate shrinking and mini-batch sizes as well as context size are fixed once and for all based on insights drawn from previous work) as well as experimentation with the Penn Treebank validation set.", "labels": [], "entities": [{"text": "Penn Treebank validation set", "start_pos": 206, "end_pos": 234, "type": "DATASET", "confidence": 0.9864417165517807}]}, {"text": "Specifically, the learning rate is set to 0.05, with mini-batch size of 128 (we do not take the average of loss over the batch, and the training set is shuffled).", "labels": [], "entities": []}, {"text": "We multiply the learning rate by 0.5 every time we shrink it and clip the gradients if their norm is larger than 12.", "labels": [], "entities": []}, {"text": "The network parameters are initialized randomly on a range from -0.01 to 0.01 and the context size is set to 16.", "labels": [], "entities": []}, {"text": "In Section 6 we show that this large context window is fully exploited.", "labels": [], "entities": []}, {"text": "For the base FFNN and CNN we varied embedding sizes (and thus, number of kernels) k = 128, 256.", "labels": [], "entities": [{"text": "FFNN", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.7241514921188354}, {"text": "CNN", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.8751527070999146}]}, {"text": "For k = 128 we explore the simple CNN, incrementally adding MLPConv and COM variations (in that order) and, alternatively, using a ML-CNN.", "labels": [], "entities": []}, {"text": "For k = 256, we only explore the former three alternatives (i.e. all but the ML-CNN).", "labels": [], "entities": [{"text": "ML-CNN", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9037692546844482}]}, {"text": "For the kernel size, we set it tow = 3 words for the simple CNN (out of options 3, 5, 7, 9), whereas for the COM variant we use w = 3 and 5, based on experimentation on PTB.", "labels": [], "entities": [{"text": "PTB", "start_pos": 169, "end_pos": 172, "type": "DATASET", "confidence": 0.9751972556114197}]}, {"text": "However, we observed the models to be generally robust to this parameter.", "labels": [], "entities": []}, {"text": "Dropout rates are tuned specifically for each combination of model and dataset based on the validation perplexity.", "labels": [], "entities": []}, {"text": "We also add small dropout (p = 0.05-0.15) when we train the networks on the smaller corpus (Penn Treebank).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9945962429046631}]}, {"text": "The experimental results for recurrent neural network language models, such as Recurrent Neural Networks (RNN) and Long-Short Term Memory models (LSTM), on the Penn Treebank are quoted from previous work; for Europarl-NC, we train our own models (we also report the performance of these in-house trained RNN and LSTM models on the Penn Treebank for reference).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 160, "end_pos": 173, "type": "DATASET", "confidence": 0.9957990944385529}, {"text": "Europarl-NC", "start_pos": 209, "end_pos": 220, "type": "DATASET", "confidence": 0.9826112389564514}, {"text": "Penn Treebank", "start_pos": 331, "end_pos": 344, "type": "DATASET", "confidence": 0.9965835213661194}]}, {"text": "Specifically, we train LSTMs with embedding size k = 256 and number of layers L = 2 as well ask = 512 with L = 1, 2.", "labels": [], "entities": []}, {"text": "We train one RNN with k = 512 and L = 2.", "labels": [], "entities": []}, {"text": "To train these models, we use the published source code from.", "labels": [], "entities": []}, {"text": "Our own models are also implemented in Torch7 for easier comparison.", "labels": [], "entities": []}, {"text": "1 Finally, we selected the best performing convolutional and recurrent language models on Europarl-NC and the Baseline FFLM to be evaluated on the ukWaC corpus.", "labels": [], "entities": [{"text": "Europarl-NC", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.9955828785896301}, {"text": "ukWaC corpus", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.9934229850769043}]}, {"text": "For all models trained on Europarl-NC and ukWaC, we speedup training by approximating the softmax with Noise Contrastive Estimation (NCE) (, with the parameters being set following previous work.", "labels": [], "entities": [{"text": "Europarl-NC", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9923964142799377}, {"text": "ukWaC", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8085958361625671}, {"text": "Noise Contrastive Estimation (NCE)", "start_pos": 103, "end_pos": 137, "type": "METRIC", "confidence": 0.8707925975322723}]}, {"text": "Concretely, for each predicted word, we sample 10 words from the unigram distribution, and the normalization factor is such that ln Z = 9.", "labels": [], "entities": []}, {"text": "For comparison, we also implemented a simpler version of the FFNN without dropout and highway layers ().", "labels": [], "entities": [{"text": "FFNN", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8088558316230774}]}, {"text": "These networks have two hidden layers () with the size of 2 times the embedding size (k), thus having the same number of parameters as our baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on Penn Treebank and Europarl-NC. Figure of merit is perplexity (lower is better). Legend: k: embedding size", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9925656318664551}, {"text": "Europarl-NC", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.8967005610466003}]}]}