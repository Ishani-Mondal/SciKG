{"title": [{"text": "There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references.", "labels": [], "entities": [{"text": "evaluating grammatical error correction (GEC)", "start_pos": 34, "end_pos": 79, "type": "TASK", "confidence": 0.8018723598548344}]}, {"text": "However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard.", "labels": [], "entities": []}, {"text": "We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics.", "labels": [], "entities": []}, {"text": "By interpolating both methods, we achieve state-of-the-art correlation with human judgments.", "labels": [], "entities": []}, {"text": "Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level.", "labels": [], "entities": []}, {"text": "We have setup a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics.", "labels": [], "entities": [{"text": "benchmarking GEC output", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6926267842451731}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) has been evaluated by comparing the changes made by a system to the corrections made in gold-standard annotations.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8342321415742239}]}, {"text": "Following the recent shared tasks in this field (e.g., ), several papers have critiqued GEC metrics and proposed new methods.", "labels": [], "entities": []}, {"text": "Existing metrics depend on gold-standard corrections and therefore have a notable weakness: systems are penalized for making corrections that do not appear in the references.", "labels": [], "entities": []}, {"text": "For example, the following output has low metric scores even though three appropriate corrections were made to the input: These changes (in red) were not seen in the references and therefore the metrics GLEU and M 2 (described in \u00a72) score this output worse than 75% of 15,000 other generated sentences.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.703866183757782}]}, {"text": "While grammaticality-based, reference-less metrics have been effective in estimating the quality of machine translation (MT) output, the utility of such metrics has not been investigated previously for GEC.", "labels": [], "entities": [{"text": "machine translation (MT) output", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.831611305475235}, {"text": "GEC", "start_pos": 202, "end_pos": 205, "type": "DATASET", "confidence": 0.583339512348175}]}, {"text": "We hypothesize that such methods can overcome this weakness in reference-based GEC metrics.", "labels": [], "entities": []}, {"text": "This paper has four contributions: 1) We develop three grammaticality metrics that are competitive with current reference-based measures and correlate very strongly with human judgments.", "labels": [], "entities": []}, {"text": "2) We achieve state-of-the-art performance when interpolating a leading reference-based metric with a grammaticality metric.", "labels": [], "entities": []}, {"text": "3) We identify an interesting result that the mean of sentence-level scores is substantially better for evaluating systems than the system-level score.", "labels": [], "entities": []}, {"text": "4) We release code for two grammaticality metrics and establish an online platform for evaluating GEC output.", "labels": [], "entities": [{"text": "GEC output", "start_pos": 98, "end_pos": 108, "type": "TASK", "confidence": 0.49837639927864075}]}], "datasetContent": [{"text": "To assess the proposed metrics, we apply the RBMs, GBMs, and interpolated metrics to score the output of 12 systems participating in the CoNLL-2014 Shared Task on GEC ( ).", "labels": [], "entities": [{"text": "GBMs", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.6050230264663696}, {"text": "CoNLL-2014 Shared Task on GEC", "start_pos": 137, "end_pos": 166, "type": "DATASET", "confidence": 0.6374264597892761}]}, {"text": "Recent works have evaluated RBMs by collecting human rankings of these system outputs and comparing them to the metric rankings (.", "labels": [], "entities": [{"text": "RBMs", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.9762192368507385}]}, {"text": "In this section, we compare each metric's ranking to the human ranking of).", "labels": [], "entities": []}, {"text": "We use 20 references for scoring with RBMs: 2 original references, 10 references collected by, and 8 references collected by.", "labels": [], "entities": []}, {"text": "The motivations for using 20 references are twofold: the best GEC evaluation method uses these 20 references with the GLEU metric (, and work in machine translation shows that more references are better for evaluation).", "labels": [], "entities": [{"text": "GLEU metric", "start_pos": 118, "end_pos": 129, "type": "METRIC", "confidence": 0.9631788432598114}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7280826270580292}]}, {"text": "Due to the low agreement discussed in \u00a73, having more references can be beneficial for evaluating a system when there are multiple viable ways of correcting a sentence.", "labels": [], "entities": []}, {"text": "Unlike previous GEC evaluations, all metrics reported here use the mean of the sentence-level scores for each system.", "labels": [], "entities": []}, {"text": "The error-count metrics, ER and LT, have stronger correlation than all RBMs except for GLEU, which is the current state of the art.", "labels": [], "entities": [{"text": "ER", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9807995557785034}, {"text": "LT", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.8117473721504211}, {"text": "GLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9957022070884705}]}, {"text": "GLEU has the strongest correlation with the human ranking (\u03c1 = 0.852, r = 0.838), followed closely by ER, which has slightly lower Pearson correlation (r = 0.829) but equally as strong rank correlation, which is arguably more important when comparing different systems.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8700027465820312}, {"text": "ER", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9957075119018555}, {"text": "Pearson correlation", "start_pos": 131, "end_pos": 150, "type": "METRIC", "confidence": 0.993798553943634}]}, {"text": "I-measure and LFM have similar strength correlations, and M 2 is the lowest performing metric, even though its correlation is still strong (\u03c1 = 0.648, r = 0.641).", "labels": [], "entities": []}, {"text": "Genectic testing is a personal decision , the kowledge that there is a possiblity that one could be a carrier or not . 3 1 Genetic testing is a personal decision , with the knowledge that there is a possibility that one could be a carrier or not . [0,1] to find the oracle value.", "labels": [], "entities": []}, {"text": "shows the correlations between the human judgments and the oracle interpolated metrics.", "labels": [], "entities": []}, {"text": "Correlations of interpolated metrics are the upper bound and the correlations of uninterpolated metrics (in the first column and first row) are the lower bound.", "labels": [], "entities": []}, {"text": "Interpolating GLEU and IM with GBMs has stronger correlation than any uninterpolated metric (i.e. \u03bb = 0 or 1), and the oracle interpolation of ER and GLEU manifests the strongest correlation with the human ranking (\u03c1 = 0.885, r = 0.867).", "labels": [], "entities": []}, {"text": "5 M 2 has the weakest correlation of all uninterpolated metrics and, when combined with GBMs, three of the interpolated metrics have \u03bb = 0, meaning the interpolated score is equivalent to the GBM and M 2 does not contribute.", "labels": [], "entities": [{"text": "GBM", "start_pos": 192, "end_pos": 195, "type": "DATASET", "confidence": 0.781233549118042}]}, {"text": "presents an example of how interpolation can help evaluation.", "labels": [], "entities": []}, {"text": "The top two sentences ranked by GLEU have misspellings that were not corrected in the NUCLE references.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.6918829679489136}, {"text": "NUCLE references", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.976972758769989}]}, {"text": "Interpolating with a GBM rightly ranks the misspelled output below the corrected output.", "labels": [], "entities": []}, {"text": "Since these experiments use a large number of references, we determine how different reference sizes affect the interpolated metrics by system- atically increasing the number of references from 1 to 20 and randomly choosing n references to use as a gold standard when calculating the RBM scores, repeating 10 times for each value n ().", "labels": [], "entities": [{"text": "RBM", "start_pos": 284, "end_pos": 287, "type": "TASK", "confidence": 0.7106806039810181}]}, {"text": "The correlation is nearly as strong with 3 and 20 references (\u03c1 = 0.884 v.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.961149275302887}]}, {"text": "0.885), and interpolating with just 1 reference is nearly as good (0.878) and improves over any uninterpolated metric.", "labels": [], "entities": []}, {"text": "We acknowledge that using GBMs is in effect using GEC systems to score other GEC systems.", "labels": [], "entities": []}, {"text": "Interestingly, we find that even if the GBMs are imperfect, they still boost performance of the RBMs.", "labels": [], "entities": []}, {"text": "GBMs have been trained to recognize errors in different contexts and, conversely, can identify correct grammatical constructions in diverse contexts, where the RBMs only recognize corrections made that appear in the gold standards, which are limited.", "labels": [], "entities": []}, {"text": "Therefore GBMs can make a nice complement to shortcomings that RBMs may have.", "labels": [], "entities": []}, {"text": "In the course of our experiments, we noticed that I-measure and GLEU have stronger correlations with the expert human ranking when using the: Correlation with human ranking when using corpusand sentence-level metrics.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9985381364822388}]}, {"text": "* indicates a significant difference from the corresponding sentence-level correlation (p < 0.05).", "labels": [], "entities": []}, {"text": "7 mean sentence-level score).", "labels": [], "entities": []}, {"text": "6 Most notably, I-measure does not correlate at all as a corpuslevel metric but has a very strong correlation at the sentence-level (specifically, \u03c1 improves from -0.055 to 0.769).", "labels": [], "entities": []}, {"text": "This could be because corpus-level statistics equally distribute counts of correct annotations overall sentences, even those where the output neglects to make any necessary corrections.", "labels": [], "entities": []}, {"text": "Sentencelevel statistics would not average the correct counts overall sentences in this same way.", "labels": [], "entities": []}, {"text": "As a result, a corpus-level statistic may over-estimate the quality of system output.", "labels": [], "entities": []}, {"text": "Deeper investigation into this phenomenon is needed to understand why the mean sentence-level scores do better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlation between the human and metric rankings.", "labels": [], "entities": []}, {"text": " Table 2: Oracle correlations between interpolated metrics and the human rankings. The \u03bb value for each metric is in parentheses.", "labels": [], "entities": []}, {"text": " Table 4: Correlation with human ranking when using corpus- and sentence-level metrics.  *  indicates a significant difference  from the corresponding sentence-level correlation (p < 0.05). 7", "labels": [], "entities": []}]}