{"title": [{"text": "Anchoring and Agreement in Syntactic Annotations", "labels": [], "entities": [{"text": "Anchoring and Agreement in Syntactic Annotations", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.60737144947052}]}], "abstractContent": [{"text": "We present a study on two key characteristics of human syntactic annotations: anchoring and agreement.", "labels": [], "entities": []}, {"text": "Anchoring is a well known cognitive bias inhuman decision making , where judgments are drawn towards pre-existing values.", "labels": [], "entities": [{"text": "Anchoring", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9068718552589417}, {"text": "decision making", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7491937875747681}]}, {"text": "We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate a clear anchoring effect and reveal un-wanted consequences, including overestima-tion of parsing performance and lower quality of annotations in comparison with human-based annotations.", "labels": [], "entities": []}, {"text": "Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing.", "labels": [], "entities": [{"text": "Penn Treebank WSJ", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9905605912208557}, {"text": "English dependency parsing", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.5596272349357605}]}, {"text": "Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for En-glish newswire.", "labels": [], "entities": [{"text": "parser bias", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8937816321849823}]}, {"text": "We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research in NLP relies heavily on the availability of human annotations for various linguistic prediction tasks.", "labels": [], "entities": [{"text": "linguistic prediction tasks", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.787225604057312}]}, {"text": "Such resources are commonly treated as de facto \"gold standards\" and are used for both training The experimental data in this study will be made publicly available. and evaluation of algorithms for automatic annotation.", "labels": [], "entities": []}, {"text": "At the same time, human agreement on these annotations provides an indicator for the difficulty of the task, and can be instrumental for estimating upper limits for the performance obtainable by computational methods.", "labels": [], "entities": []}, {"text": "Linguistic gold standards are often constructed using pre-existing annotations, generated by automatic tools.", "labels": [], "entities": []}, {"text": "The output of such tools is then manually corrected by human annotators to produce the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9284286499023438}]}, {"text": "The justification for this annotation methodology was first introduced in a set of experiments on POS tag annotation conducted as part of the Penn Treebank project.", "labels": [], "entities": [{"text": "POS tag annotation", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.5968075692653656}, {"text": "Penn Treebank project", "start_pos": 142, "end_pos": 163, "type": "DATASET", "confidence": 0.9871320525805155}]}, {"text": "In this study, the authors concluded that tagger-based annotations are not only much faster to obtain, but also more consistent and of higher quality compared to annotations from scratch.", "labels": [], "entities": []}, {"text": "Following the Penn Treebank, syntactic annotation projects for various languages, including German (), French (,) and many others, were annotated using automatic tools as a starting point.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.9944544732570648}]}, {"text": "Despite the widespread use of this annotation pipeline, there is, to our knowledge, little prior work on syntactic annotation quality and on the reliability of system evaluations on such data.", "labels": [], "entities": []}, {"text": "In this work, we present a systematic study of the influence of automatic tool output on characteristics of annotations created for NLP purposes.", "labels": [], "entities": []}, {"text": "Our investigation is motivated by the hypothesis that annotations obtained using such methodologies maybe subject to the problem of anchoring, a well established and robust cognitive bias in which human decisions are affected by pre-existing values.", "labels": [], "entities": []}, {"text": "In the presence of anchors, participants reason relative to the existing values, and as a result may provide different solutions from those they would have reported otherwise.", "labels": [], "entities": []}, {"text": "Most commonly, anchoring is manifested as an alignment towards the given values.", "labels": [], "entities": []}, {"text": "Focusing on the key NLP tasks of POS tagging and dependency parsing, we demonstrate that the standard approach of obtaining annotations via human correction of automatically generated POS tags and dependencies exhibits a clear anchoring effecta phenomenon we refer to as parser bias.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.8542484939098358}, {"text": "dependency parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.785899817943573}]}, {"text": "Given this evidence, we examine two potential adverse implications of this effect on parser-based gold standards.", "labels": [], "entities": []}, {"text": "First, we show that parser bias entails substantial overestimation of parser performance.", "labels": [], "entities": []}, {"text": "In particular, we demonstrate that bias towards the output of a specific tagger-parser pair leads to over-estimation of the performance of these tools relative to other tools.", "labels": [], "entities": []}, {"text": "Moreover, we observe general performance gains for automatic tools relative to their performance on human-based gold standards.", "labels": [], "entities": []}, {"text": "Second, we study whether parser bias affects the quality of the resulting gold standards.", "labels": [], "entities": []}, {"text": "Extending the experimental setup of, we demonstrate that parser bias may lead to lower annotation quality for parser-based annotations compared to humanbased annotations.", "labels": [], "entities": []}, {"text": "Furthermore, we conduct an experiment on interannotator agreement for POS tagging and dependency parsing which controls for parser bias.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.8460751473903656}, {"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.6996885240077972}]}, {"text": "Our experiment on a subset of section 23 of the WSJ Penn Treebank yields agreement rates of 95.65 for POS tagging and 94.17 for dependency parsing.", "labels": [], "entities": [{"text": "section 23 of the WSJ Penn Treebank", "start_pos": 30, "end_pos": 65, "type": "DATASET", "confidence": 0.7426806773458209}, {"text": "POS tagging", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.7871648967266083}, {"text": "dependency parsing", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.8486292958259583}]}, {"text": "This result is significant in light of the state of the art tagging and parsing performance for English newswire.", "labels": [], "entities": [{"text": "tagging and parsing", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.6690165797869364}, {"text": "English newswire", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.8608581721782684}]}, {"text": "With parsing reaching the level of human agreement, and tagging surpassing it, a more thorough examination of evaluation resources and evaluation methodologies for these tasks is called for.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9755154848098755}]}, {"text": "To summarize, we present the first study to measure and analyze anchoring in the standard parserbased approach to creation of gold standards for POS tagging and dependency parsing in NLP.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.8345436453819275}, {"text": "dependency parsing", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.7373596727848053}]}, {"text": "We conclude that gold standard annotations that are based on editing output of automatic tools can lead to inaccurate figures in system evaluations and lower annotation quality.", "labels": [], "entities": []}, {"text": "Our human agreement experiment, which controls for parser bias, yields agreement rates that are comparable to state of the art automatic tagging and dependency parsing performance, highlighting the need fora more extensive investigation of tagger and parser evaluation in NLP.", "labels": [], "entities": [{"text": "parser bias", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9279379546642303}, {"text": "dependency parsing", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.6787747591733932}]}], "datasetContent": [{"text": "We measure both parsing performance and interannotator agreement using tagging and parsing evaluation metrics.", "labels": [], "entities": [{"text": "parsing", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.9774255156517029}]}, {"text": "This choice allows fora direct comparison between parsing and agreement results.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9727157354354858}]}, {"text": "In this context, POS refers to tagging accuracy.", "labels": [], "entities": [{"text": "POS", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.8094411492347717}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9252389073371887}]}, {"text": "We utilize the standard metrics Unlabeled Attachment Score (UAS) and Label Accuracy (LA) to measure accuracy of head attachment and dependency labels.", "labels": [], "entities": [{"text": "Unlabeled Attachment Score (UAS)", "start_pos": 32, "end_pos": 64, "type": "METRIC", "confidence": 0.8707185387611389}, {"text": "Label Accuracy (LA)", "start_pos": 69, "end_pos": 88, "type": "METRIC", "confidence": 0.9034365177154541}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9975693821907043}]}, {"text": "We also utilize the standard parsing metric Labeled Attachment Score (LAS), which takes into account both dependency arcs and dependency labels.", "labels": [], "entities": [{"text": "parsing metric Labeled Attachment Score (LAS)", "start_pos": 29, "end_pos": 74, "type": "METRIC", "confidence": 0.7318063545972109}]}, {"text": "In all our parsing and agreement experiments, we exclude punctuation tokens from the evaluation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9667155146598816}]}], "tableCaptions": [{"text": " Table 1: Annotator bias towards taggers and parsers on 360 sentences (6,979 tokens) from the FCE. Tagging and parsing results", "labels": [], "entities": [{"text": "Annotator", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8693218231201172}, {"text": "FCE", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9641241431236267}, {"text": "Tagging and parsing", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.6057967940966288}]}, {"text": " Table 2: Human preference rates for a human-based gold stan-", "labels": [], "entities": []}, {"text": " Table 3: Breakdown of the Human preference rates for the", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.5186275839805603}]}, {"text": " Table 4: Inter-annotator agreement on 300 sentences (7,227 to-", "labels": [], "entities": []}]}