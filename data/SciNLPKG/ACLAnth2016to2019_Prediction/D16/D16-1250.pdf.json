{"title": [{"text": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance", "labels": [], "entities": []}], "abstractContent": [{"text": "Mapping word embeddings of different languages into a single space has multiple applications.", "labels": [], "entities": [{"text": "Mapping word embeddings of different languages", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8597592314084371}]}, {"text": "In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary.", "labels": [], "entities": []}, {"text": "In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task.", "labels": [], "entities": [{"text": "translation induction", "start_pos": 189, "end_pos": 210, "type": "TASK", "confidence": 0.9814155697822571}]}], "introductionContent": [{"text": "Bilingual word embeddings have attracted a lot of attention in recent times (.", "labels": [], "entities": [{"text": "Bilingual word embeddings", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7838865717252096}]}, {"text": "A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary.", "labels": [], "entities": []}, {"text": "The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.730794370174408}]}, {"text": "The first method to learn bilingual word embedding mappings was proposed by, who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries.", "labels": [], "entities": []}, {"text": "Subsequent work has proposed alternative optimization objectives to learn better mappings.", "labels": [], "entities": []}, {"text": "incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection.", "labels": [], "entities": []}, {"text": "use canonical correlation analysis to project the embeddings in both languages to a shared vector space.", "labels": [], "entities": []}, {"text": "Beyond linear mappings, apply deep canonical correlation analysis to learn a nonlinear transformation for each language.", "labels": [], "entities": []}, {"text": "Finally, additional techniques have been used to address the hubness problem in, both through the neighbor retrieval method ( ) and the training itself ( ).", "labels": [], "entities": []}, {"text": "We leave the study of non-linear transformation and other additions for further work.", "labels": [], "entities": [{"text": "non-linear transformation", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6870292872190475}]}, {"text": "In this paper, we propose a general framework to learn bilingual word embeddings.", "labels": [], "entities": []}, {"text": "We start with a basic optimization objective ( and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods.", "labels": [], "entities": []}, {"text": "Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them.", "labels": [], "entities": [{"text": "bilingual word embedding mappings", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6350931450724602}]}, {"text": "Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives.", "labels": [], "entities": [{"text": "English-Italian word translation induction", "start_pos": 31, "end_pos": 73, "type": "TASK", "confidence": 0.7642418667674065}, {"text": "English word analogy task", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.6872441321611404}]}], "datasetContent": [{"text": "In this section, we experimentally test the proposed framework and all its variants in comparison with related methods.", "labels": [], "entities": []}, {"text": "For that purpose, we use the translation induction task introduced by, which learns a bilingual mapping on a small dictionary and measures its accuracy on predicting the translation of new words.", "labels": [], "entities": [{"text": "translation induction", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.9676651954650879}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.996019184589386}, {"text": "predicting the translation of new words", "start_pos": 155, "end_pos": 194, "type": "TASK", "confidence": 0.8443277577559153}]}, {"text": "Unfortunately, the dataset they use is not public.", "labels": [], "entities": []}, {"text": "For that reason, we use the English-Italian dataset on the same task provided by  2 . The dataset contains monolingual word embeddings trained with the word2vec toolkit using the CBOW method with negative sampling (Mikolov et al., 2013a) 3 . The English embeddings were trained on a 2.8 billion word corpus (ukWaC + Wikipedia + BNC), while the 1.6 billion word corpus itWaC was used to train the Italian While CCA is typically defined in terms of correlation (thus its name), correlation is invariant to the scaling of variables, so it is possible to constrain the canonical variables to have a fixed variance, as we do, in which case correlation and covariance become equivalent 2 http://clic.cimec.unitn.it/ \u02dc georgiana.", "labels": [], "entities": []}, {"text": "dinu/down/ The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the number of negative samples to 10 embeddings.", "labels": [], "entities": []}, {"text": "The dataset also contains a bilingual dictionary learned from Europarl, split into a training set of 5,000 word pairs and a test set of 1,500 word pairs, both of them uniformly distributed in frequency bins.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9673038721084595}]}, {"text": "Accuracy is the evaluation measure.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9939515590667725}]}, {"text": "Apart from the performance of the projected embeddings in bilingual terms, we are also interested in the monolingual quality of the source language embeddings after the projection.", "labels": [], "entities": []}, {"text": "For that purpose, we use the word analogy task proposed by, which measures the accuracy on answering questions like \"what is the word that is similar to small in the same sense as biggest is similar to big?\" using simple word vector arithmetic.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.7303778976202011}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9988887906074524}]}, {"text": "The dataset they use consists of 8,869 semantic and 10,675 syntactic questions of this type, and is publicly available . In order to speedup the experiments, we follow the authors and perform an approximate evaluation by reducing the vocabulary size according to a frequency threshold of 30,000).", "labels": [], "entities": []}, {"text": "Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves.", "labels": [], "entities": []}, {"text": "With these settings, we obtain a coverage of 64.98%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9979432225227356}]}, {"text": "We implemented the proposed method in Python using NumPy, and make it available as an open source project . The code for and is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of (postprocessing instead of constrained training).", "labels": [], "entities": []}, {"text": "As for the method by, we used their original implementation in Python and MAT-LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our results in bilingual and monolingual tasks.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of our method to other work.", "labels": [], "entities": []}]}