{"title": [{"text": "Mining Inference Formulas by Goal-Directed Random Walks", "labels": [], "entities": [{"text": "Mining Inference Formulas", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7734747529029846}]}], "abstractContent": [{"text": "Deep inference on a large-scale knowledge base (KB) needs amass of formulas, but it is almost impossible to create all formulas manually.", "labels": [], "entities": []}, {"text": "Data-driven methods have been proposed to mine formulas from KBs automatically , where random sampling and approximate calculation are common techniques to handle big data.", "labels": [], "entities": []}, {"text": "Among a series of methods , Random Walk is believed to be suitable for knowledge graph data.", "labels": [], "entities": []}, {"text": "However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference.", "labels": [], "entities": []}, {"text": "Although several heuristic rules have been proposed to direct random walks, they do notwork well due to the diversity of formulas.", "labels": [], "entities": []}, {"text": "To this end, we propose a novel goal-directed inference formula mining algorithm, which directs random walks by the specific inference target at each step.", "labels": [], "entities": [{"text": "inference formula mining", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7026668787002563}]}, {"text": "The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously.", "labels": [], "entities": []}, {"text": "The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9743703603744507}, {"text": "Freebase", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8938256502151489}]}], "introductionContent": [{"text": "Recently, various knowledge bases (KBs), such as Freebase (, WordNet,, have been built, and researchers begin to explore how to make use of structural information to promote performances of several inference-based NLP applications, such as text entailment, knowledge base completion, question and answering.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9658712148666382}, {"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9362555146217346}, {"text": "text entailment", "start_pos": 240, "end_pos": 255, "type": "TASK", "confidence": 0.759131520986557}, {"text": "knowledge base completion", "start_pos": 257, "end_pos": 282, "type": "TASK", "confidence": 0.6387175718943278}, {"text": "question and answering", "start_pos": 284, "end_pos": 306, "type": "TASK", "confidence": 0.8072744011878967}]}, {"text": "Creating useful formulas is one of the most important steps in inference, and an accurate and high coverage formula set will bring a great promotion for an inference system.", "labels": [], "entities": []}, {"text": "For example, Nationality(x, y) \u2227 Nationality(z, y) \u2227 Language(z, w) \u21d2 Language(x, w) is a high-quality formula, which means people with the same nationality probably speak the same language.", "labels": [], "entities": []}, {"text": "However, it is a challenge to create formulas for open-domain KBs, where there area great variety of relation types and it is impossible to construct a complete formula set by hand.", "labels": [], "entities": []}, {"text": "Several data-driven methods, such as Inductive Logic Programming (ILP) and Markov Logic Network (MLN) (), have been proposed to mine formulas automatically from KB data, which transform frequent sub-structures of KBs, e.g., paths or loops, into formulas..a shows a sub-graph extracted from Freebase, and the formula mentioned above about Language can be generated from the loop in.d.", "labels": [], "entities": []}, {"text": "However, the running time of these traditional probabilistic inference methods is unbearable over large-scale KBs.", "labels": [], "entities": []}, {"text": "For example, MLN needs grounding for each candidate formula, i.e., it needs to enumerate all paths.", "labels": [], "entities": [{"text": "MLN", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.677600622177124}]}, {"text": "Therefore, the computation complexity of MLN increases exponentially with the scale of a KB.", "labels": [], "entities": []}, {"text": "In order to handle large-scale KBs, the random walk is usually employed to replace enumerating all possible sub-structures.", "labels": [], "entities": []}, {"text": "However, random walk is inefficient to find useful structures due to its completely randomized mechanism.", "labels": [], "entities": []}, {"text": "For example in Fig- ure 1.b, the target path (yellow one) has a small probability to be visited, the reason is that the algorithm may select all the neighboring entity to transfer with an equal probability.", "labels": [], "entities": [{"text": "Fig- ure 1.b", "start_pos": 15, "end_pos": 27, "type": "DATASET", "confidence": 0.9181120097637177}]}, {"text": "This phenomenon is very common in KBs, e.g., each entity in Freebase has more than 30 neighbors in average, so there will be about 810,000 paths with length 4, and only several are useful.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9509724378585815}]}, {"text": "There have been two types of methods proposed to improve the efficiency of random walks, but they still meet serious problems, respectively.", "labels": [], "entities": []}, {"text": "1) Increasing rounds of random walks.", "labels": [], "entities": []}, {"text": "More rounds of random walks will find more structures, but it will simultaneously introduce more noise and thus generate more false formulas.", "labels": [], "entities": []}, {"text": "For example, the loop in.c exists in Freebase, but it produces a false formula, Gender(x, y) \u2227 Gender(z, y) \u2227 Language(z, w) \u21d2 Language(x, w), which means people with the same gender speak the same language.", "labels": [], "entities": []}, {"text": "This kind of structures frequently occur in KBs even the formulas are mined with a high confidence, because there area lot of sparse structures in KBs which will lead to inaccurate confidence.", "labels": [], "entities": []}, {"text": "According to our statistics, more than 90 percent of high-confidence formulas produced by random walk are noise.", "labels": [], "entities": []}, {"text": "2) Employing heuristic rules to direct random walks.", "labels": [], "entities": []}, {"text": "This method directs random walks to find useful structures by rewriting the state transition probability matrix, but the artificial heuristic rules may only apply to a little part of formulas.", "labels": [], "entities": []}, {"text": "For example, PRA) assumes the more narrow distributions of elements in a formula are, the higher score the formula will obtain.", "labels": [], "entities": [{"text": "PRA", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6670747995376587}]}, {"text": "However, formulas with high scores in PRA are not always true.", "labels": [], "entities": [{"text": "PRA", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.8494206070899963}]}, {"text": "For example, the formula in.c has a high score in PRA, but it is not true.", "labels": [], "entities": [{"text": "PRA", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.6095662117004395}]}, {"text": "Oppositely, formulas with low scores in PRA are not always useless.", "labels": [], "entities": [{"text": "PRA", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9117340445518494}]}, {"text": "For example, the formula, F ather(x, y) \u2227 F ather(y, z) \u21d2 Grandf ather(x, t), has a low score when x and y both have several sons, but it obviously is the most effective to infer Grandf ather.", "labels": [], "entities": []}, {"text": "According to our investigations, the situations are common in KBs.", "labels": [], "entities": []}, {"text": "In this paper, we propose a Goal-directed Random Walk algorithm to resolve the above problems.", "labels": [], "entities": []}, {"text": "The algorithm employs the specific inference target as the direction at each step in the random walk process.", "labels": [], "entities": []}, {"text": "In more detail, to achieve such a goaldirected mechanism, at each step of random walk, the algorithm dynamically estimates the potentials for each neighbor by using the ultimate goal, and assigns higher probabilities to the neighbors with higher potentials.", "labels": [], "entities": []}, {"text": "Therefore, the algorithm is more inclined to visit structures which are beneficial to infer the target and avoid transferring to noise structures.", "labels": [], "entities": []}, {"text": "For example in, when the inference target is what language a person speaks, the algorithm is more inclined to walk along Nationality edge than Gender, because Nationality has greater potential than Gender to infer Language.", "labels": [], "entities": []}, {"text": "We build areal potential function based on low-rank distributional representations.", "labels": [], "entities": []}, {"text": "The reason of replacing symbols by distributional representations is that the distributional representations have less parameters and latent semantic relationship in them can contribute to estimate potentials more precisely.", "labels": [], "entities": []}, {"text": "In summary, the contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "\u2022 Compared with the basic random walk, our approach direct random walks by the inference target, which increases efficiency of mining useful formulas and has a great capability of resisting noise.", "labels": [], "entities": []}, {"text": "\u2022 Compared with the heuristic methods, our approach can learn the strategy of random walk automatically and dynamically adjust the strategy for different inference targets, while the heuristic methods need to write heuristic rules by hand and follow the same rule all the time.", "labels": [], "entities": []}, {"text": "\u2022 The experiments on link prediction task prove that our approach has a high efficiency on mining formulas and has a good performance on both WN18 and FB15K datasets.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8865806460380554}, {"text": "WN18", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.9571061134338379}, {"text": "FB15K datasets", "start_pos": 151, "end_pos": 165, "type": "DATASET", "confidence": 0.9166685342788696}]}, {"text": "The rest of this paper is structured as follows, Section 2 introduces the basic random walk for mining formulas.", "labels": [], "entities": []}, {"text": "Section 3 describes our approach in detail.", "labels": [], "entities": []}, {"text": "The experimental results and related discussions are shown in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 introduces some related works, and finally, Section 6 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first compare our approach with several state-ofart methods on link prediction task to explore our approach's overall ability of inference.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.8500812451044718}]}, {"text": "Subsequently, we evaluate formulas mined by different random walk methods to explore whether the goal-directed mechanism can increase efficiency of mining useful structures.", "labels": [], "entities": []}, {"text": "Finally, we dive deep into the formulas generated by our approach to analyze the characters of our approach.", "labels": [], "entities": []}, {"text": "We conduct experiments on both WN18 and FB15K datasets which are subsets sampled from WordNet and, respectively, and shows the statistics of them.", "labels": [], "entities": [{"text": "WN18", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9659191370010376}, {"text": "FB15K datasets", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9526766240596771}, {"text": "WordNet", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.982036292552948}]}, {"text": "For the link prediction task, we predict the missing h or t fora triplet r(h, t) in test set.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8516421914100647}]}, {"text": "The detail evaluation method is that tin r(h, t) is replaced by all entities in the KB and methods need to rank the right answer at the top of the list, and so does h in r(h, t).", "labels": [], "entities": [{"text": "detail", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9504717588424683}]}, {"text": "We report the mean of those true answer ranks and the Hits@10 under both 'raw' and 'filter' as TransE () does, where Hits@10 is the proportion of correct entities ranked in the top 10.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Link Prediction Results on both WN18 and FB15K", "labels": [], "entities": [{"text": "Link Prediction", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.845780223608017}, {"text": "WN18", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9297223687171936}, {"text": "FB15K", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.8880293965339661}]}]}