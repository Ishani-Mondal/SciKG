{"title": [], "abstractContent": [{"text": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7952238668998083}]}, {"text": "However to reach competitive performance , NMT models need to be exceedingly large.", "labels": [], "entities": []}, {"text": "In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7099341601133347}, {"text": "NMT", "start_pos": 214, "end_pos": 217, "type": "TASK", "confidence": 0.8791172504425049}]}, {"text": "We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model).", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7502050697803497}, {"text": "NMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9374213814735413}, {"text": "beam search", "start_pos": 282, "end_pos": 293, "type": "TASK", "confidence": 0.7484101355075836}]}, {"text": "Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance.", "labels": [], "entities": []}, {"text": "It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy de-coding/beam search.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.996993899345398}]}, {"text": "Applying weight pruning on top of knowledge distillation results in a student model that has 13\u00d7 fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9985309839248657}]}], "introductionContent": [{"text": "Neural machine translation (NMT)) is a deep learningbased method for translation that has recently shown promising results as an alternative to statistical approaches.", "labels": [], "entities": [{"text": "Neural machine translation (NMT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7598919918139776}]}, {"text": "NMT systems directly model the probability of the next word in the target sentence simply by conditioning a recurrent neural network on the source sentence and previously generated target words.", "labels": [], "entities": []}, {"text": "While both simple and surprisingly accurate, NMT systems typically need to have very high capacity in order to perform well: used a 4-layer LSTM with 1000 hidden units per layer (herein 4\u00d71000) and obtained state-of-the-art results on English \u2192 French with a 16-layer LSTM with 512 units per layer.", "labels": [], "entities": []}, {"text": "The sheer size of the models requires cutting-edge hardware for training and makes using the models on standard setups very challenging.", "labels": [], "entities": []}, {"text": "This issue of excessively large networks has been observed in several other domains, with much focus on fully-connected and convolutional networks for multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 151, "end_pos": 177, "type": "TASK", "confidence": 0.7703989446163177}]}, {"text": "Researchers have particularly noted that large networks seem to be necessary for training, but learn redundant representations in the process.", "labels": [], "entities": []}, {"text": "Therefore compressing deep models into smaller networks has been an active area of research.", "labels": [], "entities": []}, {"text": "As deep learning systems obtain better results on NLP tasks, compression also becomes an important practical issue with applications such as running deep learning models for speech and translation locally on cell phones.", "labels": [], "entities": []}, {"text": "Existing compression methods generally fall into two categories: (1) pruning and (2) knowledge distillation.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7671424448490143}]}, {"text": "Pruning methods (), zero-out weights or entire neurons based on an importance criterion: use (a diagonal approximation to) the Hessian to identify weights whose removal minimally impacts the objective function, while remove weights based on thresholding their absolute values.", "labels": [], "entities": []}, {"text": "Knowledge distillation approaches (;) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L 2 or cross-entropy) between the student and teacher output.", "labels": [], "entities": [{"text": "Knowledge distillation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6979789733886719}]}, {"text": "In this work, we investigate knowledge distillation in the context of neural machine translation.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7399016916751862}, {"text": "neural machine translation", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.6947406331698099}]}, {"text": "We note that NMT differs from previous work which has mainly explored non-recurrent models in the multiclass prediction setting.", "labels": [], "entities": [{"text": "NMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.774823784828186}, {"text": "multiclass prediction", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7182462513446808}]}, {"text": "For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence outputs conditioned on previous decisions.", "labels": [], "entities": [{"text": "NMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.858170747756958}, {"text": "multi-class prediction", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6982519626617432}]}, {"text": "With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to approximately match the sequence-level (as opposed to word-level) distribution of the teacher network.", "labels": [], "entities": []}, {"text": "This sequence-level approximation leads to a simple training procedure wherein the student network is trained on a newly generated dataset that is the result of running beam search with the teacher network.", "labels": [], "entities": []}, {"text": "We run experiments to compress a large state-ofthe-art 4 \u00d7 1000 LSTM model, and find that with sequence-level knowledge distillation we are able to learn a 2 \u00d7 500 LSTM that roughly matches the performance of the full system.", "labels": [], "entities": []}, {"text": "We see similar results compressing a 2 \u00d7 500 model down to 2 \u00d7 100 on a smaller data set.", "labels": [], "entities": []}, {"text": "Furthermore, we observe that our proposed approach has other benefits, such as not requiring any beam search at test-time.", "labels": [], "entities": []}, {"text": "As a result we are able to perform greedy decoding on the 2 \u00d7 500 model 10 times faster than beam search on the 4 \u00d7 1000 model with comparable performance.", "labels": [], "entities": [{"text": "beam search", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.7901858985424042}]}, {"text": "Our student models can even be run efficiently on a standard smartphone.", "labels": [], "entities": []}, {"text": "1 Finally, we apply weight pruning on top of the student network to obtain a model that has 13\u00d7 fewer parameters than the original teacher model.", "labels": [], "entities": []}, {"text": "We have released all the code for the models described in this paper.", "labels": [], "entities": []}, {"text": "1 https://github.com/harvardnlp/nmt-android 2 https://github.com/harvardnlp/seq2seq-attn", "labels": [], "entities": []}], "datasetContent": [{"text": "To test out these approaches, we conduct two sets of NMT experiments: high resource (English \u2192 German) and low resource (Thai \u2192 English).", "labels": [], "entities": []}, {"text": "The English-German data comes from WMT 2014.", "labels": [], "entities": [{"text": "WMT 2014", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.9204167723655701}]}, {"text": "The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set.", "labels": [], "entities": []}, {"text": "We keep the top 50k most frequent words, and replace the rest with UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9373699426651001}]}, {"text": "The teacher model is a 4 \u00d7 1000 LSTM (as in Luong et al.) and we train two student models: 2 \u00d7 300 and 2 \u00d7 500.", "labels": [], "entities": []}, {"text": "The Thai-English data comes from IWSLT 2015.", "labels": [], "entities": [{"text": "Thai-English data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8518783748149872}, {"text": "IWSLT 2015", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9412125945091248}]}, {"text": "8 There are 90k sentences in the While we employ a simple (unrealistic) noise function for illustrative purposes, the generative story is quite plausible if we consider a more elaborate noise function which includes additional sources of noise such as phrase reordering, replacement of words with synonyms, etc.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 252, "end_pos": 269, "type": "TASK", "confidence": 0.7394492924213409}]}, {"text": "One could view translation having two sources of variance that should be modeled separately: variance due to the source sentence (t \u223c D), and variance due to the individual translator (y \u223c (t)).", "labels": [], "entities": []}, {"text": "7 http://statmt.org/wmt14 8 https://sites.google.com/site/iwsltevaluation2015/mt-track Size of the teacher model is 2 \u00d7 500 (which performed better than 4\u00d71000, 2\u00d7750 models), and the student model is 2\u00d7100.", "labels": [], "entities": []}, {"text": "We evaluate on tokenized BLEU with multi-bleu.perl, and experiment with the following variations: Word-Level Knowledge Distillation (Word-KD) Student is trained on the original data and additionally trained to minimize the cross-entropy of the teacher distribution at the word-level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9345110654830933}]}, {"text": "We tested \u03b1 \u2208 {0.5, 0.9} and found \u03b1 = 0.5 to work better.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of source words translated per second across", "labels": [], "entities": []}]}