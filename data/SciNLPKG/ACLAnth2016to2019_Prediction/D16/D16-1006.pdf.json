{"title": [{"text": "Nested Propositions in Open Information Extraction", "labels": [], "entities": [{"text": "Nested Propositions in Open Information Extraction", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6913680136203766}]}], "abstractContent": [{"text": "The challenges of Machine Reading and Knowledge Extraction at a web scale require a system capable of extracting diverse information from large, heterogeneous corpora.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.83064866065979}, {"text": "Knowledge Extraction", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.701584056019783}]}, {"text": "The Open Information Extraction (OIE) paradigm aims at extracting assertions from large corpora without requiring a vocabulary or relation-specific training data.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7773833771546682}]}, {"text": "Most systems built on this paradigm extract binary relations from arbitrary sentences, ignoring the context under which the assertions are correct and complete.", "labels": [], "entities": []}, {"text": "They lack the expres-siveness needed to properly represent and extract complex assertions commonly found in the text.", "labels": [], "entities": []}, {"text": "To address the lack of representation power, we propose NESTIE, which uses a nested representation to extract higher-order relations, and complex, interdependent assertions.", "labels": [], "entities": []}, {"text": "Nesting the extracted propositions allows NESTIE to more accurately reflect the meaning of the original sentence.", "labels": [], "entities": [{"text": "NESTIE", "start_pos": 42, "end_pos": 48, "type": "TASK", "confidence": 0.822540283203125}]}, {"text": "Our experimental study on real-world datasets suggests that NESTIE obtains comparable precision with better minimality and informative-ness than existing approaches.", "labels": [], "entities": [{"text": "NESTIE", "start_pos": 60, "end_pos": 66, "type": "TASK", "confidence": 0.8779416084289551}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9982917904853821}]}, {"text": "NESTIE produces 1.7-1.8 times more minimal extractions and achieves 1.1-1.2 times higher informative-ness than CLAUSIE.", "labels": [], "entities": [{"text": "NESTIE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7170870900154114}]}], "introductionContent": [{"text": "Syntactic analyses produced by syntactic parsers area long way from representing the full meaning of the sentences parsed.", "labels": [], "entities": []}, {"text": "In particular, they cannot support questions like \"Who did what to whom?\", \"Where did what happen?\".", "labels": [], "entities": []}, {"text": "Owing to the large, heterogeneous corpora available at web scale, traditional approaches to information extraction) fail to scale to the millions of relations found on the web.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7940556406974792}]}, {"text": "As a response, the paradigm of Open Information Extraction (OIE) (  has seen arise in interest as it eliminates the need for domain knowledge or relation-specific annotated data.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.7923722763856252}]}, {"text": "OIE systems use a collection of patterns over the surface form or dependency tree of a sentence to extract propositions of the form (arg1,rel,arg2).", "labels": [], "entities": []}, {"text": "However, state-of-the-art OIE systems, REVERB) and OLLIE () focus on extracting binary assertions and suffer from three key drawbacks.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9768244624137878}, {"text": "OLLIE", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.8525745272636414}]}, {"text": "First, lack of expressivity of representation leads to significant information loss for higher-order relations and complex assertions.", "labels": [], "entities": []}, {"text": "This results in incomplete, uniformative and incoherent prepositions.", "labels": [], "entities": []}, {"text": "Consider Example 1 in Figure 1.", "labels": [], "entities": []}, {"text": "Important contextual information is either ignored or is subsumed in over-specified argument and relation phrases.", "labels": [], "entities": []}, {"text": "It is not possible to fix such nuances by post-processing the propositions.", "labels": [], "entities": []}, {"text": "This affects downstream applications like Question Answering) which rely on correctness and completeness of the propositions.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8101090788841248}]}, {"text": "Second, natural language frequently includes relations presented in a non-canonical form that cannot be captured by a small set of extraction patterns that only extract relation mediated by verbs or a subset of verbal patterns.", "labels": [], "entities": []}, {"text": "Consider Example 2 in that asserts, \"Rozsa Hill is the third hill near the river\", \"Rozsa Hill is Rose Hill\" and \"Rozsa Hill lies north of Castle Hill\".", "labels": [], "entities": [{"text": "Castle Hill", "start_pos": 139, "end_pos": 150, "type": "DATASET", "confidence": 0.9760801196098328}]}], "datasetContent": [{"text": "We conducted an experimental study to compare NESTIE to other state-of-the-art extractors.", "labels": [], "entities": [{"text": "NESTIE", "start_pos": 46, "end_pos": 52, "type": "TASK", "confidence": 0.682693600654602}]}, {"text": "We found that it achieves higher informativeness and produces more correct and minimal propositions than other extractors.", "labels": [], "entities": []}, {"text": "We used two datasets released by) in our experiments: 200 random sentences from Wikipedia, and 200 random sentences from New York Times (NYT).", "labels": [], "entities": [{"text": "New York Times (NYT)", "start_pos": 121, "end_pos": 141, "type": "DATASET", "confidence": 0.6344873855511347}]}, {"text": "We compared  NESTIE against three OIE systems: REVERB, OL-LIE and CLAUSIE.", "labels": [], "entities": [{"text": "NESTIE", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.4888997972011566}, {"text": "REVERB", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.993707537651062}, {"text": "OL-LIE", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.8712766766548157}]}, {"text": "Since the source code for each of the extractors was available, we independently ran the extractors on the two datasets.", "labels": [], "entities": []}, {"text": "Next, to make the extractions comparable, we configured the extractors to generate triple propositions.", "labels": [], "entities": []}, {"text": "REVERB and CLAUSIE extractions were available as triples by default.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9639376997947693}, {"text": "CLAUSIE extractions", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7887061834335327}]}, {"text": "OLLIE extends its triple proposition representation.", "labels": [], "entities": [{"text": "OLLIE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.45208683609962463}]}, {"text": "So, we generated an additional extraction for each of the possible extensions of a proposition.", "labels": [], "entities": []}, {"text": "NESTIE uses a nested representation.", "labels": [], "entities": [{"text": "NESTIE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9025790691375732}]}, {"text": "So, we simply extracted the innermost proposition in a nested representation as a triple and allowed the subject and the object in the outer proposition to contain a reference to the inner triple.", "labels": [], "entities": []}, {"text": "By preserving references the context of a proposition is retained while allowing for queries at various granularity levels.", "labels": [], "entities": []}, {"text": "We manually labeled the extractions obtained from all extractors to 1) maintain consistency, 2) additionally, assess if extracted triples were informative and minimal.", "labels": [], "entities": [{"text": "consistency", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.996644914150238}]}, {"text": "Some extractors use heuristics to identify arguments and/or relation phrase boundaries, which leads to over-specific arguments that render the extractions unusable for other downstream applications.", "labels": [], "entities": []}, {"text": "To assess the usability of extractions, we evaluated them for minimality.", "labels": [], "entities": []}, {"text": "Furthermore, the goal of our system is to extract as many propositions as possible and lose as little information as possible.", "labels": [], "entities": []}, {"text": "We measure this as informativeness of the set of the extractions fora sentence.", "labels": [], "entities": []}, {"text": "Since computing informativeness as a percentage of text contained in at least one extraction could be biased towards long extractions, we used an explicit rating scale to measure informativeness.", "labels": [], "entities": []}, {"text": "Two CS graduate student labeled each extraction for correctness (0 or 1) and minimality (0 or 1).", "labels": [], "entities": [{"text": "correctness", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9819601774215698}]}, {"text": "For each sentence, they label the set of extractions for informativeness (0-5).", "labels": [], "entities": []}, {"text": "An extraction is marked correct if it is asserted in the text and correctly captures the contextual information.", "labels": [], "entities": []}, {"text": "An extraction is considered minimal if the arguments are not over-specified i.e. they don't subsume another extraction or have conjunctions or are excessively long.", "labels": [], "entities": []}, {"text": "Lastly, they rank the set of extractions on a scale of 0-5 (0 for bad, 5 for good) based on the coverage of information in the original sentence.", "labels": [], "entities": []}, {"text": "The agreement between labelers was measured in terms of Cohens Kappa.", "labels": [], "entities": [{"text": "Cohens Kappa", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.7472746968269348}]}], "tableCaptions": []}