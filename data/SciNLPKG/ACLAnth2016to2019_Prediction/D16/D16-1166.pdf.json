{"title": [], "abstractContent": [{"text": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base.", "labels": [], "entities": [{"text": "question answering", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.8798407912254333}]}, {"text": "We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the Sim-pleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles.", "labels": [], "entities": [{"text": "single-relation question answering", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.666122317314148}, {"text": "Sim-pleQuestions dataset", "start_pos": 113, "end_pos": 137, "type": "DATASET", "confidence": 0.943174421787262}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9905341863632202}]}, {"text": "Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Single-relation factoid questions are the most common form of questions found in search query logs and community question answering websites.", "labels": [], "entities": [{"text": "community question answering", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.7220010558764139}]}, {"text": "A knowledge-base (KB) such as Freebase, DBpedia, or Wikidata can help answer such questions after users reformulate them as queries.", "labels": [], "entities": []}, {"text": "For instance, the question \"Where was Barack Obama born?\" can be answered by issuing the following KB query: \u03bb(x).place of birth However, automatically mapping a natural language question such as \"Where was Barack Obama born?\" to its corresponding KB query remains a challenging task.", "labels": [], "entities": []}, {"text": "There are three key issues that make learning this mapping non-trivial.", "labels": [], "entities": []}, {"text": "First, there are many paraphrases of the same question.", "labels": [], "entities": []}, {"text": "Second, many of the KB entries are unseen during training time; however, we still need to correctly predict them attest time.", "labels": [], "entities": []}, {"text": "Third, a KB such as Freebase typically contains millions of entities and thousands of predicates, making it difficult fora system to predict these entities at scale (.", "labels": [], "entities": []}, {"text": "In this paper, we address all three of these issues with a character-level encoder-decoder framework that significantly improves performance over state-of-the-art word-level neural models, while also providing a much more compact model that can be learned from less data.", "labels": [], "entities": []}, {"text": "First, we use along short-term memory (LSTM)) encoder to embed the question.", "labels": [], "entities": []}, {"text": "Second, to make our model robust to unseen KB entries, we extract embeddings for questions, predicates and entities purely from their character-level representations.", "labels": [], "entities": []}, {"text": "Characterlevel modeling has been previously shown to generalize well to new words not seen during training, which makes it ideal for this task.", "labels": [], "entities": [{"text": "Characterlevel modeling", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8537735044956207}]}, {"text": "Third, to scale our model to handle the millions of entities and thousands of predicates in the KB, instead of using a large output layer in the decoder to directly predict the entity and predicate, we use a general interaction function between the question embeddings and KB embeddings that measures their semantic relevance to determine the output.", "labels": [], "entities": []}, {"text": "The combined use of character-: Our encoder-decoder architecture that generates a query against a structured knowledge base.", "labels": [], "entities": []}, {"text": "We encode our question via along short-term memory (LSTM) network and an attention mechanism to produce our context vector.", "labels": [], "entities": []}, {"text": "During decoding, at each time step, we feed the current context vector and an embedding of the English alias of the previously generated knowledge base entry into an attention-based decoding LSTM to generate the new candidate entity or predicate.", "labels": [], "entities": []}, {"text": "level modeling and a semantic relevance function allows us to successfully produce likelihood scores for the KB entries that are not present in our vocabulary, a challenging task for standard encoderdecoder frameworks.", "labels": [], "entities": []}, {"text": "Our novel, character-level encoder-decoder model is compact, requires significantly less data to train than previous work, and is able to generalize well to unseen entities in test time.", "labels": [], "entities": []}, {"text": "In particular, without use of ensembles, we achieve 70.9% accuracy in the Freebase2M setting and 70.3% accuracy in the Freebase5M setting on the SimpleQuestions dataset, outperforming the previous state-of-arts of 62.7% and 63.9% () by 8.2% and 6.4% respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9990703463554382}, {"text": "Freebase2M setting", "start_pos": 74, "end_pos": 92, "type": "DATASET", "confidence": 0.9445802867412567}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9992017149925232}, {"text": "Freebase5M setting", "start_pos": 119, "end_pos": 137, "type": "DATASET", "confidence": 0.9242266118526459}, {"text": "SimpleQuestions dataset", "start_pos": 145, "end_pos": 168, "type": "DATASET", "confidence": 0.9193615317344666}]}, {"text": "Moreover, we only use the training questions provided in SimpleQuestions to train our model, which cover about 24% of words in entity aliases on the test set.", "labels": [], "entities": []}, {"text": "This demonstrates the robustness of the character-level model to unseen entities.", "labels": [], "entities": []}, {"text": "In contrast, data augmentation is usually necessary to provide more coverage for unseen entities and predicates, as done in previous work).", "labels": [], "entities": [{"text": "data augmentation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6914149224758148}]}], "datasetContent": [{"text": "We evaluate the proposed model on the SimpleQuestions dataset ().", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.9252593517303467}]}, {"text": "The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.9588445425033569}]}, {"text": "It is split into 75,910 train, 10,845 validation, and 21,687 test questions.", "labels": [], "entities": []}, {"text": "Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set.", "labels": [], "entities": []}, {"text": "For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively.", "labels": [], "entities": [{"text": "FB2M", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.6624781489372253}, {"text": "FB5M", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.5488807559013367}]}, {"text": "The former uses a KB for candidate generation which is a sub-set of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.974396288394928}, {"text": "Freebase", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.9716854095458984}]}, {"text": "In our experiments, the Memory Neural Networks (MemNNs) proposed in serve as the baselines.", "labels": [], "entities": []}, {"text": "For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions (), 15M paraphrases from, and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively.", "labels": [], "entities": [{"text": "FB2M", "start_pos": 219, "end_pos": 223, "type": "DATASET", "confidence": 0.972250759601593}, {"text": "FB5M", "start_pos": 228, "end_pos": 232, "type": "DATASET", "confidence": 0.847618579864502}]}, {"text": "In contrast, our models are trained only on the 76K questions in the training set.", "labels": [], "entities": []}, {"text": "For our model, both layers of the LSTM-based question encoder have size 200.", "labels": [], "entities": []}, {"text": "The hidden layers of the LSTM-based decoder have size 100, and the CNNs for entity and predicate embeddings have a hidden layer of size 200 and an output layer of size 100.", "labels": [], "entities": []}, {"text": "The CNNs for entity and predicate embeddings use a receptive field of size 4, \u03bb = 5, and m = 100.", "labels": [], "entities": []}, {"text": "We train the models using RMSProp with a learning rate of 1e \u22124 . In order to make the input character sequence long enough to fill up the receptive fields of multiple CNN layers, we pad each predicate or entity using three padding symbols P , a special start symbol, and a special end symbol.", "labels": [], "entities": []}, {"text": "For instance, Obama would become S start PP P ObamaP PP S end . For consistency, we apply the same padding to the questions.", "labels": [], "entities": [{"text": "consistency", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.9586883187294006}]}, {"text": "We carryout ablation studies in Sections 5.2.1 and 5.2.2 through a set of random-sampling experiments.", "labels": [], "entities": []}, {"text": "In these experiments, for each question, we randomly sample 200 entities and predicates from the test set as noise samples.", "labels": [], "entities": []}, {"text": "We then mix the gold entity and predicate into these negative samples, and evaluate the accuracy of our model in predicting the gold predicate or entity from this mixed set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9996373653411865}]}], "tableCaptions": [{"text": " Table 1: Experimental results on the SimpleQuestions dataset. MemNN results are from Bordes et al.  (2015). WQ, SIQ and PRP stand for WebQuestions, SimpleQuestions and paraphrases from WikiAnswers.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.7942772209644318}, {"text": "MemNN", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.7028109431266785}]}, {"text": " Table 2: Results for a random sampling experiment where we varied the number of layers used for convolutions and", "labels": [], "entities": []}, {"text": " Table 3: Results for a random sampling experiment where we varied the embedding type (word vs. character-level).", "labels": [], "entities": []}]}