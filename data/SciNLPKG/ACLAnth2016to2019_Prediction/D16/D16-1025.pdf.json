{"title": [{"text": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "labels": [], "entities": [{"text": "Neural versus Phrase-Based Machine Translation", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.6840293407440186}]}], "abstractContent": [{"text": "Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.8622133334477743}]}, {"text": "In particular , at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences.", "labels": [], "entities": [{"text": "IWSLT 2015 evaluation campaign", "start_pos": 23, "end_pos": 53, "type": "DATASET", "confidence": 0.7197824418544769}]}, {"text": "To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural vs. phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data.", "labels": [], "entities": [{"text": "SMT outputs", "start_pos": 140, "end_pos": 151, "type": "TASK", "confidence": 0.8364541232585907}, {"text": "IWSLT data", "start_pos": 233, "end_pos": 243, "type": "DATASET", "confidence": 0.968462735414505}]}, {"text": "For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models-such as the reordering of verbs-while pointing out other aspects that remain to be improved.", "labels": [], "entities": []}], "introductionContent": [{"text": "The wave of neural models has eventually reached the field of Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.8814403017361959}]}, {"text": "After a period in which Neural MT (NMT) was too computationally costly and resource demanding to compete with state-of-the-art Phrase-Based MT (PBMT) 1 , the situation changed in 2015.", "labels": [], "entities": []}, {"text": "For the first time, in the latest edition of IWSLT 2 (Cettolo et al., 2015), the system described in) overtook a variety of PBMT approaches with a large margin (+5.3 BLEU points) on a difficult language pair like English-German -anticipating what, most likely, will be the new NMT era.", "labels": [], "entities": [{"text": "IWSLT 2 (Cettolo et al., 2015)", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.9049196177058749}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9987615346908569}, {"text": "NMT", "start_pos": 277, "end_pos": 280, "type": "DATASET", "confidence": 0.7281931638717651}]}, {"text": "This impressive improvement follows the distance reduction previously observed in the WMT 2015 shared translation task ().", "labels": [], "entities": [{"text": "WMT 2015 shared translation task", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.7160800099372864}]}, {"text": "Just few months earlier, the NMT systems described in) ranked on par with the best phrase-based models on a couple of language pairs.", "labels": [], "entities": []}, {"text": "Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in, with the use of the attention mechanism (.", "labels": [], "entities": []}, {"text": "This evolution has several implications.", "labels": [], "entities": []}, {"text": "On one side, NMT represents a simplification with respect to previous paradigms.", "labels": [], "entities": []}, {"text": "From a management point of view, similar to PBMT, it allows fora more efficient use of human and data resources with respect to rulebased MT.", "labels": [], "entities": []}, {"text": "From the architectural point of view, a large recurrent network trained for end-to-end translation is considerably simpler than traditional MT systems that integrate multiple components and processing steps.", "labels": [], "entities": [{"text": "MT", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9690870046615601}]}, {"text": "On the other side, the NMT process is less transparent than previous paradigms.", "labels": [], "entities": []}, {"text": "Indeed, it represents a further step in the evolution from rule-based approaches that explicitly manipulate knowledge, to the statistical/data-driven framework, still comprehensible in its inner workings, to a sub-symbolic framework in which the translation process is totally opaque to the analysis.", "labels": [], "entities": []}, {"text": "What do we know about the strengths of NMT and the weaknesses of PBMT?", "labels": [], "entities": [{"text": "NMT", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.731216549873352}, {"text": "PBMT", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.8097549676895142}]}, {"text": "What are the linguistic phenomena that deep learning translation models can handle with such greater effectiveness?", "labels": [], "entities": [{"text": "deep learning translation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6419376730918884}]}, {"text": "To answer these questions and go beyond poorly informative BLEU scores, we perform the very first comparative analysis of the two paradigms in order to shed light on the factors that differentiate them and determine their large quality differences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9787303805351257}]}, {"text": "We build on evaluation data available for the IWSLT 2015 MT English-German task, and compare the results of the first four top-ranked participants.", "labels": [], "entities": [{"text": "IWSLT 2015 MT English-German task", "start_pos": 46, "end_pos": 79, "type": "DATASET", "confidence": 0.7529008507728576}]}, {"text": "We choose to focus on one language pair and one task because of the following advantages: (i) three state-of-the art PBMT systems compared against the NMT system on the same data and in the very same period (that of the evaluation campaign); (ii) a challenging language pair in terms of morphology and word order differences; (iii) availability of MT outputs' post-editing done by professional translators, which is very costly and thus rarely available.", "labels": [], "entities": [{"text": "MT outputs' post-editing", "start_pos": 348, "end_pos": 372, "type": "TASK", "confidence": 0.8974124789237976}]}, {"text": "In general, post-edits have the advantage of allowing for informative and detailed analyses since they directly point to translation errors.", "labels": [], "entities": []}, {"text": "In this specific framework, the high quality data created by professional translators guarantees reliable evaluations.", "labels": [], "entities": []}, {"text": "For all these reasons we present our study as a solid contribution to the better understanding of this new paradigm shift in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9915798902511597}]}, {"text": "After reviewing previous work (Section 2), we introduce the analyzed data and the systems that produced them (Section 3).", "labels": [], "entities": []}, {"text": "We then present three increasingly fine levels of MT quality analysis.", "labels": [], "entities": [{"text": "MT quality analysis", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.864918847878774}]}, {"text": "We first investigate how MT systems' quality varies with specific characteristics of the input, i.e. sentence length and type of content of each talk (Section 4).", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9928379654884338}]}, {"text": "Then, we focus on differences among MT systems with respect to morphology, lexical, and word order errors (Section 5).", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9804045557975769}]}, {"text": "Finally, based on the finding that word reordering is the strongest aspect of NMT compared to the other systems, we carryout a finegrained analysis of word order errors (Section 6).", "labels": [], "entities": [{"text": "word reordering", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7756607234477997}]}], "datasetContent": [{"text": "We perform a number of analyses on data and results of the IWSLT 2015 MT En-De task, which consists in translating manual transcripts of English TED talks into German.", "labels": [], "entities": [{"text": "IWSLT 2015 MT En-De task", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.7299691081047058}, {"text": "translating manual transcripts of English TED talks", "start_pos": 103, "end_pos": 154, "type": "TASK", "confidence": 0.6986872851848602}]}, {"text": "Evaluation data are publicly available through the WIT 3 repository (Cettolo et al., 2012).", "labels": [], "entities": [{"text": "WIT 3 repository (Cettolo et al., 2012)", "start_pos": 51, "end_pos": 90, "type": "DATASET", "confidence": 0.8449080169200898}]}, {"text": "Five systems participated in the MT En-De task and were manually evaluated on a representative subset of the official  talks, fora total of 600 sentences and around 10K words.", "labels": [], "entities": [{"text": "MT En-De task", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9060854514439901}]}, {"text": "Five professional translators were asked to post-edit the MT output by applying the minimal edits required to transform it into a fluent sentence with the same meaning as the source sentence.", "labels": [], "entities": [{"text": "MT output", "start_pos": 58, "end_pos": 67, "type": "TASK", "confidence": 0.8859277367591858}]}, {"text": "Data were prepared so that all translators equally post-edited the five MT outputs, i.e. 120 sentences for each evaluated system.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.8906271457672119}]}, {"text": "The resulting evaluation data consist of five new reference translations for each of the sentences in the HE set.", "labels": [], "entities": [{"text": "HE set", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.8226810395717621}]}, {"text": "Each one of these references represents the targeted translation of the system output from which it was derived, but the other four additional translations can also be used to evaluate each MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 190, "end_pos": 192, "type": "TASK", "confidence": 0.9594293236732483}]}, {"text": "We will see in the next sections how we exploited the available post-edits in the more suitable way depending on the kind of analysis carried out.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overall results on the HE Set: BLEU, computed", "labels": [], "entities": [{"text": "HE Set", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.7667241990566254}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9992339611053467}]}, {"text": " Table 4: Word reordering evaluation in terms of shift opera-", "labels": [], "entities": [{"text": "Word reordering", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7912085950374603}]}, {"text": " Table 5: Main POS tags and dependency labels of words oc-", "labels": [], "entities": []}]}