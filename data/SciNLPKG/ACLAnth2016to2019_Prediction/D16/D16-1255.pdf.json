{"title": [], "abstractContent": [{"text": "Recent work on word ordering has argued that syntactic structure is important, or even required , for effectively recovering the order of a sentence.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.739803284406662}]}, {"text": "We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task.", "labels": [], "entities": []}, {"text": "Furthermore, we show that along short-term memory (LSTM) language model is even more effective at recovering order, with our basic model outper-forming a state-of-the-art syntactic model by 11.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9986538887023926}]}, {"text": "Additional data and larger beams yield further gains, at the expense of training and search time.", "labels": [], "entities": []}], "introductionContent": [{"text": "We address the task of recovering the original word order of a shuffled sentence, referred to as bag generation (, shake-and-bake generation, or more recently, linearization, as standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (;.", "labels": [], "entities": [{"text": "bag generation", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7515254616737366}, {"text": "shake-and-bake generation", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.7282589972019196}]}, {"text": "The predominant argument of the more recent works is that jointly recovering explicit syntactic structure is crucial for determining the correct word order of the original sentence.", "labels": [], "entities": []}, {"text": "As such, these methods either generate or rely on given parse structure to reproduce the order.", "labels": [], "entities": []}, {"text": "Independently, Elman (1990) explored linearization in his seminal work on recurrent neural networks.", "labels": [], "entities": []}, {"text": "Elman judged the capacity of early recurrent neural networks via, in part, the network's ability to predict word order in simple sentences.", "labels": [], "entities": [{"text": "predict word order in simple sentences", "start_pos": 100, "end_pos": 138, "type": "TASK", "confidence": 0.7811006307601929}]}, {"text": "He notes, The order of words in sentences reflects a number of constraints.", "labels": [], "entities": []}, {"text": "Syntactic structure, selective restrictions, subcategorization, and discourse considerations are among the many factors which join together to fix the order in which words occur.", "labels": [], "entities": []}, {"text": ".here is an abstract structure which underlies the surface strings and it is this structure which provides a more insightful basis for understanding the constraints on word order.", "labels": [], "entities": []}, {"text": "It is, therefore, an interesting question to ask whether a network can learn any aspects of that underlying abstract structure.", "labels": [], "entities": []}, {"text": "Recently, recurrent neural networks have reemerged as a powerful tool for learning the latent structure of language.", "labels": [], "entities": []}, {"text": "In particular, work on long short-term memory (LSTM) networks for language modeling has provided improvements in perplexity.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7312181740999222}]}, {"text": "We revisit Elman's question by applying LSTMs to the word-ordering task, without any explicit syntactic modeling.", "labels": [], "entities": []}, {"text": "We find that language models are in general effective for linearization relative to existing syntactic approaches, with LSTMs in particular outperforming the state-of-the-art by 11.5 BLEU points, with further gains observed when training with additional text and decoding with larger beams.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9983012080192566}]}], "datasetContent": [{"text": "Setup Experiments are on PTB with sections 2-21 as training, 22 as validation, and 23 as test . We utilize two UNK types, one for initial uppercase tokens and one for all other low-frequency tokens; end sentence tokens; and start/end tokens, which are treated as words, to mark BNPs for the WORDS+BNPS task.", "labels": [], "entities": [{"text": "PTB", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9625477194786072}, {"text": "BNPs", "start_pos": 278, "end_pos": 282, "type": "METRIC", "confidence": 0.8168670535087585}, {"text": "WORDS+BNPS task", "start_pos": 291, "end_pos": 306, "type": "TASK", "confidence": 0.41801533848047256}]}, {"text": "We also use a special symbol to replace tokens that contain at least one numeric character.", "labels": [], "entities": []}, {"text": "We otherwise train with punctuation and the original case of each token, resulting in a vocabulary containing around 16K types from around 1M training tokens.", "labels": [], "entities": []}, {"text": "For experiments marked GW we augment the PTB with a subset of the Annotated Gigaword corpus ().", "labels": [], "entities": [{"text": "PTB", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.7499090433120728}, {"text": "Annotated Gigaword corpus", "start_pos": 66, "end_pos": 91, "type": "DATASET", "confidence": 0.8736649552981058}]}, {"text": "We follow  and train on a sample of 900k Agence France-Presse sentences combined with the full PTB training set.", "labels": [], "entities": [{"text": "Agence France-Presse sentences", "start_pos": 41, "end_pos": 71, "type": "DATASET", "confidence": 0.8115411599477133}, {"text": "PTB training set", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.8618555466334025}]}, {"text": "The GW models benefit from both additional data and a larger vocabulary of around 25K types, which reduces unknowns in the validation and test sets.", "labels": [], "entities": []}, {"text": "We compare the models of   (known as ZGEN), a 5-gram LM using Kneser-Ney smoothing (NGRAM) 2 , and an LSTM.", "labels": [], "entities": []}, {"text": "We experiment on the WORDS and WORDS+BNPS tasks, and we also experiment with including future costs (g), the Gigaword data (GW), and varying beam size.", "labels": [], "entities": [{"text": "WORDS", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.809562623500824}, {"text": "BNPS", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.5205578207969666}, {"text": "Gigaword data (GW)", "start_pos": 109, "end_pos": 127, "type": "METRIC", "confidence": 0.9156226873397827}]}, {"text": "We retrain ZGEN using publicly available code 3 to replicate published results.", "labels": [], "entities": []}, {"text": "The LSTM model is similar in size and architecture to the medium LSTM setup of . Our implementation uses the Torch 5 framework and is publicly available . We compare the performance of the models using the BLEU metric ().", "labels": [], "entities": [{"text": "Torch 5 framework", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.8420642217000326}, {"text": "BLEU", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.9978697299957275}]}, {"text": "In generation if there are multiple tokens of identical UNK type, we randomly replace each with possible unused tokens in the original source before calculating BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9975603818893433}]}, {"text": "For comparison purposes, we use the BLEU script distributed with the publicly available ZGEN code.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.989648163318634}, {"text": "ZGEN code", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.9616227149963379}]}, {"text": "NGRAM-64 by more than 5 BLEU points.", "labels": [], "entities": [{"text": "NGRAM-64", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9518885016441345}, {"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9988412261009216}]}, {"text": "Differences on the WORDS task are smaller, but show a similar pattern.", "labels": [], "entities": [{"text": "WORDS task", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.45456309616565704}]}, {"text": "Incorporating Gigaword further increases the result another 2 points.", "labels": [], "entities": []}, {"text": "Notably, the NGRAM model outperforms the combined result of ZGEN-64+LM+GW+POS from , which uses a 4-gram model trained on Gigaword.", "labels": [], "entities": []}, {"text": "We believe this is because the combined ZGEN model incorporates the n-gram scores as discretized indicator features instead of using the probability directly.", "labels": [], "entities": []}, {"text": "7 A beam of 512 yields a further improvement at the cost of search time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score comparison on the PTB test  set. Results from previous works (for ZGEN) are  those provided by the respective authors, except for  the WORDS task. The final number in the model  identifier is the beam size, +GW indicates additional  Gigaword data. Models marked with +POS are pro- vided with a POS dictionary derived from the PTB  training set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987099170684814}, {"text": "PTB test  set", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9615812301635742}, {"text": "PTB  training set", "start_pos": 347, "end_pos": 364, "type": "DATASET", "confidence": 0.9534627000490824}]}, {"text": " Table 2: BLEU results on the validation set for  the LSTM and NGRAM model with varying beam  sizes, future costs, additional data, and use of base  noun phrases.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9978162050247192}]}, {"text": " Table 3: Unlabeled attachment scores (UAS) on  the PTB validation set after parsing and aligning  the output. For ZGEN we also include a result us- ing the tree z  *  produced directly by the system.  For WORDS+BNPS, internal BNP arcs are always  counted as correct.", "labels": [], "entities": [{"text": "Unlabeled attachment scores (UAS)", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.8456766605377197}, {"text": "PTB validation set", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.8915292223294576}, {"text": "BNP arcs", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9319266378879547}]}]}