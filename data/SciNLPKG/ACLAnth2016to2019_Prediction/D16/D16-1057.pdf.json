{"title": [{"text": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora", "labels": [], "entities": [{"text": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.8360739776066372}]}], "abstractContent": [{"text": "A word's sentiment depends on the domain in which it is used.", "labels": [], "entities": []}, {"text": "Computational social science research thus requires sentiment lexicons that are specific to the domains being studied.", "labels": [], "entities": []}, {"text": "We combine domain-specific word embeddings with a label propagation framework to induce accurate domain-specific sentiment lexicons using small sets of seed words, achieving state-of-the-art performance competitive with approaches that rely on hand-curated resources.", "labels": [], "entities": []}, {"text": "Using our framework we perform two large-scale empirical studies to quantify the extent to which sentiment varies across time and between communities.", "labels": [], "entities": []}, {"text": "We induce and release historical sentiment lexicons for 150 years of English and community-specific sentiment lexicons for 250 online communities from the social media forum Reddit.", "labels": [], "entities": []}, {"text": "The historical lexicons show that more than 5% of sentiment-bearing (non-neutral) English words completely switched polarity during the last 150 years, and the community-specific lexicons highlight how sentiment varies drastically between different communities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Inducing domain-specific sentiment lexicons is crucial to computational social science (CSS) research.", "labels": [], "entities": []}, {"text": "Sentiment lexicons allow us to analyze key subjective properties of texts like opinions and attitudes).", "labels": [], "entities": []}, {"text": "But lexical sentiment is hugely influenced by context.", "labels": [], "entities": []}, {"text": "The word soft has a very different sentiment in an online sports community than it does in one dedicated to toy animals ( tation; now it is essentially synonymous with good ().", "labels": [], "entities": []}, {"text": "Without domain-specific lexicons, social scientific analyses can be misled by sentiment assignments biased towards domain-general contexts, neglecting factors like genre, community-specific vernacular, or demographic variation (.", "labels": [], "entities": []}, {"text": "Using experts or crowdsourcing to construct domain-specific sentiment lexicons is expensive and often time-consuming (, and is especially problematic when non-standard language (as in historical documents or obscure social media forums) prevents annotators from understanding the sociolinguistic context of the data.", "labels": [], "entities": []}, {"text": "Web-scale sentiment lexicons can be automatically induced for large socially-diffuse domains, such as the internet-at-large ( or all of Twitter ().", "labels": [], "entities": [{"text": "Web-scale sentiment lexicons", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6332922577857971}]}, {"text": "However, to study sentiment in domain-specific cases-financial documents, historical texts, or tight-knit social me-dia forums-such generic lexicons maybe inaccurate, and even introduce harmful biases (.", "labels": [], "entities": []}, {"text": "Researchers need a principled and accurate framework for inducing lexicons that are specific to their domain of study.", "labels": [], "entities": []}, {"text": "To meet these needs, we introduce SENTPROP, a framework to learn accurate sentiment lexicons from small sets of seed words and domain-specific corpora.", "labels": [], "entities": [{"text": "SENTPROP", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.7851030826568604}]}, {"text": "SENTPROP combines the well-known method of label propagation with advances in word embeddings, and unlike previous approaches, is designed to be accurate even when using modestlysized domain-specific corpora (\u223c10 7 tokens).", "labels": [], "entities": [{"text": "SENTPROP", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.5137138366699219}, {"text": "label propagation", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7589481472969055}]}, {"text": "Our framework also provides confidence scores along with the learned lexicons, which allows researchers to quantify uncertainty in a principled manner.", "labels": [], "entities": []}, {"text": "The key contributions of this work are: 1.", "labels": [], "entities": []}, {"text": "A simple state-of-the-art sentiment induction algorithm, combining high-quality word vector embeddings with a label propagation approach.", "labels": [], "entities": [{"text": "sentiment induction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.913487434387207}, {"text": "label propagation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.6995247453451157}]}, {"text": "2. A novel bootstrap-sampling framework for inferring confidence scores with the sentiment values.", "labels": [], "entities": []}, {"text": "3. Two large-scale studies that reveal how sentiment depends on both social and historical context.", "labels": [], "entities": []}, {"text": "(a) We induce community-specific sentiment lexicons for the largest 250 \"subreddit\" communities on the social-media forum Reddit, revealing substantial variation in word sentiment between communities.", "labels": [], "entities": []}, {"text": "(b) We induce historical sentiment lexicons for 150 years of English, revealing that >5% of words switched polarity during this time.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work to systematically analyze the domain-dependency of sentiment at a large-scale, across hundreds of years and hundreds of user-defined online communities.", "labels": [], "entities": []}, {"text": "All of the inferred lexicons along with code for SENTPROP and all methods evaluated are made available in the SOCIALSENT package released with this paper.", "labels": [], "entities": [{"text": "SENTPROP", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8557384610176086}]}, {"text": "The SOCIALSENT package provides a benchmark toolkit for inducing sentiment lexicons, including implementations of previously published algorithms (), which are not otherwise publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the approaches according to (i) their binary classification accuracy (ignoring the neutral class, as is common in previous work), (ii) ternary classification performance (positive vs. neutral vs. negative) , and (iii) Kendall \u03c4 rank-correlation with continuous human-annotated polarity scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.8769702315330505}]}, {"text": "For all methods in the ternary-classification condition, we use the class-mass normalization method ( to label words as positive, neutral, or negative.", "labels": [], "entities": []}, {"text": "This method assumes knowledge of the label distribution-i.e., how many positive/negative vs. neutral words there are-and simply assigns labels to best match this distribution.", "labels": [], "entities": []}, {"text": "Tables 2a-2d summarize the performance of our framework along with baselines and other state-of-We found that the baseline method of, which our method is closely related to, performed relatively poorly with these domainspecific corpora.", "labels": [], "entities": []}, {"text": "This indicates that using highquality word-vector embeddings can have a drastic impact on performance.", "labels": [], "entities": []}, {"text": "However, it is worth noting that's method was designed for high recall with massive corpora, so its poor performance in our regime is not surprising.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9987550973892212}]}, {"text": "Lastly, we found that the choice of embedding method could have a drastic impact.", "labels": [], "entities": []}, {"text": "Preliminary experiments on the COHA data showed that using word2vec SGNS vectors (with default settings) instead of our SVD-based embeddings led to a >40% performance drop for SENTPROP across all measures and a >10% performance drop for DENSIFIER.", "labels": [], "entities": [{"text": "COHA data", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.9513856470584869}, {"text": "DENSIFIER", "start_pos": 237, "end_pos": 246, "type": "DATASET", "confidence": 0.6115263104438782}]}, {"text": "It is possible that certain settings of word2vec could perform better, but previous work has shown that SVD-based methods have superior results on smaller datasets and rare-word similarity tasks (, so this result is not surprising.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on recreating known lexicons.", "labels": [], "entities": []}]}