{"title": [], "abstractContent": [{"text": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks.", "labels": [], "entities": [{"text": "Sequence-to-Sequence (seq2seq) modeling", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6493288695812225}]}, {"text": "Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9988018274307251}]}, {"text": "In this work, we introduce a model and beam-search training scheme, based on the work of Daum\u00e9 III and Marcu (2005), that extends seq2seq to learn global sequence scores.", "labels": [], "entities": []}, {"text": "This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach.", "labels": [], "entities": []}, {"text": "We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.7631746530532837}, {"text": "parsing", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.8288911581039429}, {"text": "machine translation", "start_pos": 181, "end_pos": 200, "type": "TASK", "confidence": 0.7566915452480316}]}], "introductionContent": [{"text": "Sequence-to-Sequence learning with deep neural networks (herein, seq2seq)) has rapidly become a very useful and surprisingly general-purpose tool for natural language processing.", "labels": [], "entities": [{"text": "Sequence-to-Sequence learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8963188230991364}, {"text": "natural language processing", "start_pos": 150, "end_pos": 177, "type": "TASK", "confidence": 0.6311048567295074}]}, {"text": "In addition to demonstrating impressive results for machine translation (, roughly the same model and training have also proven to be useful for sentence compression (), parsing ( , and dialogue systems, and they additionally underlie other text generation applications, such as image or video captioning (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8069278001785278}, {"text": "sentence compression", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7944110631942749}, {"text": "parsing", "start_pos": 170, "end_pos": 177, "type": "TASK", "confidence": 0.9679407477378845}, {"text": "text generation", "start_pos": 241, "end_pos": 256, "type": "TASK", "confidence": 0.7201554179191589}, {"text": "image or video captioning", "start_pos": 279, "end_pos": 304, "type": "TASK", "confidence": 0.6676792353391647}]}, {"text": "The dominant approach to training a seq2seq system is as a conditional language model, with training maximizing the likelihood of each successive target word conditioned on the input sequence and the gold history of target words.", "labels": [], "entities": []}, {"text": "Thus, training uses a strictly word-level loss, usually cross-entropy over the target vocabulary.", "labels": [], "entities": []}, {"text": "This approach has proven to be very effective and efficient for training neural language models, and seq2seq models similarly obtain impressive perplexities for word-generation tasks.", "labels": [], "entities": []}, {"text": "Notably, however, seq2seq models are not used as conditional language models at test-time; they must instead generate fully-formed word sequences.", "labels": [], "entities": []}, {"text": "In practice, generation is accomplished by searching over output sequences greedily or with beam search.", "labels": [], "entities": []}, {"text": "In this context, note that the combination of the training and generation scheme just described leads to at least two major issues: 1.", "labels": [], "entities": []}, {"text": "Exposure Bias: the model is never exposed to its own errors during training, and so the inferred histories at test-time do not resemble the gold training histories.", "labels": [], "entities": []}, {"text": "2. Loss-Evaluation Mismatch: training uses a word-level loss, while at test-time we target improving sequence-level evaluation metrics, such as BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9985702037811279}]}, {"text": "We might additionally add the concern of label bias () to the list, since wordprobabilities at each time-step are locally normalized, guaranteeing that successors of incorrect his-tories receive the same mass as do the successors of the true history.", "labels": [], "entities": []}, {"text": "In this work we develop a non-probabilistic variant of the seq2seq model that can assign a score to any possible target sequence, and we propose a training procedure, inspired by the learning as search optimization (LaSO) framework of Daum\u00e9 III and, that defines a loss function in terms of errors made during beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 310, "end_pos": 321, "type": "TASK", "confidence": 0.8260975480079651}]}, {"text": "Furthermore, we provide an efficient algorithm to backpropagate through the beam-search procedure during seq2seq training.", "labels": [], "entities": []}, {"text": "This approach offers a possible solution to each of the three aforementioned issues, while largely maintaining the model architecture and training efficiency of standard seq2seq learning.", "labels": [], "entities": []}, {"text": "Moreover, by scoring sequences rather than words, our approach also allows for enforcing hard-constraints on sequence generation at training time.", "labels": [], "entities": [{"text": "sequence generation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.7264300584793091}]}, {"text": "To test out the effectiveness of the proposed approach, we develop a general-purpose seq2seq system with beam search optimization.", "labels": [], "entities": [{"text": "beam search optimization", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.9157590270042419}]}, {"text": "We run experiments on three very different problems: word ordering, syntactic parsing, and machine translation, and compare to a highlytuned seq2seq system with attention ().", "labels": [], "entities": [{"text": "word ordering", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.8016459941864014}, {"text": "syntactic parsing", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7311175763607025}, {"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.8066650629043579}]}, {"text": "The version with beam search optimization shows significant improvements on all three tasks, and particular improvements on tasks that require difficult search.", "labels": [], "entities": [{"text": "beam search optimization", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8855939110120138}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word ordering. BLEU Scores of seq2seq, BSO,  constrained BSO, and a vanilla LSTM language model  (from Schmaltz et al, 2016). All experiments above have  K tr = 6.", "labels": [], "entities": [{"text": "Word ordering", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8047714829444885}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9987898468971252}, {"text": "BSO", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.853925347328186}, {"text": "BSO", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.6543355584144592}]}, {"text": " Table 2: Beam-size experiments on word ordering devel- opment set. All numbers reflect training with constraints  (ConBSO).", "labels": [], "entities": [{"text": "word ordering devel- opment", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6324027895927429}]}, {"text": " Table 4: Machine translation experiments on test set; re- sults below middle line are from MIXER model of Ran- zato et al. (2016). SB-\u2206 indicates sentence BLEU costs  are used in defining \u2206. XENT is similar to our seq2seq  model but with a convolutional encoder and simpler at- tention. DAD trains seq2seq with scheduled sampling  (Bengio et al., 2015). BSO, SB-\u2206 experiments above  have K tr = 6.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8060801029205322}, {"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.984592616558075}, {"text": "BSO", "start_pos": 355, "end_pos": 358, "type": "DATASET", "confidence": 0.9594075679779053}]}, {"text": " Table 5: BLEU scores obtained on the machine trans- lation development data when training with \u2206(\u02c6 y", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992187023162842}, {"text": "trans- lation development", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7259505167603493}]}]}