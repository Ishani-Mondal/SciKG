{"title": [{"text": "Recurrent Residual Learning for Sequence Classification", "labels": [], "entities": [{"text": "Recurrent Residual Learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9199811816215515}, {"text": "Sequence Classification", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.9234957993030548}]}], "abstractContent": [{"text": "In this paper, we explore the possibility of leveraging Residual Networks (ResNet), a powerful structure in constructing extremely deep neural network for image understanding, to improve recurrent neural networks (RNN) for modeling sequential data.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7501105070114136}]}, {"text": "We show that for sequence classification tasks, incorporating residual connections into recurrent structures yields similar accuracy to Long Short Term Memory (LSTM) RNN with much fewer model parameters.", "labels": [], "entities": [{"text": "sequence classification tasks", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.8469793796539307}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9991617202758789}]}, {"text": "In addition, we propose two novel models which combine the best of both residual learning and LSTM.", "labels": [], "entities": []}, {"text": "Experiments show that the new models significantly outperform LSTM.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.6933503150939941}]}], "introductionContent": [{"text": "Recurrent Neural Networks (RNNs) are powerful tools to model sequential data.", "labels": [], "entities": []}, {"text": "Among various RNN models, Long Short Term Memory (LSTM)) is one of the most effective structures.", "labels": [], "entities": []}, {"text": "In LSTM, gating mechanism is used to control the information flow such that gradient vanishing problem in vanilla RNN is better handled, and long range dependency is better captured.", "labels": [], "entities": []}, {"text": "However, as empirically verified by previous works and our own experiments, to obtain fairly good results, training LSTM RNN needs carefully designed optimization procedure;; Arjovsky et * This work was done when the author was visiting al., 2015), especially when faced with unfolded very deep architectures for fairly long sequences.", "labels": [], "entities": []}, {"text": "From another perspective, for constructing very deep neural networks, recently Residual Networks (ResNet) () have shown their effectiveness in quite a few computer vision tasks.", "labels": [], "entities": []}, {"text": "By learning a residual mapping between layers with identity skip connections (, ResNet ensures a fluent information flow, leading to efficient optimization for very deep structures (e.g., with hundreds of layers).", "labels": [], "entities": []}, {"text": "In this paper, we explore the possibilities of leveraging residual learning to improve the performances of recurrent structures, in particular, LSTM RNN, in modeling fairly long sequences (i.e., whose lengths exceed 100).", "labels": [], "entities": []}, {"text": "To summarize, our main contributions include: 1.", "labels": [], "entities": []}, {"text": "We introduce residual connecting mechanism into the recurrent structure and propose recurrent residual networks for sequence learning.", "labels": [], "entities": [{"text": "sequence learning", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7679377496242523}]}, {"text": "Our model achieves similar performances to LSTM in text classification tasks, whereas the number of model parameters is greatly reduced.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.8581693967183431}]}, {"text": "2. We present in-depth analysis of the strengths and limitations of LSTM and ResNet in respect of sequence learning.", "labels": [], "entities": [{"text": "sequence learning", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7393469512462616}]}, {"text": "3. Based on such analysis, we further propose two novel models that incorporate the strengths of the mechanisms behind LSTM and ResNet.", "labels": [], "entities": []}, {"text": "We demonstrate that our models outperform LSTM in many sequence classification tasks.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7505491971969604}]}], "datasetContent": [{"text": "We conduct comprehensive empirical analysis on sequence classification tasks.", "labels": [], "entities": [{"text": "sequence classification tasks", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.8796234726905823}]}, {"text": "Listed in the ascending order of average sequence lengths, several public datasets we use include: 1.", "labels": [], "entities": []}, {"text": "AG's news corpus 1 ,a news article corpus with categorized articles from more than 2, 000 news sources.", "labels": [], "entities": [{"text": "AG's news corpus 1", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9705175876617431}]}, {"text": "We use the dataset with 4 largest classes constructed in ( ).", "labels": [], "entities": []}, {"text": "2. IMDB movie review dataset 2 , a binary sentiment classification dataset consisting of movie review comments with positive/negative sentiment labels).", "labels": [], "entities": [{"text": "IMDB movie review dataset", "start_pos": 3, "end_pos": 28, "type": "DATASET", "confidence": 0.8053827434778214}]}, {"text": "3. 20 Newsgroups (20NG for short), an email collection dataset categorized into 20 news groups.", "labels": [], "entities": []}, {"text": "Simiar to, we use the post-processed version 3 , in which attachments, PGP keys and some duplicates are removed.", "labels": [], "entities": []}, {"text": "4. Permuted-MNIST (P-MNIST for short).", "labels": [], "entities": [{"text": "Permuted-MNIST", "start_pos": 3, "end_pos": 17, "type": "METRIC", "confidence": 0.9494596719741821}]}, {"text": "Following (, we shuffle pixels of each MNIST image (LeCun et al., 1998) with a fixed random permutation, and feed all pixels sequentially into recurrent network to predict the image label.", "labels": [], "entities": []}, {"text": "Permuted-MNIST is assumed to be a good testbed for measuring the ability of modeling very long range dependencies ().", "labels": [], "entities": [{"text": "Permuted-MNIST", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9143605828285217}]}, {"text": "Detailed statistics of each dataset are listed in.", "labels": [], "entities": []}, {"text": "For all the text datasets, we take every word as input and feed word embedding vectors pre-trained by) on Wikipedia into the recurrent neural network.", "labels": [], "entities": []}, {"text": "The topmost frequent words with 95% total frequency coverage are kept, while others are replaced by the token \"UNK\".", "labels": [], "entities": [{"text": "UNK", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.6292153000831604}]}, {"text": "We use the standard training/test split along with all these datasets and randomly pick 15% of training set as dev set, based on which we perform early stopping and for all models tune hyper-parameters such as dropout ratio (on non-recurrent layers) (), gradient clipping value () and the skip connection length L for SC-LSTM (cf. equation).", "labels": [], "entities": [{"text": "skip connection length L", "start_pos": 289, "end_pos": 313, "type": "METRIC", "confidence": 0.840405210852623}]}, {"text": "The last hidden states of recurrent networks are put into logistic regression classifiers for label predictions.", "labels": [], "entities": []}, {"text": "We use Adadelta to perform parameter optimization.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.750074028968811}]}, {"text": "All our implementations are based on) and run on one K40 GPU.", "labels": [], "entities": []}, {"text": "All the source codes and datasets can be downloaded at https://publish.illinois.", "labels": [], "entities": []}, {"text": "edu/yirenwang/emnlp16source/.", "labels": [], "entities": [{"text": "edu/yirenwang/emnlp16source", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.8092044949531555}]}, {"text": "We compare our proposed models mainly with the state-of-art standard LSTM RNN.", "labels": [], "entities": [{"text": "LSTM RNN", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.8523967862129211}]}, {"text": "In addition, to fully demonstrate the effects of residual learning in our HRL model, we employ another hybrid model as baseline, which combines LSTM and GRU (), another state-of-art RNN variant, in a similar way as HRL.", "labels": [], "entities": [{"text": "GRU", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.8708926439285278}]}, {"text": "We use LSTM+GRU to denote such a baseline.", "labels": [], "entities": [{"text": "GRU", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.8447278738021851}]}, {"text": "The model sizes (word embedding size \u00d7 hidden state size) configurations used for each dataset are listed in, \"Non-Hybrid\" refers to LSTM, RRN and SC-LSTM models, while \"Hybrid\" refers to two methods that combines two basic models: HRL and LSTM+GRU.", "labels": [], "entities": []}, {"text": "The model sizes of all hybrid models are smaller than the standard LSTM.", "labels": [], "entities": []}, {"text": "All models have only one recurrent layer.", "labels": [], "entities": []}, {"text": "All the classification accuracy numbers are listed in    half of the model parameters, indicating that residual network structure, with connecting mechanism to enhance the information flow, is also an effective approach for sequence learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8165919184684753}, {"text": "sequence learning", "start_pos": 224, "end_pos": 241, "type": "TASK", "confidence": 0.8498890995979309}]}, {"text": "However, the fact that it fails to significantly outperform other models (as it does in image classification) implies that forgetting mechanism is desired in recurrent structures to handle multiple inputs.", "labels": [], "entities": [{"text": "image classification", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.7546066045761108}]}, {"text": "2. Skip-Connected LSTM performs much better than standard LSTM.", "labels": [], "entities": [{"text": "Skip-Connected LSTM", "start_pos": 3, "end_pos": 22, "type": "DATASET", "confidence": 0.7669830322265625}]}, {"text": "For tasks with shorter sequences such as AG's News, the improvement is limited.", "labels": [], "entities": [{"text": "AG's News", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.8743776082992554}]}, {"text": "However, the improvements get more significant with the growth of sequence lengths among different datasets 4 , and the performance is particularly good in P-MNIST with very long sequences.", "labels": [], "entities": []}, {"text": "This reveals the importance of skip connections in carrying on historical information through along range of time steps, and demonstrates the effectiveness of our approach that adopts the residual connecting mechanism to improve LSTM's capability of handling long-term dependency.", "labels": [], "entities": []}, {"text": "Furthermore, SC-LSTM is robust with different hyperparam-4 t-test on SC-LSTM-P and SC-LSTM-I with p value < 0.001.", "labels": [], "entities": []}, {"text": "eter values: we test L = 10, 20, 50, 75 in P-MNIST and find the performance is not sensitive w.r.t. these L values.", "labels": [], "entities": []}, {"text": "3. HRL also outperforms standard LSTM with fewer model parameters . In comparison, the hybrid model of LSTM+GRU cannot achieve such accuracy as HRL.", "labels": [], "entities": [{"text": "HRL", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.5896934270858765}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9984534978866577}]}, {"text": "As we expected, the additional long range historical information propagated by RRN is proved to be good assistance to standard LSTM.", "labels": [], "entities": [{"text": "RRN", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8992660045623779}]}], "tableCaptions": [{"text": " Table 2: Model Sizes on Different Dataset.", "labels": [], "entities": []}, {"text": " Table 3: Classification Results (Test Accuracy).", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7144165635108948}, {"text": "Accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9411225318908691}]}]}