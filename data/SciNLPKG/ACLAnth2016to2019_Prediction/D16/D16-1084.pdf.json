{"title": [{"text": "Stance Detection with Bidirectional Conditional Encoding", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.980309247970581}]}], "abstractContent": [{"text": "Stance detection is the task of classifying the attitude Previous work has assumed that either the target is mentioned in the text or that training data for every target is given.", "labels": [], "entities": [{"text": "Stance detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9410803914070129}]}, {"text": "This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets.", "labels": [], "entities": []}, {"text": "We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently.", "labels": [], "entities": [{"text": "LSTM encoding", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.735206812620163}]}, {"text": "Performance is improved further when the conditional model is augmented with bidi-rectional encoding.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target.", "labels": [], "entities": [{"text": "SemEval 2016 Task 6 Twitter Stance Detection", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.6892363769667489}]}, {"text": "When such weak supervision is added, our approach achieves state-of-the-art results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of stance detection is to classify the attitude expressed in a text towards a given target, as \"positive\", \"negative\", or \"neutral\".", "labels": [], "entities": [{"text": "stance detection", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.9547007977962494}]}, {"text": "Such information can be useful fora variety of tasks, e.g. showed that tweets stating actual facts were affirmed by 90% of the tweets related to them, while tweets conveying false information were predominantly questioned or denied.", "labels": [], "entities": []}, {"text": "In this paper we focus on a novel stance detection task, namely tweet stance detection towards previously unseen targets (mostly entities such as politicians or issues of public interest), as defined in the SemEval Stance Detection for Twitter task.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8101443648338318}, {"text": "tweet stance detection", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8147983948389689}, {"text": "SemEval Stance Detection for Twitter task", "start_pos": 207, "end_pos": 248, "type": "TASK", "confidence": 0.9135108689467112}]}, {"text": "This task is rather difficult, firstly due to not having training data for the targets in the test set, and secondly, due to the targets not always being mentioned in the tweet.", "labels": [], "entities": []}, {"text": "For example, the tweet \"@realDon-aldTrump is the only honest voice of the @GOP\" expresses a positive stance towards the target Donald Trump.", "labels": [], "entities": []}, {"text": "However, when stance is annotated with respect to Hillary Clinton as the implicit target, this tweet expresses a negative stance, since supporting candidates from one party implies negative stance towards candidates from other parties.", "labels": [], "entities": []}, {"text": "Thus the challenge is twofold.", "labels": [], "entities": []}, {"text": "First, we need to learn a model that interprets the tweet stance towards a target that might not be mentioned in the tweet itself.", "labels": [], "entities": []}, {"text": "Second, we need to learn such a model without labelled training data for the target with respect to which we are predicting the stance.", "labels": [], "entities": []}, {"text": "In the example above, we need to learn a model for Hillary Clinton by only using training data for other targets.", "labels": [], "entities": []}, {"text": "While this renders the task more challenging, it is a more realistic scenario, as it is unlikely that labelled training data for each target of interest will be available.", "labels": [], "entities": []}, {"text": "To address these challenges we develop a neural network architecture based on conditional encoding (.", "labels": [], "entities": []}, {"text": "A long-short term memory (LSTM) network) is used to encode the target, followed by a second LSTM that encodes the tweet using the encoding of the target as its initial state.", "labels": [], "entities": []}, {"text": "We show that this approach achieves better F1 than an SVM baseline, or an independent LSTM encoding of the tweet and the target.", "labels": [], "entities": [{"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9996017813682556}]}, {"text": "Results improve fur-ther (0.4901 F1) with a bidirectional version of our model, which takes into account the context on either side of the word being encoded.", "labels": [], "entities": [{"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9902315735816956}]}, {"text": "In the context of the shared task, this would have been the second best result, except for an approach which uses automatically labelled tweets for the test targets (F1 of 0.5628).", "labels": [], "entities": [{"text": "F1", "start_pos": 166, "end_pos": 168, "type": "METRIC", "confidence": 0.9969606995582581}]}, {"text": "Lastly, when our bidirectional conditional encoding model is trained on such data, it achieves state-of-the-art performance (0.5803 F1).", "labels": [], "entities": [{"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9972357153892517}]}], "datasetContent": [{"text": "Experiments are performed on the SemEval 2016 Task 6 corpus for Stance Detection on Twitter ().", "labels": [], "entities": [{"text": "SemEval 2016 Task 6 corpus", "start_pos": 33, "end_pos": 59, "type": "DATASET", "confidence": 0.6413623452186584}, {"text": "Stance Detection on Twitter", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.8763211220502853}]}, {"text": "We report experiments for two different experimental setups: one is the unseen target setup (Section 5), which is the main focus of this paper, i.e. detecting the stance of tweets towards previously unseen targets.", "labels": [], "entities": [{"text": "detecting the stance of tweets towards previously unseen targets", "start_pos": 149, "end_pos": 213, "type": "TASK", "confidence": 0.7744405733214484}]}, {"text": "We show that conditional encoding, by reading the tweets in a target-specific way, generalises to unseen targets better than baselines which ignore the target.", "labels": [], "entities": [{"text": "conditional encoding", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7684769332408905}]}, {"text": "Next, we compare our approach to previous work in a weakly supervised framework (Section 6) and show that our approach outperforms the state-of-the-art on the SemEval 2016 Stance Detection Subtask B corpus.", "labels": [], "entities": [{"text": "SemEval 2016 Stance Detection Subtask B corpus", "start_pos": 159, "end_pos": 205, "type": "DATASET", "confidence": 0.6732322020190102}]}, {"text": "TaskB Unlab is an unlabelled corpus containing Donald Trump tweets supplied by the task organisers, and TaskB Auto-lab* is an automatically labelled version of a small portion of the corpus for the weakly supervised stance detection experiments reported in Section 6.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 216, "end_pos": 232, "type": "TASK", "confidence": 0.8087665736675262}, {"text": "Section 6", "start_pos": 257, "end_pos": 266, "type": "DATASET", "confidence": 0.8809410631656647}]}, {"text": "Finally, Crawled Unlab* is a corpus we collected for unsupervised pre-training (see Section 3.4).", "labels": [], "entities": []}, {"text": "For all experiments, the official task evaluation script is used.", "labels": [], "entities": []}, {"text": "Predictions are post processed so that if the target is contained in a tweet, the highest-scoring non-neutral stance is chosen.", "labels": [], "entities": []}, {"text": "This was motivated by the observation that in the training data most target-containing tweets express a stance, with only 16% of them being neutral.", "labels": [], "entities": []}, {"text": "The code used in our experiments is available from https://github.com/ sheffieldnlp/stance-conditional.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for the unseen target stance detection devel-", "labels": [], "entities": [{"text": "unseen target stance detection devel-", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.7435918003320694}]}, {"text": " Table 3: Results for the unseen target stance detection test", "labels": [], "entities": [{"text": "unseen target stance detection", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6210097894072533}]}, {"text": " Table 4: Results for the unseen target stance detection develop-", "labels": [], "entities": [{"text": "unseen target stance detection", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6481539309024811}]}, {"text": " Table 6: Stance Detection test results for weakly super-", "labels": [], "entities": [{"text": "Stance Detection", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9014767408370972}]}]}