{"title": [{"text": "It Takes Three to Tango: Triangulation Approach to Answer Ranking in Community Question Answering", "labels": [], "entities": [{"text": "Answer Ranking in Community Question Answering", "start_pos": 51, "end_pos": 97, "type": "TASK", "confidence": 0.6906941533088684}]}], "abstractContent": [{"text": "We address the problem of answering new questions in community forums, by selecting suitable answers to already asked questions.", "labels": [], "entities": [{"text": "answering new questions in community forums", "start_pos": 26, "end_pos": 69, "type": "TASK", "confidence": 0.8784913818041483}]}, {"text": "We approach the task as an answer ranking problem, adopting a pairwise neural network architecture that selects which of two competing answers is better.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7926606237888336}]}, {"text": "We focus on the utility of the three types of similarities occurring in the triangle formed by the original question , the related question, and an answer to the related comment, which we call relevance, relatedness, and appropriateness.", "labels": [], "entities": []}, {"text": "Our proposed neural network models the interactions among all input components using syntactic and semantic embeddings, lexical matching , and domain-specific features.", "labels": [], "entities": []}, {"text": "It achieves state-of-the-art results, showing that the three similarities are important and need to be mod-eled together.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that all feature types are relevant, but the most important ones are the lexical similarity features , the domain-specific features, and the syntactic and semantic embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information.", "labels": [], "entities": [{"text": "Question Answering (cQA) forums", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8255673348903656}, {"text": "Qatar Living", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9264945387840271}]}, {"text": "These forums typically organize their content in the form of multiple topic-oriented question-comment threads, where a question posed by a user is followed by a list of other users' comments, which intend to answer the question.", "labels": [], "entities": []}, {"text": "Many of such on-line forums are not moderated, which often results in (a) noisy and (b) redundant content, as users tend to deviate from the question and start asking new questions or engage in conversations, fights, etc.", "labels": [], "entities": []}, {"text": "Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future.", "labels": [], "entities": [{"text": "solve problem (a)", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.652950394153595}]}, {"text": "Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones.", "labels": [], "entities": []}, {"text": "This is due to the time it takes fora comment to accumulate votes.", "labels": [], "entities": []}, {"text": "Moreover, voting is prone to abuse by forum trolls).", "labels": [], "entities": []}, {"text": "Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different way.", "labels": [], "entities": []}, {"text": "This search can be hard, especially for less experienced users as most sites only offer basic search, e.g., a site search by Google.", "labels": [], "entities": []}, {"text": "Yet, solving problem (b) automatically is important both for site owners, as they want to prevent question duplication as much as possible, and for users, as finding an answer to their questions without posting means immediate satisfaction of their information needs.", "labels": [], "entities": [{"text": "question duplication", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7437301278114319}]}, {"text": "In this paper, we address the general problem of finding good answers to a given new question (referred to as original question) in one such community-created forum.", "labels": [], "entities": []}, {"text": "More specifically, we use a pairwise deep neural network to rank comments retrieved from different question-comment threads according to their relevance as answers to the original question being asked.", "labels": [], "entities": []}, {"text": "A key feature of our approach is that we investigate the contribution of the edges in the triangle formed by the pairwise interactions between the original question, the related question, and the related comments to rank comments in a unified fashion.", "labels": [], "entities": []}, {"text": "Additionally, we use three different sets of features that capture such similarity: lexical, distributed (semantics/syntax), and domain-specific knowledge.", "labels": [], "entities": []}, {"text": "The experimental results show that addressing the answer ranking task directly, i.e., modelling only the similarity between the original question and the answer-candidate comments, yields very low results.", "labels": [], "entities": [{"text": "answer ranking task", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8344032168388367}]}, {"text": "The other two edges of the triangle are needed to obtain good results, i.e., the similarity between the original question and the related question and the similarity between the related question and the related comments.", "labels": [], "entities": []}, {"text": "Both aspects add significant and cumulative improvements to the overall performance.", "labels": [], "entities": []}, {"text": "Finally, we show that the full network, including the three pairs of similarities, outperforms the state-of-the-art on a benchmark dataset.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 discusses the similarity triangle in answer ranking for cQA, Section 3 presents our pairwise neural network model for answering new questions in community forums, which integrates multiple levels of interaction, Section 4 describes the features we used, Section 5 presents our evaluation setup, the experiments and the results, Section 6 discusses some related work, and Section 7 wraps up the paper with a brief summary of the contributions and some possible directions for future work.", "labels": [], "entities": []}, {"text": "2 The Similarity Triangle in cQA presents an example illustrating the similarity triangle that we use when solving the answer ranking problem in cQA.", "labels": [], "entities": [{"text": "answer ranking problem", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.8403393824895223}]}, {"text": "In the figure, q stands for the new question, q is an existing related question, and c is a comment within the thread of question q . The edge qc relates to the main cQA task addressed in this paper, i.e., deciding whether a comment fora potentially related question is a good answer to the original question.", "labels": [], "entities": []}, {"text": "We will say that the relation captures the relevance of c for q.", "labels": [], "entities": []}, {"text": "The edge qq represents the similarity between the original and the related questions.", "labels": [], "entities": []}, {"text": "We will call this relation relatedness.", "labels": [], "entities": []}, {"text": "Finally, the edge q c represents the decision of whether c is a good answer for the question from its thread, q . We will call this relation appropriateness.", "labels": [], "entities": []}, {"text": "In this particular example, q and q are indeed related, and c is a good answer for both q and q.", "labels": [], "entities": []}, {"text": "In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q , and a comment c within the thread of q , to solve different cQA sub-tasks.", "labels": [], "entities": []}, {"text": "For example, answer selection, which selects the most appropriate comment c within the thread q , was addressed in.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.9078349471092224}]}, {"text": "Similarly, question-question similarity, which looks for the most related questions to a given question, was addressed by many authors.", "labels": [], "entities": []}, {"text": "In this paper, we solve the cQA task problem 2 in a novel way by using the three types of similarities jointly.", "labels": [], "entities": []}, {"text": "Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting.", "labels": [], "entities": [{"text": "Question Answering setting", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.778397818406423}]}, {"text": "Below we present experimental results that support this hypothesis.", "labels": [], "entities": []}, {"text": "The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA . In that evaluation exercise, q c and qq are presented as subtask A and subtask B, respectively.", "labels": [], "entities": [{"text": "SemEval 2016 Task", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6552897890408834}]}, {"text": "In this paper, we mainly use them as similarity relations to be modeled in the learning architecture to solve the answer ranking task.", "labels": [], "entities": [{"text": "answer ranking task", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8369123140970866}]}, {"text": "We use the task setup and the datasets from SemEval-2016 Task 3, focusing on subtask C ( ).", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with the data from SemEval-2016 Task 3 on \"Community Question Answering\".", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7881182630856832}, {"text": "Community Question Answering", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6188176274299622}]}, {"text": "More precisely, the problem addressed is subtask C (Question-External Comment Similarity), which is the primary cQA task.", "labels": [], "entities": [{"text": "Question-External Comment Similarity)", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.671691007912159}]}, {"text": "For a given new question (referred to as the original question), the task provides the set of the first ten related questions (retrieved by a search engine), each associated with the first ten comments appearing in the question-comment thread.", "labels": [], "entities": []}, {"text": "The goal then is to rank the total of 100 comments according to their appropriateness with respect to the original question.", "labels": [], "entities": []}, {"text": "In this framework, the retrieval part of the task is done as a pre-processing step, and the challenge is to learn to rank all good comments above all bad ones.", "labels": [], "entities": []}, {"text": "All the data comes from the QatarLiving forum, and the related questions are obtained using Google search with the original question's text limited to the www.qatarliving.com domain.", "labels": [], "entities": [{"text": "QatarLiving forum", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.9234179854393005}]}, {"text": "The task offers a higher quality training dataset TRAIN-PART1, which includes 200 original questions, 1,999 related questions and 19,990 comments, and a lower-quality TRAIN-PART2, which we did not use.", "labels": [], "entities": [{"text": "TRAIN-PART1", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9513761401176453}, {"text": "TRAIN-PART2", "start_pos": 167, "end_pos": 178, "type": "METRIC", "confidence": 0.8852406740188599}]}, {"text": "Additionally, it provides a development set (DEV, with 50 original questions, 500 related questions and 5,000 related comments) and a TEST set (70 original questions, 700 related questions and 7,000 related comments).", "labels": [], "entities": [{"text": "TEST", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9812029004096985}]}, {"text": "Apart from the class labels for subtask C, the datasets also offer class labels for subtask A (i.e., whether a comment is a good answer to the question in the thread) and subtask B (i.e., whether the related questions is relevant for the original question).", "labels": [], "entities": []}, {"text": "The results are calculated with the official scorer from the SemEval-2016 Task 3.", "labels": [], "entities": [{"text": "SemEval-2016 Task 3", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.6301137606302897}]}, {"text": "We report three ranking-based measures that are commonly accepted in the IR community: Mean average precision (MAP), which is the official evaluation measure of the task, average recall (AvgRec), and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "IR community", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.9112634658813477}, {"text": "Mean average precision (MAP)", "start_pos": 87, "end_pos": 115, "type": "METRIC", "confidence": 0.9700241188208262}, {"text": "recall (AvgRec)", "start_pos": 179, "end_pos": 194, "type": "METRIC", "confidence": 0.8960009068250656}, {"text": "mean reciprocal rank (MRR)", "start_pos": 200, "end_pos": 226, "type": "METRIC", "confidence": 0.9158518016338348}]}, {"text": "For comparison purposes, we report the results for two baselines.", "labels": [], "entities": []}, {"text": "One corresponds to a random ordering of the comments, assuming zero knowledge of the task.", "labels": [], "entities": []}, {"text": "The second one is a more realistic baseline, which keeps the question ranking from the search engine (Google search) and the chronological order of the comments within the thread of teh related question.", "labels": [], "entities": []}, {"text": "Although this maybe considered a very na\u00a8\u0131vena\u00a8\u0131ve baseline, it is actually notably informed.", "labels": [], "entities": []}, {"text": "The question ranking from Google search takes into account the relevance of the entire thread (question and comments) to the original question.", "labels": [], "entities": []}, {"text": "Moreover, there is a natural concentration of the best answers in the first comments of the threads.", "labels": [], "entities": []}, {"text": "shows the evaluation results on the TEST dataset for several variants of our pairwise neural network architecture.", "labels": [], "entities": [{"text": "TEST dataset", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.7260495871305466}]}, {"text": "Regarding our network configurations, we present the results from simpler to more complex.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the answer ranking task of our  full NN vs. variants using partial information.", "labels": [], "entities": [{"text": "answer ranking task", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.860163946946462}]}, {"text": " Table 3: Using appropriateness predictions.", "labels": [], "entities": []}, {"text": " Table 4: Comparative results with the state of the art,  i.e., the top-3 systems that participated in SemEval- 2016 Task 3, subtask C.", "labels": [], "entities": [{"text": "SemEval- 2016 Task 3", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.776922082901001}]}]}