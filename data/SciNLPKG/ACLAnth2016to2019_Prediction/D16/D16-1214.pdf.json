{"title": [{"text": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We compare the effectiveness of four different syntactic CCG parsers fora semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis.", "labels": [], "entities": [{"text": "downstream semantic analysis", "start_pos": 159, "end_pos": 187, "type": "TASK", "confidence": 0.6387049655119578}]}, {"text": "This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The past several years have seen significant progress in unsupervised grammar induction.", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6597087581952413}]}, {"text": "But how useful are unsupervised syntactic parsers for downstream NLP tasks?", "labels": [], "entities": []}, {"text": "What phenomena are they able to capture, and where would additional annotation be required?", "labels": [], "entities": []}, {"text": "Instead of standard intrinsic evaluations -attachment scores that depend strongly on the particular annotation styles of the gold treebank -we examine the utility of unsupervised and weakly supervised parsers for semantics.", "labels": [], "entities": []}, {"text": "We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the extent to which these systems recover semantic information.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.8152188460032145}]}, {"text": "We focus on English to perform a direct comparison with supervised parsers (although unsupervised or weakly supervised approaches are likely to be most beneficial for domains or languages where supervised parsers are not available).", "labels": [], "entities": []}, {"text": "* Equal contribution Specifically, we evaluate different parsing scenarios with varying amounts of supervision.", "labels": [], "entities": [{"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9621495604515076}]}, {"text": "These are designed to shed light on the question of how well syntactic knowledge correlates with performance on a semantic evaluation.", "labels": [], "entities": []}, {"text": "We evaluate the following scenarios (all of which assume POS-tagged input): 1) no supervision; 2) a lexicon containing words mapped to CCG categories; 3) a lexicon containing POS tags mapped to CCG categories; 4) sentences annotated with CCG derivations (i.e., fully supervised).", "labels": [], "entities": []}, {"text": "Our evaluation reveals which constructions are problematic for unsupervised parsers (and annotation efforts should focus on).", "labels": [], "entities": []}, {"text": "Our results indicate that unsupervised syntax is useful for semantics, while a simple semi-supervised parser outperforms a fully unsupervised approach, and could hence be a viable option for low resource languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "CCG) is a lexicalized formalism in which words are assigned syntactic types, also known as supertags, encoding subcategorization information.", "labels": [], "entities": []}, {"text": "Consider the sentence Google acquired Nest in 2014, and its CCG derivations shown in.", "labels": [], "entities": [{"text": "Google acquired Nest in 2014", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.8722700953483582}]}, {"text": "In (a) and (b), the supertag of acquired, (S\\NP)/NP, indicates that it has two arguments, and the prepositional phrase in 2014 is an adjunct, whereas in (c) the supertag ((S\\NP)/PP)/NP indicates acquired has three arguments including the prepositional phrase.", "labels": [], "entities": []}, {"text": "In (a) and (b), depending on the supertag of in, the derivation differs.", "labels": [], "entities": []}, {"text": "When trained on labeled treebanks, (a) is preferred.", "labels": [], "entities": []}, {"text": "However note that all these derivations could lead to the same semantics (e.g., to the logical form in Equation 1).", "labels": [], "entities": []}, {"text": "Without syntactic su- (c) acquired Google takes the argument in 2014 Figure 1: Example of multiple valid derivations that can be grounded to the same Freebase logical form (Eq.", "labels": [], "entities": []}, {"text": "1) even though they differ dramatically in performance under parsing metrics (5, 4, or 3 \"correct\" supertags).", "labels": [], "entities": []}, {"text": "pervision, there may not be any reason for the parser to prefer one analysis over the other.", "labels": [], "entities": []}, {"text": "One procedure to evaluate unsupervised induction methods has been to compare the assigned supertags to treebanked supertags, but this evaluation does not consider that multiple derivations could lead to the same semantics.", "labels": [], "entities": []}, {"text": "This problem is also not solved by evaluating syntactic dependencies.", "labels": [], "entities": []}, {"text": "Moreover, while many dependency standards agree on the head direction of simple constituents (e.g., noun phrases) they disagree on the most semantically useful ones (e.g., coordination and relative clauses).", "labels": [], "entities": []}, {"text": "1  The above syntax-based evaluation metrics conceal the real performance differences and their effect on downstream tasks.", "labels": [], "entities": []}, {"text": "Here we propose an extrinsic evaluation where we evaluate our ability to convert sentences to Freebase logical forms starting via CCG derivations.", "labels": [], "entities": []}, {"text": "Our motivation is that most sentences can only have a single realization in Freebase, and any derivation that could lead to this realization is potentially a correct derivation.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9524651169776917}]}, {"text": "For example, the Freebase logical form for the example sentence in is shown below, and none of its derivations are penalized if they could result in this logical form.", "labels": [], "entities": [{"text": "Freebase logical form", "start_pos": 17, "end_pos": 38, "type": "DATASET", "confidence": 0.9032272100448608}]}, {"text": "Since grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms.", "labels": [], "entities": []}, {"text": "But such datasets do not exist in the Freebase semantic parsing literature.", "labels": [], "entities": [{"text": "Freebase semantic parsing", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8434200485547384}]}, {"text": "To alleviate this prob-1 Please see for more details.", "labels": [], "entities": []}, {"text": "lem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task.", "labels": [], "entities": [{"text": "Freebase semantic parsing", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.8319365382194519}]}, {"text": "Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot.", "labels": [], "entities": []}, {"text": "The task is to fill this slot by translating the declarative sentence into a Freebase query.", "labels": [], "entities": []}, {"text": "Consider the following sentence where the entity Nest has been removed:   Our dataset SPADES (Semantic PArsing of DEclarative Sentences) is constructed from the declarative sentences collected by: SPADES Corpus Statistics one isomorphic Freebase graph to the ungrounded representation of the input sentence; 2) There are no variable nodes in the ungrounded graph (e.g., Google acquired a company is discarded whereas Google acquired the company Nest is selected).", "labels": [], "entities": []}, {"text": "We split this data into training (85%), development (5%) and testing (10%) sentences.", "labels": [], "entities": []}, {"text": "We introduce empty slots into these sentences by randomly removing an entity.", "labels": [], "entities": []}, {"text": "SPADES can be downloaded at http:// github.com/sivareddyg/graph-parser.", "labels": [], "entities": []}, {"text": "There has been other recent interest in similar datasets for sentence completion ( and machine reading (, but unlike other corpora our data is tied directly to Freebase and requires the execution of a semantic parse to correctly predict the missing entity.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7461763173341751}, {"text": "machine reading", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.7545024156570435}]}, {"text": "This is made more explicit by the fact that one third of the entities in our test set are never seen during training, so without a general approach to query creation and execution there is a limit on a system's performance.", "labels": [], "entities": [{"text": "query creation", "start_pos": 151, "end_pos": 165, "type": "TASK", "confidence": 0.7303250581026077}]}], "tableCaptions": [{"text": " Table 2: Syntactic and semantic evaluation of the parsing models. Left: Simplified labeled F1 and undirected  unlabeled F1 on CCGbank, Section 23. Right: Slot filling performance (by number of entities per sentence).", "labels": [], "entities": [{"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9038382768630981}, {"text": "CCGbank", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9733238220214844}, {"text": "Slot filling", "start_pos": 155, "end_pos": 167, "type": "TASK", "confidence": 0.9260170757770538}]}]}