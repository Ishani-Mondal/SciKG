{"title": [{"text": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention", "labels": [], "entities": [{"text": "Recognizing Implicit Discourse Relations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9041611552238464}]}], "abstractContent": [{"text": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing.", "labels": [], "entities": [{"text": "Recognizing implicit discourse relations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.87187759578228}, {"text": "Natural Language Processing", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.6448396941026052}]}, {"text": "For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations.", "labels": [], "entities": []}, {"text": "To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations.", "labels": [], "entities": []}, {"text": "Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results.", "labels": [], "entities": [{"text": "PDTB dataset", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9731082320213318}]}, {"text": "The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse relations (e.g., contrast and causality) support a set of sentences to form a coherent text.", "labels": [], "entities": []}, {"text": "Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.9083850979804993}, {"text": "summarization", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.7848221063613892}]}, {"text": "Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., \"because\", \"but\") explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives.", "labels": [], "entities": [{"text": "implicit discourse relation recognition", "start_pos": 154, "end_pos": 193, "type": "TASK", "confidence": 0.6293045952916145}]}, {"text": "Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations).", "labels": [], "entities": []}, {"text": "To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text.", "labels": [], "entities": []}, {"text": "Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text.", "labels": [], "entities": []}, {"text": "In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific learning aims, and has the potential to improve readers' reading fluency and comprehension of the text (National Institute of Child.", "labels": [], "entities": [{"text": "National Institute of Child.", "start_pos": 244, "end_pos": 272, "type": "DATASET", "confidence": 0.9413642883300781}]}, {"text": "Therefore, for the task of discourse parsing, repeated reading is necessary, as it is difficult to generalize which words are really useful on the first try and efficient features should be dynamically exploited through several passes of reading . Now, let us check one real example to elaborate the necessity of using repeated reading in discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.6798973679542542}, {"text": "discourse parsing", "start_pos": 339, "end_pos": 356, "type": "TASK", "confidence": 0.7383345067501068}]}, {"text": "Arg-1 : the use of 900 toll numbers has been expanding rapidly in recent years Arg-2 : fora while, high-cost pornography lines and services that tempt children to dial (and redial) movie or music information earned the service a somewhat sleazy image (Comparison -wsj 2100) To identify the \"Comparison\" relation between the two arguments Arg-1 and Arg-2, the most crucial clues mainly lie in some content, like \"expanding rapidly\" in Arg-1 and \"earned the service a somewhat sleazy image\" in Arg-2, since there exists a contrast between the semantic meanings of these two text spans.", "labels": [], "entities": [{"text": "Arg-2", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9015635251998901}]}, {"text": "However, it is difficult to obtain sufficient information for pinpointing these words through scanning the argument pair left to right in one pass.", "labels": [], "entities": []}, {"text": "In such case, we follow the repeated reading strategy, where we obtain the general meaning through reading the arguments for the first time, re-read them later and gradually pay close attention to the key content.", "labels": [], "entities": []}, {"text": "Recently, some approaches simulating repeated reading have witnessed their success in different tasks.", "labels": [], "entities": []}, {"text": "These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation () and the external memory which can be read and written when processing the objects (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7139944732189178}]}, {"text": "For example, drew attention to specific facts of the input sequence and processed the sequence via multiple hops to generate an answer.", "labels": [], "entities": []}, {"text": "In computation vision, pointed out that repeatedly giving attention to different regions of an image could gradually lead to more precise image representations.", "labels": [], "entities": [{"text": "computation vision", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.901688814163208}]}, {"text": "Inspired by these recent work, for discourse parsing, we propose a model that aims to repeatedly read an argument pair and gradually focus on more fine-grained parts after grasping the global information.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7105981558561325}]}, {"text": "Specifically, we design the Neural Networks with Multi-Level Attention (NNMA) consisting of one general level and several attention levels.", "labels": [], "entities": []}, {"text": "In the general level, we capture the general representations of each argument based on two bidirectional long short-term memory (LSTM) models.", "labels": [], "entities": []}, {"text": "For each attention level, NNMA generates a weight vector over the argument pair to locate the important parts related to the discourse relation.", "labels": [], "entities": []}, {"text": "And an external short-term memory is designed to store the information exploited in previous levels and help update the argument representations.", "labels": [], "entities": []}, {"text": "We stack this structure in a recurrent manner, mimicking the process of reading the arguments multiple times.", "labels": [], "entities": []}, {"text": "Finally, we use the representation output from the highest attention level to identify the discourse relation.", "labels": [], "entities": []}, {"text": "Experiments on the PDTB dataset show that our proposed model achieves the state-of-art results.", "labels": [], "entities": [{"text": "PDTB dataset", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9777343571186066}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of Implicit Discourse Relations in  PDTB.", "labels": [], "entities": []}, {"text": " Table 2: Hyper-parameters for Neural Network with  Multi-Level Attention.", "labels": [], "entities": []}, {"text": " Table 3. For four-way classification, macro- averaged F 1 and Accuracy are used as evaluation  metrics. For binary classification, F 1 is adopted to  evaluate the performance on each class.", "labels": [], "entities": [{"text": "macro- averaged F 1", "start_pos": 39, "end_pos": 58, "type": "METRIC", "confidence": 0.6434442818164825}, {"text": "Accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.995185911655426}, {"text": "F 1", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.9835463762283325}]}, {"text": " Table 3: Performances of NNMA with Different  Attention Levels.", "labels": [], "entities": []}, {"text": " Table 4: Comparison with the State-of-the-art Approaches.", "labels": [], "entities": []}]}