{"title": [{"text": "Event Detection and Co-reference with Minimal Supervision", "labels": [], "entities": [{"text": "Event Detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7202396839857101}]}], "abstractContent": [{"text": "An important aspect of natural language understanding involves recognizing and categorizing events and the relations among them.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.6527945597966512}]}, {"text": "However, these tasks are quite subtle and annotating training data for machine learning based approaches is an expensive task, resulting in supervised systems that attempt to learn complex models from small amounts of data, which they over-fit.", "labels": [], "entities": []}, {"text": "This paper addresses this challenge by developing an event detection and co-reference system with minimal supervision , in the form of a few event examples.", "labels": [], "entities": [{"text": "event detection", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7786470353603363}]}, {"text": "We view these tasks as semantic similarity problems between event mentions or event mentions and an ontology of types, thus facilitating the use of large amounts of out of domain text data.", "labels": [], "entities": []}, {"text": "Notably, our semantic re-latedness function exploits the structure of the text by making use of a semantic-role-labeling based representation of an event.", "labels": [], "entities": []}, {"text": "We show that our approach to event detection is competitive with the top supervised methods.", "labels": [], "entities": [{"text": "event detection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.8754753768444061}]}, {"text": "More significantly, we outperform state-of-the-art supervised methods for event co-reference on benchmark data sets, and support significantly better transfer across domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language understanding involves, as a key component, the need to understand events mentioned in texts.", "labels": [], "entities": [{"text": "Natural language understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7189257542292277}]}, {"text": "This entails recognizing elements such as agents, patients, actions, location and time, among others.", "labels": [], "entities": []}, {"text": "Understanding events also necessitates understanding relations among them and, as a minimum, determining whether two snippets of text represent the same event or not -the event coreference problem.", "labels": [], "entities": []}, {"text": "Events have been studied for years, but they still remain a key challenge.", "labels": [], "entities": []}, {"text": "One reason is that the frame-based structure of events necessitates addressing multiple coupled problems that are not easy to study in isolation.", "labels": [], "entities": []}, {"text": "Perhaps an even more fundamental difficulty is that it is not clear whether our current set of events' definitions is adequate ( ).", "labels": [], "entities": []}, {"text": "Thus, given the complexity and fundamental difficulties, the current evaluation methodology in this area focuses on a limited domain of events, e.g. 33 types) and 38 types in TAC KBP ( . Consequently, this allows researchers to train supervised systems that are tailored to these sets of events and that overfit the small domain covered in the annotated data, rather than address the realistic problem of understanding events in text.", "labels": [], "entities": [{"text": "TAC KBP", "start_pos": 175, "end_pos": 182, "type": "DATASET", "confidence": 0.8104367554187775}]}, {"text": "In this paper, we pursue an approach to understanding events that we believe to be more feasible and scalable.", "labels": [], "entities": []}, {"text": "Fundamentally, event detection is about identifying whether an event in context is semantically related to a set of events of a specific type; and, event co-reference is about whether two event mentions are semantically similar enough to indicate that the author intends to refer to the same thing.", "labels": [], "entities": [{"text": "event detection", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7378080934286118}]}, {"text": "Therefore, if we formulate event detection and co-reference as semantic relatedness problems, we can scale it to deal with a lot more types and, potentially, generalize across domains.", "labels": [], "entities": [{"text": "event detection", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7231184393167496}]}, {"text": "Moreover, by doing so, we facilitate the use of a lot of data that is not part of the existing annotated event collections and not even from the same domain.", "labels": [], "entities": []}, {"text": "The key chal- lenges we need to address are those of how to represent events, and how to model event similarity; both are difficult partly since events have structure.", "labels": [], "entities": []}, {"text": "We present a general event detection and coreference framework, which essentially requires no labeled data.", "labels": [], "entities": [{"text": "event detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7193113565444946}]}, {"text": "In practice, in order to map an event mention to an event ontology, as away to communicate with a user, we just need a few event examples, in plain text, for each type a user wants to extract.", "labels": [], "entities": []}, {"text": "This is a reasonable setting; after all, giving examples is the easiest way of defining event types, and is also how information needs are defined to annotators -by providing examples in the annotation guideline.", "labels": [], "entities": []}, {"text": "1 Our approach makes less assumptions than standard unsupervised methods, which typically require a collection of instances and exploit similarities among them to eventually learn a model.", "labels": [], "entities": []}, {"text": "Here, given event type definitions (in the form of a few examples), we can classify a single event into a provided ontology and determine whether two events are co-referent.", "labels": [], "entities": []}, {"text": "In this sense, our approach is similar to what has been called dataless classification.", "labels": [], "entities": [{"text": "dataless classification", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.714584082365036}]}, {"text": "summarizes the difference between our approach, MSEP (Minimally Supervised Event Pipeline) 2 , and other methods.", "labels": [], "entities": []}, {"text": "Our approach builds on two key ideas.", "labels": [], "entities": []}, {"text": "First, to represent event structures, we use the general purpose nominal and verbial semantic role labeling (SRL) representation.", "labels": [], "entities": [{"text": "verbial semantic role labeling (SRL)", "start_pos": 77, "end_pos": 113, "type": "TASK", "confidence": 0.7814752459526062}]}, {"text": "This allows us to develop a structured representation of an event.", "labels": [], "entities": []}, {"text": "Second, we embed event components, while maintaining the structure, into multiple semantic spaces, in-: An overview of the end-to-end MSEP system.", "labels": [], "entities": []}, {"text": "\"Event Examples\" are the only supervision here, which produce \"Example Vectors\".", "labels": [], "entities": []}, {"text": "No training is needed for MSEP.", "labels": [], "entities": [{"text": "MSEP", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9636417031288147}]}, {"text": "duced at a contextual, topical, and syntactic levels.", "labels": [], "entities": []}, {"text": "These semantic representations are induced from large amounts of text in away that is completely independent of the tasks at hand, and are used to represent both event mentions and event types into which we classify our events.", "labels": [], "entities": []}, {"text": "The combination of these semantic spaces, along with the structured vector representation of an event, allow us to directly determine whether a candidate event mention is a valid event or not and, if it is, of which type.", "labels": [], "entities": []}, {"text": "Moreover, with the same representation, we can evaluate event similarities and decide whether two event mentions are co-referent.", "labels": [], "entities": []}, {"text": "Consequently, the proposed MSEP, can also adapt to new domains without any training.", "labels": [], "entities": []}, {"text": "An overview of the system is shown in.", "labels": [], "entities": []}, {"text": "A few event examples are all the supervision MSEP needs; even the few decision thresholds needed to beset are determined on these examples, once and for all, and are used for all test cases we evaluate on.", "labels": [], "entities": [{"text": "MSEP", "start_pos": 45, "end_pos": 49, "type": "TASK", "confidence": 0.8430347442626953}]}, {"text": "We use two benchmark datasets to compare MSEP with baselines and supervised systems.", "labels": [], "entities": [{"text": "MSEP", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.8849069476127625}]}, {"text": "We show that MSEP performs favorably relative to state-ofthe-art supervised systems; the co-reference module, in fact, outperforms supervised approaches on B and CEAF metrics.", "labels": [], "entities": [{"text": "MSEP", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9238531589508057}]}, {"text": "The superiority of MSEP is also demonstrated in across domain settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "ACE The ACE-2005 English corpus contains fine-grained event annotations, including event trigger, argument, entity, and time-stamp annotations.", "labels": [], "entities": [{"text": "ACE The ACE-2005 English corpus", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.7638176500797271}]}, {"text": "We select 40 documents from newswire articles for event detection evaluation and the rest for training (same as).", "labels": [], "entities": [{"text": "event detection evaluation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.9222074747085571}]}, {"text": "We do 10-fold cross-validation for event co-reference.", "labels": [], "entities": []}, {"text": "TAC-KBP The TAC-KBP-2015 corpus is annotated with event nuggets that fall into 38 types and coreference relations between events.", "labels": [], "entities": [{"text": "TAC-KBP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8980497121810913}, {"text": "TAC-KBP-2015 corpus", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.8141033053398132}]}, {"text": "We use the train/test data split provided by the official TAC-6 https://levyomer.wordpress.com/2014/04/25/dependencybased-word-embeddings The event ontology of TAC-KBP (based on ERE annotation) is almost the same to that of ACE.", "labels": [], "entities": [{"text": "TAC-KBP", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.8773367404937744}]}, {"text": "To adapt our system to the TAC-KBP corpus, we use all ACE event seeds of \"Contact.Phone-Write\" for \"Contact.Correspondence\" and separate ACE event seeds of \"Movement.Transport\" into \"Movement.TransportPerson\" and \"Movement.TransportArtifact\" by manual checking.", "labels": [], "entities": [{"text": "TAC-KBP corpus", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8866663575172424}]}, {"text": "So, we use exactly the same set of event seeds for TAC-KBP with only these two changes..", "labels": [], "entities": [{"text": "TAC-KBP", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.46217766404151917}]}, {"text": "Note that the training set and cross-validation is only for competing supervised methods.", "labels": [], "entities": []}, {"text": "For MSEP, we only need to run on each corpus once for testing.", "labels": [], "entities": [{"text": "MSEP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.801997184753418}]}, {"text": "For event detection, we use standard precision, recall and F1 metrics.", "labels": [], "entities": [{"text": "event detection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8871652185916901}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9995158910751343}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9985690116882324}, {"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9992157220840454}]}, {"text": "For event co-reference, we compare all systems using standard F1 metrics: MUC (, B 3 (Bagga and Baldwin, 1998), Entity-based CEAF (CEAF e ) ( and BLANC ().", "labels": [], "entities": [{"text": "F1", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9648957848548889}, {"text": "MUC", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.6807880997657776}, {"text": "BLANC", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.9908932447433472}]}, {"text": "We use the average scores (AVG) of these four metrics as the main comparison metric.", "labels": [], "entities": [{"text": "average scores (AVG)", "start_pos": 11, "end_pos": 31, "type": "METRIC", "confidence": 0.8561099648475647}]}, {"text": "To demonstrate the superiority of the adaptation capabilities of the proposed MSEP system, we test its performance on new domains and compare with the supervised system.", "labels": [], "entities": []}, {"text": "TAC-KBP corpus contains two genres: newswire (NW) and discussion forum (DF), and they have roughly equal number of documents.", "labels": [], "entities": [{"text": "TAC-KBP corpus", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8541643619537354}]}, {"text": "When trained on NW and tested on DF, supervised methods encounter out-of-domain situations.", "labels": [], "entities": [{"text": "DF", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.8862307071685791}]}, {"text": "However, the MSEP system can adapt well.", "labels": [], "entities": []}, {"text": "12 shows that MSEP outperforms supervised methods in out-of-domain situations for both tasks.", "labels": [], "entities": [{"text": "MSEP", "start_pos": 14, "end_pos": 18, "type": "TASK", "confidence": 0.9270848035812378}]}, {"text": "The differences are statistically significant with p < 0.05.).", "labels": [], "entities": []}, {"text": "presented a structured perceptron model to detect triggers and arguments jointly.", "labels": [], "entities": []}, {"text": "Attempts have also been made to use a Distributional Semantic Model (DSM) to represent events (.", "labels": [], "entities": []}, {"text": "A shortcoming of DSMs is that they ignore the structure within the context, thus reducing the distribution to a bag of words.", "labels": [], "entities": []}, {"text": "In our work, we preserve event structure via structured vector representations constructed from event components.", "labels": [], "entities": []}, {"text": "Event co-reference is much less studied in comparison to the large body of work on entity coreference.", "labels": [], "entities": [{"text": "entity coreference", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.6961938291788101}]}, {"text": "Our work follows the event co-reference definition in . All previous work on event co-reference except Cybulska and Vossen (2012) deals only with full co-reference.", "labels": [], "entities": []}, {"text": "Early works () performed event co-reference on scenario specific events.", "labels": [], "entities": []}, {"text": "Both and worked on sentence-level co-reference, which is closer to the definition of. dealt with both entity and event coreference by taking a three-layer approach.", "labels": [], "entities": []}, {"text": "proposed a clustering algorithm using a maximum entropy model with a range of features.", "labels": [], "entities": []}, {"text": "Bejan and Harabagiu (2010) built a class of nonparametric Bayesian models using a (potentially infinite) number of features to resolve both within and cross document event co-reference.", "labels": [], "entities": []}, {"text": "formed a system with deterministic layers to make co-reference decisions iteratively while jointly resolving entity and event co-reference.", "labels": [], "entities": []}, {"text": "More recently,  presented an unsupervised model to capture semantic relations and co-reference resolution, but they did not show quantitatively how well their system performed in each of these two cases.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.7751790583133698}]}], "tableCaptions": [{"text": " Table 2: Semantic role labeling coverage. We eval- uate both \"Predicates over Triggers\" and \"SRL Ar- guments over Event Arguments\". \"All\" stands for  the combination of Verb-SRL and Nom-SRL. The  evaluation is done on all data.", "labels": [], "entities": [{"text": "Semantic role labeling coverage", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8000257909297943}, {"text": "SRL Ar- guments", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.8532038629055023}]}, {"text": " Table 3: Statistics for the ACE and TAC-KBP cor- pora. #Sent. is the number of sentences, #Men.  is the number of event mentions, and #Cluster is  the number of event clusters (including singletons).  Note that the proposed MSEP does not need any  training data.", "labels": [], "entities": [{"text": "ACE", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9258072376251221}]}, {"text": " Table 4: Event detection (trigger identification)  results. \"Span\"/\"Type\" means span/type match re- spectively.", "labels": [], "entities": [{"text": "Event detection (trigger identification)", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.7423229316870371}]}, {"text": " Table 5: Event Co-reference Results on Gold Event Triggers. \"MSEP-Coref ESA,BC,W2V,DEP \" are varia- tions of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Depen- dency Embedding representations respectively. \"MSEP-Coref ESA+AUG \" uses augmented ESA event vec- tor representation and \"MSEP-Coref ESA+AUG+KNOW \" applies knowledge to detect conflicting events. (GA)  means that we use gold event arguments instead of approximated ones from SRL.", "labels": [], "entities": []}, {"text": " Table 6: Event Co-reference End-To-End Results.", "labels": [], "entities": []}, {"text": " Table 7: Domain Transfer Results. We con- duct the evaluation on TAC-KBP corpus with the  split of newswire (NW) and discussion form (DF)  documents. Here, we choose MSEP-EMD and  MSEP-Coref ESA+AUG+KNOW as the MSEP approach  for event detection and co-reference respectively.  We use SSED and Supervised Base as the supervised  modules for comparison. For event detection, we  compare F1 scores of span plus type match while we  report the average F1 scores for event co-reference.", "labels": [], "entities": [{"text": "TAC-KBP corpus", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.8562380075454712}, {"text": "MSEP-Coref ESA+AUG+KNOW", "start_pos": 181, "end_pos": 204, "type": "METRIC", "confidence": 0.5285415103038152}, {"text": "event detection", "start_pos": 231, "end_pos": 246, "type": "TASK", "confidence": 0.7042813748121262}, {"text": "event detection", "start_pos": 358, "end_pos": 373, "type": "TASK", "confidence": 0.817121148109436}, {"text": "F1", "start_pos": 387, "end_pos": 389, "type": "METRIC", "confidence": 0.9982625842094421}, {"text": "F1", "start_pos": 450, "end_pos": 452, "type": "METRIC", "confidence": 0.9973883032798767}]}]}