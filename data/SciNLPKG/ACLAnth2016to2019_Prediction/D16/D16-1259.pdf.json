{"title": [{"text": "Unsupervised Timeline Generation for Wikipedia History Articles", "labels": [], "entities": [{"text": "Unsupervised Timeline Generation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5008796552817026}, {"text": "Wikipedia History", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.8544466495513916}]}], "abstractContent": [{"text": "This paper presents a generic approach to content selection for creating timelines from individual history articles for which no external information about the same topic is available.", "labels": [], "entities": []}, {"text": "This scenario is in contrast to existing works on timeline generation, which require the presence of a large corpus of news articles.", "labels": [], "entities": [{"text": "timeline generation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8108064830303192}]}, {"text": "To identify salient events in a given history article, we exploit lexical cues about the article's subject area, as well as time expressions that are syntactically attached to an event word.", "labels": [], "entities": []}, {"text": "We also test different methods of ensuring timeline coverage of the entire historical time span described.", "labels": [], "entities": []}, {"text": "Our best-performing method outperforms anew unsupervised base-line and an improved version of an existing supervised approach.", "labels": [], "entities": []}, {"text": "We see our work as a step towards more semantically motivated approaches to single-document summarisation.", "labels": [], "entities": [{"text": "single-document summarisation", "start_pos": 76, "end_pos": 105, "type": "TASK", "confidence": 0.5813889354467392}]}], "introductionContent": [{"text": "While there has been much work on generating history timelines automatically, these approaches are commonly evaluated on events that took place in recent decades, as they depend on the availability of large numbers of articles describing the same historical period.", "labels": [], "entities": []}, {"text": "If such a rich data source is available, it is possible to exploit document creation times, redundancy across documents, as well as back-references to earlier events in order to identify salient events.", "labels": [], "entities": []}, {"text": "For instance, the start of the Iraq War in 2003 is mentioned frequently in a general news corpus, including in articles published years after the event took place.", "labels": [], "entities": []}, {"text": "The high number of mentions suggests that the beginning of the Iraq War was an important historical event.", "labels": [], "entities": []}, {"text": "However, for most historical periods covered in history articles (e.g., Antiquity or the Middle Ages), such cues are not commonly available, as no news articles from these eras exist.", "labels": [], "entities": []}, {"text": "Generating event timelines for arbitrary historical periods is therefore a much harder problem, which requires methods that rely less heavily on the types of rich, parallel and dense information contained in news clusters.", "labels": [], "entities": [{"text": "Generating event timelines", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8717303474744161}]}, {"text": "To investigate this problem, we approach timeline generation as a special single-document summarisation task.", "labels": [], "entities": [{"text": "timeline generation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8276742100715637}]}, {"text": "In other words, we assume that the information to be summarised is contained in a single history article, and that no further mentions of specific events exist externally.", "labels": [], "entities": []}, {"text": "This is a realistic scenario, for instance, fora specialist article describing the history of music in Ancient China.", "labels": [], "entities": []}, {"text": "We introduce a method for selecting salient content in history articles of any subject area, as long as the events in the text are roughly ordered chronologically.", "labels": [], "entities": []}, {"text": "The hypothesis is that knowledge of an text's subject area can help decide which content should be selected.", "labels": [], "entities": []}, {"text": "Another intuition is that certain combinations of events should be avoided in a timeline.", "labels": [], "entities": []}, {"text": "We therefore investigate ways of encouraging a balanced selection of content from all parts of the text.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluating our algorithms, the methodology we introduced in () is used, along with the accompanying Cambridge SingleDocument Timeline Corpus (CSDTC, version 2.0), which has been made publicly available 1 .  The evaluation is based on abstract (\"deep\") meaning units called Historical Content Units (HCUs).", "labels": [], "entities": [{"text": "Cambridge SingleDocument Timeline Corpus (CSDTC, version 2.0)", "start_pos": 104, "end_pos": 165, "type": "DATASET", "confidence": 0.9253748416900635}]}, {"text": "HCUs were derived on the basis of human-created timelines.", "labels": [], "entities": []}, {"text": "Between 32 and 80 HCUs per article were annotated for the articles in the CSDTC.", "labels": [], "entities": [{"text": "articles in the CSDTC", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.6317645907402039}]}, {"text": "Each HCU is weighted by the number of timeline creators who expressed its semantic content in their timelines.", "labels": [], "entities": []}, {"text": "Because HCUs are linked to TimeML events in the surface text, it is possible to perform automatic deep evaluation without requiring any manual annotation of system summaries.", "labels": [], "entities": []}, {"text": "Algorithms are evaluated on a given input article using an adapted version of the pyramid score (), which is calculated as the ratio between the sum of all rewards for HCUs chosen by the algorithm normalised by the maximum possible score score max : where w h is the weight of HCU h (a number between 1 and the number of annotators), E is the set of events in the article, T are the events in the system timeline, and the coverage score Cov(h, E, T ) is a number between 0 and 1 that indicates to what extent the events chosen by the algorithm jointly express the semantic content of HCU h.", "labels": [], "entities": [{"text": "coverage score Cov", "start_pos": 422, "end_pos": 440, "type": "METRIC", "confidence": 0.961378792921702}]}, {"text": "The basic version of Cov(h, E, T ) is defined as follows: where v h,e j is an anchor weight between 0 and 1 which denotes to what extent event e j expresses the semantic content of HCU h, and s(T, e) is a helper function that returns 1 if the set of selected events T includes event e, and 0 otherwise.", "labels": [], "entities": []}, {"text": "The coverage score for each HCU is calculated by summing up the anchor weights of those events that the algorithm has selected.", "labels": [], "entities": [{"text": "coverage score", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9691140353679657}]}, {"text": "A coverage score of 0 means that the events mentioned in the timeline do not express the HCU's semantic content at all, while a score of 1 occurs where the HCU's content is fully expressed by the timeline.", "labels": [], "entities": [{"text": "coverage score", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.969157338142395}]}, {"text": "Scores between 0 and 1 occur in a large number of cases.", "labels": [], "entities": []}, {"text": "For instance, an HCU may express the fact that a country was invaded and destroyed.", "labels": [], "entities": []}, {"text": "If the system timeline merely contains a TimeML event that refers to the invasion, it is assigned a coverage score of 0.5 for this HCU, as it expresses only half of the HCU's semantic content.", "labels": [], "entities": [{"text": "coverage score", "start_pos": 100, "end_pos": 114, "type": "METRIC", "confidence": 0.981414794921875}]}, {"text": "Where the sum exceeds 1, the coverage score is set to a hard upper limit of 1.", "labels": [], "entities": [{"text": "coverage score", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.9863872528076172}]}, {"text": "This ensures that algorithms are not doubly rewarded for selecting multiple TimeML events expressing the same semantic content.", "labels": [], "entities": []}, {"text": "The final formula we used to calculate coverage scores is slightly more complex, as some TimeML events in the CSDTC have been grouped together into event groups.", "labels": [], "entities": [{"text": "coverage scores", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.9184218347072601}, {"text": "TimeML events in the CSDTC", "start_pos": 89, "end_pos": 115, "type": "DATASET", "confidence": 0.8344629764556885}]}, {"text": "A detailed description is given in the documentation of the corpus.", "labels": [], "entities": []}, {"text": "Pyramid scores are recall-based: The evaluation assumes a maximum number of timeline entries n, and the maximum possible score is the sum of the HCU weights of then most highly weighted HCUs.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9977675676345825}]}, {"text": "The values for n are given in the CSDTC.", "labels": [], "entities": [{"text": "CSDTC", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9781071543693542}]}], "tableCaptions": [{"text": " Table 2: Average pyramid scores across all articles ( = signif-", "labels": [], "entities": [{"text": "Average pyramid scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8411911924680074}]}]}