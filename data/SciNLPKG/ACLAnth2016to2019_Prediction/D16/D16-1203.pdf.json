{"title": [{"text": "Analyzing the Behavior of Visual Question Answering Models", "labels": [], "entities": [{"text": "Analyzing the Behavior of Visual Question Answering", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6944870863642011}]}], "abstractContent": [{"text": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA).", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.7575445920228958}]}, {"text": "The performance of most models is clustered around 60-70%.", "labels": [], "entities": []}, {"text": "In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress.", "labels": [], "entities": []}, {"text": "We analyze two models, one each from two major classes of VQA models-with-attention and without-attention and show the similarities and differences in the behavior of these models.", "labels": [], "entities": []}, {"text": "We also analyze the winning entry of the VQA Challenge 2016.", "labels": [], "entities": [{"text": "VQA Challenge 2016", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9270233909289042}]}, {"text": "Our behavior analysis reveals that despite recent progress, today's VQA models are \"my-opic\" (tend to fail on sufficiently novel instances), often \"jump to conclusions\" (con-verge on a predicted answer after 'listening' to just half the question), and are \"stubborn\" (do not change their answers across images).", "labels": [], "entities": []}], "introductionContent": [{"text": "Visual Question Answering (VQA) is a recentlyintroduced () problem where given an image and a natural language question (e.g., \"What kind of store is this?\", \"How many people are waiting in the queue?\"), the task is to automatically produce an accurate natural language answer (\"bakery\", \"5\").", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7536449978748957}]}, {"text": "A flurry of recent deep-learning based models have been proposed for VQA.", "labels": [], "entities": [{"text": "VQA", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.6069990992546082}]}, {"text": "Curiously, the performance of most methods is clustered around 60-70% (compared to human performance of 83% on open-ended task and 91% on multiple-choice task) with a mere 5% gap between the top-9 entries on the VQA Challenge 2016.", "labels": [], "entities": [{"text": "VQA Challenge 2016", "start_pos": 212, "end_pos": 230, "type": "DATASET", "confidence": 0.929070770740509}]}, {"text": "It seems clear that as a first step to understand these models, to meaningfully compare strengths and weaknesses of different models, to develop insights into their failure modes, and to identify the most fruitful directions for progress, it is crucial to develop techniques to understand the behavior of VQA models.", "labels": [], "entities": []}, {"text": "In this paper, we develop novel techniques to characterize the behavior of VQA models.", "labels": [], "entities": []}, {"text": "As concrete instantiations, we analyze two VQA models (, one each from two major classes of VQA models -with-attention and without-attention.", "labels": [], "entities": []}, {"text": "We also analyze the winning entry ( of the VQA Challenge 2016.", "labels": [], "entities": [{"text": "VQA Challenge 2016", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.9368675549825033}]}], "datasetContent": [], "tableCaptions": []}