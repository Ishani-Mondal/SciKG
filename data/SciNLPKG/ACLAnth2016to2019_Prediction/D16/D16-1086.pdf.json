{"title": [{"text": "Porting an Open Information Extraction System from English to German", "labels": [], "entities": []}], "abstractContent": [{"text": "Many downstream NLP tasks can benefit from Open Information Extraction (Open IE) as a semantic representation.", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.757732629776001}]}, {"text": "While Open IE systems are available for English, many other languages lack such tools.", "labels": [], "entities": []}, {"text": "In this paper, we present a straightforward approach for adapting PropS, a rule-based predicate-argument analysis for English, to anew language, Ger-man.", "labels": [], "entities": []}, {"text": "With this approach, we quickly obtain an Open IE system for German covering 89% of the English rule set.", "labels": [], "entities": [{"text": "English rule set", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.9282324910163879}]}, {"text": "It yields 1.6 n-ary extractions per sentence at 60% precision, making it comparable to systems for English and readily usable in downstream applications.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9980912804603577}]}], "introductionContent": [{"text": "The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)).", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.7852308196680886}]}, {"text": "Open IE has been shown to be useful fora wide range of semantic tasks, including question answering), summarization () and text comprehension (, and has consequently drawn consistent attention over the last years (.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.6237990856170654}, {"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8165930509567261}, {"text": "summarization", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.9856429696083069}]}, {"text": "Although similar applications of Open IE in other languages are obvious, most previous work focused on English, with only a few recent exceptions (.", "labels": [], "entities": []}, {"text": "For most languages, Open IE systems are still missing.", "labels": [], "entities": []}, {"text": "While one could create them from scratch, as it was done for Spanish, this can be a very laborious process, as state-of-the-art systems make use of handcrafted, linguistically motivated rules.", "labels": [], "entities": []}, {"text": "Instead, an alternative approach is to transfer the rule sets of available systems for English to the new language.", "labels": [], "entities": []}, {"text": "In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language.", "labels": [], "entities": [{"text": "Open IE tuples from English dependency parses", "start_pos": 68, "end_pos": 113, "type": "TASK", "confidence": 0.5581568351813725}]}, {"text": "We use German, a relatively close language, and the PropS system ( as examples in our analysis.", "labels": [], "entities": [{"text": "PropS system", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.9381237328052521}]}, {"text": "Instead of creating rule sets from scratch, such a transfer approach would simplify the rule creation, making it possible to build Open IE systems for other languages with relatively low effort in a short amount of time.", "labels": [], "entities": [{"text": "rule creation", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.7580735385417938}]}, {"text": "However, challenges we need to address are differences in syntax, dissimilarities in the corresponding dependency representations as well as language-specific phenomena.", "labels": [], "entities": []}, {"text": "Therefore, the existing rules cannot be directly mapped to the German part-of-speech and dependency tags in a fully automatic way, but require a careful analysis as carried out in this work.", "labels": [], "entities": []}, {"text": "Similar manual approaches to transfer rule-based systems to new languages were shown to be successful, e.g. for temporal tagging, whereas fully automatic approaches led to less competitive systems).", "labels": [], "entities": [{"text": "temporal tagging", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.7271730005741119}]}, {"text": "Our analysis reveals that a large fraction of the PropS rule set can be easily ported to German, requiring only small adaptations.", "labels": [], "entities": [{"text": "PropS rule set", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9519171913464864}]}, {"text": "With roughly 10% Sehenswert sind die Orte San Jose und San Andres, die an der n\u00f6rdlichen K\u00fcste des Pet\u00e9n-Itz\u00e1-Sees liegen.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experimental Setup Following the common evaluation protocol for Open IE systems, we manually label extractions made by our system.", "labels": [], "entities": []}, {"text": "For this purpose, we created anew dataset consisting of 300 German sentences, randomly sampled from three sources of different genres: news articles from TIGER (), German web pages from CommonCrawl () and featured Wikipedia articles.", "labels": [], "entities": []}, {"text": "For the treebank part, we ran our system using both gold and parsed dependencies to analyze the impact of parsing errors.", "labels": [], "entities": [{"text": "parsing errors", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.8668983280658722}]}, {"text": "Every tuple extracted from this set of 300 sentences was labeled independently by two annotators as corrector incorrect.", "labels": [], "entities": []}, {"text": "In line with previous work, they were instructed to label an extraction as incorrect if it has a wrong predicate or argument, including overspecified and incomplete arguments, or if it is well-formed but not entailed by the sentence.", "labels": [], "entities": []}, {"text": "Unresolved co-references were not marked as incorrect.", "labels": [], "entities": []}, {"text": "We observed an inter-annotator agreement of 85% (\u03ba = 0.63).", "labels": [], "entities": []}, {"text": "For the evaluation, we merged the labels, considering an extraction as correct only if both annotators labeled it as such.", "labels": [], "entities": []}, {"text": "Results are measured in terms of precision, the fraction of correct extractions, and yield, the total number of extractions.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9996533393859863}, {"text": "yield", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.990993320941925}]}, {"text": "A precision-yield curve is obtained by decreasing a confidence threshold.", "labels": [], "entities": [{"text": "precision-yield", "start_pos": 2, "end_pos": 17, "type": "METRIC", "confidence": 0.9987412095069885}]}, {"text": "The confidence predictor was trained on a separate development set.", "labels": [], "entities": []}, {"text": "Results From the whole corpus of 300 sentences, PropsDE extracted 487 tuples, yielding on average 1.6 per sentence with 2.9 arguments.", "labels": [], "entities": []}, {"text": "60% of them were labeled as correct.", "labels": [], "entities": []}, {"text": "shows that most extractions are made from Wikipedia articles, whereas the highest precision can be observed for newswire text.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.99751877784729}]}, {"text": "According to our expectations, web pages are most challenging, presumably due to noisier language.", "labels": [], "entities": []}, {"text": "These differences between the genres can also be seen in the precision-yield curve).", "labels": [], "entities": [{"text": "precision-yield", "start_pos": 61, "end_pos": 76, "type": "METRIC", "confidence": 0.9986423850059509}]}, {"text": "For English, state-of-the-art systems show a similar performance.", "labels": [], "entities": []}, {"text": "Ina direct comparison of several systems carried out by Del, they observed overall precisions of 58% (Reverb), 57% (ClausIE), 43% (WOE) and 43% (OLLIE) on datasets of similar genre.", "labels": [], "entities": [{"text": "precisions", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9986454844474792}, {"text": "Reverb", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.7620458602905273}, {"text": "ClausIE", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.8632088303565979}, {"text": "OLLIE", "start_pos": 145, "end_pos": 150, "type": "METRIC", "confidence": 0.9907757639884949}]}, {"text": "The reported yield per sentence is higher for ClausIE (4.2), OL-LIE (2.6) and WOE (2.1), but smaller for Reverb (1.4).", "labels": [], "entities": [{"text": "yield", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.8653374314308167}, {"text": "ClausIE", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.7951673865318298}, {"text": "OL-LIE", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.6894088983535767}, {"text": "WOE", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.6719824075698853}, {"text": "Reverb", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.6269432306289673}]}, {"text": "However, we note that in their evaluation, they configured all systems to output only two-argumentextractions.", "labels": [], "entities": []}, {"text": "For example, from a sentence such as The principal opposition parties boycotted the polls after accusations of vote-rigging.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus size (length in token) and system performance", "labels": [], "entities": []}]}