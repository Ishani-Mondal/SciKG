{"title": [{"text": "Exploiting Sentence Similarities for Better Alignments", "labels": [], "entities": [{"text": "Exploiting Sentence Similarities", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7119698226451874}, {"text": "Alignments", "start_pos": 44, "end_pos": 54, "type": "TASK", "confidence": 0.6617401838302612}]}], "abstractContent": [{"text": "We study the problem of jointly aligning sentence constituents and predicting their similarities.", "labels": [], "entities": []}, {"text": "While extensive sentence similarity data exists, manually generating reference alignments and labeling the similarities of the aligned chunks is comparatively onerous.", "labels": [], "entities": []}, {"text": "This prompts the natural question of whether we can exploit easy-to-create sentence level data to train better aligners.", "labels": [], "entities": []}, {"text": "In this paper, we present a model that learns to jointly align constituents of two sentences and also predict their similarities.", "labels": [], "entities": []}, {"text": "By taking advantage of both sentence and constituent level data, we show that our model achieves state-of-the-art performance at predicting alignments and constituent similarities.", "labels": [], "entities": [{"text": "predicting alignments", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.8865550458431244}]}], "introductionContent": [{"text": "The problem of discovering semantic relationships between two sentences has given birth to several NLP tasks over the years.", "labels": [], "entities": []}, {"text": "Textual entailment, inter alia) asks about the truth of a hypothesis sentence given another sentence (or more generally a paragraph).", "labels": [], "entities": []}, {"text": "Paraphrase identification (, inter alia) asks whether two sentences have the same meaning.", "labels": [], "entities": [{"text": "Paraphrase identification (, inter alia)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7338771990367344}]}, {"text": "Foregoing the binary entailment and paraphrase decisions, the semantic textual similarity (STS) task) asks fora numeric measure of semantic equivalence between two sentences.", "labels": [], "entities": []}, {"text": "All three tasks have attracted much interest in the form of shared tasks.", "labels": [], "entities": []}, {"text": "While various approaches have been proposed to predict these sentence relationships, a commonly employed strategy ( al., 2010a) is to postulate an alignment between constituents of the sentences and use this alignment to make the final prediction (a binary decision or a numeric similarity score).", "labels": [], "entities": []}, {"text": "The implicit assumption in such approaches is that better constituent alignments can lead to better identification of semantic relationships between sentences.", "labels": [], "entities": [{"text": "identification of semantic relationships between sentences", "start_pos": 100, "end_pos": 158, "type": "TASK", "confidence": 0.799868623415629}]}, {"text": "Constituent alignments serve two purposes.", "labels": [], "entities": []}, {"text": "First, they act as an intermediate representation for predicting the final output.", "labels": [], "entities": []}, {"text": "Second, the alignments help interpret (and debug) decisions made by the overall system.", "labels": [], "entities": []}, {"text": "For example, the alignment between the sentences in cannot only be useful to determine the equivalence of the two sentences, but also help reason about the predictions.", "labels": [], "entities": []}, {"text": "The importance of this intermediate representation led to the creation of the interpretable semantic textual similarity task () that focuses on predicting chunk-level alignments and similarities.", "labels": [], "entities": [{"text": "predicting chunk-level alignments", "start_pos": 144, "end_pos": 177, "type": "TASK", "confidence": 0.8321048418680826}]}, {"text": "However, while extensive resources exist for sentence-level relationships, human annotated chunk-aligned data is comparatively smaller.", "labels": [], "entities": []}, {"text": "In this paper, we address the following question: can we use sentence-level resources to better pre-dict constituent alignments and similarities?", "labels": [], "entities": []}, {"text": "To answer this question, we focus on the semantic textual similarity (STS) task and its interpretable variant.", "labels": [], "entities": [{"text": "semantic textual similarity (STS) task", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.7041304154055459}]}, {"text": "We propose a joint model that aligns constituents and integrates the information across the aligned edges to predict both constituent and sentence level similarity.", "labels": [], "entities": []}, {"text": "The key advantage of modeling these two problems jointly is that, during training, the sentence-level information can provide feedback to the constituent-level predictions.", "labels": [], "entities": []}, {"text": "We evaluate our model on the SemEval-2016 task of interpretable STS.", "labels": [], "entities": [{"text": "interpretable STS", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7777760922908783}]}, {"text": "We show that even without the sentence information, our joint model that uses constituent alignments and similarities forms a strong baseline.", "labels": [], "entities": []}, {"text": "Further, our easily extensible joint model can incorporate sentence-level similarity judgments to produce alignments and chunk similarities that are comparable to the best results in the shared task.", "labels": [], "entities": []}, {"text": "In summary, the contributions of this paper are: 1.", "labels": [], "entities": []}, {"text": "We present the first joint model for predicting constituent alignments and similarities.", "labels": [], "entities": [{"text": "predicting constituent alignments and similarities", "start_pos": 37, "end_pos": 87, "type": "TASK", "confidence": 0.8800325989723206}]}, {"text": "Our model can naturally take advantage of the much larger sentence-level annotations.", "labels": [], "entities": []}, {"text": "2. We evaluate our model on the SemEval-2016 task of interpretable semantic similarity and show state-of-the-art results.", "labels": [], "entities": [{"text": "SemEval-2016 task of interpretable semantic similarity", "start_pos": 32, "end_pos": 86, "type": "TASK", "confidence": 0.6781743516524633}]}], "datasetContent": [{"text": "The primary research question we seek to answer via experiments is: Can we better predict chunk alignments and similarities by taking advantage of sentence level similarity data?", "labels": [], "entities": [{"text": "predict chunk alignments", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.6191240151723226}]}, {"text": "Datasets We used the training and test data from the 2016 SemEval shared tasks of predicting semantic textual similarity (Agirre et al., 2016a) and interpretable STS (, that is, tasks 1 and 2 respectively.", "labels": [], "entities": [{"text": "predicting semantic textual similarity", "start_pos": 82, "end_pos": 120, "type": "TASK", "confidence": 0.8310947865247726}, {"text": "interpretable STS", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.5397374033927917}]}, {"text": "For our experiments, we used the headlines and images sections of the data.", "labels": [], "entities": []}, {"text": "The data for the interpretable STS task, consisting of manually aligned and scored chunks, provides the alignment datasets for training (D A ).", "labels": [], "entities": [{"text": "interpretable STS task", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7582358320554098}]}, {"text": "The headlines section of the training data consists for 756 sentence pairs, while the images section consists for 750 sentence pairs.", "labels": [], "entities": []}, {"text": "The data for the STS task acts as our sentence level training dataset (D S ).", "labels": [], "entities": [{"text": "STS task", "start_pos": 17, "end_pos": 25, "type": "TASK", "confidence": 0.8077279925346375}]}, {"text": "For the headlines section, we used the 2013 headlines test set consisting of 750 sentence pairs with gold sentence similarity scores.", "labels": [], "entities": [{"text": "2013 headlines test set", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.8046870827674866}]}, {"text": "For the images section, we used the 2014 images test set consisting of 750 examples.", "labels": [], "entities": [{"text": "2014 images test set", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.8716978579759598}]}, {"text": "We evaluated our models on the official Task 2 test set, consisting of 375 sentence pairs for both the headlines and images sections.", "labels": [], "entities": [{"text": "official Task 2 test set", "start_pos": 31, "end_pos": 55, "type": "DATASET", "confidence": 0.7852746248245239}]}, {"text": "In all experiments, we used gold standard chunk boundaries if they are available (i.e., for DA ).", "labels": [], "entities": []}, {"text": "Pre-processing We pre-processed the sentences with parts of speech using the Stanford CoreNLP toolkit ( ).", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 77, "end_pos": 101, "type": "DATASET", "confidence": 0.948642353216807}]}, {"text": "Since our setting assumes that we have the chunks as input, we used the Illinois shallow parser () to extract chunks from D S . We post-processed the predicted chunks to correct for errors using the following steps: 1.", "labels": [], "entities": []}, {"text": "Split on punctuation; 2.", "labels": [], "entities": []}, {"text": "Split on verbs in NP; 3.", "labels": [], "entities": []}, {"text": "Split on nouns in VP; 4.", "labels": [], "entities": [{"text": "Split on nouns in VP", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6823073983192444}]}, {"text": "Merge PP+NP into PP; 5.", "labels": [], "entities": []}, {"text": "Merge VP+PRT into VP if the PRT chunk is not a preposition or a subordinating conjunction; 6.", "labels": [], "entities": []}, {"text": "Merge SBAR+NP into SBAR; and 7.", "labels": [], "entities": []}, {"text": "Create new contiguous chunks using tokens that are marked as being outside a chunk by the shallow parser.", "labels": [], "entities": []}, {"text": "We found that using the above postprocessing rules, improved the F1 of chunk accuracy from 0.7865 to 0.8130.", "labels": [], "entities": [{"text": "F1 of chunk accuracy", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.8405425697565079}]}, {"text": "We also found via crossvalidation that this post-processing improved overall alignment accuracy.", "labels": [], "entities": [{"text": "alignment", "start_pos": 77, "end_pos": 86, "type": "TASK", "confidence": 0.9122849702835083}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9349494576454163}]}, {"text": "The reader may refer to other STS resources () for further improvements along this direction.", "labels": [], "entities": []}, {"text": "Experimental setup We performed stochastic gradient descent for 200 epochs in our experiments, with a mini-batch size of 20.", "labels": [], "entities": []}, {"text": "We determined the three \u03bb's using cross-validation, with different hyperparameters for examples from DA and D S .  As noted in Section 3.1, the parameter \u03b1 l combines chunk scores into sentence scores.", "labels": [], "entities": []}, {"text": "To find these hyper-parameters, we used a set of 426 sentences from the from the headlines training data that had both sentence and chunk annotation.", "labels": [], "entities": [{"text": "headlines training data", "start_pos": 81, "end_pos": 104, "type": "DATASET", "confidence": 0.6672466496626536}]}, {"text": "We simplified the search by assuming that \u03b1 Equi is always 1.0 and all labels other than OPPO have the same \u03b1.", "labels": [], "entities": [{"text": "\u03b1 Equi", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.7211602628231049}]}, {"text": "Using grid search over in increments of 0.1, we selected \u03b1's that gave us the highest Pearson correlation for sentence level similarities.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 86, "end_pos": 105, "type": "METRIC", "confidence": 0.9815181791782379}]}, {"text": "The best \u03b1's (with a Pearson correlation of 0.7635) were:", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 21, "end_pos": 40, "type": "METRIC", "confidence": 0.9712095260620117}]}], "tableCaptions": [{"text": " Table 2: F-score for headlines and images datasets. These tables show the result of our systems, baseline  and top-ranked systems. D A is our strong baseline trained on interpretable STS dataset; D A + D S is trained  on interpretable STS as well as STS dataset. The rank 1 system on headlines is Inspire (Kazmi and Sch\u00fcller,  2016) and UWB (Konopik et al., 2016) on images. Bold are the best scores.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9903473854064941}, {"text": "STS dataset", "start_pos": 251, "end_pos": 262, "type": "DATASET", "confidence": 0.8047069609165192}, {"text": "UWB", "start_pos": 338, "end_pos": 341, "type": "DATASET", "confidence": 0.7933303117752075}]}, {"text": " Table 3: F-score for the domain adaptation experi- ments. This table shows the performance of training  on different dataset combinations.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.998247504234314}]}]}