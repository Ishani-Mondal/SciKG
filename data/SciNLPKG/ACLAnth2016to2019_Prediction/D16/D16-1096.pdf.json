{"title": [{"text": "Coverage Embedding Models for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7721737225850424}]}], "abstractContent": [{"text": "In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT.", "labels": [], "entities": [{"text": "attention-based neural machine translation (NMT)", "start_pos": 30, "end_pos": 78, "type": "TASK", "confidence": 0.7170260974339077}]}, {"text": "For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes.", "labels": [], "entities": []}, {"text": "Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has gained popularity in recent years (e.g. (), especially for the attentionbased models of . The attention at each time step shows which source word the model should focus onto predict the next target word.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7889437278111776}]}, {"text": "However, the attention in each step only looks at the previous hidden state and the previous target word, there is no history or coverage information typically for each source word.", "labels": [], "entities": []}, {"text": "As a result, this kind of model suffers from issues of repeating or dropping translations.", "labels": [], "entities": []}, {"text": "The traditional statistical machine translation (SMT) systems (e.g.) address the above issues by employing a source side \"coverage vector\" for each sentence to indicate explicitly which words have been translated, which parts have not yet.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.7804588427146276}]}, {"text": "A coverage vector starts with all zeros, meaning no word has been translated.", "labels": [], "entities": []}, {"text": "If a source word at position j got translated, the coverage vector sets position j as 1, and they won't use this source word in future translation.", "labels": [], "entities": []}, {"text": "This mechanism avoids the repeating or dropping translation problems.", "labels": [], "entities": []}, {"text": "However, it is not easy to adapt the \"coverage vector\" to NMT directly, as attentions are soft probabilities, not 0 or 1.", "labels": [], "entities": []}, {"text": "And SMT approaches handle one-tomany fertilities by using phrases or hiero rules (predict several words in one step), while NMT systems only predict one word at each step.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.989616870880127}]}, {"text": "In order to alleviate all those issues, we borrow the basic idea of \"coverage vector\", and introduce a coverage embedding vector for each source word.", "labels": [], "entities": []}, {"text": "We keep updating those embedding vectors at each translation step, and use those vectors to track the coverage information.", "labels": [], "entities": []}, {"text": "Here is a brief description of our approach.", "labels": [], "entities": []}, {"text": "At the beginning of translation, we start from a full coverage embedding vector for each source word.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.976161777973175}]}, {"text": "This is different from the \"coverage vector\" in SMT in following two aspects: \u2022 each source word has its own coverage embedding vector, instead of 0 or 1, a scalar, in SMT, \u2022 we start with a full embedding vector for each word, instead of 0 in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9726110696792603}, {"text": "SMT", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.9290620684623718}]}, {"text": "After we predict a translation wordy tat time step t, we need to update each coverage embedding vector accordingly based on the attentions in the current step.", "labels": [], "entities": []}, {"text": "Our motivation is that if we observe a very high attention over xi in this step, there is a high chance that xi and y tare translation equivalent.", "labels": [], "entities": []}, {"text": "So the embedding vector of xi should come to empty (a zero vector) in a one-to-one translation case, or subtract the embedding of y t for the one-to-many translation case.", "labels": [], "entities": []}, {"text": "An empty coverage embedding of a word xi indicates this word is translated, and we cannot translate xi again in future.", "labels": [], "entities": []}, {"text": "Empirically, we model this procedure by using neural networks (gated recurrent unit (GRU) ( ) or direct subtraction).", "labels": [], "entities": []}, {"text": "Large-scale experiments over Chinese-to-English on various test sets show that our method improves the translation quality significantly over the large vocabulary NMT system (Section 5).", "labels": [], "entities": []}, {"text": "\u21b5 t,l: The architecture of attention-based NMT.The source sentence is x = (x 1 , ..., x l ) with length l, the translation is y * = (y * 1 , ..., y * m ) with length m.", "labels": [], "entities": []}, {"text": "\u2190 \u2212 hi and \u2212 \u2192 hi are bi-directional encoder states.", "labels": [], "entities": []}, {"text": "\u03b1 t,j is the attention probability at time t, position j.", "labels": [], "entities": []}, {"text": "H t is the weighted sum of encoding states.", "labels": [], "entities": []}, {"text": "st is a hidden state.", "labels": [], "entities": []}, {"text": "o t is an output state.", "labels": [], "entities": []}, {"text": "Another one layer neural network projects o t to the target output vocabulary, and conducts softmax to predict the probability distribution over the output vocabulary.", "labels": [], "entities": []}, {"text": "The attention model (in right gray box) is a two layer feedforward neural network, A t,j is an intermediate state, then another layer converts it into areal number e t,j , the final attention probability at position j is \u03b1 t,j . We plug coverage embedding models into NMT model by adding an input c t\u22121,xj to A t,j (the red dotted line).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Single system results in terms of (TER-BLEU)/2 (the lower the better) on 5 million Chinese to English  training set. NMT results are on a large vocabulary (300k) and with UNK replaced. U GRU : updating with a GRU;  U Sub : updating as a subtraction; U GRU + U Sub : combination of two methods (do not share coverage embedding  vectors); +Obj.: U GRU + U Sub with an additional objective in Equation 6, we have two \u03bbs for U GRU and U Sub  separately, and we test \u03bb GRU = 1 \u00d7 10 \u22124 and \u03bb Sub = 1 \u00d7 10 \u22122 .", "labels": [], "entities": [{"text": "TER-BLEU", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9885613322257996}, {"text": "Obj.", "start_pos": 348, "end_pos": 352, "type": "METRIC", "confidence": 0.9961523413658142}]}, {"text": " Table 2: Single system results in terms of (TER-BLEU)/2  on 11 million set. NMT results are on a large vocabulary  (500k) and with UNK replaced. Due to the time limita- tion, we only have the results of U GRU system.", "labels": [], "entities": [{"text": "TER-BLEU)/2", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9436331192652384}]}]}