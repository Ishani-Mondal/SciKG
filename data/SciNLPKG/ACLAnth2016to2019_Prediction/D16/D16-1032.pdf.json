{"title": [{"text": "Globally Coherent Text Generation with Neural Checklist Models", "labels": [], "entities": [{"text": "Coherent Text Generation", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.619913250207901}]}], "abstractContent": [{"text": "Recurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said-especially when constructing long texts.", "labels": [], "entities": []}, {"text": "We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output.", "labels": [], "entities": []}, {"text": "The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items.", "labels": [], "entities": []}, {"text": "Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (.", "labels": [], "entities": [{"text": "natural language generation tasks", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.7714031487703323}]}, {"text": "Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content.", "labels": [], "entities": []}, {"text": "These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long.", "labels": [], "entities": []}, {"text": "In this paper, we present anew recurrent neural model that maintains coherence while improv- \"salt,\" \"lime,\" etc.) have already been used (checked boxes).", "labels": [], "entities": []}, {"text": "The model is trained to interpolate an RNN (e.g., encode \"pico de gallo\" and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; \"tomatoes,\" etc.).", "labels": [], "entities": []}, {"text": "ing coverage by globally tracking what has been said and what is still left to be said incomplete texts.", "labels": [], "entities": []}, {"text": "For example, consider the challenge of generating a cooking recipe, where the title and ingredient list are provided as inputs and the system must generate a complete text that describes how to produce the desired dish.", "labels": [], "entities": []}, {"text": "Existing RNN models may lose track of which ingredients have already been mentioned, especially during the generation of along recipe with many ingredients.", "labels": [], "entities": []}, {"text": "Recent work has focused on adapting neural network architectures to improve coverage () with application to generating customer service responses, such as hotel information, where a single sentence is generated to describe a few key ideas.", "labels": [], "entities": []}, {"text": "Our focus is instead on developing a model that maintains coherence while producing longer texts or covering longer input specifications (e.g., along ingredient list).", "labels": [], "entities": []}, {"text": "More specifically, our neural checklist model generates a natural language description for achieving a goal, such as generating a recipe fora particular dish, while using anew checklist mechanism to keep track of an agenda of items that should be mentioned, such as a list of ingredients (see).", "labels": [], "entities": []}, {"text": "The checklist model learns to interpolate among three components at each time step: (1) an encoder-decoder language model that generates goal-oriented text, (2) an attention model that tracks remaining agenda items that need to be introduced, and (3) an attention model that tracks the used, or checked, agenda items.", "labels": [], "entities": []}, {"text": "Together, these components allow the model to learn representations that best predict which words should be included in the text and when references to agenda items should be checked off the list (see check marks in).", "labels": [], "entities": []}, {"text": "We evaluate our approach on anew cooking recipe generation task and the dialogue act generation from.", "labels": [], "entities": [{"text": "cooking recipe generation task", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.6777222082018852}]}, {"text": "In both cases, the model must correctly describe a list of agenda items: an ingredient list or a set of facts, respectively.", "labels": [], "entities": []}, {"text": "Generating recipes additionally tests the ability to maintain coherence in long procedural texts.", "labels": [], "entities": []}, {"text": "Experiments in dialogue generation demonstrate that our approach outperforms previous work with up to a 4 point BLEU improvement.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8600887656211853}, {"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9881439805030823}]}, {"text": "Our model also scales to cooking recipes, where both automated and manual evaluations demonstrate that it maintains the strong local coherence of baseline RNN techniques while significantly improving the global coverage by effectively integrating the agenda items.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model was implemented and trained using the Torch scientific computing framework for Lua.", "labels": [], "entities": [{"text": "Torch scientific computing framework", "start_pos": 48, "end_pos": 84, "type": "DATASET", "confidence": 0.8692789226770401}, {"text": "Lua", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.4856495261192322}]}, {"text": "Experiments We evaluated neural checklist models on two natural language generation tasks.", "labels": [], "entities": [{"text": "natural language generation tasks", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.8211606591939926}]}, {"text": "The first task is cooking recipe generation.", "labels": [], "entities": [{"text": "cooking recipe generation", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.8348377148310343}]}, {"text": "Given a recipe title (i.e., the name of the dish) as the goal and the list of ingredients as the agenda, the system must generate the correct recipe text.", "labels": [], "entities": []}, {"text": "Our second evaluation is based on the task from for generating dialogue responses for hotel and restaurant information systems.", "labels": [], "entities": []}, {"text": "The task is to generate a natural language response given a query type (e.g., informing or querying) and a list of facts to convey (e.g., a hotel's name and address).", "labels": [], "entities": []}, {"text": "Parameters We constrain the gradient norm to 5.0 and initialize parameters uniformly on.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9543381929397583}]}, {"text": "We used abeam of size 10 for generation.", "labels": [], "entities": []}, {"text": "Based on dev set performance, a learning rate of 0.1 was chosen, and the temperature hyperparameters (\u03b2, \u03b3) were (5, 2) for the recipe task and (1, 10) for the dialogue task.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9344422817230225}]}, {"text": "The models for the recipe task had a hidden state size of k = 256; the 3 http://torch.ch/ models for the dialogue task had k = 80 to compare to previous models.", "labels": [], "entities": []}, {"text": "We use a batch size 30 for the recipe task and 10 for the dialogue task.", "labels": [], "entities": []}, {"text": "Recipe data and pre-processing We use the Now You're Cooking!", "labels": [], "entities": [{"text": "Now You're Cooking!", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.8588179826736451}]}, {"text": "recipe library: the data set contains over 150,000 recipes in the Meal-Master TM format.", "labels": [], "entities": []}, {"text": "We heuristically removed sentences that were not recipe steps (e.g., author notes, nutritional information, publication information).", "labels": [], "entities": []}, {"text": "82,590 recipes were used for training, and 1,000 each for development and testing.", "labels": [], "entities": []}, {"text": "We filtered out recipes to avoid exact duplicates between training and dev (test) sets.", "labels": [], "entities": []}, {"text": "We collapsed multi-word ingredient names into single tokens using word2phrase ran on the training data ingredient lists.", "labels": [], "entities": [{"text": "training data ingredient lists", "start_pos": 89, "end_pos": 119, "type": "DATASET", "confidence": 0.7707898989319801}]}, {"text": "Titles and ingredients were cleaned of non-word tokens.", "labels": [], "entities": []}, {"text": "Ingredients additionally were stripped of amounts (e.g., \"1 tsp\").", "labels": [], "entities": []}, {"text": "4.6, we approximate true values for the interpolation weights and attention updates for recipes based on string match between the recipe text and the ingredient list.", "labels": [], "entities": []}, {"text": "The first ingredient reference in a sentence cannot be the first token or after a comma (e.g., the bold tokens cannot be ingredients in \"oil the pan\" and \"in a large bowl, mix [...]\").", "labels": [], "entities": []}, {"text": "Recipe data statistics Automatic recipe generation is difficult due to the length of recipes, the size of the vocabulary, and the variety of possible dishes.", "labels": [], "entities": [{"text": "Automatic recipe generation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.606582244237264}]}, {"text": "In our training data, the average recipe length is 102 tokens, and the longest recipe has 814 tokens.", "labels": [], "entities": []}, {"text": "The vocabulary of the recipe text from the training data (i.e., the text of the recipe not including the title or ingredient list) has 14,103 unique tokens.", "labels": [], "entities": []}, {"text": "About 31% of tokens in the recipe vocabulary occur at least 100 times in the training data; 8.6% of the tokens occur at least 1000 times.", "labels": [], "entities": []}, {"text": "The training data also represents a wide variety of recipe types, defined by the recipe titles.", "labels": [], "entities": []}, {"text": "Of 3793 title tokens, only 18.9% of the title tokens in the title vocabulary occur at least 100 times in the training data, which demonstrates the large variability in the titles.", "labels": [], "entities": []}, {"text": "Dialogue system data and processing We used the hotel and restaurant dialogue system corpus and the same train-development-test split from.", "labels": [], "entities": []}, {"text": "We used the same pre-processing, sets of reference samples, and baseline output, and we were given model output to compare against.", "labels": [], "entities": []}, {"text": "For training, slot values (e.g., \"Red Door Cafe\") were replaced by generic tokens (e.g., \"NAME TOKEN\").", "labels": [], "entities": [{"text": "NAME TOKEN", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.7755536437034607}]}, {"text": "After generation, generic tokens were swapped back to specific slot values.", "labels": [], "entities": []}, {"text": "Minor post-processing included removing duplicate determiners from the relexicalization and merging plural \"-s\" tokens onto their respective words.", "labels": [], "entities": []}, {"text": "After replacing specific slot values with generic tokens, the training data vocabulary size of the hotel corpus is 445 tokens, and that of the restaurant corpus is 365 tokens.", "labels": [], "entities": []}, {"text": "The task has eight goals (e.g., inform, confirm).", "labels": [], "entities": []}, {"text": "Models Our main baseline EncDec is a model using the RNN Encoder-Decoder framework proposed by  and.", "labels": [], "entities": [{"text": "RNN Encoder-Decoder framework", "start_pos": 53, "end_pos": 82, "type": "DATASET", "confidence": 0.8480164011319479}]}, {"text": "The model encodes the goal and then each agenda item in sequence and then decodes the text using GRUs.", "labels": [], "entities": []}, {"text": "The encoder has two sets of parameters: one for the goal and the other for the agenda items.", "labels": [], "entities": []}, {"text": "For the dialogue task, we also compare against the SC-LSTM system from and the handcrafted rule-based generator described in that paper.", "labels": [], "entities": []}, {"text": "For the recipe task, we also compare against three other baselines.", "labels": [], "entities": []}, {"text": "The first is a basic attention model, Attention, that generates an attention encoding by comparing the hidden state ht to the agenda.", "labels": [], "entities": []}, {"text": "That encoding is added to the hidden state, and a nonlinear transformation is applied to the result before projecting into the output space.", "labels": [], "entities": []}, {"text": "We also present a nearest neighbor baseline (NN) that simply copies over an existing recipe text based on the input similarity computed using cosine similarity over the title and the ingredient list.", "labels": [], "entities": []}, {"text": "Finally, we present a hybrid approach (NN-Swap) that revises a nearest neighbor recipe using the neural checklist model.", "labels": [], "entities": []}, {"text": "The neural checklist model is forced to generate the returned recipe nearly verbatim, except that it can generate new strings to replace any extraneous ingredients.", "labels": [], "entities": []}, {"text": "Our neural checklist model is labeled Checklist.", "labels": [], "entities": [{"text": "Checklist", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.8848782777786255}]}, {"text": "We also present the Checklist+ model, which interactively re-writes a recipe to better cover the input agenda: if the generated text does not use every agenda item, embeddings corresponding to missing items are multiplied by increasing weights and anew recipe is generated.", "labels": [], "entities": []}, {"text": "This process repeats until the We thank the authors for sharing their system outputs.: Quantitative results on the recipe task.", "labels": [], "entities": []}, {"text": "The line with ot = ht has the results for the non-interpolation ablation.", "labels": [], "entities": []}, {"text": "new recipe does not contain new items.", "labels": [], "entities": []}, {"text": "We also report the performance of our checklist model without the additional weak supervision of heuristic ingredient references (-no supervision) (see Sec. 4.6).", "labels": [], "entities": []}, {"text": "we also evaluate two ablations of our checklist model on the recipe task.", "labels": [], "entities": []}, {"text": "First, we remove the linear interpolation and instead use ht as the output (see Sec. 4.2).", "labels": [], "entities": []}, {"text": "Second, we remove the previously used item reference model by changing ref -type() to a 2-way classifier between new ingredient references and all other tokens (see Sec. 4.4).", "labels": [], "entities": []}, {"text": "Metrics We include commonly used metrics like BLEU-4, and METEOR.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9979532957077026}, {"text": "METEOR", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.8905981183052063}]}, {"text": "Because neither of these metrics can measure how well the generated recipe follows the input goal and the agenda, we also define two additional metrics.", "labels": [], "entities": []}, {"text": "The first measures the percentage of the agenda items corrected used, while the second measures the number of extraneous items incorrectly introduced.", "labels": [], "entities": []}, {"text": "Both these metrics are computed based on simple string match and can miss certain referring expressions (e.g., \"meat\" to refer to \"pork\").", "labels": [], "entities": []}, {"text": "Because of the approximate nature of these automated metrics, we also report a human evaluation.", "labels": [], "entities": []}, {"text": "METEOR; this is due to a number of recipes that have very similar text but make different dishes.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.4761040210723877}]}], "tableCaptions": [{"text": " Table 1: Quantitative results on the recipe task. The line with", "labels": [], "entities": []}, {"text": " Table 2: Human evaluation results on the generated and true", "labels": [], "entities": []}, {"text": " Table 3: Quantitative evaluation of the top generations in the  hotel and restaurant domains", "labels": [], "entities": []}]}