{"title": [{"text": "The Effects of Data Size and Frequency Range on Distributional Semantic Models", "labels": [], "entities": [{"text": "Data Size", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.6724550724029541}]}], "abstractContent": [{"text": "This paper investigates the effects of data size and frequency range on distributional semantic models.", "labels": [], "entities": []}, {"text": "We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency.", "labels": [], "entities": []}, {"text": "Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted fac-torized model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional Semantic Models (DSMs) have become a staple in natural language processing.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7450158844391505}]}, {"text": "The various parameters of DSMs -e.g. size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measureshave been thoroughly studied (, and are now well understood.", "labels": [], "entities": []}, {"text": "The impact of various processing models -matrix-based models, neural networks, and hashing methods -have also enjoyed considerable attention lately, with at times conflicting conclusions ().", "labels": [], "entities": []}, {"text": "The consensus interpretation of such experiments seems to be that the choice of processing model is less important than the parameterization of the models, since the various processing models all result in more or less equivalent DSMs (provided that the parameterization is comparable).", "labels": [], "entities": []}, {"text": "One of the least researched aspects of DSMs is the effect on the various models of data size and frequency range of the target items.", "labels": [], "entities": [{"text": "DSMs", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.9641733765602112}]}, {"text": "The only previous work in this direction that we are aware of is, who report that on small data (the CHILDES corpus), simple matrix-based models outperform neural network-based ones.", "labels": [], "entities": [{"text": "CHILDES corpus", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.8957636058330536}]}, {"text": "Unfortunately, Asr et al. do not include any experiments using the same models applied to bigger data, making it difficult to compare their results with previous studies, since implementational details and parameterization will be different.", "labels": [], "entities": []}, {"text": "There is thus still a need fora consistent and fair comparison of the performance of various DSMs when applied to data of varying sizes.", "labels": [], "entities": []}, {"text": "In this paper, we seek an answer to the question: which DSM should we opt for if we only have access to limited amounts of data?", "labels": [], "entities": []}, {"text": "We are also interested in the related question: which DSM should we opt for if our target items are infrequent?", "labels": [], "entities": []}, {"text": "The latter question is particularly crucial, since one of the major assets of DSMs is their applicability to create semantic representations for ever-expanding vocabularies from text feeds, in which new words may continuously appear in the low-frequency ranges.", "labels": [], "entities": []}, {"text": "In the next section, we introduce the contending DSMs and the general experiment setup, before turning to the experiments and our interpretation of the results.", "labels": [], "entities": []}, {"text": "We conclude with some general advice.", "labels": [], "entities": []}, {"text": "to gain an understanding of the effect of data size and frequency range on the various models, we focus primarily on the differences in processing models, hence the following typology of DSMs.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 summarizes the results over the different test  settings. The most notable aspect of these results DSM  TOEFL  ESL  SL MEN  RW  1 million words", "labels": [], "entities": [{"text": "DSM", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.7606757283210754}, {"text": "TOEFL  ESL  SL MEN  RW", "start_pos": 113, "end_pos": 135, "type": "METRIC", "confidence": 0.8113600015640259}]}]}