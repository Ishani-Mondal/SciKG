{"title": [{"text": "Modelling Interaction of Sentence Pair with Coupled-LSTMs", "labels": [], "entities": []}], "abstractContent": [{"text": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks.", "labels": [], "entities": []}, {"text": "However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence.", "labels": [], "entities": []}, {"text": "In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs.", "labels": [], "entities": []}, {"text": "Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences.", "labels": [], "entities": []}, {"text": "We then aggregate these interactions and use a dynamic pooling to select the most informative features.", "labels": [], "entities": []}, {"text": "Experiments on two very large datasets demonstrate the efficacy of our proposed architectures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification, question answering and machine translation) and soon.", "labels": [], "entities": [{"text": "text classification", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7628762423992157}, {"text": "question answering", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.8255178928375244}, {"text": "machine translation", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.691491574048996}]}, {"text": "Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching.", "labels": [], "entities": [{"text": "text semantic matching", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7220572928587595}]}, {"text": "Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses (.", "labels": [], "entities": [{"text": "text semantic matching", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.7585127353668213}]}, {"text": "According to the phases of interaction between two sentences, previous models can be classified into three categories.", "labels": [], "entities": []}, {"text": "Weak interaction Models Some early works focus on sentence level interactions, such as ARC-I(), CNTN(  and soon.", "labels": [], "entities": []}, {"text": "These models first encode two sequences with some basic (Neural Bag-of-words, BOW) or advanced (RNN, CNN) components of neural networks separately, and then compute the matching score based on the distributed vectors of two sentences.", "labels": [], "entities": [{"text": "BOW", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9497274160385132}]}, {"text": "In this paradigm, two sentences have no interaction until arriving final phase.", "labels": [], "entities": []}, {"text": "Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN (  and MultiPerspective CNN (.", "labels": [], "entities": []}, {"text": "Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN ( , Attention LSTM().", "labels": [], "entities": []}, {"text": "These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level.", "labels": [], "entities": []}, {"text": "Strong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions, such as ARC-II (), MV-LSTM ( and DF-LSTMs().", "labels": [], "entities": [{"text": "ARC-II", "start_pos": 155, "end_pos": 161, "type": "DATASET", "confidence": 0.7333390712738037}]}, {"text": "These models can easily capture the difference between semantic capacity of two sentences.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew deep neural network architecture to model the strong interactions of two sentences.", "labels": [], "entities": []}, {"text": "Different with modelling two sentences with separated LSTMs, we utilize two interdependent LSTMs, called coupled-LSTMs, to fully affect each other at different time steps.", "labels": [], "entities": []}, {"text": "The output of coupled-LSTMs at each step depends on both sentences.", "labels": [], "entities": []}, {"text": "Specifically, we propose two interdependent ways for the coupled-LSTMs: loosely coupled model (LC-LSTMs) and tightly coupled model (TCLSTMs).", "labels": [], "entities": []}, {"text": "Similar to bidirectional LSTM for single sentence), there are four directions can be used in coupled-LSTMs.", "labels": [], "entities": []}, {"text": "To utilize all the information of four directions of coupled-LSTMs, we aggregate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals.", "labels": [], "entities": []}, {"text": "Finally, we feed them into a fully connected layer, followed by an output layer to compute the matching score.", "labels": [], "entities": []}, {"text": "The contributions of this paper can be summarized as follows.", "labels": [], "entities": []}, {"text": "1. Different with the architectures of using similarity matrix, our proposed architecture directly model the strong interactions of two sentences with coupled-LSTMs, which can capture the useful local semantic relevances of two sentences.", "labels": [], "entities": []}, {"text": "Our architecture can also capture the multiple granular interactions by several stacked coupled-LSTMs layers.", "labels": [], "entities": []}, {"text": "2. Compared to previous works on text matching, we perform extensive empirical studies on two very large datasets.", "labels": [], "entities": [{"text": "text matching", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.8502210676670074}]}, {"text": "The massive scale of the datasets allows us to train a very deep neural network and present an elaborate qualitative analysis of our models, which gives an intuitive understanding how our model worked.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we investigate the empirical performances of our proposed model on two different text matching tasks: classification task (recognizing textual entailment) and ranking task (matching of question and answer).", "labels": [], "entities": [{"text": "text matching", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.7040872573852539}]}, {"text": "Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences.", "labels": [], "entities": [{"text": "Recognizing textual entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.912240207195282}]}, {"text": "We use the Stanford Natural Language Inference Corpus (SNLI).", "labels": [], "entities": [{"text": "Stanford Natural Language Inference Corpus (SNLI)", "start_pos": 11, "end_pos": 60, "type": "DATASET", "confidence": 0.8280003443360329}]}, {"text": "This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators.", "labels": [], "entities": []}, {"text": "SNLI is two orders of magnitude larger than all other existing RTE corpora.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7668592929840088}]}, {"text": "Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.7150737643241882}]}, {"text": "Our proposed two C-LSTMs models with four stacked blocks outperform all the competitor models, which indicates that our thinner and deeper network does work effectively.", "labels": [], "entities": []}, {"text": "Besides, we can see both LC-LSTMs and TCLSTMs benefit from multi-directional layer, while the latter obtains more gains than the former.", "labels": [], "entities": []}, {"text": "We attribute this discrepancy between two models to their different mechanisms of controlling the information flow from depth dimension.", "labels": [], "entities": []}, {"text": "Matching question answering (MQA) is atypical task for semantic matching.", "labels": [], "entities": [{"text": "Matching question answering (MQA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8489700456460317}, {"text": "semantic matching", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8173494637012482}]}, {"text": "Given a question, we need select a correct answer from some candidate answers.", "labels": [], "entities": []}, {"text": "In this paper, we use the dataset collected from Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.8285225331783295}]}, {"text": "Answers with the getByCategory function  provided in Yahoo!", "labels": [], "entities": []}, {"text": "Answers API, which produces 963, 072 questions and corresponding best answers.", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7390184998512268}]}, {"text": "We then select the pairs in which the length of questions and answers are both in the interval, thus obtaining 220, 000 question answer pairs to form the positive pairs.", "labels": [], "entities": []}, {"text": "For negative pairs, we first use each question's best answer as a query to retrieval top 1, 000 results from the whole answer set with Lucene, where 4 or 9 answers will be selected randomly to construct the negative pairs.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 135, "end_pos": 141, "type": "DATASET", "confidence": 0.9219375848770142}]}, {"text": "The whole dataset is divided into training, validation and testing data with proportion 20 : 1 : 1.", "labels": [], "entities": []}, {"text": "Moreover, we give two test settings: selecting the best answer from 5 and 10 candidates respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hyper-parameters for our model on two  tasks.", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9535257816314697}]}, {"text": " Table 2: Results on SNLI corpus.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.805639922618866}]}, {"text": " Table 3: Multiple interpretable neurons and the word-pairs/phrase-pairs captured by these neurons.", "labels": [], "entities": []}, {"text": " Table 4: Results on Yahoo question-answer pairs  dataset.", "labels": [], "entities": [{"text": "Yahoo question-answer pairs  dataset", "start_pos": 21, "end_pos": 57, "type": "DATASET", "confidence": 0.9146701842546463}]}]}