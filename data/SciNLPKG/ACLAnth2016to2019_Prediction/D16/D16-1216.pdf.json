{"title": [{"text": "Interpreting Neural Networks to Improve Politeness Comprehension", "labels": [], "entities": [{"text": "Improve Politeness Comprehension", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7998828093210856}]}], "abstractContent": [{"text": "We present an interpretable neural network approach to predicting and understanding politeness in natural language requests.", "labels": [], "entities": [{"text": "predicting and understanding politeness in natural language requests", "start_pos": 55, "end_pos": 123, "type": "TASK", "confidence": 0.808018684387207}]}, {"text": "Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work.", "labels": [], "entities": []}, {"text": "More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning.", "labels": [], "entities": [{"text": "politeness prediction", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.9173407852649689}]}, {"text": "For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories.", "labels": [], "entities": []}, {"text": "Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neu-ral networks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.998769223690033}]}], "introductionContent": [{"text": "Politeness theories include key components such as modality, indirection, deference, and impersonalization.", "labels": [], "entities": []}, {"text": "Positive politeness strategies focus on making the hearer feelgood through offers, promises, and jokes.", "labels": [], "entities": []}, {"text": "Negative politeness examples include favor seeking, orders, and requests.", "labels": [], "entities": [{"text": "favor seeking", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.7215122133493423}]}, {"text": "Differentiating among politeness types is a highly nontrivial task, because it depends on factors such as a context, relative power, and culture.", "labels": [], "entities": []}, {"text": "Danescu-Niculescu- proposed a useful computational framework for predicting politeness in natural language requests by designing various lexical and syntactic features about key politeness theories, e.g., first or second person start vs. plural.", "labels": [], "entities": [{"text": "predicting politeness in natural language requests", "start_pos": 65, "end_pos": 115, "type": "TASK", "confidence": 0.7834668656190237}]}, {"text": "However, manually identifying such politeness features is very challenging, because there exist several complex theories and politeness in natural language is often realized via subtle markers and non-literal cues.", "labels": [], "entities": []}, {"text": "Neural networks have been achieving high performance in sentiment analysis tasks, via their ability to automatically learn short and long range spatial relations.", "labels": [], "entities": [{"text": "sentiment analysis tasks", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9601521492004395}]}, {"text": "However, it is hard to interpret and explain what they have learned.", "labels": [], "entities": []}, {"text": "In this paper, we first propose to address politeness prediction via simple CNNs working directly on the raw text.", "labels": [], "entities": [{"text": "politeness prediction", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.906361997127533}]}, {"text": "This helps us avoid the need for any complex, manuallydefined linguistic features, while still performing better than such featurized systems.", "labels": [], "entities": []}, {"text": "More importantly, we next present an intuitive interpretation of what these successful neural networks are learning, using the challenging politeness task as a testbed.", "labels": [], "entities": []}, {"text": "To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (, and have also been recently adopted in NLP for recurrent neural networks ().", "labels": [], "entities": []}, {"text": "The neuron activation clustering method not only rediscovers and extends several manually defined features from politeness theories, but also uncovers multiple novel strategies, whose importance we measure quantitatively.", "labels": [], "entities": [{"text": "neuron activation clustering", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7357410589853922}]}, {"text": "The first derivative saliency technique allows us to identify the impact of each phrase on the final politeness prediction score via heatmaps, revealing useful politeness markers and cues.", "labels": [], "entities": [{"text": "politeness prediction", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.7487506866455078}]}, {"text": "Finally, we also plot lexical embeddings before and after training, showing how specific politeness markers move and cluster based on their polarity.", "labels": [], "entities": []}, {"text": "Such visualization strategies should also be useful for understanding similar state-of-the-art neural network models on various other NLP tasks.", "labels": [], "entities": []}, {"text": "Importantly, our activation clusters reveal two novel politeness strategies, namely indefinite pronouns and punctuation.", "labels": [], "entities": []}, {"text": "Both strategies display high politeness and top-quartile scores (as defined by Danescu-Niculescu-).", "labels": [], "entities": []}, {"text": "Also, when added back as new features to the original featurized system, they improve its performance and reduce the accuracy gap between the featurized system and the neural model, thus providing a clear, quantitative interpretation of the success of these neural networks in automatically learning useful features.", "labels": [], "entities": [{"text": "accuracy gap", "start_pos": 117, "end_pos": 129, "type": "METRIC", "confidence": 0.9785207509994507}]}], "datasetContent": [{"text": "We used the two datasets released by DanescuNiculescu-: Wikipedia (Wiki) and Stack Exchange (SE), containing community requests with politeness labels.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9637914896011353}]}, {"text": "Their 'feature development' was done on the Wiki dataset, and SE was used as the 'feature transfer' domain.", "labels": [], "entities": [{"text": "Wiki dataset", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.9750989675521851}, {"text": "SE", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.963043212890625}]}, {"text": "We use a simpler train-validation-test split based setup for these datasets instead of the original leave-one-out crossvalidation setup, which makes training extremely slow for any neural network or sizable classifier.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy Results on Wikipedia and Stack Exchange.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9843065738677979}, {"text": "Wikipedia", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.9261985421180725}]}, {"text": " Table 2: Extending Table 3 of Danescu-Niculescu-Mizil et al. (2013) with our novelly discovered politeness strategies.", "labels": [], "entities": []}]}