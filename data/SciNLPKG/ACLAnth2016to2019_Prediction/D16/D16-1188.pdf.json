{"title": [{"text": "Regularizing Text Categorization with Clusters of Words", "labels": [], "entities": [{"text": "Regularizing Text Categorization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9137622912724813}]}], "abstractContent": [{"text": "Regularization is a critical step in supervised learning to not only address overfitting, but also to take into account any prior knowledge we may have on the features and their dependence.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9285820722579956}]}, {"text": "In this paper, we explore state-of-the-art structured regularizers and we propose novel ones based on clusters of words from LSI topics, word2vec embeddings and graph-of-words document representation.", "labels": [], "entities": []}, {"text": "We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy.", "labels": [], "entities": [{"text": "text classification", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.8218514919281006}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9008588790893555}]}, {"text": "Code and data are available online 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Harnessing the full potential in text data has always been a key task for the NLP and ML communities.", "labels": [], "entities": []}, {"text": "The properties hidden under the inherent high dimensionality of text are of major importance in tasks such as text categorization and opinion mining.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7693930566310883}, {"text": "opinion mining", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.8583027720451355}]}, {"text": "Although simple models like bag-of-words manage to perform well, the problem of overfitting still remains.", "labels": [], "entities": []}, {"text": "Regularization as proven in is of paramount importance in Natural Language Processing and more specifically language modeling, structured prediction, and classification.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9221342206001282}, {"text": "language modeling", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7092629671096802}, {"text": "structured prediction", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.7408640682697296}]}, {"text": "In this paper we build upon the work of who introduce prior knowledge of data as a regularization term.", "labels": [], "entities": []}, {"text": "One of the most popular structured regularizers, the group lasso (), was proposed to avoid large L2 norms for groups of weights.", "labels": [], "entities": []}, {"text": "In this paper, we propose novel linguistic structured regularizers that capitalize on the clusters learned from texts using the word2vec and graph-ofwords document representation, which can be seen as group lasso variants.", "labels": [], "entities": []}, {"text": "The extensive experiments we conducted demonstrate these regularizers can boost standard bag-of-words models on most cases tested in the task of text categorization, by imposing additional unused information as bias.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7756848633289337}]}], "datasetContent": [{"text": "We evaluated our structured regularizers on several well-known datasets for the text categorization task.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8067731261253357}]}, {"text": "summarizes statistics about the ten datasets we used in our experiments.", "labels": [], "entities": []}, {"text": "As features we use unigram frequency concatenated with an additional unregularized bias term.", "labels": [], "entities": []}, {"text": "We reproduce standard regularizers like lasso, ridge, elastic and state-of-the-art structured regularizers like sentence, LDA as baselines and compare them with our proposed methods.", "labels": [], "entities": []}, {"text": "For LSI, LDA and word2vec we use the gensim package) in Python.", "labels": [], "entities": []}, {"text": "For the learning part we used Matlab and specifically code by.", "labels": [], "entities": [{"text": "Matlab", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9177591800689697}]}, {"text": "We split the training set in a stratified manner to retain the percentage of classes.", "labels": [], "entities": []}, {"text": "We use 80% of the data for training and 20% for validation.", "labels": [], "entities": []}, {"text": "All the hyperparameters are tuned on the development dataset, using accuracy as the evaluation criterion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9993942975997925}]}, {"text": "For lasso and ridge regularization, we choose \u03bb from {10 \u22122 , 10 \u22121 , 1, 10, 10 2 }.", "labels": [], "entities": []}, {"text": "For elastic net, we perform grid search on the same set of values as ridge and lasso experiments for \u03bb rid and \u03bb las . For the LDA, LSI, sentence, graph-of-words (GoW), word2vec regularizers, we perform grid search on the same set of values as ridge and lasso experiments for the \u03c1, \u03bb glas , \u03bb las parameters.", "labels": [], "entities": []}, {"text": "In the case we get the same accuracy on the development data, the model with the highest sparsity is selected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9991193413734436}]}, {"text": "For    LDA we set the number of topics to 1000 and we keep the 10 most probable words of each topic as a group.", "labels": [], "entities": [{"text": "LDA", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.6995635032653809}]}, {"text": "For LSI we keep 1000 latent dimensions and we select the 10 most significant words per topic.", "labels": [], "entities": []}, {"text": "For the clustering process on word2vec we ran Minibatch-Kmeans for max 2000 clusters.", "labels": [], "entities": [{"text": "Minibatch-Kmeans", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9651403427124023}]}, {"text": "For each word belonging to a cluster, we also keep the top 5 or 10 nearest words so that we introduce overlapping groups.", "labels": [], "entities": []}, {"text": "The intuition behind this is that words can be part of multiple \"concepts\" or topics, thus they can belong to many clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Descriptive statistics of the datasets", "labels": [], "entities": []}, {"text": " Table 2: Accuracy results on the test sets. Bold font marks the best performance for a dataset. * indicates statistical significance", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987381100654602}]}, {"text": " Table 3: Fraction (in %) of non-zero feature weights in each model for each dataset: the smaller, the more compact the model.", "labels": [], "entities": [{"text": "Fraction", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9808031320571899}]}, {"text": " Table 4: Number of groups.", "labels": [], "entities": []}, {"text": " Table 5: Time (in seconds) for learning with best hyperparameters.", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9699908494949341}]}]}