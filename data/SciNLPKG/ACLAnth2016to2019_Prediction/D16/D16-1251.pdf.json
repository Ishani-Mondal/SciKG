{"title": [{"text": "Measuring the behavioral impact of machine translation quality improvements with A/B testing", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7723537683486938}]}], "abstractContent": [{"text": "In this paper we discuss a process for quantifying the behavioral impact of a domain-customized machine translation system deployed on a large-scale e-commerce platform.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7215794622898102}]}, {"text": "We discuss several machine translation systems that we trained using aligned text from product listing descriptions written in multiple languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7584913372993469}]}, {"text": "We document the quality improvements of these systems as measured through automated quality measures and crowdsourced human quality assessments.", "labels": [], "entities": []}, {"text": "We then measure the effect of these quality improvements on user behavior using an automated A/B testing framework.", "labels": [], "entities": []}, {"text": "Through testing we observed an increase in key e-commerce metrics, including a significant increase in purchases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality evaluation is an essential task when training a machine translation (MT) system.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8405618488788604}]}, {"text": "While automatic evaluation methods like BLEU () can be useful for estimating translation quality, a higher score is no guarantee of quality improvement).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9963769316673279}, {"text": "estimating translation", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.6646993458271027}]}, {"text": "Previous studies (e.g.) have compared human evaluations of MT to metrics like BLEU and found close correspondence between the two.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.986566960811615}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9952955842018127}]}, {"text": "argued that relatively small differences in BLEU can indicate significant MT quality differences and suggested that human evaluation, the traditional alternative to automated metrics like BLEU, is therefore unnecessarily time-consuming and costly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9959587454795837}, {"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9821711778640747}, {"text": "BLEU", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.8879720568656921}]}, {"text": "explored the use of crowdsourcing platforms for evaluating MT quality, with good results.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9850703477859497}]}, {"text": "However, we are not aware of any research that investigates the effect of improved MT on human behavior.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9902821779251099}]}, {"text": "Ina commercial application, like an e-commerce platform, it is desirable to have a high degree of confidence in the material effect of MT quality differences: any MT system change should positively impact user experiences.", "labels": [], "entities": [{"text": "MT", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.9827517867088318}]}, {"text": "Etsy is an online marketplace for handmade and vintage items, with over 40 million active listings and a community of buyers and sellers located around the world.", "labels": [], "entities": []}, {"text": "Visitors can use MT to translate the text of product descriptions, product reviews, and private messages, making it possible for members to communicate effectively with one another, even when they speak different languages.", "labels": [], "entities": []}, {"text": "These multilingual interactions facilitated by MT, such as reading nonnative listing descriptions or conversing with a foreign seller, are integral to the user experience.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9775066375732422}]}, {"text": "However, due to the unique nature of the products available in the marketplace, a generic third party MT system 1 often falls short when translating usergenerated content.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9528947472572327}]}, {"text": "One challenging lexical item is \"clutch.\"", "labels": [], "entities": []}, {"text": "A generic engine, trained on commonly available parallel text, translates clutch as an \"automotive clutch.\"", "labels": [], "entities": []}, {"text": "In this marketplace, however, clutch almost always means \"purse.\"", "labels": [], "entities": []}, {"text": "A mistake like this is problematic: a user who sees this incorrect machine translation may lose confidence in that listing and possibly in the marketplace as a whole.", "labels": [], "entities": []}, {"text": "To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service 2 to train a custom MT engine for English to French translations.", "labels": [], "entities": []}, {"text": "To validate that the retrained MT systems were materially improved, we used a two step validation process, first using crowd-sourced evaluations with Amazon's Mechanical Turk, and secondly using A/B testing, away of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9458678960800171}]}], "datasetContent": [{"text": "The crowdsourced evaluation of the three systems favored System 3.", "labels": [], "entities": []}, {"text": "provides a summary of the results.", "labels": [], "entities": []}, {"text": "Neither System 1 nor System 2 showed a significant difference between selection of translations provided by the trained or untrained system: chi-squared tests did not detect a significant difference between number of responses favoring the trained system and number of responses favoring the generic system (p = 0.1948 and p = 0.26, respectively, for the two systems).", "labels": [], "entities": []}, {"text": "However, a chi-squared test indicated a significant preference for System 3, which was chosen 35% more often than the generic system (p = 0.0048).", "labels": [], "entities": []}, {"text": "Based on the crowd-sourced results, we proceeded to A/B test System 3 against the generic translation system baseline.", "labels": [], "entities": [{"text": "A/B", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.806350310643514}]}, {"text": "The lack of improvements for System 1 and System 2 detected using the crowd-sourcing methods was somewhat surprising, given the large BLEU score improvements observed for all three systems.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9756698608398438}]}, {"text": "We believe this lends further support to critiques of BLEU as a standalone machine translation quality metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9282439947128296}]}, {"text": "In this case, it is possible that Systems 1 and 2 achieved high BLEU improvements due to over-fitting the training data from which the test set was drawn.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.99951171875}]}, {"text": "We might speculate that this is due to the presence of low-quality translations from limited-bilingual sellers, or the presence of MT generated by a different online tool in some sellers' translations.", "labels": [], "entities": []}, {"text": "By tuning the system using a high-quality, professionallytranslated test set, we reduced overall BLEU but increased quality as judged by bilingual evaluators.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9992033839225769}, {"text": "quality", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9721368551254272}]}, {"text": "We used the automated BLEU calculation provided by the third-party translation service to obtain scores for each of the three translation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9967116117477417}]}, {"text": "All three systems had significant BLEU improvements after retraining, as shown in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9994230270385742}]}, {"text": "We be-  lieve System 3 has a lower BLEU score than the others because it was tuned on a different data set: the professionally-translated, in-domain sentences from product listing descriptions.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9864225387573242}]}, {"text": "This made the system's output less like the automatically-selected test set than the others, but closer, presumably, to the highquality, low-noise tuning translations sourced from professional translators.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sets used for three MT system retrainings. *The", "labels": [], "entities": [{"text": "MT system retrainings", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8987952272097269}]}, {"text": " Table 2: BLEU score improvements for three translation sys-", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9687372744083405}]}, {"text": " Table 3: Results from crowdsourced evaluations of three trans-", "labels": [], "entities": []}, {"text": " Table 4: The trained translation system's (System 3) improve-", "labels": [], "entities": [{"text": "trained translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.5179559588432312}]}, {"text": " Table 4: a 3.37% increase in pages per  visit (p = 0.00153 95% CI [1.29, 5.46]), an 8.72%  increase in purchase rate (p = 0.00513 95% CI  [2.61, 14.82]), and a 2.92% increase in add-to-cart  (p = 0.04689 95% CI [0.04, 5.8]).", "labels": [], "entities": []}]}