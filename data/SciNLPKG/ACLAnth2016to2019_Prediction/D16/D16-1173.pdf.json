{"title": [{"text": "Deep Neural Networks with Massive Learned Knowledge", "labels": [], "entities": []}], "abstractContent": [{"text": "Regulating deep neural networks (DNNs) with human structured knowledge has shown to be of great benefit for improved accuracy and in-terpretability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9982509016990662}]}, {"text": "We develop a general framework that enables learning knowledge and its confidence jointly with the DNNs, so that the vast amount of fuzzy knowledge can be incorporated and automatically optimized with little manual efforts.", "labels": [], "entities": []}, {"text": "We apply the framework to sentence sentiment analysis, augmenting a DNN with massive linguistic constraints on discourse and polarity structures.", "labels": [], "entities": [{"text": "sentence sentiment analysis", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.8580057223637899}]}, {"text": "Our model substantially enhances the performance using less training data, and shows improved inter-pretability.", "labels": [], "entities": []}, {"text": "The principled framework can also be applied to posterior regularization for regulating other statistical models.", "labels": [], "entities": [{"text": "posterior regularization", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6705909669399261}]}], "introductionContent": [{"text": "Deep neural networks (DNNs) have achieved remarkable success in a large variety of application domains (;).", "labels": [], "entities": []}, {"text": "However, the powerful end-to-end learning comes with limitations, including the requirement on massive amount of labeled data, uninterpretability of prediction results, and difficulty of incorporating human intentions and domain knowledge.", "labels": [], "entities": []}, {"text": "To alleviate these drawbacks, recent work has focused on training DNNs with extra domain-specific features), combining oracle similarity constraints (, modeling output correlations (, and others.", "labels": [], "entities": []}, {"text": "Recently, proposed a general distillation framework that transfers knowledge expressed as first-order logic (FOL) rules into neural networks, where FOL constraints are integrated via posterior regularization (.", "labels": [], "entities": []}, {"text": "Despite the intuitiveness of FOL rules and the impressive performance in various tasks, the approach, as with the previous posterior constraint methods (), has been limited to simple a priori fixed constraints with manually selected weights, lacking the ability of inducing and adapting abstract knowledge from data.", "labels": [], "entities": []}, {"text": "This issue is further exacerbated in the context of regulating DNNs that map raw data directly into the label space, leaving a huge semantic gap in between, and making it unfeasible to express rich human knowledge built on the intermediate abstract concepts.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a generalized framework which enables a learning procedure for knowledge representations and their weights jointly with the regulated DNN models.", "labels": [], "entities": []}, {"text": "This greatly extends the applicability to massive structures in diverse forms, such as structured models and soft logic rules, facilitating practitioners to incorporate rich domain expertise and fuzzy constraints.", "labels": [], "entities": []}, {"text": "Specifically, we propose a mutual distillation method that iteratively transfers information between DNN and structured knowledge, resulting in effective integration of the representation learning capacity of DNN and the generalization power of structured knowledge.", "labels": [], "entities": []}, {"text": "Our method does not require additional supervision beyond raw data-labels for knowledge learning.", "labels": [], "entities": []}, {"text": "We present an instantiation of our method in the task of sentence sentiment analysis.", "labels": [], "entities": [{"text": "sentence sentiment analysis", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.846317708492279}]}, {"text": "We aug-ment abase convolutional network with linguistic knowledge that encourages coherent sentiment transitions across the clauses in terms of discourse relations.", "labels": [], "entities": []}, {"text": "All uncertain modules, such as clause relation and polarity identification, are automatically learned from data, freeing practitioners from exhaustive specification.", "labels": [], "entities": [{"text": "polarity identification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7031742334365845}]}, {"text": "We further improve the model by integrating thousands of soft word polarity and negation rules, with their confidence directly induced from the data.", "labels": [], "entities": []}, {"text": "Trained with only sentence level supervisions, our model substantially outperforms plain neural networks learned from both sentence and clause labels.", "labels": [], "entities": []}, {"text": "Our method also shows enhanced generalization on limited data size, and improved interpretability of predictions.", "labels": [], "entities": []}, {"text": "Our work enjoys general versatility on diverse types of structured knowledge and neural architectures.", "labels": [], "entities": []}, {"text": "The principled knowledge and weight learning approach can also be applied to the posterior constraint frameworks () for regulating other statistical models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on the widely-used sentiment classification benchmarks.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.9059668481349945}]}, {"text": "Our knowledge 87.0 6 CNN+But-q ( 87.1 +phrases 7 CNN 87.2 8 Tree-LSTM ( 88.0 9 MC-CNN 88.1 10 CNN+But-q ( 89.2 11 MVCNN (Yin and Schutze, 2015) 89.4: Classification performance on SST2.", "labels": [], "entities": [{"text": "MVCNN (Yin and Schutze, 2015) 89.4", "start_pos": 114, "end_pos": 148, "type": "DATASET", "confidence": 0.8339785006311204}]}, {"text": "The top and second blocks use only sentence-level annotations for training, while the bottom block uses both sentence-and phrases-level annotations.", "labels": [], "entities": []}, {"text": "We report the accuracy of both the regularized teacher model q and the student model p after distillation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9997245669364929}]}, {"text": "enriched model significantly outperforms plain neural networks.", "labels": [], "entities": []}, {"text": "We obtain even higher improvements with limited data sizes.", "labels": [], "entities": []}, {"text": "Comparison with extensive other potential knowledge learning methods shows the effectiveness of our framework.", "labels": [], "entities": []}, {"text": "Our model also shows improved interpretability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification performance on SST2. The top and", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8593857884407043}, {"text": "SST2", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.5080370903015137}]}, {"text": " Table 2: Classification performance on the CR dataset. We", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9140386581420898}, {"text": "CR dataset", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9272173047065735}]}, {"text": " Table 4: The top 5 positive (left) and negative (right) words", "labels": [], "entities": []}]}