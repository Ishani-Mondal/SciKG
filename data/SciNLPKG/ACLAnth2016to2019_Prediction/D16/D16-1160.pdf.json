{"title": [{"text": "Exploiting Source-side Monolingual Data in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6640589237213135}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become anew paradigm.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8016997575759888}]}, {"text": "Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT.", "labels": [], "entities": []}, {"text": "However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of NMT, especially when the parallel corpus is far from sufficient.", "labels": [], "entities": []}, {"text": "In this paper, we propose two approaches to make full use of the source-side monolingual data in NMT.", "labels": [], "entities": []}, {"text": "The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.9387162625789642}]}, {"text": "The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously.", "labels": [], "entities": []}, {"text": "The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) following the encoder-decoder architecture proposed by) has become the novel paradigm and obtained state-ofthe-art translation quality for several language pairs, such as English-to-French and English-to-German (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7484058678150177}]}, {"text": "This endto-end NMT typically consists of two recurrent neural networks.", "labels": [], "entities": []}, {"text": "The encoder network maps the source sentence of variable length into the context vector representation; and the decoder network generates the target translation word byword starting from the context vector.", "labels": [], "entities": []}, {"text": "Currently, most NMT methods utilize only the sentence aligned parallel corpus for model training, which limits the capacity of the model.", "labels": [], "entities": []}, {"text": "Recently, inspired by the successful application of target monolingual data in conventional statistical machine translation (SMT) (), and attempt to enhance the decoder network model of NMT by incorporating the targetside monolingual data so as to boost the translation fluency.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.7785877287387848}]}, {"text": "They report promising improvements by using the target-side monolingual data.", "labels": [], "entities": []}, {"text": "In contrast, the source-side monolingual data is not fully explored.", "labels": [], "entities": []}, {"text": "adopt a simple autoencoder or skip-thought method ( to exploit the source-side monolingual data, but no significant BLEU gains are reported.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9983483552932739}]}, {"text": "Note that, in parallel to our efforts, have explored the usage of both source and target monolingual data using a similar semi-supervised reconstruction method, in which two NMTs are employed.", "labels": [], "entities": []}, {"text": "One translates the source-side monolingual data into target translations, and the other reconstructs the source-side monolingual data from the target translations.", "labels": [], "entities": []}, {"text": "In this work, we investigate the usage of the source-side large-scale monolingual data in NMT and aim at greatly enhancing its encoder network so that we can obtain high quality context vector representations.", "labels": [], "entities": []}, {"text": "To achieve this goal, we propose two approaches.", "labels": [], "entities": []}, {"text": "Inspired by handling source-side monolingual corpus in SMT and () exploiting target-side monolingual data in NMT, the first approach adopts the self-learning algorithm to generate adequate synthetic parallel data for NMT training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.963200569152832}, {"text": "NMT training", "start_pos": 217, "end_pos": 229, "type": "TASK", "confidence": 0.9094758033752441}]}, {"text": "In this method, we first build the baseline machine translation system with the available aligned sentence pairs, and then obtain more synthetic parallel data by translating the source-side monolingual sentences with the baseline system.", "labels": [], "entities": []}, {"text": "The proposed second approach applies the multitask learning framework to predict the target translation and the reordered source-side sentences at the same time.", "labels": [], "entities": []}, {"text": "The main idea behind is that we build two NMTs: one is trained on the aligned sentence pairs to predict the target sentence from the source sentence, while the other is trained on the source-side monolingual corpus to predict the reorderd source sentence from original source sentences . It should be noted that the two NMTs share the same encoder network so that they can help each other to strengthen the encoder model.", "labels": [], "entities": []}, {"text": "In this paper, we make the following contributions: \u2022 To fully investigate the source-side monolingual data in NMT, we propose and compare two methods.", "labels": [], "entities": []}, {"text": "One attempts to enhance the encoder network of NMT by producing rich synthetic parallel corpus using a self-learning algorithm, and the other tries to perform machine translation and source sentence reordering simultaneously with a multi-task learning architecture.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7236543744802475}]}, {"text": "\u2022 The extensive experiments on Chinese-toEnglish translation show that our proposed methods significantly outperform the strong NMT baseline augmented with the attention mechanism.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6143742203712463}]}, {"text": "We also find that the usage of the source-side monolingual data in NMT is more effective than that in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9794892072677612}]}, {"text": "Furthermore, we find that more monolingual data does not always improve the translation quality and only relevant monolingual data helps.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the data set used in our experiments, data preprocessing, the training and evaluation details, and all the translation methods we compare in experiments.", "labels": [], "entities": []}, {"text": "We perform two tasks on Chinese-to-English translation: one for small data set and the other for large-scale data set.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.6007236838340759}]}, {"text": "Our small training data includes 0.63M sentence pairs (after data cleaning) extracted from LDC corpora . The large-scale data set contains about 2.1M sentence pairs including the small training data.", "labels": [], "entities": []}, {"text": "For validation, we choose NIST 2003 (MT03) dataset.", "labels": [], "entities": [{"text": "NIST 2003 (MT03) dataset", "start_pos": 26, "end_pos": 50, "type": "DATASET", "confidence": 0.9418291548887888}]}, {"text": "For testing, we use NIST 2004 (MT04), NIST 2005 (MT05) and NIST 2006 (MT06) datasets.", "labels": [], "entities": [{"text": "NIST 2004 (MT04)", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.8811129927635193}, {"text": "NIST 2005 (MT05)", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8541404247283936}, {"text": "NIST 2006 (MT06) datasets", "start_pos": 59, "end_pos": 84, "type": "DATASET", "confidence": 0.9140187601248423}]}, {"text": "As for the source-side monolingual data, we collect about 20M Chinese sentences from LDC and we retain the sentences in which more than 50% words should appear in the sourceside portion of the bilingual training data, resulting in 6.5M monolingual sentences for small training data set (12M for large-scale training data set) ordered by the word hit rate.", "labels": [], "entities": []}, {"text": "Each NMT model is trained on GPU K40 using stochastic gradient decent algorithm AdaGrad ().", "labels": [], "entities": [{"text": "GPU K40", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9361348450183868}]}, {"text": "We use mini batch size of 32.", "labels": [], "entities": []}, {"text": "The word embedding dimension of source and target language is 500 and the size of hidden layer is set to 1024.", "labels": [], "entities": []}, {"text": "The training time for each model ranges from 5 days to 10 days for small training data set and ranges from 8 days to 15 days for large training data   set 8 . We use case-insensitive 4-gram BLEU score as the evaluation metric ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 190, "end_pos": 200, "type": "METRIC", "confidence": 0.9683156609535217}]}], "tableCaptions": [{"text": " Table 2: Translation results (BLEU score) for different translation methods in large-scale training data.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9423595666885376}, {"text": "BLEU score)", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.9705126484235128}]}]}