{"title": [{"text": "Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models", "labels": [], "entities": [{"text": "Adapting Grammatical Error", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8976794481277466}]}], "abstractContent": [{"text": "An important aspect for the task of grammatical error correction (GEC) that has not yet been adequately explored is adaptation based on the native language (L1) of writers, despite the marked influences of L1 on second language (L2) writing.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 36, "end_pos": 70, "type": "TASK", "confidence": 0.7668544153372446}]}, {"text": "In this paper, we adapt a neural network joint model (NNJM) using L1-specific learner text and integrate it into a statistical machine translation (SMT) based GEC system.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 115, "end_pos": 152, "type": "TASK", "confidence": 0.7628327210744222}]}, {"text": "Specifically, we train an NNJM on general learner text (not L1-specific) and subsequently train on L1-specific data using a Kullback-Leibler divergence regularized objective function in order to preserve generalization of the model.", "labels": [], "entities": []}, {"text": "We incorporate this adapted NNJM as a feature in an SMT-based English GEC system and show that adaptation achieves significant F 0.5 score gains on English texts written by L1 Chinese, Russian, and Spanish writers.", "labels": [], "entities": [{"text": "SMT-based English GEC", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.896164615948995}, {"text": "F 0.5 score", "start_pos": 127, "end_pos": 138, "type": "METRIC", "confidence": 0.9821127653121948}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) deals with the automatic correction of errors (spelling, grammar, and collocation errors), particularly in non-native written text.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8235943963130316}, {"text": "automatic correction of errors (spelling, grammar, and collocation errors)", "start_pos": 50, "end_pos": 124, "type": "TASK", "confidence": 0.7317910331946152}]}, {"text": "The native language (L1) background of the writer has a noticeable influence on the errors made in second language (L2) writing, and considering this factor can potentially improve the performance of GEC systems.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence written by a Finnish writer): \"When they had escaped in the police car they sat under the tree.\"", "labels": [], "entities": []}, {"text": "The preposition in appears to be grammatically correct.", "labels": [], "entities": []}, {"text": "However, in the given context, the preposition 'from' is the correct choice in place of the preposition 'in'.", "labels": [], "entities": []}, {"text": "Finnish learners of English tend to overgeneralize the use of the preposition 'in'.", "labels": [], "entities": []}, {"text": "Knowledge of L1 makes the correction more probable whenever the preposition in appears in texts written by Finnish writers.", "labels": [], "entities": []}, {"text": "Similarly, Chinese learners of English tend to make frequent verb tense and verb form errors, since Chinese lacks verb inflection.", "labels": [], "entities": []}, {"text": "The cross-linguistic influence of L1 on L2 writing is a highly complex phenomenon, and the errors made by learners cannot be directly attributed to the similarities or differences between the two languages.", "labels": [], "entities": []}, {"text": "As Ortega (2009) points out, learners seem to operate on two complementary principles: \"what works in L1 may work in L2 because human languages are fundamentally alike; but if it sounds too L1-like, it will probably notwork in L2\".", "labels": [], "entities": []}, {"text": "In this paper, we follow a data-driven approach to model these influences and adapt GEC systems using L2 texts written by writers of the same L1 background.", "labels": [], "entities": []}, {"text": "The two most popular approaches for grammatical error correction are the classification approach () and the statistical machine translation (SMT) approach ().", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.828843375047048}, {"text": "statistical machine translation (SMT)", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.7773150155941645}]}, {"text": "The SMT approach has emerged as a popular paradigm for GEC because of its ability to learn text transformations from illformed to well-formed text enabling it to correct a wide variety of errors including complex errors that are difficult to handle for the classification approach (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9906919598579407}, {"text": "GEC", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.77786785364151}]}, {"text": "The phrasebased SMT approach has been used in state-ofthe-art GEC systems ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.8858597278594971}]}, {"text": "The SMT approach does not model error types specifically, nor does it require linguistic analysis like parsing and part-of-speech (POS) tagging.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9950146079063416}, {"text": "part-of-speech (POS) tagging", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.6128117918968201}]}, {"text": "We adopt a phrase-based SMT approach to GEC in this paper.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.898048460483551}, {"text": "GEC", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.626118004322052}]}, {"text": "Additionally, we implement and incorporate a neural network joint model (NNJM)) as a feature in our SMT-based GEC system.", "labels": [], "entities": [{"text": "SMT-based GEC", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.8039746284484863}]}, {"text": "It is easy to integrate an NNJM into the SMT decoding framework as it uses a fixed-window context and it has shown to improve SMT-based GEC.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9128064513206482}, {"text": "SMT-based GEC", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.8433828055858612}]}, {"text": "We adapt the NNJM to L1-specific data (i.e., English text written by writers of a particular L1) and obtain significant improvements over the baseline which uses an unadapted NNJM.", "labels": [], "entities": []}, {"text": "Adaptation is done by using the unadapted NNJM trained on general domain data (i.e., not L1-specific) using a log likelihood objective function with selfnormalization () as the starting point, and training for subsequent iterations using the smaller L1-specific in-domain data with a modified objective function which includes a KullbackLeibler (KL) divergence regularization term.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9653880596160889}, {"text": "NNJM", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9087978005409241}]}, {"text": "This modified objective function prevents overfitting on the smaller in-domain data and preserves the generalization capability of the NNJM.", "labels": [], "entities": [{"text": "NNJM", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.9560843706130981}]}, {"text": "We show that this method of adaptation works on very small and highquality L1-specific data as well (50-100 essays).", "labels": [], "entities": []}, {"text": "In summary, the two major contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "(1) This is the first work that performs L1-based adaptation for GEC using the SMT approach and covering all error types.", "labels": [], "entities": [{"text": "GEC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.743930459022522}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9778602123260498}]}, {"text": "(2) We introduce a novel method of NNJM adaptation and demonstrate that this method can work with indomain data that are much smaller than the general domain data.", "labels": [], "entities": [{"text": "NNJM adaptation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8555841445922852}]}], "datasetContent": [{"text": "The training data consist of two corpora: the NUS Corpus of Learner English (NUCLE) A sentence pair where the source or target sentence has more than 80 words is also removed from both NUCLE and Lang-8.", "labels": [], "entities": [{"text": "NUS Corpus of Learner English (NUCLE)", "start_pos": 46, "end_pos": 83, "type": "DATASET", "confidence": 0.9510167986154556}, {"text": "NUCLE", "start_pos": 185, "end_pos": 190, "type": "DATASET", "confidence": 0.9767835140228271}, {"text": "Lang-8", "start_pos": 195, "end_pos": 201, "type": "DATASET", "confidence": 0.9456859827041626}]}, {"text": "The statistics of the data after pre-processing are shown in  We obtain L1-specific in-domain data for adaptation based on the L1 information provided in Lang-8.", "labels": [], "entities": []}, {"text": "Adaptation is performed on English texts written by learners of three different L1 backgrounds: Chinese, Russian, and Spanish.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9260135889053345}]}, {"text": "The statistics of the in-domain data from Lang-8 for each L1 are given in.", "labels": [], "entities": []}, {"text": "For each L1, its out-of-domain data are obtained by excluding the L1-specific in-domain data (from) from the combined training data (CONCAT).", "labels": [], "entities": []}, {"text": "We also evaluate our system on the benchmark CoNLL-2014 shared task ( ) test set for GEC in English.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task ( ) test set", "start_pos": 45, "end_pos": 80, "type": "DATASET", "confidence": 0.7958540490695408}]}, {"text": "The CoNLL-2014 shared task consists of 1,312 sentences with two annotators.", "labels": [], "entities": []}, {"text": "We also perform evaluation on the extension of CoNLL-2014 test set (, which contains eight additional sets of annotations over the two sets of annotations provided in the original test set.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9540370305379232}]}, {"text": "Following the settings of the CoNLL-2014 shared task, we tune our unadapted baseline system and the L1-adapted systems on the CoNLL-2013 shared task test set consisting of 1,381 test sentences.", "labels": [], "entities": [{"text": "CoNLL-2014 shared task", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.8426146507263184}, {"text": "CoNLL-2013 shared task test set", "start_pos": 126, "end_pos": 157, "type": "DATASET", "confidence": 0.8855281472206116}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "We find that only the systems adapted based on L1 Chinese improves over the unadapted baseline system (S CONCAT + NNJM BASELINE ).", "labels": [], "entities": [{"text": "S CONCAT + NNJM", "start_pos": 103, "end_pos": 118, "type": "METRIC", "confidence": 0.698685809969902}, {"text": "BASELINE", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.5659335851669312}]}, {"text": "When the smallersized, high-quality FCE data is used for adaptation the margin of improvement is higher.", "labels": [], "entities": [{"text": "FCE data", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.7454669177532196}, {"text": "adaptation", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.9766342043876648}, {"text": "margin", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9902488589286804}]}, {"text": "This could be due to large proportion of Chinese learner written text in CoNLL-2014 test set, as the essays are written by the students of National University of Singapore comprising mostly of native Chinese speakers.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.9533851146697998}]}, {"text": "Adaptation to L1 Russian and Spanish, does not help the system on CoNLL-2014 test set.", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.9799707134564718}]}, {"text": "We also compare our baseline SMT-based system with other state-of-the-art GEC systems.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9782170057296753}]}, {"text": "Our baseline system which is SMT-based, achieves the best F 0.5 score compared to other systems using the SMT approach alone, making it a competitive SMT-based GEC baseline.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9620105624198914}, {"text": "F 0.5 score", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9864471356074015}, {"text": "SMT", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9635259509086609}, {"text": "SMT-based", "start_pos": 150, "end_pos": 159, "type": "TASK", "confidence": 0.9613301157951355}, {"text": "GEC baseline", "start_pos": 160, "end_pos": 172, "type": "DATASET", "confidence": 0.6953613758087158}]}, {"text": "Overall,  achieves the best F 0.5 score (47.40) after adding classifier components, spelling checker, punctuation and capitalization correction components in a pipeline with their SMT-based system.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.9867924253145853}, {"text": "SMT-based", "start_pos": 180, "end_pos": 189, "type": "TASK", "confidence": 0.972011387348175}]}, {"text": "However, their SMTbased system alone achieves an F 0.5 score of 39.48 only.", "labels": [], "entities": [{"text": "SMTbased", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9587964415550232}, {"text": "F 0.5 score", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9913246035575867}]}], "tableCaptions": [{"text": " Table 1: Statistics of training data", "labels": [], "entities": []}, {"text": " Table 2. For each L1, its out-of-domain data  are obtained by excluding the L1-specific in-domain  data (from", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the FCE dataset for each L1", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9815134406089783}]}, {"text": " Table 4: Precision (P), recall (R), and F0.5 of L1-based adaptation of GEC systems. All results are averaged over 5 runs of tuning", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9362282902002335}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.951007753610611}, {"text": "F0.5", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.999276340007782}]}, {"text": " Table 5: ST denotes F0.5 scores on the shared task test set and", "labels": [], "entities": [{"text": "ST", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9917033910751343}, {"text": "F0.5", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9959612488746643}]}]}