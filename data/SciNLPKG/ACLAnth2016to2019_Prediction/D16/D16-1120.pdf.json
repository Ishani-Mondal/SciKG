{"title": [{"text": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English", "labels": [], "entities": [{"text": "Demographic Dialectal Variation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8534934123357137}]}], "abstractContent": [{"text": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language.", "labels": [], "entities": []}, {"text": "We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter.", "labels": [], "entities": []}, {"text": "We propose a distantly supervised model to identify AAE-like language from de-mographics associated with geo-located messages , and we verify that this language follows well-known AAE linguistic phenomena.", "labels": [], "entities": []}, {"text": "In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers.", "labels": [], "entities": [{"text": "language identification and dependency parsing", "start_pos": 48, "end_pos": 94, "type": "TASK", "confidence": 0.7629568815231323}]}, {"text": "We also provide an ensemble classifier for language identification which eliminates this disparity and release anew corpus of tweets containing AAE-like language.", "labels": [], "entities": [{"text": "language identification", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.7151380628347397}]}, {"text": "Data and software resources are available at:", "labels": [], "entities": []}], "introductionContent": [{"text": "Owing to variation within a standard language, regional and social dialects exist within languages across the world.", "labels": [], "entities": []}, {"text": "These varieties or dialects differ from the standard variety in syntax (sentence structure), phonology (sound structure), and the inventory of words and phrases (lexicon).", "labels": [], "entities": []}, {"text": "Dialect communities often align with geographic and sociological factors, as language variation emerges within distinct social networks, or is affirmed as a marker of social identity.", "labels": [], "entities": []}, {"text": "As many of these dialects have traditionally existed primarily in oral contexts, they have historically been underrepresented in written sources.", "labels": [], "entities": []}, {"text": "Consequently, NLP tools have been developed from text which aligns with mainstream languages.", "labels": [], "entities": []}, {"text": "With the rise of social media, however, dialectal language is playing an increasingly prominent role in online conversational text, for which traditional NLP tools maybe insufficient.", "labels": [], "entities": []}, {"text": "This impacts many applications: for example, dialect speakers' opinions maybe mischaracterized under social media sentiment analysis or omitted altogether.", "labels": [], "entities": []}, {"text": "Since this data is now available, we seek to analyze current NLP challenges and extract dialectal language from online data.", "labels": [], "entities": []}, {"text": "Specifically, we investigate dialectal language in publicly available Twitter data, focusing on AfricanAmerican English (AAE), a dialect of Standard American English (SAE) spoken by millions of people across the United States.", "labels": [], "entities": []}, {"text": "AAE is a linguistic variety with defined syntactic-semantic, phonological, and lexical features, which have been the subject of a rich body of sociolinguistic literature.", "labels": [], "entities": []}, {"text": "In addition to the linguistic characterization, reference to its speakers and their geographical location or speech communities is important, especially in light of the historical development of the dialect.", "labels": [], "entities": []}, {"text": "Not all African-Americans speak AAE, and not all speakers of AAE are African-American; nevertheless, speakers of this variety have close ties with specific communities of African-Americans).", "labels": [], "entities": []}, {"text": "Due to its widespread use, established history in the sociolinguistic literature, and demographic associations, AAE provides an ideal starting point for the development of a statistical model that uncovers dialectal language.", "labels": [], "entities": []}, {"text": "In fact, its presence in social media is attracting increasing interest for natural language processing () and sociolinguistic research.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.6626285413901011}]}, {"text": "In this work we: \u2022 Develop a method to identify demographically-aligned text and language from geo-located messages ( \u00a72), based on distant supervision of geographic census demographics through a statistical model that assumes a soft correlation between demographics and language.", "labels": [], "entities": []}, {"text": "\u2022 Validate our approach by verifying that text aligned with African-American demographics follows well-known phonological and syntactic properties of AAE, and document the previously unattested ways in which such text diverges from SAE ( \u00a73).", "labels": [], "entities": []}, {"text": "\u2022 Demonstrate racial disparity in the efficacy of NLP tools for language identification and dependency parsing-they perform poorly on this text, compared to text associated with white speakers ( \u00a74, \u00a75).", "labels": [], "entities": [{"text": "language identification", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.7329868376255035}, {"text": "dependency parsing-they", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7740789651870728}]}, {"text": "\u2022 Improve language identification for U.S. online conversational text with a simple ensemble classifier using our demographicallybased distant supervision method, aiming to eliminate racial disparity inaccuracy rates ( \u00a74.2).", "labels": [], "entities": [{"text": "language identification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7480975985527039}]}, {"text": "\u2022 Provide a corpus of 830,000 tweets aligned with African-American demographics.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our analysis from \u00a74.1 indicates that this method would correct erroneous false negatives for AAE  For the General set these are an approximation; see text.", "labels": [], "entities": [{"text": "AAE", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.6618581414222717}, {"text": "General set", "start_pos": 107, "end_pos": 118, "type": "DATASET", "confidence": 0.9634385406970978}]}, {"text": "messages in the training set for the model.", "labels": [], "entities": []}, {"text": "We further confirm this by testing the classifier on a sample of 2.2 million geolocated tweets sent in the U.S. in 2014, which are not in the training set.", "labels": [], "entities": []}, {"text": "In addition to performance on the entire sample, we examine our classifier's performance on messages whose posterior probability of using AA-or white-associated terms was greater than 0.8 within the sample, which in this section we will call high AA and high white messages, respectively.", "labels": [], "entities": []}, {"text": "Our classifier's precision is high across the board, at 100% across manually annotated samples of 200 messages from each sample.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995842576026917}]}, {"text": "Since we are concerned about the system's overall recall, we impute recall (Table 4) by assuming that all high AA and high white messages are indeed English.", "labels": [], "entities": [{"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9968706965446472}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9980272650718689}, {"text": "AA", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9652489423751831}]}, {"text": "Recall for langid.py alone is calculated by n N , where n is the number of messages predicted to be English by langid.py, and N is the total number of messages in the set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9648891091346741}]}, {"text": "(This is the complement of, except evaluated on the test set.)", "labels": [], "entities": []}, {"text": "We estimate the ensemble's recall as n+m N , where m = (n flip )P (English | f lip) is the expected number of correctly changed classifications (from non-English to English) by the ensemble and the second term is the precision (estimated as 1.0).", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9987691044807434}, {"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9995645880699158}]}, {"text": "We observe the baseline system has considerable difference in recall between the groups which is solved by the ensemble.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9994596838951111}]}, {"text": "We also apply the same calculation to the general set of all 2.2 million messages; the baseline classifies 88% as English.", "labels": [], "entities": []}, {"text": "This is a less accurate approximation of recall since we have observed a substantial presence of non-English messages.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.998785674571991}]}, {"text": "The ensemble classifies an additional 5.4% of the messages as English; since these are all (or nearly all) correct, this reflects at least a 5.4% gain to recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9752472639083862}]}, {"text": "Given the lexical and syntactic variation of AAE compared to SAE, we hypothesize that syntactic analysis tools also have differential accuracy.", "labels": [], "entities": [{"text": "AAE", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.6809563636779785}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9980200529098511}]}, {"text": "demonstrate this for part-ofspeech tagging, finding that SAE-trained taggers had disparate accuracy on AAE versus non-AAE tweets.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7529278695583344}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9976040720939636}, {"text": "AAE", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.6374923586845398}]}, {"text": "We assess a publicly available syntactic dependency parser on our AAE and white-aligned corpora.", "labels": [], "entities": [{"text": "AAE", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.6945680379867554}]}, {"text": "Syntactic parsing for tweets has received some research attention; Foster et al.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8940728306770325}]}, {"text": "(2011) create a corpus of constituent trees for English tweets, and's Tweeboparser is trained on a Twitter corpus annotated with a customized unlabeled dependency formalism; since its data was uniformly sampled from tweets, we expect it may have low disparity between demographic groups.", "labels": [], "entities": []}, {"text": "We focus on widely used syntactic representations, testing the SyntaxNet neural network-based dependency parser (), 11 which reports state-of-the-art results, including for web corpora.", "labels": [], "entities": [{"text": "SyntaxNet neural network-based dependency parser", "start_pos": 63, "end_pos": 111, "type": "TASK", "confidence": 0.5482005298137664}]}, {"text": "We evaluate it against anew manual annotation of 200 messages, 100 randomly sampled from each of the AA-and white-aligned corpora described in \u00a72.3.", "labels": [], "entities": []}, {"text": "SyntaxNet outputs grammatical relations conforming to the Stanford Dependencies (SD) system (de, which we used to annotate messages using Brat, 12 comparing to predicted parses for reference.", "labels": [], "entities": [{"text": "Stanford Dependencies (SD) system", "start_pos": 58, "end_pos": 91, "type": "DATASET", "confidence": 0.7932318945725759}, {"text": "Brat", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9696455001831055}]}, {"text": "Message order was randomized and demographic inferences were hidden from the annotator.", "labels": [], "entities": []}, {"text": "To increase statistical power relative to annotation effort, we developed a partial annotation approach to only annotate edges for the root word of the first major sentence in a message.", "labels": [], "entities": []}, {"text": "Generally, we found that that SD worked well as a descriptive formalism for tweets' syntax; we describe handling of AAE and Internet-specific non-standard issues in the appendix.", "labels": [], "entities": [{"text": "AAE", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.4966450333595276}]}, {"text": "We evaluate labeled recall of the annotated edges for each message set: Parser Bootstrapped standard errors (from 10,000 message resamplings) are in parentheses; differences are statistically significant (p < 10 \u22126 in both cases).", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9182637333869934}, {"text": "Parser Bootstrapped standard errors", "start_pos": 72, "end_pos": 107, "type": "METRIC", "confidence": 0.8643336743116379}]}, {"text": "The white-aligned accuracy rate of 80.4% is broadly inline with previous work (compare to the parser's unlabeled accuracy of 89% on English Web Treebank full annotations), but parse quality is much worse on AAE tweets at 64.0%.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 18, "end_pos": 31, "type": "METRIC", "confidence": 0.9687712490558624}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9171908497810364}, {"text": "English Web Treebank full annotations", "start_pos": 132, "end_pos": 169, "type": "DATASET", "confidence": 0.9131377696990967}]}, {"text": "We test the Stanford CoreNLP neural network dependency parser) using the english SD model that outputs this formalism; 13 its disparity is worse.", "labels": [], "entities": [{"text": "CoreNLP neural network dependency parser", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.8299775838851928}]}, {"text": "used a similar parser 14 on Twitter text; our analysis suggests this approach may suffer from errors caused by the parser.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: AAE syntactic constructions and the ratios of their", "labels": [], "entities": [{"text": "AAE syntactic constructions", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8105214436848959}]}, {"text": " Table 4: Imputed recall of English messages in 2014 messages.", "labels": [], "entities": [{"text": "Imputed", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.867592990398407}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.870707631111145}]}]}