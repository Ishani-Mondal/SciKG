{"title": [{"text": "Semi-Supervised Learning of Sequence Models with the Method of Moments", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a fast and scalable method for semi-supervised learning of sequence models, based on anchor words and moment matching.", "labels": [], "entities": []}, {"text": "Our method can handle hidden Markov models with feature-based log-linear emissions.", "labels": [], "entities": []}, {"text": "Unlike other semi-supervised methods, no decoding passes are necessary on the unlabeled data and no graph needs to be constructed-only one pass is necessary to collect moment statistics.", "labels": [], "entities": []}, {"text": "The model parameters are estimated by solving a small quadratic program for each feature.", "labels": [], "entities": []}, {"text": "Experiments on part-of-speech (POS) tagging for Twitter and fora low-resource language (Malagasy) show that our method can learn from very few annotated sentences.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6456006705760956}]}], "introductionContent": [{"text": "Statistical learning of NLP models is often limited by the scarcity of annotated data.", "labels": [], "entities": []}, {"text": "Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets.", "labels": [], "entities": []}, {"text": "Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs.", "labels": [], "entities": []}, {"text": "More scalable learning algorithms are in demand.", "labels": [], "entities": []}, {"text": "In this paper, we propose a moment-matching method for semi-supervised learning of sequence models.", "labels": [], "entities": []}, {"text": "Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (, supervised learning with latent variables) and topic modeling (.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 200, "end_pos": 214, "type": "TASK", "confidence": 0.7766539454460144}]}, {"text": "These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding.", "labels": [], "entities": []}, {"text": "Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor.", "labels": [], "entities": []}, {"text": "Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (.", "labels": [], "entities": []}, {"text": "Rather than identifying anchor words from unlabeled data (, we extract them from a small labeled dataset or from a dictionary.", "labels": [], "entities": []}, {"text": "Given the anchor words, the estimation of the model parameters can be made efficient by collecting moment statistics from unlabeled data, then solving a small quadratic program for each word.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We adapt anchor methods to semi-supervised learning of generative sequence models.", "labels": [], "entities": []}, {"text": "\u2022 We show how our method can also handle loglinear feature-based emissions.", "labels": [], "entities": []}, {"text": "\u2022 We apply this model to POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.8286557495594025}]}, {"text": "Our experiments on the Twitter dataset introduced by and on the dataset introduced by for Malagasy, a low-resource language, show that our method does particularly well with very little labeled data, outperforming semi-supervised EM and self-training.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our method on two tasks: POS tagging of Twitter text (in English), and POS tagging fora low-resource language (Malagasy).", "labels": [], "entities": [{"text": "POS tagging of Twitter text", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.8201009631156921}, {"text": "POS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.7436879873275757}]}, {"text": "For all the experiments, we used the universal POS tagset (, which consists of K = 12 tags.", "labels": [], "entities": [{"text": "POS tagset", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.7598309516906738}]}, {"text": "We compared our method against supervised baselines (HMM and FHMM), which use the labeled data only, and two semi-supervised baselines that exploit the unlabeled data: self-training and EM.", "labels": [], "entities": [{"text": "FHMM", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.5390591025352478}]}, {"text": "For the Twitter experiments, we also evaluated a stacked architecture in which we derived features from our model's predictions to improve a state-ofthe-art POS tagger (MEMM).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagging accuracies on Twitter. Shown are  the supervised and semi-supervised baselines, and our  moment-based method, trained with 150 training labeled  sequences, and the full labeled corpus (1000 sequences).", "labels": [], "entities": []}, {"text": " Table 2: Tagging accuracy for the MEMM POS tagger of  Owoputi et al. (2013) with additional features from our  model's posteriors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9604703783988953}, {"text": "MEMM POS tagger", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.5571863949298859}]}, {"text": " Table 3: Tagging accuracies for the Malagasy dataset.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9856330752372742}, {"text": "Malagasy dataset", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.8508550822734833}]}]}