{"title": [{"text": "Neural Headline Generation on Abstract Meaning Representation", "labels": [], "entities": [{"text": "Neural Headline Generation on Abstract Meaning Representation", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7227766002927508}]}], "abstractContent": [{"text": "Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks.", "labels": [], "entities": [{"text": "tackling natural language generation tasks", "start_pos": 90, "end_pos": 132, "type": "TASK", "confidence": 0.8440566658973694}]}, {"text": "This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model.", "labels": [], "entities": []}, {"text": "We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM.", "labels": [], "entities": []}, {"text": "Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7947776913642883}]}], "introductionContent": [{"text": "Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (), image captioning (, video description (, and headline generation (.", "labels": [], "entities": [{"text": "tackling natural language generation (NLG) tasks", "start_pos": 79, "end_pos": 127, "type": "TASK", "confidence": 0.76764802262187}, {"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.7666875123977661}, {"text": "image captioning", "start_pos": 159, "end_pos": 175, "type": "TASK", "confidence": 0.762768030166626}, {"text": "video description", "start_pos": 179, "end_pos": 196, "type": "TASK", "confidence": 0.6978794932365417}, {"text": "headline generation", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.8386343419551849}]}, {"text": "This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 111, "end_pos": 138, "type": "TASK", "confidence": 0.6583104133605957}]}, {"text": "There are several directions for enhancement.", "labels": [], "entities": []}, {"text": "This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 139, "end_pos": 150, "type": "TASK", "confidence": 0.8432859182357788}, {"text": "dependency parsing", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.8039371967315674}, {"text": "named entity recognition", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.6369974315166473}, {"text": "semantic role labeling", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.6715596516927084}]}, {"text": "Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks.", "labels": [], "entities": []}, {"text": "To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR).", "labels": [], "entities": [{"text": "headline generation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8555910885334015}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 141, "end_pos": 178, "type": "TASK", "confidence": 0.7101065615812937}]}, {"text": "The method is essentially an extension of attention-based summarization (ABS) (.", "labels": [], "entities": [{"text": "attention-based summarization (ABS)", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.5943736493587494}]}, {"text": "Our proposed method encodes results obtained from an AMR parser by using a modified version of Tree-LSTM encoder as additional information of the baseline ABS model.", "labels": [], "entities": []}, {"text": "Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8661254346370697}, {"text": "summaries (headlines) from original longer sentences", "start_pos": 205, "end_pos": 257, "type": "TASK", "confidence": 0.8268249183893204}]}, {"text": "We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR).", "labels": [], "entities": [{"text": "ABS", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9955062866210938}, {"text": "AMR", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.8602622151374817}]}], "datasetContent": [{"text": "To demonstrate the effectiveness of our proposed method, we conducted experiments on benchmark data of the abstractive headline generation task described in.", "labels": [], "entities": [{"text": "abstractive headline generation task", "start_pos": 107, "end_pos": 143, "type": "TASK", "confidence": 0.6796078383922577}]}], "tableCaptions": [{"text": " Table 1: Results of methods on each dataset. We marked  *  on the ABS+AMR results if we observed  statistical difference (p < 0.05) between ABS (re-run) and ABS+AMR on the t-test. (R-1: ROUGE-1, R-2:  ROUGE-2, R-L: ROUGE-L)", "labels": [], "entities": []}]}