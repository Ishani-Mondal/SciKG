{"title": [{"text": "An Evaluation of Parser Robustness for Ungrammatical Sentences", "labels": [], "entities": []}], "abstractContent": [{"text": "For many NLP applications that require a parser, the sentences of interest may not be well-formed.", "labels": [], "entities": []}, {"text": "If the parser can overlook problems such as grammar mistakes and produce a parse tree that closely resembles the correct analysis for the intended sentence, we say that the parser is robust.", "labels": [], "entities": []}, {"text": "This paper compares the performances of eight state-of-the-art dependency parsers on two domains of ungrammat-ical sentences: learner English and machine translation outputs.", "labels": [], "entities": []}, {"text": "We have developed an evaluation metric and conducted a suite of experiments.", "labels": [], "entities": []}, {"text": "Our analyses may help practitioners to choose an appropriate parser for their tasks, and help developers to improve parser robustness against ungrammatical sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Previous works have shown that, in general, parser performances degrade when applied to out-ofdomain sentences).", "labels": [], "entities": []}, {"text": "If a parser performs reasonably well fora wide range of out-of-domain sentences, it is said to be robust (.", "labels": [], "entities": []}, {"text": "Sentences that are ungrammatical, awkward, or too casual/colloquial can all be seen as special kinds of out-of-domain sentences.", "labels": [], "entities": []}, {"text": "These types of sentences are commonplace for NLP applications, from product reviews and social media analysis to intelligent language tutors and multilingual processing.", "labels": [], "entities": []}, {"text": "Since parsing is an essential component for many applications, it is natural to ask: Are some parsers more robust than others against sentences that are not well-formed?", "labels": [], "entities": [{"text": "parsing", "start_pos": 6, "end_pos": 13, "type": "TASK", "confidence": 0.965765118598938}]}, {"text": "Previous works on parser evaluation that focused on accuracy and speed have not taken ungrammatical sentences into consideration.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8049344420433044}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.998630940914154}, {"text": "speed", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9852083921432495}]}, {"text": "In this paper, we report a set of empirical analyses of eight leading dependency parsers on two domains of ungrammatical text: English-as-a-Second Language (ESL) learner text and machine translation (MT) outputs.", "labels": [], "entities": [{"text": "machine translation (MT) outputs", "start_pos": 179, "end_pos": 211, "type": "TASK", "confidence": 0.8470095892747244}]}, {"text": "We also vary the types of training sources; the parsers are trained with the Penn Treebank (to be comparable with other studies) and Tweebank, a treebank on tweets (to be a bit more like the test domain) ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9950574636459351}]}, {"text": "The main contributions of the paper are: \u2022 a metric and methodology for evaluating ungrammatical sentences without referring to a gold standard corpus; \u2022 a quantitative comparison of parser accuracy of leading dependency parsers on ungrammatical sentences; this may help practitioners to select an appropriate parser for their applications; and \u2022 a suite of robustness analyses for the parsers on specific kinds of problems in the ungrammatical sentences; this may help developers to improve parser robustness in the future.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.8478249311447144}]}], "datasetContent": [{"text": "Parser evaluation for ungrammatical texts presents some domain-specific challenges.", "labels": [], "entities": [{"text": "Parser evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9433452785015106}]}, {"text": "The typical approach to evaluate parsers is to compare parser out-puts against manually annotated gold standards.", "labels": [], "entities": []}, {"text": "Although there area few small semi-manually constructed treebanks on learner texts or tweets (Daiber and van der Goot, 2016), their sizes make them unsuitable for the evaluation of parser robustness.", "labels": [], "entities": []}, {"text": "Moreover, some researchers have raised valid questions about the merit of creating a treebank for ungrammatical sentences or adapting the annotation schema.", "labels": [], "entities": []}, {"text": "A \"gold-standard free\" alternative is to compare the parser output for each problematic sentence with the parse tree of the corresponding correct sentence.", "labels": [], "entities": []}, {"text": "used this approach over a small set of ungrammatical sentences and showed that parser's accuracy is different for different types of errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9881786704063416}]}, {"text": "A limitation of this approach is that the comparison works best when the differences between the problematic sentence and the correct sentence are small.", "labels": [], "entities": []}, {"text": "This is not the case for some ungrammatical sentences (especially from MT systems).", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9445913434028625}]}, {"text": "Another closely-related approach is to semi-automatically create treebanks from artificial errors.", "labels": [], "entities": []}, {"text": "For example, Foster generated artificial errors to the sentences from the Penn Treebank for evaluating the effect of error types on parsers.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9930389523506165}]}, {"text": "In another work, proposed an unsupervised evaluation of parser robustness based on the introduction of artificial spelling errors in error-free sentences.", "labels": [], "entities": []}, {"text": "adapted a similar method to compare the robustness of four parsers over sentences with misspelled words.", "labels": [], "entities": []}, {"text": "Our proposed evaluation methodology is similar to the \"gold-standard free\" approach; we compare the parser output for an ungrammatical sentence with the automatically generated parse tree of the corresponding correct sentence.", "labels": [], "entities": []}, {"text": "In the next section, we discuss our evaluation metric to address the concerns that some ungrammatical sentences maybe very different from their corrected versions.", "labels": [], "entities": []}, {"text": "This allows us to evaluate parsers with more realistic data that exhibit a diverse set of naturally occurring errors, instead of artificially generated errors or limited error types.", "labels": [], "entities": []}, {"text": "For the purpose of robustness evaluation, we take the automatically produced parse tree of a well-formed sentence as \"gold-standard\" and compare the parser output for the corresponding problematic sentence against it.", "labels": [], "entities": []}, {"text": "Even if the \"gold-standard\" is not perfectly correct in absolute terms, it represents the norm from which parse trees of problematic sentences diverge: if a parser were robust against ungrammatical sentences, its output for these sentences should be similar to its output for the well-formed ones.", "labels": [], "entities": []}, {"text": "Determining the evaluation metric for comparing these trees, however, presents another challenge.", "labels": [], "entities": []}, {"text": "Since the words of the ungrammatical sentence and its grammatical counterpart do not necessarily match (an example is given in), we cannot use standard metrics such as.", "labels": [], "entities": []}, {"text": "We also cannot use adapted metrics for comparing parse trees of unmatched sentences (e.g.,)), because these metrics consider all the words regardless of the mismatches (extra or missing words) between two sentences.", "labels": [], "entities": []}, {"text": "This is a problem for comparing ungrammatical sentences to grammatical ones because a parser is unfairly penalized when it assigns relations to extra words and when it does not assign relations to missing words.", "labels": [], "entities": []}, {"text": "Since a parser cannot modify the sentence, we do not want to penalize these extraneous or missing relations; on the other hand, we do want to identify cascading effects on the parse tree due to a grammar error.", "labels": [], "entities": []}, {"text": "For the purpose of evaluating parser robustness against ungrammatical sentences, we propose a modified metric in which the dependencies connected to unmatched (extra or missing) error words are ignored.", "labels": [], "entities": []}, {"text": "A more formal definition is as follows: \u2022 Shared dependency is a mutual dependency between two trees; \u2022 Error-related dependency is a dependency connected to an extra word 1 in the sentence; \u2022 Precision is (# of shared dependencies) / (# of dependencies of the ungrammatical sentence - # of error-related dependencies of the ungrammatical sentence); \u2022 Recall is (# of shared dependencies) / (# of dependencies of the grammatical sentence -# of error-related dependencies of the grammatical sentence); and \u2022 Robustness F 1 is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 193, "end_pos": 202, "type": "METRIC", "confidence": 0.9767723679542542}, {"text": "Recall", "start_pos": 352, "end_pos": 358, "type": "METRIC", "confidence": 0.942942202091217}, {"text": "Robustness F 1", "start_pos": 507, "end_pos": 521, "type": "METRIC", "confidence": 0.9002697070439657}, {"text": "precision", "start_pos": 546, "end_pos": 555, "type": "METRIC", "confidence": 0.9989944100379944}, {"text": "recall", "start_pos": 560, "end_pos": 566, "type": "METRIC", "confidence": 0.9935139417648315}]}, {"text": "shows an example in which the ungrammatical sentence has an unnecessary word, \"about\", so the three dependencies connected to it are counted as error-related dependencies.", "labels": [], "entities": []}, {"text": "The two shared dependencies between the trees result in a precision of 2/(5\u22123) = 1, recall of 2/(4\u22120) = 0.5, and Robustness F 1 of 66%.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9990664124488831}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9996907711029053}, {"text": "Robustness F 1", "start_pos": 113, "end_pos": 127, "type": "METRIC", "confidence": 0.8216450611750284}]}, {"text": "Our experiments are conducted over a wide range of dependency parsers that are trained on two different treebanks: Penn Treebank (PTB) and Tweebank.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.9649947643280029}, {"text": "Tweebank", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.859920859336853}]}, {"text": "We evaluate the robustness of parsers over two datasets that contain ungrammatical sentences: writings of English-as-a-Second language learners and machine translation outputs.", "labels": [], "entities": []}, {"text": "We choose datasets for which the corresponding correct sentences are available (or easily reconstructed).", "labels": [], "entities": []}, {"text": "In the robustness evaluation metric (Section 3), shared dependencies and error-related dependencies are detected based on alignments between words in the ungrammatical and grammatical sentences.", "labels": [], "entities": []}, {"text": "We find the alignments in the FCE and MT data in a slightly different way.", "labels": [], "entities": [{"text": "FCE and MT data", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.7053080126643181}]}, {"text": "In the FCE dataset, in which the error words are annotated, the grammatical and ungrammatical sentences can easily be aligned.", "labels": [], "entities": [{"text": "FCE dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9554228186607361}]}, {"text": "In the MT dataset, we use the TER (Translation Error Rate) tool (default settings) to find alignments.", "labels": [], "entities": [{"text": "MT dataset", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9090995788574219}, {"text": "TER (Translation Error Rate)", "start_pos": 30, "end_pos": 58, "type": "METRIC", "confidence": 0.8540438612302145}]}, {"text": "In our experiments, we present unlabeled robustness F 1 micro-averaged across the test sentences.", "labels": [], "entities": [{"text": "unlabeled robustness F 1 micro-averaged", "start_pos": 31, "end_pos": 70, "type": "METRIC", "confidence": 0.7091573715209961}]}, {"text": "We consider punctuations when parsers are trained with the PTB data, because punctuations can be a source of ungrammaticality.", "labels": [], "entities": [{"text": "PTB data", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9552395045757294}]}, {"text": "However, we ignore punctuations when parsers are trained with the Tweebank data, because punctuations are not annotated in the tweets with their dependencies.", "labels": [], "entities": [{"text": "Tweebank data", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9338155388832092}]}, {"text": "The experiments aim to address the following questions given separate training and test data: 1.", "labels": [], "entities": []}, {"text": "How do parsers perform on erroneous sentences?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsers' performance in terms of accuracy and robustness. The best result in each column is given  in bold, and the worst result is in italics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9994162321090698}]}]}