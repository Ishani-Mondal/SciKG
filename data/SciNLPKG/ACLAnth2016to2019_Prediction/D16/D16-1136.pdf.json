{"title": [{"text": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools.", "labels": [], "entities": []}, {"text": "However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy.", "labels": [], "entities": []}, {"text": "We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages.", "labels": [], "entities": []}, {"text": "Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the mono-lingual word similarity and cross-lingual document classification task.", "labels": [], "entities": [{"text": "cross-lingual document classification", "start_pos": 196, "end_pos": 233, "type": "TASK", "confidence": 0.6389377812544504}]}], "introductionContent": [{"text": "Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis , dependency parsing (, machine translation ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9615079462528229}, {"text": "dependency parsing", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.8381054401397705}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7697913944721222}]}, {"text": "Crosslingual word embeddings area natural extension facilitating various crosslingual tasks, e.g. through transfer learning.", "labels": [], "entities": []}, {"text": "A model builtin a source resource-rich language can then applied to the target resource poor languages.", "labels": [], "entities": []}, {"text": "A key barrier for crosslingual transfer is lexical matching between the source and the target language.", "labels": [], "entities": [{"text": "crosslingual transfer", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.834921270608902}]}, {"text": "Crosslingual word embeddings area natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space ().", "labels": [], "entities": []}, {"text": "Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing.", "labels": [], "entities": [{"text": "document classification", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7694695591926575}, {"text": "dependency parsing", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.8470887541770935}]}, {"text": "We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity and bilingual lexicon induction tasks.", "labels": [], "entities": []}, {"text": "Moreover, many prior work () used bilingual or comparable corpus which is also expensive for many low-resource languages.", "labels": [], "entities": []}, {"text": "impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods.", "labels": [], "entities": []}, {"text": "To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (.", "labels": [], "entities": []}, {"text": "However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm.", "labels": [], "entities": []}, {"text": "Other methods use a dictionary as the bridge between languages (), however they do not adequately handle translation ambiguity.", "labels": [], "entities": []}, {"text": "Our model uses a bilingual dictionary from Panlex () as the source of bilingual signal.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.9734551906585693}]}, {"text": "Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9156453609466553}]}, {"text": "Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through incorporating multiple dictionary translations (word sense and translation are closely linked).", "labels": [], "entities": []}, {"text": "In addition to the dictionary, our method only requires monolingual data.", "labels": [], "entities": []}, {"text": "Our approach is an extension of the continuous bag-ofwords (CBOW) model) to inject multilingual training signal based on dictionary translations.", "labels": [], "entities": []}, {"text": "We experiment with several variations of our model, whereby we predict only the translation or both word and its translation and consider different ways of using the different learned center-word versus context embeddings in application tasks.", "labels": [], "entities": []}, {"text": "We also propose a regularisation method to combine the two embedding matrices during training.", "labels": [], "entities": []}, {"text": "Together, these modifications substantially improve the performance across several tasks.", "labels": [], "entities": []}, {"text": "Our final model achieves state-of-the-art performance on bilingual lexicon induction task, large improvement over word similarity task compared with previous published crosslingual word embeddings, and competitive result on cross-lingual document classification task.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.6776918917894363}, {"text": "cross-lingual document classification task", "start_pos": 224, "end_pos": 266, "type": "TASK", "confidence": 0.7313083484768867}]}, {"text": "Notably, our embedding combining techniques are general, yielding improvements also for monolingual word embedding.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: \u2022 Proposing anew crosslingual training method for learning vector embeddings, based only on monolingual corpora and a bilingual dictionary; \u2022 Evaluating several methods for combining embeddings, which are shown to help in both crosslingual and monolingual evaluations; and \u2022 Achieving consistent results which are competitive in monolingual, bilingual and crosslingual transfer settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental evaluation seeks to determine how well lexical distances in the learned embedding spaces match with known lexical similarity judgements from bilingual and monolingual lexical resources.", "labels": [], "entities": []}, {"text": "To this end, in \u00a76 we test crosslingual distances using a bilingual lexicon induction task in which we evaluate the embeddings in terms of how well nearby pairs of words from two languages in the embedding space match with human judgements.", "labels": [], "entities": []}, {"text": "Next, to evaluate the monolingual embeddings we evaluate word similarities in a single language against standard similarity datasets ( \u00a77).", "labels": [], "entities": []}, {"text": "Lastly, to demonstrate the usefulness of our embeddings in a task-based setting, we evaluate on crosslingual document classification ( \u00a79).", "labels": [], "entities": [{"text": "crosslingual document classification", "start_pos": 96, "end_pos": 132, "type": "TASK", "confidence": 0.652694433927536}]}, {"text": "Monolingual Data The monolingual data is taken from the pre-processed Wikipedia dump from Al-.", "labels": [], "entities": [{"text": "Wikipedia dump from Al-.", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.9486760199069977}]}, {"text": "The data is already cleaned and tokenized.", "labels": [], "entities": []}, {"text": "We additionally lower-case all words.", "labels": [], "entities": []}, {"text": "Normally monolingual word embeddings are trained on billions of words.", "labels": [], "entities": []}, {"text": "However, obtaining that much monolingual data fora low-resource language is infeasible.", "labels": [], "entities": []}, {"text": "Therefore, we only select the first 5 million sentences (around 100 million words) for each language.", "labels": [], "entities": []}, {"text": "Dictionary A bilingual dictionary is the only source of bilingual correspondence in our technique.", "labels": [], "entities": []}, {"text": "We prefer a dictionary that covers many languages, such that our approach can be applied widely to many low-resource languages.", "labels": [], "entities": []}, {"text": "We use Panlex, a dictionary which currently covers around 1300 language varieties with about 12 million expressions.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 7, "end_pos": 13, "type": "DATASET", "confidence": 0.959316611289978}]}, {"text": "The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc.", "labels": [], "entities": [{"text": "PanLex", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.9630606770515442}]}, {"text": "Accordingly, Panlex has high language coverage but often noisy translations.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9479005336761475}]}, {"text": "4 summarizes the sizes of monolingual corpora and dictionaries for each pair of language in our experiments.", "labels": [], "entities": []}, {"text": "We also experimented with a crowd-sourced dictionary from Wiktionary.", "labels": [], "entities": []}, {"text": "Our initial observation was that the translation quality was better but with a lower-coverage.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.96396803855896}]}, {"text": "For example, for en-it dictionary, Panlex and Wiktionary have a coverage of 42.1% and 16.8% respectively for the top 100k most frequent English words from Wikipedia.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.9571884274482727}]}, {"text": "The average number of translations are 5.2 and 1.9 respectively.", "labels": [], "entities": [{"text": "translations", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.8525362610816956}]}, {"text": "We observed similar trend using Panlex and Wiktionary dictionary in our model.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9450181126594543}]}, {"text": "However, using Panlex results in much better performance.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.9278525114059448}]}, {"text": "We can run the model on the combined dictionary from both Panlex and Wiktionary but we leave it for future work.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.9794370532035828}]}, {"text": "ment using Panlex and Wiktionary dictionaries.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.9697409272193909}]}, {"text": "The result with Panlex is substantially worse than with Wiktionary.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.8840927481651306}]}, {"text": "This confirms our hypothesis in \u00a72.", "labels": [], "entities": []}, {"text": "That is the context might be corrupted if we just randomly replace the training data with the translation from noisy dictionary such as Panlex.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 136, "end_pos": 142, "type": "DATASET", "confidence": 0.9821995496749878}]}, {"text": "Our model when randomly picking the translation is similar to, using the Panlex dictionary.", "labels": [], "entities": [{"text": "Panlex dictionary", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.9768231511116028}]}, {"text": "The biggest difference is that they replace the training data (both context and middle word) while we fix the context and only replace the middle word.", "labels": [], "entities": []}, {"text": "For a high coverage yet noisy dictionary such as Panlex, our approach gives better average score.", "labels": [], "entities": [{"text": "Panlex", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9735114574432373}]}, {"text": "Comparing our two most basic models (EM selection and random selection), it is clear that the model using EM to select the translation outperforms random selection by a significant margin.", "labels": [], "entities": []}, {"text": "Our joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for nl-en.", "labels": [], "entities": []}, {"text": "We use equation to combine both context embeddings V and word embeddings U for all three language pairs.", "labels": [], "entities": []}, {"text": "This modification during training substantially improves the performance.", "labels": [], "entities": []}, {"text": "More importantly, all our improvements are consistent for all three language pairs and both evaluation metrics, showing the robustness of our models.", "labels": [], "entities": []}, {"text": "Our combined model out-performed previous approaches by a large margin.", "labels": [], "entities": []}, {"text": "Vuli\u00b4c used bilingual comparable data, but this might be hard to obtain for some language pairs.", "labels": [], "entities": []}, {"text": "Their performance on nl-en is poor because their comparable data between en and nl is small.", "labels": [], "entities": []}, {"text": "Besides, they also use POS tagger and lemmatizer to filter only Noun and reduce the morphology complexity during training.", "labels": [], "entities": []}, {"text": "These tools might not be available for many languages.", "labels": [], "entities": []}, {"text": "For a fairer comparison to their work, we also use the same Treetagger to lemmatize the output of our combined model before evaluation.", "labels": [], "entities": []}, {"text": "(+lemmatization) shows some improvements but minor.", "labels": [], "entities": []}, {"text": "It demonstrates that our model is already good at disambiguating morphology.", "labels": [], "entities": []}, {"text": "For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tokens in millions for the source  and target languages in each language pair. Also  shown is the number of entries in the bilingual dic- tionary in thousands. The number in the parenthesis  shows the token coverage in the dictionary on each  monolingual corpus.", "labels": [], "entities": []}, {"text": " Table 3: Bilingual Lexicon Induction performance from es, it, nl to en. Gouws and S\u00f8gaard (2015)  + Panlex/Wikt is our reimplementation using Panlex/Wiktionary dictionary. All our models use Panlex as  the dictionary. We reported the recall at 1 and 5. The best performance is bold.", "labels": [], "entities": [{"text": "Panlex/Wiktionary dictionary", "start_pos": 143, "end_pos": 171, "type": "DATASET", "confidence": 0.8109540343284607}, {"text": "recall", "start_pos": 235, "end_pos": 241, "type": "METRIC", "confidence": 0.998524010181427}]}, {"text": " Table 5: Performance on en-it BLI and en mono- lingual similarity WordSim353 (WS-en) for various  combining algorithms mentioned in  \u00a74.3 w.r.t just  using U or V alone (after joint-training). We use  \u03b3 = 0.5 for interpolation and \u03b4 = 0.01 for regular- ization with the choice of V, U or interpolation of  both V+U", "labels": [], "entities": [{"text": "WordSim353 (WS-en", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.7652462124824524}]}, {"text": " Table 6: CLDC performance for both en \u2192 de and  de \u2192 en direction for many CLWE. The MT base- line uses phrase-based statistical machine translation  to translate the source language to target language  (", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9397478103637695}, {"text": "phrase-based statistical machine translation", "start_pos": 105, "end_pos": 149, "type": "TASK", "confidence": 0.580119401216507}]}]}