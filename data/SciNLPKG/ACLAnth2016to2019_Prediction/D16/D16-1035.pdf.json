{"title": [{"text": "Discourse Parsing with Attention-based Hierarchical Neural Networks", "labels": [], "entities": [{"text": "Discourse Parsing with Attention-based Hierarchical Neural Networks", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.5889094982828412}]}], "abstractContent": [{"text": "RST-style document-level discourse parsing remains a difficult task and efficient deep learning models on this task have rarely been presented.", "labels": [], "entities": [{"text": "RST-style document-level discourse parsing", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8934497684240341}]}, {"text": "In this paper, we propose an attention-based hierarchical neural network model for discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7443502247333527}]}, {"text": "We also incorporate tensor-based transformation function to model complicated feature interactions.", "labels": [], "entities": []}, {"text": "Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.", "labels": [], "entities": []}], "introductionContent": [{"text": "A document is formed by a series of coherent text units.", "labels": [], "entities": []}, {"text": "Document-level discourse parsing is a task to identify the relations between the text units and to determine the structure of the whole document the text units form.", "labels": [], "entities": [{"text": "Document-level discourse parsing", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7878166238466898}]}, {"text": "Rhetorical Structure Theory (RST) () is one of the most influential discourse theories.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8540436377127966}]}, {"text": "According to RST, the discourse structure of a document can be represented by a Discourse Tree (DT).", "labels": [], "entities": [{"text": "RST", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9237764477729797}]}, {"text": "Each leaf of a DT denotes a text unit referred to as an Elementary Discourse Unit (EDU) and an inner node of a DT represents a text span which is constituted by several adjacent EDUs.", "labels": [], "entities": []}, {"text": "DTs can be utilized by many NLP tasks including automatic document summarization (), question-answering and sentiment analysis) etc.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6205605566501617}, {"text": "sentiment analysis", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.783183753490448}]}, {"text": "Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art approaches heavily rely on manual feature engineering (.", "labels": [], "entities": [{"text": "RSTstyle discourse parsing", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.9318967461585999}]}, {"text": "While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of. propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees.", "labels": [], "entities": [{"text": "RST-style discourse parsing", "start_pos": 219, "end_pos": 246, "type": "TASK", "confidence": 0.9275790055592855}]}, {"text": "However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function they use is weak at modeling complicated interactions which has been stated by.", "labels": [], "entities": []}, {"text": "As many documents contain more than a hundred EDUs which form quite along sequence, those weaknesses may lead to inferior results on this task.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use a hierarchical bidirectional Long Short-Term Memory (bi-LSTM) network to learn representations of text spans.", "labels": [], "entities": []}, {"text": "Comparing with vanilla recursive/recurrent neural networks, LSTM-based networks can store information fora long period of time and don't suffer from gradient vanishing problem.", "labels": [], "entities": []}, {"text": "We apply a hierarchical bi-LSTM network because the way words form an EDU and EDUs form a text span is different and thus they should be modeled separately and hierarchically.", "labels": [], "entities": []}, {"text": "On top of that, we apply attention mechanism to attend overall EDUs to pickup prominent semantic information of a text span.", "labels": [], "entities": []}, {"text": "Besides, we use tensor-based transformation function to model complicated feature interactions and thus it can produce combinatorial features.", "labels": [], "entities": []}, {"text": "We summarize contributions of our work as follows: \u2022 We propose to use a hierarchical bidirectional LSTM network to learn the compositional semantic representations of text spans, which naturally matches and models the intrinsic hierarchical structure of text spans.", "labels": [], "entities": []}, {"text": "\u2022 We extend our hierarchical bi-LSTM network with attention mechanism to allow the network to focus on the parts of input containing prominent semantic information for the compositional representations of text spans and thus alleviate the problem caused by the limited memory of LSTM for long text spans.", "labels": [], "entities": []}, {"text": "\u2022 We adopt a tensor-based transformation function to allow explicit feature interactions and apply tensor factorization to reduce the parameters and computations.", "labels": [], "entities": []}, {"text": "\u2022 We use two level caches to intensively accelerate our probabilistic CKY-like parsing process.", "labels": [], "entities": [{"text": "CKY-like parsing", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.4004160463809967}]}, {"text": "The rest of this paper is organized as follows: Section 2 gives the details of our parsing model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.9694217443466187}]}, {"text": "Section 3 describes our parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9745746850967407}]}, {"text": "Section 4 gives our training criterion.", "labels": [], "entities": []}, {"text": "Section 5 reports the experimental results of our approach.", "labels": [], "entities": []}, {"text": "Section 6 introduces the related work.", "labels": [], "entities": []}, {"text": "Conclusions are given in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on RST Discourse Treebank 1 (RST-DT).", "labels": [], "entities": [{"text": "RST Discourse Treebank 1 (RST-DT)", "start_pos": 25, "end_pos": 58, "type": "DATASET", "confidence": 0.8304288642747062}]}, {"text": "It is partitioned into a set of 347 documents for training and a set of 38 documents for test.", "labels": [], "entities": []}, {"text": "Non-binary relations are converted into a cascade of right-branching binary relations.", "labels": [], "entities": []}, {"text": "The standard metrics of RST-style discourse parsing evaluation include blank tree structure referred to as span (S), tree structure with nuclearity (N) indication and tree structure with rhetorical relation (R) indication.", "labels": [], "entities": [{"text": "RST-style discourse parsing evaluation", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.9394268095493317}]}, {"text": "Following other RSTstyle discourse parsing systems, we evaluate the relation metric in 18 coarse-grained relation classes.", "labels": [], "entities": [{"text": "RSTstyle discourse parsing", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.9196352362632751}]}, {"text": "Since our work focus does not include EDU segmentation, we evaluate our system with gold-standard EDU segmentation and we apply the same setting on this to other discourse parsing systems for fair comparison.", "labels": [], "entities": [{"text": "EDU segmentation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7162109464406967}, {"text": "discourse parsing", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.7472033798694611}]}, {"text": "The dimension of word embeddings is set to be 50 and the dimension of POS embeddings is set to be 10.", "labels": [], "entities": []}, {"text": "We pre-trained the word embeddings with GloVe () on English Gigaword 2 and we fine-tune them during training.", "labels": [], "entities": [{"text": "English Gigaword 2", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.8798021475474039}]}, {"text": "Considering some words are pretrained by GloVe but don't appear in the RST-DT training set, we want to use their embeddings if they appear in test set.", "labels": [], "entities": [{"text": "RST-DT training set", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.7133558392524719}]}, {"text": "Following, we expand our vocabulary with those words using a matrix W \u2208 R 50\u00d750 that maps word embeddings from the pre-trained word embedding space to the fine-tuned word embedding space.", "labels": [], "entities": []}, {"text": "The objective function for training the matrix Wis as follows: where V tuned , V pretrained \u2208 R |V |\u00d750 contain finetuned and pre-trained embeddings of words appearing in training set respectively, |V | is the size of RST-DT training set vocabulary and b is the bias term also to be trained.", "labels": [], "entities": []}, {"text": "We lemmatize all the words appeared and represent all numbers with a special token.", "labels": [], "entities": []}, {"text": "We use Stanford CoreNLP toolkit ( ) to preprocess the text including lemmatization, POS tagging etc.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 7, "end_pos": 31, "type": "DATASET", "confidence": 0.9317743182182312}, {"text": "POS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.8123588860034943}]}, {"text": "We use Theano library ( to implement our parsing model.", "labels": [], "entities": [{"text": "Theano library", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.9536135196685791}]}, {"text": "We randomly initialize all parameters within (-0.012, 0.012) except word embeddings.", "labels": [], "entities": []}, {"text": "We adopt dropout strategy () to avoid overfitting and we set the dropout rate to be 0.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance comparison for different settings of", "labels": [], "entities": []}, {"text": " Table 3: Performance comparison for whether to map OOV", "labels": [], "entities": [{"text": "OOV", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.49059170484542847}]}, {"text": " Table 4: Performance comparison with other state-of-the-art", "labels": [], "entities": []}, {"text": " Table 5: Performance comparison with the deep learning model", "labels": [], "entities": []}]}