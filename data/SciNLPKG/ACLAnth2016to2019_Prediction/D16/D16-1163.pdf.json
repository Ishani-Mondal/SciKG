{"title": [{"text": "Transfer Learning for Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Low-Resource Neural Machine Translation", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.6313833743333817}]}], "abstractContent": [{"text": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.8379653096199036}]}, {"text": "We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9680689573287964}]}, {"text": "Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training.", "labels": [], "entities": []}, {"text": "Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9983325600624084}]}, {"text": "Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair.", "labels": [], "entities": [{"text": "unknown word replacement", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6663197378317515}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9989874958992004}, {"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7335404753684998}, {"text": "syntax based machine translation (SBMT)", "start_pos": 146, "end_pos": 185, "type": "TASK", "confidence": 0.8231547900608608}]}, {"text": "Additionally , using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.", "labels": [], "entities": [{"text": "SBMT", "start_pos": 84, "end_pos": 88, "type": "TASK", "confidence": 0.9346234202384949}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9988683462142944}, {"text": "low-resource machine translation", "start_pos": 157, "end_pos": 189, "type": "TASK", "confidence": 0.6277070045471191}]}], "introductionContent": [{"text": "Neural machine translation (NMT) ( ) is a promising paradigm for extracting translation knowledge from parallel text.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7883860816558202}, {"text": "extracting translation knowledge from parallel text", "start_pos": 65, "end_pos": 116, "type": "TASK", "confidence": 0.8929990033308665}]}, {"text": "NMT systems have achieved competitive accuracy rates under large-data training conditions for language pairs   such as English-French.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9989105463027954}]}, {"text": "However, neural methods are data-hungry and learn poorly from low-count events.", "labels": [], "entities": []}, {"text": "This behavior makes vanilla NMT a poor choice for low-resource languages, where parallel data is scarce.", "labels": [], "entities": []}, {"text": "shows that for 4 low-resource languages, a standard string-to-tree statistical MT system (SBMT) () strongly outperforms NMT, even when NMT uses the state-of-the-art local attention plus feedinput techniques from.", "labels": [], "entities": []}, {"text": "In this paper, we describe a method for substantially improving NMT results on these languages.", "labels": [], "entities": [{"text": "NMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9430626034736633}]}, {"text": "Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model).", "labels": [], "entities": []}, {"text": "We find that we can optimize our results by fixing certain parameters of the parent model and letting the rest be fine-tuned by the child model.", "labels": [], "entities": []}, {"text": "We report NMT improvements from transfer learning of 5.6 BLEU on average, and we provide an analysis of why the method works.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8687542676925659}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9988405108451843}]}, {"text": "The final NMT system approaches strong SBMT baselines in all four language pairs, and exceeds SBMT performance in one of them.", "labels": [], "entities": []}, {"text": "Furthermore, we show that NMT is an exceptional re-scorer of 'traditional' MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a re-scoring model.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9704270362854004}]}, {"text": "We provide a brief description of our NMT model in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 gives some background on transfer learning and explains how we use it to improve machine translation performance.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9719083309173584}, {"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7865106463432312}]}, {"text": "Our main experiments translating Hausa, Turkish, Uzbek, and Urdu into English with the help of a French-English parent model are presented in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 explores alternatives to our model to enhance understanding.", "labels": [], "entities": []}, {"text": "We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language.", "labels": [], "entities": []}, {"text": "We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and that using a translation model trained on bilingual corpora as a parent is essential.", "labels": [], "entities": []}, {"text": "We show the effects of freezing, finetuning, and smarter initialization of different components of the attention-based NMT system during transfer.", "labels": [], "entities": [{"text": "finetuning", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9601299166679382}]}, {"text": "We compare the learning curves of transfer and no-transfer models, showing that transfer solves an overfitting problem, not a search problem.", "labels": [], "entities": []}, {"text": "We summarize our contributions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate how well our transfer method works we apply it to a variety of low-resource languages, both stand-alone and for re-scoring a strong SBMT baseline.", "labels": [], "entities": []}, {"text": "We report large BLEU increases across the board with our transfer method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9995081424713135}]}, {"text": "For all of our experiments with low-resource languages we use French as the parent source language and for child source languages we use Hausa, Turkish, Uzbek, and Urdu.", "labels": [], "entities": []}, {"text": "The target language is always English.", "labels": [], "entities": []}, {"text": "shows parallel training data set sizes for the child languages, where the language with the most data has only 1.8m English tokens.", "labels": [], "entities": []}, {"text": "For comparison, our parent French-English model uses a training set with 300 million English tokens and achieves 26 BLEU on the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9993335604667664}]}, {"text": "Table 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer.", "labels": [], "entities": [{"text": "SBMT", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9078472852706909}, {"text": "NMT baselines", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8762647211551666}]}, {"text": "There is a large gap between the SBMT and NMT systems when our transfer method is not used.", "labels": [], "entities": []}, {"text": "The SBMT system used in this paper is a stringto-tree statistical machine translation system ().", "labels": [], "entities": [{"text": "stringto-tree statistical machine translation", "start_pos": 40, "end_pos": 85, "type": "TASK", "confidence": 0.5940965712070465}]}, {"text": "In this system there are two count-based 5-gram language models.", "labels": [], "entities": []}, {"text": "One is trained on the English side of the WMT 2015 English-French dataset and the other is trained on the English side of the low-resource bitext.", "labels": [], "entities": [{"text": "WMT 2015 English-French dataset", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.8986953347921371}]}, {"text": "Additionally, the SBMT models use thousands of sparsely-occurring, lexicalized syntactic features (  ingrate, parameter initialization range, dropout rate, and hidden state size for all the experiments.", "labels": [], "entities": [{"text": "SBMT", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.8791818618774414}]}, {"text": "For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K, and a source vocabulary size of 30K.", "labels": [], "entities": []}, {"text": "The child models are trained with a dropout probability of 0.5, as in.", "labels": [], "entities": []}, {"text": "The common parent model is trained with a dropout probability of 0.2.", "labels": [], "entities": []}, {"text": "The learning rate used for both child and parent models is 0.5 with a decay rate of 0.9 when the development perplexity does not improve.", "labels": [], "entities": []}, {"text": "The child models are all trained for 100 epochs.", "labels": [], "entities": []}, {"text": "We re-scale the gradient when the gradient norm of all parameters is greater than 5.", "labels": [], "entities": []}, {"text": "The initial parameter range is [-0.08, +0.08].", "labels": [], "entities": []}, {"text": "We also initialize our forget-gate biases to 1 as specified by and.", "labels": [], "entities": []}, {"text": "For decoding we use abeam search of width 12.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Our method significantly improves NMT results for", "labels": [], "entities": [{"text": "NMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9523086547851562}]}, {"text": " Table 4: Data used for a low-resource Spanish-English task.", "labels": [], "entities": []}, {"text": " Table 6: A better match between parent and child languages should improve transfer results. We devised a child language called", "labels": [], "entities": []}, {"text": " Table 7: Starting with the parent French-English model (BLEU =24.4, PPL=6.2), we randomly assign Uzbek word types to French", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9992616772651672}, {"text": "PPL", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9511203765869141}]}, {"text": " Table 8: Transfer for Uzbek-English NMT with parent models", "labels": [], "entities": [{"text": "Transfer", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.8448498845100403}]}]}