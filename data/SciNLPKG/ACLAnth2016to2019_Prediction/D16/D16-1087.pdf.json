{"title": [{"text": "Named Entity Recognition for Novel Types by Transfer Learning", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7088803648948669}]}], "abstractContent": [{"text": "In named entity recognition, we often don't have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.617190808057785}]}, {"text": "In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model.", "labels": [], "entities": []}, {"text": "That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch.", "labels": [], "entities": []}], "introductionContent": [{"text": "There are two main approaches to named entity recognition (NER): (i) build sequence labelling models such as conditional random fields (CRFs) () on a large manually-labelled training corpus (); and (ii) exploit knowledge bases to recognise mentions of entities in text ().", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8001371920108795}]}, {"text": "For many social media-based or security-related applications, however, we cannot assume that we will have access to either of these.", "labels": [], "entities": []}, {"text": "An alternative is to have a small amount of in-domain training data and access to large-scale annotated data in a second domain, and perform transfer learning over both the features and label set.", "labels": [], "entities": []}, {"text": "This is the problem setting in this paper.", "labels": [], "entities": []}, {"text": "NER of novel named entity (NE) types poses two key challenges.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9623942971229553}]}, {"text": "First is the issue of sourcing labelled training data.", "labels": [], "entities": [{"text": "sourcing labelled training", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.9067294200261434}]}, {"text": "Handcrafted features play a key role in supervised NER models), but if we have only limited training amounts of training data, we will be hampered in our ability to reliably learn feature weights.", "labels": [], "entities": [{"text": "NER", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9595282077789307}]}, {"text": "Second, the absence of target NE types in the source domain makes transfer difficult, as we cannot directly apply a model trained over the source domain to the target domain.", "labels": [], "entities": [{"text": "transfer", "start_pos": 66, "end_pos": 74, "type": "TASK", "confidence": 0.9702510237693787}]}, {"text": "show that even if the NE label set is identical across domains, large discrepancies in the label distribution can lead to poor performance.", "labels": [], "entities": []}, {"text": "Despite these difficulties, it is possible to transfer knowledge between domains, as related NE types often share lexical and context features.", "labels": [], "entities": []}, {"text": "For example, the expressions give lectures and attend tutorials often occur near mentions of NE types PROFESSOR and STUDENT.", "labels": [], "entities": []}, {"text": "If only PROFESSOR is observed in the source domain but we can infer that the two classes are similar, we can leverage the training data to learn an NER model for STUDENT.", "labels": [], "entities": [{"text": "STUDENT", "start_pos": 162, "end_pos": 169, "type": "TASK", "confidence": 0.8224495649337769}]}, {"text": "In practice, differences between NE classes are often more subtle than this, but if we can infer, for example, that the novel NE type STUDENT aligns with NE types PERSON and UNIVERSITY, we can compose the context features of PERSON and UNIVERSITY to induce a model for STUDENT.", "labels": [], "entities": []}, {"text": "In this paper, we propose a transfer learning-based approach to NER in novel domains with label mismatch over a source domain.", "labels": [], "entities": [{"text": "NER", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9882158041000366}]}, {"text": "We first train an NER model on a large source domain training corpus, and then learn the correlation between the source and target NE types.", "labels": [], "entities": []}, {"text": "In the last step, we reuse the model parameters of the second step to initialise a linearchain CRF and fine tune it to learn domain-specific patterns.", "labels": [], "entities": []}, {"text": "We show that our methods achieve up to 160% improvement in F-score over a strong baseline, based on only 125 target-domain training sentences.", "labels": [], "entities": [{"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9987485408782959}]}], "datasetContent": [{"text": "We use CADEC () and I2B2 (Ben Abacha and Zweigenbaum, 2011) as target corpora with the standard training and test splits.", "labels": [], "entities": []}, {"text": "From each training set, we holdout 10% as the development set.", "labels": [], "entities": []}, {"text": "As source corpora, we adopt CoNLL (Tjong Kim and.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.6645553112030029}]}, {"text": "In order to test the impact of the target domain training data size on results, we split the training set of CADEC and I2B2 into 10 partitions based on a log scale, and created 10 successively larger training sets by merging these partitions from smallest to largest (with the final merge resulting in the full training set).", "labels": [], "entities": []}, {"text": "For all methods, we report the macro-averaged F1 over only the NE classes that are novel to the target domain.", "labels": [], "entities": [{"text": "F1", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9046826362609863}]}, {"text": "In order to verify if TransInit is able to capture semantic relatedness between source and target NE types, we inspected the parameter matrix W t of the LR classifier in the step of learning type correlations.", "labels": [], "entities": []}, {"text": "The corresponding elements in W t indeed receive much higher values than the semantically-unrelated NE type pairs.", "labels": [], "entities": []}, {"text": "When less than 300 target training sentences are used, these automatically discovered positive correlations directly lead to 10 times higher F1 scores for these types than the baseline Embed, which does not have a transfer learning step.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.985449880361557}]}, {"text": "Since TransInit is able to transfer the knowledge of multiple source types to related target types, this advantage leads to more than 10% improvement in terms of F1 score on these types compared with LabelEmbed, given merely 268 training sentences in I2B2.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.985115259885788}]}, {"text": "We also observe that, in case of few target training examples, LabelEmbed is more robust than CCA if the correlation of types can be inferred from their names.", "labels": [], "entities": []}, {"text": "We study the effects of transferring a large number of source types to target types by using BBN, which has 64 types.", "labels": [], "entities": [{"text": "BBN", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.8471728563308716}]}, {"text": "Here, the novel types of I2B2 w.r.t.", "labels": [], "entities": []}, {"text": "BBN are DOCTOR, PATIENT, HOSPITAL, PHONE, and ID.", "labels": [], "entities": [{"text": "BBN", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.953280508518219}, {"text": "DOCTOR", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9584923386573792}, {"text": "PATIENT", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.984126627445221}, {"text": "HOSPITAL", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9467127919197083}, {"text": "PHONE", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9945482611656189}, {"text": "ID", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9478298425674438}]}, {"text": "For these types, TransInit successfully recognises PERSON as the most related type to DOCTOR, as well as CARDINAL as the most related type to ID.", "labels": [], "entities": [{"text": "CARDINAL", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8086552023887634}]}, {"text": "In contrast, CCA often fails to identify meaningful type alignments, especially for small training data sizes.", "labels": [], "entities": []}, {"text": "CADEC is definitely the most challenging task when trained on CoNLL, because there is no semantic connection between two of the target NE types (DRUG and DISEASE) and any of the source NE types.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.8402798771858215}]}, {"text": "In this case, the baseline LabelEmbed achieves competitive results with TransInit.", "labels": [], "entities": [{"text": "TransInit", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9332661628723145}]}, {"text": "This suggests that the class names reflect semantic correlations between source and target types, and there are not many shared textual patterns between any pair of source and target NE types in the respective datasets.", "labels": [], "entities": []}, {"text": "Even with a complex model such as a neural network, the transfer of knowledge from the source types to the target types is not an easy task.", "labels": [], "entities": []}, {"text": "shows that with a three-layer neural network, the whole model performs poorly.", "labels": [], "entities": []}, {"text": "This is due to the fact that the hard tanh layer suffers from saturated function values.", "labels": [], "entities": []}, {"text": "We inspected the values of the output hidden units computed by W s x on a random sample of target training examples before training on the target corpora.", "labels": [], "entities": []}, {"text": "Most values are either highly positive or negative, which is challenging for online learning algorithms.", "labels": [], "entities": []}, {"text": "This is due to the fact that these hidden units are unnormalised probabilities produced by the source domain classifier.", "labels": [], "entities": []}, {"text": "Therefore, removing the hidden non-linear-layer layer leads to a dramatic performance improvement.", "labels": [], "entities": []}, {"text": "Moreover, also shows that further performance improvement is achieved by reducing the two-layer architecture into a linear chain CRF.", "labels": [], "entities": []}, {"text": "And updating the hidden layers leads to up to 27% higher F1 scores than not updating them in the second step of TransInit, which indicates that the neural networks need to update lower-level features to overcome the covariate shift problem.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9833588898181915}]}], "tableCaptions": []}