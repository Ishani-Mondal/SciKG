{"title": [{"text": "Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction", "labels": [], "entities": []}], "abstractContent": [{"text": "Center-embedding is difficult to process and is known as a rare syntactic construction across languages.", "labels": [], "entities": []}, {"text": "In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited center-embedding.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.709608182311058}]}, {"text": "The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth.", "labels": [], "entities": []}, {"text": "We apply the technique to learning of famous generative model, the dependency model with valence (Klein and Manning , 2004).", "labels": [], "entities": []}, {"text": "Cross-linguistic experiments on Universal Dependencies show that often our method boosts the performance from the base-line, and competes with the current state-of-the-art model in a number of languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human languages in the world are divergent, but they also exhibit many striking similarities.", "labels": [], "entities": []}, {"text": "At the level of syntax, one attractive hypothesis for such regularities is that any grammars of languages have evolved under the pressures, or biases, to avoid structures that are difficult to process.", "labels": [], "entities": []}, {"text": "For example it is known that many languages have a preference for shorter dependencies (, which originates from the difficulty in processing longer dependencies.", "labels": [], "entities": []}, {"text": "Such syntactic regularities can also be useful in applications, in particular in unsupervised ( or weaklysupervised () grammar induction tasks, where the models try to recover the syntactic structure of language without access to the syntactically annotated data, e.g., from raw or partof-speech tagged text only.", "labels": [], "entities": [{"text": "grammar induction tasks", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.8355945150057474}]}, {"text": "In these settings, finding better syntactic regularities universal across languages is essential, as they work as a small cue to the correct linguistic structures.", "labels": [], "entities": []}, {"text": "A preference exploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM (), or model parameters), and this has been the key to success of learning (.", "labels": [], "entities": []}, {"text": "In this paper, we explore the utility for another universal syntactic bias that has not yet been exploited in grammar induction: a bias against centerembedding.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7591660916805267}]}, {"text": "Center-embedding is a syntactic construction on which a clause is embedded into another one.", "labels": [], "entities": []}, {"text": "An example is \"The reporter [who the senator [who Mary met] attacked] ignored the president.\", where \"who Mary met\" is embedded in a larger relative clause.", "labels": [], "entities": []}, {"text": "These constructions are known to cause memory overflow), and also are rarely observed crosslinguistically).", "labels": [], "entities": []}, {"text": "Our learning method exploits this universal property of language.", "labels": [], "entities": []}, {"text": "Intuitively during learning our models explore the restricted search space, which excludes linguistically implausible trees, i.e., those with deeper levels of center-embedding.", "labels": [], "entities": []}, {"text": "We describe how these constraints can be imposed in EM with the inside-outside algorithm.", "labels": [], "entities": []}, {"text": "The central: A set of transitions in left-corner parsing.", "labels": [], "entities": []}, {"text": "The rules on the right side are the side conditions, in which P is the set of rules of a given CFG.", "labels": [], "entities": [{"text": "CFG", "start_pos": 95, "end_pos": 98, "type": "DATASET", "confidence": 0.9420884251594543}]}, {"text": "idea is to tabulate left-corner parsing, on which its stack depth captures the degree of center-embedding of a partial parse.", "labels": [], "entities": [{"text": "tabulate left-corner parsing", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.8272953828175863}]}, {"text": "Each chart item keeps the current stack depth and we discard all items where the depth exceeds some threshold.", "labels": [], "entities": []}, {"text": "The technique is general and can be applicable to any model on PCFG; in this paper, specifically, we describe how to apply the idea on the dependency model with valence (DMV) (), a famous generative model for dependency grammar induction.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 209, "end_pos": 237, "type": "TASK", "confidence": 0.8292784889539083}]}, {"text": "We focus our evaluation on grammar induction from part-of-speech tagged text, comparing the effect of several biases including the one against longer dependencies.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.77590611577034}]}, {"text": "Our main empirical finding is that though two biases, avoiding center-embedding and favoring shorter dependencies, are conceptually similar (both favor simpler grammars), often they capture different aspects of syntax, leading to different grammars.", "labels": [], "entities": []}, {"text": "In particular our bias cooperates well with additional small syntactic cue such as the one that the sentence root tends to be a verb or a noun, with which our models compete with the strong baseline relying on a larger number of hand crafted rules on POS tags.", "labels": [], "entities": []}, {"text": "Our contributions are: the idea to utilize leftcorner parsing fora tool to constrain the models of syntax (Section 3), the formulation of this idea for DMV (Section 4), and cross-linguistic experiments across 25 languages to evaluate the universality of the proposed approach (Sections 5 and 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "A sound evaluation metric in grammar induction is known as an open problem (, which essentially arises from the ambiguity in the notion of head.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7400098443031311}]}, {"text": "For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks The problem is that both trees are correct under some linguistic theories but the standard metric, unlabeled attachment score (UAS), only takes into account the annotation of the current gold data.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 323, "end_pos": 355, "type": "METRIC", "confidence": 0.7663223395744959}]}, {"text": "Our goal in this experiment is to assess the effect of our structural constraints.", "labels": [], "entities": []}, {"text": "To this end, we try to eliminate such arbitrariness in our evaluation as much as possible in the following way: \u2022 We experiment on UD, in which every treebank follows the consistent UD style annotation.", "labels": [], "entities": []}, {"text": "\u2022 We restrict the model to explore only trees that follow the UD style annotation during learning , by prohibiting every function word 6 in a sentence to have any dependents.", "labels": [], "entities": []}, {"text": "\u2022 We calculate UAS in a standard way.", "labels": [], "entities": [{"text": "UAS", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.569473147392273}]}, {"text": "We use UD of version 1.2.", "labels": [], "entities": []}, {"text": "Some treebanks are very small, so we select the top 25 largest languages.", "labels": [], "entities": []}, {"text": "The input to the model is coarse universal POS tags.", "labels": [], "entities": []}, {"text": "All models are trained on sentences of length \u2264 15 and tested on \u2264 40.", "labels": [], "entities": []}, {"text": "Initialization Much previous work of dependency grammar induction relies on the technique called harmonic initialization, which also biases the model towards shorter dependencies ().", "labels": [], "entities": [{"text": "Initialization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9815650582313538}, {"text": "dependency grammar induction", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.8133742610613505}]}, {"text": "Since our focus is to seethe effect of structural constraints, we do not try this and initialize models uniformly.", "labels": [], "entities": []}, {"text": "However, we add a baseline model with this initialization in our comparison to seethe relative strength of our approach.", "labels": [], "entities": []}, {"text": "Models For the baseline, we employ a variant of DMV with features (, which is simple yet known to boost the performance well.", "labels": [], "entities": []}, {"text": "The feature templates are almost the same; the only change is that we add backoff features for STOP probabilities that ignore both direction and adjacency, which we found slightly improves the performance in a preliminary experiment.", "labels": [], "entities": []}, {"text": "We set the regularization parameter to 10 though in practice we found the model is less sensitive to this value.", "labels": [], "entities": []}, {"text": "We run 100 iterations of EM for each setting.", "labels": [], "entities": []}, {"text": "The dif-ference of each model is then the type of constraints imposed during the E-step 7 , or initialization: \u2022 Baseline (FUNC): Function word constraints; \u2022 HARM: FUNC with harmonic initialization; \u2022 DEP: FUNC + stack depth constraints (Eq.", "labels": [], "entities": []}, {"text": "3); \u2022 LEN: FUNC + soft dependency length bias, which we describe below.", "labels": [], "entities": [{"text": "LEN", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9987205862998962}, {"text": "FUNC", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9937481880187988}, {"text": "soft dependency length bias", "start_pos": 18, "end_pos": 45, "type": "METRIC", "confidence": 0.645799532532692}]}, {"text": "For DEP, we use \u03b4 = 1.\u03be to denote the relaxed maximum depth allowing span length up to \u03be.", "labels": [], "entities": [{"text": "DEP", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9494041800498962}]}, {"text": "LEN is the previously explored structural bias (), which penalizes longer dependencies by modifying each attachment score: where \u03b3 (\u2265 0) determines the strength of the bias and |h \u2212 a| is (string) distance between hand a.", "labels": [], "entities": [{"text": "LEN", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9589776396751404}]}, {"text": "Note that DEP and LEN are closely related; generally center-embedded constructions are accompanied by longer dependencies so LEN also penalizes center-embedding implicitly.", "labels": [], "entities": []}, {"text": "However, the opposite is not true and there exist many constructions with longer dependencies without center-embedding.", "labels": [], "entities": []}, {"text": "By comparing these two settings, we discuss the worth of focusing on constraining center-embedding relative to the simpler bias on dependency length.", "labels": [], "entities": []}, {"text": "Finally we also add the system of in our comparison.", "labels": [], "entities": []}, {"text": "This system encodes many manually crafted rules between POS tags with the posterior regularization technique.", "labels": [], "entities": []}, {"text": "For example, the model is encouraged to find NOUN \u2192 ADJ relationship.", "labels": [], "entities": []}, {"text": "Our systems cannot access to these core grammatical rules so it is our strongest baseline.", "labels": [], "entities": []}, {"text": "8 Constraining root word We also seethe effects of the constraints when a small amount of grammatical rule is provided.", "labels": [], "entities": []}, {"text": "In particular, we restrict the candidate root words of the sentence to a noun or a verb; similar rules have been encoded in past work such as and the CCG induction system of.", "labels": [], "entities": []}, {"text": "Hyperparameters Selecting hyperparameters in multilingual grammar induction is difficult; some works tune values for each language based on the development set (, but this violates the assumption of unsupervised learning.", "labels": [], "entities": [{"text": "multilingual grammar induction", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6410474379857382}]}, {"text": "We instead follow many works and select the values with the English data.", "labels": [], "entities": []}, {"text": "For this, we use the WSJ data, which we obtain in UD style from the Stanford CoreNLP (ver. 3.6.0).", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9135936200618744}, {"text": "Stanford CoreNLP (ver. 3.6.0", "start_pos": 68, "end_pos": 96, "type": "DATASET", "confidence": 0.9086811900138855}]}, {"text": "WSJ shows the result on WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9891083240509033}]}, {"text": "Both DEP and LEN have one parameter: the maximum depth \u03b4, and \u03b3 (Eq. 5), and the figure shows the sensitivity on them.", "labels": [], "entities": [{"text": "LEN", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8009775280952454}]}, {"text": "Note that x-axis = 0 represents FUNC.", "labels": [], "entities": [{"text": "FUNC", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.981056272983551}]}, {"text": "For LEN, we can seethe optimal parameter \u03b3 is 0.1, and degrades the performance when increasing the value; i.e., the small bias is the best.", "labels": [], "entities": [{"text": "LEN", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5969256162643433}]}, {"text": "For DEP, we find the best setting is 1.3, i.e., allowing embedded constituents of length 3 or less (\u03be = 3 in Eq. 4).", "labels": [], "entities": [{"text": "DEP", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.847961962223053}]}, {"text": "We can see that allowing depth 2 degrades the performance, indicating that depth 2 allows too many trees and does not reduce the search space effectively.", "labels": [], "entities": []}, {"text": "Multilingual results shows the main multilingual results.", "labels": [], "entities": []}, {"text": "When we see \"No root constraint\" block, we notice that our DEP boosts the performance in many languages (e.g., Bulgarian, French, We then move onto the settings with the constraint on root tags.", "labels": [], "entities": []}, {"text": "Interestingly, in these settings DEP performs the best.", "labels": [], "entities": [{"text": "DEP", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.7789691090583801}]}, {"text": "The model competes with Naseem et al.'s system in average, and outperforms it in many languages, e.g., Bulgarian, Czech, etc.", "labels": [], "entities": []}, {"text": "LEN, on the other hand, decreases the average score.", "labels": [], "entities": [{"text": "LEN", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9965054988861084}, {"text": "average score", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.9728896021842957}]}, {"text": "Analysis Why does DEP perform well in particular with the restriction on root candidates?", "labels": [], "entities": [{"text": "DEP", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8504639267921448}]}, {"text": "To shed light on this, we inspected the output parses of English with no root constraints, and found that the types of errors are very different across constraints.", "labels": [], "entities": []}, {"text": "shows atypical example of the difference.", "labels": [], "entities": []}, {"text": "One difference between trees is in the constructions of phrase \"On ...", "labels": [], "entities": []}, {"text": "LEN predicts that \"On the next two\" comprises a constituent, which modifies \"pictures\" while DEP predicts that \"the ...", "labels": [], "entities": [{"text": "DEP", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.5769458413124084}]}, {"text": "pictures\" comprises a constituent, which is correct, although the head of the determiner is incorrectly predicted.", "labels": [], "entities": []}, {"text": "On the other hand, LEN works well to find more primitive dependency arcs between POS tags, such as arcs from verbs to nouns, which are often incorrectly recognized by DEP.", "labels": [], "entities": []}, {"text": "These observations may partially answer the  question above.", "labels": [], "entities": []}, {"text": "The main source of improvements by DEP is detections of constituents, but this constraint itself does not help to resolve some core dependency relationships, e.g., arcs from verbs to nouns.", "labels": [], "entities": []}, {"text": "The constraint on root POS tags is thus orthogonal to this approach, and it may help to find such core dependencies.", "labels": [], "entities": []}, {"text": "On the other hand, the dependency length bias is the most effective to find basic dependency relationships between POS tags while the resulting tree may involve implausible constituents.", "labels": [], "entities": []}, {"text": "Thus the effect of the length bias seems somewhat overlapped with the root POS constraints, which maybe the reason why they do not well collaborate with each other.", "labels": [], "entities": []}, {"text": "Bracket scores We verify the above intuition quantitatively.", "labels": [], "entities": [{"text": "Bracket", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9835905432701111}]}, {"text": "To this end, we convert both the predicted and gold dependency trees into the unlabeled bracket structures, and then compare them on the standard PARSEVAL metrics.", "labels": [], "entities": []}, {"text": "This bracket tree is not binarized; for example, we extract (X ab (X c d)) from the tree ab c d.  is in Enlgish the bracket and dependency scores are only loosely correlated.", "labels": [], "entities": []}, {"text": "In, UASs for FUNC, DEP, and LEN are 37.2, 39.8, and 52.1, respectively, though F1 of DEP is substantially higher.", "labels": [], "entities": [{"text": "UASs", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9967525005340576}, {"text": "FUNC", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.8580166101455688}, {"text": "LEN", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9959741234779358}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9997932314872742}]}, {"text": "This suggests that DEP often finds more linguistically plausible structures even when the improvement in UAS is modest.", "labels": [], "entities": [{"text": "DEP", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9720706343650818}]}, {"text": "We conjecture that this performance change between constraints essentially arise due to the nature of DEP, which eliminates center-embedding, i.e., implausible constituent structures, rather than dependency arcs.", "labels": [], "entities": []}, {"text": "Combining DEP and LEN These results suggest DEP and LEN capture different aspects of syntax.", "labels": [], "entities": []}, {"text": "To furuther understand this difference, we now evaluate the models with both constraints.", "labels": [], "entities": []}, {"text": "shows the average scores across languages (without root constraints).", "labels": [], "entities": []}, {"text": "Interestingly, the combination (DEP+LEN) performs the best in UAS while the worst in bracket F1.", "labels": [], "entities": [{"text": "DEP", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9641794562339783}, {"text": "LEN", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.5162438154220581}, {"text": "UAS", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.5993072390556335}, {"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9553393125534058}]}, {"text": "This indicates the ability of DEP to find good constituent boundaries is diminished by combining LEN.", "labels": [], "entities": [{"text": "LEN", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.855018675327301}]}, {"text": "We feel the results are expected observing that center-embedded constructions area special case of longer dependency constructions.", "labels": [], "entities": []}, {"text": "In other words, LEN is a stronger constraint than DEP in that the structures penalized by DEP are only a subset of structures penalized by LEN.", "labels": [], "entities": [{"text": "LEN", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.8487220406532288}]}, {"text": "Thus when LEN and DEP are combined LEN overwhelms, and the advantage of DEP is weakened.", "labels": [], "entities": [{"text": "DEP", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.6287420392036438}, {"text": "DEP", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.7541744112968445}]}, {"text": "This also suggests not penalizing all longer dependencies is important for learning accurate grammars.", "labels": [], "entities": []}, {"text": "The improvement of UAS suggests there are also collaborative effects in some aspect.", "labels": [], "entities": [{"text": "UAS", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.5231546759605408}]}], "tableCaptions": [{"text": " Table 1: Attachment scores on UD with or without  root POS constraints. A-Greek = Ancient Greek.  N10 = Naseem et al. (2010) with modified rules.", "labels": [], "entities": []}, {"text": " Table 2: Unlabeled bracket scores in various set- tings. Avg. is the average score across languages.", "labels": [], "entities": [{"text": "Avg.", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9963167905807495}]}, {"text": " Table 3: Average scores of DEP, LEN, and the com- bination.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9871141910552979}, {"text": "DEP", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9380217790603638}, {"text": "LEN", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.995912492275238}]}]}