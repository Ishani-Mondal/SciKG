{"title": [{"text": "Reasoning about Pragmatics with Neural Listeners and Speakers", "labels": [], "entities": [{"text": "Reasoning about Pragmatics with Neural Listeners", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7803934017817179}]}], "abstractContent": [{"text": "We present a model for contrastively describing scenes, in which context-specific behavior results from a combination of inference-driven pragmatics and learned semantics.", "labels": [], "entities": []}, {"text": "Like previous learned approaches to language generation , our model uses a simple feature-driven architecture (here a pair of neural \"lis-tener\" and \"speaker\" models) to ground language in the world.", "labels": [], "entities": [{"text": "language generation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.753034234046936}]}, {"text": "Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances.", "labels": [], "entities": []}, {"text": "For training, our approach requires only ordinary captions, annotated without demonstration of the pragmatic behavior the model ultimately exhibits.", "labels": [], "entities": []}, {"text": "In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to 69% using existing techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a model for describing scenes and objects by reasoning about context and listener behavior.", "labels": [], "entities": []}, {"text": "By incorporating standard neural modules for image retrieval and language modeling into a probabilistic framework for pragmatics, our model generates rich, contextually appropriate descriptions of structured world representations.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7254133522510529}]}, {"text": "This paper focuses on a reference game RG played between a listener Land a speaker S.", "labels": [], "entities": []}, {"text": "1. Reference candidates r 1 and r 2 are revealed to both players.", "labels": [], "entities": []}, {"text": "2. S is secretly assigned a random target t \u2208 {1, 2}.", "labels": [], "entities": []}, {"text": "3. S produces a description d = S(t, r 1 , r 2 ), which is shown to L.", "labels": [], "entities": []}, {"text": "4. L chooses c = L(d, r 1 , r 2 ).", "labels": [], "entities": []}, {"text": "This description mentions a tree, the distinguishing object present in (a) but not in (b), and situates it with respect to other objects and events in the scene.", "labels": [], "entities": []}, {"text": "shows an example drawn from a standard captioning dataset ().", "labels": [], "entities": [{"text": "standard captioning dataset", "start_pos": 30, "end_pos": 57, "type": "DATASET", "confidence": 0.6047268211841583}]}], "datasetContent": [{"text": "We evaluate our model on the reference game RG described in the introduction.", "labels": [], "entities": []}, {"text": "In particular, we construct instances of RG using the Abstract Scenes Dataset introduced by.", "labels": [], "entities": [{"text": "Abstract Scenes Dataset", "start_pos": 54, "end_pos": 77, "type": "DATASET", "confidence": 0.6132495204607645}]}, {"text": "Example scenes are shown in and.", "labels": [], "entities": []}, {"text": "The dataset contains pictures constructed by humans and described in natural language.", "labels": [], "entities": []}, {"text": "Scene representations are available both as rendered images and as feature representations containing the identity and location of each object; as noted in Section 3.1, we use this feature set to produce our referent representation f (r).", "labels": [], "entities": []}, {"text": "This dataset was previously used fora variety of language and vision tasks (e.g.,).", "labels": [], "entities": []}, {"text": "It consists of 10,020 scenes, each annotated with up to 6 captions.", "labels": [], "entities": []}, {"text": "The abstract scenes dataset provides a more challenging version of RG than anything we are aware of in the existing computational pragmatics literature, which has largely used the TUNA corpus of isolated object descriptions ( or small synthetic datasets (.", "labels": [], "entities": [{"text": "RG", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9114378690719604}, {"text": "TUNA corpus", "start_pos": 180, "end_pos": 191, "type": "DATASET", "confidence": 0.9044848382472992}]}, {"text": "By contrast, the abstract scenes data was generated by humans looking at complex images with numerous objects, and features grammatical errors, misspellings, and a vocabulary an order of magnitude larger than TUNA.", "labels": [], "entities": [{"text": "TUNA", "start_pos": 209, "end_pos": 213, "type": "DATASET", "confidence": 0.9012318849563599}]}, {"text": "Unlike previous work, we have no prespecified indomain grammar, and no direct supervision of the relationship between scene features and lexemes.", "labels": [], "entities": []}, {"text": "We perform a human evaluation using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.9472630818684896}]}, {"text": "We begin by holding out a development set and a test set; each held-out set contains 1000 scenes and their accompanying descriptions.", "labels": [], "entities": []}, {"text": "For each held-out set, we construct two sets of 200 paired (target, distractor) scenes: All, with up to four differences between paired scenes, and Hard, with exactly one difference between paired scenes.", "labels": [], "entities": []}, {"text": "(We take the number of differences between scenes to be the number of objects that appear in one scene but not the other.)", "labels": [], "entities": []}, {"text": "We report two evaluation metrics.", "labels": [], "entities": []}, {"text": "Fluency is determined by showing human raters isolated sentences, and asking them to rate linguistic quality on a scale from 1-5.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9590757489204407}]}, {"text": "Accuracy is success rate at RG: as in, humans are shown two images and a model-generated description, and asked to select the image matching the description.", "labels": [], "entities": [{"text": "RG", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.979564368724823}]}, {"text": "In the remainder of this section, we measure the tradeoff between fluency and accuracy that results from different mixtures of the base models (Section 4.1), measure the number of samples needed to obtain good performance from the reasoning listener (Section 4.2), and attempt to approximate the reasoning listener with a monolithic \"compiled\" listener (Section 4.3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9989005327224731}]}, {"text": "In Section 4.4 we report final accuracies for our approach and baselines.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9460039734840393}]}, {"text": "Based on the following sections, we keep \u03bb = 0.02 and use 100 samples to generate predictions.", "labels": [], "entities": []}, {"text": "We evaluate on the test set, comparing this Reasoning model S1 to two baselines: Literal, an image captioning model trained normally on the abstract scene captions (corresponding to our L0), and Contrastive, a model trained with a soft contrastive objective, and previously used for visual referring expression generation (.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 290, "end_pos": 321, "type": "TASK", "confidence": 0.6206066807111105}]}, {"text": "Our reasoning model outperforms both the literal baseline and previous work by a substantial margin, achieving an improvement of 17% on all pairs set and 15% on hard", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: S1 accuracy vs. number of samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9787038564682007}]}, {"text": " Table 2: Success rates at RG on abstract scenes. \"Literal\" is  a captioning baseline corresponding to the base speaker S0.  \"Contrastive\" is a reimplementation of the approach of", "labels": [], "entities": [{"text": "Success", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9798704385757446}, {"text": "RG", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.8280599117279053}]}, {"text": " Table 3: Comparison of the \"compiled\" pragmatic speaker  model with literal and explicitly reasoning speakers. The mod- els are evaluated on subsets of the development set, arranged by  difficulty: column headings indicate the number of differences  between the target and distractor scenes.", "labels": [], "entities": []}]}