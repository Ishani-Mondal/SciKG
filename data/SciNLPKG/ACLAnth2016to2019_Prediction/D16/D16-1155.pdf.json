{"title": [{"text": "Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration", "labels": [], "entities": []}], "abstractContent": [{"text": "To enable language-based communication and collaboration with cognitive robots, this paper presents an approach where an agent can learn task models jointly from language instruction and visual demonstration using an And-Or Graph (AoG) representation.", "labels": [], "entities": []}, {"text": "The learned AoG captures a hierarchical task structure where linguistic labels (for language communication) are grounded to corresponding state changes from the physical environment (for perception and action).", "labels": [], "entities": []}, {"text": "Our empirical results on a cloth-folding domain have shown that, although state detection through visual processing is full of uncertainties and error prone, by a tight integration with language the agent is able to learn an effective AoG for task representation.", "labels": [], "entities": [{"text": "state detection", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.7288353592157364}]}, {"text": "The learned AoG can be further applied to infer and interpret ongoing actions from new visual demonstration using linguistic labels at different levels of granularity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given tremendous advances in robotics, computer vision, and natural language processing, anew generation of cognitive robots have emerged that aim to collaborate with humans in joint tasks.", "labels": [], "entities": []}, {"text": "To facilitate natural and efficient communication with these physical agents, natural language processing will need to go beyond traditional symbolic representations, but rather ground language to sensors (e.g., visual perception) and actuators (e.g., lower-level control systems) of physical agents.", "labels": [], "entities": []}, {"text": "The internal task * The first two authors contributed equally to this paper.", "labels": [], "entities": []}, {"text": "representation will need to capture both higher-level concepts (for language communication) and lowerlevel visual features (for perception and action).", "labels": [], "entities": []}, {"text": "To address this need, we have developed an approach on learning procedural tasks jointly from language instruction and visual demonstration.", "labels": [], "entities": []}, {"text": "In particular, we use And-Or Graph (AoG), which has been used in many computer vision tasks and robotic applications (, to represent a hierarchical task model that not only captures symbolic concepts (extracted from language instructions) but also the corresponding visual state changes from the physical environment (detected by computer vision algorithms).", "labels": [], "entities": []}, {"text": "Different from previous works that ground language to perception (, a key innovation in our framework is that language is no longer grounded just to perceived objects in the environment, but is further grounded to a hierarchical structure of state changes where the states are perceived from the environment during visual demonstration.", "labels": [], "entities": []}, {"text": "The state of environment is an important notion in robotic systems as the change of states drives planning for lower-level robotic actions.", "labels": [], "entities": []}, {"text": "Thus, connecting language concepts to state changes, our learned AoG provides a unified representation that integrates language and vision to not only support language-based communication but also facilitate robot action planning and execution in the future.", "labels": [], "entities": []}, {"text": "More specifically, within this AoG framework, we have developed and evaluated our algorithms in the context of learning a cloth-folding task.", "labels": [], "entities": []}, {"text": "Although cloth-folding appears simple and intuitive for humans, it represents significant challenges for both vision and robotics systems.", "labels": [], "entities": []}, {"text": "Furthermore, although symbolic language processing in this domain is easy due to limited use of vocabulary, grounded language understanding is particularly challenging.", "labels": [], "entities": [{"text": "symbolic language processing", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.7372748255729675}, {"text": "grounded language understanding", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.7221924861272176}]}, {"text": "A simple phrase (e.g., \"fold in half\") could have different grounded meanings (e.g., lower-level representation) given different contexts.", "labels": [], "entities": []}, {"text": "Thus, this clothfolding domain is a good starting point to focus on grounding language to task structures.", "labels": [], "entities": []}, {"text": "Our empirical results have shown that, although state detection from the physical world can be extremely noisy, our learning algorithm that tightly incorporates language is capable of acquiring an effective and meaningful task model to compensate the uncertainties in visual processing.", "labels": [], "entities": [{"text": "state detection", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.743958443403244}]}, {"text": "Once the AoG for the task is learned, it can be applied by our inference algorithm, for example, to infer on-going actions from new visual demonstration and generate linguistic labels at different levels of granularity to facilitate human-agent communication.", "labels": [], "entities": [{"text": "AoG", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9926447868347168}]}], "datasetContent": [{"text": "Using the setting as described in Section 3, we collected 45 T-shirt folding demonstrations from 6 people to evaluate our AoG learning and inference methods.", "labels": [], "entities": [{"text": "T-shirt folding", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7150105535984039}]}, {"text": "More specifically, we conducted a 5-fold cross validation.", "labels": [], "entities": []}, {"text": "In each fold, 36 demonstrations were used for training to learn a task AoG.", "labels": [], "entities": []}, {"text": "Then the remaining 9 demonstrations were used for testing, in which the learned AoG is further applied to process each of the testing visual demonstrations.", "labels": [], "entities": [{"text": "AoG", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9916011691093445}]}, {"text": "Motivated by earlier work on plan/activity recognition using CFG-based models), we use an extrinsic task that automatically assigns linguistic labels to new demonstrations to evaluate the quality of the learned AoG and the effectiveness of the inference algorithm.", "labels": [], "entities": [{"text": "plan/activity recognition", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.6095353960990906}]}, {"text": "This involves three steps: (1) parse the video using the learned AoG; (2) identify linguistic labels associated with terminal or nonterminal nodes in the parse tree; and (3) compare the identified linguistic labels with the manually annotated labels.", "labels": [], "entities": []}, {"text": "We conduct the evaluation at two levels: \u2022 Primitive actions: use linguistic labels associated with terminal nodes to describe the primitive actions in each video.", "labels": [], "entities": []}, {"text": "This level provides detailed descriptions on how the observed task procedure is performed step-by-step.", "labels": [], "entities": []}, {"text": "\u2022 Complex actions: use linguistic labels associated with nonterminal nodes to describe complex actions.", "labels": [], "entities": []}, {"text": "This provides a high-level \"summary\" of the detailed low-level actions.", "labels": [], "entities": []}, {"text": "The capability to recognize fine-grained primitive actions as well as high-level complex actions in a task procedure and to communicate those in language is important for many real-world AI applications such as human-robot collaboration and visual question answering ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 248, "end_pos": 266, "type": "TASK", "confidence": 0.6950137913227081}]}], "tableCaptions": [{"text": " Table 1: Performance of recognizing complex actions using the", "labels": [], "entities": []}]}