{"title": [{"text": "Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data", "labels": [], "entities": [{"text": "Deceptive Review Spam Detection", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.752832256257534}]}], "abstractContent": [{"text": "Existing work on detecting deceptive reviews primarily focuses on feature engineering and applies off-the-shelf supervised classification algorithms to the problem.", "labels": [], "entities": [{"text": "detecting deceptive reviews", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.8623490929603577}, {"text": "feature engineering", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8092491328716278}]}, {"text": "Then, one real challenge would be to manually recognize plentiful ground truth spam review data for model building, which is rather difficult and often requires domain expertise in practice.", "labels": [], "entities": [{"text": "model building", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7588900327682495}]}, {"text": "In this paper, we propose to exploit the related-ness of multiple review spam detection tasks and readily available unlabeled data to address the scarcity of labeled opinion spam data.", "labels": [], "entities": [{"text": "review spam detection", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7213201522827148}]}, {"text": "We first develop a multi-task learning method based on logistic regression (MTL-LR), which can boost the learning fora task by sharing the knowledge contained in the training signals of other related tasks.", "labels": [], "entities": []}, {"text": "To leverage the unlabeled data, we introduce a graph Lapla-cian regularizer into each base model.", "labels": [], "entities": []}, {"text": "We then propose a novel semi-supervised multi-task learning method via Laplacian regular-ized logistic regression (SMTL-LLR) to further improve the review spam detection performance.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 155, "end_pos": 169, "type": "TASK", "confidence": 0.756818413734436}]}, {"text": "We also develop a stochastic alternating method to cope with the optimization for SMTL-LLR.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 82, "end_pos": 90, "type": "TASK", "confidence": 0.9230801463127136}]}, {"text": "Experimental results on real-world review data demonstrate the benefit of SMTL-LLR over several well-established baseline methods.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 74, "end_pos": 82, "type": "TASK", "confidence": 0.9842917919158936}]}], "introductionContent": [{"text": "Nowadays, more and more individuals and organizations have become accustomed to consulting usergenerated reviews before making purchases or online bookings.", "labels": [], "entities": []}, {"text": "Considering great commercial benefits, merchants, however, have tried to hire people to write undeserving positive reviews to promote their own products or services, and meanwhile to post malicious negative reviews to defame those of their competitors.", "labels": [], "entities": []}, {"text": "The fictitious reviews and opinions, which are deliberately created in order to promote or demote targeted entities, are known as deceptive opinion spam (Jindal and).", "labels": [], "entities": []}, {"text": "By formulating deceptive opinion spam detection as a classification problem, existing work primarily focuses on extracting different types of features and applies off-the-shelf supervised classification algorithms to the problem (.", "labels": [], "entities": [{"text": "formulating deceptive opinion spam detection", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.6581335246562958}]}, {"text": "Then, one weakness of previous work lies in the demand of manually recognizing a large amount of ground truth review spam data for model training.", "labels": [], "entities": []}, {"text": "Unlike other forms of spamming activities, such as email or web spam, deceptive opinion spam, which has been deliberately written to sound authentic, is more difficult to be recognized by manual read.", "labels": [], "entities": []}, {"text": "In an experiment, three undergraduate students were (randomly) invited to identify spam reviews from nonspam ones in hotel domain.", "labels": [], "entities": []}, {"text": "As shown in, their average accuracy is merely 57.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994161128997803}]}, {"text": "Then, given a limited set of labeled review data fora domain, e.g., hotel, it is almost impossible to build a robust classification model for detecting deceptive spam reviews in reality.", "labels": [], "entities": []}, {"text": "In this work, we deal with the problem of detecting a textual review as spam or not, i.e., nonspam.", "labels": [], "entities": []}, {"text": "We consider each deceptive review spam detection problem within each domain, e.g., detecting spam hotel/restuarnt reviews from hotel/restaurnat domain, to be a different task.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.6876343190670013}, {"text": "detecting spam hotel/restuarnt reviews from hotel/restaurnat domain", "start_pos": 83, "end_pos": 150, "type": "TASK", "confidence": 0.8417735533280806}]}, {"text": "Previous studies have empirically shown that learning multiple related tasks simultaneously can significantly improve performance relative to learning each task independently, especially when only a few labeled data per task are available).", "labels": [], "entities": []}, {"text": "Thus, given the limited labeled review data for each domain, we formulate the review spam detection tasks for multiple domains, e.g., hotel, restaurant, and soon, as a multi-task learning problem.", "labels": [], "entities": [{"text": "review spam detection", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.6972344915072123}]}, {"text": "We develop a multi-task learning method via logistic regression (MTL-LR) to address the problem.", "labels": [], "entities": []}, {"text": "One key advantage of the method is that it allows to boost the learning for one review spam detection task by leveraging the knowledge contained in the training signals of other related tasks.", "labels": [], "entities": [{"text": "spam detection task", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.788140873114268}]}, {"text": "Then, there is often a large quantity of review data freely available online.", "labels": [], "entities": []}, {"text": "In order to leverage the unlabeled data, we introduce a graph Laplacian regularizer into each base logistic regression model.", "labels": [], "entities": []}, {"text": "We extend MTL-LR, and propose a novel semi-supervised multi-task learning model via Laplacian regularized logistic regression (SMTL-LLR) to further boost the review spam detection performance under the multi-task learning setting.", "labels": [], "entities": [{"text": "MTL-LR", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.6376413106918335}, {"text": "spam detection", "start_pos": 165, "end_pos": 179, "type": "TASK", "confidence": 0.6986943632364273}]}, {"text": "Moreover, to cope with the optimization problem for SMTL-LLR, we also develop a stochastic alternating optimization method, which is computationally efficient.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 52, "end_pos": 60, "type": "TASK", "confidence": 0.9413506388664246}]}, {"text": "To the best of our knowledge, this is the first work that generalizes opinion spam detection from independent single-task learning to symmetric multitask learning setting.", "labels": [], "entities": [{"text": "opinion spam detection", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.8361300230026245}]}, {"text": "By symmetric, we mean that the setting seeks to improve the performance of all learning tasks simultaneously.", "labels": [], "entities": []}, {"text": "In this sense, it is different from transfer learning, where the objective is to improve the performance of a target task using information from source tasks.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.9505196809768677}]}, {"text": "Under this new setting, we can exploit the commonality shared by related review spam detection tasks as well as readily available unlabeled data, and then alleviate the scarcity of labeled spam review data.", "labels": [], "entities": [{"text": "spam detection tasks", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7761787275473276}]}, {"text": "Experimental results on real-world review data demonstrate the superiority of SMTL-LLR over several representative baseline methods.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 78, "end_pos": 86, "type": "TASK", "confidence": 0.9507941007614136}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the proposed methods and stochastic alternating optimization algorithm.", "labels": [], "entities": [{"text": "stochastic alternating optimization", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.6729532678922018}]}, {"text": "Then, in Section 4, we present the experimental results in detail, and conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the proposed multitask learning methods MTL-LR and SMTL-LLR for review spam detection, and demonstrate the improved effectiveness of the methods over other wellestablished baselines.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7738288044929504}]}, {"text": "We followed previous work, and leveraged text unigram and bigram term-frequency features to train our models for review spam detection.", "labels": [], "entities": [{"text": "review spam detection", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7368415693442026}]}, {"text": "This problem setting is quite useful, for example, when user behavior data are sparse or even not available in practical applications.", "labels": [], "entities": []}, {"text": "Supervised classification models, such as logistic regression (LR) and support vector machines (SVM), have been used to identify fake review spam ().", "labels": [], "entities": []}, {"text": "We compared our methods with the two models.", "labels": [], "entities": []}, {"text": "Semisupervised positive-unlabeled (PU) learning was employed for review spam detection, then we chose one representative PU learning method () to evaluate our models.", "labels": [], "entities": [{"text": "spam detection", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.8129100799560547}]}, {"text": "We did not compare our methods with the two-view co-training method, which was used for fake review detection), because the reviewer view data are not available in the ground truth review sets.", "labels": [], "entities": [{"text": "fake review detection", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7183841268221537}]}, {"text": "Instead, we selected a well-known semi-supervised transductive SVM (TSVM)) to evaluate our models.", "labels": [], "entities": []}, {"text": "Different from the proposed methods, we trained each of above baselines in a single domain, because they are single-task learning methods.", "labels": [], "entities": []}, {"text": "Moreover, we also compared our methods with one well-established multi-task learning baseline MTRL (, which has not been used for review spam detection problem.", "labels": [], "entities": [{"text": "MTRL", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.6335867643356323}, {"text": "spam detection", "start_pos": 137, "end_pos": 151, "type": "TASK", "confidence": 0.8167378604412079}]}, {"text": "It is important to specify appropriate values for the parameters in the proposed methods.", "labels": [], "entities": []}, {"text": "In our setting, we used the learning rates \u03b7 t that asymptotically decrease with iteration numbers.", "labels": [], "entities": []}, {"text": "Following previous work, we conducted five-fold cross-validation experiments, and determined the values of the regularization and hyper parameters via a grid-search method.", "labels": [], "entities": []}, {"text": "reports the spam and nonspam review detection accuracy of our methods SMTL-LLR and MTL-LR against all other baseline methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.5504160523414612}, {"text": "SMTL-LLR", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.6821216940879822}]}, {"text": "In terms of 5% significance level, the differences between SMTL-LLR and the baseline methods are considered to be statistically significant.", "labels": [], "entities": [{"text": "significance level", "start_pos": 15, "end_pos": 33, "type": "METRIC", "confidence": 0.9614282250404358}, {"text": "SMTL-LLR", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.9441975355148315}]}, {"text": "Under symmetric multi-task learning setting, our methods SMTL-LLR and MTL-LR outperform all other baselines for identifying spam reviews from nonspam ones.", "labels": [], "entities": []}, {"text": "MTL-LR achieves the average accuracy of 85.2% across the three domains, which is 3.1% and 3.4% better than LR and SVM trained in the single task learning setting, and 1.2% higher than MTRL.", "labels": [], "entities": [{"text": "MTL-LR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5928825736045837}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9955719709396362}, {"text": "MTRL", "start_pos": 184, "end_pos": 188, "type": "DATASET", "confidence": 0.8841370344161987}]}, {"text": "Training with a large quantity of unlabeled review data in addition to labeled ones, SMTL-LLR improves the performance of MTL-LR, and achieves the best average accuracy of 87.2% across the domains, which is 3.2% better than that of MTRL, and is 4.3% better than TSVM, a semi-supervised single task learning model.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.9578273892402649}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.984835147857666}]}, {"text": "PU gives the worst performance, because learning only with partially labeled positive review data (spam) and unlabeled data may not generalize as well as other methods.", "labels": [], "entities": [{"text": "PU", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8425279259681702}]}, {"text": "plots SMTL-LLR accuracy versus unlabeled data sizes from 0 to 10,000, where 0 corresponds to using only labeled data to build the model, i.e., MTL-LR.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 6, "end_pos": 14, "type": "TASK", "confidence": 0.9035671353340149}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9708706736564636}, {"text": "MTL-LR", "start_pos": 143, "end_pos": 149, "type": "DATASET", "confidence": 0.6720056533813477}]}, {"text": "Note that we first randomly sampled 2,000 unlabeled reviews to build the first set, and then created the second set by appending another randomly selected set of 2,000 reviews to the previous one.", "labels": [], "entities": []}, {"text": "We repeated the process until all the unlabeled review data sets were created.", "labels": [], "entities": []}, {"text": "We observed that learning from unlabeled reviews does help to boost the performance of MTL-LR, which was trained with labeled data alone.", "labels": [], "entities": [{"text": "MTL-LR", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.4919089674949646}]}, {"text": "The performance of SMTL-LLR improves when training with more and more unlabeled review data.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.9636859893798828}]}, {"text": "This is because the useful patterns learned from unlabeled data perhaps supports SMTL-LLR to generalize better.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 81, "end_pos": 89, "type": "TASK", "confidence": 0.9193769097328186}]}, {"text": "But continuing to learn from much more unlabeled reviews may even harm the performance.", "labels": [], "entities": []}, {"text": "One explanation is that appending more unlabeled data may also incur noisy information to learning process.", "labels": [], "entities": []}, {"text": "Interestingly, the performance of SMTL-LLR keeps increasing on the doctor domain, when training with more and more unlabeled reviews up to 10,000.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 34, "end_pos": 42, "type": "TASK", "confidence": 0.7975115180015564}]}, {"text": "From above observations, we conclude that an elaborately selected set of high-quality unlabeled review data may help SMTL-LLR to learn better.", "labels": [], "entities": [{"text": "SMTL-LLR", "start_pos": 117, "end_pos": 125, "type": "TASK", "confidence": 0.9768313765525818}]}], "tableCaptions": [{"text": " Table 1: Performance of human judges for review spam  detection in hotel domain (Ott et al., 2011), where F- spam/F-nonspam means F-score for spam/nonspam label.", "labels": [], "entities": [{"text": "review spam  detection", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.5903965334097544}, {"text": "F", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.9423502683639526}, {"text": "F-score", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.9848157167434692}]}, {"text": " Table 3: Spam and nonspam review detection results in  the doctor, hotel, and restaurant review domains.", "labels": [], "entities": []}]}