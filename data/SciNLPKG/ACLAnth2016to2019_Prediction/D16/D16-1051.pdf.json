{"title": [{"text": "Towards a Convex HMM Surrogate for Word Alignment", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.7755658030509949}]}], "abstractContent": [{"text": "Among the alignment models used in statistical machine translation (SMT), the hidden Markov model (HMM) is arguably the most elegant: it performs consistently better than IBM Model 3 and is very close in performance to the much more complex IBM Model 4.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.8176428228616714}]}, {"text": "In this paper we discuss a model which combines the structure of the HMM and IBM Model 2.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.9050251642862955}]}, {"text": "Using this surrogate, our experiments show that we can attain a similar level of alignment quality as the HMM model implemented in GIZA++ (Och and Ney, 2003).", "labels": [], "entities": []}, {"text": "For this model, we derive its convex relaxation and show that it too has strong performance despite not having the local optima problems of non-convex objectives.", "labels": [], "entities": []}, {"text": "In particular, the word alignment quality of this new convex model is significantly above that of the standard IBM Models 2 and 3, as well as the popular (and still non-convex) IBM Model 2 variant of (Dyer et al., 2013).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.5836880058050156}]}], "introductionContent": [{"text": "The IBM translation models are widely used in modern statistical translation systems.", "labels": [], "entities": [{"text": "IBM translation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5679969489574432}, {"text": "statistical translation", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7618655860424042}]}, {"text": "Typically, one seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.7320773243904114}]}, {"text": "Among the IBM Models, perhaps the most elegant is the HMM model ().", "labels": [], "entities": []}, {"text": "The HMM is the last model whose expectation step is * Currently at Google.", "labels": [], "entities": []}, {"text": "\u2020 Currently on leave at Google.", "labels": [], "entities": []}, {"text": "both exact and simple, and it attains a level of accuracy that is very close to the results achieved by much more complex models.", "labels": [], "entities": [{"text": "exact", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9475633502006531}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9994478821754456}]}, {"text": "In particular, experiments have shown that IBM Models 1, 2, and 3 all perform worse than the HMM and Model 4 benefits greatly from being seeded by the HMM.", "labels": [], "entities": []}, {"text": "In this paper we make the following contributions: \u2022 We derive anew alignment model which combines the structure of the HMM and IBM Model 2 and show that its performance is very close to that of the HMM.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 128, "end_pos": 139, "type": "DATASET", "confidence": 0.9045911033948263}]}, {"text": "There are several reasons why such a result would be of value (for more on this, see  and), for example).", "labels": [], "entities": []}, {"text": "\u2022 The main goal of this work is not to eliminate highly non-convex models such as the HMM entirely but, rather, to develop anew, powerful, convex alignment model and thus push the boundary of these theoretically justified techniques further.", "labels": [], "entities": []}, {"text": "Building on the work of (Simion et al., 2015a), we derive a convex relaxation for the new model and show that its performance is close to that of the HMM.", "labels": [], "entities": []}, {"text": "Although it does not beat the HMM, the new convex model improves upon the standard IBM Model 2 significantly.", "labels": [], "entities": []}, {"text": "Moreover, the convex relaxation also performs better than the strong IBM 2 variant FastAlign (), IBM Model 3, and the other available convex alignment models detailed in and ).", "labels": [], "entities": [{"text": "IBM Model 3", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.8394071459770203}]}, {"text": "\u2022 We derive a parameter estimation algorithm for new model and its convex relaxation based on the EM algorithm.", "labels": [], "entities": []}, {"text": "Our model has both HMM emission probabilities and IBM Model 2's distortions, so we can use Model 2 to seed both the model's lexical and distortion parameters.", "labels": [], "entities": [{"text": "HMM emission probabilities", "start_pos": 19, "end_pos": 45, "type": "METRIC", "confidence": 0.6554024418195089}]}, {"text": "For the convex model, we need not use any initialization heuristics since the EM algorithm we derive is guaranteed to converge to a local optima that is also global.", "labels": [], "entities": []}, {"text": "The goal of our work is to present a model which is convex and has state of the art empirical performance.", "labels": [], "entities": []}, {"text": "Although one step of this task was achieved for IBM Model 2 (), our target goal deals with a much more local-optima-laden, non-convex objective.", "labels": [], "entities": []}, {"text": "Finally, whereas IBM 2 in some ways leads to a clear method of attack, we will discuss why the HMM presents challenges that require the insertion of this new surrogate.", "labels": [], "entities": []}, {"text": "We adopt the notation introduced in) of having 1 m 2 n denote the training scheme of m IBM Model 1 EM iterations followed by initializing Model 2 with these parameters and running n IBM Model 2 EM iterations.", "labels": [], "entities": []}, {"text": "We denote by H the HMM and note that it too can be seeded by running Model 1 followed by Model 2.", "labels": [], "entities": []}, {"text": "Additionally, we denote our model as 2 H , and note that it has distortion parameters like IBM Model 2 and emission parameters like that of the HMM.", "labels": [], "entities": [{"text": "distortion", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9640496969223022}, {"text": "IBM Model 2", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.9185881416002909}]}, {"text": "Under this notation, we let 1 m 2 n 2 o H denote running Model 1 form iterations, then Model 2 for n iteration, and then finally our Model for o iterations.", "labels": [], "entities": []}, {"text": "As before, we are seeding from the more basic to the more complex model in turn.", "labels": [], "entities": []}, {"text": "We denote the convex relaxation of 2 H by 2 HC . Throughout this paper, for any integer N , we use [N ] to denote {1 . .", "labels": [], "entities": []}, {"text": "N } and [N ] 0 to denote {0 . .", "labels": [], "entities": []}, {"text": "Finally, in our presentation, \"convex function\" means a function for which a local maxima also global, for example, f (x) = \u2212x 2 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Alignment quality results for IBM2-HMM (2H) and", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.915258526802063}]}, {"text": " Table 2: Alignment quality results for IBM2-HMM and its", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9167003035545349}]}]}