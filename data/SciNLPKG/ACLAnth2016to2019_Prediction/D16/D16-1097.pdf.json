{"title": [{"text": "Neural Morphological Analysis: Encoding-Decoding Canonical Segments", "labels": [], "entities": [{"text": "Neural Morphological Analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8746116558710734}, {"text": "Canonical Segments", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7638911008834839}]}], "abstractContent": [{"text": "Canonical morphological segmentation aims to divide words into a sequence of standardized segments.", "labels": [], "entities": [{"text": "Canonical morphological segmentation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.764840563138326}]}, {"text": "In this work, we propose a character-based neural encoder-decoder model for this task.", "labels": [], "entities": []}, {"text": "Additionally, we extend our model to include morpheme-level and lexical information through a neural reranker.", "labels": [], "entities": []}, {"text": "We set the new state of the art for the task improving previous results by up to 21% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9988800883293152}]}, {"text": "Our experiments cover three languages: English, German and Indonesian.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological segmentation aims to divide words into morphemes, meaning-bearing sub-word units.", "labels": [], "entities": [{"text": "Morphological segmentation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9378524124622345}]}, {"text": "Indeed, segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (), keyword spotting (), machine translation () and parsing (Seeker and C \u00b8 etino\u02d8 glu, 2015).", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.6160752177238464}, {"text": "keyword spotting", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7664844393730164}, {"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.807344913482666}]}, {"text": "In the literature, most research has traditionally focused on surface segmentation, whereby a word w is segmented into a sequence of substrings whose concatenation is the entire word; see fora survey.", "labels": [], "entities": [{"text": "surface segmentation", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7757124602794647}]}, {"text": "In contrast, we consider canonical segmentation: w is divided into a sequence of standardized segments.", "labels": [], "entities": []}, {"text": "To make the difference concrete, consider the following example: the surface segmentation of the complex English word achievability is achiev+abil+ity, whereas its canonical segmentation is achieve+able+ity, i.e., we restore the alterations made during word formation.", "labels": [], "entities": []}, {"text": "Canonical versions of morphological segmentation have been introduced multiple times in the literature.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.9477646946907043}]}, {"text": "Canonical segmentation has several representational advantages over surface segmentation, e.g., whether two words share a morpheme is no longer obfuscated by orthography.", "labels": [], "entities": [{"text": "Canonical segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8806762397289276}]}, {"text": "However, it also introduces a hard algorithmic challenge: in addition to segmenting a word, we must reverse orthographic changes, e.g., mapping achievability \u2192achieveableity.", "labels": [], "entities": []}, {"text": "Computationally, canonical segmentation can be seen as a sequence-to-sequence problem: we must map a word form to a canonicalized version with segmentation boundaries.", "labels": [], "entities": [{"text": "canonical segmentation", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7379220128059387}]}, {"text": "Inspired by the recent success of neural encoder-decoder models) for sequence-to-sequence problems in NLP, we design a neural architecture for the task.", "labels": [], "entities": []}, {"text": "However, a na\u00a8\u0131vena\u00a8\u0131ve application of the encoder-decoder model ignores much of the linguistic structure of canonical segmentation-it cannot directly model the individual canonical segments, e.g., it cannot easily produce segment-level embeddings.", "labels": [], "entities": []}, {"text": "To solve this, we use a neural reranker on top of the encoder-decoder, allowing us to embed both characters and entire segments.", "labels": [], "entities": []}, {"text": "The combined approach outperforms the state of the art by a wide margin (up to 21% accuracy) in three languages: English, German and Indonesian.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.999169111251831}]}], "datasetContent": [{"text": "To enable comparison to earlier work, we use a dataset that was prepared by for canonical segmentation.", "labels": [], "entities": [{"text": "canonical segmentation", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.622084379196167}]}], "tableCaptions": []}