{"title": [{"text": "Creating Causal Embeddings for Question Answering with Minimal Supervision", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.811208963394165}]}], "abstractContent": [{"text": "A common model for question answering (QA) is that a good answer is one that is closely related to the question, where re-latedness is often determined using general-purpose lexical models such as word embed-dings.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8953573346138001}]}, {"text": "We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which maybe determined through task-specific embeddings.", "labels": [], "entities": []}, {"text": "With causality as a use case, we implement this insight in three steps.", "labels": [], "entities": []}, {"text": "First, we generate causal embeddings cost-effectively by boot-strapping cause-effect pairs extracted from free text using a small set of seed patterns.", "labels": [], "entities": []}, {"text": "Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect.", "labels": [], "entities": []}, {"text": "Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embed-dings.", "labels": [], "entities": []}, {"text": "We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo!", "labels": [], "entities": []}, {"text": "We show that explicitly modeling causality improves performance in both tasks.", "labels": [], "entities": []}, {"text": "In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).", "labels": [], "entities": [{"text": "P", "start_pos": 45, "end_pos": 46, "type": "METRIC", "confidence": 0.9983910322189331}]}], "introductionContent": [{"text": "Question answering (QA), i.e., finding short answers to natural language questions, is one of the most important but challenging tasks on the road towards natural language understanding.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.932075846195221}, {"text": "finding short answers to natural language questions", "start_pos": 31, "end_pos": 82, "type": "TASK", "confidence": 0.6842132040432521}, {"text": "natural language understanding", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.6618844370047251}]}, {"text": "A common approach for QA is to prefer answers that are closely related to the question, where relatedness is often determined using lexical semantic models such as word embeddings.", "labels": [], "entities": [{"text": "QA", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9792659878730774}]}, {"text": "While appealing for its robustness to natural language variation, this onesize-fits-all approach does not take into account the wide range of distinct question types that can appear in any given question set, and that are best addressed individually (.", "labels": [], "entities": []}, {"text": "Given the variety of question types, we suggest that a better approach is to look for answers that are related to the question through the appropriate relation, e.g., a causal question should have a causeeffect relation with its answer.", "labels": [], "entities": []}, {"text": "If we adopt this view, and continue to work with embeddings as a mechanism for assessing relationship, this raises a key question: how do we train and use task-specific embeddings cost-effectively?", "labels": [], "entities": []}, {"text": "Using causality as a use case, we answer this question with a framework for producing causal word embeddings with minimal supervision, and a demonstration that such taskspecific embeddings significantly benefit causal QA.", "labels": [], "entities": []}, {"text": "In particular, the contributions of this work are: (1) A methodology for generating causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns, e.g., X causes Y.", "labels": [], "entities": []}, {"text": "We then train dedicated embedding (as well as two other distributional similarity) models over this data.", "labels": [], "entities": []}, {"text": "have modified the algorithm of to use an arbitrary, rather than linear, context.", "labels": [], "entities": []}, {"text": "Here we make this context task-specific, i.e., the context of a cause is its effect.", "labels": [], "entities": []}, {"text": "Further, to mitigate sparsity and noise, our models are bidirectional, and noise aware (by incorporating the likelihood of noise in the training process).", "labels": [], "entities": []}, {"text": "(2) The insight that QA benefits from task-specific embeddings.", "labels": [], "entities": []}, {"text": "We implement a QA system that uses the above causal embeddings to answer questions and demonstrate that they significantly improve performance over a strong baseline.", "labels": [], "entities": []}, {"text": "Further, we show that causal embeddings encode complementary information to vanilla embeddings, even when trained from the same knowledge resources.", "labels": [], "entities": []}, {"text": "(3) An analysis of direct vs. indirect evaluations for task-specific word embeddings.", "labels": [], "entities": []}, {"text": "We evaluate our causal models both directly, in terms of measuring their capacity to rank causally-related word pairs over word pairs of other relations, as well as indirectly in the downstream causal QA task.", "labels": [], "entities": []}, {"text": "In both tasks, our analysis indicates that including causal models significantly improves performance.", "labels": [], "entities": []}, {"text": "However, from the direct evaluation, it is difficult to estimate which models will perform best in realworld tasks.", "labels": [], "entities": []}, {"text": "Our analysis re-enforces recent observations about the limitations of word similarity evaluations (: we show that they have limited coverage and may align poorly with real-world tasks.", "labels": [], "entities": [{"text": "word similarity evaluations", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.834608793258667}]}], "datasetContent": [{"text": "We begin the assessment of our models with a direct evaluation to determine whether or not the proposed approaches capture causality better than generalpurpose word embeddings and whether their robustness improves upon a simple database look-up.", "labels": [], "entities": []}, {"text": "For this evaluation, we follow the protocol of.", "labels": [], "entities": []}, {"text": "In particular, we create a collection of word pairs, half of which are causally related, with the other half consisting of other relations.", "labels": [], "entities": []}, {"text": "These pairs are then ranked by our models and several baselines, with the goal of ranking the causal pairs above the others.", "labels": [], "entities": []}, {"text": "The embedding models rank the pairs using the cosine similarity between the target vector for the causal word and the context vector of the effect word.", "labels": [], "entities": []}, {"text": "The alignment model ranks pairs using the probability P (Effect|Cause) given by IBM Model 1, and the CNN ranks pairs by the value of the output returned by the network.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.9428851008415222}]}, {"text": "The main objective of our work is to investigate the impact of a customized causal embedding model for QA.", "labels": [], "entities": []}, {"text": "Following our direct evaluation, which solely evaluated the degree to which our models directly encode causality, here we evaluate each of our proposed causal models in terms of their contribution to a downstream real-world QA task.", "labels": [], "entities": []}, {"text": "Our QA system uses a standard reranking approach ().", "labels": [], "entities": []}, {"text": "In this architecture, the candidate answers are initially extracted and ranked using a shallow candidate retrieval (CR) component that uses solely information retrieval techniques, then they are re-ranked using a \"learning to rank\" approach.", "labels": [], "entities": []}, {"text": "In particular, we used SVM rank 8 , a Support Vector Machines classifier adapted for ranking, and re-ranked the candidate answers with a set of features derived from both the initial CR score and the models we have introduced.", "labels": [], "entities": []}, {"text": "For our model combinations (see), the feature set includes the CR score and the features from each of the models in the combination.", "labels": [], "entities": [{"text": "CR score", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9828213155269623}]}], "tableCaptions": []}