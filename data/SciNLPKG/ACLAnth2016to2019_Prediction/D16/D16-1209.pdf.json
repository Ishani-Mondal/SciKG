{"title": [], "abstractContent": [{"text": "We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs.", "labels": [], "entities": []}, {"text": "Our model has agate that adaptively finds the optimal mixture of the character-level and word-level inputs.", "labels": [], "entities": []}, {"text": "The gate creates the final vector representation of a word by combining two distinct representations of the word.", "labels": [], "entities": []}, {"text": "The character-level inputs are converted into vector representations of words using a bidirec-tional LSTM.", "labels": [], "entities": []}, {"text": "The word-level inputs are projected into another high-dimensional space by a word lookup table.", "labels": [], "entities": []}, {"text": "The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words.", "labels": [], "entities": []}, {"text": "Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural networks (RNNs) achieve state-ofthe-art performance on fundamental tasks of natural language processing (NLP) such as language modeling (RNN-LM) (.", "labels": [], "entities": []}, {"text": "RNN-LMs are usually based on the wordlevel information or subword-level information such as characters (, and predictions are made at either word level or subword level respectively.", "labels": [], "entities": []}, {"text": "In word-level LMs, the probability distribution over the vocabulary conditioned on preceding words is computed at the output layer using a softmax function.", "labels": [], "entities": []}, {"text": "Word-level LMs require a predefined vocabulary size since the computational complexity of a softmax function grows with respect to the vocabulary size.", "labels": [], "entities": []}, {"text": "This closed vocabulary approach tends to ignore rare words and typos, as the words do not appear in the vocabulary are replaced with an outof-vocabulary (OOV) token.", "labels": [], "entities": []}, {"text": "The words appearing in vocabulary are indexed and associated with highdimensional vectors.", "labels": [], "entities": []}, {"text": "This process is done through a word lookup table.", "labels": [], "entities": []}, {"text": "Although this approach brings a high degree of freedom in learning expressions of words, information about morphemes such as prefix, root, and suffix is lost when the word is converted into an index.", "labels": [], "entities": []}, {"text": "Also, word-level language models require some heuristics to differentiate between the OOV words, otherwise it assigns the exactly same vector to all the OOV words.", "labels": [], "entities": []}, {"text": "These are the major limitations of word-level LMs.", "labels": [], "entities": []}, {"text": "In order to alleviate these issues, we introduce an RNN-LM that utilizes both character-level and word-level inputs.", "labels": [], "entities": []}, {"text": "In particular, our model has agate that adaptively choose between two distinct ways to represent each word: a word vector derived from the character-level information and a word vector stored in the word lookup table.", "labels": [], "entities": []}, {"text": "This gate is trained to make this decision based on the input word.", "labels": [], "entities": []}, {"text": "According to the experiments, our model with the gate outperforms other models on the Penn Treebank (PTB), BBC, and IMDB Movie Review datasets.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.973145580291748}, {"text": "BBC", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.8075720071792603}, {"text": "IMDB Movie Review datasets", "start_pos": 116, "end_pos": 142, "type": "DATASET", "confidence": 0.9320566952228546}]}, {"text": "Also, the trained gating values show that the gating mechanism effectively utilizes the character-level information when it encounters rare words.", "labels": [], "entities": []}, {"text": "Related Work Character-level language models that make word-level prediction have recently been proposed.", "labels": [], "entities": [{"text": "word-level prediction", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.770713746547699}]}, {"text": "introduce the compositional character-to-word (C2W) model that takes as input character-level representation of a word and generates vector representation of the word using a bidirectional LSTM ().", "labels": [], "entities": []}, {"text": "propose a convolutional neural network (CNN) based character-level language model and achieve the state-of-the-art perplexity on the PTB dataset with a significantly fewer parameters.", "labels": [], "entities": [{"text": "PTB dataset", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.9810231924057007}]}, {"text": "Moreover, word-character hybrid models have been studied on different NLP tasks.", "labels": [], "entities": []}, {"text": "apply a word-character hybrid language model on Chinese using a neural network language model (.", "labels": [], "entities": []}, {"text": "produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.736159473657608}]}, {"text": "investigate RNN models that predict characters based on the character and word level inputs.", "labels": [], "entities": []}, {"text": "present word-character hybrid neural machine translation systems that consult the character-level information for rare words.", "labels": [], "entities": [{"text": "word-character hybrid neural machine translation", "start_pos": 8, "end_pos": 56, "type": "TASK", "confidence": 0.6573227286338806}]}], "datasetContent": [{"text": "As the standard metric for language modeling, perplexity (PPL) is used to evaluate the model performance.", "labels": [], "entities": [{"text": "perplexity (PPL)", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.7727377265691757}]}, {"text": "Perplexity over the test set is computed as PPL = exp , where N is the number of words in the test set, and p (w i |w <i ) is the conditional probability of a word w i given all the preceding words in a sentence.", "labels": [], "entities": []}, {"text": "We use Theano (2016) to implement all the models.", "labels": [], "entities": [{"text": "Theano (2016)", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.9649549424648285}]}, {"text": "The code for the models is available from https://github.com/ nyu-dl/gated_word_char_rlm.", "labels": [], "entities": []}, {"text": "Penn Treebank We use the Penn Treebank Corpus () preprocessed by.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9895786345005035}, {"text": "Penn Treebank Corpus", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.9963374932607015}]}, {"text": "We use 10k most frequent words and 51 characters.", "labels": [], "entities": []}, {"text": "In the training phase, we use only sentences with less than 50 words.", "labels": [], "entities": []}, {"text": "BBC We use the BBC corpus prepared by.", "labels": [], "entities": [{"text": "BBC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9318605661392212}, {"text": "BBC corpus prepared", "start_pos": 15, "end_pos": 34, "type": "DATASET", "confidence": 0.9757420221964518}]}, {"text": "We use 10k most frequent words and 62 characters.", "labels": [], "entities": []}, {"text": "In the training phase, we use sentences with less than 50 words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Validation and test perplexities on Penn Treebank (PTB), BBC, IMDB Movie Reviews datasets.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.9774466514587402}, {"text": "BBC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9152134656906128}, {"text": "IMDB Movie Reviews datasets", "start_pos": 72, "end_pos": 99, "type": "DATASET", "confidence": 0.9195166826248169}]}]}