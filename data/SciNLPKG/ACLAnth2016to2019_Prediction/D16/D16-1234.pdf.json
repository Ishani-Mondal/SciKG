{"title": [{"text": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment", "labels": [], "entities": [{"text": "Lexical Entailment", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8128238320350647}]}], "abstractContent": [{"text": "We consider the task of predicting lexical entailment using distributional vectors.", "labels": [], "entities": [{"text": "predicting lexical entailment", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.9199268221855164}]}, {"text": "We perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs.", "labels": [], "entities": []}, {"text": "We find that the model strongly learns to identify hypernyms using Hearst patterns, which are well known to be predictive of lexical relations.", "labels": [], "entities": []}, {"text": "We present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to Principal Component Analysis.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7221391201019287}, {"text": "Principal Component Analysis", "start_pos": 125, "end_pos": 153, "type": "TASK", "confidence": 0.6320149103800455}]}, {"text": "Our model combines the extracted features with the strengths of other proposed models in the literature, and matches or outperforms prior work on multiple data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the field of Natural Language Processing has developed, more ambitious semantic tasks are starting to be addressed, such as Question Answering (QA) and Recognizing Textual Entailment (RTE).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.8210554420948029}, {"text": "Recognizing Textual Entailment (RTE)", "start_pos": 155, "end_pos": 191, "type": "TASK", "confidence": 0.7963584760824839}]}, {"text": "These systems often depend on the use of lexical resources like WordNet in order to infer entailments for individual words, but these resources are expensive to develop, and always have limited coverage.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9705982804298401}]}, {"text": "To address these issues, many works have considered on how lexical entailments can be derived automatically using distributional semantics.", "labels": [], "entities": []}, {"text": "Some focus mostly on the use of unsupervised techniques, and study measures which emphasize particular word relations ().", "labels": [], "entities": []}, {"text": "Many are based on the Distributional Inclusion Hypothesis, which states that the contexts in which a hypernym appears area superset of its hyponyms' contexts.", "labels": [], "entities": []}, {"text": "More recently, a great deal of work has pushed toward using supervised methods (), varying by their experimental setup or proposed model.", "labels": [], "entities": []}, {"text": "Yet the literature disagrees about which models are strongest (, or even if they work at all ( ).", "labels": [], "entities": []}, {"text": "Indeed,  showed that two existing lexical entailment models fail to account for similarity between the antecedent and consequent, and conclude that such models are only learning to predict prototypicality: that is, they predict that cat entails animal because animal is usually entailed, and therefore will also predict that sofa entails animal.", "labels": [], "entities": []}, {"text": "Yet it remains unclear why such models make for such strong baselines (.", "labels": [], "entities": []}, {"text": "We present a novel qualitative analysis of one prototypicality classifier, giving new insight into why prototypicality classifiers perform strongly in the literature.", "labels": [], "entities": []}, {"text": "We find the model overwhelmingly learns to identify hypernyms using Hearst patterns available in the distributional space, like \"animals such as cats\" and \"animals including cats.\"", "labels": [], "entities": []}, {"text": "These patterns have long been used to identify lexical relations).", "labels": [], "entities": []}, {"text": "We propose a novel model which exploits this behavior as a method of feature extraction, which we call H-feature detectors.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7295930236577988}]}, {"text": "Using an iterative procedure similar to Principal Component Analysis, our model is able to extract and learn using multiple Hfeature detectors.", "labels": [], "entities": [{"text": "Principal Component Analysis", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.626896987358729}]}, {"text": "Our model also integrates overall word similarity and Distributional Inclusion, bringing together strengths of several models in the literature.", "labels": [], "entities": [{"text": "Distributional Inclusion", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.6971356272697449}]}, {"text": "Our model matches or outperforms prior work on multiple data sets.", "labels": [], "entities": []}, {"text": "The code, data sets, and model predictions are made available for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use a variation of 20-fold cross validation which accounts for lexical overlap.", "labels": [], "entities": []}, {"text": "To simplify explanation, we first explain how we generate splits for training/testing, and then afterwards introduce validation methodology.", "labels": [], "entities": []}, {"text": "We first pool all the words from the antecedent (LHS) side of the data into a set, and split these lexical items into 20 distinct cross-validation folds.", "labels": [], "entities": []}, {"text": "For each fold F i , we then use all pairs (w, H) where w \u2208 F i as the test set pairs.", "labels": [], "entities": []}, {"text": "That is, if \"car\" is in the test set fold, then \"car \u2192 vehicle\" and \"car truck\" will appear as test set pairs.", "labels": [], "entities": []}, {"text": "The training set will then be every pair which does not contain any overlap with the test set; e.g. the training set will be all pairs which do not contain \"car\", \"truck\" or \"vehicle\" as either the antecedent or consequent.", "labels": [], "entities": []}, {"text": "This ensures that both (1) there is zero lexical overlap between training and testing and (2) every pair is used as an item in a test fold exactly once.", "labels": [], "entities": []}, {"text": "One quirk of this setup is that all test sets are approximately the same size, but training sizes vary dramatically.", "labels": [], "entities": []}, {"text": "This setup differs from those of previous works like and , who both use single, fixed train/test/val sets without lexical overlap.", "labels": [], "entities": []}, {"text": "We find our setup has several advantages over fixed sets.", "labels": [], "entities": []}, {"text": "First, we find there can be considerable variance if the train/test set is regenerated with a different random seed, indicating that multiple trials are necessary.", "labels": [], "entities": []}, {"text": "Second, fixed setups consistently discard roughly half the data as ineligible for either training or test, as lexical items appear in many pairs.", "labels": [], "entities": []}, {"text": "Our CV-like setup allows us to evaluate performance over every item in the data set exactly once, making a much more efficient and representative use of the original data set.", "labels": [], "entities": []}, {"text": "Our performance metric is F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9791432917118073}]}, {"text": "This is more  representative than accuracy, as most of the data sets are heavily unbalanced.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995198249816895}]}, {"text": "We report the mean F1 scores across all cross validation folds.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9797094464302063}]}, {"text": "In order to evaluate how important each of the various F features are to the model, we also performed an ablation experiment where the classifier is not given the similarity (slot 1), prototype H-feature detectors (slots 2 and 3) or the inclusion features (slot 4).", "labels": [], "entities": []}, {"text": "To evaluate the importance of these features, we fix the regularization parameter at C = 1, and train all ablated classifiers on each training fold with number of iterations n = 1, . .", "labels": [], "entities": []}, {"text": "shows the decrease (absolute difference) in performance between the full and ablated models on the development sets, so higher numbers indicate greater feature importance.", "labels": [], "entities": []}, {"text": "We find the similarity feature is extremely important in the LEDS, BLESS and Medical data sets, therefore reinforcing the findings of . The similarity feature is especially important in the LEDS and BLESS data sets, where negative examples include many random pairs.", "labels": [], "entities": [{"text": "LEDS", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8873838186264038}, {"text": "BLESS", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9194424748420715}, {"text": "Medical data sets", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.9610554774602255}, {"text": "BLESS data sets", "start_pos": 199, "end_pos": 214, "type": "DATASET", "confidence": 0.929667055606842}]}, {"text": "The detector features are moderately important for the Medical and TM14 data sets, and critically important on BLESS, where we found the strongest evi-4 Bootstrap test, p < .01.", "labels": [], "entities": [{"text": "Medical and TM14 data sets", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.8872026801109314}, {"text": "BLESS", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9509202837944031}]}, {"text": "dence of Hearst patterns in the H-feature detectors.", "labels": [], "entities": []}, {"text": "Surprisingly, the detector features are moderately detrimental on the LEDS data set, though this can also be understood in the data set's construction: since the negative examples are randomly shuffled positive examples, the same detector signal will appear in both positive and negative examples.", "labels": [], "entities": [{"text": "LEDS data set", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.8899394273757935}]}, {"text": "Finally, we find the model performs somewhat robustly without the inclusion feature, but still is moderately impactful on three of the four data sets, lending further evidence to the Distributional Inclusion Hypothesis.", "labels": [], "entities": []}, {"text": "In general, we find all three components are valuable sources of information for identifying hypernymy and lexical entailment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Mean F1 scores for each model and data set.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.968759298324585}, {"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.7547313570976257}]}, {"text": " Table 4: Absolute decrease in mean F1 on the development", "labels": [], "entities": [{"text": "Absolute decrease in mean", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.862924799323082}, {"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.7681719064712524}]}, {"text": " Table 5: Most similar contexts to the H-feature detector for each iteration of the PCA-like procedure. This model was trained on all", "labels": [], "entities": []}]}