{"title": [{"text": "Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English Speakers do on Twitter?", "labels": [], "entities": []}], "abstractContent": [{"text": "Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010).", "labels": [], "entities": []}, {"text": "Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers.", "labels": [], "entities": [{"text": "Paucity", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.879610538482666}]}, {"text": "In this paper, we report a study on 430,000 unique tweets from Indian users, specifically Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment.", "labels": [], "entities": []}, {"text": "To this end, we develop classifiers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments.", "labels": [], "entities": [{"text": "opinion detection", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7324248254299164}]}, {"text": "Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing.", "labels": [], "entities": []}, {"text": "As an aside, we explore some common pragmatic functions of code-switching through sentiment detection.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.9446168839931488}]}], "introductionContent": [{"text": "The pattern of language use in a multilingual society is a complex interplay of socio-linguistic, discursive and pragmatic factors.", "labels": [], "entities": []}, {"text": "Sometimes speakers have a preference fora particular language for certain conversational and discourse settings; on other occasions, there is fluid alteration between two or more languages in a single conversation, also known as Code-switching (CS) or Code-mixing . Under- * * This work was done when the author was a Research Fellow at Microsoft Research Lab India.", "labels": [], "entities": []}, {"text": "Although some linguists differentiate between Codeswitching and Code-mixing, this paper will use the two terms interchangeably.", "labels": [], "entities": []}, {"text": "standing and characterizing language preference in multilingual societies has been the subject matter of linguistic inquiry for over half a century (see for an overview).", "labels": [], "entities": [{"text": "characterizing language preference", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.7086266676584879}]}, {"text": "Conversational phenomena such as CS were observed only in speech and therefore, all previous studies are based on data collected from a small set of speakers or from interviews.", "labels": [], "entities": []}, {"text": "With the growing popularity of social media, we now have an abundance of conversation-like data that exhibit CS and other speech phenomena, hitherto unseen in text ( ).", "labels": [], "entities": []}, {"text": "Leveraging such data from Twitter, we conduct a large-scale study on language preference, if any, for the expression of opinion and sentiment by Hindi-English (Hi-En) bilinguals.", "labels": [], "entities": []}, {"text": "We first build a corpus of 430,000 unique Indiaspecific tweets across four domains (sports, entertainment, politics and current events) and automatically classify the tweets by their language: English, Hindi and Hi-En CS.", "labels": [], "entities": []}, {"text": "We then develop an opinion detector for each language class to further categorize them into opinionated and non-opinionated tweets.", "labels": [], "entities": []}, {"text": "Sentiment detectors further classify the opinionated tweets as positive, negative or neutral.", "labels": [], "entities": []}, {"text": "Our study shows that there is a strong preference towards Hindi (i.e. the native language or L1) over English (L2) for expression of negative opinion.", "labels": [], "entities": []}, {"text": "The effect is clearly visible in CS tweets, where a switch from English to Hindi is often correlated with a switch from a positive to negative sentiment.", "labels": [], "entities": []}, {"text": "This is referred to as the polarity-switch function of CS (.", "labels": [], "entities": []}, {"text": "Using the same experimental technique, we also explore other pragmatic functions of CS, such as reinforcement and narrative-evaluative.", "labels": [], "entities": []}, {"text": "Apart from being the first large-scale quantitative study of language preference in multilingual societies, this work also has several other contributions: (a) We develop one of the first opinion and sentiment classifiers for Romanized Hindi and CS Hi-En tweets with higher accuracy than the only known previous attempt.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 274, "end_pos": 282, "type": "METRIC", "confidence": 0.9943459630012512}]}, {"text": "(b) We present a novel methodology for automatically detecting pragmatic functions of codeswitching through opinion and sentiment detection.", "labels": [], "entities": [{"text": "opinion and sentiment detection", "start_pos": 108, "end_pos": 139, "type": "TASK", "confidence": 0.6254478842020035}]}, {"text": "The rest of the paper is organized as follows: Sec.", "labels": [], "entities": []}, {"text": "2 introduces language preference, functions of CS and Hindi-English bilingualism on the web.", "labels": [], "entities": []}, {"text": "3 formulates the problem and presents the fundamental questions that this paper seeks to answer.", "labels": [], "entities": []}, {"text": "4 and 5 discuss dataset creation and opinion and sentiment detection techniques respectively.", "labels": [], "entities": [{"text": "dataset creation", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7157876342535019}, {"text": "sentiment detection", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7873134315013885}]}, {"text": "6 evaluates the hypotheses in light of the observations on the tweet corpus.", "labels": [], "entities": [{"text": "tweet corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9228250086307526}]}, {"text": "7, and raise some interesting sociolinguistic questions for future studies.", "labels": [], "entities": []}], "datasetContent": [{"text": "We collected tweets with certain India-specific hashtags  T BL : Tweets from users who are certainly Hi-En bilinguals, which are approximately 55% (240,000) of the tweets in T All . We define a user to be a Hi-En bilingual if there is at least one mr tweet from the user, or if the user has tweeted at least once in Hindi (hd or hr) and once in English (er).", "labels": [], "entities": []}, {"text": "T spo , T mov , T pol , T eve : Topic-wise corpora for sports, movies, politics and events.", "labels": [], "entities": [{"text": "T eve", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.8956142961978912}]}, {"text": "T CS : Tweets with inter-sentential CS.", "labels": [], "entities": []}, {"text": "We define these as tweets containing at least one sequence of 5 contiguous Hindi words and one sequence of 5 contiguous English words.", "labels": [], "entities": []}, {"text": "The corpus has 3,357 tweets.", "labels": [], "entities": []}, {"text": "SAC: 1000 monolingual tweets (er, hr, hd) and 260 mixed (mr) tweets manually annotated with sentiment and opinion labels.", "labels": [], "entities": [{"text": "SAC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7675660848617554}]}, {"text": "These were annotated by two linguists, both fluent Hi-En speakers.", "labels": [], "entities": []}, {"text": "The annotators first checked whether the tweet is opinionated or \u2297 and then identified polarity of the opinionated tweets (+, \u2212 or 0).", "labels": [], "entities": []}, {"text": "Thus, the tweets are classified into the four classes in the set 3.", "labels": [], "entities": []}, {"text": "If a tweet contains both opinion and \u2297, each fragment was individually annotated.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement is 77.5% (\u03ba = 0.59) for opinion annotation and 68.4% (\u03ba = 0.64) overall four classes.", "labels": [], "entities": [{"text": "opinion annotation", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7085695415735245}]}, {"text": "A third linguist independently corrected the disagreements.", "labels": [], "entities": []}, {"text": "LLC Test : 141 er, 137 hr, and 241 mr tweets annotated by a Hi-En bilingual form the test set for the Language Labeling system (Sec. 5.1).", "labels": [], "entities": [{"text": "LLC Test", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.819426417350769}]}, {"text": "SAC and LLC Test can be downloaded and used for research purposes . Note that apart from SAC and LLC Test , all corpora are subsets of T All . For generalizability of our observations, it is important to ensure that the tweets in T All come from a large number of users and the datasets do not over-represent a small set of users.", "labels": [], "entities": [{"text": "SAC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9351863265037537}, {"text": "LLC Test", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.8410593569278717}]}, {"text": "In, we plot the minimum fraction of users required (x-axis) to cover a certain percentage of the tweets in T All (y-axis).", "labels": [], "entities": []}, {"text": "Tweets from at least 10%, i.e., 12.5K users are needed to cover 50% of the corpus.", "labels": [], "entities": []}, {"text": "As expected, we do observe a powerlaw-like distribution, where a few users contribute a large number of tweets, and a large number of users contribute a few tweets each.", "labels": [], "entities": []}, {"text": "We believe that 12.5K users is sufficient to ensure an unbiased study.", "labels": [], "entities": []}, {"text": "Further, we classify the users into three specific groups (i) news channels, (ii) general users (having \u2264 10,000 followers), (iii) popular users or celebrities (having > 10,000 followers).", "labels": [], "entities": []}, {"text": "Interestingly, for both T All , and T BL corpora, we observe that around 98% of all users are general, and 96% of all tweets come from such users.", "labels": [], "entities": [{"text": "BL", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.5010972619056702}]}, {"text": "Hence, most observations from these corpora are expected to be representative of the average online linguistic behavior of a Hi-En bilingual.", "labels": [], "entities": []}, {"text": "diagrammatically summarizes our experimental method.", "labels": [], "entities": []}, {"text": "We identify the language used in each tweet before detecting opinion and sentiment.", "labels": [], "entities": []}, {"text": "We evaluated the language labeling system on the LLC T est corpus, on which the precision (recall) values were 0.93(0.91), 0.90(0.85) and 0.88(0.92) for er, hr and mr classes respectively.", "labels": [], "entities": [{"text": "LLC T est corpus", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.8340722322463989}, {"text": "precision (recall)", "start_pos": 80, "end_pos": 98, "type": "METRIC", "confidence": 0.8297348022460938}]}, {"text": "The tweetlevel classification accuracy was 89.8%.", "labels": [], "entities": [{"text": "tweetlevel classification", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.5648594498634338}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9732497334480286}]}, {"text": "The opinion and sentiment classifiers were evaluated using 10-fold cross validation on the SAC dataset.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7407218813896179}, {"text": "SAC dataset", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.8741152882575989}]}, {"text": "For comparison, we also reimplemented the dictionary and dependency-based method by.", "labels": [], "entities": []}, {"text": "The accuracy of the opinion classifier on the er tweets was found to be 65.7%, 7% lower than our system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995099306106567}]}, {"text": "We also compared our mr sentiment classifier with that of.", "labels": [], "entities": []}, {"text": "As their method performs two class sentiment detection (+ and \u2212), we select such tweets from SAC.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7025125473737717}]}, {"text": "Their system achieves an accuracy of 68.2%, which is 4% lower than the accuracy of our system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9997627139091492}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9995146989822388}]}, {"text": "An analysis of the errors showed more false negatives (i.e., opinions labeled \u2297) than false positives in opinion classification.", "labels": [], "entities": [{"text": "opinion classification", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7366048097610474}]}, {"text": "Sentiment misclassification is uniformly distributed.", "labels": [], "entities": [{"text": "Sentiment misclassification", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9137554466724396}]}, {"text": "reports the accuracy of the opinion classifier for feature ablation experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9994352459907532}]}, {"text": "For all three language-script pairs, lexicon and non-word (emoticons, elongated words, hashtags, exclamation) features are the most effective, though all features have some positive contribution towards the final accuracy of opinion detection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9928106665611267}, {"text": "opinion detection", "start_pos": 225, "end_pos": 242, "type": "TASK", "confidence": 0.78193598985672}]}, {"text": "For hr and hd tweets, domain knowledge is significant, as shown by the 4% accuracy drop with removing the domain lexicon.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9989283680915833}]}, {"text": "In this section, we report our experiments on 430,000 unique tweets (T All ), and its various subsets as defined in Sec 4.", "labels": [], "entities": []}, {"text": "First, we run the language detection system on the corpora.", "labels": [], "entities": [{"text": "language detection", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7371464967727661}]}, {"text": "We see that language preference varies by topic, which is not surprising.", "labels": [], "entities": []}, {"text": "Due to paucity of space, the correlation between language usage and topic will not be discussed at length here, but we will highlight cases where the differences are striking.", "labels": [], "entities": []}, {"text": "We apply the language-specific opinion and sentiment classifiers to tweets detected as the corresponding language class.", "labels": [], "entities": []}, {"text": "In the following subsections, we empirically investigate the hypotheses.", "labels": [], "entities": []}, {"text": "shows pr(\u2297|\u03bb\u03c3; T ), pr(\u2212|\u03bb\u03c3; T ) and pr(\u2212|\u03bb\u03c3; T )/pr(+|\u03bb\u03c3; T ) for T All , T BL and two randomly selected topics -Movie and Politics.", "labels": [], "entities": [{"text": "BL", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.6184702515602112}]}, {"text": "The statistics are fairly consistent over the corpora, with slight differences but similar trends in T mov .  We need the first statistic in order to investigate Hypothesis I (Eqs.", "labels": [], "entities": []}, {"text": "6 and 7), and the two latter ones for verifying Hypothesis II (Eqs. 8 and 9).", "labels": [], "entities": [{"text": "verifying Hypothesis II", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7597644825776418}]}], "tableCaptions": [{"text": " Table 2: Accuracy of the opinion and sentiment classi- fiers. All values are in %.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9970744848251343}]}, {"text": " Table 3: Feature ablation experiments for the opinion  classifiers. NONE represents the case when all features  were used. The two smallest values (pertaining to the  two most effective features) are shown in bold.", "labels": [], "entities": [{"text": "NONE", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9743368029594421}]}, {"text": " Table 4: Distribution across classes in \u039b", "labels": [], "entities": []}, {"text": " Table 5: Sentiment across languages: Statistics concern- ing hypotheses I and II.", "labels": [], "entities": []}, {"text": " Table 6: T CS statistics for testing hypotheses Ia and IIa", "labels": [], "entities": []}]}