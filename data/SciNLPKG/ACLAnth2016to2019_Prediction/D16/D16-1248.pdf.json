{"title": [{"text": "Why Neural Translations are the Right Length", "labels": [], "entities": [{"text": "Neural Translations", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.856808602809906}]}], "abstractContent": [{"text": "We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality.", "labels": [], "entities": []}], "introductionContent": [{"text": "The neural encoder-decoder framework for machine translation provides new tools for addressing the field's difficult challenges.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8046610653400421}]}, {"text": "In this framework), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector.", "labels": [], "entities": []}, {"text": "We then use another recurrent network (decoder) to convert that vector into a target sentence.", "labels": [], "entities": []}, {"text": "In this paper, we train long shortterm memory (LSTM) neural units) trained with back-propagation through time.", "labels": [], "entities": []}, {"text": "A remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length.", "labels": [], "entities": []}, {"text": "When we evaluate the system on previously unseen test data, using BLEU (), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9986749291419983}, {"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.9078406095504761}]}, {"text": "Thus, no brevity penalty is incurred.", "labels": [], "entities": []}, {"text": "This behavior seems to come for free, without special design.", "labels": [], "entities": []}, {"text": "By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length.", "labels": [], "entities": [{"text": "MT (SMT)", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.8206579834222794}, {"text": "length", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.872525155544281}]}, {"text": "The original mechanism comes from the IBM SMT group, whose famous Models 1-5 included a learned table (y|x), with x and y being the lengths of source and target sentences).", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.8379629850387573}]}, {"text": "But they did not deploy this table when decoding a foreign sentence f into an English sentence e; it did not participate in incremental scoring and pruning of candidate translations.", "labels": [], "entities": []}, {"text": "As a result (: \"However, fora given f, if the goal is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones.", "labels": [], "entities": []}, {"text": "As a result, short English strings are improperly favored over longer English strings.", "labels": [], "entities": []}, {"text": "This tendency is counteracted in part by the following modification: Replace P(f|e) with c length(e) \u00b7 P(f|e) for some empirically chosen constant c.", "labels": [], "entities": []}, {"text": "This modification is treatment of the symptom rather than treatment of the disease itself, but it offers some temporary relief.", "labels": [], "entities": []}, {"text": "The cure lies in better modeling.\"", "labels": [], "entities": []}, {"text": "More temporary relief came from Minimum Error-Rate Training (MERT), which automatically sets c to maximize BLEU score.", "labels": [], "entities": [{"text": "Minimum Error-Rate Training (MERT)", "start_pos": 32, "end_pos": 66, "type": "METRIC", "confidence": 0.771626686056455}, {"text": "BLEU score", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9714813232421875}]}, {"text": "MERT also sets weights for the language model P(e), translation model P(f|e), and other features.", "labels": [], "entities": []}, {"text": "The length feature combines so sensitively with other features that MERT frequently returns to it as it revises one weight at a time.", "labels": [], "entities": [{"text": "length", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9928186535835266}, {"text": "MERT", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.7641530632972717}]}, {"text": "NMT's ability to correctly model length is remarkable for these reasons: \u2022 SMT relies on maximum BLEU training to obtain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9902682900428772}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9947018027305603}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9948598146438599}]}, {"text": "\u2022 Standard SMT models explicitly \"cross off\" source words and phrases as they are translated, so it is clear when an SMT decoder has finished translating a sentence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9894188046455383}, {"text": "SMT decoder", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.9008333683013916}]}, {"text": "NMT systems lack this explicit mechanism.", "labels": [], "entities": []}, {"text": "\u2022 SMT decoding involves heavy search, so if one MT output path delivers an infelicitous ending, another path can be used.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 2, "end_pos": 14, "type": "TASK", "confidence": 0.9500015377998352}]}, {"text": "NMT decoding explores far fewer hypotheses, using a tight beam without recombination.", "labels": [], "entities": [{"text": "NMT decoding", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6765657365322113}]}, {"text": "In this paper, we investigate how length regulation works in NMT.", "labels": [], "entities": [{"text": "length regulation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7602523267269135}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: R 2 values showing how differently-chosen sets of 10", "labels": [], "entities": []}, {"text": " Table 2: Sets of k units chosen by beam search to optimally", "labels": [], "entities": []}]}