{"title": [{"text": "Learning to Answer Questions from Wikipedia Infoboxes", "labels": [], "entities": [{"text": "Learning to Answer Questions from Wikipedia Infoboxes", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7334322759083339}]}], "abstractContent": [{"text": "A natural language interface to answers on the Web can help us access information more efficiently.", "labels": [], "entities": []}, {"text": "We start with an interesting source of information-infoboxes in Wikipedia that summarize factoid knowledge-and develop a comprehensive approach to answering questions with high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9897876977920532}]}, {"text": "We first build a system to access data in infoboxes in a struc-tured manner.", "labels": [], "entities": []}, {"text": "We use our system to construct a crowdsourced dataset of over 15,000 high-quality, diverse questions.", "labels": [], "entities": []}, {"text": "With these questions , we train a convolutional neural network model that outperforms models that achieve top results in similar answer selection tasks.", "labels": [], "entities": [{"text": "answer selection tasks", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.7719869414965311}]}], "introductionContent": [{"text": "The goal of open-domain question answering is to provide high-precision access to information.", "labels": [], "entities": [{"text": "open-domain question answering", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.607153465350469}]}, {"text": "With many sources of knowledge on the Web, selecting the right answer to a user's question remains challenging.", "labels": [], "entities": []}, {"text": "Wikipedia contains over five million articles in its English version.", "labels": [], "entities": []}, {"text": "Providing a natural language interface to answers in Wikipedia is an important step towards more effective information access.", "labels": [], "entities": []}, {"text": "Many Wikipedia articles have an infobox, a table that summarizes key information in the article in the form of attribute-value pairs like \"Narrated by: Fred Astaire\".", "labels": [], "entities": []}, {"text": "This data source is appealing for question answering because it covers abroad range of facts that are inherently relevant: a human editor manually highlighted this information in the infobox.", "labels": [], "entities": [{"text": "question answering", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9234696924686432}]}, {"text": "Although many infoboxes appear to be similar, they are only semi-structured-few attributes have consistent value types across articles, infobox templates do not mandate which attributes must be included, and editors are allowed to add articlespecific attributes.", "labels": [], "entities": []}, {"text": "Infobox-like tables are very common on the Web.", "labels": [], "entities": []}, {"text": "Since it is infeasible to incorporate every such source into structured knowledge bases like Freebase (, we need techniques that do not rely on ontology or value type information.", "labels": [], "entities": []}, {"text": "We focus on the answer selection problem, where the goal is to select the best answer out of a given candidate set of attribute-value pairs from infoboxes corresponding to a named entity in the question.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.8688542544841766}]}, {"text": "Table 1 illustrates how questions from users may have little lexical overlap with the correct attribute-value pair.", "labels": [], "entities": []}, {"text": "Answer selection is an important subtask in building an end-to-end question answering system.", "labels": [], "entities": [{"text": "Answer selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9507934153079987}, {"text": "question answering", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7077514976263046}]}, {"text": "Our work has two main contributions: (1) We compiled the INFOBOXQA dataset, a crowdsourced corpus of over 15,000 questions with answers from infoboxes in 150 articles in Wikipedia.", "labels": [], "entities": [{"text": "INFOBOXQA dataset", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.8582142889499664}]}, {"text": "Unlike existing answer selection datasets with answers from knowledge bases or long-form text, INFOBOXQA targets tabular data that is not augmented with value types or linked to an ontology.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.8344894051551819}]}, {"text": "(2) We built a multichannel convolutional neural network (CNN) model that achieves the best results on INFOBOXQA compared to other neural network models in the answer selection task.", "labels": [], "entities": [{"text": "INFOBOXQA", "start_pos": 103, "end_pos": 112, "type": "DATASET", "confidence": 0.7119254469871521}, {"text": "answer selection task", "start_pos": 160, "end_pos": 181, "type": "TASK", "confidence": 0.9148664077123007}]}], "datasetContent": [{"text": "Infoboxes are designed to be human-readable, not machine-interpretable.", "labels": [], "entities": []}, {"text": "This allowed us to devise a crowdsourced assignment where we ask participants to generate questions from infoboxes.", "labels": [], "entities": []}, {"text": "With little to no training, humans can form coherent questions out of terse, potentially ambiguous attribute-value pairs.", "labels": [], "entities": []}, {"text": "Wikipedia does not provide away to access specific information segments; its API returns the entire article.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8874396085739136}]}, {"text": "We first worked on the data access problem and developed a system called WikipediaBase to robustly extract attribute-value pairs from infoboxes.", "labels": [], "entities": [{"text": "WikipediaBase", "start_pos": 73, "end_pos": 86, "type": "DATASET", "confidence": 0.9584565162658691}]}, {"text": "Inspired by Omnibase (), we organize infoboxes in an object-attribute-value data model, where an object (Lake Titicaca) has an attribute (\"Surface area\") with a value (8,372 km 2 ).", "labels": [], "entities": []}, {"text": "Attributes are grouped by infobox class (for instance, the film class contains attributes like \"Directed by\" and \"Cinematography\").", "labels": [], "entities": []}, {"text": "The data model allowed us to extend WikipediaBase to information outside of infoboxes.", "labels": [], "entities": [{"text": "WikipediaBase", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8904107809066772}]}, {"text": "We implemented methods for accessing images, categories, and article sections.", "labels": [], "entities": []}, {"text": "We then created a question-writing assignment where participants see infoboxes constructed using data from WikipediaBase.", "labels": [], "entities": [{"text": "WikipediaBase", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.990009605884552}]}, {"text": "These infoboxes visually resembled the original ones in Wikipedia but were designed to control for variables.", "labels": [], "entities": []}, {"text": "To prevent participants from only generating questions for attributes at the top of the table, the order of attributes was randomly shuffled.", "labels": [], "entities": []}, {"text": "To ensure that the task could be completed in a reasonable amount of time, infoboxes were partitioned into assignments with up to ten attributes.", "labels": [], "entities": []}, {"text": "A major goal of this data collection was to gather question paraphrases.", "labels": [], "entities": [{"text": "gather question paraphrases", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.5929237405459086}]}, {"text": "For each attribute, we asked participants to write two questions.", "labels": [], "entities": []}, {"text": "It is likely that at least one of the questions will use words from the attribute, but requiring an  additional question encouraged them to think of alternative phrasings.", "labels": [], "entities": []}, {"text": "Every infobox in the experiment included a picture to help disambiguate the article.", "labels": [], "entities": []}, {"text": "For instance, the cover image for \"Grand Theft Auto III\" (in concert with the values in the infobox) makes it reasonably clear that the assignment is about a video game and not a type of crime.", "labels": [], "entities": []}, {"text": "We asked participants to include an explicit referent to the article title in each question (e.g., \"Where was Albert Einstein born?\" instead of \"Where was he born?\").", "labels": [], "entities": []}, {"text": "We analyzed the occurrences of infobox attributes in Wikipedia and found that they fit a rapidlydecaying exponential distribution with along tail of attributes that occur in few articles.", "labels": [], "entities": []}, {"text": "This distribution means that with a carefully chosen subset of articles we can achieve a large coverage of frequently appearing attributes.", "labels": [], "entities": []}, {"text": "We developed a greedy approximation algorithm that selects a subset of infobox classes, picks a random sample of articles in the class, and chooses three representative articles that contain the largest quantity of attributes.", "labels": [], "entities": []}, {"text": "150 articles from 50 classes were selected, covering roughly half of common attributes found in Wikipedia.", "labels": [], "entities": []}, {"text": "The dataset contains example questions q i , with an attribute-value pair (a i , vi ) that answers the question.", "labels": [], "entities": []}, {"text": "To generate negative examples for the answer selection task, we picked every other tuple (a j , v j ); 8j 6 = i from the infobox that contains the correct answer.", "labels": [], "entities": [{"text": "answer selection task", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.8935362497965494}]}, {"text": "If we know that a question asks about a specific entity, we must consider every attribute in the entity's infobox as a possible answer.", "labels": [], "entities": []}, {"text": "In INFOBOXQA, candidate answers are just attribute-value pairs with no type information.", "labels": [], "entities": [{"text": "INFOBOXQA", "start_pos": 3, "end_pos": 12, "type": "DATASET", "confidence": 0.8517792820930481}]}, {"text": "Because of this, every attribute in the infobox is indistinguishable a priori, and is thus in the candidate set.", "labels": [], "entities": []}, {"text": "Not having type information makes the task harder but also more realistic.", "labels": [], "entities": []}, {"text": "The dataset is available online.", "labels": [], "entities": []}, {"text": "We implemented Tri-CNN in the dataset-sts 3 framework for semantic text similarity, built on top of the Keras deep learning library.", "labels": [], "entities": [{"text": "semantic text similarity", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6603880524635315}, {"text": "Keras deep learning library", "start_pos": 104, "end_pos": 131, "type": "DATASET", "confidence": 0.8450373858213425}]}, {"text": "The framework aims to unify various sentence matching tasks, including answer selection, and provides implementations for variants of sentencematching models that achieve state-of-the-art results on the TREC answer selection dataset (.", "labels": [], "entities": [{"text": "sentence matching", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7150755673646927}, {"text": "answer selection", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.9173196256160736}, {"text": "TREC answer selection dataset", "start_pos": 203, "end_pos": 232, "type": "DATASET", "confidence": 0.6720630750060081}]}, {"text": "We evaluated the performance of various models in dataset-sts against INFOBOXQA for the task of answer selection.", "labels": [], "entities": [{"text": "INFOBOXQA", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.8282096982002258}, {"text": "answer selection", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.9037869870662689}]}, {"text": "We report the average and the standard deviation for mean average precision (MAP) and mean reciprocal rank (MRR) from five-fold cross validation.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 53, "end_pos": 81, "type": "METRIC", "confidence": 0.93711718916893}, {"text": "mean reciprocal rank (MRR)", "start_pos": 86, "end_pos": 112, "type": "METRIC", "confidence": 0.9225350022315979}]}, {"text": "We used 10% of the training set for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.9474644064903259}]}, {"text": "In answer selection, a model learns a function to score candidate answers; the set of candidate answers is already given.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.9123826026916504}]}, {"text": "Entity linking is needed to generate candidate answers and is often treated as a separate module.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6483318954706192}]}, {"text": "For INFOBOXQA, we asked humans to generate questions from pre-specified infoboxes.", "labels": [], "entities": [{"text": "INFOBOXQA", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8210753202438354}]}, {"text": "Given this setup, we already know which entity the question refers to; we also know that the question is answerable by the infobox.", "labels": [], "entities": []}, {"text": "Entity linking was therefore out of scope in our experiments.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8351232409477234}]}, {"text": "By effectively asking humans to identify the named entity, our evaluation results are not affected by noise caused by a faulty entity linking strategy.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the INFOBOXQA dataset.", "labels": [], "entities": [{"text": "INFOBOXQA dataset", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.9752357006072998}]}, {"text": " Table 3: Results of five-fold cross validation. Our Tri-CNN", "labels": [], "entities": [{"text": "Tri-CNN", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.49138227105140686}]}]}