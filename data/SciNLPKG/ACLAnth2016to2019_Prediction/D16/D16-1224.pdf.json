{"title": [{"text": "AMR-to-text generation as a Traveling Salesman Problem", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9092004299163818}]}], "abstractContent": [{"text": "The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning fora given AMR graph.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9876570999622345}]}, {"text": "We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asym-metric generalized traveling salesman problem (AGTSP).", "labels": [], "entities": []}, {"text": "A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution.", "labels": [], "entities": [{"text": "TSP solver", "start_pos": 79, "end_pos": 89, "type": "TASK", "confidence": 0.785046249628067}]}, {"text": "The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9797845184803009}, {"text": "SemEval-2016 Task8 dataset", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.8775143027305603}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8476810654004415}]}, {"text": "Shown in, the nodes of an AMR graph (e.g. \"boy\", \"go-01\" and \"want-01\") represent concepts, and the edges (e.g. \"ARG0\" and \"ARG1\") represent relations between concepts.", "labels": [], "entities": []}, {"text": "AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8912350833415985}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.6918150931596756}]}, {"text": "AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 100, "end_pos": 137, "type": "TASK", "confidence": 0.8279797434806824}]}, {"text": "The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9888478219509125}]}, {"text": "This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (.", "labels": [], "entities": []}, {"text": "There has been work dealing with text-to-AMR parsing ().", "labels": [], "entities": [{"text": "text-to-AMR parsing", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7095216810703278}]}, {"text": "On the other hand, relatively little work has been done on AMR-to-text generation.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.980707198381424}]}, {"text": "One recent exception is, who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence.", "labels": [], "entities": []}, {"text": "Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP).", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9479520916938782}]}, {"text": "Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence for the whole AMR by ordering the translations.", "labels": [], "entities": []}, {"text": "To cut the AMR and translate each fragment, we match the input AMR with rules, each consisting of a rooted, connected AMR fragment and a corresponding translation.", "labels": [], "entities": []}, {"text": "These rules serve in a similar way to rules in SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9926034808158875}]}, {"text": "We learn the rules by a modified version of the sampling algorithm of, and use the rule matching algorithm of . For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered.", "labels": [], "entities": [{"text": "AMR", "start_pos": 248, "end_pos": 251, "type": "DATASET", "confidence": 0.8080084323883057}]}, {"text": "To generate a sentence for the whole AMR, we search for an ordered cut, and concatenate translations of all rules in the cut.", "labels": [], "entities": []}, {"text": "TSP is used to traverse different cuts and determine the best order.", "labels": [], "entities": [{"text": "TSP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.39109233021736145}]}, {"text": "Intuitively, our method is similar to phrase-based SMT, which first cuts the input sentence into phrases, then obtains the translation for each source phrase, before finally generating the target sentence by ordering the translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.7198118567466736}]}, {"text": "Although the computational cost of our method is low, the initial experiment is promising, yielding a BLEU score of 22.44 on a standard benchmark.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9996564388275146}]}], "datasetContent": [], "tableCaptions": []}