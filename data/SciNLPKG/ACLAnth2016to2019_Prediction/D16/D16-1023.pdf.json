{"title": [{"text": "Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification", "labels": [], "entities": [{"text": "Cross-Domain Sentiment Classification", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.7192903757095337}]}], "abstractContent": [{"text": "In this paper, we study cross-domain sentiment classification with neural network archi-tectures.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.7163937290509542}]}, {"text": "We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 169, "end_pos": 193, "type": "TASK", "confidence": 0.9548723995685577}]}, {"text": "We also propose to jointly learn this sentence embedding together with the sentiment classifier itself.", "labels": [], "entities": []}, {"text": "Experiment results demonstrate that our proposed joint model outperforms several state-of-the-art methods on five benchmark datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews, sentiment classification has received continuous attention in the NLP community for over a decade (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.9331678450107574}]}, {"text": "One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9559382200241089}]}, {"text": "This is because sentiments are often expressed with domain-specific words and expressions.", "labels": [], "entities": []}, {"text": "For example, in the Movie domain, words such as moving and engaging are usually positive, but they may not be relevant in the Restaurant domain.", "labels": [], "entities": []}, {"text": "Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain.", "labels": [], "entities": []}, {"text": "Much work has been done in sentiment analysis to address this domain adaptation problem).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9765997529029846}]}, {"text": "Among them, an appealing method is the Structural Correspondence Learning (SCL) method, which uses pivot feature prediction tasks to induce a projected feature space that works well for both the source and the target domains.", "labels": [], "entities": [{"text": "Structural Correspondence Learning (SCL)", "start_pos": 39, "end_pos": 79, "type": "TASK", "confidence": 0.6853341658910116}]}, {"text": "The intuition behind is that these pivot prediction tasks are highly correlated with the original task.", "labels": [], "entities": [{"text": "pivot prediction", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7113220989704132}]}, {"text": "For sentiment classification, first chose pivot words which have high mutual information with the sentiment labels, and then setup the pivot prediction tasks to be the predictions of each of these pivot words using the other words.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.9732190668582916}]}, {"text": "However, the original SCL method is based on traditional discrete feature representations and linear classifiers.", "labels": [], "entities": []}, {"text": "In recent years, with the advances of deep learning in NLP, multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.9769938290119171}]}, {"text": "In these models, dense, real-valued feature vectors and non-linear classification functions are used.", "labels": [], "entities": []}, {"text": "By using real-valued word embeddings pre-trained from a large corpus, these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words.", "labels": [], "entities": []}, {"text": "And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space.", "labels": [], "entities": []}, {"text": "Therefore, it would be interesting to explore how these neural network models could be extended for cross-domain sentiment classification.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 100, "end_pos": 137, "type": "TASK", "confidence": 0.7586620052655538}]}, {"text": "There has been some recent studies on neural network-based domain adaptation).", "labels": [], "entities": [{"text": "neural network-based domain adaptation", "start_pos": 38, "end_pos": 76, "type": "TASK", "confidence": 0.7359096705913544}]}, {"text": "They use Stacked Denoising Auto-encoders (SDA) to induce a hidden representation that presumably works well across domains.", "labels": [], "entities": []}, {"text": "However, SDA is fully unsupervised and does not consider the end task we need to solve, i.e., the sentiment classification task.", "labels": [], "entities": [{"text": "SDA", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9583845734596252}, {"text": "sentiment classification task", "start_pos": 98, "end_pos": 127, "type": "TASK", "confidence": 0.9425845940907797}]}, {"text": "In contrast, the idea behind SCL is to use carefullychosen auxiliary tasks that correlate with the end task to induce a hidden representation.", "labels": [], "entities": [{"text": "SCL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9611181020736694}]}, {"text": "Another line of work aims to learn a low dimensional representation for each feature in both domains based on predicting its neighboring features.", "labels": [], "entities": []}, {"text": "Different from these methods, we aim to directly learn sentence embeddings that work well across domains.", "labels": [], "entities": []}, {"text": "In this paper, we aim to extend the main idea behind SCL to neural network-based solutions to sentiment classification to address the domain adaptation problem.", "labels": [], "entities": [{"text": "SCL", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9790646433830261}, {"text": "sentiment classification", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.9262678921222687}, {"text": "domain adaptation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.728474885225296}]}, {"text": "Specifically, we borrow the idea of using pivot prediction tasks from SCL.", "labels": [], "entities": [{"text": "pivot prediction tasks", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7749297519524893}]}, {"text": "But instead of learning thousands of pivot predictors and performing singular value decomposition on the learned weights, which all relies on linear transformations, we introduce only two auxiliary binary prediction tasks and directly learn a non-linear transformation that maps an input to a dense embedding vector.", "labels": [], "entities": []}, {"text": "Moreover, different from SCL and the auto-encoderbased methods, in which the hidden feature representation and the final classifier are learned sequentially, we propose to jointly learn the hidden feature representation together with the sentiment classification model itself, and we show that joint learning works better than sequential learning.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 238, "end_pos": 262, "type": "TASK", "confidence": 0.7254025340080261}]}, {"text": "We conduct experiments on a number of different source and target domains for sentence-level sentiment classification.", "labels": [], "entities": [{"text": "sentence-level sentiment classification", "start_pos": 78, "end_pos": 117, "type": "TASK", "confidence": 0.8160998225212097}]}, {"text": "We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of our data sets.", "labels": [], "entities": []}, {"text": " Table 1. Movie1 2 and  Movie2 3 are movie reviews labeled by", "labels": [], "entities": []}, {"text": " Table 2: Comparison of classification accuracies of different methods.  *  indicates that our joint method is significantly better than", "labels": [], "entities": []}, {"text": " Table 3: Comparison of our method Joint with NaiveNN and", "labels": [], "entities": []}]}