{"title": [{"text": "Supervised Keyphrase Extraction as Positive Unlabeled Learning", "labels": [], "entities": [{"text": "Keyphrase Extraction", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.6635711044073105}]}], "abstractContent": [{"text": "The problem of noisy and unbalanced training data for supervised keyphrase extraction results from the subjectivity of keyphrase assignment , which we quantify by crowdsourc-ing keyphrases for news and fashion magazine articles with many annotators per document.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7129655927419662}, {"text": "keyphrase assignment", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.7803235352039337}]}, {"text": "We show that annotators exhibit substantial disagreement, meaning that single annotator data could lead to very different training sets for supervised keyphrase extractors.", "labels": [], "entities": [{"text": "keyphrase extractors", "start_pos": 151, "end_pos": 171, "type": "TASK", "confidence": 0.700131356716156}]}, {"text": "Thus, annotations from single authors or readers lead to noisy training data and poor extraction performance of the resulting supervised extractor.", "labels": [], "entities": []}, {"text": "We provide a simple but effective solution to still work with such data by reweighting the importance of unlabeled candidate phrases in a two stage Positive Unlabeled Learning setting.", "labels": [], "entities": []}, {"text": "We show that performance of trained keyphrase extractors approximates a classi-fier trained on articles labeled by multiple an-notators, leading to higher average F 1 scores and better rankings of keyphrases.", "labels": [], "entities": [{"text": "keyphrase extractors", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7591448426246643}, {"text": "F 1 scores", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.9875198403994242}]}, {"text": "We apply this strategy to a variety of test collections from different backgrounds and show improvements over strong baseline models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Keyphrase extraction is the task of extracting a selection of phrases from a text document to concisely summarize its contents.", "labels": [], "entities": [{"text": "Keyphrase extraction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8194023072719574}]}, {"text": "Applications of keyphrases range from summarization (D') to contextual advertisement () or simply as aid for navigation through large text corpora.", "labels": [], "entities": [{"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9242884516716003}, {"text": "navigation through large text corpora", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.7616477131843566}]}, {"text": "Existing work on automatic keyphrase extraction can be divided in supervised and unsupervised approaches.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7559371888637543}]}, {"text": "While unsupervised approaches are domain independent and do not require labeled training data, supervised keyphrase extraction allows for more expressive feature design and is reported to outperform unsupervised methods on many occasions (.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.7463070750236511}]}, {"text": "A requirement for supervised keyphrase extractors is the availability of labeled training data.", "labels": [], "entities": [{"text": "keyphrase extractors", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7698851227760315}]}, {"text": "In literature, training collections for supervised keyphrase extraction are generated in different settings.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.6995153427124023}]}, {"text": "In these collections, keyphrases for text documents are either supplied by the authors or their readers.", "labels": [], "entities": []}, {"text": "In the first case, authors of academic papers or news articles assign keyphrases to their content to enable fast indexing or to allow for the discovery of their work in electronic libraries.", "labels": [], "entities": []}, {"text": "Other collections are created by crowdsourcing () or based on explicit deliberation by a small group of readers (.", "labels": [], "entities": []}, {"text": "A minority of test collections provide multiple opinions per document, but even then the amount of opinions per document is kept minimal).", "labels": [], "entities": []}, {"text": "The traditional procedure for supervised keyphrase extraction is reformulating the task as a binary classification of keyphrase candidates.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7723113000392914}]}, {"text": "Supervision for keyphrase extraction faces several shortcomings.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.8473022878170013}]}, {"text": "Candidate phrases (generated in a separate candidate generation procedure), which are not annotated as keyphrases, are seen as non-keyphrase and are used as negative training data for the supervised classifiers.", "labels": [], "entities": []}, {"text": "First, on many occasions these negative phrases outnumber true keyphrases many times, creating an unbalanced training set (.", "labels": [], "entities": []}, {"text": "Second, as noted: authors do not always choose keyphrases that best describe the content of their paper, but they may choose phrases to slant their work a certain way, or to maximize its chance of being noticed by searchers.", "labels": [], "entities": []}, {"text": "Another problem is that keyphrases are inherently subjective, i.e., keyphrases assigned by one annotator are not the only correct ones.", "labels": [], "entities": []}, {"text": "These assumptions have consequences for training, developing and evaluating supervised models.", "labels": [], "entities": []}, {"text": "Unfortunately, a large collection of annotated documents by reliable annotators with high overlap per document is missing, making it difficult to study disagreement between annotators or the resulting influence on trained extractors, as well as to provide a reliable evaluation setting.", "labels": [], "entities": []}, {"text": "In this paper, we address these problems by creating a large test collection of articles with many different opinions per article, evaluate the effect on extraction performance, and present a procedure for supervised keyphrase extraction with noisy labels.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 217, "end_pos": 237, "type": "TASK", "confidence": 0.7051088064908981}]}], "datasetContent": [{"text": "Hasan and Ng (2010) have shown that techniques for keyphrase extraction are inconsistent and need to be tested across different test collections.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.8542463481426239}]}, {"text": "Next to our collections with multiple opinions (Online News and Lifestyle Magazines), we apply the reweighting strategy on test collections with sets of author-assigned keyphrases: two sets from CiteSeer abstracts from the World Wide Web Conference (WWW) and Knowledge Discovery and Data Mining (KDD), similar to the ones used in.", "labels": [], "entities": [{"text": "Knowledge Discovery and Data Mining (KDD)", "start_pos": 259, "end_pos": 300, "type": "TASK", "confidence": 0.8273842222988605}]}, {"text": "The Inspec dataset is a collection of 2,000 abstracts commonly used in keyphrase extraction literature, where we use the ground truth phrases from controlled vocabulary.", "labels": [], "entities": [{"text": "Inspec dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8712283670902252}, {"text": "keyphrase extraction literature", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.8351577321688334}]}, {"text": "Descriptive statistics of these test collections are given in.", "labels": [], "entities": []}, {"text": "We use a rich feature set consisting of statistical, structural, and semantic properties for each candidate phrase, that have been reported as effective in previous studies on supervised extractors (): (i) term frequency, (ii) number of tokens in the phrase, (iii) length of the longest term in the phrase, (iv) number of capital letters in the phrase, (v) the phrase's POS-tags, (vi) relative position of first occurrence, (vii) span (relative last occurrence minus relative first occurrence), (viii) TF*IDF (IDF's trained on large background collections from the same source) and (ix) Topical Word Importance, a feature measuring the similarity between the word-topic topic-document distributions presented in (, with topic models trained on background collections from a corresponding source of content.", "labels": [], "entities": [{"text": "span", "start_pos": 430, "end_pos": 434, "type": "METRIC", "confidence": 0.9924777150154114}, {"text": "TF*IDF", "start_pos": 502, "end_pos": 508, "type": "METRIC", "confidence": 0.9154068827629089}]}, {"text": "As classifier we use gradient boosted decision trees implemented in the XGBoost package.", "labels": [], "entities": []}, {"text": "During development, this classifier consistently outperformed Naive Bayes and linear classifiers like logistic regression or support vector machines.", "labels": [], "entities": []}, {"text": "We compare the reweighting strategy with uniform reweighting and strategies to counter the imbalance or noise of the training collections, such as subsampling, weighting unlabeled training data as in, and self-training in which only confident initial predictions are used as positive and negative data.", "labels": [], "entities": []}, {"text": "For every method, global thresholds are chosen to optimize the macro averaged F 1 per document (MAF 1 ).", "labels": [], "entities": [{"text": "macro averaged F 1 per document (MAF 1 )", "start_pos": 63, "end_pos": 103, "type": "METRIC", "confidence": 0.7753806948661804}]}, {"text": "Next to the threshold sensitive F 1 , we report on ranking quality using the Precision@5 metric.", "labels": [], "entities": [{"text": "threshold sensitive F 1", "start_pos": 12, "end_pos": 35, "type": "METRIC", "confidence": 0.6743105500936508}, {"text": "Precision@5 metric", "start_pos": 77, "end_pos": 95, "type": "METRIC", "confidence": 0.8443886488676071}]}, {"text": "Results are shown in with five-fold crossvalidation.", "labels": [], "entities": []}, {"text": "To study the effect of reweighting, we limit training collections during folds to 100 documents for each test collection.", "labels": [], "entities": []}, {"text": "Our approach consistently improves on single annotator trained classifiers, on one occasion even outperforming a training collection with multiple opinions.", "labels": [], "entities": []}, {"text": "Compensating for imbalance and noise tends to have less effect when the ratio of keyphrases versus candidates is high (as for Inspec) or training collection is very large.", "labels": [], "entities": []}, {"text": "When the amount of training documents increases, the ratio of noisy versus true negative labels drops.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Description of test collections.", "labels": [], "entities": []}, {"text": " Table 3: Mean average F 1 score per document and precision for five most confident keyphrases on different  test collections.", "labels": [], "entities": [{"text": "Mean average F 1 score", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.866999340057373}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9997561573982239}]}]}