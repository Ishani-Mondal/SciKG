{"title": [{"text": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "labels": [], "entities": [{"text": "Improving LSTM-based Video Description", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8678815811872482}]}], "abstractContent": [{"text": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos.", "labels": [], "entities": [{"text": "generation of natural language descriptions of videos", "start_pos": 91, "end_pos": 144, "type": "TASK", "confidence": 0.6607683088098254}]}, {"text": "Specifically, we integrate both a neu-ral language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description.", "labels": [], "entities": [{"text": "video description", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7054290920495987}]}, {"text": "We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "labels": [], "entities": [{"text": "Youtube videos", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9683166146278381}]}], "introductionContent": [{"text": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired.", "labels": [], "entities": [{"text": "content-based video retrieval", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.752129852771759}, {"text": "video description", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.7118786573410034}]}, {"text": "The most effective recent methods) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language.", "labels": [], "entities": [{"text": "machine translation (MT) from video to natural language", "start_pos": 96, "end_pos": 151, "type": "TASK", "confidence": 0.8435736775398255}]}, {"text": "Deep learning methods such as RNNs need large training corpora; however, there is alack of highquality paired video-sentence data.", "labels": [], "entities": []}, {"text": "In contrast, raw text corpora are widely available and exhibit rich linguistic structure that can aid video description.", "labels": [], "entities": [{"text": "video description", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.6191689521074295}]}, {"text": "Most work in statistical MT utilizes both a language model trained on a large corpus of monolingual target language data as well as a translation model trained on more limited parallel bilingual data.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.7058145403862}]}, {"text": "This paper explores methods to incorporate knowledge from language corpora to capture general linguistic regularities to aid video description.", "labels": [], "entities": [{"text": "video description", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7060895264148712}]}, {"text": "This paper integrates linguistic information into a video-captioning model based on Long Short Term Memory (LSTM)) RNNs which have shown state-of-the-art performance on the task.", "labels": [], "entities": []}, {"text": "Further, LSTMs are also effective as language models (LMs).", "labels": [], "entities": []}, {"text": "Our first approach (early fusion) is to pre-train the network on plain text before training on parallel video-text corpora.", "labels": [], "entities": []}, {"text": "Our next two approaches, inspired by recent MT work (, integrate an LSTM LM with the existing video-to-text model.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.979642391204834}]}, {"text": "Furthermore, we also explore replacing the standard one-hot word encoding with distributional vectors trained on external corpora.", "labels": [], "entities": []}, {"text": "We present detailed comparisons between the approaches, evaluating them on a standard Youtube corpus and two recent large movie description datasets.", "labels": [], "entities": [{"text": "Youtube corpus", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.9876692593097687}]}, {"text": "The results demonstrate significant improvements in grammaticality of the descriptions (as determined by crowdsourced human evaluations) and more modest improvements in descriptive quality (as determined by both crowdsourced human judgements and standard automated comparison to human-generated descriptions).", "labels": [], "entities": []}, {"text": "Our main contributions are 1) multiple ways to incorporate knowledge from external text into an existing captioning model, 2) extensive experiments comparing the methods on three large video-caption datasets, and 3) human judgements to show that external linguistic knowledge has a significant impact on grammar.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our language model was trained on sentences from Gigaword, BNC, UkWaC, and Wikipedia.", "labels": [], "entities": []}, {"text": "The vocabulary consisted of 72,700 most frequent tokens also containing GloVe embeddings.", "labels": [], "entities": []}, {"text": "Following the evaluation in, we compare our models on the Youtube dataset (Chen and Dolan, 2011), as well as two large movie description corpora: MPII-MD ( and M-VAD ( ).", "labels": [], "entities": [{"text": "Youtube dataset", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.9924140870571136}]}, {"text": "We evaluate performance using machine translation (MT) metrics ME-TEOR) and BLEU () to compare the machinegenerated descriptions to human ones.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.8072289884090423}, {"text": "ME-TEOR", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.6092585921287537}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9990832805633545}]}, {"text": "For the movie corpora which have just a single description we use only METEOR which is more robust.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9933246970176697}]}, {"text": "We also obtain human judgements using Amazon Turk on a random subset of 200 video clips for each dataset.", "labels": [], "entities": [{"text": "Amazon Turk", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9348255097866058}]}, {"text": "Each sentence was rated by 3 workers on a Likert scale of 1 to 5 (higher is better) for relevance and grammar.", "labels": [], "entities": [{"text": "relevance", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9365680813789368}]}, {"text": "No video was provided during grammar evaluation.", "labels": [], "entities": [{"text": "grammar evaluation", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9265688359737396}]}, {"text": "For movies, due to copyright, we only evaluate on grammar.", "labels": [], "entities": []}, {"text": "Comparison of the proposed techniques in shows that Deep Fusion performs well on both ME-TEOR and BLEU; incorporating Glove embeddings substantially increases METEOR, and combining them both does best.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9979634284973145}, {"text": "METEOR", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.96195387840271}]}, {"text": "Our final model is an ensemble (weighted average) of the Glove, and the two Glove+Deep Fusion models trained on the external and in-domain COCO ( ) sentences.", "labels": [], "entities": []}, {"text": "We note here that the state-of-the-art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.1) which proposes a superior visual processing pipeline using attention to encode the video.", "labels": [], "entities": [{"text": "HRNE (Pan et al., 2015) (METEOR 33.1)", "start_pos": 70, "end_pos": 107, "type": "DATASET", "confidence": 0.7809718027710915}]}, {"text": "Human ratings also correlate well with the ME-TEOR scores, confirming that our methods give a modest improvement in descriptive quality.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9854830503463745}]}, {"text": "However, incorporating linguistic knowledge significantly 2 improves the grammaticality of the results, making them more comprehensible to human users.", "labels": [], "entities": []}, {"text": "We experimented multiple ways to incorporate word embeddings: (1) GloVe input: Replacing one-hot vectors with GloVe on the LSTM input performed best.", "labels": [], "entities": []}, {"text": "(2) Fine-tuning: Initializing with GloVe and subsequently fine-tuning the embedding matrix reduced validation results by 0.4 METEOR.", "labels": [], "entities": [{"text": "Fine-tuning", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9006407856941223}, {"text": "GloVe", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9674878120422363}, {"text": "METEOR", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9987640380859375}]}, {"text": "(3) Input and Predict.", "labels": [], "entities": [{"text": "Input", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8532245755195618}, {"text": "Predict", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9845045804977417}]}, {"text": "Training the LSTM to accept and predict GloVe vectors, as described in Section 3, performed similar to (1).", "labels": [], "entities": []}, {"text": "All scores reported in correspond to the setting in (1) with GloVe embeddings only as input.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Youtube dataset: METEOR and BLEU@4 in %,  and human ratings (1-5) on relevance and grammar. Best  results in bold, * indicates significant over S2VT.", "labels": [], "entities": [{"text": "Youtube dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9801632165908813}, {"text": "METEOR", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.952568769454956}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.998029887676239}]}, {"text": " Table 2: Movie Corpora: METEOR (%) and human  grammar ratings (1-5, higher is better). Best results in  bold, * indicates significant over S2VT.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9916985034942627}]}]}