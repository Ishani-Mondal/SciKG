{"title": [{"text": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles", "labels": [], "entities": [{"text": "Span-Based Constituency Parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6731602648893992}]}], "abstractContent": [{"text": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9544346332550049}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9356663227081299}]}, {"text": "Despite striking results in dependency parsing, however , neural models have not surpassed state-of-the-art approaches in constituency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8536220788955688}, {"text": "constituency parsing", "start_pos": 122, "end_pos": 142, "type": "TASK", "confidence": 0.8829807639122009}]}, {"text": "To remedy this, we introduce anew shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features.", "labels": [], "entities": []}, {"text": "We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n 3) oracles for standard dependency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.890810489654541}, {"text": "O(1) time", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.902170193195343}, {"text": "dependency parsing", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7652911245822906}]}, {"text": "Training with this oracle, we achieve the best F 1 scores on both English and French of any parser that does not use reranking or external data.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9849081039428711}]}], "introductionContent": [{"text": "Parsing is an important problem in natural language processing which has been studied extensively for decades.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.946318507194519}, {"text": "natural language processing", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6338296929995219}]}, {"text": "Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms.", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9846221804618835}, {"text": "constituency parsing", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8824377357959747}, {"text": "dependency parsing", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.8121540248394012}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9992074370384216}]}, {"text": "There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.9502601027488708}, {"text": "dependency parsing", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.8440221846103668}]}, {"text": "In constituency parsing, however, neural approaches are still behind the state-of-the-art (; see more details in Section 5.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.9111129939556122}]}, {"text": "To remedy this, we design anew parsing framework that is more suitable for constituency parsing, and that can be accurately modeled by neural networks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9653259515762329}, {"text": "constituency parsing", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.9092889428138733}]}, {"text": "Observing that constituency parsing is primarily focused on sentence spans (rather than individual words, as is dependency parsing), we propose a novel adaptation of the shift-reduce system which reflects this focus.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.90403613448143}, {"text": "dependency parsing", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7178783565759659}]}, {"text": "In this system, the stack consists of sentence spans rather than partial trees.", "labels": [], "entities": []}, {"text": "It is also factored into two types of parser actions, structural and label actions, which alternate during a parse.", "labels": [], "entities": []}, {"text": "The structural actions area simplified analogue of shift-reduce actions, omitting the directionality of reduce actions, while the label actions directly assign nonterminal symbols to sentence spans.", "labels": [], "entities": []}, {"text": "Our neural model processes the sentence once for each parse with a recurrent network.", "labels": [], "entities": []}, {"text": "We represent parser configurations with a very small number of span features (4 for structural actions and 3 for label actions).", "labels": [], "entities": []}, {"text": "Extending, each span is represented as the difference of recurrent output from multiple layers in each direction.", "labels": [], "entities": []}, {"text": "No pretrained embeddings are required.", "labels": [], "entities": []}, {"text": "We also extend the idea of dynamic oracles from dependency to constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.8250931203365326}]}, {"text": "The latter is significantly more difficult than the former due to F 1 being a combination of precision and recall, and yet we propose a simple and extremely efficient oracle (amortized O(1) time).", "labels": [], "entities": [{"text": "F 1", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9867731332778931}, {"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9993876218795776}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9984651803970337}, {"text": "amortized O(1) time", "start_pos": 175, "end_pos": 194, "type": "METRIC", "confidence": 0.8762941857179006}]}, {"text": "This oracle is proved optimal for F 1 as well as both of its components, precision and recall.", "labels": [], "entities": [{"text": "F 1", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9074057936668396}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9996740818023682}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9991365075111389}]}, {"text": "Trained with this oracle, our parser achieves what we believe to be the best results for any parser without reranking which was trained only on the Penn Treebank and the French Treebank, despite the fact that it is not only lineartime, but also strictly greedy.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 148, "end_pos": 161, "type": "DATASET", "confidence": 0.995916485786438}, {"text": "French Treebank", "start_pos": 170, "end_pos": 185, "type": "DATASET", "confidence": 0.9897231161594391}]}, {"text": "We make the following main contributions: \u2022 A novel factored transition parsing system where the stack elements are sentence spans rather than partial trees (Section 2).", "labels": [], "entities": [{"text": "factored transition parsing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7152990102767944}]}, {"text": "\u2022 A neural model where sentence spans are represented as differences of output from a multilayer bi-directional LSTM (Section 3).", "labels": [], "entities": []}, {"text": "\u2022 The first provably optimal dynamic oracle for 1 constituency parsing which is also extremely efficient (amortized O(1) time) (Section 4).", "labels": [], "entities": [{"text": "1 constituency parsing", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.5624862909317017}, {"text": "amortized O(1) time", "start_pos": 106, "end_pos": 125, "type": "METRIC", "confidence": 0.8427641491095225}]}, {"text": "\u2022 The best F 1 scores of any single-model, closed training set, parser for English and French.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9771930774052938}]}, {"text": "We are also publicly releasing the source code for one implementation of our parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experiments on both the Penn English Treebank () and the French Treebank ().", "labels": [], "entities": [{"text": "Penn English Treebank", "start_pos": 35, "end_pos": 56, "type": "DATASET", "confidence": 0.9556547005971273}, {"text": "French Treebank", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9926848709583282}]}, {"text": "In both cases, all stateaction training pairs fora given sentence are used at the same time, greatly increasing training speed since all examples for the same sentence share the same forward and backward pass through the recurrent part of the network.", "labels": [], "entities": []}, {"text": "Updates are performed in minibatches of 10 sentences, and we shuffle the training sentences before each epoch.", "labels": [], "entities": []}, {"text": "The results we report are trained for 10 epochs.", "labels": [], "entities": []}, {"text": "The only regularization which we employ during training is dropout (, which is applied with probability 0.5 to the recurrent outputs.", "labels": [], "entities": []}, {"text": "It is applied separately to the input to the second LSTM layer for each sentence, and to the input to the ReLU hidden layer (span features) for each stateaction pair.", "labels": [], "entities": []}, {"text": "We use the ADADELTA method) to schedule learning rates for all weights.", "labels": [], "entities": [{"text": "ADADELTA", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9755228757858276}]}, {"text": "All of these design choices are summarized in.", "labels": [], "entities": []}, {"text": "In order to account for unknown words during training, we also adopt the strategy described by, where words in the training set are replaced with the unknownword symbol UNK with probability where f (w) is the number of times the word appears in the training corpus.", "labels": [], "entities": []}, {"text": "We choose the parameter z so that the training and validation corpora have approximately the same proportion of unknown words.", "labels": [], "entities": []}, {"text": "For the Penn Treebank, for example, we used z = 0.8375 so that both the validation set and the (rest of the) training set contain approximately 2.76% unknown words.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.995629221200943}]}, {"text": "This approach was helpful but not critical, improving F 1 (on dev) by about 0.1 over training without any unknown words.", "labels": [], "entities": [{"text": "F 1", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9911922812461853}]}, {"text": "For these experiments, we performed very little hyperparameter tuning, due to time and resource contraints.", "labels": [], "entities": []}, {"text": "We have every reason to believe that performance could be improved still further with such techniques as random restarts, larger hidden layers, external embeddings, and hyperparameter grid search, as demonstrated by.", "labels": [], "entities": [{"text": "hyperparameter grid search", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.5718933939933777}]}, {"text": "We also note that while our parser is very accurate even with greedy decoding, the model is easily adaptable for beam search, particularly since the parsing system already uses a fixed number of actions.", "labels": [], "entities": [{"text": "beam search", "start_pos": 113, "end_pos": 124, "type": "TASK", "confidence": 0.9055342078208923}]}, {"text": "Beam search could also be made considerably more efficient by caching post-hidden-layer feature components for sentence spans, essentially using the precomputation trick described by, but on a per-sentence basis.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9267492592334747}]}], "tableCaptions": [{"text": " Table 1: Features used for the parser. No label or tree-structure", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameters.  \u2020 French only.", "labels": [], "entities": []}, {"text": " Table 3. We were  surprised that flattening the distribution seemed to  be the least effective, as training accuracy (taking  into account sampled actions) lagged somewhat be- hind validation accuracy. Ultimately, the best results  were for \u03b1 = 1, which we used for final testing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9185733199119568}, {"text": "be- hind validation accuracy", "start_pos": 173, "end_pos": 201, "type": "METRIC", "confidence": 0.5822615385055542}]}, {"text": " Table 3: Comparison of performance on PTB development set", "labels": [], "entities": [{"text": "PTB development set", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.9102837840716044}]}, {"text": " Table 4: Comparison of performance of different parsers on", "labels": [], "entities": []}, {"text": " Table 5: Results on French Treebank.  *  reranking,  \u2021 external.", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.9867560863494873}, {"text": "reranking", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9621555209159851}]}]}