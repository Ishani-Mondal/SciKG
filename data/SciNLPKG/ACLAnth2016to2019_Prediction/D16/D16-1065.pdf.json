{"title": [{"text": "AMR Parsing with an Incremental Joint Model", "labels": [], "entities": [{"text": "AMR Parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7732871472835541}]}], "abstractContent": [{"text": "To alleviate the error propagation in the traditional pipelined models for Abstract Meaning Representation (AMR) parsing, we formulate AMR parsing as a joint task that performs the two subtasks: concept identification and relation identification simultaneously.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR) parsing", "start_pos": 75, "end_pos": 120, "type": "TASK", "confidence": 0.7734261410576957}, {"text": "formulate AMR parsing", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.7681559721628824}, {"text": "concept identification", "start_pos": 195, "end_pos": 217, "type": "TASK", "confidence": 0.7547049522399902}, {"text": "relation identification", "start_pos": 222, "end_pos": 245, "type": "TASK", "confidence": 0.7880914807319641}]}, {"text": "To this end, we first develop a novel component-wise beam search algorithm for relation identification in an incremental fashion, and then incorporate the decoder into a unified framework based on multiple-beam search, which allows for the bi-directional information flow between the two subtasks in a single incre-mental model.", "labels": [], "entities": [{"text": "relation identification", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.861749678850174}]}, {"text": "Experiments on the public datasets demonstrate that our joint model significantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 207, "end_pos": 218, "type": "TASK", "confidence": 0.9592003226280212}]}], "introductionContent": [{"text": "Producing semantic representations of text is motivated not only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 196, "end_pos": 214, "type": "TASK", "confidence": 0.8368176519870758}, {"text": "textual entailment", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.7304219007492065}, {"text": "machine translation", "start_pos": 239, "end_pos": 258, "type": "TASK", "confidence": 0.7857812345027924}]}, {"text": "described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.7311991254488627}]}, {"text": "Recently, some literatures reported some promising applications of AMR.", "labels": [], "entities": [{"text": "AMR", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9234364032745361}]}, {"text": "presented an unsupervised entity linking system with AMR, achieving the performance comparable to the supervised state-of-the-art.", "labels": [], "entities": [{"text": "AMR", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.842365026473999}]}, {"text": "demonstrated a novel abstractive summarization framework driven by the AMR graph that shows promising results.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.5165010243654251}, {"text": "AMR graph", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.882932722568512}]}, {"text": "showed that AMR can significantly improve the accuracy of a biomolecular interaction extraction system compared to only using surface-and syntax-based features.", "labels": [], "entities": [{"text": "AMR", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7970306873321533}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9989281296730042}, {"text": "biomolecular interaction extraction", "start_pos": 60, "end_pos": 95, "type": "TASK", "confidence": 0.6358712613582611}]}, {"text": "presented a question-answering system by exploiting the AMR representation, obtaining good performance.", "labels": [], "entities": []}, {"text": "Automatic AMR parsing is still in a nascent stage.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.908075362443924}]}, {"text": "built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.8546555042266846}, {"text": "JAMR", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.833070695400238}, {"text": "concept identification", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.7365768849849701}, {"text": "relation identification", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.8029948770999908}]}, {"text": "Considering that node generation is an important limiting factor in AMR parsing, proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9756278097629547}, {"text": "concept identification subtask", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.7838295499483744}]}, {"text": "However, the overall architecture is still based on the pipelined model.", "labels": [], "entities": []}, {"text": "As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream prediction.", "labels": [], "entities": []}, {"text": "The downstream components, however, cannot impact earlier decision.", "labels": [], "entities": []}, {"text": "For example, for the verb \"affect\" in the example shown in, there exist two possible concepts: \"affect-01\" and \"affect-02\".", "labels": [], "entities": []}, {"text": "Comparatively, the first concept has more common use cases than the second one.", "labels": [], "entities": []}, {"text": "But, when the verb \"affect\" is followed by the noun \"ac-cent\", it should evoke the concept \"affect-02\".", "labels": [], "entities": []}, {"text": "Obviously, the correct concept choice for the verb \"affect\" should exploit a larger context, and even the whole semantic structure of the sentence, which is more probable to be unfolded at the downstream relation identification stage.", "labels": [], "entities": []}, {"text": "This example indicates that it is necessary to allow for the interaction of information between the two stages.", "labels": [], "entities": []}, {"text": "To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model.", "labels": [], "entities": []}, {"text": "The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.8527741432189941}, {"text": "extraction of entity mentions and relations", "start_pos": 148, "end_pos": 191, "type": "TASK", "confidence": 0.8393115599950155}]}, {"text": "However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9196753203868866}]}, {"text": "In addition, the nodes in the graph are latent.", "labels": [], "entities": []}, {"text": "One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework.", "labels": [], "entities": []}, {"text": "To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are required as input, as the MSCG algorithm in JAMR.", "labels": [], "entities": [{"text": "relation identification", "start_pos": 98, "end_pos": 121, "type": "TASK", "confidence": 0.7208396792411804}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9984661340713501}, {"text": "JAMR", "start_pos": 334, "end_pos": 338, "type": "DATASET", "confidence": 0.9067052006721497}]}, {"text": "Secondly, we adopt a segment-based decoder similar to the multiple-beam algorithm) for concept identification, and then incorporate the CWBS algorithm for relation identification into this framework, combining the two subtasks in a single incremental model.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.7650924623012543}, {"text": "relation identification", "start_pos": 155, "end_pos": 178, "type": "TASK", "confidence": 0.8228950202465057}]}, {"text": "For parameter estimation, \"violation-fixing\" perceptron is adopted since it is designed specifically for inexact search in structured learning (.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6909456849098206}]}, {"text": "Experimental results show that the proposed joint framework significantly outperforms the pipelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous studies on AMR parsing, our experiments were performed on the newswire sections of LDC2013E117 and LDC2014T12, and we also follow the official split for training, development and evaluation.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9656575918197632}, {"text": "newswire sections of LDC2013E117", "start_pos": 81, "end_pos": 113, "type": "DATASET", "confidence": 0.8804146647453308}, {"text": "LDC2014T12", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.8180733919143677}]}, {"text": "Finally, we also show our parsers performance on the full LDC2014T12 dataset.", "labels": [], "entities": [{"text": "LDC2014T12 dataset", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.9713957011699677}]}, {"text": "We evaluate the performance of our parser using Smatch v2.0 , which counts the precision, recall and F1 of the concepts and relations together.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.999582827091217}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9984257221221924}, {"text": "F1", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9986080527305603}]}], "tableCaptions": [{"text": " Table 1: Features associated with the concept fragments.", "labels": [], "entities": []}, {"text": " Table 2: Results of two different relation identification algo-", "labels": [], "entities": []}, {"text": " Table 3: Comparison between our joint approaches and the", "labels": [], "entities": []}, {"text": " Table 4: Final results of various methods.", "labels": [], "entities": []}, {"text": " Table 5: Final results on the full LDC2014T12 dataset.", "labels": [], "entities": [{"text": "LDC2014T12 dataset", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.9645269513130188}]}]}