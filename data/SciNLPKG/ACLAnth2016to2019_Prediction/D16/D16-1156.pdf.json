{"title": [{"text": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes", "labels": [], "entities": [{"text": "Resolving Language and Vision Ambiguities", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8561015725135803}, {"text": "Joint Segmentation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.6071043610572815}, {"text": "Prepositional Attachment Resolution in Captioned Scenes", "start_pos": 73, "end_pos": 128, "type": "TASK", "confidence": 0.7180251578489939}]}], "abstractContent": [{"text": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images.", "labels": [], "entities": [{"text": "semantic segmentation", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7009298354387283}, {"text": "prepositional phrase attachment resolution", "start_pos": 75, "end_pos": 117, "type": "TASK", "confidence": 0.7568227499723434}]}, {"text": "Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image.", "labels": [], "entities": []}, {"text": "If we consider the sentence \"I shot an elephant in my pajamas\", looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both.", "labels": [], "entities": []}, {"text": "Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair.", "labels": [], "entities": [{"text": "semantic segmentation", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7488787770271301}, {"text": "prepositional phrase attachment resolution", "start_pos": 95, "end_pos": 137, "type": "TASK", "confidence": 0.7179779708385468}]}, {"text": "We show that our semantic segmentation and preposi-tional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation.", "labels": [], "entities": [{"text": "semantic segmentation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6949176639318466}, {"text": "phrase attachment resolution", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.7892616887887319}]}, {"text": "Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning.", "labels": [], "entities": [{"text": "multiple-module reasoning", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.7158828973770142}]}, {"text": "Our vision and language approach significantly out-performs the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments.", "labels": [], "entities": [{"text": "Stanford Parser (De Marneffe et al., 2006)", "start_pos": 64, "end_pos": 106, "type": "DATASET", "confidence": 0.8675718784332276}]}, {"text": "We also make small improvements over DeepLab-CRF (Chen et al., 2015).", "labels": [], "entities": [{"text": "DeepLab-CRF", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9014295935630798}]}], "introductionContent": [{"text": "Perception and intelligence problems are hard.", "labels": [], "entities": [{"text": "Perception", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9151307344436646}]}, {"text": "Whether we are interested in understanding an im- Ambiguity: (dog next to woman) on couch vs dog next to (woman on couch) Figure 1: Overview of our approach.", "labels": [], "entities": [{"text": "Ambiguity", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9514508247375488}]}, {"text": "We propose a model for simultaneous 2D semantic segmentation and prepositional phrase attachment resolution by reasoning about sentence parses.", "labels": [], "entities": [{"text": "2D semantic segmentation", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6754751801490784}, {"text": "prepositional phrase attachment resolution", "start_pos": 65, "end_pos": 107, "type": "TASK", "confidence": 0.7220726162195206}, {"text": "sentence parses", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7433788478374481}]}, {"text": "The language and vision modules each produce M diverse hypotheses, and the goal is to select a pair of consistent hypotheses.", "labels": [], "entities": []}, {"text": "In this example the ambiguity to be resolved from the image caption is whether the dog is standing on or next to the couch.", "labels": [], "entities": []}, {"text": "Both modules benefit by selecting a pair of compatible hypotheses.", "labels": [], "entities": []}, {"text": "age or a sentence, our algorithms must operate under tremendous levels of ambiguity.", "labels": [], "entities": []}, {"text": "When a human reads the sentence \"I eat sushi with tuna\", it is clear that the preposition phrase \"with tuna\" modifies \"sushi\" and not the act of eating, but this maybe ambiguous to a machine.", "labels": [], "entities": []}, {"text": "This problem of determining whether a prepositional phrase (\"with tuna\") modifies a noun phrase (\"sushi\") or verb phrase (\"eating\") is formally known as Prepositional Phrase Attachment Resolution (PPAR) (.", "labels": [], "entities": [{"text": "Prepositional Phrase Attachment Resolution (PPAR)", "start_pos": 153, "end_pos": 202, "type": "TASK", "confidence": 0.708620297057288}]}, {"text": "Consider the captioned scene shown in Fig-ure 1.", "labels": [], "entities": [{"text": "Fig-ure 1", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9211056530475616}]}, {"text": "The caption \"A dog is standing next to a woman on a couch\" exhibits a PP attachment ambiguity -\"(dog next to woman) on couch\" vs \"dog next to (woman on couch)\".", "labels": [], "entities": []}, {"text": "It is clear that having access to image segmentations can help resolve this ambiguity, and having access to the correct PP attachment can help image segmentation.", "labels": [], "entities": [{"text": "image segmentation", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.8047959208488464}]}, {"text": "There are two main roadblocks that keep us from writing a single unified model (say a graphical model) to perform both tasks: (1) Inaccurate Models -empirical studies) have repeatedly found that models are often inaccurate and miscalibrated -their \"most-likely\" beliefs are placed on solutions far from the ground-truth.", "labels": [], "entities": []}, {"text": "(2) Search Space Explosion -jointly reasoning about multiple modalities is difficult due to the combinatorial explosion of search space ({exponentially-many segmentations} \u00d7 {exponentially-many sentence-parses}).", "labels": [], "entities": [{"text": "Search Space Explosion", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.5703137218952179}]}, {"text": "In this paper, we address the problem of simultaneous object segmentation (also called semantic segmentation) and PPAR in captioned scenes.", "labels": [], "entities": [{"text": "object segmentation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7279102504253387}, {"text": "semantic segmentation)", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.7642004489898682}]}, {"text": "To the best of our knowledge this is the first paper to do so.", "labels": [], "entities": []}, {"text": "Our main thesis is that a set of diverse plausible hypotheses can serve as a concise interpretable summary of uncertainty in vision and language 'modules' (What does the semantic segmentation module see in the world? What does the PPAR module describe?) and form the basis for tractable joint reasoning (How do we reconcile what the semantic segmentation module sees in the world with how the PPAR module describes it?).", "labels": [], "entities": []}, {"text": "Given our two modules with M hypotheses each, how can we integrate beliefs across the segmentation and sentence parse modules to pick the best pair of hypotheses?", "labels": [], "entities": [{"text": "sentence parse", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7194412350654602}]}, {"text": "Our key focus is consistency -correct hypotheses from different modules will be correct in a consistent way, but incorrect hypotheses will be incorrect in incompatible ways.", "labels": [], "entities": [{"text": "consistency", "start_pos": 17, "end_pos": 28, "type": "METRIC", "confidence": 0.9592329263687134}]}, {"text": "Specifically, we develop a MEDIATOR model that scores pairs for consistency and searches overall M 2 pairs to pick the highest scoring one.", "labels": [], "entities": [{"text": "MEDIATOR", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8532447814941406}, {"text": "consistency", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9754598140716553}]}, {"text": "We demonstrate our approach on three datasets -ABSTRACT-50S), PASCAL-50S, and PASCALContext-50S ().", "labels": [], "entities": [{"text": "ABSTRACT-50S", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.5357455611228943}, {"text": "PASCAL-50S", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.7213186025619507}, {"text": "PASCALContext-50S", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.7958647608757019}]}, {"text": "We show that our vision+language approach significantly outperforms the Stanford Parser) by 20.66% (36.42% relative) for ABSTRACT-50S, 17.91% (28.69% relative) for PASCAL-50S, and by 12.83% (25.28% relative) for PASCAL-Context-50S.", "labels": [], "entities": [{"text": "ABSTRACT-50S", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.7102751135826111}, {"text": "PASCAL-Context-50S", "start_pos": 212, "end_pos": 230, "type": "DATASET", "confidence": 0.8579903841018677}]}, {"text": "We also make small but consistent improvements over DeepLab-CRF).", "labels": [], "entities": [{"text": "DeepLab-CRF", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.8801523447036743}]}], "datasetContent": [{"text": "We now describe the setup of our experiments, provide implementation details of the modules, and describe the consistency features.", "labels": [], "entities": []}, {"text": "Access to rich annotated image + caption datasets is crucial for performing quantitative evaluations.", "labels": [], "entities": []}, {"text": "Since this is the first paper to study the problem of joint segmentation and PPAR, no standard datasets for this task exist so we had to curate our own annotations for PPAR on three image caption datasets -ABSTRACT-50S (), PASCAL-50S () (expands the UIUC PASCAL sentence dataset () from 5 captions per image to 50), and PASCAL-Context-50S () (which uses the PAS-CAL Context image annotations and the same sentences as PASCAL-50S).", "labels": [], "entities": [{"text": "ABSTRACT-50S", "start_pos": 206, "end_pos": 218, "type": "METRIC", "confidence": 0.5270822644233704}, {"text": "UIUC PASCAL sentence dataset", "start_pos": 250, "end_pos": 278, "type": "DATASET", "confidence": 0.857527494430542}]}, {"text": "Our annotations are publicly available on the authors' webpages.", "labels": [], "entities": []}, {"text": "To curate the PASCAL-Context-50S PPAR annotations, we first select all sentences that have preposition phrase attachment ambiguities.", "labels": [], "entities": [{"text": "PASCAL-Context-50S PPAR annotations", "start_pos": 14, "end_pos": 49, "type": "DATASET", "confidence": 0.6842030088106791}]}, {"text": "We then plotted the distribution of prepositions in these sentences.", "labels": [], "entities": []}, {"text": "The top 7 prepositions are used, as there is a large drop in the frequencies beyond these.", "labels": [], "entities": []}, {"text": "The 7 prepositions are: \"on\", \"with\", \"next to\", \"in front of\", \"by\", \"near\", and \"down\".", "labels": [], "entities": []}, {"text": "We then further sampled sentences to ensure uniform distribution across prepositions.", "labels": [], "entities": []}, {"text": "We perform a similar filtering for PASCAL-50S and ABSTRACT-50S (using the top-6 prepositions for ABSTRACT-50S).", "labels": [], "entities": [{"text": "PASCAL-50S", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.7060203552246094}, {"text": "ABSTRACT-50S", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8360100984573364}, {"text": "ABSTRACT-50S", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.909579336643219}]}, {"text": "Details are in the supplement.", "labels": [], "entities": []}, {"text": "We consider a preposition ambiguous if there are at least two parsings where one of the two objects in the preposition dependency is the same across the two parsings while the other object is different (e.g. (dog on couch) and (woman on couch)).", "labels": [], "entities": []}, {"text": "To summarize the statistics of all three datasets: 1.", "labels": [], "entities": []}, {"text": "ABSTRACT-50S (): 25,000 sentences (50 per image) with 500 images from abstract scenes made from clipart.", "labels": [], "entities": [{"text": "ABSTRACT-50S", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8499712944030762}]}, {"text": "Filtering for captions containing the top-6 prepositions resulted in 399 sentences describing 201 unique images.", "labels": [], "entities": []}, {"text": "These 6 prepositions are: \"with\", 'next to\", \"on top of\", \"in front of\", \"behind\", and \"under\".", "labels": [], "entities": []}, {"text": "Overall, there are 502 total prepositions, 406 ambiguous prepositions, 80.88% ambiguity rate and 60 sentences with multiple ambiguous prepositions.", "labels": [], "entities": [{"text": "ambiguity rate", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.9284464716911316}]}, {"text": "2. PASCAL-50S () to produce M parsings of the sentence.", "labels": [], "entities": [{"text": "PASCAL-50S", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.784001350402832}]}, {"text": "In addition to the parse trees, the module can also output dependencies, which make syntactical relationships more explicit.", "labels": [], "entities": []}, {"text": "Dependencies come in the form dependency type(word 1 , word 2 ), such as the preposition dependency prep on(woman-8, couch-11) (the number indicates the word position in sentence).", "labels": [], "entities": []}, {"text": "To evaluate, we count the percentage of preposition attachments that the parse gets correct.", "labels": [], "entities": []}, {"text": "In our experiments, we compare our proposed approach (MEDIATOR) to the highest scoring solution predicted independently from each module.", "labels": [], "entities": [{"text": "MEDIATOR", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.98131263256073}]}, {"text": "For semantic segmentation this is the output of DeepLab-CRF ( and for the PPAR module this is the 1-best output of the Stanford Parser Ablative Study.", "labels": [], "entities": [{"text": "semantic segmentation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8098297119140625}, {"text": "Stanford Parser Ablative Study", "start_pos": 119, "end_pos": 149, "type": "DATASET", "confidence": 0.7092095613479614}]}, {"text": "Ours-CASCADE: This ablation studies the importance of multiple hypothesis.", "labels": [], "entities": []}, {"text": "For each module (say y), we feed the single-best output of the other module z 1 as input.", "labels": [], "entities": []}, {"text": "Each module learns its own weight w using exactly the same consistency features and learning algorithm as MEDI-ATOR and predicts one of the plausible hypotheses\u02c6y hypotheses\u02c6 hypotheses\u02c6y CASCADE = argmax y\u2208Y w \u03c6(x, y, z 1 ).", "labels": [], "entities": [{"text": "CASCADE", "start_pos": 188, "end_pos": 195, "type": "METRIC", "confidence": 0.9914876222610474}]}, {"text": "This ablation of our system is similar to and helps us in disentangling the benefits of multiple hypothesis and joint reasoning.", "labels": [], "entities": []}, {"text": "Finally, we note that Ours-CASCADE can be viewed as special cases of MEDIATOR.", "labels": [], "entities": []}, {"text": "Let MEDI-ATOR-(M y , M z ) denote our approach run with M y hypotheses for the first module and M z for the second.", "labels": [], "entities": [{"text": "MEDI-ATOR-(", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.971677154302597}]}, {"text": "Then INDEP corresponds to MEDIATOR-(1, 1) and CASCADE corresponds to predicting they solution from MEDIATOR-(M y , 1) and the z solution from MEDIATOR-(1, M z ).", "labels": [], "entities": [{"text": "INDEP", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9970049262046814}, {"text": "MEDIATOR-(1", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.8608928124109904}, {"text": "CASCADE", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9978035092353821}]}, {"text": "To get an upper-bound on our approach, we report oracle, the accuracy of the most accurate tuple in 10 \u00d7 10 tuples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9994664788246155}]}, {"text": "In the main paper, our results are presented where MEDIATOR was trained with equally weighted loss (\u03b1 = 0.5), but we provide additional results for varying values of \u03b1 in the supplement.", "labels": [], "entities": [{"text": "MEDIATOR", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.6324854493141174}]}, {"text": "Recall that we have two types of features -(1) score features \u03c6 S (y i ) and \u03c6 S (z j ), which try to capture how likely solutions y i and z j are respectively, and (2) consistency features \u03c6 C (y i , z j ), which capture how consistent the PP attachments in z j are with the segmentation in y i . For each (object 1 , preposition, object 2 ) in z j , we compute 6 features between object 1 and object 2 segmentations in y i . Since the humans writing the captions may use multiple synonymous words (e.g. dog, puppy) for the same visual entity, we use word2vec ( similarities to map the nouns in the sentences to the corresponding dataset categories.", "labels": [], "entities": [{"text": "consistency", "start_pos": 169, "end_pos": 180, "type": "METRIC", "confidence": 0.9652541875839233}]}, {"text": "\u2022 Semantic Segmentation Score Features (\u03c6 S (y i )) (2-dim): We use ranks and solution scores from DeepLab-CRF ().", "labels": [], "entities": []}, {"text": "\u2022 PPAR Score Features (\u03c6 S (z i )) (9-dim): We use ranks and the log probability of parses from), and 7 binary indicators for PASCAL (6 for ABSTRACT-50S) denoting which prepositions are present in the parse.: Example on PASCAL-50S (\"A dog is standing next to a woman on a couch.\").", "labels": [], "entities": [{"text": "PPAR Score", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.7109717130661011}, {"text": "PASCAL", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.5906079411506653}, {"text": "ABSTRACT-50S", "start_pos": 140, "end_pos": 152, "type": "DATASET", "confidence": 0.6456382870674133}, {"text": "PASCAL-50S", "start_pos": 220, "end_pos": 230, "type": "DATASET", "confidence": 0.8149263262748718}]}, {"text": "The ambiguity in this sentence \"(dog next to woman) on couch\" vs \"dog next to (woman on couch)\".", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9574865102767944}]}, {"text": "We calculate the horizontal and vertical distances between the segmentation centers of \"person\" and \"couch\" and between the segmentation centers of \"dog\" and \"couch\".", "labels": [], "entities": []}, {"text": "We see that the \"dog\" is much further below the couch (53.91) than the woman (2.65).", "labels": [], "entities": []}, {"text": "So, if the MEDIATOR model learned that \"on\" means the first object is above the second object, we would expect it to choose the \"person on couch\" preposition parsing.", "labels": [], "entities": [{"text": "person on couch\" preposition parsing", "start_pos": 129, "end_pos": 165, "type": "TASK", "confidence": 0.6841805875301361}]}, {"text": "\u2022 Inter-Module Consistency Features (56-dim): For each of the 7 prepositions, 8 features are calculated: -One feature is the Euclidean distance between the center of the segmentation masks of the two objects connected by the preposition.", "labels": [], "entities": []}, {"text": "These two objects in the segmentation correspond to the categories with which the soft similarity of the two objects in the sentence is highest among all PASCAL categories.", "labels": [], "entities": []}, {"text": "-Four features capture max{0, (normalized -directional-distance)}, where directionaldistance measures above/below/left/right displacements between the two objects in the segmentation, and normalization involves dividing by height/width.", "labels": [], "entities": []}, {"text": "-One feature is the ratio of sizes between object 1 and object 2 in the segmentation.", "labels": [], "entities": []}, {"text": "-Two features capture the word2vec similarity between the two objects in PPAR (say 'puppy' and 'kitty') with their most similar PASCAL category (say 'dog' and 'cat'), where these features are 0 if the categories are not present in segmentation.", "labels": [], "entities": []}, {"text": "A visual illustration for some of these features for PASCAL can be seen in.", "labels": [], "entities": [{"text": "PASCAL", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.6226365566253662}]}, {"text": "In the case where an object parsed from z j is not present in the segmentation y i , the distance features are set to 0.", "labels": [], "entities": []}, {"text": "The ratio of areas features (area of smaller object / area of larger object) are also set to 0 assuming that the smaller object is missing.", "labels": [], "entities": []}, {"text": "In the case where an object has two or more connected components in the segmentation, the distances are computed w.r.t. the centroid of the segmentation and the area is computed as the number of pixels in the union of the instance segmentation masks.", "labels": [], "entities": []}, {"text": "We also calculate 20 features for PASCAL-50S and 59 features for PASCAL-Context-50S that capture that consistency between y i and z j , in terms of presence/absence of PASCAL categories.", "labels": [], "entities": [{"text": "PASCAL-50S", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.7646571397781372}]}, {"text": "For each noun in PPAR we compute its word2vec similarity with all PASCAL categories.", "labels": [], "entities": [{"text": "PPAR", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.8159226179122925}, {"text": "word2vec similarity", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.6436994075775146}]}, {"text": "For each of the PASCAL categories, the feature is the sum of similarities (with the PASCAL category) overall nouns if the category is present in segmentation, and is -1 times the sum of similarities overall nouns otherwise.", "labels": [], "entities": []}, {"text": "This feature set was not used for ABSTRACT-50S, since these features were intended to help improve the accuracy of the semantic segmentation module.", "labels": [], "entities": [{"text": "ABSTRACT-50S", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.7357821464538574}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9978669881820679}, {"text": "semantic segmentation module", "start_pos": 119, "end_pos": 147, "type": "TASK", "confidence": 0.7597660024960836}]}, {"text": "For ABSTRACT-50S, we only use the 5 distance features, resulting in a 30-dim feature vector.", "labels": [], "entities": [{"text": "ABSTRACT-50S", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7471915483474731}]}], "tableCaptions": [{"text": " Table 1: Results on our subset of ABSTRACT-50S.", "labels": [], "entities": [{"text": "ABSTRACT-50S", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.7285240888595581}]}, {"text": " Table 2: Results on our subset of the PASCAL-50S and PASCAL-Context-50S datasets. We are able to significantly  outperform the Stanford Parser and make small improvements over DeepLab-CRF for PASCAL-50S.", "labels": [], "entities": [{"text": "PASCAL-Context-50S datasets", "start_pos": 54, "end_pos": 81, "type": "DATASET", "confidence": 0.897872656583786}]}]}