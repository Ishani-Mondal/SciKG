{"title": [{"text": "Jointly Embedding Knowledge Graphs and Logical Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest.", "labels": [], "entities": []}, {"text": "Most existing methods perform the embedding task using only fact triples.", "labels": [], "entities": []}, {"text": "Logical rules, although containing rich background information, have not been well studied in this task.", "labels": [], "entities": []}, {"text": "This paper proposes a novel method of jointly embedding knowledge graphs and logical rules.", "labels": [], "entities": []}, {"text": "The key idea is to represent and model triples and rules in a unified framework.", "labels": [], "entities": []}, {"text": "Specifically, triples are represented as atomic formulae and modeled by the translation assumption , while rules represented as complex formulae and modeled by t-norm fuzzy logics.", "labels": [], "entities": []}, {"text": "Embedding then amounts to minimizing a global loss over both atomic and complex for-mulae.", "labels": [], "entities": []}, {"text": "In this manner, we learn embeddings compatible not only with triples but also with rules, which will certainly be more predictive for knowledge acquisition and inference.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 134, "end_pos": 155, "type": "TASK", "confidence": 0.7717807292938232}]}, {"text": "We evaluate our method with link prediction and triple classification tasks.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7782580256462097}, {"text": "triple classification", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.6619329005479813}]}, {"text": "Experimental results show that joint embedding brings significant and consistent improvements over state-of-the-art methods.", "labels": [], "entities": []}, {"text": "Particularly, it enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of our method to learn more predictive embeddings.", "labels": [], "entities": [{"text": "prediction of new facts", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.833009198307991}]}], "introductionContent": [{"text": "Knowledge graphs (KGs) provide rich structured information and have become extremely useful resources for many NLP related applications like * Corresponding author: Quan word sense disambiguation) and information extraction.", "labels": [], "entities": [{"text": "* Corresponding author: Quan word sense disambiguation", "start_pos": 141, "end_pos": 195, "type": "TASK", "confidence": 0.4978306330740452}, {"text": "information extraction", "start_pos": 201, "end_pos": 223, "type": "TASK", "confidence": 0.8563558161258698}]}, {"text": "A typical KG represents knowledge as multi-relational data, stored in triples of the form (head entity, relation, tail entity), e.g.,.", "labels": [], "entities": []}, {"text": "Although powerful in representing structured data, the symbolic nature of such triples makes KGs, especially large-scale KGs, hard to manipulate.", "labels": [], "entities": []}, {"text": "Recently, a promising approach, namely knowledge graph embedding, has been proposed and successfully applied to various).", "labels": [], "entities": []}, {"text": "The key idea is to embed components of a KG including entities and relations into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the KG.", "labels": [], "entities": []}, {"text": "The embeddings contain rich semantic information about entities and relations, and can significantly enhance knowledge acquisition and inference ).", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.7377243041992188}]}, {"text": "Most existing methods perform the embedding task based solely on fact triples ().", "labels": [], "entities": []}, {"text": "The only requirement is that the learned embeddings should be compatible with those facts.", "labels": [], "entities": []}, {"text": "While logical rules contain rich background information and are extremely useful for knowledge acquisition and inference), they have not been well studied in this task.  and tried to leverage both embedding methods and logical rules for KG completion.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7606523931026459}, {"text": "KG completion", "start_pos": 237, "end_pos": 250, "type": "TASK", "confidence": 0.9328919351100922}]}, {"text": "In their work, however, rules are modeled separately from embedding methods, serving as postprocessing steps, and thus will not help to obtain better embeddings.", "labels": [], "entities": []}, {"text": "recently proposed a joint model which injects first-order logic into embeddings.", "labels": [], "entities": []}, {"text": "But it focuses on the relation extraction task, and creates vector embeddings for entity pairs rather than individual entities.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8155439794063568}]}, {"text": "Since entities do not have their own embeddings, relations between unpaired entities cannot be effectively discovered (.", "labels": [], "entities": []}, {"text": "In this paper we introduce KALE, anew approach that learns entity and relation Embeddings by jointly modeling Knowledge And Logic.", "labels": [], "entities": [{"text": "KALE", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.6326459646224976}]}, {"text": "Knowledge triples are taken as atoms and modeled by the translation assumption, i.e., relations act as translations between head and tail entities ( . A triple (e i , r k , e j ) is scored by \u2225e i + r k \u2212 e j \u2225 1 , where e i , r k , and e j are the vector embeddings for entities and relations.", "labels": [], "entities": []}, {"text": "The score is then mapped to the unit interval to indicate the truth value of that triple.", "labels": [], "entities": []}, {"text": "Logical rules are taken as complex formulae constructed by combining atoms with logical connectives (e.g., \u2227 and \u21d2), and modeled by t-norm fuzzy logics.", "labels": [], "entities": []}, {"text": "The truth value of a rule is a composition of the truth values of the constituent atoms, defined by specific logical connectives.", "labels": [], "entities": []}, {"text": "In this way, KALE represents triples and rules in a unified framework, as atomic and complex formulae respectively.", "labels": [], "entities": []}, {"text": "gives a simple illustration of the framework.", "labels": [], "entities": []}, {"text": "After unifying triples and rules, KALE minimizes a global loss involving both of them to obtain entity and relation embeddings.", "labels": [], "entities": []}, {"text": "The learned embeddings are therefore compatible not only with triples but also with rules, which will definitely be more predictive for knowledge acquisition and inference.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.7556620836257935}]}, {"text": "The main contributions of this paper are summarized as follows.", "labels": [], "entities": []}, {"text": "(i) We devise a unified framework that jointly models triples and rules to obtain more predictive entity and relation embeddings.", "labels": [], "entities": []}, {"text": "The new framework KALE is general enough to handle any type of rules that can be represented as first-order logic formulae.", "labels": [], "entities": []}, {"text": "(ii) We evaluate KALE with link prediction and triple classification tasks on WordNet and.", "labels": [], "entities": [{"text": "KALE", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.7599186301231384}, {"text": "link prediction", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7303162813186646}, {"text": "triple classification", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6693694144487381}, {"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9861634373664856}]}, {"text": "Experimental results show significant and consistent improvements over state-of-the-art methods.", "labels": [], "entities": []}, {"text": "Particularly, joint embedding enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of KALE to learn more predictive embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically evaluate KALE with two tasks: (i) link prediction and (ii) triple classification.", "labels": [], "entities": [{"text": "KALE", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.8462814092636108}, {"text": "link prediction", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7854598462581635}, {"text": "triple classification", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7036942839622498}]}, {"text": "We further create logical rules for each dataset, in the form of \u2200x, y : (x, r s , y) \u21d2 (x, rt , y) or \u2200x, y, z : (x, r s 1 , y) \u2227 (y, r s 2 , z) \u21d2 (x, rt , z).", "labels": [], "entities": []}, {"text": "To do so, we first run TransE to get entity and relation embeddings, and calculate the truth value for each of such rules according to Eq.", "labels": [], "entities": []}, {"text": "(2) or Eq.", "labels": [], "entities": [{"text": "Eq", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.891132116317749}]}, {"text": "Then we rank all such rules by their truth values and manually filter those ranked at the top.", "labels": [], "entities": []}, {"text": "We finally create 47 rules on FB122, and 14 on WN18 (see for examples).", "labels": [], "entities": [{"text": "FB122", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9346615672111511}, {"text": "WN18", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.9800994992256165}]}, {"text": "The rules are then instantiated with concrete entities (grounding).", "labels": [], "entities": []}, {"text": "Ground rules in which at least one constituent triple is observed in the training set are used in joint learning.", "labels": [], "entities": []}, {"text": "Note that some of the test triples can be inferred by directly applying these rules on the training set (pure logical inference).", "labels": [], "entities": []}, {"text": "On each dataset, we further split the test set into two parts, test-I and test-II.", "labels": [], "entities": []}, {"text": "The former contains triples that cannot be directly inferred by pure logical inference, and the latter the remaining test triples.", "labels": [], "entities": []}, {"text": "gives some statistics of the datasets, including the number of entities, relations, triples in training/validation/test-I/test-II set, and ground rules.  and TransR are extensions of TransE.", "labels": [], "entities": []}, {"text": "They further allow entities to have distinct embeddings when involved in different relations, by introducing relationspecific hyperplanes and projection matrices respectively.", "labels": [], "entities": []}, {"text": "All the three methods have been demonstrated to perform well on WordNet and Freebase data.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9711355566978455}, {"text": "Freebase data", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9226270318031311}]}, {"text": "We further test our approach in three different scenarios.", "labels": [], "entities": []}, {"text": "(i) KALE-Trip uses triples alone to perform the embedding task, i.e., only the training triples are included in the optimization Eq.", "labels": [], "entities": []}, {"text": "It is a linearly transformed version of TransE.", "labels": [], "entities": []}, {"text": "The only difference is that relation embeddings are normalized in KALE-Trip, but not in TransE.", "labels": [], "entities": [{"text": "KALE-Trip", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.7920252084732056}, {"text": "TransE", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.9160450100898743}]}, {"text": "(ii) KALE-Pre first repeats pure logical inference on the training set and adds inferred triples as additional training data, until no further triples can be inferred.", "labels": [], "entities": [{"text": "KALE-Pre", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.7652561664581299}]}, {"text": "Both original and inferred triples are then included in the optimization.", "labels": [], "entities": []}, {"text": "For example, given a logical rule \u2200x, y : (x, r s , y) \u21d2 (x, rt , y), anew triple (e i , rt , e j ) can be inferred if (e i , r s , e j ) is observed in the training set, and both triples will be used as training instances for embedding.", "labels": [], "entities": []}, {"text": "(iii) KALE-Joint is the joint learning scenario, which considers both training triples and ground rules in the optimization.", "labels": [], "entities": [{"text": "KALE-Joint", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.4934127628803253}]}, {"text": "In the aforementioned example, training triple (e i , r s , e j ) and ground rule (e i , r s , e j ) \u21d2 (e i , rt , e j ) will be used in the training process of KALE-Joint, without explicitly incorporating triple (e i , rt , e j ).", "labels": [], "entities": [{"text": "KALE-Joint", "start_pos": 161, "end_pos": 171, "type": "DATASET", "confidence": 0.7578319907188416}]}, {"text": "However, to ensure fair comparison, we randomly initialize all the methods in our experiments.", "labels": [], "entities": []}, {"text": "For all the methods, we create 100 mini-batches on each dataset, and tune the embedding dimension din {20, 50, 100}.", "labels": [], "entities": []}, {"text": "For TransE, TransH, and TransR which score a triple by a distance in R + , we tune the learning rate \u03b7 in {0.001, 0.01, 0.1}, and the margin \u03b3 in {1, 2, 3, 4}.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9651424487431844}]}, {"text": "For KALE which scores a triple (as well as aground rule) by a soft truth value in the unit interval, we set the learning rate \u03b7 in {0.01, 0.02, 0.05, 0.1}, and the margin \u03b3 in {0.1, 0.12, 0.15, 0.2}.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 112, "end_pos": 127, "type": "METRIC", "confidence": 0.9025676449139913}]}, {"text": "KALE allows triples and rules to have different weights, with the former fixed to 1, and the latter (denoted by \u03bb) selected in {0.001, 0.01, 0.1, 1}.", "labels": [], "entities": [{"text": "KALE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6453890204429626}]}], "tableCaptions": [{"text": " Table 3: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 4: Link prediction results on the test-I, test-II, and test-all sets of FB122 and WN18 (raw setting).", "labels": [], "entities": [{"text": "FB122", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.5525754690170288}, {"text": "WN18", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8641559481620789}]}, {"text": " Table 5: Link prediction results on the test-I, test-II, and test-all sets of FB122 and WN18 (filtered setting).", "labels": [], "entities": [{"text": "FB122", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.6545352339744568}, {"text": "WN18", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.873461127281189}]}, {"text": " Table 6: Comparison between KALE-Trip and KALE-Joint on Test-Incl and Test-Excl of FB122 and WN18.", "labels": [], "entities": [{"text": "KALE-Trip", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.7953736782073975}, {"text": "FB122", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.9648247361183167}, {"text": "WN18", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.8221087455749512}]}]}