{"title": [{"text": "IRT-based Aggregation Model of Crowdsourced Pairwise Comparisons for Evaluating Machine Translations", "labels": [], "entities": [{"text": "IRT-based Aggregation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.815635472536087}, {"text": "Evaluating Machine Translations", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7120036780834198}]}], "abstractContent": [{"text": "Recent work on machine translation has used crowdsourcing to reduce costs of manual evaluations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8354938924312592}]}, {"text": "However, crowdsourced judgments are often biased and inaccurate.", "labels": [], "entities": []}, {"text": "In this paper , we present a statistical model that aggregates many manual pairwise comparisons to robustly measure a machine translation system's performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7364727258682251}]}, {"text": "Our method applies graded response model from item response theory (IRT), which was originally developed for academic tests.", "labels": [], "entities": [{"text": "item response theory (IRT)", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.7594264000654221}]}, {"text": "We conducted experiments on a public dataset from the Workshop on Statistical Machine Translation 2013, and found that our approach resulted in highly in-terpretable estimates and was less affected by noisy judges than previously proposed methods .", "labels": [], "entities": [{"text": "Statistical Machine Translation 2013", "start_pos": 66, "end_pos": 102, "type": "TASK", "confidence": 0.7290268614888191}]}], "introductionContent": [{"text": "Manual evaluation is a primary means of interpreting the performance of machine translation (MT) systems and evaluating the accuracy of automatic evaluation metrics.", "labels": [], "entities": [{"text": "Manual evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7030122876167297}, {"text": "machine translation (MT)", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.817809522151947}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9971664547920227}]}, {"text": "It is also essential for natural language processing tasks such as summarization and dialogue systems, where (1) the number of correct outputs is unlimited, and (2) na\u00a8\u0131vena\u00a8\u0131ve text matching cannot judge the correctness, that is, an evaluator must consider syntactic and semantic information.", "labels": [], "entities": [{"text": "summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9836904406547546}]}, {"text": "Recent work has used crowdsourcing to reduce costs of manual evaluations.", "labels": [], "entities": []}, {"text": "However, the judgments of crowd workers are often noisy and unreliable because they are not experts.", "labels": [], "entities": []}, {"text": "To maintain quality, evaluation tasks implemented using crowdsourcing should be simple.", "labels": [], "entities": []}, {"text": "Thus, many previous studies focused on pairwise comparisons instead of absolute evaluations.", "labels": [], "entities": []}, {"text": "The same task is given to multiple workers, and their responses are aggregated to obtain a reliable answer.", "labels": [], "entities": []}, {"text": "We must, therefore, develop methods that robustly estimate the MT performance based on many pairwise comparisons.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9855154752731323}]}, {"text": "Some aggregation methods have been proposed for MT competitions hosted by the Workshop on Statistical Machine Translation (WMT) (, where a ranking of the submitted systems is produced by aggregating many manual judgments of pairwise comparisons of system outputs.", "labels": [], "entities": [{"text": "MT competitions", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.9222134947776794}, {"text": "Statistical Machine Translation (WMT)", "start_pos": 90, "end_pos": 127, "type": "TASK", "confidence": 0.7657273858785629}]}, {"text": "However, existing methods do not consider the following important issues.", "labels": [], "entities": []}, {"text": "Interpretability of the estimates: For the purpose of evaluation, their results must be interpretable so that we could use the results to improve MT systems and the next MT evaluation campaigns.", "labels": [], "entities": [{"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9776326417922974}, {"text": "MT evaluation", "start_pos": 170, "end_pos": 183, "type": "TASK", "confidence": 0.9263508319854736}]}, {"text": "Existing methods, however, only yield system-level scores.", "labels": [], "entities": []}, {"text": "Judge sensitivity: Some judges can examine the quality of translations with consistent standards, but others cannot (.", "labels": [], "entities": []}, {"text": "Sensitivities to the translation quality and judges' own standards are important factors.", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9651532173156738}]}, {"text": "Evaluation of a newly submitted system: Previous approaches considered all pairwise combinations of systems and must compare a newly submitted system with all the submitted systems.", "labels": [], "entities": []}, {"text": "This made it difficult to allow participants to submit their systems after starting the evaluation step.", "labels": [], "entities": []}, {"text": "To address these issues, we use a model from item response theory (IRT).", "labels": [], "entities": [{"text": "item response theory (IRT)", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.786568949619929}]}, {"text": "This theory was originally developed for psychometrics, and has applications to academic tests.", "labels": [], "entities": []}, {"text": "IRT models are highly interpretable and are supported by theoretical and empirical studies.", "labels": [], "entities": [{"text": "IRT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9797353744506836}]}, {"text": "For example, we can estimate the informativeness of a question in a test based on the responses of examinees.", "labels": [], "entities": []}, {"text": "We focused on aggregating many pairwise comparisons with a baseline translation so that we could use the analogy of standard academic tests.", "labels": [], "entities": []}, {"text": "Each system of interest yields translations, and the translations are compared with a baseline translation by multiple human judges.", "labels": [], "entities": []}, {"text": "Each judge produces a preference judgment.", "labels": [], "entities": []}, {"text": "The pairwise comparisons correspond to questions in academic tests, a judge's sensitivity to the translation quality is mapped to discrimination of questions, and the relative difficulty of winning the pairwise comparison is mapped to the difficulty of questions.", "labels": [], "entities": []}, {"text": "MT systems correspond to students that take academic tests, and IRT models can be naturally applied to estimate the latent performance (ability) of MT systems (students).", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8929468393325806}]}, {"text": "Additionally, our approach, fixing baseline translations, can easily evaluate a newly submitted system.", "labels": [], "entities": []}, {"text": "We only need to compare the new system with the baseline instead of testing all pairwise combinations of the submitted systems.", "labels": [], "entities": []}, {"text": "Our contributions are summarized as follows.", "labels": [], "entities": []}, {"text": "We propose an IRT-based aggregation model of pairwise comparisons with highly interpretable parameters.", "labels": [], "entities": [{"text": "IRT-based", "start_pos": 14, "end_pos": 23, "type": "TASK", "confidence": 0.9505035281181335}]}, {"text": "2. We simulated noisy judges on the WMT13 dataset and demonstrated that our model is less affected by the noisy judges than previously proposed methods.", "labels": [], "entities": [{"text": "WMT13 dataset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9865323007106781}]}], "datasetContent": [{"text": "We conducted experiments on the WMT13 manual evaluation dataset for 10 language pairs.", "labels": [], "entities": [{"text": "WMT13 manual evaluation dataset", "start_pos": 32, "end_pos": 63, "type": "DATASET", "confidence": 0.8760893493890762}]}, {"text": "3 For details of the evaluation data, seethe overview of WMT13 ().", "labels": [], "entities": [{"text": "WMT13", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.8998556137084961}]}], "tableCaptions": [{"text": " Table 1: Correlation and nDCG between the estimated  system performance and gold scores for the WMT13  Spanish-English task, based on noisy judges. The val- ues were averaged over all the datasets. The GRM scores  were averaged over all baselines. The differences from  the GRM are reported for the HM and EW.", "labels": [], "entities": [{"text": "nDCG", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9814990162849426}, {"text": "WMT13  Spanish-English task", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.587940533955892}, {"text": "val- ues", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9566815495491028}, {"text": "GRM", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.9840832352638245}, {"text": "GRM", "start_pos": 275, "end_pos": 278, "type": "METRIC", "confidence": 0.9317021369934082}]}]}