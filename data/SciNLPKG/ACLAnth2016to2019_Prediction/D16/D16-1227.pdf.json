{"title": [{"text": "Learning to refine text based recommendations", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a text-based recommendation engine that utilizes recurrent neural networks to flexibly map textual input into continuous vector representations tailored to the recommendation task.", "labels": [], "entities": []}, {"text": "Here, the text objects are documents such as Wikipedia articles or question and answer pairs.", "labels": [], "entities": []}, {"text": "As neural models require substantial training time, we introduce a sequential component so as to quickly adjust the learned metric over objects as additional evidence accrues.", "labels": [], "entities": []}, {"text": "We evaluate the approach on recommending Wikipedia descriptions of ingredients to their associated product categories.", "labels": [], "entities": []}, {"text": "We also exemplify the sequential metric adjustment on retrieving similar Stack Exchange AskUbuntu questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern recommender problems involve complex objects, often described in textual form.", "labels": [], "entities": []}, {"text": "In order to learn to predict how disparate objects may go together, it is helpful to first map them into a common representation where they are easily compared, regardless of their origin.", "labels": [], "entities": []}, {"text": "Neural models are particularly well-suited for this task as continuous vector representations of objects can be tailored in a flexible way to the desired task.", "labels": [], "entities": []}, {"text": "While these models have been shown to be effective across NLP tasks (, they take considerable time to learn and are therefore ill-suited to be adjusted rapidly as additional evidence accumulates.", "labels": [], "entities": []}, {"text": "We cast our text-to-text recommendation problem in two phases.", "labels": [], "entities": [{"text": "text-to-text recommendation", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6565914899110794}]}, {"text": "In the first phase, flexible neural textto-vector mappings are learned from currently available data.", "labels": [], "entities": []}, {"text": "Such mappings are optimized to function well in a collaborative filtering setting.", "labels": [], "entities": []}, {"text": "For example, in the context of recommending food product categories for ingredients based on their Wikipedia pages, the continuous vectors are adjusted so that their inner product directly reflects the degree of association between the objects.", "labels": [], "entities": []}, {"text": "Once learned, the mapping can be applied to any previously unseen text to yield the corresponding vector representation, and therefore also used for predicting associations.", "labels": [], "entities": [{"text": "predicting associations", "start_pos": 149, "end_pos": 172, "type": "TASK", "confidence": 0.8826893270015717}]}, {"text": "In the second phase, we no longer adjust text-tovector mappings but rather parameterize and learn how the vectors are compared.", "labels": [], "entities": []}, {"text": "For example, we can optimize the metric separately for each new ingredient based on a few category observations for that ingredient.", "labels": [], "entities": []}, {"text": "The goal of this second phase is to specifically boost the accuracy when the neural baseline (unaware of the new evidence) would otherwise not perform well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995755553245544}]}, {"text": "Our approach builds on the recent work on recurrent convolutional models to obtain text-to-vector mappings (.", "labels": [], "entities": []}, {"text": "This architecture is particularly well suited for noisy Wikipedia pages as it can learn to omit and highlight different parts of the text, as needed.", "labels": [], "entities": []}, {"text": "The additional sequential component is a regularized logistic regression model (for ingredient-product prediction) or a ranking model (for question retrieval).", "labels": [], "entities": [{"text": "ingredient-product prediction", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.722876787185669}, {"text": "question retrieval", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7573591768741608}]}, {"text": "We demonstrate the accuracy of the baseline neural recommender and the gains from the second sequential phase in both of these tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9996150732040405}]}], "datasetContent": [{"text": "Ingredients: We use the FoodEssentials LabelAPI 2 and Rapid Alert System for Food and Feed (RASFF) 3 databases to extract 5439 ingredients and the product categories they appear in.", "labels": [], "entities": [{"text": "FoodEssentials LabelAPI 2", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.8157172997792562}, {"text": "Rapid Alert System", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.9226942459742228}]}, {"text": "On average, each ingredient appears in 16.3 product categories (out of 131 categories).", "labels": [], "entities": []}, {"text": "We leverage Mechanical Turk to link each ingredient to the appropriate Wikipedia article.", "labels": [], "entities": []}, {"text": "From the 5439 ingredients, there are 1680 unique Wikipedia articles.", "labels": [], "entities": []}, {"text": "Each ingredient summary description has a median of 169 tokens.", "labels": [], "entities": []}, {"text": "AskUbuntu: The dataset consists of 167k questions and 16k user-marked similar question pairs taking from a 2014 dump of AskUbuntu website.", "labels": [], "entities": [{"text": "AskUbuntu", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9163715243339539}]}], "tableCaptions": [{"text": " Table 2: We show the mean absolute improvement in the mean average precision (MAP) over the unobserved data points for each", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 55, "end_pos": 83, "type": "METRIC", "confidence": 0.9347531696160635}]}, {"text": " Table 3: Results of the RNN model on the ingredient dataset,", "labels": [], "entities": [{"text": "ingredient dataset", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.6680394560098648}]}]}