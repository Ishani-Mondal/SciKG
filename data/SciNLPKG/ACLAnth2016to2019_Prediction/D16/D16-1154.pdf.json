{"title": [{"text": "Long-Short Range Context Neural Networks for Language Modeling", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7338449060916901}]}], "abstractContent": [{"text": "The goal of language modeling techniques is to capture the statistical and structural properties of natural languages from training corpora.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7266809642314911}]}, {"text": "This task typically involves the learning of short range dependencies, which generally model the syntactic properties of a language and/or long range dependencies, which are semantic in nature.", "labels": [], "entities": []}, {"text": "We propose in this paper anew multi-span architecture, which separately models the short and long context information while it dynamically merges them to perform the language modeling task.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.713009774684906}]}, {"text": "This is done through a novel recurrent Long-Short Range Context (LSRC) network, which explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time.", "labels": [], "entities": []}, {"text": "This new architecture is an adaptation of the Long-Short Term Memory network (LSTM) to take into account the linguistic properties.", "labels": [], "entities": []}, {"text": "Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art language modeling techniques.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.9728414297103882}, {"text": "Large Text Compression Benchmark (LTCB) corpus", "start_pos": 67, "end_pos": 113, "type": "DATASET", "confidence": 0.5620830655097961}]}], "introductionContent": [{"text": "A high quality Language Model (LM) is considered to bean integral component of many systems for speech and language technology applications, such as machine translation), speech recognition, etc.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.778808981180191}, {"text": "speech recognition", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.8328324556350708}]}, {"text": "The goal of an LM is to identify and predict probable sequences of predefined linguistic units, which are typically words.", "labels": [], "entities": []}, {"text": "These predictions are typically guided by the semantic and syntactic properties encoded by the LM.", "labels": [], "entities": []}, {"text": "In order to capture these properties, classical LMs were typically developed as fixed (short) context techniques such as, the word count-based methods, commonly known as N -gram language models, as well as the Feedforward Neural Networks (FFNN), which were introduced as an alternative to overcome the exponential growth of parameters required for larger context sizes in N -gram models.", "labels": [], "entities": []}, {"text": "In order to overcome the short context constraint and capture long range dependencies known to be present in language, proposed to use Latent Semantic Analysis (LSA) to capture the global context, and then combine it with the standard N -gram models, which capture the local context.", "labels": [], "entities": []}, {"text": "Ina similar but more recent approach, showed that Recurrent Neural Network (RNN)-based LM performance can be significantly improved using an additional global topic information obtained using Latent Dirichlet Allocation (LDA).", "labels": [], "entities": [{"text": "Recurrent Neural Network (RNN)-based LM", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.5506367571651936}]}, {"text": "In fact, although recurrent architectures theoretically allow the context to indefinitely cycle in the network, Hai have shown that, in practice, this information changes quickly in the classical RNN ( structure, and that it is experimentally equivalent to an 8-gram FFNN.", "labels": [], "entities": []}, {"text": "Another alternative to model linguistic dependencies, Long-Short Term Memory (LSTM), addresses some learning issues from the original RNN by controlling the longevity of context information in the net-work.", "labels": [], "entities": []}, {"text": "This architecture, however, does not particularly model long/short context but rather uses a single state to model the global linguistic context.", "labels": [], "entities": []}, {"text": "Motivated by the works in, this paper proposes a novel neural architecture which explicitly models 1) the local (short) context information, generally syntactic, as well as 2) the global (long) context, which is semantic in nature, using two separate recurrent hidden states.", "labels": [], "entities": []}, {"text": "These states evolve in parallel within a long-short range context network.", "labels": [], "entities": []}, {"text": "In doing so, the proposed architecture is particularly adapted to model natural languages that manifest local-global context information in their linguistic properties.", "labels": [], "entities": []}, {"text": "Section 2 presents a brief overview of short vs long range context language modeling techniques.", "labels": [], "entities": [{"text": "context language modeling", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6678542892138163}]}, {"text": "Section 3 introduces the novel architecture, Long-Short Range Context (LSRC), which explicitly models these two dependencies.", "labels": [], "entities": [{"text": "Long-Short Range Context (LSRC)", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6615057239929835}]}, {"text": "Then, Section 4 evaluates the proposed network in comparison to different state-of-the-art language models on the PTB and the LTCB corpus.", "labels": [], "entities": [{"text": "PTB", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.9613384008407593}, {"text": "LTCB corpus", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.9543559551239014}]}, {"text": "Finally, we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed architecture on two different benchmark tasks.", "labels": [], "entities": []}, {"text": "The first set of experiments was conducted on the commonly used Penn Treebank (PTB) corpus using the same experimental setup adopted in (Mikolov et al., 2011) and (.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) corpus", "start_pos": 64, "end_pos": 90, "type": "DATASET", "confidence": 0.9773207704226176}]}, {"text": "Namely, sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation an testing, respectively.", "labels": [], "entities": [{"text": "validation", "start_pos": 88, "end_pos": 98, "type": "TASK", "confidence": 0.9505118131637573}]}, {"text": "The vocabulary was limited to the most 10k frequent words while the remaining words were mapped to the token <unk>.", "labels": [], "entities": []}, {"text": "In order to evaluate how the proposed approach performs on large corpora in comparison to other methods, we run a second set of experiments on the Large Text Compression Benchmark (LTCB) (Mahoney, 2011).", "labels": [], "entities": []}, {"text": "This corpus is based on the enwik9 dataset which contains the first 10 9 bytes of enwiki-20060303-pages-articles.xml.", "labels": [], "entities": [{"text": "enwik9 dataset", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9698163270950317}]}, {"text": "We adopted the same training-test-validation data split as well as the the same data processing 1 which were used in ().", "labels": [], "entities": []}, {"text": "The vocabulary is limited to the most 80k frequent words with all remaining words replaced by <unk>.", "labels": [], "entities": []}, {"text": "Details about the sizes of these two corpora can be found in Similarly to the RNN LM toolkit 2 (Mikolov et al., 2011), we have used a single end sentence tag between each two consecutive sentences, whereas the begin sentence tag was not included 3 .  For the PTB experiments, the FFNN and FOFE models use a word embedding size of 200, whereas the hidden layer(s) size is fixed at 400, with all hidden units using the Rectified Linear Unit (ReLu) i.e., f (x) = max(0, x) as activation function.", "labels": [], "entities": [{"text": "PTB", "start_pos": 259, "end_pos": 262, "type": "DATASET", "confidence": 0.8345853090286255}, {"text": "FFNN", "start_pos": 280, "end_pos": 284, "type": "DATASET", "confidence": 0.8077810406684875}, {"text": "FOFE", "start_pos": 289, "end_pos": 293, "type": "DATASET", "confidence": 0.4946151673793793}]}, {"text": "We also use the same learning setup adopted in ().", "labels": [], "entities": []}, {"text": "Namely, we use the stochastic gradient descent algorithm with a mini-batch size of 200, the learning rate is initialized to 0.4, the momentum is set to 0.9, the weight decay is fixed at 4\u00d710 \u22125 , whereas the training is done in epochs.", "labels": [], "entities": []}, {"text": "The weights initialization follows the normalized initialization proposed in.", "labels": [], "entities": []}, {"text": "Similarly to, the learning rate is halved when no significant improvement of the validation data log-likelihood is observed.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 18, "end_pos": 31, "type": "METRIC", "confidence": 0.9617518186569214}]}, {"text": "Then, we continue with seven more epochs while halving the learning rate after each epoch.", "labels": [], "entities": []}, {"text": "Regarding the recurrent models, we use f = tanh(\u00b7) as activation function for all recurrent layers, whereas \"f = sigmoid(\u00b7)\" is used for the input, forget and output gates of LSTM and LSRC.", "labels": [], "entities": []}, {"text": "The additional non-recurrent layer in D-LSRC, however, uses the ReLu activation function.", "labels": [], "entities": []}, {"text": "The word embedding size was set to 200 for LSTM and LSRC whereas it is the same as the hidden layer size for RNN (result of the RNN equation 4).", "labels": [], "entities": []}, {"text": "In order to illustrate the effectiveness of the LSRC model, we also report the results when the embedding size is fixed at 100, LSRC(100).", "labels": [], "entities": [{"text": "LSRC", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.8548422455787659}]}, {"text": "The training uses the BPTT algorithm for 5 time steps.", "labels": [], "entities": [{"text": "BPTT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.6245153546333313}]}, {"text": "Similarly to short context models, the mini-batch was set to 200.", "labels": [], "entities": []}, {"text": "The learning rate, however, was set to 1.0 and the weight decay to 5 \u00d7 10 \u22125 . The use of momentum did not lead to any additional improvement.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9685745537281036}, {"text": "weight decay", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.935920000076294}]}, {"text": "Moreover, the data is processed sequentially without any sentence independence assumption.", "labels": [], "entities": []}, {"text": "Thus, the recurrent mod-els will be able to capture long range dependencies that exist beyond the sentence boundary.", "labels": [], "entities": []}, {"text": "In order to compare the model sizes, we also report the Number of Parameters (NoP) to train for each of the models above.", "labels": [], "entities": [{"text": "Number of Parameters (NoP)", "start_pos": 56, "end_pos": 82, "type": "METRIC", "confidence": 0.9686343868573507}]}, {"text": "shows the perplexity evaluation on the PTB test set.", "labels": [], "entities": [{"text": "PTB test set", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9771045843760172}]}, {"text": "As a first observation, we can clearly see that the proposed approach outperforms all other models for all configurations, in particular, RNN and LSTM.", "labels": [], "entities": [{"text": "RNN", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.7936987280845642}]}, {"text": "This observation includes other models that were reported in the literature, such as random forest LM (, structured LM) and syntactic neural network LM ().", "labels": [], "entities": []}, {"text": "More particularly, we can conclude that LSRC, with an embedding size of 100, achieves a better performance than all other models while reducing the number of parameters by \u2248 29% and \u2248 17% compared to RNN and LSTM, respectively.", "labels": [], "entities": []}, {"text": "Increasing the embedding size to 200, which is used by the other models, improves significantly the performance with a resulting NoP comparable to LSTM.", "labels": [], "entities": [{"text": "NoP", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.7820813059806824}]}, {"text": "The significance of the improvements obtained here over LSTM were confirmed through a statistical significance t-test, which led to p-values \u2264 10 \u221210 fora significance level of 5% and 0.01%, respectively.", "labels": [], "entities": []}, {"text": "The results of the deep models in show that adding a single non-recurrent hidden layer to LSRC can significantly improve the performance.", "labels": [], "entities": []}, {"text": "In fact, the additional layer bridges the gap between the LSRC models with an embedding size of 100 and 200, respectively.", "labels": [], "entities": []}, {"text": "The resulting architectures outperform the other deep recurrent models with a significant reduction of the number of parameters (for the embedding size 100), and without usage of dropout regularization, L p and maxout units or gradient control techniques compared to the deep RNN 4 (D-RNN).", "labels": [], "entities": []}, {"text": "We can conclude from these experiments that the explicit modeling of short and long range dependencies using two separate hidden states improves the performance while significantly reducing the number of parameters.", "labels": [], "entities": []}, {"text": "In order to show the consistency of the LSRC improvement over the other recurrent models, we report the variation of the models performance with respect to the hidden layer size in.", "labels": [], "entities": [{"text": "consistency", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9860495924949646}]}, {"text": "This figure shows that increasing the LSTM or RNN hidden layer size could not achieve a similar performance to the one obtained using LSRC with a small layer size (e.g., 300).", "labels": [], "entities": []}, {"text": "It is also worth mentioning that this observation holds when comparing a 2-recurrent layers LSTM to LSRC with an additional non-recurrent layer.", "labels": [], "entities": []}, {"text": "The LTCB experiments use the same PTB setup with minor modifications.", "labels": [], "entities": [{"text": "LTCB", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.91289883852005}, {"text": "PTB", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.6852721571922302}]}, {"text": "The results shown in Table 3 follow the same experimental setup proposed in (.", "labels": [], "entities": []}, {"text": "More precisely, these results were obtained without use of momentum or weight decay (due to the long training time required for this corpus), the mini-batch size was set to 400, the learning rate was set to 0.4 and the BPTT step was fixed at 5.", "labels": [], "entities": [{"text": "BPTT step", "start_pos": 219, "end_pos": 228, "type": "METRIC", "confidence": 0.9316843748092651}]}, {"text": "The FFNN and FOFE architectures use 2 hidden layers of size 600, whereas RNN, LSTM and LSRC have a single hidden layer of size 600.", "labels": [], "entities": [{"text": "FFNN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8296951651573181}]}, {"text": "Moreover, the word embedding size was set to 200 for all models except RNN, which was set to 600.", "labels": [], "entities": [{"text": "RNN", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.9206880331039429}]}, {"text": "We also report results for an LSTM with 2 recurrent layers as well as for LSRC with an additional non-recurrent layer.", "labels": [], "entities": []}, {"text": "The recurrent layers are marked with an \"R\" in.", "labels": [], "entities": []}, {"text": "The results shown in generally confirm the conclusions we drew from the PTB experiments above.", "labels": [], "entities": [{"text": "PTB experiments", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9279001951217651}]}, {"text": "In particular, we can see that the proposed LSRC model largely outperforms all other models.", "labels": [], "entities": []}, {"text": "In particular, LSRC clearly outperforms LSTM with a negligible increase in the number of parameters (resulting from the additional 200 \u00d7 200 = 0.04M local connection weights U cl ) for the single layer results.", "labels": [], "entities": []}, {"text": "We can also see that this improvement is maintained for deep models (2 hidden layers), where the LSRC model achieves a slightly better performance while reducing the number of parameters by \u2248 2.5M and speeding up the training time by \u2248 20% compared to deep LSTM.", "labels": [], "entities": []}, {"text": "The PTB and LTCB results clearly highlight the importance of recurrent models to capture long range dependencies for LM tasks.", "labels": [], "entities": [{"text": "PTB", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.6343494653701782}]}, {"text": "The training of these models, however, requires large amounts of data to significantly outperform short context models.", "labels": [], "entities": []}, {"text": "This can be seen in the performance of RNN and LSTM in the PTB and LTCB tables above.", "labels": [], "entities": [{"text": "RNN", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.7440741062164307}, {"text": "PTB", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.947460949420929}, {"text": "LTCB", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.6033450365066528}]}, {"text": "We can also conclude from these results that the explicit modeling of long and short context in a multi-span model can lead to a significant improvement over state-of-the are models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: LMs performance on the PTB test set.", "labels": [], "entities": [{"text": "PTB test set", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9875777761141459}]}, {"text": " Table 3: LMs performance on the LTCB test set.", "labels": [], "entities": [{"text": "LTCB test set", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9791677395502726}]}]}