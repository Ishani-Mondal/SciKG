{"title": [{"text": "How Transferable are Neural Networks in NLP Applications?", "labels": [], "entities": []}], "abstractContent": [{"text": "Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9523254334926605}]}, {"text": "It is particularly important to neural networks, which are very likely to be overfitting.", "labels": [], "entities": []}, {"text": "In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning.", "labels": [], "entities": [{"text": "image processing", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.8495145440101624}, {"text": "neural network-based transfer learning", "start_pos": 83, "end_pos": 121, "type": "TASK", "confidence": 0.7885783314704895}]}, {"text": "For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent.", "labels": [], "entities": []}, {"text": "In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transfer learning, or sometimes known as domain adaptation, plays an important role in various natural language processing (NLP) applications, especially when we do not have large enough datasets for the task of interest (called the target task T ).", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9551897048950195}, {"text": "domain adaptation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.730560764670372}]}, {"text": "In such scenarios, we would like to transfer or adapt knowledge from other domains (called the source domains/tasks S) so as to mitigate the problem of overfitting and to improve model performance in T . For traditional feature-rich or kernel-based models, researchers have developed a variety of elegant methods for domain adaptation; examples include EasyAdapt; Daum\u00e9 III et * Yan Xu is currently a research scientist at Inveno Co., Ltd.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 317, "end_pos": 334, "type": "TASK", "confidence": 0.7759218811988831}]}, {"text": "1 Code released on https://sites.google.com/site/transfernlp/ 2 In this paper, we do not distinguish the conceptual difference between transfer learning and domain adaptation.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.8943836987018585}, {"text": "domain adaptation", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.7216633707284927}]}, {"text": "Domain-in the sense we use throughout this paper-is defined by datasets.", "labels": [], "entities": []}, {"text": "al., 2010), instance weighting (, and structural correspondence learning (.", "labels": [], "entities": [{"text": "instance weighting", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8563213646411896}]}, {"text": "Recently, deep neural networks are emerging as the prevailing technical solution to almost every field in NLP.", "labels": [], "entities": []}, {"text": "Although capable of learning highly nonlinear features, deep neural networks are very prone to overfitting, compared with traditional methods.", "labels": [], "entities": []}, {"text": "Transfer learning therefore becomes even more important.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9689094722270966}]}, {"text": "Fortunately, neural networks can be trained in a transferable way by their incremental learning nature: we can directly use trained (tuned) parameters from a source task to initialize the network in the target task; alternatively, we may also train two tasks simultaneously with some parameters shared.", "labels": [], "entities": []}, {"text": "But their performance should be verified by empirical experiments.", "labels": [], "entities": []}, {"text": "Existing studies have already shown some evidence of the transferability of neural features.", "labels": [], "entities": []}, {"text": "For example, in image processing, low-level neural layers closely resemble Gabor filters or color blobs; they can be transferred well to different tasks.", "labels": [], "entities": [{"text": "image processing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.8389104008674622}]}, {"text": "suggest that high-level layers are also transferable in general visual recognition; further investigate the transferability of neural layers in different levels of abstraction.", "labels": [], "entities": [{"text": "general visual recognition", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6184947987397512}]}, {"text": "Although transfer learning is promising in image processing, conclusions appear to be less clear in NLP applications.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.969115823507309}, {"text": "image processing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.8540248572826385}]}, {"text": "Image pixels are low-level signals, which are generally continuous and less related to semantics.", "labels": [], "entities": []}, {"text": "By contrast, natural language tokens are discrete: each word well reflects the thought of humans, but neighboring words do not share as much information as pixels in images do.", "labels": [], "entities": []}, {"text": "Previous neural NLP studies have casually applied transfer techniques, but their results are not consistent.", "labels": [], "entities": []}, {"text": "apply multi-task learning to SRL, NER, POS, and CHK, 3 but obtain only 0.04-0.21% error reduction 4 (out of abase error rate of 16-18%)., on the contrary, improve a natural language inference task from an accuracy of 71.3% to 80.8% by initializing parameters with an additional dataset of 550,000 samples.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.9514350295066833}, {"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9984460473060608}]}, {"text": "Therefore, more systematic studies are needed to shed light on transferring neural networks in the field of NLP.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our study, we conducted two series of experiments using six open datasets as follows.", "labels": [], "entities": []}, {"text": "\u2022 Experiment I: Sentence classification \u2212 IMDB.", "labels": [], "entities": [{"text": "Sentence classification", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.936475396156311}, {"text": "IMDB", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8132582306861877}]}, {"text": "A large dataset for binary sentiment classification (positive vs. negative).", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.8354914387067159}]}, {"text": "A small dataset for binary sentiment classification.", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.863202691078186}]}, {"text": "A (small) dataset for 6-way question classification (e.g., location, time, and number).", "labels": [], "entities": [{"text": "6-way question classification", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6578055918216705}]}, {"text": "\u2022 Experiment II: Sentence-pair classification \u2212 SNLI.", "labels": [], "entities": [{"text": "Sentence-pair classification", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.9251165986061096}, {"text": "SNLI", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.7925050258636475}]}, {"text": "A large dataset for sentence entailment recognition.", "labels": [], "entities": [{"text": "sentence entailment recognition", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.8393251498540243}]}, {"text": "The classification objectives are entailment, contradiction, and neutral.", "labels": [], "entities": []}, {"text": "A small dataset with exactly the same classification objective as SNLI.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.7875790596008301}]}, {"text": "A (small) dataset for paraphrase detection.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.9617959856987}]}, {"text": "The objective is binary classification: judging whether two sentences have the same meaning.", "labels": [], "entities": []}, {"text": "In each experiment, the large dataset serves as the source domain and small ones are the target domains.", "labels": [], "entities": []}, {"text": "presents statistics of the above datasets.", "labels": [], "entities": []}, {"text": "We distinguish two scenarios of transfer regarding semantic similarity: (1) semantically equivalent transfer (IMDB\u2192MR, SNLI\u2192SICK), that is, the tasks of Sand T are defined by the same meaning, and (2) semantically different transfer (IMDB\u2192QC, SNLI\u2192MSRP).", "labels": [], "entities": []}, {"text": "Examples are also illustrated in to demonstrate semantic relatedness.", "labels": [], "entities": []}, {"text": "It should be noticed that in image or speech processing (, the input of neural networks pretty much consists of raw signals; hence, low-level feature detectors are almost always transferable, even if manually distinguish artificial objects and natural ones in an image classification task.", "labels": [], "entities": [{"text": "image classification task", "start_pos": 263, "end_pos": 288, "type": "TASK", "confidence": 0.8020512461662292}]}, {"text": "Distinguishing semantic relatedness-which emerges from very low layers of either word embeddings or the successive hidden layer-is specific to NLP and also anew insight of our paper.", "labels": [], "entities": [{"text": "Distinguishing semantic relatedness-which", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7119550903638204}]}, {"text": "As we shall see in Sections 5 and 6, the transferability of neural networks in NLP is more sensitive to semantics than in image processing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics and examples of the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy (%) without transfer. We also include re-", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991727471351624}]}, {"text": " Table 3: Main results of neural transfer learning by INIT. We", "labels": [], "entities": [{"text": "neural transfer learning", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.7567091683546702}, {"text": "INIT", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9174689650535583}]}]}