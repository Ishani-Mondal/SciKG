{"title": [{"text": "Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity", "labels": [], "entities": [{"text": "Tagging", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9647702574729919}]}], "abstractContent": [{"text": "We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs.", "labels": [], "entities": []}, {"text": "Variable-order CRFs achieve faster inference by including features for only some of the tag n-grams.", "labels": [], "entities": []}, {"text": "Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer.", "labels": [], "entities": []}, {"text": "An active-set outer loop allows the feature set to grow as far as needed.", "labels": [], "entities": []}, {"text": "On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2-6x speedup over a baseline, with no significant drop inaccuracy.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.707865983247757}, {"text": "Universal Dependencies dataset", "start_pos": 66, "end_pos": 96, "type": "DATASET", "confidence": 0.900397519270579}]}], "introductionContent": [{"text": "Conditional Random Fields (CRFs) () area convenient formalism for sequence labeling tasks common in NLP.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.6661666532357534}]}, {"text": "A CRF defines a featurerich conditional distribution over tag sequences (output) given an observed word sequence (input).", "labels": [], "entities": []}, {"text": "The key advantage of the CRF framework is the flexibility to consider arbitrary features of the input, as well as enough features over the output structure to encourage it to be well-formed and consistent.", "labels": [], "entities": []}, {"text": "However, inference in CRFs is fast only if the features over the output structure are limited.", "labels": [], "entities": []}, {"text": "For example, an order-k CRF (or \"k-CRF\" for short, with k > 1 being \"higher-order\") allows expressive features over a window of k+1 adjacent tags (as well as the input), and then inference takes time O(n\u00b7|Y | k+1 ), where Y is the set of tags and n is the length of the input.", "labels": [], "entities": []}, {"text": "How large does k need to be?", "labels": [], "entities": []}, {"text": "Typically k = 2 works well, with big gains from 0 \u2192 1 and modest * Equal contribution Figure 1: Speed-accuracy tradeoff curves on test data for the 5 languages.", "labels": [], "entities": []}, {"text": "Large dark circles represent the kCRFs of ascending orders along x-axis (marked on for Slovenian).", "labels": [], "entities": []}, {"text": "Smaller triangles each represent a VoCRF discovered by sweeping the speed parameters \u03b3.", "labels": [], "entities": []}, {"text": "We find faster models at similar accuracy to the best k-CRFs ( \u00a75).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9982718229293823}]}, {"text": "gains from 1 \u2192 2 ().", "labels": [], "entities": []}, {"text": "Small k maybe sufficient when there is enough training data to allow the model to attend to many fine-grained features of the input ().", "labels": [], "entities": []}, {"text": "For example, when predicting POS tags in morphologicallyrich languages, certain words are easily tagged based on their spelling without considering the context (k = 0).", "labels": [], "entities": [{"text": "predicting POS tags", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7339165111382803}]}, {"text": "In fact, such languages tend to have a more free word order, making tag context less useful.", "labels": [], "entities": []}, {"text": "We investigate a hybrid approach that gives the accuracy of higher-order models while reducing runtime.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9994211196899414}]}, {"text": "We build on variable-order CRFs () (VoCRF), which support features on tag subsequences of mixed orders.", "labels": [], "entities": []}, {"text": "Since only modest gains are obtained from moving to higher-order models, we posit that only a small fraction of the higher-order features are necessary.", "labels": [], "entities": []}, {"text": "We introduce a hyperparameter \u03b3 that discourages the model from using many higher-order features (= faster inference) and a hyperparameter \u03bb that encourages generalization.", "labels": [], "entities": []}, {"text": "Thus, sweeping a range of values for \u03b3 and \u03bb gives rise to a number of operating points along the speed-accuracy curve (triangle points in).", "labels": [], "entities": []}, {"text": "We present three contributions: (1) A simplified exposition of VoCRFs, including an algorithm for computing gradients that is asymptotically more efficient than prior art (.", "labels": [], "entities": []}, {"text": "(2) We develop a structure learning algorithm for discovering the essential set of higher-order dependencies so that inference is fast and accurate.", "labels": [], "entities": []}, {"text": "(3) We investigate the effectiveness of our approach on POS tagging in five diverse languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9229783415794373}]}, {"text": "We find that the amount of required context for accurate prediction is highly language-dependent.", "labels": [], "entities": [{"text": "accurate prediction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.5660003423690796}]}, {"text": "In all languages, however, our approach meets the accuracy of fixed-order models at a fraction of the runtime.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992581009864807}]}], "datasetContent": [{"text": "Data: We conduct experiments on multilingual POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.8135637938976288}]}, {"text": "The task is to label each word in a sentence with one of |Y | = 17 labels.", "labels": [], "entities": []}, {"text": "We train on five typologically-diverse languages from the Universal Dependencies (UD) corpora (Petrov et al., 2012): Basque, Bulgarian, Hindi, Norwegian and Slovenian.", "labels": [], "entities": []}, {"text": "For each language, we start with the original train / dev / test split in the UD dataset, then move random sentences from train into dev until the dev set has 3000 sentences.", "labels": [], "entities": [{"text": "UD dataset", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9299264550209045}]}, {"text": "This ensures more stable hyperparameter tuning.", "labels": [], "entities": []}, {"text": "We use these new splits below.", "labels": [], "entities": []}, {"text": "Eval: We train models with (\u03bb, \u03b3) \u2208 {10 \u22124 \u00b7 m, 10 \u22123 \u00b7m, 10 \u22122 \u00b7m}\u00d7{0, 0.1\u00b7m, 0.2\u00b7m, . .", "labels": [], "entities": []}, {"text": ", m}, where m is the number of training sentences.", "labels": [], "entities": []}, {"text": "To tag a dev or test sentence, we choose its most probable tag sequence.", "labels": [], "entities": []}, {"text": "For each of several model sizes, Table 1 selects the model of that size that achieved the highest per-token tagging accuracy on the dev set, and reports that model's accuracy on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9223567843437195}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9983586668968201}]}, {"text": "Features: Recall from \u00a73 that our features include non-stationary zeroth-order features f (1) as well as the stationary features based on W.", "labels": [], "entities": []}, {"text": "For f (1) (x, t, y t ) we consider the following language-agnostic properties of (x, t): \u2022 The identities of the tokens x t\u22123 , ..., x t+3 , and the token bigrams (x t+1 , x t ), (x t , x t\u22121 ), VoCRF at different model sizes |W| (which is proportional to runtime) 0 (17) 1: Part-of-speech tagging with Universal Tags: This table shows test results on 5 languages at different target runtimes.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 275, "end_pos": 297, "type": "TASK", "confidence": 0.7230685353279114}]}, {"text": "Each row's best results are in boldface, where ties inaccuracy are broken in favor of faster models.", "labels": [], "entities": []}, {"text": "Superscript k indicates that the accuracy is significantly different from the k-CRF (paired permutation test, p < 0.05) and this superscript is in blue/red if the accuracy is higher/lower than the k-CRF.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9995658993721008}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9941399097442627}]}, {"text": "In all cases, we find a VoCRF (underlined) that is about as accurate as the 2-CRF (i.e., not significantly less accurate) and far faster, since the 2-CRF has |W| = 4913.", "labels": [], "entities": []}, {"text": "(x t\u22121 , x t+1 ).", "labels": [], "entities": []}, {"text": "We use special boundary symbols for tokens at positions beyond the start or end of the sentence.", "labels": [], "entities": []}, {"text": "\u2022 Prefixes and suffixes of x t , up to 4 characters long, that occur \u2265 5 times in the training data.", "labels": [], "entities": []}, {"text": "\u2022 Indicators for whether x t is all caps, is lowercase, or has a digit.", "labels": [], "entities": []}, {"text": "\u2022 Word shape of x t , which maps the token string into the following character classes (uppercase, lowercase, number) with punctuation unmodified (e.g., VoCRF-like \u21d2 AaAAA-aaaa, $5,432.10 \u21d2 $8,888.88).", "labels": [], "entities": []}, {"text": "For efficiency, we hash these properties into 2 22 bins.", "labels": [], "entities": []}, {"text": "The f (1) features are obtained by conjoining these bins with y t (): e.g., there is a feature that returns 0 unless y t = NOUN, in which case it counts the number of bin 1234567's properties that (x, t) has.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9909727573394775}]}, {"text": "(The f (2) features are not hashed.)", "labels": [], "entities": []}, {"text": "Results: Our results are presented in and.", "labels": [], "entities": []}, {"text": "We highlight two key points: (i) Across all languages we learned a tagger about as accurate as a 2-CRF, but much faster.", "labels": [], "entities": []}, {"text": "(ii) The size of the set W required is highly language-dependent.", "labels": [], "entities": []}, {"text": "For many languages, learning a full k-CRF is wasteful; our method resolves this problem.", "labels": [], "entities": []}, {"text": "In each language, the fastest \"good\" VoCRF is rather faster than the fastest \"good\" k-CRF (where \"good\" means statistically indistinguishable from the 2-CRF).", "labels": [], "entities": []}, {"text": "These two systems are underlined; the underlined VoCRF systems are smaller than the underlined k-CRF systems (for the 5 languages respectively) by factors of 1.9, 6.4, 3.4, 1.9, and 2.9.", "labels": [], "entities": []}, {"text": "In every language, we learn a VoCRF with |W| \u2264 850 that is not significantly worse than a 2-CRF with |W| = 17 3 = 4913.", "labels": [], "entities": []}, {"text": "We also notice an interesting language-dependent effect, whereby certain languages require a small number of tag strings in order to perform well.", "labels": [], "entities": []}, {"text": "For example, Hindi has a competitive model that ignores the previous tag y t\u22121 unless it is in {NOUN, VERB, ADP, PROPN}: thus the stationary features are 17 unigrams plus 4 \u00d7 17 bigrams, fora total of |W| = 85.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9024552702903748}, {"text": "VERB", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9143434166908264}]}, {"text": "At the other extreme, the Slavic languages Slovenian and Bulgarian seem to require more expressive models over the tag space, remembering as many as 98 useful left-context histories (unigrams and bigrams) for the current tag.", "labels": [], "entities": []}, {"text": "An interesting direction for future research would be to determine which morpho-syntactic properties of a language tend to increase the complexity of tagging.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Part-of-speech tagging with Universal Tags: This table shows test results on 5 languages at different target  runtimes. Each row's best results are in boldface, where ties in accuracy are broken in favor of faster models. Superscript  k indicates that the accuracy is significantly different from the k-CRF (paired permutation test, p < 0.05) and this  superscript is in blue/red if the accuracy is higher/lower than the k-CRF. In all cases, we find a VoCRF (underlined) that  is about as accurate as the 2-CRF (i.e., not significantly less accurate) and far faster, since the 2-CRF has |W| = 4913.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7535174489021301}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9980936646461487}, {"text": "accuracy", "start_pos": 266, "end_pos": 274, "type": "METRIC", "confidence": 0.997804582118988}, {"text": "accuracy", "start_pos": 397, "end_pos": 405, "type": "METRIC", "confidence": 0.9838870167732239}]}]}