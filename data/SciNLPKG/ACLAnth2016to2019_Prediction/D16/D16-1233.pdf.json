{"title": [{"text": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "labels": [], "entities": [{"text": "Conditional Generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8702040314674377}]}], "abstractContent": [{"text": "Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks.", "labels": [], "entities": [{"text": "language generation tasks", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.7935488422711691}]}, {"text": "In this work we study various model ar-chitectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework.", "labels": [], "entities": []}, {"text": "A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector.", "labels": [], "entities": [{"text": "snapshot learning", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8493889272212982}]}, {"text": "The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two.", "labels": [], "entities": []}, {"text": "Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance.", "labels": [], "entities": []}, {"text": "Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent Neural Network (RNN)-based conditional language models (LM) have been shown to be very effective in tackling a number of real world problems, such as machine translation (MT) () and image caption generation.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.8403694570064545}, {"text": "image caption generation", "start_pos": 192, "end_pos": 216, "type": "TASK", "confidence": 0.8708274364471436}]}, {"text": "Recently, RNNs were applied to task of generating sentences from an explicit semantic representation).", "labels": [], "entities": []}, {"text": "Attention-based methods ( and Long Short-term Memory (LSTM)-like) gating mechanisms () have both been studied to improve generation quality.", "labels": [], "entities": []}, {"text": "Although it is now clear that LSTMbased conditional LMs can generate plausible natural language, less effort has been put in comparing the different model architectures.", "labels": [], "entities": [{"text": "LSTMbased conditional LMs", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7003303567568461}]}, {"text": "Furthermore, conditional generation models are typically tested on relatively straightforward tasks conditioned on a single source (e.g. a sentence or an image) and where the goal is to optimise a single metric (e.g. BLEU).", "labels": [], "entities": [{"text": "conditional generation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7232212424278259}, {"text": "BLEU", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9945576190948486}]}, {"text": "In this work, we study the use of conditional LSTMs in the generation component of neural network (NN)-based dialogue systems which depend on multiple conditioning sources and optimising multiple metrics.", "labels": [], "entities": []}, {"text": "Neural conversational agents ( are direct extensions of the sequence-to-sequence model) in which a conversation is cast as a source to target transduction problem.", "labels": [], "entities": []}, {"text": "However, these models are still far from real world applications because they lack any capability for supporting domain specific tasks, for example, being able to interact with databases ( and aggregate useful information into their responses.", "labels": [], "entities": []}, {"text": "Recent work by, however, proposed an end-to-end trainable neural dialogue system that can assist users to complete specific tasks.", "labels": [], "entities": []}, {"text": "Their system used both distributed and symbolic representations to capture user intents, and these collectively condition a NN language generator to generate system responses.", "labels": [], "entities": []}, {"text": "Due to the diversity of the conditioning information sources, the best way to represent and combine them is non-trivial.", "labels": [], "entities": []}, {"text": "In, the objective function for learning the dialogue policy and language generator depends solely on the likelihood of the output sentences.", "labels": [], "entities": []}, {"text": "However, this sequential supervision signal may not be informative enough to learn a good conditioning vector representation resulting in a generation process which is dominated by the LM.", "labels": [], "entities": []}, {"text": "This can often lead to inappropriate system outputs.", "labels": [], "entities": []}, {"text": "In this paper, we therefore also investigate the use of snapshot learning which attempts to mitigate this problem by heuristically applying companion supervision signals to a subset of the conditioning vector.", "labels": [], "entities": []}, {"text": "This idea is similar to deeply supervised nets () in which the final cost from the output layer is optimised together with the companion signals generated from each intermediary layer.", "labels": [], "entities": []}, {"text": "We have found that snapshot learning offers several benefits: (1) it consistently improves performance; (2) it learns discriminative and robust feature representations and alleviates the vanishing gradient problem; (3) it appears to learn transparent and interpretable subspaces of the conditioning vector.", "labels": [], "entities": [{"text": "snapshot learning", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8334101438522339}]}], "datasetContent": [{"text": "Dataset The dataset used in this work was collected in the Wizard-of-Oz online data collection described by, in which the task of the system is to assist users to find a restaurant in Cambridge, UK area.", "labels": [], "entities": [{"text": "Wizard-of-Oz online data collection", "start_pos": 59, "end_pos": 94, "type": "DATASET", "confidence": 0.7396150231361389}, {"text": "Cambridge, UK area", "start_pos": 184, "end_pos": 202, "type": "DATASET", "confidence": 0.8287671804428101}]}, {"text": "There are three informable slots (food, pricerange, area) that users can use to constrain the search and six requestable slots (address, phone, postcode plus the three informable   slots) that the user can ask a value for once a restaurant has been offered.", "labels": [], "entities": []}, {"text": "There are 676 dialogues in the dataset (including both finished and unfinished dialogues) and approximately 2750 turns in total.", "labels": [], "entities": []}, {"text": "The database contains 99 unique restaurants.", "labels": [], "entities": []}, {"text": "Training The training procedure was divided into two stages.", "labels": [], "entities": []}, {"text": "Firstly, the belief tracker parameters \u03b8 b were pre-trained using cross entropy errors between tracker labels and predictions.", "labels": [], "entities": []}, {"text": "Having fixed the tracker parameters, the remaining parts of the model \u03b8 \\b are trained using the cross entropy errors from the generation network LM, where y t j and pt j are output token targets and predictions respectively, at turn t of output step j, L ss (\u00b7) is the snapshot cost from Equation 2, and \u03bb is the tradeoff parameter in which we set to 1 for all models trained with snapshot learning.", "labels": [], "entities": []}, {"text": "We treated each dialogue as a batch and used stochastic gradient descent with a small l2 regularisation term to train the model.", "labels": [], "entities": []}, {"text": "The collected corpus was partitioned into a training, validation, and testing sets in the ratio 3:1:1.", "labels": [], "entities": []}, {"text": "Early stopping was implemented based on the validation set considering only LM log-likelihoods.", "labels": [], "entities": []}, {"text": "Gradient clipping was set to 1.", "labels": [], "entities": [{"text": "Gradient clipping", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.9095055162906647}]}, {"text": "The hidden layer sizes were set to 50, and the weights were randomly initialised between -0.3 and 0.3 including word embeddings.", "labels": [], "entities": []}, {"text": "The vocabulary size is around 500 for both input and output, in which rare words and words that can be delexicalised have been removed.", "labels": [], "entities": []}, {"text": "Decoding In order to compare models trained with different recipes rather than decoding strategies, we decode all the trained models with the average log probability of tokens in the sentence.", "labels": [], "entities": []}, {"text": "We applied beam search with a beamwidth equal to 10, the search stops when an end-of-sentence token is generated.", "labels": [], "entities": []}, {"text": "In order to consider language variability, we ran decoding until 5 candidates were obtained and performed evaluation on them.", "labels": [], "entities": [{"text": "language variability", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7251887917518616}]}, {"text": "Metrics We compared models trained with different recipes by performing a corpus-based evaluation in which the model is used to predict each system response in the held-out test set.", "labels": [], "entities": []}, {"text": "Three evaluation metrics were used: BLEU score (on top-1 and top-5 candidates) (), slot matching rate and objective task success rate ( . The dialogue is marked as successful if both: (1) the offered entity matches the task that was specified to the user, and (2) the system answered all the associated information requests (e.g. what is the address?) from the user.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9792567789554596}, {"text": "slot matching rate", "start_pos": 83, "end_pos": 101, "type": "METRIC", "confidence": 0.908311108748118}]}, {"text": "The slot matching rate is the percentage of delexicalised tokens (e.g. reference.", "labels": [], "entities": [{"text": "slot matching", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7021949589252472}]}, {"text": "We computed the BLEU scores on the skeletal sentence forms before substituting with the actual entity values.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9993879795074463}]}, {"text": "All the results were averaged over 10 random initialised networks.", "labels": [], "entities": []}, {"text": "Results shows the evaluation results.", "labels": [], "entities": []}, {"text": "The numbers to the left and right of each table cell are the same model trained w/o and w/ snapshot learning.", "labels": [], "entities": []}, {"text": "The first observation is that snapshot learning consistently improves on most metrics regardless of the model architecture.", "labels": [], "entities": [{"text": "snapshot learning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8550698757171631}]}, {"text": "This is especially true for BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.988972008228302}]}, {"text": "We think this maybe attributed to the more discriminative conditioning vector learned through the snapshot method, which makes the learning of the conditional LM easier.", "labels": [], "entities": []}, {"text": "In the first block belief state representation, we compare the effect of two different belief representations.", "labels": [], "entities": []}, {"text": "As can be seen, using a succinct representation is better (summary>full) because the identity of each categorical value in the belief state does not help when the generation decisions are done in skeletal form.", "labels": [], "entities": []}, {"text": "In fact, the full belief state representation may encourage the model to learn incorrect coadaptation among features when the data is scarce.", "labels": [], "entities": []}, {"text": "In the conditional architecture block, we compare the three different conditional generation architectures as described in section 3.2.1.", "labels": [], "entities": []}, {"text": "This result shows that the language model type (lm) and memory type (mem) networks perform better in terms of BLEU score and slot matching rate, while the hybrid type (hybrid) networks achieve higher task success.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.98047736287117}]}, {"text": "This is probably due to the degree of separation be-  tween the LM and conditioning vector: a coupling approach (lm, mem) sacrifices the conditioning vector but learns a better LM and higher BLEU; while a complete separation (hybrid) learns a better conditioning vector and offers a higher task success.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9985135197639465}]}, {"text": "Lastly, in the attention-based model block we train the three architectures with the attention mechanism and compare them again.", "labels": [], "entities": []}, {"text": "Firstly, the characteristics of the three models we observed above also hold for attention-based models.", "labels": [], "entities": []}, {"text": "Secondly, we found that the attention mechanism improves all the three architectures on task success rate but not BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9991626739501953}]}, {"text": "This is probably due to the limitations of using n-gram based metrics like BLEU to evaluate the generation quality).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.994782030582428}]}], "tableCaptions": [{"text": " Table 2: Average activation of gates on test set.", "labels": [], "entities": []}]}