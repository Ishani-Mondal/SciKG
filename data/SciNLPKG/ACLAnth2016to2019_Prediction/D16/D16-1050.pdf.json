{"title": [], "abstractContent": [{"text": "Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7749750912189484}]}, {"text": "In this paper, we propose a variational model to learn this conditional distribution for neu-ral machine translation: a variational encoder-decoder model that can be trained end-to-end.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.589062511920929}]}, {"text": "Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations.", "labels": [], "entities": []}, {"text": "In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound.", "labels": [], "entities": [{"text": "posterior inference", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7068205922842026}]}, {"text": "Experiments on both Chinese-English and English-German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.", "labels": [], "entities": [{"text": "English-German translation tasks", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.7017961939175924}, {"text": "variational neural machine translation", "start_pos": 96, "end_pos": 134, "type": "TASK", "confidence": 0.8071715086698532}, {"text": "vanilla neural machine translation", "start_pos": 178, "end_pos": 212, "type": "TASK", "confidence": 0.6180808991193771}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in along training pipeline.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8138771851857504}]}, {"text": "It requires a much smaller memory than * Corresponding author phrase-or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table.", "labels": [], "entities": [{"text": "* Corresponding author phrase-or syntax-based statistical machine translation (SMT)", "start_pos": 39, "end_pos": 122, "type": "TASK", "confidence": 0.6507749557495117}]}, {"text": "Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9894503951072693}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7433987557888031}]}, {"text": "Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations 1 ().", "labels": [], "entities": []}, {"text": "Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism ( ) to identify semantic alignments between source and target words.", "labels": [], "entities": []}, {"text": "Due to potential errors in these alignments, the attention-based context vector maybe insufficient to capture the entire meaning of a source sentence, hence resulting in undesirable translation phenomena (.", "labels": [], "entities": []}, {"text": "Unlike the vanilla encoder-decoder framework, we model underlying semantics of bilingual sentence pairs explicitly.", "labels": [], "entities": []}, {"text": "We assume that there exists a continuous latent variable z from this underlying semantic space.", "labels": [], "entities": []}, {"text": "And this variable, together with x, guides the translation process, i.e. p. With this assumption, the original conditional probability evolves into the following formulation: This brings in the benefits that the latent variable z can serve as a global semantic signal that is complementary to the attention-based context vector for generating good translations when the model learns undesirable attentions.", "labels": [], "entities": [{"text": "translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9617308378219604}]}, {"text": "However, although this latent variable enables us to explicitly model underlying semantics of translation pairs, the incorporation of it into the above probabilistic model has two challenges: 1) the posterior inference in this model is intractable; 2) large-scale training, which lays the ground for the data-driven NMT, is accordingly problematic.", "labels": [], "entities": []}, {"text": "In order to address these issues, we propose a variational encoder-decoder model to neural machine translation (VNMT), motivated by the recent success of variational neural models ( ).", "labels": [], "entities": [{"text": "neural machine translation (VNMT)", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.7913554410139719}]}, {"text": "illustrates the graphic representation of VNMT.", "labels": [], "entities": [{"text": "VNMT", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9050049781799316}]}, {"text": "As deep neural networks are capable of learning highly nonlinear functions, we employ them to fit the latentvariable-related distributions, i.e. the prior and posterior, to make the inference tractable.", "labels": [], "entities": []}, {"text": "The former is modeled to be conditioned on the source side alone p \u03b8 (z|x), because the source and target part of a sentence pair usually share the same semantics so that the source sentence should contain the prior information for inducing the underlying semantics.", "labels": [], "entities": []}, {"text": "The latter, instead, is approximated from all observed variables q \u03c6 (z|x, y), i.e. both the source and the target sides.", "labels": [], "entities": []}, {"text": "In order to efficiently train parameters, we apply a reparameterization technique ) on the variational lower bound.", "labels": [], "entities": []}, {"text": "This enables us to use standard stochastic gradient optimization for training the proposed model.", "labels": [], "entities": []}, {"text": "Specifically, there are three essential components in VNMT (The detailed architecture is illustrated in): \u2022 A variational neural encoder transforms source/target sentence into distributed representations, which is the same as the encoder of NMT ( ) (see section 3.1).", "labels": [], "entities": [{"text": "VNMT", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8076022267341614}]}, {"text": "\u2022 A variational neural inferer infers the representation of z according to the learned source representations (i.e. p \u03b8 (z|x)) together with the target ones (i.e. q \u03c6 (z|x, y)), where the reparameterization technique is employed (see section 3.2).", "labels": [], "entities": []}, {"text": "\u2022 And a variational neural decoder integrates the latent representation of z to guide the generation of target sentence (i.e. p(y|z, x)) together with the attention mechanism (see section 3.3).", "labels": [], "entities": []}, {"text": "Augmented with the posterior approximation and reparameterization, our VNMT can still be trained end-to-end.", "labels": [], "entities": [{"text": "VNMT", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.7441277503967285}]}, {"text": "This makes our model not only efficient in translation, but also simple in implementation.", "labels": [], "entities": [{"text": "translation", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9797946810722351}]}, {"text": "To train our model, we employ the conventional maximum likelihood estimation.", "labels": [], "entities": []}, {"text": "Experiments on both Chinese-English and English-German translation tasks show that VNMT achieves significant improvements over several strong baselines.", "labels": [], "entities": [{"text": "English-German translation tasks", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.7071955998738607}, {"text": "VNMT", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.6232742071151733}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores on the NIST Chinese-English translation task. AVG = average BLEU scores on test sets. We  highlight the best results in bold for each test set. \"\u2191/\u21d1\": significantly better than Moses (p < 0.05/p < 0.01); \"+/++\":  significantly better than GroundHog (p < 0.05/p < 0.01);", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988242983818054}, {"text": "NIST Chinese-English translation task", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.7460645735263824}, {"text": "AVG", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9980491399765015}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.998875081539154}, {"text": "GroundHog", "start_pos": 261, "end_pos": 270, "type": "DATASET", "confidence": 0.9512801170349121}]}, {"text": " Table 2: BLEU scores on the new dataset. All improvements are significant at p < 0.01.  System  Architecture  BLEU  Existing end-to-end NMT systems  Jean et al. (2015)  RNNSearch  16.46  Jean et al. (2015)  RNNSearch + unk replace  18.97  Jean et al. (2015)  RNNsearch + unk replace + large vocab  19.40  Luong et al. (2015a) LSTM with 4 layers + dropout + local att. + unk replace 20.90  Our end-to-end NMT systems", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998794674873352}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9931058287620544}]}]}