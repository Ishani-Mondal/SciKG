{"title": [{"text": "Learning to Translate for Multilingual Question Answering", "labels": [], "entities": [{"text": "Learning to Translate for Multilingual Question Answering", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.5580923386982509}]}], "abstractContent": [{"text": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa.", "labels": [], "entities": [{"text": "multilingual question answering", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6344171663125356}]}, {"text": "In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, context-based, and grammar-based.", "labels": [], "entities": [{"text": "translation", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9617327451705933}]}, {"text": "We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights.", "labels": [], "entities": []}, {"text": "On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong base-line (p < 0.05): translating all text into En-glish, then training a classifier based only on English (original or translated) text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) is a specific form of the information retrieval (IR) task, where the goal is to find relevant well-formed answers to a posed question.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9275752782821656}, {"text": "information retrieval (IR) task", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8641765614350637}]}, {"text": "Most QA pipelines consist of three main stages: (a) preprocessing the question and collection, (b) retrieval of candidate answers in the collection, and (c) ranking answers with respect to their relevance to the question and return the top N answers.", "labels": [], "entities": []}, {"text": "The types of questions can range from factoid (e.g., \"What is the capital of France?\") to causal (e.g., \"Why are trees green?\"), and opinion questions (e.g., \"Should USA lower the drinking age?\").", "labels": [], "entities": []}, {"text": "The most common approach to multilingual QA (MLQA) has been to translate all content into its * This work was completed while author was an employee of Raytheon BBN Technologies.", "labels": [], "entities": [{"text": "multilingual QA (MLQA)", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7236105680465699}, {"text": "Raytheon BBN Technologies", "start_pos": 152, "end_pos": 177, "type": "DATASET", "confidence": 0.7755491733551025}]}, {"text": "most probable English translation via machine translation (MT) systems.", "labels": [], "entities": [{"text": "English translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.6137863397598267}, {"text": "machine translation (MT)", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8390941023826599}]}, {"text": "This strong baseline, which we refer to as one-best MT (1MT), has been successful in prior work (.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.7933260202407837}]}, {"text": "However, recent advances in cross-lingual IR (CLIR) show that one can do better by representing the translation space as a probability distribution).", "labels": [], "entities": [{"text": "cross-lingual IR", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.6627297699451447}]}, {"text": "In addition, MT systems perform substantially worse with user-generated text, such as web forums (Van der, which provide extra motivation to consider alternative translation approaches for higher recall.", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9822850227355957}, {"text": "recall", "start_pos": 196, "end_pos": 202, "type": "METRIC", "confidence": 0.9931588768959045}]}, {"text": "To our knowledge, it has yet to be shown whether these recent advancements in CLIR transfer to MLQA.", "labels": [], "entities": [{"text": "MLQA", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.8151558041572571}]}, {"text": "We introduce a novel answer ranking approach for MLQA (i.e., Learning to Translate or L2T), a model that learns the optimal translation of question and/or candidate answer, based on how well it discriminates between good and bad answers.", "labels": [], "entities": []}, {"text": "We achieve this by introducing a set of features that encapsulate lexical and semantic similarities between a question and a candidate answer through various translation strategies (Section 3.1).", "labels": [], "entities": []}, {"text": "The model then learns feature weights for each combination of translation direction and method, through a discriminative training process.", "labels": [], "entities": []}, {"text": "Once a model is trained, it can be used for MLQA, by sorting each candidate answer in the collection by model score.", "labels": [], "entities": [{"text": "MLQA", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.9059743881225586}]}, {"text": "Instead of learning a single model to score candidate answers in any language, it might be meaningful to train a separate model that can learn to discriminate between good and bad answers in each language.", "labels": [], "entities": []}, {"text": "This can let each model learn feature weights custom to the language, therefore allowing a more fine-grained ranking (Section 3.4).", "labels": [], "entities": []}, {"text": "We call this alternative approach Learning to Custom Translate (L2CT).", "labels": [], "entities": [{"text": "Learning to Custom Translate (L2CT)", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.7211720900876182}]}, {"text": "Experiments on the DARPA Broad Operational Language Technologies (BOLT) IR task 1 confirm that L2T yields statistically significant improvements over a strong baseline (p < 0.05), in three out of four experiments.", "labels": [], "entities": [{"text": "DARPA Broad Operational Language Technologies (BOLT) IR task 1", "start_pos": 19, "end_pos": 81, "type": "DATASET", "confidence": 0.5749982866373929}]}, {"text": "L2CT outperformed the baseline as well, but was not more effective than L2T.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to perform controlled experiments and gain more insights, we split our evaluation into four separate tasks: three tasks focus on retrieving answers from posts written in a specified language (English-only, Arabic-only, or Chinese-only) 9 , and the last task is not restricted to any language (Mixedlanguage).", "labels": [], "entities": []}, {"text": "All experiments were conducted on the DARPA BOLT-IR task.", "labels": [], "entities": [{"text": "DARPA", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.592229425907135}, {"text": "BOLT-IR", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.7903516292572021}]}, {"text": "The collection consists of 12.6M Arabic, 7.5M Chinese, and 9.6M English Web forum posts.", "labels": [], "entities": []}, {"text": "All runs use a set of 45 nonfactoid (mostly opinion and causal) English questions, from a range of topics.", "labels": [], "entities": []}, {"text": "All questions and forum posts were processed with an information extraction (IE) toolkit), which performs sentence-splitting, named entity recognition, coreference resolution, parsing, and part-ofspeech tagging.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.6383537252744039}, {"text": "coreference resolution", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.9194918870925903}, {"text": "part-ofspeech tagging", "start_pos": 189, "end_pos": 210, "type": "TASK", "confidence": 0.7029292583465576}]}, {"text": "All non-English posts were translated into English (one-best only), and all questions were translated into Arabic and Chinese (probabilistic translation methods from Section 3.1).", "labels": [], "entities": []}, {"text": "For all experiments, we used the same state-of-the-art English\u2194Arabic (En-Ar) and English\u2194Chinese (En-Ch) MT systems ().", "labels": [], "entities": []}, {"text": "Models were trained on parallel corpora from NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10M En-Ar words; 30M EnCh words).", "labels": [], "entities": [{"text": "NIST OpenMT 2012", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9009343584378561}, {"text": "BOLT", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.6193516850471497}]}, {"text": "Word alignments were learned with GIZA++ (Och and Ney, 2003) (five iterations of IBM Models 1-4 and HMM).", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.674948200583458}]}, {"text": "After all preprocessing, features were computed using the original post and question text, and their translations.", "labels": [], "entities": []}, {"text": "Training data were created by having annotators label all sentences of the top 200 documents retrieved by Indri from each collection (for each question).", "labels": [], "entities": []}, {"text": "Due to the nature of retrieval tasks, training data usually contains an unbalanced portion of negative examples.", "labels": [], "entities": []}, {"text": "Hence, we split the data into balanced subsets (each sharing the same set of positively labeled data) and train multiple classifiers, Shortened as Eng, Arz, and Cmn, respectively.", "labels": [], "entities": [{"text": "Arz", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.8505772352218628}]}, {"text": "then take a majority vote when predicting.", "labels": [], "entities": [{"text": "predicting", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9749184846878052}]}, {"text": "For testing, we froze the set of candidate answers and applied the trained classifier to each questionanswer pair, generating a ranked list of answers for each question.", "labels": [], "entities": []}, {"text": "This ranked list was evaluated by average precision (AP).", "labels": [], "entities": [{"text": "average precision", "start_pos": 34, "end_pos": 51, "type": "METRIC", "confidence": 0.7736939191818237}, {"text": "AP)", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9404543936252594}]}, {"text": "Due to the size and redundancy of the collections, we sometimes end up with over 1000 known relevant answers fora question.", "labels": [], "entities": []}, {"text": "So it is neither reasonable nor meaningful to compute AP until we reach 100% recall (e.g., 11-point AP) for these cases.", "labels": [], "entities": [{"text": "AP", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9960640072822571}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9849861264228821}, {"text": "11-point AP)", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.6990634600321451}]}, {"text": "Instead, we computed AP-k, by accumulating precision values at every relevant answer until we get k relevant answers.", "labels": [], "entities": [{"text": "AP-k", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9716947674751282}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9949548840522766}]}, {"text": "In order to provide a single metric for the test set, it is common to report the mean average precision (MAP), which in this case is the average of the AP-k values across all questions.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 81, "end_pos": 109, "type": "METRIC", "confidence": 0.9542832374572754}]}, {"text": "As described earlier, the baseline system computes similarity between question text and the one-best translation of the candidate answer (we run the sentence through our state-of-the-art MT system).", "labels": [], "entities": [{"text": "similarity", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9661810398101807}]}, {"text": "After translation, we compute similarity via scoring the match between the parse of the question text and the parse of the candidate answer, using our finely-tuned IE toolkit [reference removed for anonymization].", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9666341543197632}, {"text": "similarity", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.949370265007019}]}, {"text": "This results in three different similarity features: matching the tree node similarity, edge similarity, and full tree similarity.", "labels": [], "entities": [{"text": "edge similarity", "start_pos": 88, "end_pos": 103, "type": "METRIC", "confidence": 0.884002298116684}]}, {"text": "Feature weights are then learned by training this classifier discriminatively on the training data described above.", "labels": [], "entities": []}, {"text": "This already performs competitively, outperforming the simpler baseline where we compute a single similarity score between question and translated text, and matching the performance of the system by Chaturvedi et al. on the BOLT evaluation.", "labels": [], "entities": [{"text": "BOLT", "start_pos": 224, "end_pos": 228, "type": "METRIC", "confidence": 0.6489906907081604}]}, {"text": "Baseline MAP values are reported on the leftmost column of.", "labels": [], "entities": [{"text": "MAP", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.5170215368270874}]}, {"text": "In the baseline approach, we do not perform any data selection, and use all available data for training the classifier.", "labels": [], "entities": []}, {"text": "In order to test our hypothesis that selecting a linguistically-motivated subset of the training data might help, we used 10-fold cross-validation to choose the optimal data set (among seven options described in Section 3.3).", "labels": [], "entities": []}, {"text": "Results indicate that including English or Arabic sentences when training a classifier for Chinese-only QA is a bad idea, since effectiveness increases when restricted to Chinese sentences (lang=ch).", "labels": [], "entities": []}, {"text": "On the other hand, for the remaining three tasks, the most effective training data set is annot=en+consist.", "labels": [], "entities": []}, {"text": "These selections are consistent across all ten folds, and the difference is statistically significant for all but Arabic-only.", "labels": [], "entities": []}, {"text": "The second column in displays the MAP achieved when data selection is applied before training the baseline model.", "labels": [], "entities": [{"text": "MAP", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.982326865196228}]}, {"text": "To measure the impact of our novel features, we trained classifiers using either LexCL, LexQL, or both feature sets (Section 3.2).", "labels": [], "entities": [{"text": "LexCL", "start_pos": 81, "end_pos": 86, "type": "DATASET", "confidence": 0.9602985382080078}, {"text": "LexQL", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.9102449417114258}]}, {"text": "In these experiments, the data is fixed to the optimal subset found earlier.", "labels": [], "entities": []}, {"text": "Results are summarized on right side of.", "labels": [], "entities": []}, {"text": "Statistically significants improvements over Baseline/Baseline+Data selection are indicated with single/double underlining.", "labels": [], "entities": []}, {"text": "For Arabic-only QA, adding LexQL features yields greatest improvements over the baseline, while the same statement holds for LexCL features for the Chinese-only task.", "labels": [], "entities": []}, {"text": "For the English-only and mixed-language tasks, the most significant increase in MAP is observed with all of our probabilistic bilingual features.", "labels": [], "entities": [{"text": "MAP", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.8046254515647888}]}, {"text": "For all but Arabic-only QA, the MAP is statistically significantly better (p < 0.05) than the baseline; for Chinese-only and mixedlanguage tasks, it also outperforms baseline plus data selection (p < 0.05).", "labels": [], "entities": [{"text": "MAP", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9266383051872253}]}, {"text": "12 All of this indicates the effectiveness of our probabilistic question translation, as well as our data selection strategy.", "labels": [], "entities": [{"text": "probabilistic question translation", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6499390204747518}]}, {"text": "Understanding the contribution of each of the four Note that bilingual features are not expected to help on the English-only task, and the improvements come solely from data selection.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: L2T evaluated using MAP with 10-fold cross-", "labels": [], "entities": []}, {"text": " Table 3: L2T vs. L2CT for multilingual QA.", "labels": [], "entities": []}]}