{"title": [{"text": "A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification", "labels": [], "entities": [{"text": "Relation Classification", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.898809403181076}]}], "abstractContent": [{"text": "With the renaissance of neural network in recent years, relation classification has again become a research hotspot in natural language processing, and leveraging parse trees is a common and effective method of tackling this problem.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.9594093859195709}, {"text": "natural language processing", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.6662068665027618}]}, {"text": "In this work, we offer anew perspective on utilizing syntactic information of dependency parse tree and present a position encoding convolutional neural network (PECNN) based on dependency parse tree for relation classification.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7178815603256226}, {"text": "relation classification", "start_pos": 204, "end_pos": 227, "type": "TASK", "confidence": 0.8275966346263885}]}, {"text": "First, tree-based position features are proposed to encode the relative positions of words in dependency trees and help enhance the word representations.", "labels": [], "entities": []}, {"text": "Then, based on a redefinition of \"context\", we design two kinds of tree-based convolution kernels for capturing the semantic and structural information provided by dependency trees.", "labels": [], "entities": []}, {"text": "Finally, the features extracted by convolution module are fed to a classifier for labelling the semantic relations.", "labels": [], "entities": []}, {"text": "Experiments on the benchmark dataset show that PECNN outperforms state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "We also compare the effect of different position features and visualize the influence of tree-based position feature by tracing back the con-volution process.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation classification focuses on classifying the semantic relations between pairs of marked entities in given sentences (.", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9461118578910828}, {"text": "classifying the semantic relations between pairs of marked entities", "start_pos": 35, "end_pos": 102, "type": "TASK", "confidence": 0.6622065405050913}]}, {"text": "It is a fundamental task which can serve as a pre-existing system and provide prior knowledge for information ex- * Corresponding authors traction, natural language understanding, information retrieval, etc.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 148, "end_pos": 178, "type": "TASK", "confidence": 0.6402930716673533}, {"text": "information retrieval", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.8128181099891663}]}, {"text": "However, automatic recognition of semantic relation is challenging.", "labels": [], "entities": [{"text": "automatic recognition of semantic relation", "start_pos": 9, "end_pos": 51, "type": "TASK", "confidence": 0.7202134788036346}]}, {"text": "Traditional feature based approaches rely heavily on the quantity and quality of hand-crafted features and lexical resources, and it is time-consuming to select an optimal subset of relevant features in order to maximize performance.", "labels": [], "entities": []}, {"text": "Though kernel based methods get rid of the feature selection process, they need elaborately designed kernels and are also computationally expensive.", "labels": [], "entities": []}, {"text": "Recently, with the renaissance of neural network, deep learning techniques have been adopted to provide end-to-end solutions for many classic NLP tasks, such as sentence modeling) and machine translation ().", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.780326247215271}, {"text": "machine translation", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.7916605472564697}]}, {"text": "Recursive Neural Network (RNN)) and Convolutional Neural Network (CNN) () have proven powerful in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 98, "end_pos": 121, "type": "TASK", "confidence": 0.9110726416110992}]}, {"text": "In contrast to traditional approaches, neural network based methods own the ability of automatic feature learning and alleviate the problem of severe dependence on human-designed features and kernels.", "labels": [], "entities": []}, {"text": "However, previous researches imply that some features exploited by traditional methods are still informative and can help enhance the performance of neural network in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 167, "end_pos": 190, "type": "TASK", "confidence": 0.9433130025863647}]}, {"text": "One simple but effective approach is to concatenate lexical level features to features extracted by neural network and directly pass the combined vector to classifier.", "labels": [], "entities": []}, {"text": "In this way,, achieve better performances when considering some external features produced by existing NLP tools.", "labels": [], "entities": []}, {"text": "Another more sophisticated method adjusts the structure of neural network according to the parse trees of input sentences.", "labels": [], "entities": []}, {"text": "The results of () empirically suggest syntactic structures from recursive models might offer useful power in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.9386245906352997}]}, {"text": "Besides relation classification, parse tree also gives neural network a big boost in other NLP tasks (.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.9374599158763885}]}, {"text": "Dependency parse tree is valuable in relation classification task.", "labels": [], "entities": [{"text": "Dependency parse", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7787253558635712}, {"text": "relation classification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.9154249131679535}]}, {"text": "According to our observation, dependency tree usually shortens the distances between pairs of marked entities and helps trim off redundant words, when comparing with plain text.", "labels": [], "entities": []}, {"text": "For example, in the sentence shown in, two marked entities span the whole sentence, which brings much noise to the recognition of their relation.", "labels": [], "entities": []}, {"text": "By contrast, in the dependency tree corresponding to the sentence, the path between two marked entities comprises only four words and extracts a key phrase \"caused by\" that clearly implies the relation of entities.", "labels": [], "entities": []}, {"text": "This property of dependency tree is ubiquitous and consistent with the Shortest Path Hypothesis which is accepted by previous studies.", "labels": [], "entities": []}, {"text": "To better utilize the powerful neural network and make the best of the abundant linguistic knowledge provided by parse tree, we propose a position encoding convolutional neural network (PECNN) based on dependency parse tree for relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 228, "end_pos": 251, "type": "TASK", "confidence": 0.8425283133983612}]}, {"text": "In our model, to sufficiently benefit from the important property of dependency tree, we introduce the position feature and modify it in the context of parse tree.", "labels": [], "entities": []}, {"text": "Tree-based position features encode the relative positions between each word and marked entities in a dependency tree, and help the network pay more attention to the key phrases in sentences.", "labels": [], "entities": []}, {"text": "Moreover, with a redefinition of \"context\", we design two kinds of tree-based convolution kernels for capturing the structural information and salient features of sentences.", "labels": [], "entities": []}, {"text": "To sum up, our contributions are: 1) We propose a novel convolutional neural network with tree-based convolution kernels for relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 125, "end_pos": 148, "type": "TASK", "confidence": 0.8685660362243652}]}, {"text": "2) We confirm the feasibility of transferring the position feature from plain text to dependency tree, and compare the performances of different position features by experiments.", "labels": [], "entities": []}, {"text": "3) Experimental results on the benchmark dataset show that our proposed method outperforms the state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "To make the mechanism of our model clear, we also visualize the influence of tree-based position feature on relation classification task.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.8660146594047546}]}], "datasetContent": [{"text": "To evaluate our method, we conduct experiments on the SemEval2010 Task 8 dataset which is a widely used benchmark for relation classification.", "labels": [], "entities": [{"text": "SemEval2010 Task 8 dataset", "start_pos": 54, "end_pos": 80, "type": "DATASET", "confidence": 0.7142794728279114}, {"text": "relation classification", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.9259508848190308}]}, {"text": "The dataset contains 8, 000 training sentences and 2, 717 test sentences.", "labels": [], "entities": []}, {"text": "In each sentence, two entities are marked as target entities.", "labels": [], "entities": []}, {"text": "The predefined target relations include 9 directed relations and an undirected Other class.", "labels": [], "entities": []}, {"text": "The 9 directed relations are Cause-Effect, ComponentWhole, Content-Container, Entity-Destination, Entity-Origin, Instrument-Agency, MemberCollection, Message-Topic and Product-Producer.", "labels": [], "entities": []}, {"text": "\"Directed\" here means, for example, CauseEffect(e 1 , e 2 ) and Cause-Effect(e 2 , e 1 ) are two different relations.", "labels": [], "entities": []}, {"text": "In another word, the directionality of relation also matters.", "labels": [], "entities": []}, {"text": "And sentences that do not belong to any directed relation are labelled as Other.", "labels": [], "entities": []}, {"text": "Therefore, relation classification on this dataset is a 19-class classification problem.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.9224373400211334}, {"text": "19-class classification", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7461281716823578}]}, {"text": "Following previous studies, we use the official evaluation metric, macro-averaged F1-score with directionality taken into account and the Other class ignored.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9417949318885803}]}], "tableCaptions": [{"text": " Table 1: Comparison of different relation classification models. The symbol  *  indicates the results with special treatment of the", "labels": [], "entities": [{"text": "relation classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.737652063369751}]}, {"text": " Table 2: Comparison of different position features.", "labels": [], "entities": []}]}