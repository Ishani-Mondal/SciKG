{"title": [], "abstractContent": [{"text": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7373368740081787}]}, {"text": "Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years.", "labels": [], "entities": []}, {"text": "However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses.", "labels": [], "entities": []}, {"text": "We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge that can be derived from a semantic network.", "labels": [], "entities": []}, {"text": "Our approach provides multiple advantages in comparison to the previous approaches, including its high coverage and the ability to generate accurate representations even for infrequent word senses.", "labels": [], "entities": [{"text": "coverage", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9704738259315491}]}, {"text": "We carryout evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modeling the meanings of linguistic items in a machine-interpretable form, i.e., semantic representation, is one of the oldest, yet most active, areas of research in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Modeling the meanings of linguistic items", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8809017539024353}, {"text": "Natural Language Processing (NLP)", "start_pos": 166, "end_pos": 199, "type": "TASK", "confidence": 0.7009116510550181}]}, {"text": "The field has recently experienced a resurgence of interest with neural network-based models that view the representation task as a language modeling problem and learn dense representations (usually referred to as embeddings) by efficiently processing massive amounts of texts.", "labels": [], "entities": []}, {"text": "However, either in its conventional count-based form or the recent predictive approach, the prevailing objective of representing each word type as a single point in the semantic space has a major limitation: it ignores the fact that words can have multiple meanings and conflates all these meanings into a single representation.", "labels": [], "entities": []}, {"text": "This objective can have negative impacts on accurate semantic modeling, e.g., semantically unrelated words that are synonymous to different senses of a word are pulled towards each other in the semantic space).", "labels": [], "entities": []}, {"text": "For example, the two semantically-unrelated words squirrel and keyboard are pulled towards each other in the semantic space for their similarities to two different senses of mouse, i.e., rodent and computer input device.", "labels": [], "entities": []}, {"text": "Recently, there has been a growing interest in addressing the meaning conflation deficiency of word representations.", "labels": [], "entities": []}, {"text": "A series of techniques have been developed to associate a word to multiple points in the semantic space by clustering its contexts in a given text corpus and learning distinct representations for individual clusters).", "labels": [], "entities": []}, {"text": "However, these techniques usually assume a fixed number of word senses per word type, disregarding the fact that the number of senses per word can range from one (monosemy) to dozens.", "labels": [], "entities": []}, {"text": "tackled this issue by allowing the number to be dynamically adjusted for each word during training.", "labels": [], "entities": []}, {"text": "However, the approach and all the other clusteringbased techniques still suffer from the fact that the computed sense representations are not linked to any sense inventory, a linking which would require large amounts of sense-annotated data).", "labels": [], "entities": []}, {"text": "In addition, because of their dependence on knowledge derived from a text corpus, these techniques are generally unable to learn accurate representations for word senses that are infrequent in the underlying corpus.", "labels": [], "entities": []}, {"text": "Knowledge-based techniques tackle these issues by deriving sense-specific knowledge from external sense inventories, such as WordNet, and learning representations that are linked to the sense inventory.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.951128363609314}]}, {"text": "These approaches either use sense definitions and employ Word Sense Disambiguation (WSD) to gather sense-specific contexts or take advantage of the properties of WordNet, such as synonymy and direct semantic relations.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.7006879647572836}]}, {"text": "However, the non-optimal WSD techniques and the shallow utilization of knowledge from WordNet do not allow these techniques to learn accurate and high-coverage semantic representations for all senses in the inventory.", "labels": [], "entities": [{"text": "WSD", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9000604748725891}, {"text": "WordNet", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9295870065689087}]}, {"text": "We propose a technique that de-conflates a given word representation into its constituent sense representations by exploiting deep knowledge from the semantic network of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 170, "end_pos": 177, "type": "DATASET", "confidence": 0.9560161828994751}]}, {"text": "Our approach provides the following three main advantages in comparison to the past work: (1) our representations are linked to the WordNet sense inventory and, accordingly, the number of senses fora word is a dynamic parameter which matches that defined by WordNet; (2) the deep exploitation of WordNet's semantic network allows us to obtain accurate semantic representations, even for word senses that are infrequent in generic text corpora; and (3) our methodology involves only minimal parameter tuning and can be effectively applied to any sense inventory that is viewable as a semantic network and to any word representation technique.", "labels": [], "entities": [{"text": "WordNet sense inventory", "start_pos": 132, "end_pos": 155, "type": "DATASET", "confidence": 0.9005851546923319}, {"text": "WordNet", "start_pos": 258, "end_pos": 265, "type": "DATASET", "confidence": 0.9591913819313049}]}, {"text": "We evaluate our sense representations in two tasks: word similarity (both incontext and in-isolation) and cross-level semantic similarity.", "labels": [], "entities": [{"text": "cross-level semantic similarity", "start_pos": 106, "end_pos": 137, "type": "TASK", "confidence": 0.5983915328979492}]}, {"text": "Experimental results show that the proposed technique can provide consistently high performance across six datasets, outperforming the recent state of the art on most of them.", "labels": [], "entities": []}], "datasetContent": [{"text": "We benchmarked our sense representation approach against several recent techniques on two standard tasks: word similarity ( \u00a73.2), for which we evaluate on both in-isolation and in-context similarity datasets, and cross-level semantic similarity ( \u00a73.3).", "labels": [], "entities": [{"text": "sense representation", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.7372252345085144}, {"text": "word similarity", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.7112914323806763}, {"text": "cross-level semantic similarity", "start_pos": 214, "end_pos": 245, "type": "TASK", "confidence": 0.6160798768202463}]}, {"text": "As our word representations, we used the 300-d Word2vec () word embeddings trained on the Google News dataset 4 mainly for their popularity across different NLP applications.", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.8061688343683878}]}, {"text": "However, our approach is equally applicable to any count-based representation technique ( or any other embedding approach (.", "labels": [], "entities": []}, {"text": "We leave the evaluation and comparison of various word representation techniques with different training approaches, objectives, and dimensionalities to future work.", "labels": [], "entities": [{"text": "word representation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7068343311548233}]}, {"text": "Recall from \u00a72.3 that our procedure for learning sense representations needs only one parameter to be tuned, i.e., \u03bb.", "labels": [], "entities": []}, {"text": "We did not perform an extensive tuning on the value of this parameter and set its value to 1 /5 after trying four different values (1, 1 /2, 1 /5, and 1 /10) on a small validation dataset.", "labels": [], "entities": []}, {"text": "We leave the more systematic tuning of the parameter and the choice of alternative decay functions (cf. \u00a72.3) to future work.", "labels": [], "entities": []}, {"text": "The size of the sense biasing words lists.", "labels": [], "entities": []}, {"text": "Also recall from \u00a72.2 that the extracted lists of sense biasing words were originally as large as the total number of unique strings in WordNet (around 150K in ver. 3.0).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9524977803230286}]}, {"text": "But, given that we use an exponential decay function in our learning algorithm (cf. \u00a72.3), the impact of the low-ranking words in the list is negligible.", "labels": [], "entities": []}, {"text": "In fact, we observed that taking a very small portion of the top-ranking words, i.e., the top 25, produces similarity scores that are on par with those generated when the full lists were considered.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 107, "end_pos": 124, "type": "METRIC", "confidence": 0.9594832956790924}]}, {"text": "Therefore, we experimented with the down-sized lists which enabled us to generate very quickly sense representations for all word senses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9728057384490967}]}, {"text": "where the similarity between two words is computed as the average of all the pairwise similarities between their senses, and (2) AvgSimC: where each pairwise sense similarity is weighted by the relevance of each sense to its corresponding context.", "labels": [], "entities": [{"text": "AvgSimC", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9963609576225281}]}, {"text": "For all the other datasets, since words are not provided with any context (they are in isolation), we measure the similarity between two words as that between their most similar senses.", "labels": [], "entities": []}, {"text": "In all the experiments, we use the cosine distance as our similarity measure.", "labels": [], "entities": [{"text": "similarity", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9596580266952515}]}, {"text": "show the results of our system, DE-CONF, and the comparison systems on the SCWS and the other four similarity datasets, respectively.", "labels": [], "entities": [{"text": "DE-CONF", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.8606843948364258}, {"text": "SCWS", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.942177414894104}]}, {"text": "In both tables we also report the word vectors baseline, whenever they are available, which is computed by directly comparing the corresponding word representations of the two words (\u2208 V).", "labels": [], "entities": []}, {"text": "Note that the word-based baseline does not apply to the approach of as it is purely based on the semantic network of WordNet and does not use any pre-trained word embeddings.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9460451602935791}]}, {"text": "Across the four strategies, S2A proves to be the most effective for DECONF and the representations of.", "labels": [], "entities": [{"text": "DECONF", "start_pos": 68, "end_pos": 74, "type": "TASK", "confidence": 0.5083339214324951}]}, {"text": "The representations of     egy whereas those of Iacobacci et al. do not show a consistent trend with relatively low performance across the four strategies.", "labels": [], "entities": []}, {"text": "Also, a comparison of our results across the S2W and S2A strategies reveals that a word's aggregated representation, i.e., the centroid of the representations of its senses, is more accurate than its original word representation.", "labels": [], "entities": []}, {"text": "Our analysis showed that the performance of the approaches of and were hampered partly due to their limited coverage.", "labels": [], "entities": []}, {"text": "In fact, the former was unable to model around 35% of the synsets in WordNet 1.7.1, mainly for its shallow exploitation of knowledge from WordNet, whereas the latter approach did not cover around 15% of synsets in WordNet 3.0.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9382614493370056}]}, {"text": "provide near-full coverage for word senses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9704023003578186}]}, {"text": "However, the relatively low performance of their system shows that the usage of glosses in WordNet and the automated disambiguation have not resulted in accurate sense representations.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.9543446898460388}]}, {"text": "Thanks to its deep exploitation of the underlying resource, our approach provides more reliable representations and full coverage for all word senses and synsets in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9564759731292725}]}, {"text": "The three best-performing systems in the task are Meerkat Mafia () (r = 37.5, \u03c1 = 39.3), SimCompass () (r = 35.4, \u03c1 = 34.9), and SemantiKLUE () (r = 17.9, \u03c1 = 18.8).", "labels": [], "entities": []}, {"text": "Note that these systems are specifically designed for the cross-level similarity measurement task.", "labels": [], "entities": [{"text": "cross-level similarity measurement task", "start_pos": 58, "end_pos": 97, "type": "TASK", "confidence": 0.7662405967712402}]}, {"text": "For instance, the best-ranking system in the task leverages a compilation of several dictionaries, including The American Heritage Dictionary, Wiktionary and WordNet, in order to handle slang terms and rare usages, which leads to its competitive performance ().", "labels": [], "entities": [{"text": "American Heritage Dictionary", "start_pos": 113, "end_pos": 141, "type": "DATASET", "confidence": 0.8343784809112549}, {"text": "WordNet", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.9155452251434326}]}], "tableCaptions": [{"text": " Table 3: Pearson (r \u00d7 100) and Spearman (\u03c1 \u00d7 100) correlation scores on four standard word similarity benchmarks. For each", "labels": [], "entities": [{"text": "Pearson (r \u00d7 100) and Spearman (\u03c1 \u00d7 100) correlation", "start_pos": 10, "end_pos": 62, "type": "METRIC", "confidence": 0.7935691667454583}]}, {"text": " Table 4: Spearman correlation scores (\u03c1 \u00d7 100) on the Stan-", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.5109988749027252}, {"text": "Stan-", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.9489806294441223}]}]}