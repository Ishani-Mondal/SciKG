{"title": [{"text": "Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network", "labels": [], "entities": [{"text": "Intra-Sentential Subject Zero Anaphora Resolution", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.6280798196792603}]}], "abstractContent": [{"text": "This paper proposes a method for intra-sentential subject zero anaphora resolution in Japanese.", "labels": [], "entities": [{"text": "intra-sentential subject zero anaphora resolution", "start_pos": 33, "end_pos": 82, "type": "TASK", "confidence": 0.6864240050315857}]}, {"text": "Our proposed method utilizes a Multi-column Convolutional Neural Network (MCNN) for predicting zero anaphoric relations.", "labels": [], "entities": [{"text": "predicting zero anaphoric relations", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.8568689376115799}]}, {"text": "Motivated by Centering Theory and other previous works, we exploit as clues both the surface word sequence and the dependency tree of a target sentence in our MCNN.", "labels": [], "entities": []}, {"text": "Even though the F-score of our method was lower than that of the state-of-the-art method, which achieved relatively high recall and low precision , our method achieved much higher precision (>0.8) in a wide range of recall levels.", "labels": [], "entities": [{"text": "F-score", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.999173104763031}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9983293414115906}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9946887493133545}, {"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9972968697547913}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9959532022476196}]}, {"text": "We believe such high precision is crucial for real-world NLP applications and thus our method is preferable to the state-of-the-art method.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9950802326202393}]}], "introductionContent": [{"text": "In such pro-drop languages as Japanese, Chinese and Italian, pronouns are frequently omitted in text.", "labels": [], "entities": []}, {"text": "For example, the subject of uketa (suffered) is unrealized in the following Japanese example (1): (1) sono-houkokusho-wa seifu i -ga The report pointed out that the government i agreed to a treaty and (it i ) suffered economically.", "labels": [], "entities": []}, {"text": "The omitted argument is called a zero anaphor, which is represented using \u03d5.", "labels": [], "entities": []}, {"text": "In example (1), zero anaphor \u03d5 i refers to its antecedent, seifu i (government).", "labels": [], "entities": []}, {"text": "Such a reference phenomenon is called zero anaphora.", "labels": [], "entities": []}, {"text": "Identifying zero anaphoric relations is an essential task in developing such accurate NLP applications as information extraction and machine translation for pro-drop languages.", "labels": [], "entities": [{"text": "Identifying zero anaphoric relations", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8223029971122742}, {"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.8498881459236145}, {"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7822234034538269}]}, {"text": "For example, in Japanese, 60% of subjects in newspaper articles are unrealized as zero anaphors (.", "labels": [], "entities": []}, {"text": "This paper proposes a method for intra-sentential subject zero anaphora resolution, in which a zero anaphor and its antecedent appear in the same sentence and the zero anaphor must be a subject of a predicate, for Japanese.", "labels": [], "entities": [{"text": "intra-sentential subject zero anaphora resolution", "start_pos": 33, "end_pos": 82, "type": "TASK", "confidence": 0.6673401832580567}]}, {"text": "We target subject zero anaphors because they represent 85% of the intrasentential zero anaphora in our data set (example (1) is such a case).", "labels": [], "entities": []}, {"text": "Furthermore, this work focuses on intra-sentential zero anaphora because intersentential cases, in which a zero anaphor and its antecedent do not appear in the same sentence, are extremely difficult.", "labels": [], "entities": []}, {"text": "The accuracy of the state-of-theart method for resolving inter-sentential anaphora is low (), and we believe the current technologies are not mature enough to deal with inter-sentential cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996159076690674}]}, {"text": "Our method locally predicts the likelihood of a zero anaphoric relation between every possible combination of potential zero anaphor and potential antecedent without considering the other (potential) zero anaphoric relations in the same sentence.", "labels": [], "entities": []}, {"text": "The final determination of zero anaphoric relations for each zero anaphor in a given sentence is done in a greedy way; only the most likely candidate antecedent for each zero anaphor is selected as its antecedent as far as the likelihood score exceeds a given threshold.", "labels": [], "entities": []}, {"text": "This approach contrasts with global optimization methods, which have recently become popular.", "labels": [], "entities": [{"text": "global optimization", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7216493189334869}]}, {"text": "These methods use the constraints among possible zero anaphoric relations, such as \"if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate\", and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning () and integer linear programming (.", "labels": [], "entities": []}, {"text": "Although the global optimization methods have outperformed the previous greedy-style methods, our contention is that greedy-style methods can still, in a certain sense, outperform the state-of-the-art global optimization methods.'s global optimization method achieved the state-ofthe-art F-score for Japanese intra-sentential subject zero anaphora resolution, but its performance has not yet reached a level of practical use.", "labels": [], "entities": [{"text": "F-score", "start_pos": 288, "end_pos": 295, "type": "METRIC", "confidence": 0.983126699924469}, {"text": "Japanese intra-sentential subject zero anaphora resolution", "start_pos": 300, "end_pos": 358, "type": "TASK", "confidence": 0.6067981074253718}]}, {"text": "In our setting, for example, it actually obtained a precision of only 0.61, and even after attempting to obtain more reliable zero anaphoric relations by several modifications, we could only achieve 0.80 precision at extremely low recall levels (<0.01).", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9985424280166626}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9910111427307129}, {"text": "recall", "start_pos": 231, "end_pos": 237, "type": "METRIC", "confidence": 0.9971683621406555}]}, {"text": "On the other hand, while our proposed greedy-style method obtained a lower F-score than Ouchi et al.'s method, it achieved much higher precision in a wide range of recall levels (e.g., around 0.8 precision at 0.25 in recall and around 0.7 precision at 0.4 in recall).", "labels": [], "entities": [{"text": "F-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9991664886474609}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9989854693412781}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9845593571662903}, {"text": "precision", "start_pos": 196, "end_pos": 205, "type": "METRIC", "confidence": 0.9848857522010803}, {"text": "precision", "start_pos": 239, "end_pos": 248, "type": "METRIC", "confidence": 0.959930956363678}, {"text": "recall", "start_pos": 259, "end_pos": 265, "type": "METRIC", "confidence": 0.9621198177337646}]}, {"text": "We believe such high precision is crucial to realworld applications, even though the recall remains low, and thus our method is preferable to Ouchi et al.'s method in that sense.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9971925616264343}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9991905093193054}]}, {"text": "In our proposed method, we use a Multi-column Convolutional Neural Network (MCNN), which is a variant of a Convolutional Neural Network (CNN) ().", "labels": [], "entities": []}, {"text": "An MCNN has several independent columns, each of which has its own convolutional and pooling layers.", "labels": [], "entities": []}, {"text": "The outputs of all the columns are combined in the final layer to provide a final prediction.", "labels": [], "entities": []}, {"text": "In this work, motivated by Centering Theory ( and other previous works, we exploit as distinct columns the word sequences obtained from the surface word sequence and the dependency tree of a target sentence in our MCNN.", "labels": [], "entities": []}, {"text": "Although the existing works also exploited such word sequences, they used only particular types of information from them as features based on the researchers' linguistic insights.", "labels": [], "entities": []}, {"text": "In contrast, we minimized such feature engineering due to using an MCNN.", "labels": [], "entities": [{"text": "MCNN", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9304648041725159}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly overview previous work on zero anaphora resolution.", "labels": [], "entities": [{"text": "zero anaphora resolution", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6135243276755015}]}, {"text": "In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture.", "labels": [], "entities": [{"text": "zero anaphora resolution", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.656234085559845}]}, {"text": "We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The documents in the corpus were divided into five subsets, three of which were used as a training data set, one as a development data set, and one as a testing data set.", "labels": [], "entities": []}, {"text": "The statistics of our data set are summarized in.", "labels": [], "entities": []}, {"text": "We evaluated the performance of our intra-sentential subject zero anaphora resolution method and three baseline methods described below using the revised annotated results in our data set.", "labels": [], "entities": [{"text": "intra-sentential subject zero anaphora resolution", "start_pos": 36, "end_pos": 85, "type": "TASK", "confidence": 0.6649621605873108}]}, {"text": "We implemented our MCNN using Theano (.", "labels": [], "entities": []}, {"text": "We pre-trained 300-dimensional word embedding vectors for 1,658,487 words 5 using Skip-gram with a negative-sampling algorithm (Mikolov et al., 2013) 6 on a set of all the sentences extracted from Wikipedia articles 7 (35,975,219 sentences).", "labels": [], "entities": []}, {"text": "We removed from the training data all the words that only appeared once before training.", "labels": [], "entities": []}, {"text": "In training, we treated them as unknown words and assigned them a random vector.", "labels": [], "entities": []}, {"text": "To avoid overfitting, we applied early-stopping and dropout) of 0.5 to the final layer.", "labels": [], "entities": [{"text": "early-stopping", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.9715607166290283}]}, {"text": "We used an SGD with mini-batches of 100 and a learning rate decay of 0.95.", "labels": [], "entities": [{"text": "learning rate decay", "start_pos": 46, "end_pos": 65, "type": "METRIC", "confidence": 0.9368215600649515}]}, {"text": "We ran ten epochs through all of the training data, where each epoch consisted of many mini-batch updates.", "labels": [], "entities": []}, {"text": "We utilized 3-, 4-and 5-grams with 100 filters each and used the F-score of positive instances as our evaluation metric.", "labels": [], "entities": [{"text": "F-score", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9982524514198303}]}, {"text": "The total number of the nodes in the final layers of our MCNN was 3,300: 11 columns \u00d7 3 N -gram \u00d7 100 filters.", "labels": [], "entities": []}, {"text": "Word segmentation, PoS tagging and dependency parsing of the sentences in the NAIST Text Corpus were performed by a Japanese morphological analyzer, MeCab 8 (), and a depentwo sets.dency parser, J.DepP 9).", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6843474805355072}, {"text": "PoS tagging", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.8350801169872284}, {"text": "dependency parsing", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7360983192920685}, {"text": "NAIST Text Corpus", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9692106445630392}]}], "tableCaptions": [{"text": " Table 1: Statistics of our data set", "labels": [], "entities": []}, {"text": " Table 3: Results of instance-wise evaluation for anaphoric and cataphoric sets", "labels": [], "entities": []}]}