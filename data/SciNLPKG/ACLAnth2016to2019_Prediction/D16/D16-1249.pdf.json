{"title": [{"text": "Supervised Attentions for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.861345092455546}]}], "abstractContent": [{"text": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8289097547531128}, {"text": "neural machine translation", "start_pos": 65, "end_pos": 91, "type": "TASK", "confidence": 0.668459971745809}]}, {"text": "We simply compute the distance between the machine attentions and the \"true\" alignments, and minimize this cost in the training procedure.", "labels": [], "entities": []}, {"text": "Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has gained popularity in recent two years (e.g. (, especially for the attentionbased models of.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8269500335057577}]}, {"text": "The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word.", "labels": [], "entities": [{"text": "NMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9829729795455933}]}, {"text": "However, the attention or alignment quality of NMT is still very low (.", "labels": [], "entities": [{"text": "attention or alignment quality", "start_pos": 13, "end_pos": 43, "type": "METRIC", "confidence": 0.6823461800813675}]}, {"text": "In this paper, we alleviate the above issue by utilizing the alignments (human annotated data or machine alignments) of the training set.", "labels": [], "entities": []}, {"text": "Given the alignments of all the training sentence pairs, we add an alignment distance cost to the objective function.", "labels": [], "entities": []}, {"text": "Thus, we not only maximize the log translation probabilities, but also minimize the alignment distance cost.", "labels": [], "entities": []}, {"text": "Large-scale experiments over Chineseto-English on various test sets show that our best method fora single system improves the translation quality significantly over the large vocabulary NMT system (Section 5) and beats the state-of-theart syntax-based system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Single system results in terms of (TER-BLEU)/2 (T-B, the lower the better) on 5 million Chinese to English training set.", "labels": [], "entities": [{"text": "TER-BLEU)/2", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9697364370028178}, {"text": "T-B", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9025219082832336}, {"text": "Chinese to English training set", "start_pos": 98, "end_pos": 129, "type": "DATASET", "confidence": 0.605676406621933}]}, {"text": " Table 2: Alignment F1 scores of different models.", "labels": [], "entities": [{"text": "Alignment F1 scores", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7516706387201945}]}]}