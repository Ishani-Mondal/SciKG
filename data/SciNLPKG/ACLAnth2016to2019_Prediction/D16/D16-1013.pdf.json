{"title": [{"text": "Natural Language Comprehension with the EpiReader", "labels": [], "entities": [{"text": "EpiReader", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.5949466824531555}]}], "abstractContent": [{"text": "We present EpiReader, a novel model for machine comprehension of text.", "labels": [], "entities": []}, {"text": "Machine comprehension of unstructured, real-world text is a major research goal for natural language processing.", "labels": [], "entities": [{"text": "Machine comprehension of unstructured, real-world text", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8217264328684125}, {"text": "natural language processing", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.6491115788618723}]}, {"text": "Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions.", "labels": [], "entities": []}, {"text": "EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text.", "labels": [], "entities": []}, {"text": "We present experiments demonstrating that EpiReader sets anew state-of-the-art on the CNN and Children's Book Test benchmarks, outperforming previous neural models by a significant margin.", "labels": [], "entities": [{"text": "CNN and Children's Book Test benchmarks", "start_pos": 86, "end_pos": 125, "type": "DATASET", "confidence": 0.8687851173537118}]}], "introductionContent": [{"text": "When humans reason about the world, we tend to formulate a variety of hypotheses and counterfactuals, then test them in turn by physical or thought experiments.", "labels": [], "entities": []}, {"text": "The philosopher Epicurus first formalized this idea in his Principle of Multiple Explanations: if several theories are consistent with the observed data, retain them all until more data is observed.", "labels": [], "entities": []}, {"text": "In this paper, we argue that the same principle can be applied to machine comprehension of natural language.", "labels": [], "entities": []}, {"text": "We propose a deep neural comprehension model, trained end-to-end, that we call EpiReader.", "labels": [], "entities": [{"text": "EpiReader", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.8312417268753052}]}, {"text": "Comprehension of natural language by machines, at a near-human level, is a prerequisite for an extremely broad class of useful applications of artificial intelligence.", "labels": [], "entities": []}, {"text": "Indeed, most human knowledge is collected in the natural language of text.", "labels": [], "entities": []}, {"text": "Machine comprehension (MC) has therefore garnered significant attention from the machine learning research community.", "labels": [], "entities": [{"text": "Machine comprehension (MC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9050524950027465}]}, {"text": "Machine comprehension is typically evaluated by posing a set of questions based on a supporting text passage, then scoring a system's answers to those questions.", "labels": [], "entities": [{"text": "Machine comprehension", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7834627330303192}]}, {"text": "Such tests are objectively gradable and may assess a range of abilities, from basic understanding to causal reasoning to inference ().", "labels": [], "entities": []}, {"text": "In the past year, two large-scale MC datasets have been released: the CNN/Daily Mail corpus, consisting of news articles from those outlets (, and the Children's Book Test (CBT), consisting of short excerpts from books available through Project Gutenberg (.", "labels": [], "entities": [{"text": "CNN/Daily Mail corpus", "start_pos": 70, "end_pos": 91, "type": "DATASET", "confidence": 0.8616951107978821}, {"text": "Children's Book Test (CBT)", "start_pos": 151, "end_pos": 177, "type": "DATASET", "confidence": 0.7547846947397504}]}, {"text": "The size of these datasets (on the order of 10 5 distinct questions) makes them amenable to data-intensive deep learning techniques.", "labels": [], "entities": []}, {"text": "Both corpora use Clozestyle questions, which are formulated by replacing a word or phrase in a given sentence with a placeholder token.", "labels": [], "entities": []}, {"text": "The task is then to find the answer that \"fills in the blank\".", "labels": [], "entities": []}, {"text": "In tandem with these corpora, a host of neural machine comprehension models has been developed (.", "labels": [], "entities": []}, {"text": "We compare EpiReader to these earlier models through training and evaluation on the CNN and CBT datasets.", "labels": [], "entities": [{"text": "CNN and CBT datasets", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.7842597812414169}]}, {"text": "EpiReader factors into two components.", "labels": [], "entities": []}, {"text": "The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text; we call this the Extractor.", "labels": [], "entities": []}, {"text": "The second component reranks the proposed answers based on deeper semantic comparisons with the text; we call this the Reasoner.", "labels": [], "entities": []}, {"text": "We can summarize this process as Extract \u2192 Hypothesize \u2192 Test 2 . The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment (RTE) (), also known as natural language inference.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 147, "end_pos": 183, "type": "TASK", "confidence": 0.6181745280822118}]}, {"text": "This process is computationally demanding.", "labels": [], "entities": []}, {"text": "Thus, the Extractor serves the important function of filtering a large set of potential answers down to a small, tractable set of likely candidates for more thorough testing.", "labels": [], "entities": []}, {"text": "The two-stage process is an analogue of structured prediction cascades, wherein a sequence of increasingly complex models progressively filters the output space in order to trade off between model complexity and limited computational resources.", "labels": [], "entities": []}, {"text": "We demonstrate that this cascade-like framework is applicable to machine comprehension and can be trained end-to-end with stochastic gradient descent.", "labels": [], "entities": []}, {"text": "The Extractor follows the form of a pointer network (, and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question.", "labels": [], "entities": []}, {"text": "This approach was used (on its own) for question answering with the Attention Sum Reader (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9256805181503296}]}, {"text": "The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness.", "labels": [], "entities": []}, {"text": "The Reasoner forms hypotheses by inserting the candidate answers into the question, then estimates the concordance of each hypothesis with each sentence in the supporting text.", "labels": [], "entities": []}, {"text": "We use these estimates as a measure of the evidence fora hypothesis, and aggregate evidence overall sentences.", "labels": [], "entities": []}, {"text": "In the end, we combine the Reasoner's evidence with the Extractor's probability estimates to produce a final ranking of the answer candidates.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we formally define the problem to be solved and give some background on the datasets used in our tests.", "labels": [], "entities": []}, {"text": "In Section 3 we describe EpiReader, focusing on its two components and how they combine.", "labels": [], "entities": []}, {"text": "Section 4 discusses related work, and Section 5 details our experimental results and analysis.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "EpiReader's task is to answer a Cloze-style question by reading and comprehending a supporting passage of text.", "labels": [], "entities": [{"text": "comprehending a supporting passage of text", "start_pos": 68, "end_pos": 110, "type": "TASK", "confidence": 0.7433105309804281}]}, {"text": "The training and evaluation data consist of tuples (Q, T , a * , A), where Q is the question (a sequence of words {q 1 , ...q |Q| }), T is the text (a sequence of words {t 1 , ..., t |T | }), A is a set of possible answers {a 1 , ..., a |A| }, and a * \u2208 A is the correct answer.", "labels": [], "entities": []}, {"text": "All words come from a vocabulary V , and A \u2282 T . In each question, there is a placeholder token indicating the missing word to be filled in.", "labels": [], "entities": []}, {"text": "CNN This corpus is built using articles scraped from the CNN website.", "labels": [], "entities": [{"text": "CNN This corpus", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9646554191907247}, {"text": "CNN website", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9475387930870056}]}, {"text": "The articles themselves form the text passages, and questions are generated synthetically from short summary statements that accompany each article.", "labels": [], "entities": []}, {"text": "These summary points are (presumably) written by human authors.", "labels": [], "entities": []}, {"text": "Each question is created by replacing a named entity in a summary point with a placeholder token.", "labels": [], "entities": []}, {"text": "All named entities in the articles and questions are replaced with anonymized tokens that are shuffled for each (Q, T ) pair.", "labels": [], "entities": []}, {"text": "This forces the model to rely only on the text, rather than learning world knowledge about the entities during training.", "labels": [], "entities": []}, {"text": "The CNN corpus (henceforth CNN) was presented by.", "labels": [], "entities": [{"text": "CNN corpus (henceforth CNN)", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.9076393246650696}]}, {"text": "Children's Book Test This corpus is constructed similarly to CNN, but from children's books available through Project Gutenberg.", "labels": [], "entities": [{"text": "Children's Book Test", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9357971549034119}]}, {"text": "Rather than articles, the text passages come from book excerpts of 20 sentences.", "labels": [], "entities": []}, {"text": "Since no summaries are provided, a question is generated by replacing a single word in the next (i.e. 21st) sentence.", "labels": [], "entities": []}, {"text": "The corpus distinguishes questions based on the type of word that is replaced: named entity, common noun, verb, or preposition.", "labels": [], "entities": []}, {"text": "Like, we focus only on the first two classes since showed that stan-dard LSTM language models already achieve humanlevel performance on the latter two.", "labels": [], "entities": []}, {"text": "Unlike in the CNN corpora, named entities are not anonymized and shuffled in the Children's Book Test (CBT).", "labels": [], "entities": [{"text": "Children's Book Test (CBT)", "start_pos": 81, "end_pos": 107, "type": "DATASET", "confidence": 0.8118574619293213}]}, {"text": "The different methods of construction for questions in each corpus mean that CNN and CBT assess different aspects of comprehension.", "labels": [], "entities": []}, {"text": "The summary points of CNN area condensed paraphrasing of information in the text; thus, determining the correct answer relies mostly on recognizing textual entailment.", "labels": [], "entities": [{"text": "CNN area", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8408543467521667}]}, {"text": "On the other hand, CBT is about story prediction.", "labels": [], "entities": [{"text": "CBT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8939661979675293}, {"text": "story prediction", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.8295972943305969}]}, {"text": "It is a comprehension task insofar as comprehension is likely necessary for story prediction, but comprehension alone may not be sufficient.", "labels": [], "entities": [{"text": "story prediction", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8158080279827118}]}, {"text": "Indeed, there are some CBT questions that are unanswerable given the preceding context.", "labels": [], "entities": [{"text": "CBT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8928932547569275}]}], "tableCaptions": [{"text": " Table 1: Hyperparameter settings for best EpiReaders. D is", "labels": [], "entities": [{"text": "D", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9861293435096741}]}, {"text": " Table 2: Model comparison on the CBT and CNN datasets. Results marked with 1 are from Hill et al. (2016), with 2 are from", "labels": [], "entities": [{"text": "CBT", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8270781636238098}, {"text": "CNN datasets", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8200474083423615}]}, {"text": " Table 3: Ablation study on CBT-CN validation set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9426184892654419}, {"text": "CBT-CN validation set", "start_pos": 28, "end_pos": 49, "type": "DATASET", "confidence": 0.7585320671399435}]}]}