{"title": [], "abstractContent": [{"text": "To learn text understanding models with millions of parameters one needs massive amounts of data.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8158925175666809}]}, {"text": "In this work, we argue that generating data can compensate for this need.", "labels": [], "entities": []}, {"text": "While defining generic data generators is difficult , we propose to allow generators to be \"weakly\" specified in the sense that a set of parameters controls how the data is generated.", "labels": [], "entities": []}, {"text": "Consider for example generators where the example templates, grammar, and/or vocabulary is determined by this set of parameters.", "labels": [], "entities": []}, {"text": "Instead of manually tuning these parameters, we learn them from the limited training data at our disposal.", "labels": [], "entities": []}, {"text": "To achieve this, we derive an efficient algorithm called GENERE that jointly estimates the parameters of the model and the undetermined generation parameters.", "labels": [], "entities": [{"text": "GENERE", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.6339818239212036}]}, {"text": "We illustrate its benefits by learning to solve math exam questions using a highly parametrized sequence-to-sequence neural network.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many tasks require a large amount of training data to be solved efficiently, but acquiring such amounts is costly, both in terms of time and money.", "labels": [], "entities": []}, {"text": "In several situations, a human trainer can provide their domain knowledge in the form of a generator of virtual data, such as a negative data sampler for implicit feedback in recommendation systems, physical 3D rendering engines as a simulator of data in a computer vision system, simulators of physical processes to solve science exam question, and math problem generators for automatically solving math word problems.", "labels": [], "entities": []}, {"text": "* Contributed equally to this work.", "labels": [], "entities": []}, {"text": "Domain-specific data simulators can generate an arbitrary amount of data that can be treated exactly the same way as standard observations, but since they are virtual, they can also be seen as regularizers dedicated to the task we want to solve).", "labels": [], "entities": []}, {"text": "While simple, the idea of data simulation is powerful and can lead to significantly better estimations of a predictive model because it prevents overfitting.", "labels": [], "entities": []}, {"text": "At the same time it is subject to a strong model bias, because such data generators often generate data that is different from the observed data.", "labels": [], "entities": []}, {"text": "Creating virtual samples is strongly linked to transfer learning when the task to transfer is correlated to the objective (.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.9292050302028656}]}, {"text": "The computer vision literature adopted this idea very early through the notion of virtual samples.", "labels": [], "entities": []}, {"text": "Such samples have a natural interpretation: by creating artificial perturbations of an image, its semantics is likely to be unchanged, i.e. training samples can be rotated, blurred, or slightly cropped without changing the category of the objects contained in the image ().", "labels": [], "entities": []}, {"text": "However, for natural language applications the idea of creating invariant transformations is difficult to apply directly, as simple meaning-preserving transformations -such as the replacement of words by their synonyms or active-passive verb transformations -are quite limited.", "labels": [], "entities": []}, {"text": "More advanced meaning-preserving transformations would require an already good model that understands natural language.", "labels": [], "entities": []}, {"text": "A more structure-driven approach is to build top-down generators, such as probabilistic grammars, with a much wider coverage of linguistic phe-nomena.", "labels": [], "entities": []}, {"text": "This way of being able to leverage many years of research in computational linguistics to create good data generators would be a natural and useful reuse of scientific knowledge, and better than blindly believing in the current trend of \"data takes all\".", "labels": [], "entities": []}, {"text": "While the idea of generating data is straightforward, one could argue that it maybe difficult to come up with good generators.", "labels": [], "entities": []}, {"text": "What we mean by a good generator is the ability to help predicting test data when the model is trained on the generated data.", "labels": [], "entities": [{"text": "predicting test", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8834338784217834}]}, {"text": "In this paper, we will show several types of generators, some contributing more than others in their ability to generalize to unseen data.", "labels": [], "entities": []}, {"text": "When designing a good generator there are several decisions one must make: should we generate data by modifying existing training samples, or \"go wild\" and derive a full probabilistic context-free grammar that could possibly generate unnatural examples and add noise to the estimator?", "labels": [], "entities": []}, {"text": "While we do not arrive at a specific framework to build programs that generate virtual data, in this work we assume that a domain expert can easily write a program in a programming language of her choice, leaving some generation parameters unspecified.", "labels": [], "entities": []}, {"text": "In our approach these unspecified parameters are automatically learned, by selecting the ones most compatible with the model and the training data.", "labels": [], "entities": []}, {"text": "In the next section, we introduce GENERE, a generic algorithm that extends any gradient-based learning approach with a data generator that can be tuned while learning the model on the training data using stochastic optimization.", "labels": [], "entities": [{"text": "GENERE", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.620439350605011}]}, {"text": "In Section 2.2, we show how GENERE can be adapted to handle a (possibly non-differentiable) black-box sampler without requiring modifications to it.", "labels": [], "entities": [{"text": "GENERE", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.6113842725753784}]}, {"text": "We also illustrate how this framework can be implemented in practice fora specific use case: the automatic solving of math exam problems.", "labels": [], "entities": [{"text": "automatic solving of math exam problems", "start_pos": 97, "end_pos": 136, "type": "TASK", "confidence": 0.780727873245875}]}, {"text": "Further discussion is given in the concluding section.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we illustrate how GENERE can learn to identify the correct generator, when the data generating family is a mixture of multiple data generators and only one of these distributions -say p 1 -has been used to generate the data.", "labels": [], "entities": [{"text": "GENERE", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.7375717163085938}]}, {"text": "The other distributions (p 2 , \u00b7 \u00b7 \u00b7 , p K ) are generating input-output data samples (x, y) with different distributions.", "labels": [], "entities": []}, {"text": "We verified that the algorithm correctly identifies the correct data distribution, and hence leads to better generalization performances than what the model achieves without the generator.", "labels": [], "entities": []}, {"text": "In this illustrative experiment, a simple text-toequation translation problem is created, where inputs are sentences describing an equation such as \"compute one plus three minus two\", and outputs are symbolic equations, such as \"X = 1 + 3 -2\".", "labels": [], "entities": [{"text": "text-toequation translation", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7039197236299515}]}, {"text": "Numbers were varying between -20 and 20, and equations could have 2 or 3 numbers with 2 or 3 operations.", "labels": [], "entities": []}, {"text": "As our model, we used a 20-dimensional sequence-to-sequence model with LSTM recurrent units.", "labels": [], "entities": []}, {"text": "The model was initialized using 200 iterations of standard gradient descent on the log-probability of the output.", "labels": [], "entities": []}, {"text": "GENERE was run for 500 iterations, varying the fraction of real and generated samples from 0% to 100%.", "labels": [], "entities": [{"text": "GENERE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.943897008895874}]}, {"text": "A 2 regularization of magnitude 0.1 was applied to the model.", "labels": [], "entities": []}, {"text": "The baseline smoothing coefficient was set to 0.98 and the shrinkage parameter was set to 0.99.", "labels": [], "entities": []}, {"text": "All the experiments were repeated 10 times and a constant learning rate of 0.1 was used.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9663650393486023}]}, {"text": "Results are shown on, where the average loss computed on the test data is plotted against the fraction of real data used during learning.", "labels": [], "entities": []}, {"text": "We can see that the best generalization performance is obtained when there is a balanced mix of real and artificial data, but the proportion depends on the amount of training data: on the left hand side, the best performance is obtained with generated data only, meaning that the number of training samples is so small that GENERE only used the training data to select the best base generator (the component p 1 ), and the best performance is attained using only generated data.", "labels": [], "entities": [{"text": "GENERE", "start_pos": 324, "end_pos": 330, "type": "DATASET", "confidence": 0.8694767355918884}]}, {"text": "The plot on the right hand side is interesting because it contains more training data, and the best performance is not obtained using only the generator, but with 40% of the real data, illustrating the fact that it is beneficial to jointly use real and simulated data during training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Test accuracies of the math-exam methods on the", "labels": [], "entities": [{"text": "accuracies", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.7469387054443359}]}]}