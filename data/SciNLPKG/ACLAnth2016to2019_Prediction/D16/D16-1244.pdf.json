{"title": [{"text": "A Decomposable Attention Model for Natural Language Inference", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.6820488174756368}]}], "abstractContent": [{"text": "We propose a simple neural architecture for natural language inference.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6575884222984314}]}, {"text": "Our approach uses attention to decompose the problem into subprob-lems that can be solved separately, thus making it trivially parallelizable.", "labels": [], "entities": []}, {"text": "On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 7, "end_pos": 57, "type": "DATASET", "confidence": 0.6063501425087452}]}, {"text": "Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language inference (NLI) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis.", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7805984020233154}]}, {"text": "NLI is a central problem in language understanding and recently the large SNLI corpus of 570K sentence pairs was created for this task).", "labels": [], "entities": [{"text": "language understanding", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7980844974517822}, {"text": "SNLI corpus", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.7568894028663635}]}, {"text": "We present anew model for NLI and leverage this corpus for comparison with prior work.", "labels": [], "entities": []}, {"text": "A large body of work based on neural networks for text similarity tasks including NLI has been published in recent years (;, inter alia).", "labels": [], "entities": [{"text": "text similarity tasks", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8020845850308737}]}, {"text": "The dominating trend in these models is to build complex, deep text representation models, for example, with convolutional networks (, CNNs henceforth) or long short-term memory networks, LSTMs henceforth) with the goal of deeper sentence comprehension.", "labels": [], "entities": []}, {"text": "While these approaches have yielded impressive results, they are often computationally very expensive, and result in models having millions of parameters (excluding embeddings).", "labels": [], "entities": []}, {"text": "Here, we take a different approach, arguing that for natural language inference it can often suffice to simply align bits of local text substructure and then aggregate this information.", "labels": [], "entities": []}, {"text": "For example, consider the following sentences: \u2022 Bob is in his room, but because of the thunder and lightning outside, he cannot sleep.", "labels": [], "entities": []}, {"text": "\u2022 It is sunny outside.", "labels": [], "entities": []}, {"text": "The first sentence is complex in structure and it is challenging to construct a compact representation that expresses its entire meaning.", "labels": [], "entities": []}, {"text": "However, it is fairly easy to conclude that the second sentence follows from the first one, by simply aligning Bob with Bob and cannot sleep with awake and recognizing that these are synonyms.", "labels": [], "entities": []}, {"text": "Similarly, one can conclude that It is sunny outside contradicts the first sentence, by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible.", "labels": [], "entities": []}, {"text": "We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework; with considerably fewer parameters, our model outperforms more complex existing neural architectures.", "labels": [], "entities": []}, {"text": "In contrast to existing approaches, our approach only relies on alignment and is fully computationally decomposable with respect to the input text.", "labels": [], "entities": []}, {"text": "An overview of our approach is given in.", "labels": [], "entities": []}, {"text": "Given two sentences, where each word is repre-sented by an embedding vector, we first create a soft alignment matrix using neural attention (.", "labels": [], "entities": []}, {"text": "We then use the (soft) alignment to decompose the task into subproblems that are solved separately.", "labels": [], "entities": []}, {"text": "Finally, the results of these subproblems are merged to produce the final classification.", "labels": [], "entities": []}, {"text": "In addition, we optionally apply intra-sentence attention) to endow the model with a richer encoding of substructures prior to the alignment step.", "labels": [], "entities": []}, {"text": "Asymptotically our approach does the same total work as a vanilla LSTM encoder, while being trivially parallelizable across sentence length, which can allow for considerable speedups in low-latency settings.", "labels": [], "entities": []}, {"text": "Empirical results on the SNLI corpus show that our approach achieves state-of-the-art results, while using almost an order of magnitude fewer parameters compared to complex LSTM-based approaches.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.779914379119873}]}], "datasetContent": [{"text": "We evaluate our approach on the Stanford Natural Language Inference (SNLI) dataset.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 32, "end_pos": 82, "type": "DATASET", "confidence": 0.7213149443268776}]}, {"text": "Given a sentences pair (a, b), the task is to predict whether b is entailed by a, b contradicts a, or whether their relationship is neutral.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Train/test accuracies on the SNLI dataset and number of parameters (excluding embeddings) for each approach.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8511325716972351}]}, {"text": " Table 2: Breakdown of accuracy with respect to classes on SNLI", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8543885350227356}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.973446249961853}, {"text": "SNLI", "start_pos": 59, "end_pos": 63, "type": "TASK", "confidence": 0.5887059569358826}]}]}