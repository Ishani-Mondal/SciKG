{"title": [{"text": "Building an Evaluation Scale using Item Response Theory", "labels": [], "entities": []}], "abstractContent": [{"text": "Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (ac-curacy/precision/recall/F1).", "labels": [], "entities": [{"text": "ac-curacy", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9540337324142456}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.724331796169281}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.7450572848320007}, {"text": "F1", "start_pos": 153, "end_pos": 155, "type": "METRIC", "confidence": 0.8495511412620544}]}, {"text": "The current assumption is that all items in a given test set are equal with regards to difficulty and discriminating power.", "labels": [], "entities": [{"text": "difficulty", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9573104381561279}]}, {"text": "We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation.", "labels": [], "entities": [{"text": "Item Response Theory (IRT)", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.7869119246800741}, {"text": "NLP system evaluation", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.7410314281781515}]}, {"text": "IRT is able to describe characteristics of individual items-their difficulty and discriminating power-and can account for these characteristics in its estimation of human intelligence or ability for an NLP task.", "labels": [], "entities": [{"text": "IRT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.695572555065155}]}, {"text": "In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment.", "labels": [], "entities": [{"text": "IRT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9940659999847412}, {"text": "Recognizing Textual Entailment", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.8995938102404276}]}, {"text": "By collecting a large number of human responses and fitting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics.", "labels": [], "entities": []}, {"text": "We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.9795839786529541}, {"text": "IRT score", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9756326973438263}]}], "introductionContent": [{"text": "Advances in artificial intelligence have made it possible to compare computer performance directly with human intelligence).", "labels": [], "entities": []}, {"text": "In most cases, a common approach to evaluating the performance Data and code will be made available for download at https://people.cs.umass.edu/lalor/irt.html of anew system is to compare it against an unseen gold-standard test dataset (GS items).", "labels": [], "entities": []}, {"text": "Accuracy, recall, precision and F1 scores are commonly used to evaluate NLP applications.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9955306649208069}, {"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9995043277740479}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9995487332344055}, {"text": "F1 scores", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9814131855964661}]}, {"text": "These metrics assume that GS items have equal weight for evaluating performance.", "labels": [], "entities": []}, {"text": "However, individual items are different: some maybe so hard that most/all NLP systems answer incorrectly; others maybe so easy that every NLP system answers correctly.", "labels": [], "entities": []}, {"text": "Neither item type provides meaningful information about the performance of an NLP system.", "labels": [], "entities": []}, {"text": "Items that are answered incorrectly by some systems and correctly by others are useful for differentiating systems according to their individual characteristics.", "labels": [], "entities": []}, {"text": "In this paper we introduce Item Response Theory (IRT) from psychometrics and demonstrate its application to evaluating NLP systems.", "labels": [], "entities": [{"text": "Item Response Theory (IRT)", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7696345448493958}]}, {"text": "IRT is a theory of evaluation for characterizing test items and estimating human ability from their performance on such tests.", "labels": [], "entities": [{"text": "IRT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9207538366317749}]}, {"text": "IRT assumes that individual test questions (referred to as \"items\" in IRT) have unique characteristics such as difficulty and discriminating power.", "labels": [], "entities": [{"text": "IRT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9487981200218201}]}, {"text": "These characteristics can be identified by fitting a joint model of human ability and item characteristics to human response patterns to the test items.", "labels": [], "entities": []}, {"text": "Items that do not fit the model are removed and the remaining items can be considered a scale to evaluate performance.", "labels": [], "entities": []}, {"text": "IRT assumes that the probability of a correct answer is associated with both item characteristics and individual ability, and therefore a collection of items of varying characteristics can determine an individual's overall ability.", "labels": [], "entities": [{"text": "IRT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9390166997909546}]}, {"text": "Our aim is to build an intelligent evaluation metric to measure performance for NLP tasks.", "labels": [], "entities": []}, {"text": "With IRT we can identify an appropriate set of items to measure ability in relation to the overall human population as scored by an IRT model.", "labels": [], "entities": []}, {"text": "This process serves two purposes: (i) to identify individual items appropriate fora test set that measures ability on a particular task, and (ii) to use the resulting set of items as an evaluation set in its own right, to measure the ability of future subjects (or NLP models) for the same task.", "labels": [], "entities": []}, {"text": "These evaluation sets can measure the ability of an NLP system with a small number of items, leaving a larger percentage of a dataset for training.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: First, we introduce IRT and describe its benefits and methodology.", "labels": [], "entities": [{"text": "IRT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9853922724723816}]}, {"text": "Second, we apply IRT to Recognizing Textual Entailment (RTE) and show that evaluation sets consisting of a small number of sampled items can provide meaningful information about the RTE task.", "labels": [], "entities": [{"text": "IRT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9662306904792786}, {"text": "Recognizing Textual Entailment (RTE)", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.7462413460016251}, {"text": "RTE task", "start_pos": 182, "end_pos": 190, "type": "TASK", "confidence": 0.8702048361301422}]}, {"text": "Our IRT analyses show that different items exhibit varying degrees of difficulty and discrimination power and that high accuracy does not always translate to high scores in relation to human performance.", "labels": [], "entities": [{"text": "IRT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8888698816299438}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9959220886230469}]}, {"text": "By incorporating IRT, we can learn more about dataset items and move past treating each test case as equal.", "labels": [], "entities": [{"text": "IRT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8952955007553101}]}, {"text": "Using IRT as an evaluation metric allows us to compare NLP systems directly to the performance of humans.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Summary statistics from the AMT HITs.", "labels": [], "entities": [{"text": "AMT HITs", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8208020627498627}]}, {"text": " Table 3: Comparison of Fleiss' \u03ba scores with scores from SNLI", "labels": [], "entities": [{"text": "Fleiss' \u03ba scores", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.9618894259134928}, {"text": "SNLI", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.7132259011268616}]}, {"text": " Table 4: Parameter estimates of the retained items", "labels": [], "entities": [{"text": "Parameter", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9727557897567749}]}, {"text": " Table 5: Theta scores and area under curve percentiles for", "labels": [], "entities": [{"text": "Theta", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9262009859085083}, {"text": "area", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.958791971206665}]}]}