{"title": [], "abstractContent": [{"text": "This paper focuses on the study of recognizing discontiguous entities.", "labels": [], "entities": [{"text": "recognizing discontiguous entities", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.8463940819104513}]}, {"text": "Motivated by a previous work, we propose to use a novel hyper-graph representation to jointly encode discon-tiguous entities of unbounded length, which can overlap with one another.", "labels": [], "entities": []}, {"text": "To compare with existing approaches, we first formally introduce the notion of model ambiguity, which defines the difficulty level of interpreting the outputs of a model, and then formally analyze the theoretical advantages of our model over previous existing approaches based on linear-chain CRFs.", "labels": [], "entities": []}, {"text": "Our empirical results also show that our model is able to achieve significantly better results when evaluated on standard data with many discontiguous entities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building effective automatic named entity recognition (NER) systems that is capable of extracting useful semantic shallow information from texts has been one of the most important tasks in the field of natural language processing.", "labels": [], "entities": [{"text": "automatic named entity recognition (NER)", "start_pos": 19, "end_pos": 59, "type": "TASK", "confidence": 0.7934148992810931}, {"text": "natural language processing", "start_pos": 202, "end_pos": 229, "type": "TASK", "confidence": 0.6513436436653137}]}, {"text": "An effective NER system can typically play an important role in certain downstream NLP tasks such as relation extraction, event extraction, and knowledge base construction (; Al-Rfou and Skiena, 2012).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8724498152732849}, {"text": "event extraction", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7731803953647614}, {"text": "knowledge base construction", "start_pos": 144, "end_pos": 171, "type": "TASK", "confidence": 0.6301999588807424}]}, {"text": "Most traditional NER systems are capable of extracting entities 1 as short spans of texts.", "labels": [], "entities": []}, {"text": "Two basic assumptions are typically made when extract-  ing entities: 1) entities do not overlap with one another, and 2) each entity consists of a contiguous sequence of words.", "labels": [], "entities": []}, {"text": "These assumptions allow the task to be modeled as a sequence labeling task, for which many existing models are readily available, such as linear-chain CRFs.", "labels": [], "entities": [{"text": "sequence labeling task", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7255222797393799}]}, {"text": "While the above two assumptions are valid for most cases, they are not always true.", "labels": [], "entities": []}, {"text": "For example, in the entity University of New Hampshire of type ORG there exists another entity New Hampshire of type LOC.", "labels": [], "entities": []}, {"text": "This violates the first assumption above, yet it is crucial to extract both entities for subsequent tasks such as relation extraction and knowledge base construction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.9037240147590637}, {"text": "knowledge base construction", "start_pos": 138, "end_pos": 165, "type": "TASK", "confidence": 0.6644026239713033}]}, {"text": "Researchers therefore have proposed to tackle the above issues in NER using more sophisticated models.", "labels": [], "entities": [{"text": "NER", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9834012985229492}]}, {"text": "Such efforts still largely rely on the second assumption.", "labels": [], "entities": []}, {"text": "Unfortunately, the second assumption is also not always true in practice.", "labels": [], "entities": []}, {"text": "There are also cases where the entities are composed of multiple discontiguous sequences of words, such as in disorder mention recognition in clinical texts (, where the entities (disorder mentions in this case) maybe discontiguous.", "labels": [], "entities": [{"text": "disorder mention recognition in clinical texts", "start_pos": 110, "end_pos": 156, "type": "TASK", "confidence": 0.7768422762552897}]}, {"text": "Consider the example shown in.", "labels": [], "entities": []}, {"text": "In this example there are four enti-ties, the first one, hiatal hernia, is a conventional contiguous entity.", "labels": [], "entities": []}, {"text": "The second one, laceration ...", "labels": [], "entities": []}, {"text": "esophagus, is a discontiguous entity, consisting of two parts.", "labels": [], "entities": []}, {"text": "The third and fourth ones, blood in stomach and stomach ...", "labels": [], "entities": []}, {"text": "lac (for stomach laceration), are overlapping with each other, with the fourth being discontiguous at the same time.", "labels": [], "entities": [{"text": "stomach laceration)", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7755989531675974}]}, {"text": "For such discontiguous entities which can potentially overlap with other entities in complex manners, existing approaches such as those based on simple sequence tagging models have difficulties handling them accurately.", "labels": [], "entities": []}, {"text": "This stems from the fact that there is a very large number of possible entity combinations in a sentence when the entities can be discontiguous and overlapping.", "labels": [], "entities": []}, {"text": "Motivated by this, in this paper we propose a novel model that can better represent both contiguous and discontiguous entities which can overlap with one another.", "labels": [], "entities": []}, {"text": "Our major contributions can be summarized as follows: \u2022 We propose a novel model that is able to represent both contiguous and discontiguous entities.", "labels": [], "entities": []}, {"text": "\u2022 Theoretically, we introduce the notion of model ambiguity for quantifying the ambiguity of different NER models that can handle discontiguous entities.", "labels": [], "entities": []}, {"text": "We present a study and make comparisons about different models' ambiguity under this theoretical framework.", "labels": [], "entities": []}, {"text": "\u2022 Empirically, we demonstrate that our model can significantly outperform conventional approaches designed for handling discontiguous entities on data which contains many discontiguous entities.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the three models on the SMALL dataset and the LARGE dataset.", "labels": [], "entities": [{"text": "SMALL dataset", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8683891296386719}, {"text": "LARGE dataset", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9799478054046631}]}, {"text": "Note that in both the SMALL and LARGE dataset, about half of all mentions are discontiguous, both in training and test set.", "labels": [], "entities": [{"text": "SMALL", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7108986377716064}, {"text": "LARGE dataset", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.837075263261795}]}, {"text": "We also want to see whether training on a set where the majority of the mentions are contiguous will affect the performance on recognizing discontiguous mentions.", "labels": [], "entities": []}, {"text": "So we also performed another experiment where we trained each model on the original training set where the majority of the entities are contiguous.", "labels": [], "entities": []}, {"text": "We refer to this original dataset as \"Train-Orig\" (it contains 10,405 sentences, including those with no entities) and the: Results on the two datasets and two different training data after optimizing regularization hyperparameter \u03bb in development set.", "labels": [], "entities": []}, {"text": "The -ENH and -ALL suffixes refer to the ENOUGH and ALL heuristics.", "labels": [], "entities": []}, {"text": "The best result in each column is put in boldface.", "labels": [], "entities": []}, {"text": "earlier one as \"Train-Disc\".", "labels": [], "entities": [{"text": "Train-Disc\"", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.8974751532077789}]}, {"text": "First we trained each model on the training set, varying the regularization hyperparameter \u03bb, 6 then the \u03bb with best result in the development set using the respective ENOUGH heuristic for each model is chosen for final result in the test set.", "labels": [], "entities": []}, {"text": "For each experiment setting, we show precision (P), recall (R) and F1 measure.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9465374946594238}, {"text": "recall (R)", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9607737064361572}, {"text": "F1 measure", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9897216558456421}]}, {"text": "Precision is the percentage of the mentions predicted by the model which are correct, recall is the percentage of mentions in the dataset correctly discovered by the model, and F1 measure is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9936345815658569}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9993496537208557}, {"text": "F1 measure", "start_pos": 177, "end_pos": 187, "type": "METRIC", "confidence": 0.9932773411273956}, {"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9991322159767151}, {"text": "recall", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.9921466708183289}]}, {"text": "To seethe ambiguity of each model empirically, we run the decoding process for each model given the gold output structure, which is the true label sequence for the linear-chain model and the true mention-encoded hypergraph for our models.", "labels": [], "entities": []}, {"text": "We used the entities from the training and development sets for this experiment, and we compare the \"Original\" datasets with the \"Discontiguous\" subset to see that the ambiguity is more pronounced when there are more discontiguous entities.", "labels": [], "entities": []}, {"text": "Then we show the precision and recall errors (defined as 1 \u2212 P and 1 \u2212 R, respectively) in.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9996023774147034}, {"text": "recall errors", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9872601330280304}]}, {"text": "Since the ALL heuristics generates all possible mentions from the given encoding, theoretically it should give perfect recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9956574440002441}]}, {"text": "However, due to errors in the training data, there are mentions which can-: Results on the LARGE dataset when entities are split into three types: A, B, and N.", "labels": [], "entities": [{"text": "LARGE dataset", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9041752219200134}]}, {"text": "#Ent is the number of entities not be properly encoded in the models . Removing these errors results in perfect recall (0% recall error).", "labels": [], "entities": [{"text": "Ent", "start_pos": 1, "end_pos": 4, "type": "METRIC", "confidence": 0.9771256446838379}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.8601176738739014}, {"text": "recall error", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.9837195873260498}]}, {"text": "This means that all models are complete: they can encode any mention combinations.", "labels": [], "entities": []}, {"text": "We see however, a very huge difference on the precision error between the linear-chain model and our models, even more when most of the entities are discontiguous.", "labels": [], "entities": [{"text": "precision error", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.978626161813736}]}, {"text": "For the discontiguous subset with the ALL heuristic, the linear-chain model produced 5,463 entities, while the SHARED and SPLIT model produced 2,020 and 2,006 entities, respectively.", "labels": [], "entities": [{"text": "SHARED", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.7739438414573669}]}, {"text": "The total number of gold entities is 1,991.", "labels": [], "entities": []}, {"text": "This means one encoding in the linear-chain model produces much more distinct mention combinations compared to our model, which again shows that the linearchain model has more ambiguity.", "labels": [], "entities": []}, {"text": "Similarly, we can deduce that the SHARED model has slightly more ambiguity compared to the SPLIT model.", "labels": [], "entities": [{"text": "SHARED", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.5564676523208618}]}, {"text": "This confirms our theoretical result presented previously.", "labels": [], "entities": []}, {"text": "It is also worth noting that in the ENOUGH heuristic our models have smaller errors compared to the linear-chain model, showing that when both models can predict the true output structure (the correct There are 19 errors in the original dataset, and 6 in the discontiguous subset, which include duplicate mentions and mentions with incorrect boundaries label sequence for the baseline model and mentionencoded hypergraph for our models), it is easier in our models to get the desired mention combinations.", "labels": [], "entities": [{"text": "ENOUGH heuristic", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.80471470952034}]}, {"text": "We used the LARGE dataset with the multiple-type entities for this experiment.", "labels": [], "entities": [{"text": "LARGE dataset", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.9288347363471985}]}, {"text": "We ran our two models and the linear-chain CRF model with the ENOUGH heuristic on this multi-type dataset, in the same setting as Train-Orig in previous experiments, and the result is shown in.", "labels": [], "entities": [{"text": "ENOUGH", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9749526977539062}]}, {"text": "We used the best lambda from the main experiment for this experiment.", "labels": [], "entities": []}, {"text": "There is a performance drop compared to the LARGE-Train-Orig results in, which is expected since the presence of multiple types make the task harder.", "labels": [], "entities": []}, {"text": "But in general we still see that our models are still better than the baseline, especially the SPLIT model, which shows that in the presence of multiple types, our models can still work better than the baseline model.", "labels": [], "entities": [{"text": "SPLIT", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.7280163764953613}]}], "tableCaptions": [{"text": " Table 1: The statistics of the data. Tr80 and Tr20 refers to the", "labels": [], "entities": [{"text": "Tr80", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.7807278037071228}, {"text": "Tr20", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.6886809468269348}]}, {"text": " Table 2: Results on the two datasets and two different training data after optimizing regularization hyperparameter \u03bb in development", "labels": [], "entities": []}, {"text": " Table 3: Precision and recall errors (%) of each model in the", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988524913787842}, {"text": "recall errors", "start_pos": 24, "end_pos": 37, "type": "METRIC", "confidence": 0.961372584104538}]}, {"text": " Table 4: Results on the LARGE dataset when entities are split", "labels": [], "entities": [{"text": "LARGE dataset", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8519313335418701}]}]}