{"title": [{"text": "Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling", "labels": [], "entities": [{"text": "Interpolative Smoothing", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.6354331970214844}]}], "abstractContent": [{"text": "In this work we present a generalisation of the Modified Kneser-Ney interpolative smoothing for richer smoothing via additional discount parameters.", "labels": [], "entities": []}, {"text": "We provide mathematical underpinning for the estimator of the new discount parameters, and showcase the utility of our rich MKN language models on several Euro-pean languages.", "labels": [], "entities": []}, {"text": "We further explore the in-terdependency among the training data size, language model order, and number of discount parameters.", "labels": [], "entities": []}, {"text": "Our empirical results illustrate that larger number of discount parameters , i) allows for better allocation of mass in the smoothing process, particularly on small data regime where statistical sparsity is severe , and ii) leads to significant reduction in perplexity, particularly for out-of-domain test sets which introduce higher ratio of out-of-vocabulary words.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic language models (LMs) are the core of many natural language processing tasks, such as machine translation and automatic speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7973698973655701}, {"text": "automatic speech recognition", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.6375032862027487}]}, {"text": "m-gram models, the cornerstone of language modeling, decompose the probability of an utterance into conditional probabilities of words given a fixed-length context.", "labels": [], "entities": []}, {"text": "Due to sparsity of the events in natural language, smoothing techniques are critical for generalisation beyond the training text when estimating the parameters of m-gram LMs.", "labels": [], "entities": []}, {"text": "This is particularly important when the training text is small, e.g. building language models for translation or speech recognition in low-resource languages.", "labels": [], "entities": [{"text": "translation or speech recognition", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.6666994094848633}]}, {"text": "A widely used and successful smoothing method is interpolated Modified Kneser-Ney (MKN).", "labels": [], "entities": [{"text": "smoothing", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.9649664759635925}]}, {"text": "This method uses a linear interpolation of higher and lower order m-gram probabilities by preserving probability mass via absolute discounting.", "labels": [], "entities": []}, {"text": "In this paper, we extend MKN by introducing additional discount parameters, leading to a richer smoothing scheme.", "labels": [], "entities": [{"text": "MKN", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.5549786686897278}]}, {"text": "This is particularly important when statistical sparsity is more severe, i.e., in building high-order LMs on small data, or when out-of-domain test sets are used.", "labels": [], "entities": []}, {"text": "Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more compact) using advanced data structures such as succinct suffix trees.", "labels": [], "entities": [{"text": "MKN language modeling", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.845097005367279}]}, {"text": "An exception is Hierarchical Pitman-Yor Process LMs) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an approximate inference method.", "labels": [], "entities": []}, {"text": "Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference.", "labels": [], "entities": [{"text": "MKN smoothing realising", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.9044898947079977}]}, {"text": "We provide estimators for our additional discount parameters by extending the discount bounds in MKN.", "labels": [], "entities": [{"text": "MKN", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.8589968681335449}]}, {"text": "We empirically analyze our enriched MKN LMs on several European languages in in-and outof-domain settings.", "labels": [], "entities": [{"text": "MKN LMs", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.8163750171661377}]}, {"text": "The results show that our discounting mechanism significantly improves the perplexity compared to MKN and offers a more elegant way of dealing with out-of-vocabulary (OOV) words and domain mismatch.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the effect of using different numbers of discount parameters on perplexity using the Finnish (FI), Spanish (ES), German (DE), English (EN) portions of the Europarl v7 (    of news-test 2015 (all denoted as NT) , and ii) extreme using a 24 hour period of streamed Finnish, and Spanish tweets 6 (denoted as TW), and the German and English sections of the patent description of medical translation task 7 (denoted as MED).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 166, "end_pos": 174, "type": "DATASET", "confidence": 0.9616566896438599}, {"text": "medical translation task 7", "start_pos": 386, "end_pos": 412, "type": "TASK", "confidence": 0.7878488376736641}, {"text": "MED", "start_pos": 425, "end_pos": 428, "type": "METRIC", "confidence": 0.8791307210922241}]}, {"text": "See for statistics of the training and test sets.", "labels": [], "entities": []}, {"text": "). This effect is consistent across the Europarl corpora, and for all LM orders.", "labels": [], "entities": [{"text": "Europarl corpora", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.987869918346405}]}, {"text": "We observe a substantial improvements even form = 10-gram models (see).", "labels": [], "entities": []}, {"text": "On the medical test set which has 9 times higher OOV ratio, the perplexity reduction shows a similar trend.", "labels": [], "entities": [{"text": "medical test set", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.7347569366296133}, {"text": "OOV ratio", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9891256988048553}, {"text": "perplexity reduction", "start_pos": 64, "end_pos": 84, "type": "METRIC", "confidence": 0.9663285911083221}]}, {"text": "However, these reductions vanish when an in-domain test set is used.", "labels": [], "entities": []}, {"text": "Note that we use the same treatment of OOV words for computing the perplexities which is used in KenLM (Heafield, 2013).", "labels": [], "entities": [{"text": "KenLM (Heafield, 2013)", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.7543982764085134}]}], "tableCaptions": [{"text": " Table 1: Perplexity for various m-gram orders m \u2208 2, 3, 10 and training languages from Europarl, using different  numbers of discount parameters for MKN. MKN (D [1...3] ), MKN (D [1...4] ), MKN (D [1...10] ) represent vanilla MKN,  MKN with 1 more discounts, and MKN with 7 more discount parameters, respectively. Test sets sources EU, NT,  TW, MED are Europarl, news-test, Twitter, and medical patent descriptions, respectively. OOV is reported as the ratio", "labels": [], "entities": [{"text": "Europarl", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9698262214660645}, {"text": "EU", "start_pos": 333, "end_pos": 335, "type": "DATASET", "confidence": 0.5138850212097168}, {"text": "NT", "start_pos": 337, "end_pos": 339, "type": "DATASET", "confidence": 0.7198991775512695}, {"text": "MED", "start_pos": 346, "end_pos": 349, "type": "METRIC", "confidence": 0.9018182754516602}, {"text": "Europarl", "start_pos": 354, "end_pos": 362, "type": "DATASET", "confidence": 0.9806477427482605}, {"text": "OOV", "start_pos": 431, "end_pos": 434, "type": "METRIC", "confidence": 0.997994065284729}]}]}