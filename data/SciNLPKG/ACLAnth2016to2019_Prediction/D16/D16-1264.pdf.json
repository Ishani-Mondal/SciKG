{"title": [{"text": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8158295154571533}]}], "abstractContent": [{"text": "We present the Stanford Question Answering Dataset (SQuAD), anew reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 15, "end_pos": 58, "type": "DATASET", "confidence": 0.8009550401142665}]}, {"text": "We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees.", "labels": [], "entities": []}, {"text": "We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple base-line (20%).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9910938739776611}]}, {"text": "However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.", "labels": [], "entities": []}, {"text": "The dataset is freely available at https://stanford-qa.com.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading Comprehension (RC), or the ability to read text and then answer questions about it, is a challenging task for machines, requiring both understanding of natural language and knowledge about the world.", "labels": [], "entities": [{"text": "Reading Comprehension (RC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8231800198554993}]}, {"text": "Consider the question \"what causes precipitation to fall?\" posed on the passage in.", "labels": [], "entities": []}, {"text": "In order to answer the question, one might first locate the relevant part of the passage \"precipitation ...", "labels": [], "entities": []}, {"text": "falls under gravity\", then reason that \"under\" refers to a cause (not location), and thus determine the correct answer: \"gravity\".", "labels": [], "entities": []}, {"text": "How can we get a machine to make progress on the challenging task of reading comprehension?", "labels": [], "entities": []}, {"text": "Historically, large, realistic datasets have played In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity.", "labels": [], "entities": []}, {"text": "The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail...", "labels": [], "entities": []}, {"text": "Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud.", "labels": [], "entities": []}, {"text": "Short, intense periods of rain in scattered locations are called \"showers\".", "labels": [], "entities": []}], "datasetContent": [{"text": "We begin with a survey of existing reading comprehension and question answering (QA) datasets, highlighting a variety of task formulation and creation strategies (see for an overview).", "labels": [], "entities": [{"text": "reading comprehension and question answering (QA)", "start_pos": 35, "end_pos": 84, "type": "TASK", "confidence": 0.7721538618206978}]}, {"text": "A data-driven approach to reading comprehension goes back to, who curated a dataset of 600 real 3rd-6th grade reading comprehension questions.", "labels": [], "entities": []}, {"text": "Their pattern matching baseline was subsequently improved by a rule-based system) and a logistic regression model ().", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.7579552233219147}]}, {"text": "More recently, curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question.", "labels": [], "entities": []}, {"text": "Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress.", "labels": [], "entities": []}, {"text": "Both curated datasets, although real and difficult, are too small to support very expressive statistical models.", "labels": [], "entities": []}, {"text": "Some datasets focus on deeper reasoning abilities.", "labels": [], "entities": []}, {"text": "Algebra word problems require understanding a story well enough to turn it into a system of equa-tions, which can be easily solved to produce the answer ().", "labels": [], "entities": []}, {"text": "BAbI ( ), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task.", "labels": [], "entities": [{"text": "BAbI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5156503319740295}]}, {"text": "describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge.", "labels": [], "entities": [{"text": "solving 4th grade science exams", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.8438823699951172}]}, {"text": "The goal of open-domain QA is to answer a question from a large collection of documents.", "labels": [], "entities": []}, {"text": "The annual evaluations at the Text REtreival Conference (TREC)) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy!", "labels": [], "entities": [{"text": "Text REtreival Conference (TREC))", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.5188892632722855}, {"text": "Jeopardy!", "start_pos": 146, "end_pos": 155, "type": "TASK", "confidence": 0.5888603329658508}]}, {"text": "(. Recently, created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a specific span in the sentence.", "labels": [], "entities": [{"text": "WikiQA dataset", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9565317332744598}, {"text": "sentence selection", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7077460139989853}]}, {"text": "Selecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns), using dependency trees), and using a factor graph over multiple sentences (.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.8364124894142151}]}, {"text": "One key difference between our RC setting and answer extraction is that answer extraction typically exploits the fact that the answer occurs in multiple documents (), which is more lenient than in our setting, where a system only has access to a single reading passage.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8952307999134064}, {"text": "answer extraction", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8650692999362946}]}, {"text": "Recently, researchers have constructed cloze datasets, in which the goal is to predict the missing word (often a named entity) in a passage.", "labels": [], "entities": []}, {"text": "Since these datasets can be automatically generated from naturally occurring data, they can be extremely large.", "labels": [], "entities": []}, {"text": "The Children's Book Test (CBT) (, for example, involves predicting a blanked-out word of a sentence given the 20 previous sentences.", "labels": [], "entities": [{"text": "Children's Book Test (CBT)", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.7586031470979963}]}, {"text": "constructed a corpus of cloze style questions by blanking out entities in abstractive summaries of CNN / Daily News articles; the goal is to fill in the entity based on the original article.", "labels": [], "entities": [{"text": "CNN / Daily News articles", "start_pos": 99, "end_pos": 124, "type": "DATASET", "confidence": 0.8586550951004028}]}, {"text": "While the size of this dataset is impressive, showed that the dataset requires less reasoning than previously thought, and concluded that performance is almost saturated.", "labels": [], "entities": []}, {"text": "One difference between SQuAD questions and cloze-style queries is that answers to cloze queries are single words or entities, while answers in SQuAD often include non-entities and can be much longer phrases.", "labels": [], "entities": []}, {"text": "Another difference is that SQuAD focuses on questions whose answers are entailed by the passage, whereas the answers to cloze-style queries are merely suggested by the passage.", "labels": [], "entities": []}, {"text": "We collect our dataset in three stages: curating passages, crowdsourcing question-answers on those passages, and obtaining additional answers.", "labels": [], "entities": []}, {"text": "To retrieve high-quality articles, we used Project Nayuki's Wikipedia's internal PageRanks to obtain the top 10000 articles of English Wikipedia, from which we sampled 536 articles uniformly at random.", "labels": [], "entities": []}, {"text": "From each of these articles, we extracted individual paragraphs, stripping away images, figures, tables, and discarding paragraphs shorter than 500 characters.", "labels": [], "entities": []}, {"text": "The result was 23,215 paragraphs for the 536 articles covering a wide range of topics, from musical celebrities to abstract concepts.", "labels": [], "entities": []}, {"text": "We partitioned the articles randomly into a training set (80%), a development set (10%), and a test set (10%).", "labels": [], "entities": []}, {"text": "Next, we employed crowdworkers to create questions.", "labels": [], "entities": []}, {"text": "We used the Daemo platform (, with Amazon Mechanical Turk as its backend.", "labels": [], "entities": [{"text": "Daemo platform", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9629769027233124}]}, {"text": "Crowdworkers were required to have a 97% HIT acceptance rate, a minimum of 1000 HITs, and be located in the United States or Canada.", "labels": [], "entities": [{"text": "HIT acceptance rate", "start_pos": 41, "end_pos": 60, "type": "METRIC", "confidence": 0.7455775539080302}]}, {"text": "Workers were asked to spend 4 minutes on every paragraph, and paid $9 per hour for the number of hours required to complete the article.", "labels": [], "entities": []}, {"text": "The task was reviewed favorably by crowdworkers, receiving positive comments on Turkopticon.", "labels": [], "entities": [{"text": "Turkopticon", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.9626737236976624}]}, {"text": "On each paragraph, crowdworkers were tasked with asking and answering up to 5 questions on the content of that paragraph.", "labels": [], "entities": []}, {"text": "The questions had to be entered in a text field, and the answers had to be highlighted in the paragraph.", "labels": [], "entities": []}, {"text": "To guide the workers, tasks contained a sample paragraph, and examples of good and bad questions and answers on that paragraph along with the reasons they were categorized as such.", "labels": [], "entities": []}, {"text": "Additionally, crowdworkers were encouraged to ask questions in their own words, without copying word phrases from the paragraph.", "labels": [], "entities": []}, {"text": "On the interface, this was reinforced by a reminder prompt at the beginning of every paragraph, and by disabling copy-paste functionality on the paragraph text.", "labels": [], "entities": []}, {"text": "To get an indication of human performance on SQuAD and to make our evaluation more robust, we obtained at least 2 additional answers for each question in the development and test sets.", "labels": [], "entities": []}, {"text": "In the secondary answer generation task, each crowdworker was shown only the questions along with the paragraphs of an article, and asked to select the shortest span in the paragraph that answered the question.", "labels": [], "entities": [{"text": "answer generation task", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7543711960315704}]}, {"text": "If a question was not answerable by a span in the paragraph, workers were asked to submit the question without marking an answer.", "labels": [], "entities": []}, {"text": "Workers were recommended a speed of 5 questions for 2 minutes, and paid at the same rate of $9 per hour for the number of hours required for the entire article.", "labels": [], "entities": [{"text": "speed", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9603333473205566}]}, {"text": "Over the development and test sets, 2.6% of questions were marked unanswerable by at least one of the additional crowdworkers.", "labels": [], "entities": []}, {"text": "To understand the properties of SQuAD, we analyze the questions and answers in the development set.", "labels": [], "entities": []}, {"text": "Specifically, we explore the (i) diversity of answer types, (ii) the difficulty of questions in terms of type of reasoning required to answer them, and (iii) the degree of syntactic divergence between the question and answer sentences.", "labels": [], "entities": []}, {"text": "We automatically categorize the answers as follows: We first separate the numerical and non-numerical answers.", "labels": [], "entities": []}, {"text": "The non-numerical answers are categorized using constituency parses and POS tags generated by Stanford CoreNLP.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.902391254901886}]}, {"text": "The proper noun phrases are further split into person, location and other entities using NER tags.", "labels": [], "entities": []}, {"text": "In, we can see dates and other numbers makeup 19.8% of the data; 32.6% of the answers are proper nouns of three different types; 31.8% are common noun phrases answers; and the remaining 15.8% are made up of adjective phrases, verb phrases, clauses and other types.", "labels": [], "entities": []}, {"text": "Reasoning required to answer questions.", "labels": [], "entities": []}, {"text": "To get a better understanding of the reasoning required to answer the questions, we sampled 4 questions from each of the 48 articles in the development set, and then manually labeled the examples with the categories shown in Major correspondences between the question and the answer sentence are synonyms.", "labels": [], "entities": []}, {"text": "Q: What is the Rankine cycle sometimes called?", "labels": [], "entities": []}, {"text": "Sentence: The Rankine cycle is sometimes referred to as a practical Carnot cycle.", "labels": [], "entities": []}, {"text": "We use two different metrics to evaluate model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9888277053833008}]}, {"text": "Both metrics ignore punctuations and articles (a, an, the).", "labels": [], "entities": []}, {"text": "This metric measures the percentage of predictions that match anyone of the ground truth answers exactly.", "labels": [], "entities": []}, {"text": "(Macro-averaged) F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9617276787757874}]}, {"text": "This metric measures the average overlap between the prediction and ground truth answer.", "labels": [], "entities": []}, {"text": "We treat the prediction and ground truth as bags of tokens, and compute their F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9958305954933167}]}, {"text": "We take the maximum F1 overall of the ground truth answers fora given question, and then average overall of the questions.", "labels": [], "entities": [{"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.998247504234314}]}], "tableCaptions": [{"text": " Table 2: We automatically partition our answers into the fol-", "labels": [], "entities": []}, {"text": " Table 5: Performance of various methods and humans. Logis-", "labels": [], "entities": []}, {"text": " Table 6: Performance with feature ablations. We find that lexi-", "labels": [], "entities": []}, {"text": " Table 7: Performance stratified by answer types. Logistic re-", "labels": [], "entities": []}]}