{"title": [{"text": "Numerically Grounded Language Models for Semantic Error Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction.", "labels": [], "entities": [{"text": "Semantic error detection and correction", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7412764549255371}, {"text": "fact checking", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.8793956637382507}, {"text": "grammatical error correction", "start_pos": 119, "end_pos": 147, "type": "TASK", "confidence": 0.624128927787145}]}, {"text": "Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities.", "labels": [], "entities": []}, {"text": "Our approach uses language models grounded in numbers within the text.", "labels": [], "entities": []}, {"text": "Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases.", "labels": [], "entities": []}, {"text": "Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches.", "labels": [], "entities": [{"text": "F1", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9998156428337097}, {"text": "semantic error correction", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.6143430272738138}]}, {"text": "Conditioning on a knowledge base yields further improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many real world scenarios it is important to detect and potentially correct semantic errors and inconsistencies in text.", "labels": [], "entities": []}, {"text": "For example, when clinicians compose reports, some statements in the text maybe inconsistent with measurements taken from the patient.", "labels": [], "entities": []}, {"text": "Error rates in clinical data range from 2.3% to 26.9% () and many of them are number-based errors ().", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9936209321022034}]}, {"text": "Likewise, a blog writer may make statistical claims that contradict facts recorded in databases.", "labels": [], "entities": []}, {"text": "Numerical concepts constitute 29% of contradictions in Wikipedia and GoogleNews) and 8.8% of contradictory pairs in entailment datasets ().", "labels": [], "entities": []}, {"text": "\"EF\" is a clinical term and stands for \"ejection fraction\".", "labels": [], "entities": [{"text": "EF", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9642020463943481}]}, {"text": "These inconsistencies may stem from oversight, lack of reporting guidelines or negligence.", "labels": [], "entities": []}, {"text": "In fact they may not even be errors at all, but point to interesting outliers or to errors in a reference database.", "labels": [], "entities": []}, {"text": "In all cases, it is important to spot and possibly correct such inconsistencies.", "labels": [], "entities": []}, {"text": "This task is known as semantic error correction (SEC) (.", "labels": [], "entities": [{"text": "semantic error correction (SEC)", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.5877962509791056}]}, {"text": "In this paper, we propose a SEC approach to support clinicians with writing patient reports.", "labels": [], "entities": []}, {"text": "A SEC system reads a patient's structured background information from a knowledge base (KB) and their clinical report.", "labels": [], "entities": []}, {"text": "Then it recommends improvements to the text of the report for semantic consistency.", "labels": [], "entities": []}, {"text": "An example of an inconsistency is shown in.", "labels": [], "entities": []}, {"text": "The SEC system has been trained on a dataset of records and learnt that the phrases \"non dilated\" and \"severely dilated\" correspond to high and low values for \"EF\" (abbreviation for \"ejection fraction\", a clinical measurement), respectively.", "labels": [], "entities": [{"text": "EF", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.9166710376739502}]}, {"text": "If the system is then presented with the phrase \"non dilated\" in the context of a low value, it will detect a semantic inconsistency and correct the text to \"severely dilated\".", "labels": [], "entities": []}, {"text": "Our contributions are: 1) a straightforward extension to recurrent neural network (RNN) language models for grounding them in numbers available in the text; 2) a simple method for modelling text conditioned on an incomplete KB by lexicalising it; 3) our evaluation on a semantic error correction task for clinical records shows that our method achieves F1 improvements of 5 and 6 percentage points with grounding and KB conditioning, respectively, over an ungrounded approach (F1 of 49%).", "labels": [], "entities": [{"text": "semantic error correction task", "start_pos": 270, "end_pos": 300, "type": "TASK", "confidence": 0.7264406085014343}, {"text": "F1", "start_pos": 353, "end_pos": 355, "type": "METRIC", "confidence": 0.9994513392448425}, {"text": "F1", "start_pos": 477, "end_pos": 479, "type": "METRIC", "confidence": 0.9991057515144348}]}], "datasetContent": [{"text": "We report perplexity and adjusted perplexity of our LMs on the test set for all tokens and token classes.", "labels": [], "entities": []}, {"text": "Adjusted perplexity is not sensitive to OOV-rates and thus allows for meaningful comparisons across token classes.", "labels": [], "entities": []}, {"text": "Perplexities are high for numeric tokens because they form a large proportion of the vocabulary.", "labels": [], "entities": [{"text": "Perplexities", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9703648090362549}]}, {"text": "The grounded and g-conditional models achieved a 33.3% and 36.9% improvement in perplexity, respectively, over the base LM model.", "labels": [], "entities": []}, {"text": "Conditioning without grounding yields only slight improvements, because most of the numerical values from the lexicalised KB are out-of-vocabulary.", "labels": [], "entities": []}, {"text": "The qualitative example in demonstrates how numeric values influence the probability of tokens given their history.", "labels": [], "entities": []}, {"text": "We select a document from the development set and substitute its numeric val- ues as we vary EF (the rest are set by solving a known system of equations).", "labels": [], "entities": [{"text": "EF", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9899939894676208}]}, {"text": "The selected exact values were unseen in the training data.", "labels": [], "entities": []}, {"text": "We calculate the probabilities for observing the document with different word choices {\"non\", \"mildly\", \"severely\"} under the grounded LM and find that \"non dilated\" is associated with higher EF values.", "labels": [], "entities": [{"text": "EF", "start_pos": 192, "end_pos": 194, "type": "METRIC", "confidence": 0.9712499380111694}]}, {"text": "This shows that it has captured semantic dependencies on numbers.", "labels": [], "entities": []}, {"text": "We evaluate SEC systems on the corrupted dataset (Section 3) for detection and correction.", "labels": [], "entities": []}, {"text": "For detection, we report precision, recall and F1 scores in.", "labels": [], "entities": [{"text": "detection", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9470148086547852}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9997604489326477}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9995860457420349}, {"text": "F1 scores", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9830521643161774}]}, {"text": "Our g-conditional model achieves the best results, a total F1 improvement of 2 points over the base LM model and 7 points over the best baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9995595812797546}]}, {"text": "The conditional model without grounding performs slightly worse in the F1 metric than the base LM.", "labels": [], "entities": [{"text": "F1", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9973758459091187}]}, {"text": "Note that with more hypotheses the random baseline behaves more similarly to always.", "labels": [], "entities": []}, {"text": "Our hypothesis generator generated on average 12 hypotheses per document.", "labels": [], "entities": []}, {"text": "The results of never are zero as it fails to detect any error.", "labels": [], "entities": [{"text": "never", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9895879030227661}]}, {"text": "For correction, we report mean average precision (MAP) in addition to the same metrics as for detection).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 26, "end_pos": 54, "type": "METRIC", "confidence": 0.9416796863079071}]}, {"text": "The former measures the position of the ranking of the correct hypothesis.", "labels": [], "entities": []}, {"text": "The always (never) baseline ranks the correct hypothesis at the top (bottom).", "labels": [], "entities": []}, {"text": "Again, the g-conditional model  yields the best results, achieving an improvement of 6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.998710036277771}, {"text": "MAP", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9633508920669556}, {"text": "F1", "start_pos": 178, "end_pos": 180, "type": "METRIC", "confidence": 0.9773574471473694}]}, {"text": "The conditional model without grounding has the worst performance among the LM-based models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for clinical dataset. Counts for non-numeric", "labels": [], "entities": []}, {"text": " Table 3: Language modelling evaluation results on the test set.", "labels": [], "entities": [{"text": "Language modelling evaluation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.805380642414093}]}, {"text": " Table 4: Error detection results on the test set. We report preci-", "labels": [], "entities": [{"text": "Error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.817966490983963}, {"text": "preci-", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9184527397155762}]}, {"text": " Table 5: Error correction results on the test set. We report mean", "labels": [], "entities": [{"text": "Error correction", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8950769305229187}]}]}