{"title": [{"text": "Key-Value Memory Networks for Directly Reading Documents", "labels": [], "entities": []}], "abstractContent": [{"text": "Directly reading documents and being able to answer questions from them is an unsolved challenge.", "labels": [], "entities": []}, {"text": "To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8927848100662231}]}, {"text": "Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase.", "labels": [], "entities": []}, {"text": "In this work we introduce anew method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation.", "labels": [], "entities": []}, {"text": "To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WIKIMOVIES, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7284888029098511}]}, {"text": "Our method reduces the gap between all three settings.", "labels": [], "entities": []}, {"text": "It also achieves state-of-the-art results on the existing WIKIQA benchmark.", "labels": [], "entities": [{"text": "WIKIQA benchmark", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.945277214050293}]}], "introductionContent": [{"text": "Question answering (QA) has been along standing research problem in natural language processing, with the first systems attempting to answer questions by directly reading documents).", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9509656667709351}]}, {"text": "The development of large-scale Knowledge Bases (KBs) such as Freebase ( helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9698488712310791}]}, {"text": "Unfortunately, KBs have intrinsic limitations such as their inevitable incompleteness and fixed schemas that cannot support all varieties of answers.", "labels": [], "entities": []}, {"text": "Since information extraction (IE) (), intended to fill in missing information in KBs, is neither accurate nor reliable enough, collections of raw textual resources and documents such as Wikipedia will always contain more information.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 6, "end_pos": 33, "type": "TASK", "confidence": 0.8561527788639068}]}, {"text": "As a result, even if KBs can be satisfactory for closed-domain problems, they are unlikely to scale up to answer general questions on any topic.", "labels": [], "entities": []}, {"text": "Starting from this observation, in this work we study the problem of answering by directly reading documents.", "labels": [], "entities": [{"text": "answering by directly reading documents", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.713828730583191}]}, {"text": "Retrieving answers directly from text is harder than from KBs because information is far less structured, is indirectly and ambiguously expressed, and is usually scattered across multiple documents.", "labels": [], "entities": [{"text": "Retrieving answers directly from text", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8746191620826721}]}, {"text": "This explains why using a satisfactory KB-typically only available in closed domains-is preferred over raw text.", "labels": [], "entities": []}, {"text": "We postulate that before trying to provide answers that are not in KBs, document-based QA systems should first reach KB-based systems' performance in such closed domains, where clear comparison and evaluation is possible.", "labels": [], "entities": []}, {"text": "To this end, this paper introduces WIKIMOVIES, anew analysis tool that allows for measuring the performance of QA systems when the knowledge source is switched from a KB to unstructured documents.", "labels": [], "entities": [{"text": "WIKIMOVIES", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.5973284840583801}]}, {"text": "WIKIMOVIES contains \u223c100k questions in the movie domain, and was designed to be answerable by using either a perfect KB (based on OMDb 1 ), Wikipedia pages or an imper-fect KB obtained through running an engineered IE pipeline on those pages.", "labels": [], "entities": [{"text": "WIKIMOVIES", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9367578029632568}]}, {"text": "To bridge the gap between using a KB and reading documents directly, we still lack appropriate machine learning algorithms.", "labels": [], "entities": []}, {"text": "In this work we propose the Key-Value Memory Network (KV-MemNN), anew neural network architecture that generalizes the original Memory Network ( and can work with either knowledge source.", "labels": [], "entities": []}, {"text": "The KV-MemNN performs QA by first storing facts in a key-value structured memory before reasoning on them in order to predict an answer.", "labels": [], "entities": [{"text": "QA", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.950477123260498}]}, {"text": "The memory is designed so that the model learns to use keys to address relevant memories with respect to the question, whose corresponding values are subsequently returned.", "labels": [], "entities": []}, {"text": "This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values, while still being trained using standard backpropagation via stochastic gradient descent.", "labels": [], "entities": []}, {"text": "Our experiments on WIKIMOVIES indicate that, thanks to its key-value memory, the KV-MemNN consistently outperforms the original Memory Network, and reduces the gap between answering from a human-annotated KB, from an automatically extracted KB or from directly reading Wikipedia.", "labels": [], "entities": [{"text": "WIKIMOVIES", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8950398564338684}]}, {"text": "We confirm our findings on WIKIQA (, another Wikipedia-based QA benchmark where no KB is available, where we demonstrate that KV-MemNN can reach state-of-the-art resultssurpassing the most recent attention-based neural network models.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.9239192008972168}]}], "datasetContent": [{"text": "This section describes our experiments on WIKI-MOVIES and WIKIQA.", "labels": [], "entities": [{"text": "WIKI-MOVIES", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.943575918674469}, {"text": "WIKIQA", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.9477870464324951}]}], "tableCaptions": [{"text": " Table 2: Test results (% hits@1) on WIKIMOVIES, comparing", "labels": [], "entities": [{"text": "WIKIMOVIES", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.9681458473205566}]}, {"text": " Table 3: Development set performance (% hits@1) with differ-", "labels": [], "entities": []}, {"text": " Table 4: Breakdown of test results (% hits@1) on WIKI-", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.957266092300415}, {"text": "WIKI-", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9716002643108368}]}, {"text": " Table 5: Analysis of test set results (% hits@1) for KB vs.", "labels": [], "entities": []}, {"text": " Table 6: Test results on WikiQA.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.9497025609016418}]}]}