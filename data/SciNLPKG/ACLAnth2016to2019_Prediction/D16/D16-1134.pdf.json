{"title": [{"text": "HUME: Human UCCA-Based Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Human UCCA-Based Evaluation of Machine Translation", "start_pos": 6, "end_pos": 56, "type": "TASK", "confidence": 0.7544885327418646}]}], "abstractContent": [{"text": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.6883530169725418}]}, {"text": "However, these provide no insight into possible errors, and do not scale well with sentence length.", "labels": [], "entities": []}, {"text": "We argue fora semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality , and enabling the construction and tuning of semantics-based MT.", "labels": [], "entities": [{"text": "MT output", "start_pos": 101, "end_pos": 110, "type": "TASK", "confidence": 0.8828555643558502}]}, {"text": "We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme.", "labels": [], "entities": []}, {"text": "HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output.", "labels": [], "entities": []}, {"text": "We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human judgement should be the ultimate test of the quality of an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9931142926216125}]}, {"text": "Nevertheless, common measures for human MT evaluation, such as adequacy and fluency judgements or the relative ranking of possible translations, are problematic in two ways.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9519574642181396}]}, {"text": "First, as the quality of translation is multifaceted, it is difficult to quantify the quality of the entire sentence in a single number.", "labels": [], "entities": []}, {"text": "This is indeed * * All authors contributed equally to this reflected in the diminishing inter-annotator agreement (IAA) rates of human ranking measures with the sentence length).", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA) rates", "start_pos": 88, "end_pos": 125, "type": "METRIC", "confidence": 0.8681232631206512}]}, {"text": "Second, a sentence-level quality score does not indicate what parts of the sentence are badly translated, and so cannot inform developers in repairing these errors.", "labels": [], "entities": []}, {"text": "These problems are partially addressed by measures that decompose over parts of the evaluated translation, often words or n-grams (see \u00a72 fora brief survey of previous work).", "labels": [], "entities": []}, {"text": "A promising line of research decomposes metrics over semantically defined units, quantifying the similarity of the output and the reference in terms of their verb argument structure; the most notable of these measures is HMEANT (.", "labels": [], "entities": []}, {"text": "We propose the HUME metric, a human evaluation measure that decomposes over UCCA semantic units.", "labels": [], "entities": []}, {"text": "UCCA) is an appealing candidate for semantic analysis, due to its cross-linguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their inter-relations.", "labels": [], "entities": [{"text": "UCCA)", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9351656436920166}, {"text": "semantic analysis", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.9352113306522369}]}, {"text": "HUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence.", "labels": [], "entities": []}, {"text": "We are thus avoiding the semantic annotation of machine-generated text, which is often garbled or semantically unclear.", "labels": [], "entities": []}, {"text": "This also allows the re-use of the source semantic annotation for measuring the quality of different translations of the same source sentence and avoids relying on reference translations, which have been shown to bias annotators (.", "labels": [], "entities": []}, {"text": "After a brief review ( \u00a72), we describe HUME in detail ( \u00a73).", "labels": [], "entities": [{"text": "HUME", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.6823717355728149}]}, {"text": "Our experiments with four language pairs: English to Czech, German, Polish and Romanian ( \u00a74) document HUME's inter-annotator agreement and efficiency (time of annotation).", "labels": [], "entities": []}, {"text": "We further empirically compare HUME with direct assessment of human adequacy ratings ( \u00a75), and conclude by discussing the differences with HMEANT ( \u00a76).", "labels": [], "entities": [{"text": "HUME", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.544523298740387}, {"text": "HMEANT", "start_pos": 140, "end_pos": 146, "type": "DATASET", "confidence": 0.5637142658233643}]}], "datasetContent": [{"text": "In order to validate the HUME metric, we ran an annotation experiment with one source language (English), and four target languages (Czech, German, Polish and Romanian), using text from the public health domain.", "labels": [], "entities": [{"text": "HUME", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.5032472014427185}]}, {"text": "Semantically accurate translation is paramount in this domain, which makes it particularly suitable for semantic MT evaluation.", "labels": [], "entities": [{"text": "Semantically accurate translation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5232550799846649}, {"text": "MT evaluation", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.8924573659896851}]}, {"text": "HUME is evaluated in terms of its consistency (inter-annotator The interleaving words are \"...", "labels": [], "entities": [{"text": "consistency", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9753451347351074}]}, {"text": "und beide berichtet berichteten ...\"", "labels": [], "entities": []}, {"text": "\"... and both report reported ...\"), which doesn't form any coherent relation with the rest of the sentence.", "labels": [], "entities": []}, {"text": "agreement), efficiency (time of annotation) and validity (by comparing it with crowd-sourced adequacy judgements).", "labels": [], "entities": [{"text": "validity", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.99846351146698}]}, {"text": "For each of the four language pairs under consideration we built phrase-based MT systems using Moses ().", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.8781749606132507}]}, {"text": "These were trained on large parallel data sets extracted from OPUS (, and the data sets released for the WMT14 medical translation task (, giving between 45 and 85 million sentences of training data, depending on the language pair.", "labels": [], "entities": [{"text": "OPUS", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9142301082611084}, {"text": "WMT14 medical translation task", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.7840330749750137}]}, {"text": "These translation systems were used to translate texts derived from both NHS 24 6 and Cochrane 7 into the four languages.", "labels": [], "entities": [{"text": "NHS 24 6", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9874918063481649}, {"text": "Cochrane 7", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.9034345746040344}]}, {"text": "NHS 24 is a public body providing healthcare and health-service related information in Scotland; Cochrane is an international NGO which provides independent systematic reviews on health-related research.", "labels": [], "entities": [{"text": "NHS 24", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9453825652599335}, {"text": "Scotland", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.8806671500205994}]}, {"text": "NHS 24 texts come from the \"Health A-Z\" section in the NHS Inform website, and Cochrane texts come from their plain language summaries and abstracts.", "labels": [], "entities": [{"text": "NHS 24 texts", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9714395801226298}, {"text": "NHS Inform website", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.9556690653165182}]}], "tableCaptions": [{"text": " Table 1: HUME-annotated #sentences and #units.", "labels": [], "entities": []}, {"text": " Table 2: Median annotation times per sentence, in sec- onds.  *  : no timing information is available, as this was a  collection of annotators, working in parallel.", "labels": [], "entities": [{"text": "timing", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9728879928588867}]}]}