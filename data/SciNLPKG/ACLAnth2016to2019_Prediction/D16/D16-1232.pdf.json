{"title": [{"text": "Nonparametric Bayesian Models for Spoken Language Understanding", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose anew generative approach for semantic slot filling task in spoken language understanding using a nonparamet-ric Bayesian formalism.", "labels": [], "entities": [{"text": "semantic slot filling task", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.7380436584353447}, {"text": "spoken language understanding", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.70294322570165}]}, {"text": "Slot filling is typically formulated as a sequential labeling problem , which does not directly deal with the posterior distribution of possible slot values.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9659261405467987}]}, {"text": "We present a nonparametric Bayesian model involving the generation of arbitrary natural language phrases, which allows an explicit calculation of the distribution over an infinite set of slot values.", "labels": [], "entities": []}, {"text": "We demonstrate that this approach significantly improves slot estimation accuracy compared to the existing sequential labeling algorithm.", "labels": [], "entities": [{"text": "slot estimation", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7936011850833893}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.92360520362854}]}], "introductionContent": [{"text": "Spoken language understanding (SLU) refers to the challenge of recognizing a speaker's intent from a natural language utterance, which is typically defined as a slot filling task.", "labels": [], "entities": [{"text": "Spoken language understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8981745143731436}, {"text": "recognizing a speaker's intent from a natural language utterance", "start_pos": 63, "end_pos": 127, "type": "TASK", "confidence": 0.5214832663536072}, {"text": "slot filling task", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7421653668085734}]}, {"text": "For example, in the utterance \"Remind me to call John at 9am tomorrow\", the specified information {\"time\": \"9am tomorrow\"} and {\"subject\": \"to call John\"} should be extracted.", "labels": [], "entities": []}, {"text": "The term slot refers to a variable such as the time or subject that is expected to be filled with a value provided through the user's utterance.", "labels": [], "entities": []}, {"text": "The slot filling task is typically formulated as a sequential labeling problem as shown in.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9218092958132426}]}, {"text": "This labeling scheme naturally represents the recognition of arbitrary phrases that appear in the transcription of an utterance.", "labels": [], "entities": []}, {"text": "Formally speaking, when we assume a given set of slots {s 1 , ..., s M } and denote the corresponding slot values by {v s 1 , ..., v s M } where v s i \u2208 V s i , the domain of each slot value V s i is an infinite set of word sequences.", "labels": [], "entities": []}, {"text": "In this paper, we use the term arbitrary slot filling task to refer to this implicit problem statement, which inherently underlies the sequential labeling formulation.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7813343207041422}]}, {"text": "In contrast, a different line of work has explored the case where V s i is provided as a finite set of possible values that can be handled by a backend system.", "labels": [], "entities": []}, {"text": "We refer to this type of task as a categorical slot filling task.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7941621144612631}]}, {"text": "In this case, the slot filling task is regarded as a classification problem that explicitly considers a value-based prediction, as shown in.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.9298951625823975}]}, {"text": "From this point of view, we can say that a distribution of slot values is actually concentrated in a small set of typical phrases, even in the arbitrary slot filling task, because users basically know what kind of function is offered by the system.", "labels": [], "entities": [{"text": "slot filling task", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.7846719721953074}]}, {"text": "To reflect this observation, in this paper we explore the value-based formulation approach for arbitrary slot filling tasks.", "labels": [], "entities": [{"text": "slot filling tasks", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7994207143783569}]}, {"text": "Unlike the sequential labeling formulation, which is basically position-based label prediction, our method directly estimates the posterior distribution over an infinite set of possible values for each slot V s i . The distribution is represented by using a Dirichlet process, which is a nonparametric Bayesian formalism that generates a categorical distribution for any space.", "labels": [], "entities": [{"text": "position-based label prediction", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6587813397248586}]}, {"text": "We demonstrate that this approach improves estimation accuracy in the arbitrary slot filling task compared with conventional sequential labeling approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.8480650782585144}, {"text": "slot filling task", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7750763495763143}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the existing approaches for categorical and arbitrary slot filling tasks and intro-duce related work.", "labels": [], "entities": [{"text": "slot filling tasks", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7921049396197001}]}, {"text": "In Section 3, we present our nonparametric Bayesian formulation, the hierarchical Dirichlet process slot model (HDPSM), which directly models an infinite set of slot values.", "labels": [], "entities": []}, {"text": "On the basis of the HDPSM, we develop a generative utterance model that allows us to compute the posterior probability of slot values in Section 4.", "labels": [], "entities": [{"text": "HDPSM", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8846245408058167}, {"text": "generative utterance", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.9092606008052826}]}, {"text": "In Section 5, we introduce a two-stage slot filling algorithm that consists of a candidate generation step and a candidate ranking step using the proposed model.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.8198258280754089}]}, {"text": "In Section 6, we show the experimental results for multiple datasets in different domains to demonstrate that the proposed algorithm performs better than the baseline sequential labeling method.", "labels": [], "entities": []}, {"text": "We conclude in Section 7 with a brief summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of the proposed generative model with an experiment using the algorithm name #utterances #slots max.", "labels": [], "entities": []}, {"text": "diversity: Datasets in the experiment.", "labels": [], "entities": []}, {"text": "diversity refers to the maximum number of value types that are taken by a slot.", "labels": [], "entities": []}, {"text": "We adopt a conditional random field (CRF) as a candidate generation algorithm that generates N -best estimation as candidates.", "labels": [], "entities": []}, {"text": "For the CRF, we apply commonly used features including unigram and bigram of the surface form and part of speech of the word.", "labels": [], "entities": []}, {"text": "We used CRF++ 3 as the CRF implementation.", "labels": [], "entities": []}, {"text": "The performance of our method is evaluated using two datasets from different languages, as summarized in.", "labels": [], "entities": []}, {"text": "The first dataset is provided by the third Dialog State Tracking Challenge, hereafter referred to as the DSTC corpus.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.7655281573534012}, {"text": "DSTC corpus", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.9677457213401794}]}, {"text": "The DSTC corpus consists of dialogs in the tourist information domain.", "labels": [], "entities": [{"text": "DSTC corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8898968398571014}]}, {"text": "In our experiment, we use the user's first utterance in each dialog, which typically describes the user's query to the system.", "labels": [], "entities": []}, {"text": "Utterances without any slot information are excluded.", "labels": [], "entities": [{"text": "Utterances", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.840470552444458}]}, {"text": "We manually modified the annotated slot values into \"asis form\" to allow a sequential labeling method to extract the ground-truth values.", "labels": [], "entities": []}, {"text": "This identification process can be done in a semi-automatic manner that involves no expert knowledge.", "labels": [], "entities": [{"text": "identification process", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.8824541866779327}]}, {"text": "We apply the part of speech tagger in NLTK 4 for the CRF application.", "labels": [], "entities": [{"text": "speech tagger", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7003118246793747}, {"text": "NLTK 4", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.8930290043354034}]}, {"text": "The second dataset is a weather corpus consisting of user utterances in an in-house corpus of humanmachine dialogues in the weather domain.", "labels": [], "entities": []}, {"text": "It contains 1,442 questions spoken in Japanese.", "labels": [], "entities": []}, {"text": "In this corpus, the number of value types for each slot is higher than DSTC, which indicates a more challenging task.", "labels": [], "entities": [{"text": "DSTC", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.7080628275871277}]}, {"text": "We applied the Japanese morphological analyzer MeCab () to segment the Japanese text into words before applying CRF.", "labels": [], "entities": [{"text": "Japanese morphological analyzer MeCab", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.44811849296092987}]}, {"text": "For both datasets, we examine the effect of the amount of available annotated utterances by varying the number of training data in 25, 50, 75, 100, 200, 400, 800, all.", "labels": [], "entities": []}, {"text": "The methods are compared in terms of slot estimation accuracy.", "labels": [], "entities": [{"text": "slot estimation", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.7745263874530792}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.8508645296096802}]}, {"text": "Let n c be the number of utterances for which the estimated slot Sand the ground-truth slot\u02c6Sslot\u02c6 slot\u02c6S are perfectly matched, and let n e be the number of the utterances including an estimation error.", "labels": [], "entities": []}, {"text": "The slot estimation accuracy is simply calculated as nc nc+ne . All evaluation scores are calculated as the average of 10-fold cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.5200884938240051}]}, {"text": "We also conduct a binomial test to examine the statistical significance of the improvement in the proposed algorithm compared to the CRF baseline.   tor.", "labels": [], "entities": [{"text": "CRF baseline.   tor", "start_pos": 133, "end_pos": 152, "type": "DATASET", "confidence": 0.9278452098369598}]}, {"text": "The asterisks (*) beside the HDP accuracy indicate the statistical significance against CRF best, which is tested using the binomial test.", "labels": [], "entities": [{"text": "HDP accuracy", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.7313605844974518}]}, {"text": "Results show that our proposed method performs significantly better than CRF.", "labels": [], "entities": []}, {"text": "Especially when the amount of training data is limited, the proposed method outperforms the baseline.", "labels": [], "entities": []}, {"text": "This property is attractive for practical speech recognition systems that offer many different functions.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7902134954929352}]}, {"text": "Accurate recognition at an early stage of development allows a practitioner to launch a service that results in quickly collecting hundreds of speech examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets in the experiment. Max. diversity refers to", "labels": [], "entities": []}, {"text": " Table 2: Slot estimation accuracy for the DSTC corpus. The", "labels": [], "entities": [{"text": "Slot estimation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7769120931625366}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9358126521110535}, {"text": "DSTC corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.8723264634609222}]}, {"text": " Table 3: Slot estimation accuracy for the Japanese weather cor-", "labels": [], "entities": [{"text": "Slot estimation", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9094517827033997}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9810588955879211}, {"text": "Japanese weather cor-", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.8607667088508606}]}]}