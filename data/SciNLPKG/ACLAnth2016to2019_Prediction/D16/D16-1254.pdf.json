{"title": [{"text": "Transition-Based Dependency Parsing with Heuristic Backtracking", "labels": [], "entities": [{"text": "Transition-Based Dependency Parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5310362080732981}]}], "abstractContent": [{"text": "We introduce a novel approach to the decoding problem in transition-based parsing: heuris-tic backtracking.", "labels": [], "entities": []}, {"text": "This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using confidence estimates of transition decisions as a heuristic to guide the starting points of the search.", "labels": [], "entities": []}, {"text": "This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9194931983947754}, {"text": "beam search", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.8310351967811584}]}, {"text": "When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese.", "labels": [], "entities": [{"text": "unlabeled attachment score", "start_pos": 79, "end_pos": 105, "type": "METRIC", "confidence": 0.6850837270418803}]}], "introductionContent": [{"text": "Transition-based parsing, one of the most prominent dependency parsing techniques, constructs a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure.", "labels": [], "entities": [{"text": "Transition-based parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7121728658676147}, {"text": "dependency parsing", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8128684163093567}]}, {"text": "Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence.", "labels": [], "entities": [{"text": "Transition-based parsing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7764250040054321}]}, {"text": "In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions.", "labels": [], "entities": []}, {"text": "Recent models include the greedy Stack-LSTM model of and the globally normalized feed-forward networks of.", "labels": [], "entities": []}, {"text": "These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers.", "labels": [], "entities": []}, {"text": "One common search technique is beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.8506870865821838}]}, {"text": "( In beamsearch, a fixed number of candidate transition sequences are generated, and the highest-scoring sequence is chosen as the answer.", "labels": [], "entities": []}, {"text": "One downside to beam search is that it often results in a significant amount of wasted predictions.", "labels": [], "entities": [{"text": "beam search", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9733133316040039}]}, {"text": "A constant number of beams are explored at all points throughout the sentence, leading to some unnecessary exploration towards the beginning of the sentence, and potentially insufficient exploration towards the end.", "labels": [], "entities": []}, {"text": "One way that this problem can be mitigated is by using a dynamically-sized beam.", "labels": [], "entities": []}, {"text": "When using this technique, at each step, prune all beams whose scores are below some value s, where sis calculated based upon the distribution of scores of available beams.", "labels": [], "entities": []}, {"text": "Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of the highest-scoring beam.", "labels": [], "entities": []}, {"text": "Another approach to solving this issue is given by.", "labels": [], "entities": []}, {"text": "They introduced selectional branching, which involves performing an initial greedy parse, and then using confidence estimates on each prediction to spawn additional beams.", "labels": [], "entities": []}, {"text": "Relative to standard beam-search, this reduces the average number of predictions required to parse a sentence, resulting in a speed-up.", "labels": [], "entities": []}, {"text": "In this paper, we introduce heuristic backtracking, which expands on the ideas of selectional branching by integrating a search strategy based on a heuristic function: a function which estimates the future cost of taking a particular decision.", "labels": [], "entities": [{"text": "selectional branching", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.8239277303218842}]}, {"text": "When paired with a good heuristic, heuristic backtracking maintains the property of reducing wasted predictions, but allows us to more fully explore the space of possible transition sequences (as compared to selectional branching).", "labels": [], "entities": []}, {"text": "In this paper, we use a heuristic based on the confidence of transition predictions.", "labels": [], "entities": []}, {"text": "We also introduce anew optimization: heuristic backtracking with cutoff.", "labels": [], "entities": []}, {"text": "Since heuristic backtracking produces results incrementally, it is possible to stop the search early if we have found an answer that we believe to be the gold parse, saving time proportional to the number of backtracks remaining.", "labels": [], "entities": []}, {"text": "We compare the performance of these various decoding algorithms with the Stack-LSTM parser, and achieve slightly higher accuracy than beam search, in significantly less time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9987793564796448}]}], "datasetContent": [{"text": "To test the effectiveness of heuristic backtracking, we compare it with other decoding techniques: greedy, beam search, 3 , dynamic beam search, and selectional branching.", "labels": [], "entities": []}, {"text": "We then try heuristic backtracking (see Section 3.1), and heuristic backtracking with cutoff (see Section 3.4).", "labels": [], "entities": []}, {"text": "Note that beam search was not used for early-update training ().", "labels": [], "entities": [{"text": "beam search", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9512328803539276}]}, {"text": "We use the same greedy training strategy for all models, and we only change the decoding strategy.", "labels": [], "entities": []}, {"text": "We tested the performance of these algorithms on the English SD and Chinese CTB.", "labels": [], "entities": [{"text": "English SD and Chinese CTB", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.858601701259613}]}, {"text": "4 A single model was trained using the techniques described in Section 2, and used as the transition model for all decoding algorithms.", "labels": [], "entities": []}, {"text": "Each decoding technique was tested with varying numbers of beams; as b increased, both the predictions per sentence and accuracy trended upwards.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.999157190322876}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "Note that we report results for only the highestaccuracy b (in the development set) for each.", "labels": [], "entities": []}, {"text": "We also report the results of the cutoff model in.", "labels": [], "entities": []}, {"text": "The same greedily-trained model as above was used to generate candidate parses and confidence estimates for each transition, and then the cutoff model was trained to use these confidence esti- mates to discriminate between correctly-parsed and incorrectly-parsed sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: UAS and LAS of various decoding meth- ods. Pred/Sent refers to number of predictions made  by the Stack-LSTM per sentence.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9106511473655701}, {"text": "LAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9755800366401672}, {"text": "Pred", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9923542737960815}]}]}