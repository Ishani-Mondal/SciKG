{"title": [{"text": "Exploiting Mutual Benefits between Syntax and Semantic Roles using Neural Network", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate mutual benefits between syntax and semantic roles using neural network models, by studying a parsing\u2192SRL pipeline, a SRL\u2192parsing pipeline, and a simple joint model by embedding sharing.", "labels": [], "entities": []}, {"text": "The integration of syntactic and semantic features gives promising results in a Chinese Semantic Tree-bank, demonstrating large potentials of neural models for joint parsing and semantic role labeling .", "labels": [], "entities": [{"text": "Chinese Semantic Tree-bank", "start_pos": 80, "end_pos": 106, "type": "DATASET", "confidence": 0.7619369626045227}, {"text": "joint parsing", "start_pos": 160, "end_pos": 173, "type": "TASK", "confidence": 0.6082316190004349}, {"text": "semantic role labeling", "start_pos": 178, "end_pos": 200, "type": "TASK", "confidence": 0.6186278859774271}]}], "introductionContent": [{"text": "The correlation between syntax and semantics has been a fundamental problem in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6410536468029022}]}, {"text": "As a shallow semantic task, semantic role labeling (SRL) models have traditionally been built upon syntactic parsing results (.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7771668682495753}, {"text": "syntactic parsing", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7579492628574371}]}, {"text": "It has been shown that parser output features play a crucial role for accurate SRL (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.7630655169487}]}, {"text": "On the reverse direction, semantic role features have been used to improve parsing (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.9660288095474243}]}, {"text": "Existing methods typically use semantic features to rerank n-best lists of syntactic parsing models ().", "labels": [], "entities": []}, {"text": "There has also been attempts to learn syntactic parsing and semantic role labeling models jointly, but most such efforts have led to negative results; Van Den Bosch et al.,.", "labels": [], "entities": [{"text": "syntactic parsing and semantic role labeling", "start_pos": 38, "end_pos": 82, "type": "TASK", "confidence": 0.6978912800550461}]}, {"text": "* Work done while the first author was visiting SUTD.", "labels": [], "entities": [{"text": "SUTD", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.7945547103881836}]}, {"text": "With the rise of deep learning, neural network models have been used for semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.8029151558876038}]}, {"text": "Recently, it has been shown that a neural semantic role labeler can give state-of-the-art accuracies without using parser output features, thanks to the use of recurrent neural network structures that automatically capture syntactic information ().", "labels": [], "entities": [{"text": "neural semantic role labeler", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6414946988224983}]}, {"text": "In the parsing domain, neural network models have also been shown to give state-of-the-art results recently.", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9700779318809509}]}, {"text": "The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing\u2192SRL and SRL\u2192parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting.", "labels": [], "entities": [{"text": "parser-independent neural SRL", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.7564058899879456}, {"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9655097126960754}, {"text": "SRL", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9712434411048889}, {"text": "parsing\u2192SRL", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.749179740746816}]}, {"text": "Different from traditional models that rely on manual feature combinations for joint learning tasks, neural network models induce non-linear feature combinations automatically from input word and Part-of-Speech (POS) embeddings.", "labels": [], "entities": []}, {"text": "This allows more complex feature sharing between multiple tasks to be achieved effectively).", "labels": [], "entities": []}, {"text": "We take a first step 1 in such investigation by cou-pling a state-of-the-art neural semantic role labeler ( ) and a state-of-the-art neural parser ( ).", "labels": [], "entities": []}, {"text": "First, we propose a novel parsing\u2192SRL pipeline using a tree Long ShortTerm Memory (LSTM) model) to represent parser outputs, before feeding them to the neural SRL model as inputs.", "labels": [], "entities": [{"text": "parsing\u2192SRL pipeline", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8481482118368149}]}, {"text": "Second, we investigate a SRL\u2192parsing pipeline, using semantic role label embeddings to enrich parser features.", "labels": [], "entities": [{"text": "SRL\u2192parsing pipeline", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.9054068177938461}]}, {"text": "Third, we build a joint training model by embedding sharing, which is the most shallow level of parameter sharing between deep neural networks.", "labels": [], "entities": []}, {"text": "This simple strategy is immune to significant differences between the network structures of the two models, which prevent direct sharing of deeper network parameters.", "labels": [], "entities": []}, {"text": "We choose a Chinese semantic role treebank ( for preliminary experiments, which offers consistent dependency between syntax and semantic role representations, thereby facilitates the application of standard LSTM models.", "labels": [], "entities": []}, {"text": "Results show that the methods give improvements to both parsing and SRL accuracies, demonstrating large potentials of neural networks for the joint task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.9802106022834778}, {"text": "SRL", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.6944265961647034}]}, {"text": "Our contributions can be summarized as: \u2022 We show that the state-of-the-art LSTM semantic role labeler of, which has been shown to be able to induce syntactic features automatically, can still be improved using parser output features via tree LSTM (; \u2022 We show that state-of-the-art neural parsing can be improved by using semantic role features; \u2022 We show that parameter sharing between neural parsing and SRL improves both sub tasks, which is inline with the observation of between POS tagging, chunking and SRL.", "labels": [], "entities": [{"text": "LSTM semantic role labeler", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.5856685787439346}, {"text": "SRL", "start_pos": 407, "end_pos": 410, "type": "TASK", "confidence": 0.8972886204719543}, {"text": "POS tagging", "start_pos": 484, "end_pos": 495, "type": "TASK", "confidence": 0.6179539412260056}]}, {"text": "\u2022 Our code and all models are released at https://github.com/ShiPeng95/ShallowJoint.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets We choose Chinese Semantic Treebank ( for our experiments.", "labels": [], "entities": [{"text": "Chinese Semantic Treebank", "start_pos": 19, "end_pos": 44, "type": "DATASET", "confidence": 0.8619605898857117}]}, {"text": "Similar to the CoNLL corpora () and different from PropBank, it is a dependency-based corpus rather than a constituent-based corpus.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9443352818489075}]}, {"text": "The corpus contains syntactic dependency arc and semantic role annotations in a consistent form, hence facilitating the joint task.", "labels": [], "entities": []}, {"text": "We follow the standard split for the training, development and test sets, as shown in.", "labels": [], "entities": []}, {"text": "There is a large number of singletons in the training set and a large number of out-of-vocabulary (OOV) words in the development set.", "labels": [], "entities": []}, {"text": "We use the mechanism of  to stochastically set singletons as UNK token in each training iteration with a probability punk . The hyperparameter punk is set to 0.2.", "labels": [], "entities": []}, {"text": "For parameters used in Stack-LSTM, we follow . We set the number of embeddings by intuition, and decide to have the size of word embedding twice as large as that of charac-  ter embedding, and the size of character embedding larger than the size of POS embedding.", "labels": [], "entities": []}, {"text": "More specifically, we fix the size of word embeddings n w to 64, character embeddings n char to 32, POS embeddings n pos to 30, action embeddings n dep to 30, and semantic role embeddings n srl to 30.", "labels": [], "entities": []}, {"text": "The LSTM input size is set to 128 and the LSTM hidden size to 128.", "labels": [], "entities": []}, {"text": "We randomly initialize each parameter to areal r+c ], where r is the number of input unit and c is the number of output unit.", "labels": [], "entities": []}, {"text": "To minimize the influence of external information, we did not pretrain the embedding values.", "labels": [], "entities": []}, {"text": "In addition, we apply a Gaussian noise N (0, 0.2) to word embeddings during training to prevent overfitting.", "labels": [], "entities": []}, {"text": "We optimize model parameters using stochastic gradient descent with momentum.", "labels": [], "entities": []}, {"text": "The same learning rate decay mechanism of  is used.", "labels": [], "entities": []}, {"text": "The best model parameters are selected according to a score metric on the development set.", "labels": [], "entities": []}, {"text": "For different tasks, we use different score metrics to evaluate the parameters.", "labels": [], "entities": []}, {"text": "Since there are there metrics, F1, UAS and LAS, possibly reported at the same time, we use the weighted average to consider the effect of all metrics when choosing the best model on the dev set.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9986975193023682}, {"text": "UAS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9561424851417542}, {"text": "LAS", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9927768707275391}]}, {"text": "In particular, we use F 1 for SRL, 0.5 \u00d7 LAS + 0.5 \u00d7 U AS for parsing, and 0.5 \u00d7 F 1 + 0.25 \u00d7 U AS + 0.25 \u00d7 LAS for the joint task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.7974331974983215}, {"text": "LAS", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9422110915184021}, {"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9846709966659546}]}], "tableCaptions": [{"text": " Table 1: Statistics of Chinese Semantic Treebank.", "labels": [], "entities": [{"text": "Chinese Semantic Treebank", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.8589080174763998}]}, {"text": " Table 2: Results. Bi-LSTM and S-LSTM are two baseline", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9855087399482727}]}]}