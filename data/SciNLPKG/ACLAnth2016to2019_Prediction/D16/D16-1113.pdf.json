{"title": [], "abstractContent": [{"text": "Word embedding models learn vectorial word representations that can be used in a variety of NLP applications.", "labels": [], "entities": []}, {"text": "When training data is scarce, these models risk losing their generalization abilities due to the complexity of the models and the overfitting to finite data.", "labels": [], "entities": []}, {"text": "We propose a regularized embedding formulation , called Robust Gram (RG), which penalizes overfitting by suppressing the disparity between target and context embeddings.", "labels": [], "entities": [{"text": "Robust Gram (RG)", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.8636772394180298}]}, {"text": "Our experimental analysis shows that the RG model trained on small datasets generalizes better compared to alternatives, is more robust to variations in the training set, and correlates well to human similarities in a set of word similarity tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings represent each word as a unique vector in a linear vector space, encoding particular semantic and syntactic structure of the natural language (.", "labels": [], "entities": []}, {"text": "In various lingual tasks, these sequence prediction models shown superior results over the traditional count-based models).", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.6656227111816406}]}, {"text": "Tasks such as sentiment analysis) and sarcasm detection () enjoys the merits of these features.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.9813008904457092}, {"text": "sarcasm detection", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.860560953617096}]}, {"text": "These word embeddings optimize features and predictors simultaneously, which can be interpreted as a factorization of the word cooccurence matrix C.", "labels": [], "entities": []}, {"text": "In most realistic scenarios these models have to be learned from a small training set.", "labels": [], "entities": []}, {"text": "Furthermore, word distributions are often skewed, and optimizing the reconstruction of\u02c6Cof\u02c6 of\u02c6C puts too much emphasis on the high frequency pairs ().", "labels": [], "entities": []}, {"text": "On the other hand, by having an unlucky and scarce data sample, the estimated\u02c6Cestimated\u02c6 estimated\u02c6C rapidly deviates from the underlying true cooccurence, in particular for low-frequency pairs (.", "labels": [], "entities": []}, {"text": "Finally, noise (caused by stemming, removal of high frequency pairs, typographical errors, etc.) can increase the estimation error heavily (.", "labels": [], "entities": [{"text": "estimation error", "start_pos": 114, "end_pos": 130, "type": "METRIC", "confidence": 0.8746672868728638}]}, {"text": "It is challenging to derive a computationally tractable algorithm that solves all these problems.", "labels": [], "entities": []}, {"text": "Spectral factorization approaches usually employ Laplace smoothing or a type of SVD weighting to alleviate the effect of the noise.", "labels": [], "entities": []}, {"text": "Alternatively, iteratively optimized embeddings such as Skip Gram (SG) model) developed various mechanisms such as undersampling of highly frequent hub words apriori, and throwing rare words out of the training.", "labels": [], "entities": [{"text": "Skip Gram (SG) model", "start_pos": 56, "end_pos": 76, "type": "DATASET", "confidence": 0.8171203633149465}]}, {"text": "Here we propose a fast, effective and generalizable embedding approach, called Robust Gram, that penalizes complexity arising from the factorized embedding spaces.", "labels": [], "entities": []}, {"text": "This design alleviates the need from tuning the aforementioned pseudo-priors and the preprocessing procedures.", "labels": [], "entities": []}, {"text": "Experimental results show that our regularized model 1) generalizes better given a small set of samples while other methods yield insufficient generalization 2) is more robust to arbitrary perturbations in the sample set and alternations in the preprocessing specifications 3) achieves much better performance on word similarity task, especially when similarity pairs contains unique and hardly observed words in the vocabulary.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 313, "end_pos": 333, "type": "TASK", "confidence": 0.8019381364186605}]}], "datasetContent": [{"text": "The experiments are performed on a subset of the Wikipedia corpus containing approximately 15M words.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.9346007108688354}]}, {"text": "For a systematic comparison, we use the same symmetric window size adopted in), 10.", "labels": [], "entities": []}, {"text": "Stochastic gradient learning rate is set to 0.05.", "labels": [], "entities": []}, {"text": "Embedding dimensionality is set to 100 for model selection and sensitivity analysis.", "labels": [], "entities": [{"text": "model selection", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7761323750019073}, {"text": "sensitivity analysis", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7570131719112396}]}, {"text": "Unless otherwise is stated, we discard the most frequent 20 hub words to yield a final vocabulary of 26k words.", "labels": [], "entities": []}, {"text": "To understand the relative merit of our approach 2 , Skip Gram model is picked as the baseline.", "labels": [], "entities": []}, {"text": "To retain the learning speed, and avoid inctractability of maximum likelihood learning, we learn our embeddings with Noise Contrastive Estimation using a negative sample ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's \u03c1 coefficient. Higher is better.", "labels": [], "entities": [{"text": "Spearman's \u03c1 coefficient", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6249525621533394}]}]}