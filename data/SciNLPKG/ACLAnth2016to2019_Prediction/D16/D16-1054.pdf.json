{"title": [{"text": "On Generating Characteristic-rich Question Sets for QA Evaluation", "labels": [], "entities": [{"text": "QA Evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9142707586288452}]}], "abstractContent": [{"text": "We present a semi-automated framework for constructing factoid question answering (QA) datasets, where an array of question characteristics are formalized, including structure complexity , function, commonness, answer cardi-nality, and paraphrasing.", "labels": [], "entities": [{"text": "factoid question answering (QA)", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.782845159371694}]}, {"text": "Instead of collecting questions and manually characterizing them, we employ a reverse procedure, first generating a kind of graph-structured logical forms from a knowledge base, and then converting them into questions.", "labels": [], "entities": []}, {"text": "Our work is the first to generate questions with explicitly specified characteristics for QA evaluation.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9233671128749847}]}, {"text": "We construct anew QA dataset with over 5,000 logical form-question pairs, associated with answers from the knowledge base, and show that datasets constructed in this way enable fine-grained analyses of QA systems.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.7186712473630905}]}, {"text": "The dataset can be found in https://github.com/ ysu1989/GraphQuestions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Factoid question answering (QA) has gained great attention recently, owing to the fast growth of large knowledge bases (KBs) such as DBpedia () and Freebase (, which avail QA systems of comprehensive and precise knowledge of encyclopedic scope (.", "labels": [], "entities": [{"text": "Factoid question answering (QA)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8629191120465597}, {"text": "DBpedia", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9494516849517822}, {"text": "Freebase", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.9435250759124756}]}, {"text": "With the blossoming of QA systems, evaluation is becoming an increasingly important problem.", "labels": [], "entities": [{"text": "QA", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9154018759727478}, {"text": "evaluation", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9554426670074463}]}, {"text": "QA datasets, consisting of a set of questions with ground-truth answers, are critical for both comparing existing systems and gaining insights to develop new systems.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8564646542072296}]}, {"text": "Questions have rich characteristics, constituting dimensions along which question difficulty varies.", "labels": [], "entities": []}, {"text": "Some questions are difficult due to their complex semantic structure (\"Who was the coach when Michael Jordan stopped playing for the Chicago Bulls?\"), while some others maybe difficult because they require a precise quantitative analysis over the answer space (\"What is the best-selling smartphone in 2015?\").", "labels": [], "entities": []}, {"text": "Many other characteristics shall be considered too, e.g., what topic a question is about (questions about common topics maybe easier to answer) and how many answers there are (it is harder to achieve a high recall in case of multiple answers).", "labels": [], "entities": [{"text": "recall", "start_pos": 207, "end_pos": 213, "type": "METRIC", "confidence": 0.996900200843811}]}, {"text": "Worse still, due to the flexibility of natural language, different people often describe the same question in different ways, i.e., paraphrasing.", "labels": [], "entities": []}, {"text": "It is important fora QA system to be robust to paraphrasing.", "labels": [], "entities": []}, {"text": "A QA dataset explicitly specifying such question characteristics allows for fine-grained inspection of system performance.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 2, "end_pos": 12, "type": "DATASET", "confidence": 0.7001610398292542}]}, {"text": "However, to the best of our knowledge, none of the existing QA datasets) provides question characteristics.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.8439564406871796}]}, {"text": "In this work, we make the first attempt to generate questions with explicitly specified characteristics, and examine the impact of various question characteristics in QA.", "labels": [], "entities": []}, {"text": "We present a semi-automated framework to construct QA datasets with characteristic specification from a knowledge base.", "labels": [], "entities": []}, {"text": "The framework revolves around an intermediate graph query representation, which helps to formalize question characteristics and collect answers.", "labels": [], "entities": []}, {"text": "We first automatically generate graph queries from a knowledge base, and then employ human annotators to convert graph queries into questions.", "labels": [], "entities": []}, {"text": "Automating graph query generation brings with it the challenge of assessing the quality of graph queries and filtering out bad ones.", "labels": [], "entities": [{"text": "graph query generation", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.6876240372657776}]}, {"text": "Our framework tackles the challenge by combining structured information in the knowledge base and statistical information from the Web.", "labels": [], "entities": []}, {"text": "First, we identify redundant components in a graph query and develop techniques to remove them.", "labels": [], "entities": []}, {"text": "Furthermore, based on the frequency of entities, classes, and relations mined from the Web, we quantify the commonness of a graph query and filter out too rare ones.", "labels": [], "entities": []}, {"text": "We employ a semi-automated approach for the conversion from graph query to natural language question, which provides two levels of paraphrasing: Common lexical forms of an entity (e.g., \"Queen Elizabeth\" and \"Her Majesty the Queen\" for ElizabethII) mined from the Web are used as entity paraphrases, and the remaining parts of a question are paraphrased by annotators.", "labels": [], "entities": []}, {"text": "As a result, dozens of paraphrased questions can be produced fora single graph query.", "labels": [], "entities": []}, {"text": "To demonstrate the usefulness of question characteristics in QA evaluation, we construct anew dataset with over 5,000 questions based on Freebase using the proposed framework, and extensively evaluate several QA systems.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9387564957141876}, {"text": "Freebase", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.9672451019287109}]}, {"text": "A couple of new findings about system performance and question difficulty are discussed.", "labels": [], "entities": []}, {"text": "For example, different from the results based on previous QA datasets ( ), we find that semantic parsing in general works better than information extraction on our dataset.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.8168317973613739}, {"text": "semantic parsing", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.742924302816391}, {"text": "information extraction", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.7531211674213409}]}, {"text": "Information extraction based QA systems have trouble dealing with questions requiring aggregation or with multiple answers.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7342593371868134}]}, {"text": "A holistic understanding of the whole question is often needed for hard questions.", "labels": [], "entities": []}, {"text": "The experiments point out an array of issues that future QA systems may need to solve.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have constructed anew QA dataset, named GRAPHQUESTIONS, using the proposed framework, and tested several QA systems to show that it enables fine-grained inspection of QA systems.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.7196723967790604}, {"text": "GRAPHQUESTIONS", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.9768638014793396}]}, {"text": "We first randomly generated a set of minimal graph queries, and removed the ones whose commonness is below a certain threshold.", "labels": [], "entities": []}, {"text": "The remaining graph queries were then screened by graduate students, and a canonical question was generated for each query, with each being verified by at least two students.", "labels": [], "entities": []}, {"text": "We recruited 160 crowdsourcing workers from Amazon MTurk to generate sentence-level paraphrases of the canonical questions.", "labels": [], "entities": [{"text": "Amazon MTurk", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.8341644108295441}]}, {"text": "Trivial paraphrases (e.g., \"which city\" vs. \"what city\") were manually removed to retain a high diversity in paraphrasing.", "labels": [], "entities": []}, {"text": "At most 3 entity-level paraphrases were used for each sentence-level paraphrase.", "labels": [], "entities": []}, {"text": "GRAPHQUESTIONS contains 500 graph queries, 2,460 sentence-level paraphrases, and 5,166 questions 2 . The dataset presents a high diversity and covers a wide range of domains including People, Astronomy, Medicine, etc.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.5929155945777893}]}, {"text": "Specifically, it contains 148, 506, 596, 376 and 3,026 distinct domains, classes, relations, topic entities, and words, respectively.", "labels": [], "entities": []}, {"text": "We evenly split GRAPHQUESTIONS into a training set and a testing set.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 16, "end_pos": 30, "type": "METRIC", "confidence": 0.5645865797996521}]}, {"text": "All the paraphrases of the same graph query are in the same set.", "labels": [], "entities": []}, {"text": "While there are other question characteristics derivable from graph query, we will focus on the following ones: structure complexity, function, commonness, paraphrasing, and answer cardinality.", "labels": [], "entities": []}, {"text": "use the number of edges to quantify structure complexity, and limit it to at most 3.", "labels": [], "entities": []}, {"text": "Commonness is limited to log 10 (p(q)) \u2265 \u221240 (c.f.", "labels": [], "entities": []}, {"text": "As shown in Section 7.4.2, such questions are already very hard for existing QA systems.", "labels": [], "entities": []}, {"text": "Nevertheless, the proposed framework can be used to generate questions with different characteristic distributions.", "labels": [], "entities": []}, {"text": "Some statistics are shown in and more finegrained statistics can be found in Appendix D. Several example questions are shown in Table 2.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9645771384239197}]}, {"text": "Sentence-level paraphrasing requires to handle both commands (the first example) and \"Wh\" questions, light verbs (\"Who did nine eleven?\"), and changes of syntactic structure (\"The September 11 attacks were carried outwith the involvement of what terrorist organizations?\").", "labels": [], "entities": [{"text": "Sentence-level paraphrasing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8945665955543518}]}, {"text": "Entitylevel paraphrasing tests the capability of QA systems on abbreviation (\"NYC\" for New York City), world knowledge (\"Her Majesty the Queen\" for ElizabethII), or even common typos (\"Shakspeare\" for WilliamShakespeare).", "labels": [], "entities": []}, {"text": "Numbers and dates are also common, e.g., \"Which computer operating system was released on Sept. the 20th, 2008?\"", "labels": [], "entities": []}, {"text": "We compare several QA datasets constructed from Freebase, shown in.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.768913209438324}, {"text": "Freebase", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.5074094533920288}]}, {"text": "Datasets focusing on single-relation questions are of a larger scale, but are also of a significant lack in question characteristics.", "labels": [], "entities": []}, {"text": "Overall GRAPHQUESTIONS presents the highest diversity in question characteristics.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 8, "end_pos": 22, "type": "METRIC", "confidence": 0.9399336576461792}]}, {"text": "Compared with the scores on WEBQUESTIONS (30%-40%), the scores on GRAPHQUESTIONS are lower.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.5861340761184692}, {"text": "GRAPHQUESTIONS", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.9870204925537109}]}, {"text": "This is because GRAPHQUES-TIONS contains questions over a broader range of difficulty levels.", "labels": [], "entities": [{"text": "GRAPHQUES-TIONS", "start_pos": 16, "end_pos": 31, "type": "METRIC", "confidence": 0.9053667783737183}]}, {"text": "For example, it is more diverse in topics (Appendix D); also the scores become much closer when excluding paraphrasing (Section 7.4.2).", "labels": [], "entities": [{"text": "Appendix", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9226969480514526}]}, {"text": "JACANA achieves a comparable F1 score with SEMPRE and PARASEMPRE on WEBQUES-TIONS ( ).", "labels": [], "entities": [{"text": "JACANA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8171729445457458}, {"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9859499931335449}, {"text": "SEMPRE", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9990172386169434}, {"text": "PARASEMPRE", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9987210631370544}, {"text": "WEBQUES-TIONS", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.5558815598487854}]}, {"text": "On GRAPHQUESTIONS, however, SEMPRE and PARASEMPRE significantly outperform JACANA (both p < 0.0001).", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 3, "end_pos": 17, "type": "METRIC", "confidence": 0.6624575853347778}, {"text": "SEMPRE", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9988182187080383}, {"text": "PARASEMPRE", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9986811280250549}, {"text": "JACANA", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.7884864211082458}]}, {"text": "The following experiments will give more insights about where the performance difference comes from.", "labels": [], "entities": []}, {"text": "On the other hand, JACANA is much faster, showing an advantage of information extraction.", "labels": [], "entities": [{"text": "JACANA", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.482968807220459}, {"text": "information extraction", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.890910267829895}]}, {"text": "The semantic parsing systems spend a lot of time on executing SPARQL queries.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7196281850337982}]}, {"text": "Bypassing SPARQL and directly working on the knowledge base maybe a promising way to speedup semantic parsing on large knowledge bases ( ).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.7183973789215088}]}, {"text": "2 WEBQUESTIONSSP is WEBQUESTIONS with manually annotated logical forms.", "labels": [], "entities": [{"text": "WEBQUESTIONSSP", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.8592190742492676}, {"text": "WEBQUESTIONS", "start_pos": 20, "end_pos": 32, "type": "METRIC", "confidence": 0.5348067283630371}]}, {"text": "Only those with a full logical form are included (4737 / 5810).", "labels": [], "entities": []}, {"text": "With explicitly specified question characteristics, we are able to further inspect QA systems.", "labels": [], "entities": []}, {"text": "We first breakdown system performance by structure.", "labels": [], "entities": []}, {"text": "Answer quality is in general sensitive to the complexity of question structure: As the number of edges increases, F1 score decreases ().", "labels": [], "entities": [{"text": "F1 score", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9885850250720978}]}, {"text": "The tested systems often fail to take into account auxiliary constraints in a question.", "labels": [], "entities": []}, {"text": "For example, for \"How many children of Ned Stark were born in Winterfell?\"", "labels": [], "entities": [{"text": "Ned Stark were born in Winterfell", "start_pos": 39, "end_pos": 72, "type": "DATASET", "confidence": 0.8740617533524832}]}, {"text": "SEMPRE fails to identify the constraint \"born in Winterfell\", so it also considers Ned Stark's bastard son, Jon Snow, as an answer, who was not born in Winterfell.", "labels": [], "entities": [{"text": "SEMPRE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5556976199150085}, {"text": "Ned Stark's bastard son, Jon Snow", "start_pos": 83, "end_pos": 116, "type": "DATASET", "confidence": 0.7708476781845093}]}, {"text": "Answering questions involving multiple relations using large knowledge bases remain an open problem.", "labels": [], "entities": [{"text": "Answering questions involving multiple relations", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8473263740539551}]}, {"text": "The large size of knowledge bases prohibits exhaustive search, so smarter algorithms are needed to efficiently prune the answer space.", "labels": [], "entities": []}, {"text": "Berant and Liang (2015) point out an interesting direction, leveraging agenda-based parsing with imitation learning for efficient search in the answer space.", "labels": [], "entities": [{"text": "agenda-based parsing", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.6590550541877747}]}, {"text": "In terms of functions, while SEMPRE and PARASEMPRE perform well on count questions, all the tested systems perform poorly on questions with superlatives or comparatives ().", "labels": [], "entities": [{"text": "SEMPRE", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9684039950370789}, {"text": "PARASEMPRE", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9901099801063538}]}, {"text": "JACANA has trouble dealing with functions because it does not conduct quantitative analysis over the answer space.", "labels": [], "entities": [{"text": "JACANA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8746750950813293}]}, {"text": "SEMPRE and PARASEMPRE do not generate logical forms with superlatives and comparatives, so they cannot answer such questions well.", "labels": [], "entities": [{"text": "SEMPRE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9557727575302124}, {"text": "PARASEMPRE", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9913251399993896}]}, {"text": "Not surprisingly, more common questions are in general easier to answer).", "labels": [], "entities": []}, {"text": "An interesting observation is that SEM-PRE's performance gets worse on the most common questions.", "labels": [], "entities": [{"text": "SEM-PRE", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9719707369804382}]}, {"text": "The cause is likely rooted in how the QA systems construct their candidate answer sets.", "labels": [], "entities": []}, {"text": "PARASEMPRE and JACANA exhaustively construct candidate sets, while SEMPRE employs a bottom-up beam search, making it more sensitive to the size of the candidate answer space.", "labels": [], "entities": [{"text": "PARASEMPRE", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9540956020355225}, {"text": "JACANA", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.849353015422821}, {"text": "SEMPRE", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.5638207793235779}]}, {"text": "Common entities like UnitedStatesOfAmerica are often featured by a high degree in knowledge bases (e.g., 1 million neighboring entities), which dramatically increases the size of the candidate answer space.", "labels": [], "entities": [{"text": "UnitedStatesOfAmerica", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.9817461967468262}]}, {"text": "During SEM-PRE's iterative beam search, many correct logical forms may have fallen off beam before getting into the final candidate set.", "labels": [], "entities": [{"text": "SEM-PRE", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.9248884916305542}]}, {"text": "We checked the percentage of questions for which the correct logical form is in the final candidate set, and found that it decreased from 19.8% to 16.7% when commonness increased from -15 to -5, providing an evidence for the intuition.", "labels": [], "entities": []}, {"text": "It is critical fora system to tolerate the wording varieties of users.", "labels": [], "entities": []}, {"text": "We make the first effort to evaluate QA systems on paraphrasing.", "labels": [], "entities": [{"text": "QA", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9525989294052124}]}, {"text": "For each system, we rank, in descending order, all the paraphrases derived from the same graph query by their F1 score achieved by the system, and then compute the average F1 score of each rank.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9852032959461212}, {"text": "F1 score", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9844399094581604}]}, {"text": "In(d), the decreasing rate of a curve thus describes a system's robustness to paraphrasing; the higher, the less robust.", "labels": [], "entities": []}, {"text": "All the systems achieve a reasonable score on the top-1 paraphrases, i.e., when a system can choose the paraphrase it can best answer.", "labels": [], "entities": []}, {"text": "The F1 scores drop quickly in general.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9700485169887543}]}, {"text": "On the fourth-ranked paraphrases, the F1 score of SEM-PRE, PARASEMPRE, and JACANA are respectively only 37.65%, 53.2%, and 36.2% of their score on the top-1 paraphrases.", "labels": [], "entities": [{"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9998179078102112}, {"text": "SEM-PRE", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.43095454573631287}, {"text": "PARASEMPRE", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9822856187820435}, {"text": "JACANA", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.8871515393257141}]}, {"text": "Leveraging paraphrasing in its model, PARASEMPRE does seem to be more robust.", "labels": [], "entities": [{"text": "PARASEMPRE", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.8092049956321716}]}, {"text": "The results show that how to handle paraphrased questions is still a challenging problem.", "labels": [], "entities": []}, {"text": "SEMPRE and JACANA get a significantly lower F1 score (both p < 0.0001) on multi-answer questions, mainly coming from a decrease on recall.", "labels": [], "entities": [{"text": "SEMPRE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7186789512634277}, {"text": "JACANA", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9112425446510315}, {"text": "F1 score", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9919733107089996}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9982811212539673}]}, {"text": "The decrease of PARASEMPRE is not significant (p=0.29).", "labels": [], "entities": [{"text": "PARASEMPRE", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.999627947807312}]}, {"text": "The particularly significant decrease of JACANA demon-  strates the difficulty of training a classifier that can predict all of the answers correctly; semantic parsing is more robust in this case.", "labels": [], "entities": [{"text": "JACANA demon-  strates", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6675793081521988}, {"text": "semantic parsing", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.7581925988197327}]}, {"text": "The precision of SEMPRE is high because it generates no response for many questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996833801269531}, {"text": "SEMPRE", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.5232694149017334}]}, {"text": "Note that under the current definition, the average F1 score is not the harmonic mean of the average precision and recall (c.f. Section 7.3).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9841014742851257}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9911057353019714}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9978272318840027}]}], "tableCaptions": [{"text": " Table 1: Characteristic statistics. |A| is answer cardinality. Refer to Appendix D for paraphrase and other fine-grained distributions.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.983182430267334}]}, {"text": " Table 2: Example questions and characteristics. Three sentence-level paraphrases are shown for each graph query, with the last", "labels": [], "entities": []}, {"text": " Table 3. Datasets focus- ing on single-relation questions are of a larger scale,", "labels": [], "entities": []}, {"text": " Table 3: Comparison of QA datasets constructed from Freebase. GRAPHQUESTIONS is the richest in question characteristics.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.7007457911968231}, {"text": "GRAPHQUESTIONS", "start_pos": 63, "end_pos": 77, "type": "METRIC", "confidence": 0.9957951307296753}]}, {"text": " Table 4: Overall performance on GRAPHQUESTIONS.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 33, "end_pos": 47, "type": "METRIC", "confidence": 0.8604855537414551}]}, {"text": " Table 5: Performance breakdown by answer cardinality |A|.", "labels": [], "entities": [{"text": "answer cardinality", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6678132861852646}, {"text": "A", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9132981896400452}]}]}