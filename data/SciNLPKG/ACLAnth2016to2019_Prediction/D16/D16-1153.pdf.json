{"title": [{"text": "Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7303466002146403}, {"text": "Low Resource Transfer Settings", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.7011400759220123}]}], "abstractContent": [{"text": "Named Entity Recognition is a well established information extraction task with many state of the art systems existing fora variety of languages.", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7263475656509399}, {"text": "information extraction task", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.8270337581634521}]}, {"text": "Most systems rely on language specific resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually.", "labels": [], "entities": []}, {"text": "In this paper , we introduce an attentional neural model which only uses language universal phonolog-ical character representations with word em-beddings to achieve state of the art performance in a monolingual setting using supervision and which can quickly adapt to anew language with minimal or no data.", "labels": [], "entities": []}, {"text": "We demonstrate that phonological character representations facilitate cross-lingual transfer, out-perform orthographic representations and incorporating both attention and phonological features improves statistical efficiency of the model in 0-shot and low data transfer settings with no task specific feature engineering in the source or target language.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.7589217722415924}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) () is an information extraction task that deals with finding and classifying entities in text into a fixed set of types of interest.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.770641009012858}, {"text": "information extraction task", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.8059083620707194}]}, {"text": "It is challenging fora variety of reasons.", "labels": [], "entities": []}, {"text": "Named Entities (NEs) can be arbitrarily synthesized (eg. people's/organization's names).", "labels": [], "entities": []}, {"text": "NEs are often not subject to uniform cross-linguistic spelling conventions: compare France (English) and Frantsiya (Uzbek).", "labels": [], "entities": [{"text": "NEs", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.935797929763794}]}, {"text": "NEs occur rarely in data which makes generalization difficult.", "labels": [], "entities": [{"text": "generalization", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.9752296805381775}]}, {"text": "Skewed class statistics necessitate measures to prevent models from merely favoring a majority class.", "labels": [], "entities": []}, {"text": "Named entities must also be annotated in context (eg. \" ORG \" vs. \"[New York] LOC \").", "labels": [], "entities": []}, {"text": "Lexical ambiguity (Turkey-country vs. bird), semantic ambiguity (\"I work at the [New York Times] ORG \" vs. \"I read the New York Times\") and sparsity induced by morphology add complexity.", "labels": [], "entities": []}, {"text": "Despite the challenges mentioned above, competent monolingual systems that rely on having sufficient annotated data, knowledge and resources available for engineering features have been developed.", "labels": [], "entities": []}, {"text": "A more challenging task is to design a model that retains competence in monolingual scenarios and can easily be transferred to a low resource language with minimum overhead in terms of data annotation requirements and feature engineering.", "labels": [], "entities": []}, {"text": "This transfer setting introduces additional challenges such as varying character usage conventions across languages with same script, differing scripts, lack of NE transliteration, varying morphology, different lexicons and mutual non-intelligibility to name a few.", "labels": [], "entities": []}, {"text": "We propose the following changes over prior work (  to address the challenges of the low-resource transfer setting.", "labels": [], "entities": [{"text": "low-resource transfer", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.73869389295578}]}, {"text": "Language universal phonological character representations instead of orthographic ones.", "labels": [], "entities": []}, {"text": "2. Attention over characters of a word while labeling it with an NE category.", "labels": [], "entities": []}, {"text": "We show that using phonological character representations instead does not negatively impact performance on two languages: Spanish and Turkish.", "labels": [], "entities": []}, {"text": "We then show that using global phonological representations enables model transfer from one or more source languages to a target language with no extra effort, even when the languages use different scripts.", "labels": [], "entities": []}, {"text": "We demonstrate that while attention over characters of words has marginal utility in monolingual and high resource settings, it greatly improves the statistical efficiency of the model in 0-shot and low resource transfer settings.", "labels": [], "entities": []}, {"text": "We do require a mapping from a language's script to phonological feature space which is script specific and not task specific.", "labels": [], "entities": []}, {"text": "This presents little or no overhead due to existence of tools like PanPhon ().", "labels": [], "entities": [{"text": "PanPhon", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9667900204658508}]}, {"text": "provides a high level overview of our model.", "labels": [], "entities": []}, {"text": "We model the words of a sentence at the type level and the token level.", "labels": [], "entities": []}, {"text": "At the type level (ignorant of sentential context), we use bidirectional character LSTMs as in to compose characters of a word to obtain its word representation and concatenate this with a word embedding that captures distributional semantics.", "labels": [], "entities": []}, {"text": "This can memorize entities or capture morphological and suffixal clues that can help at a discriminative task like NER.", "labels": [], "entities": []}, {"text": "We compose type level word representations with bidirectional LSTMs to obtain token level (cognizant of sentential context) representations.", "labels": [], "entities": []}, {"text": "Using token level word representations along with an attentional context vector for each word based on the sequence of characters it contains, we generate score functions used by a Conditional Random Field (CRF) for inference.", "labels": [], "entities": []}, {"text": "To facilitate transfer across languages with different scripts, we use Epitran 1 and PanPhon.", "labels": [], "entities": [{"text": "PanPhon", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9674660563468933}]}], "datasetContent": [{"text": "We conduct four different experiments: report results from monolingual experiments in Spanish.", "labels": [], "entities": []}, {"text": "In table 3, we report the performance of our best model against other state-ofthe-art models for the Spanish).", "labels": [], "entities": []}, {"text": "Our model performs marginally better than other benchmarks with the optimal configuration of hyper-parameters and using pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "report ablation study results, which reveal that using pre-trained word embeddings without using character LSTMs yields a very strong baseline that already out-performs various previous benchmarks.", "labels": [], "entities": []}, {"text": "Using character LSTMs that compose orthographic character representations yields a +0. 37.1 Our best transfer model* 51.2: NIST evaluations for Uyghur.", "labels": [], "entities": []}, {"text": "* indicates transfer from Uzbek and Turkish +0.12 F1 with attention.", "labels": [], "entities": [{"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.5350931286811829}]}, {"text": "Using phonological character representations instead yields an improvement of +0.47 F1 and a further +0.8 F1 with attention.", "labels": [], "entities": [{"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9990807771682739}, {"text": "F1", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.9977800250053406}]}, {"text": "Thus, phonological representations benefit more from attention applied over them to beat out orthographic representations in that scenario.", "labels": [], "entities": []}, {"text": "Using sparse features indicating the character category (alphabet vs. number vs. punctuation/non-phonetic) and capitalization in conjunction with with phonological character representations and word embeddings with attention over phonological characters yields the best configuration that slightly outperforms the best published models so far.", "labels": [], "entities": []}, {"text": "Given that many previous benchmarks used features that rely heavily on orthography, this is an encouraging result since one would expect to lose some performance by using more abstract phonological representations as explained in section 2..", "labels": [], "entities": []}, {"text": "highlight results from monolingual experiments on Turkish.", "labels": [], "entities": []}, {"text": "This dataset is much more challenging since the annotated training courpus is significantly smaller than the CoNLL 2002 shared task dataset and because Turkish is an agglutinative language exhibiting sparsity inducing morphology which leads to huge vocabulary size.", "labels": [], "entities": [{"text": "CoNLL 2002 shared task dataset", "start_pos": 109, "end_pos": 139, "type": "DATASET", "confidence": 0.9068290829658509}]}, {"text": "As a competitive baseline, we train the LSTM CRF described in ( ) due to its documented ability to obtain state-of-the-art monolingual results for many languages with minimal feature engineering.", "labels": [], "entities": [{"text": "LSTM CRF", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.6437806189060211}]}, {"text": "Our best model from the Turkish ablation study outperforms this baseline.", "labels": [], "entities": [{"text": "Turkish ablation study", "start_pos": 24, "end_pos": 46, "type": "DATASET", "confidence": 0.8921841581662496}]}, {"text": "We also see a stark contrast between the ablation study results for Turkish compared to Spanish.", "labels": [], "entities": [{"text": "ablation", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9034734964370728}]}, {"text": "Firstly, word embeddings alone perform rather poorly due to the challenges of reliably estimating them fora large vocabulary given a small dataset.", "labels": [], "entities": []}, {"text": "Character composed representations of words provide a significant performance boost (+17.27 F1 for the best model).", "labels": [], "entities": [{"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9970034956932068}]}, {"text": "Secondly, usage of sparse character features (like capitalization) seems to hurt performance in all but the last model in table 4.", "labels": [], "entities": []}, {"text": "Thirdly, phonological and orthographic character representations are complementary in the case of Turkish, unlike Spanish.", "labels": [], "entities": []}, {"text": "This is not too surprising since Turkish exhibits phonological phenomena like vowel harmony.", "labels": [], "entities": []}, {"text": "Lack of vowel harmony in a word could give-away a foreign word or a named entity for example.", "labels": [], "entities": []}, {"text": "Results show that attention is helpful as well.", "labels": [], "entities": []}, {"text": "We would also like to point out that the only models in the ablation studies eligible for transfer are those that do not use orthographic character representations.", "labels": [], "entities": []}, {"text": "Among these, the model that uses phonological representation with attention and word vectors performs the best and also outperforms the baseline system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Ablation Tests on Spanish CoNLL 2002. Bold indi-", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9950587749481201}, {"text": "Spanish CoNLL 2002", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.887833297252655}]}, {"text": " Table 3: Comparison with benchmarks. * indicates a model", "labels": [], "entities": []}, {"text": " Table 4: Ablation Tests on Turkish Bold indicates the best", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975851774215698}, {"text": "Turkish Bold", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9459838271141052}]}, {"text": " Table 7: Model Transfer from Uzbek (Source) to Turkish (Target) at different target data availability thresholds", "labels": [], "entities": []}, {"text": " Table 8: Monolingual Turkish baseline at different data availability thresholds", "labels": [], "entities": []}]}