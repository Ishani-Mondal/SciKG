{"title": [{"text": "On-and Off-Topic Classification and Semantic Annotation of User-Generated Software Requirements", "labels": [], "entities": [{"text": "Semantic Annotation of User-Generated Software Requirements", "start_pos": 36, "end_pos": 95, "type": "TASK", "confidence": 0.5601161271333694}]}], "abstractContent": [{"text": "Users prefer natural language software requirements because of their usability and accessibility.", "labels": [], "entities": []}, {"text": "When they describe their wishes for software development, they often provide off-topic information.", "labels": [], "entities": []}, {"text": "We therefore present REaCT 1 , an automated approach for identifying and semantically annotating the on-topic parts of requirement descriptions.", "labels": [], "entities": [{"text": "REaCT 1", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.643381804227829}]}, {"text": "It is designed to support requirement engineers in the elicitation process on detecting and analyzing requirements in user-generated content.", "labels": [], "entities": []}, {"text": "Since no lexical resources with domain-specific information about requirements are available, we created a corpus of requirements written in controlled language by instructed users and uncontrolled language by uninstructed users.", "labels": [], "entities": []}, {"text": "We annotated these requirements regarding predicate-argument structures, conditions , priorities, motivations and semantic roles and used this information to train clas-sifiers for information extraction purposes.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.7823419272899628}]}, {"text": "REaCT achieves an accuracy of 92% for the on-and off-topic classification task and an F 1-measure of 72% for the semantic annotation.", "labels": [], "entities": [{"text": "REaCT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6697599291801453}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9996598958969116}, {"text": "on-and off-topic classification task", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.7804843634366989}, {"text": "F 1-measure", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.989641398191452}]}], "introductionContent": [{"text": "\"Requirements are what the software product, or hardware product, or service, or whatever you intend to build, is meant to do and to be\".", "labels": [], "entities": []}, {"text": "This intuitive description of requirements has one disadvantage.", "labels": [], "entities": []}, {"text": "It is as vague as a requirement that is written by an untrained user.", "labels": [], "entities": []}, {"text": "More generally, functional requirements define what a product, system or process, or Requirements Extraction and Classification Tool apart of it is meant to do).", "labels": [], "entities": []}, {"text": "Due to its expressiveness, natural language (NL) became a popular medium of communication between users and developers during the requirement elicitation process (de Almeida Ferreira and da).", "labels": [], "entities": []}, {"text": "Especially in large ICT projects, requirements, wishes, and ideas of up to thousands of different users have to be grasped (.", "labels": [], "entities": []}, {"text": "For this purpose, requirement engineers collect their data, look for project-relevant concepts and summarize the identified technical features.", "labels": [], "entities": []}, {"text": "However, this hand-crafted aggregation and translation process from NL to formal specifications is error-prone (.", "labels": [], "entities": []}, {"text": "Since people are getting tired and unfocused during this monotonous work, the risk of information loss increases.", "labels": [], "entities": [{"text": "information loss", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.8066240847110748}]}, {"text": "Hence, this process should be automated as far as possible to support requirement engineers.", "labels": [], "entities": []}, {"text": "In this paper, we introduce our approach to identify and annotate requirements in user-generated content.", "labels": [], "entities": []}, {"text": "We acquired feature requests for open source software from SourceForge 2 , specified by (potential) users of the software.", "labels": [], "entities": []}, {"text": "We divided these requests into off-topic information and (on-topic) requirements to train a binary text classifier.", "labels": [], "entities": []}, {"text": "This allows an automated identification of new requirements in user-generated content.", "labels": [], "entities": [{"text": "identification of new requirements in user-generated content", "start_pos": 25, "end_pos": 85, "type": "TASK", "confidence": 0.7379373141697475}]}, {"text": "In addition, we collected requirements in controlled language from the NFR corpus and from web pages with user-story explanations.", "labels": [], "entities": [{"text": "NFR corpus", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.9835697710514069}]}, {"text": "We annotated the semantically rele-vant parts of the acquired requirements for information extraction purposes.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.8121862411499023}]}, {"text": "This will support requirements engineers on requirement analysis and enables a further processing such as disambiguation or the resolution of incomplete expressions.", "labels": [], "entities": [{"text": "resolution of incomplete expressions", "start_pos": 128, "end_pos": 164, "type": "TASK", "confidence": 0.8303765952587128}]}, {"text": "This paper is structured as follows: In Section 2, we discuss the notion of requirements.", "labels": [], "entities": []}, {"text": "Then we provide an overview of previous work (Section 3), before we introduce lexical resources necessary for our method (Section 4).", "labels": [], "entities": []}, {"text": "The approach itself is presented in Section 5 before it is evaluated in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude this work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned in Section 5, the data was separated in a ratio of 4:1 in a training and a test set.", "labels": [], "entities": []}, {"text": "We trained all classifiers on the training set with their defined settings from the automated algorithm configuration.", "labels": [], "entities": []}, {"text": "Subsequently, we evaluated these classifiers on the test set.", "labels": [], "entities": []}, {"text": "Our results are shown in that lists the accuracy for the best classifier per algorithm family of the on-/off-topic classification task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9995855689048767}, {"text": "on-/off-topic classification task", "start_pos": 101, "end_pos": 134, "type": "TASK", "confidence": 0.7976456403732299}]}, {"text": "The ExtraTreeClassifier performs best on the test data with an accuracy of 92%.", "labels": [], "entities": [{"text": "ExtraTreeClassifier", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.651276171207428}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9996441602706909}]}, {"text": "The accuracy was calculated with the following formula: accuracy = #true positives + #true negatives #classif ied requirements The ExtraTreeClassifier is an implementation of Extremely Randomized).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993937015533447}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995064735412598}, {"text": "ExtraTreeClassifier", "start_pos": 131, "end_pos": 150, "type": "DATASET", "confidence": 0.869101881980896}]}, {"text": "We achieved the best result when using character n-grams as features in the model with a fixed length of 4.", "labels": [], "entities": []}, {"text": "Thereby, we considered term occurrence instead of term frequency and IDF.", "labels": [], "entities": [{"text": "IDF", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9980300068855286}]}, {"text": "Before creating the bag-of-words model, the approach removes stopwords.", "labels": [], "entities": []}, {"text": "Furthermore, the frequency of the POS tags and their dependencies are used as features.", "labels": [], "entities": []}, {"text": "In total, the ExtraTreeClassifier used 167 estimators based on entropy in the ensemble (algorithmspecific parameters shows the values for precision, recall, and F 1 of the ExtraTreeClassifier.", "labels": [], "entities": [{"text": "ExtraTreeClassifier", "start_pos": 14, "end_pos": 33, "type": "DATASET", "confidence": 0.8709574937820435}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9994444251060486}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.999091625213623}, {"text": "F 1", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9944206774234772}, {"text": "ExtraTreeClassifier", "start_pos": 172, "end_pos": 191, "type": "DATASET", "confidence": 0.923613429069519}]}, {"text": "In brief, the introduced approach detects requirements in usergenerated content with an average F 1 -score of 91%.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9921872764825821}]}, {"text": "provides an overview of the results of the semantic annotation task.", "labels": [], "entities": []}, {"text": "To determine the F 1 -score, the agreement of the predicted and the a priori given annotations is necessary to count an element as true positive.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9817726612091064}, {"text": "agreement", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9824082255363464}]}], "tableCaptions": [{"text": " Table 1: Number of annotated elements per category in our", "labels": [], "entities": []}, {"text": " Table 8: Evaluation results for the on-/off-topic classification", "labels": [], "entities": []}, {"text": " Table 9: F1-scores of best classifiers per algorithm family in the", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985753297805786}]}, {"text": " Table 10: Evaluation results for the semantic annotation with", "labels": [], "entities": []}]}