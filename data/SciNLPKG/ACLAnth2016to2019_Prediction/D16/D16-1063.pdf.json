{"title": [{"text": "WordRank: Learning Word Embeddings via Robust Ranking", "labels": [], "entities": []}], "abstractContent": [{"text": "Embedding words in a vector space has gained a lot of attention in recent years.", "labels": [], "entities": []}, {"text": "While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear.", "labels": [], "entities": []}, {"text": "In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics.", "labels": [], "entities": []}, {"text": "Then, based on this insight , we propose a novel framework Wor-dRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses.", "labels": [], "entities": []}, {"text": "The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.6728231608867645}]}, {"text": "Our algorithm is very competitive to the state-of-the-arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy).", "labels": [], "entities": []}, {"text": "With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark.", "labels": [], "entities": [{"text": "WordRank", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9104909300804138}]}, {"text": "Our multi-node distributed implementation of WordRank is publicly available for general usage.", "labels": [], "entities": [{"text": "WordRank", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9594209790229797}]}], "introductionContent": [{"text": "Embedding words into a vector space, such that semantic and syntactic regularities between words are preserved, is an important sub-task for many applications of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.6589169104894003}]}, {"text": "generated considerable excitement in the machine learning and natural language processing communities by introducing a neural network based model, which they call word2vec.", "labels": [], "entities": []}, {"text": "It was shown that word2vec produces state-of-the-art performance on both word similarity as well as word analogy tasks.", "labels": [], "entities": [{"text": "word analogy tasks", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8145900368690491}]}, {"text": "The word similarity task is to retrieve words that are similar to a given word.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7127971351146698}]}, {"text": "On the other hand, word analogy requires answering queries of the form a:b;c:?, where a, b, and care words from the vocabulary, and the answer to the query must be semantically related to c in the same way as b is related to a.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.805144339799881}]}, {"text": "This is best illustrated with a concrete example: Given the query king:queen;man:?", "labels": [], "entities": []}, {"text": "we expect the model to output woman.", "labels": [], "entities": []}, {"text": "The impressive performance of word2vec led to a flurry of papers, which tried to explain and improve the performance of word2vec both theoretically () and empirically (.", "labels": [], "entities": []}, {"text": "One interpretation of word2vec is that it is approximately maximizing the positive pointwise mutual information (PMI), and showed that directly optimizing this gives good results.", "labels": [], "entities": [{"text": "positive pointwise mutual information (PMI)", "start_pos": 74, "end_pos": 117, "type": "METRIC", "confidence": 0.6071559148175376}]}, {"text": "On the other hand, showed performance comparable to word2vec by using a modified matrix factorization model, which optimizes a log loss.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, showed that much of the performance gains of these new word embedding methods are due to certain hyperparameter optimizations and system-design choices.", "labels": [], "entities": []}, {"text": "In other words, if one sets up careful experiments, then existing word embedding models more or less perform comparably to each other.", "labels": [], "entities": []}, {"text": "We conjecture that this is because, at a high level, all these methods are based on the following template: From a large text corpus eliminate infrequent words, and compute a |W| \u00d7 |C| word-context co-occurrence count matrix; a context is a word which appears less than d distance away from a given word in the text, where dis a tunable parameter.", "labels": [], "entities": []}, {"text": "Let w \u2208 W be a word and c \u2208 C be a context, and let X w,c be the (potentially normalized) co-occurrence count.", "labels": [], "entities": []}, {"text": "One learns a function f (w, c) which approximates a transformed version of X w,c . Different methods differ essentially in the transformation function they use and the parametric form off ().", "labels": [], "entities": []}, {"text": "For example,) uses f (w, c) = u w , v c where u wand v care k dimensional vectors, \u00b7, \u00b7\u00b7 denotes the Euclidean dot product, and one approximates f (w, c) \u2248 log X w,c . On the other hand, as show, word2vec can be seen as using the same f (w, c) as GloVe but trying to approximate f (w, c) \u2248 PMI(X w,c ) \u2212 log n, where PMI(\u00b7) is the pairwise mutual information and n is the number of negative samples.", "labels": [], "entities": [{"text": "PMI", "start_pos": 317, "end_pos": 320, "type": "METRIC", "confidence": 0.8559879064559937}]}, {"text": "In this paper, we approach the word embedding task from a different perspective by formulating it as a ranking problem.", "labels": [], "entities": []}, {"text": "That is, given a word w, we aim to output an ordered list (c 1 , c 2 , \u00b7 \u00b7 \u00b7 ) of context words from C such that words that cooccur with w appear at the top of the list.", "labels": [], "entities": []}, {"text": "If rank(w, c) denotes the rank of c in the list, then typical ranking losses optimize the following objective: (w,c)\u2208\u2126 \u03c1 (rank(w, c)), where \u2126 \u2282 W \u00d7 C is the set of word-context pairs that co-occur in the corpus, and \u03c1(\u00b7) is a ranking loss function that is monotonically increasing and concave (see Sec. 2 fora justification).", "labels": [], "entities": []}, {"text": "Casting word embedding as ranking has two distinctive advantages.", "labels": [], "entities": [{"text": "Casting word embedding", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8909487922986349}]}, {"text": "First, our method is discriminative rather than generative; in other words, instead of modeling (a transformation of) X w,c directly, we only aim to model the relative order of X w,\u00b7 values in each row.", "labels": [], "entities": []}, {"text": "This formulation fits naturally to popular word embedding tasks such as word similarity/analogy since instead of the likelihood of each word, we are interested in finding the most relevant words in a given context 1 . Second, casting word 1 Roughly speaking, this difference in viewpoint is analogous to the difference between pointwise loss function vs listembedding as a ranking problem enables us to design models robust to noise) and focusing more on differentiating top relevant words, a kind of attention mechanism that has been proved very useful in deep learning.", "labels": [], "entities": [{"text": "word similarity/analogy", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.837941475212574}]}, {"text": "Both issues are very critical in the domain of word embedding since (1) the co-occurrence matrix might be noisy due to grammatical errors or unconventional use of language, i.e., certain words might cooccur purely by chance, a phenomenon more acute in smaller document corpora collected from diverse sources; and (2) it's very challenging to sort out a few most relevant words from a very large vocabulary, thus some kind of attention mechanism that can trade off the resolution on most relevant words with the resolution on less relevant words is needed.", "labels": [], "entities": [{"text": "word embedding", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7084623873233795}]}, {"text": "We will show in the experiments that our method can mitigate some of these issues; with 17 million tokens our method performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we first evaluate the impact of the weight r w,c and the ranking loss function \u03c1(\u00b7) on the test performance using a small dataset.", "labels": [], "entities": [{"text": "ranking loss function \u03c1", "start_pos": 77, "end_pos": 100, "type": "METRIC", "confidence": 0.8590029925107956}]}, {"text": "We then pick the best performing model and compare it against word2vec () and GloVe ().", "labels": [], "entities": []}, {"text": "We closely follow the framework of to setup a careful and fair comparison of the three methods.", "labels": [], "entities": []}, {"text": "Our code is publicly available at https://bitbucket.", "labels": [], "entities": []}, {"text": "org/shihaoji/wordrank.", "labels": [], "entities": []}, {"text": "Training Corpus Models are trained on a combined corpus of 7.2 billion tokens, which consists of the 2015 Wikipedia dump with 1.6 billion tokens, the WMT14 News Crawl 5 with 1.7 billion tokens, the \"One Billion Word Language Modeling Benchmark\" 6 with almost 1 billion tokens, and UMBC Corpus Size 17M * 32M 64M 128M 256M 512M 1.0B 1.6B 7.2B Vocabulary Size * This is the Text8 dataset from http://mattmahoney.net/dc/text8.zip, which is widely used for word embedding demo.", "labels": [], "entities": [{"text": "WMT14 News Crawl 5", "start_pos": 150, "end_pos": 168, "type": "DATASET", "confidence": 0.962585061788559}, {"text": "UMBC Corpus Size 17M", "start_pos": 281, "end_pos": 301, "type": "DATASET", "confidence": 0.8535896390676498}, {"text": "Text8 dataset", "start_pos": 372, "end_pos": 385, "type": "DATASET", "confidence": 0.9307313561439514}]}, {"text": "webbase corpus 7 with around 3 billion tokens.", "labels": [], "entities": [{"text": "webbase corpus 7", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.883795956770579}]}, {"text": "The pre-processing pipeline breaks the paragraphs into sentences, tokenizes and lowercases each corpus with the Stanford tokenizer.", "labels": [], "entities": []}, {"text": "We further cleanup the dataset by removing non-ASCII characters and punctuation, and discard sentences that are shorter than 3 tokens or longer than 500 tokens.", "labels": [], "entities": []}, {"text": "In the end, we obtain a dataset of 7.2 billion tokens, with the first 1.6 billion tokens from Wikipedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.9417682886123657}]}, {"text": "When we want to experiment with a smaller corpus, we extract a subset which contains the specified number of tokens.", "labels": [], "entities": []}, {"text": "Co-occurrence matrix construction We use the GloVe code to construct the co-occurrence matrix X, and the same matrix is used to train GloVe and WordRank models.", "labels": [], "entities": [{"text": "GloVe code", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9218812584877014}]}, {"text": "When constructing X, we must choose the size of the vocabulary, the context window and whether to distinguish left context from right context.", "labels": [], "entities": []}, {"text": "We follow the findings and design choices of GloVe and use asymmetric window of size win with a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count.", "labels": [], "entities": []}, {"text": "Specifically, when the corpus is small (e.g., 17M, 32M, 64M) we let win = 15 and for larger corpora we let win = 10.", "labels": [], "entities": []}, {"text": "The larger window size alleviates the data sparsity issue for small corpus at the expense of adding more noise to X.", "labels": [], "entities": []}, {"text": "The parameter settings used in our experiments are summarized in.", "labels": [], "entities": []}, {"text": "Using the trained model It has been shown by that combining the u wand v c vectors with equal weights gives a small boost 7 http://ebiquity.umbc.edu/resource/html/ id/351 in performance.", "labels": [], "entities": []}, {"text": "This vector combination was originally motivated as an ensemble method (), and later provided a different interpretation of its effect on the cosine similarity function, and show that adding context vectors effectively adds first-order similarity terms to the second-order similarity function.", "labels": [], "entities": []}, {"text": "In our experiments, we find that vector combination boosts the performance in word analogy task when training set is small, but when dataset is large enough (e.g., 7.2 billion tokens), vector combination doesn't help anymore.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.8415525754292806}]}, {"text": "More interestingly, for the word similarity task, we find that vector combination is detrimental in all the cases, sometimes even substantially 8 . Therefore, we will always use u won word similarity task, and use u w + v con word analogy task unless otherwise noted.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8118717869122823}, {"text": "word similarity", "start_pos": 184, "end_pos": 199, "type": "TASK", "confidence": 0.6750611364841461}, {"text": "word analogy", "start_pos": 226, "end_pos": 238, "type": "TASK", "confidence": 0.7122721970081329}]}, {"text": "Word Similarity We use six datasets to evaluate word similarity: WS-353 ( Word Analogies For this task, we use the Google analogy dataset (.", "labels": [], "entities": [{"text": "Word Similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5775936394929886}, {"text": "Google analogy dataset", "start_pos": 115, "end_pos": 137, "type": "DATASET", "confidence": 0.7701236804326376}]}, {"text": "It contains 19544 word analogy questions, partitioned into 8869 semantic and 10675 syntactic questions.", "labels": [], "entities": [{"text": "word analogy questions", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.784133235613505}]}, {"text": "A question is correctly answered only if the algorithm selects the word that is exactly the same as the correct word in the question: synonyms are thus counted as mistakes.", "labels": [], "entities": []}, {"text": "There are two ways to answer these questions, namely, by using 3CosAdd or 3CosMul (see) for details).", "labels": [], "entities": []}, {"text": "We will report scores by using 3CosAdd by default, and indicate when 3CosMul gives better performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parameter settings used in the experiments.", "labels": [], "entities": []}, {"text": " Table 2: Performance of different \u03c1 functions on Text8 dataset with 17M tokens.", "labels": [], "entities": [{"text": "Text8 dataset", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9749234914779663}]}, {"text": " Table 3: Performance of the best word2vec, GloVe and WordRank models, learned from 7.2 billion tokens, on six similarity tasks", "labels": [], "entities": []}]}