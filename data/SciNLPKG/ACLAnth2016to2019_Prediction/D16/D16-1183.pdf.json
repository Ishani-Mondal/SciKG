{"title": [], "abstractContent": [{"text": "We present a shift-reduce CCG semantic parser.", "labels": [], "entities": []}, {"text": "Our parser uses a neural network architecture that balances model capacity and computational cost.", "labels": [], "entities": []}, {"text": "We train by transferring a model from a computationally expensive log-linear CKY parser.", "labels": [], "entities": []}, {"text": "Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse.", "labels": [], "entities": []}, {"text": "We evaluate on AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.8161273896694183}]}, {"text": "Our parser performs comparably to the CKY parser, while doing significantly fewer operations.", "labels": [], "entities": []}, {"text": "We also present results for greedy semantic parsing with a relatively small drop in performance.", "labels": [], "entities": [{"text": "greedy semantic parsing", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6070905228455862}]}], "introductionContent": [{"text": "Shift-reduce parsing is a class of parsing methods that guarantees a linear number of operations in sentence length.", "labels": [], "entities": [{"text": "Shift-reduce parsing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8215070962905884}]}, {"text": "This is a desired property for practical applications that require processing large amounts of text or real-time response.", "labels": [], "entities": []}, {"text": "Recently, such techniques were used to build state-of-the-art syntactic parsers, and have demonstrated the effectiveness of deep neural architectures for decision making in lineartime dependency parsing.", "labels": [], "entities": [{"text": "lineartime dependency parsing", "start_pos": 173, "end_pos": 202, "type": "TASK", "confidence": 0.687117209037145}]}, {"text": "In contrast, semantic parsing often relies on algorithms with polynomial number of operations, which results in slow parsing times unsuitable for practical applications.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.8605638146400452}]}, {"text": "In this paper, we apply shift-reduce parsing to semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.8113023042678833}]}, {"text": "Specifically, we study transferring a learned Combinatory Categorial) from a dynamic-programming CKY model to a shift-reduce neural network architecture.", "labels": [], "entities": []}, {"text": "We focus on the feed-forward architecture of, where each parsing step is a multi-class classification problem.", "labels": [], "entities": []}, {"text": "The state of the parser is represented using simple feature embeddings that are passed through a multilayer perceptron to select the next action.", "labels": [], "entities": []}, {"text": "While simple, the capacity of this model to capture interactions between primitive features, instead of relying on sparse complex features, has led to new state-of-the-art performance (.", "labels": [], "entities": []}, {"text": "However, applying this architecture to semantic parsing presents learning and inference challenges.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7809898555278778}]}, {"text": "In contrast to dependency parsing, semantic parsing corpora include sentences labeled with the system response or the target formal representation, and omit derivation information.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8195000290870667}, {"text": "semantic parsing", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7302937358617783}]}, {"text": "CCG induction from such data relies on latent-variable techniques and requires careful initialization (e.g.,.", "labels": [], "entities": [{"text": "CCG induction", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7953667640686035}]}, {"text": "Such feature initialization does not directly transfer to a neural network architecture with dense embeddings, and the use of hidden layers further complicates learning by adding a large number of latent variables.", "labels": [], "entities": []}, {"text": "We focus on data that includes sentence-representation pairs, and learn from a previously induced log-linear CKY parser.", "labels": [], "entities": []}, {"text": "This drastically simplifies learning, and can be viewed as bootstrapping a fast parser from a slow one.", "labels": [], "entities": []}, {"text": "While this dramatically narrows down the number of parses per sentence, it does not eliminate ambiguity.", "labels": [], "entities": []}, {"text": "In our experiments, we often get multiple correct parses, up to 49K in some cases.", "labels": [], "entities": []}, {"text": "We also observe that the CKY parser generates no parses for Some old networks remain inoperable NP /N N /N N S\\N P /(N /N ) N /N \u03bbf.A(\u03bbx.f (x) \u2227 quant(x, \u03bbf.\u03bbx.f (x)\u2227 \u03bbn.network(n) \u03bbf.\u03bbx.f (\u03bbr.remain-01(r)\u2227 \u03bbf.\u03bbx.f (x) \u2227 ARG3(x, A(\u03bbs.some(s)))) mod(x, A(\u03bbo.old(o))) ARG1(r, x)) A(\u03bbp.possible(p) \u2227 polarity(p, \u2212)\u2227 domain(p, A(\u03bbo.operate-01(o)))))", "labels": [], "entities": [{"text": "ARG3", "start_pos": 221, "end_pos": 225, "type": "DATASET", "confidence": 0.6743613481521606}, {"text": "ARG1", "start_pos": 266, "end_pos": 270, "type": "DATASET", "confidence": 0.7956836819648743}]}], "datasetContent": [{"text": "Task and Data We evaluate on AMR parsing with CCG.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8641069531440735}, {"text": "CCG", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9249401688575745}]}, {"text": "AMR is a general-purpose meaning representation, which has been used in multiple tasks (, We use the newswire portion of AMR Bank 1.0 release (LDC2014T12), which displays some of the fundamental challenges in semantic parsing, including long newswire sentences with abroad array of syntactic and semantic phenomena.", "labels": [], "entities": [{"text": "AMR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8514819741249084}, {"text": "AMR Bank 1.0 release (LDC2014T12)", "start_pos": 121, "end_pos": 154, "type": "DATASET", "confidence": 0.9060518997056144}, {"text": "semantic parsing", "start_pos": 209, "end_pos": 225, "type": "TASK", "confidence": 0.7672539949417114}]}, {"text": "We follow the standard train/dev/test split of 6603/826/823 sentences.", "labels": [], "entities": []}, {"text": "We evaluate with the SMATCH metric . Our parser is incorporated into the two-stage approach of.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.7390897274017334}]}, {"text": "The approach includes a bi-directional and deterministic conversion between AMR and lambda calculus.", "labels": [], "entities": []}, {"text": "Distant references, for example such as introduced by pronouns, are represented using Skolem IDs, globally-scoped existentiallyquantified unique IDs.", "labels": [], "entities": []}, {"text": "A derivation includes a CCG tree, which maps the sentence to an underspecified logical form, and a constant mapping, which maps underspecified elements to their fully specified form.", "labels": [], "entities": []}, {"text": "The key to the approach is the underspecified logical forms, where distant references and most relations are not fully specified, but instead represented as placeholders.", "labels": [], "entities": []}, {"text": "shows an example AMR, its lambda calculus conversion, and its underspecified logical form.", "labels": [], "entities": []}, {"text": "() use a CKY parser to identify the best CCG tree, and a factor graph for the second stage.", "labels": [], "entities": []}, {"text": "We integrate our shiftreduce parser into the two-stage setup by replacing the CKY parser.", "labels": [], "entities": []}, {"text": "We use the same CCG configuration and integrate our parser into the join probabilistic model.", "labels": [], "entities": []}, {"text": "Formally, given a sentence x, the probability of an AMR logical form z is where u is an underspecified logical form, Y(u) is the set of CCG trees with u at the root.", "labels": [], "entities": [{"text": "AMR logical form z", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.5865172147750854}]}, {"text": "We use our shift-reduce parser to compute p(y | x) and use the pre-trained model from for p(z | u, x).", "labels": [], "entities": []}, {"text": "Following, we disallow configurations that will not result in a valid AMR, and design a heuristic post-processing technique to recover a single logical form from terminal configurations that include multiple disconnected partial trees on the stack.", "labels": [], "entities": []}, {"text": "We use the recovery technique when no complete parses are available.", "labels": [], "entities": []}, {"text": "Tools We evaluate with the SMATCH metric . We use EasyCCG () for CCGBank categories (Section 4.1).", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.6524611711502075}, {"text": "EasyCCG", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.5763973593711853}]}, {"text": "We implement our system using Cornell SPF, and the deeplearning4j library.", "labels": [], "entities": [{"text": "Cornell SPF", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9356751441955566}]}, {"text": "The setup of also includes the Illinois NER and Stanford CoreNLP POS Tagger ().", "labels": [], "entities": [{"text": "Illinois NER", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9393330514431}, {"text": "Stanford CoreNLP POS Tagger", "start_pos": 48, "end_pos": 75, "type": "DATASET", "confidence": 0.9122215062379837}]}, {"text": "Parameters and Initialization We minimize our loss on a held-out 10% of the training data to tune our parameters, and train the final model on the full data.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9703826904296875}]}, {"text": "We set the number of epochs T = 3, regularization coefficient 2 = 10 \u22126 , learning rate 10 http://deeplearning4j.org/ shows development results.", "labels": [], "entities": []}, {"text": "We trained each model three times and report the best performance.", "labels": [], "entities": []}, {"text": "We observed a variance of roughly 0.5 in these runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development SMATCH results.", "labels": [], "entities": [{"text": "Development SMATCH", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.5344626009464264}]}, {"text": " Table 2: Test SMATCH results. 12", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.44178709387779236}]}]}