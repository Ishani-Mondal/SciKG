{"title": [{"text": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "labels": [], "entities": [{"text": "Human Attention in Visual Question Answering", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6907030840714773}]}], "abstractContent": [{"text": "We conduct large-scale studies on 'human at-tention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.8016571601231893}]}, {"text": "We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question.", "labels": [], "entities": []}, {"text": "Thus, we introduce the VQA-HAT (Human ATtention) dataset.", "labels": [], "entities": [{"text": "VQA-HAT (Human ATtention) dataset", "start_pos": 23, "end_pos": 56, "type": "DATASET", "confidence": 0.6186999380588531}]}, {"text": "We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualiza-tions) and quantitatively (via rank-order correlation).", "labels": [], "entities": []}, {"text": "Overall, our experiments show that current VQA attention models do not seem to be looking at the same regions as humans.", "labels": [], "entities": [{"text": "VQA attention", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.6450100839138031}]}], "introductionContent": [{"text": "It helps to pay attention.", "labels": [], "entities": []}, {"text": "Humans have the ability to quickly perceive a scene by selectively attending to parts of the image instead of processing the whole scene in its entirety).", "labels": [], "entities": []}, {"text": "Inspired by human attention, a recent trend in computer vision and deep learning is to build computational models of attention.", "labels": [], "entities": []}, {"text": "Given an input signal, these models learn to attend to parts of it for further processing and have been successfully applied in machine translation (, object recognition (), image captioning ( and visual question answering (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7956017553806305}, {"text": "object recognition", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.8048839569091797}, {"text": "image captioning", "start_pos": 174, "end_pos": 190, "type": "TASK", "confidence": 0.787525475025177}, {"text": "question answering", "start_pos": 204, "end_pos": 222, "type": "TASK", "confidence": 0.6912019848823547}]}, {"text": "In this work, we study attention for the task of Visual Question Answering (VQA).", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7682438790798187}]}, {"text": "Unlike image captioning, where a coarse understanding of an image * Denotes equal contribution. is often sufficient for producing generic descriptions, visual questions selectively target different areas of an image including background details and underlying context.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7197118401527405}]}, {"text": "This suggests that a VQA model may benefit from an explicit or implicit attention mechanism to answer a question correctly.", "labels": [], "entities": []}, {"text": "In this work, we are interested in the following questions: 1) Which image regions do humans choose to look at in order to answer questions about images?", "labels": [], "entities": []}, {"text": "2) Do deep VQA models with attention mechanisms attend to the same regions as humans?", "labels": [], "entities": []}, {"text": "We design and conduct studies to collect \"human attention maps\".", "labels": [], "entities": []}, {"text": "shows human attention maps on the same image for two different questions.", "labels": [], "entities": []}, {"text": "When asked 'What type is the surface?', humans choose to look at the floor, while attention for 'Which game is being played?' is concentrated around the player and racket.", "labels": [], "entities": []}, {"text": "These human attention maps can be used both for evaluating machine-generated attention maps and for explicitly training attention-based models.", "labels": [], "entities": []}, {"text": "First, we design game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large-scale VQA dataset (; this VQA-HAT (Human ATtention) dataset is publicly available at our project webpage 1 Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models () and a task-independent saliency baseline () against our human attention maps through visualizations and rank-order correlation.", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 156, "end_pos": 167, "type": "DATASET", "confidence": 0.9114669561386108}, {"text": "VQA-HAT (Human ATtention) dataset", "start_pos": 176, "end_pos": 209, "type": "DATASET", "confidence": 0.5782969097296397}]}, {"text": "We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human attention maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49.", "labels": [], "entities": []}, {"text": "It is well understood that task-independent saliency maps have a 'center bias').", "labels": [], "entities": []}, {"text": "After we control for this center bias, we find that the correlation of task-independent saliency is poor (as expected), while trends for machine-generated VQA-attention maps remain the same, which confirms our key finding that current VQA attention models do not seem to be looking at the same regions as humans.", "labels": [], "entities": []}], "datasetContent": [{"text": "We design and test multiple game-inspired novel interfaces for conducting large-scale human studies on AMT.", "labels": [], "entities": [{"text": "AMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.8743845224380493}]}, {"text": "Our basic interface design consists of a \"deblurring\" exercise for answering visual questions.", "labels": [], "entities": []}, {"text": "Specifically, we present subjects with a blurred image and a question about the image, and ask subjects to sharpen regions of the image that will help them answer the question correctly, in a smooth, clickand-drag, 'coloring' motion with the mouse.", "labels": [], "entities": []}, {"text": "The sharpening is gradual: successively scrubbing the same region progressively sharpens it.", "labels": [], "entities": [{"text": "sharpening", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9684523344039917}, {"text": "sharpens", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9635642170906067}]}, {"text": "We experiment with multiple variants of the data collection interface.", "labels": [], "entities": []}, {"text": "Analysis of the interfaces as well as details of the human evaluation studies conducted to converge on the final interface used for results in this main document have been included in the supplement.", "labels": [], "entities": []}, {"text": "The human evaluation studies consisted of showing these attention-sharpened images to humans and asking them to answer the question.", "labels": [], "entities": []}, {"text": "Based on these human studies, we pick the \"Blurred Image with Answer\" interface, where subjects were shown the correct answer in addition to the question and blurred image, and asked to deblur as few regions as possible such that someone can answer the question just by looking at the sharpened regions.", "labels": [], "entities": []}, {"text": "Since the payment structure on AMT encourage completing tasks as quickly as possible, this implicitly incentivizes subjects to deblur as few regions as possible.", "labels": [], "entities": []}, {"text": "Our followup human studies on these collected maps show that other subjects are able to answer questions based on these collected maps (details in supplement).", "labels": [], "entities": []}, {"text": "Thus, overall we achieve a balance between highlighting too little or too much.", "labels": [], "entities": []}, {"text": "Note that the \"Blurred Image with Answer\" interface used to collect attention maps is a verification task as opposed to actual question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7402581870555878}]}, {"text": "We show subjects an answer and ask them to sharpen regions that will help them answer the question correctly, as opposed to showing them just the question and asking them for the answer as well as relevant sharpened regions in the image (\"Blurred Image without Answer\" interface).", "labels": [], "entities": []}, {"text": "Attention maps collected via this verification task \"Blurred Image with Answer\" are more informative (in terms of human VQA accuracy) than those collected for \"Blurred Image without Answer\" -78.7% vs. 75.2%.", "labels": [], "entities": [{"text": "Blurred Image with Answer\"", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.691122043132782}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.5302165746688843}, {"text": "Blurred Image without Answer\"", "start_pos": 160, "end_pos": 189, "type": "TASK", "confidence": 0.7341103911399841}]}, {"text": "We collected human attention maps for 58475 train (out of 248349 total) and 1374 val (out of 121512 total) question-image pairs from the VQA dataset.", "labels": [], "entities": [{"text": "58475 train", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.8003601133823395}, {"text": "VQA dataset", "start_pos": 137, "end_pos": 148, "type": "DATASET", "confidence": 0.9915174245834351}]}, {"text": "This dataset is publicly available 1 . Overall, we conducted approximately 20000 Human Intelligence Tasks (HITs) on AMT, among 800 unique workers.", "labels": [], "entities": [{"text": "AMT", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.7074960470199585}]}, {"text": "shows examples of collected human attention maps.", "labels": [], "entities": []}, {"text": "To visualize the collected dataset, we cluster the human attention maps and visualize the average attention map and example questions falling in each of them for 6 selected clusters in.", "labels": [], "entities": []}], "tableCaptions": []}