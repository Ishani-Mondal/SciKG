{"title": [{"text": "Event Embeddings for Semantic Script Modeling", "labels": [], "entities": [{"text": "Semantic Script Modeling", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6767205595970154}]}], "abstractContent": [{"text": "Semantic scripts is a conceptual representation which defines how events are organized into higher level activities.", "labels": [], "entities": []}, {"text": "Practically all the previous approaches to inducing script knowledge from text relied on count-based techniques (e.g., generative models) and have not attempted to compo-sitionally model events.", "labels": [], "entities": []}, {"text": "In this work, we introduce a neural network model which relies on distributed compositional representations of events.", "labels": [], "entities": []}, {"text": "The model captures statistical dependencies between events in a scenario, overcomes some of the shortcomings of previous approaches (e.g., by more effectively dealing with data spar-sity) and outperforms count-based counterparts on the narrative cloze task.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is generally believed that the lack of knowledge on how individual events are organized into higher-level scenarios is one of the major obstacles for natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 153, "end_pos": 183, "type": "TASK", "confidence": 0.6672532856464386}]}, {"text": "Texts often do not provide a detailed specification of underlying events as writers rely on the ability of humans to read between the lines or, more specifically, on their commonsense knowledge of underlying scenarios.", "labels": [], "entities": []}, {"text": "For example, going to a restaurant involves entering the restaurant, getting seated, making an order and soon.", "labels": [], "entities": []}, {"text": "Consequently, when describing a visit to a restaurant, a writer will not specify all the events, as they are obvious to the reader.", "labels": [], "entities": []}, {"text": "This kind of knowledge is typically refereed to as semantic scripts, and, in this work, will aim to capture some aspects of this knowledge within our probabilistic model.", "labels": [], "entities": []}, {"text": "Early work on scripts focused on manual construction of knowledge bases and rule-based systems for inference using these knowledge bases (.", "labels": [], "entities": []}, {"text": "More recent approaches relied on automatically learning script knowledge either from crowd-sourced or naturally-occurring texts).", "labels": [], "entities": []}, {"text": "Most of these methods represent events as verbal predicates along with tuples of their immediate arguments (i.e. syntactic dependents of the predicate).", "labels": [], "entities": []}, {"text": "These approaches model statistical dependencies between events (or, more formally, mentions of events) in a document, often restricting their model to capturing dependencies only between events sharing at least one entity (a common protagonist).", "labels": [], "entities": []}, {"text": "We generally follow this tradition in our approach.", "labels": [], "entities": []}, {"text": "Much of this previous work has focused on count-based techniques using, for example, either the generative framework ( or relying on information-theoretic measures such as pointwise mutual information (PMI).", "labels": [], "entities": []}, {"text": "Some of these techniques treat predicate-argument structures as anatomic whole (e.g.,), in other words their probability estimates are based on cooccurrences of entire (predicate, arguments) tuples.", "labels": [], "entities": []}, {"text": "Clearly such methods fail to adequately take into account compositional nature of expressions used to refer to events and suffer from data sparsity.", "labels": [], "entities": []}, {"text": "In this work our goal is to overcome the shortcomings of the count-based methods described above by representing events as real-valued vectors (event embeddings), with the embeddings computed in a compositional way relying on the predicate and its arguments.", "labels": [], "entities": []}, {"text": "These embeddings capture semantic properties of events: events which differ in surface forms of their constituents but are semantically similar will get similar embeddings.", "labels": [], "entities": []}, {"text": "The event embeddings are used and estimated within our probabilistic model of semantic scripts.", "labels": [], "entities": []}, {"text": "We evaluate our model on predicting left-out events (the narrative cloze task) where it outperforms existing count-based methods.", "labels": [], "entities": [{"text": "predicting left-out events", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.8503991564114889}]}], "datasetContent": [{"text": "We evaluated models for narrative cloze task with three metrics Recall@50 and Accuracy and Event Perplexity.", "labels": [], "entities": [{"text": "Recall@50", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.8980289697647095}, {"text": "Accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9978335499763489}]}, {"text": "Recall@50 is the standard metric used for evaluating script models ().", "labels": [], "entities": [{"text": "Recall@50", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8750101327896118}]}, {"text": "The idea here is to evaluate top 50 predictions of a script model on a test script with a missing event.", "labels": [], "entities": []}, {"text": "The metric is calculated as fraction of the predictions containing the gold held-out event.", "labels": [], "entities": []}, {"text": "Its value lies in the range 0 (worst) and 1(best).", "labels": [], "entities": []}, {"text": "Accuracy is anew metric introduced by.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.995577335357666}]}, {"text": "This metric evaluates the event prediction, taking into account prediction of each constituent.", "labels": [], "entities": [{"text": "event prediction", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.6808797419071198}]}, {"text": "Specifically, it is defined as average of the accuracy of the predicate, the dependency, the first argument and the second argument predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9992457628250122}]}, {"text": "This is a more robust metric as it does not treat an event as anatomic unit.", "labels": [], "entities": []}, {"text": "This is in contrast to Recall@50 which penalizes semantically correct guesses and awards only events which have exactly the same surface form.", "labels": [], "entities": []}, {"text": "The baseline models and our model are probabilistic by nature.", "labels": [], "entities": []}, {"text": "Taking inspiration from language modeling community, we propose anew metric Event Perplexity.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7304911613464355}]}, {"text": "We define event perplexity as 2 \u2212 1 . The perplexity measure, like the accuracy takes into account the constituents of an event and is a good indicator of the model predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9972122311592102}]}, {"text": "Narrative Cloze task was tested on 29,943 test set scripts.", "labels": [], "entities": [{"text": "Narrative Cloze task", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6796122193336487}]}, {"text": "The results are shown in and 3.", "labels": [], "entities": []}, {"text": "We evaluated with two versions of the cloze task.", "labels": [], "entities": []}, {"text": "In the first version, events are the predicate argument tuple as defined before.", "labels": [], "entities": []}, {"text": "Second version, evaluates on predicates only i.e. an event is not a tuple but only a predicate.", "labels": [], "entities": []}, {"text": "Our model, NNSM outperforms both the unigram and M-Pr models on both the versions of the task with all the metrics.", "labels": [], "entities": []}, {"text": "This further strengthens our hypothesis of: Model evaluation on predicate only event test set for narrative cloze task against the baselines having distributed representation for events rather than atomic representations.", "labels": [], "entities": []}, {"text": "Unigram, although a simple model, is competitive with the M-Pr model.", "labels": [], "entities": [{"text": "Unigram", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8701323866844177}]}, {"text": "We performed an interesting experiment.", "labels": [], "entities": []}, {"text": "We evaluated another simplistic baseline model, most frequent event.", "labels": [], "entities": []}, {"text": "This baseline model predicts by sampling from top-5 most frequent predicate/argument in the full event narrative cloze task.", "labels": [], "entities": []}, {"text": "Surprisingly, the accuracy reported by this simple baseline is 45.04% which is slightly more than our best performing NNSM model and much more than the M-Pr baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.999702513217926}]}, {"text": "This simple looking baseline is hard to beat by both count-based methods and NNSM.", "labels": [], "entities": [{"text": "NNSM", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9230627417564392}]}, {"text": "None of the previous methods have been evaluated against this baseline.", "labels": [], "entities": []}, {"text": "We propose using this baseline for evaluation of script models.", "labels": [], "entities": []}, {"text": "We think this outperformance is due to skewed distribution of the predicate and arguments in the corpus.", "labels": [], "entities": []}, {"text": "As we found empirically, these distributions have a very long tail and this makes it hard for the models to beat the most frequent baseline.", "labels": [], "entities": []}, {"text": "Similar to narrative cloze, adversarial narrative cloze task was evaluated on 29,943 test set scripts.", "labels": [], "entities": []}, {"text": "In each of the event sequence an event was replaced by a random event.", "labels": [], "entities": []}, {"text": "The results for the adversarial narrative cloze task are shown in and 5.", "labels": [], "entities": []}, {"text": "As evident from the results Unigram model is as good as random.", "labels": [], "entities": []}, {"text": "In this task as well, our model outperforms the count based M-Pr model by 2.3% and 2.9% for full and pred model respec-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Model evaluation on test set for narrative  cloze task against the baselines", "labels": [], "entities": []}, {"text": " Table 3: Model evaluation on predicate only event  test set for narrative cloze task against the base- lines", "labels": [], "entities": []}, {"text": " Table 4: Model evaluation on test set for adversar- ial narrative cloze task against the baselines", "labels": [], "entities": []}, {"text": " Table 5: Model evaluation on predicate only test  set for adversarial narrative cloze task against the  baselines", "labels": [], "entities": []}]}