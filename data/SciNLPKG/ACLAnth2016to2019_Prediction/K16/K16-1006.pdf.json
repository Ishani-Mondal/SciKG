{"title": [{"text": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM", "labels": [], "entities": [{"text": "Learning Generic Context Embedding", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.6250560656189919}]}], "abstractContent": [{"text": "Context representations are central to various NLP tasks, such as word sense disam-biguation, named entity recognition, co-reference resolution, and many more.", "labels": [], "entities": [{"text": "word sense disam-biguation", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.6937282681465149}, {"text": "named entity recognition", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.6535803973674774}, {"text": "co-reference resolution", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.734053835272789}]}, {"text": "In this work we present a neural model for efficiently learning a generic context embedding function from large corpora, using bidirectional LSTM.", "labels": [], "entities": []}, {"text": "With a very simple application of our context representations , we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word em-beddings.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.720380112528801}, {"text": "lexical substitution", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.7289233207702637}, {"text": "word sense disambiguation tasks", "start_pos": 175, "end_pos": 206, "type": "TASK", "confidence": 0.7566225603222847}]}, {"text": "We release our code and pre-trained models, suggesting they could be useful in a wide variety of NLP tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generic word embeddings capture semantic and syntactic information about individual words in a compact low-dimensional representation.", "labels": [], "entities": []}, {"text": "While they are trained to optimize a generic taskindependent objective function, word embeddings were found useful in abroad range of NLP tasks, making an overall huge impact in recent years.", "labels": [], "entities": []}, {"text": "A major advancement in this field was the introduction of highly efficient models, such as word2vec () and GloVe), for learning generic word embeddings from very large corpora.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.8589992523193359}]}, {"text": "Capturing information from such corpora substantially increased the value of word embeddings to both unsupervised and semi-supervised NLP tasks.", "labels": [], "entities": []}, {"text": "To make inferences regarding a concrete target word instance, good representations of both the target word type and the given context are helpful.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"I can't find\", we need to consider both the target word April and its context \"I can't find [ ]\" to infer that April probably refers to a person.", "labels": [], "entities": []}, {"text": "This principle applies to various tasks, including word sense disambiguation, co-reference resolution and named entity recognition (NER).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7337285677591959}, {"text": "co-reference resolution", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.720512643456459}, {"text": "named entity recognition (NER)", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.7560170938571295}]}, {"text": "Like target words, contexts are commonly represented via word embeddings.", "labels": [], "entities": []}, {"text": "In an unsupervised setting, such representations were found useful for measuring context-sensitive similarity, word sense disambiguation (), word sense induction, lexical substitution (), sentence completion ( and more.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.6783976753552755}, {"text": "word sense induction", "start_pos": 141, "end_pos": 161, "type": "TASK", "confidence": 0.7193002700805664}, {"text": "sentence completion", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7630798816680908}]}, {"text": "The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings.", "labels": [], "entities": []}, {"text": "We note that such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole.", "labels": [], "entities": []}, {"text": "In supervised settings, various NLP systems use labeled data to learn how to consider context word representations in a more optimized task-specific way.", "labels": [], "entities": []}, {"text": "This was done in tasks, such as chunking, NER, semantic role labeling, and co-reference resolution (, mostly by considering the embeddings of words in a window around the target of interest.", "labels": [], "entities": [{"text": "chunking", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.9657018780708313}, {"text": "semantic role labeling", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.6599898437658945}, {"text": "co-reference resolution", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7422786355018616}]}, {"text": "More recently, bidirectional recurrent neural networks, and specifically bidirectional LSTMs, were used in such tasks to learn internal representations of wider sentential contexts (.", "labels": [], "entities": []}, {"text": "Since supervised data is usually limited in size, it has been shown that training such systems, using word embeddings that were pre-trained on large corpora, improves performance significantly.", "labels": [], "entities": []}, {"text": "Yet, pre-trained word embeddings carry limited information regarding the inter-dependencies between target words and their sentential context as a whole.", "labels": [], "entities": []}, {"text": "To model this (and more), the supervised systems still need to rely heavily on their albeit limited supervised data.", "labels": [], "entities": []}, {"text": "In this work we present context2vec, an unsupervised model and toolkit 1 for efficiently learning generic context embedding of wide sentential contexts, using bidirectional LSTM.", "labels": [], "entities": []}, {"text": "Essentially, we use large plain text corpora to learn a neural model that embeds entire sentential contexts and target words in the same low-dimensional space, which is optimized to reflect inter-dependencies between targets and their entire sentential context as a whole.", "labels": [], "entities": []}, {"text": "To demonstrate their high quality, we show that with a very simple application of our context representations, we are able to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the common average-of-word-embeddings representation (denoted AWE).", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 178, "end_pos": 197, "type": "TASK", "confidence": 0.7249943315982819}, {"text": "lexical substitution", "start_pos": 199, "end_pos": 219, "type": "TASK", "confidence": 0.7137530148029327}, {"text": "word sense disambiguation tasks", "start_pos": 224, "end_pos": 255, "type": "TASK", "confidence": 0.7368757054209709}, {"text": "AWE", "start_pos": 353, "end_pos": 356, "type": "METRIC", "confidence": 0.980290412902832}]}, {"text": "We further hypothesize that both unsupervised and semi-supervised systems may benefit from using our pre-trained models, instead or in addition to individual pre-trained word embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We intend context2vec's generic context embedding function to be integrated into various more optimized task-specific systems.", "labels": [], "entities": []}, {"text": "However, to demonstrate its qualities independently, we address three different types of tasks by the simple means of measuring cosine distances between its embedded representations.", "labels": [], "entities": []}, {"text": "Yet, we compare our performance against the state-of-the-art results of highly competitive task-optimized systems on each task.", "labels": [], "entities": []}, {"text": "In addition we use AWE as a baseline representing a commonly used generic context representation, which like ours, can represent variable-length contexts with a fixed-size vector.", "labels": [], "entities": []}, {"text": "Our evaluation includes the following tasks: sentence completion, lexical substitution and supervised word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7840375304222107}, {"text": "lexical substitution", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.718813106417656}, {"text": "word sense disambiguation (WSD)", "start_pos": 102, "end_pos": 133, "type": "TASK", "confidence": 0.7081832041343054}]}, {"text": "The hyperparameters used in our reported experiments with context2vec are summarized in Table 5.", "labels": [], "entities": []}, {"text": "In preliminary development experiments, we used only 200 units for representing sentential contexts, and then saw significant improvement in results, when moving to 600 units.", "labels": [], "entities": []}, {"text": "Increasing the representation size to 1,000 units did not seem to further improve results.", "labels": [], "entities": []}, {"text": "With mini-batches of 1,000 sentences at a time, we started by training our models with a single iteration over the 2-billion-word ukWaC corpus.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 130, "end_pos": 142, "type": "DATASET", "confidence": 0.986810028553009}]}, {"text": "This took \u223c30 hours, using a single Tesla K80 GPU.", "labels": [], "entities": []}, {"text": "For the smaller 50-million-word MSCC learning corpus, a full iteration with a batch size of 100 took only about 3 hours.", "labels": [], "entities": [{"text": "MSCC learning corpus", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.6607382893562317}]}, {"text": "For this corpus, we started with 5 training iterations.", "labels": [], "entities": []}, {"text": "To explore the rare-word bias effect of the vocabulary smoothing factor \u03b1, we varied its value in our development experiments.", "labels": [], "entities": [{"text": "vocabulary smoothing", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.6547395586967468}]}, {"text": "The results appear in on the left hand side.", "labels": [], "entities": []}, {"text": "Since we preferred to keep our model as simple as possible, based on these results, we chose the single: Development set results.", "labels": [], "entities": []}, {"text": "iters+ denotes the best model found when running more training iterations with \u03b1 = 0.75.", "labels": [], "entities": []}, {"text": "AWE config: W5/sent denotes using a 5-word-window/full-sentence, and stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.", "labels": [], "entities": [{"text": "AWE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9312301874160767}]}, {"text": "value \u03b1 = 0.75 for all of our test sets experiments.", "labels": [], "entities": []}, {"text": "With this choice, we also tried training our models with more iterations and found that with 3 iterations over the ukWaC corpus and 10 iterations over the MSCC corpus we can obtain some further improvement in results, see iters+ in.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.992533951997757}, {"text": "MSCC corpus", "start_pos": 155, "end_pos": 166, "type": "DATASET", "confidence": 0.9598848521709442}]}, {"text": "The results of our experiments with all of the AWE variants, described in section 3.2, appear on the right hand side of.", "labels": [], "entities": []}, {"text": "For brevity, we report only the best and worst configuration for each benchmark.", "labels": [], "entities": []}, {"text": "As can be seen, in two out of four benchmarks, a window of 5 words yields better performance than a full sentential context, suggesting that the AWE representation is not very successful in leveraging effectively long range information.", "labels": [], "entities": []}, {"text": "Removing stop words or using tf-idf weights improves performance significantly.", "labels": [], "entities": []}, {"text": "However, the results are still much lower than the ones achieved with context2vec.", "labels": [], "entities": []}, {"text": "To raise the bar, in each test-set experiment we used the best AWE configuration found for the corresponding development-set experiment.", "labels": [], "entities": [{"text": "AWE", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9980382323265076}]}], "tableCaptions": [{"text": " Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with \u03b1 = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.", "labels": [], "entities": [{"text": "AWE", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.784485936164856}]}]}