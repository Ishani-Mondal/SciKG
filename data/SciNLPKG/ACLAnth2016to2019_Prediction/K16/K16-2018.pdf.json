{"title": [{"text": "Discourse Relation Sense Classification with Two-Step Classifiers", "labels": [], "entities": [{"text": "Discourse Relation Sense Classification", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8360872343182564}]}], "abstractContent": [{"text": "Discourse Relation Sense Classification is the classification task of assigning a sense to discourse relations, and is apart of the series of tasks in discourse parsing.", "labels": [], "entities": [{"text": "Discourse Relation Sense Classification", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8196648061275482}, {"text": "discourse parsing", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7060229182243347}]}, {"text": "This paper analyzes the characteristics of the data we work with and describes the system we submitted to the CoNLL-2016 Shared Task.", "labels": [], "entities": [{"text": "CoNLL-2016 Shared Task", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.7128809094429016}]}, {"text": "Our system uses two sets of two-step classifiers for Explicit and AltLex relations and Implicit and EntRel relations, respectively.", "labels": [], "entities": []}, {"text": "Regardless of the simplicity of the implementation, it achieves competitive performance using minimalistic features.", "labels": [], "entities": []}, {"text": "The submitted version of our system ranked 8th with an overall F 1 score of 0.5188.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9928191105524699}]}, {"text": "The evaluation on the test dataset achieved the best performance for Explicit relations with an F 1 score of 0.9022.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9919053117434183}]}], "introductionContent": [{"text": "In the CoNLL-2015 Shared Task on Shallow Discourse Parsing (, all the participants adopted some variation of the pipeline architecture proposed by.", "labels": [], "entities": [{"text": "Shallow Discourse Parsing", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6033166448275248}]}, {"text": "Among the components of the architecture, the main challenges are the exact argument extraction and Non-Explicit sense classification (.", "labels": [], "entities": [{"text": "argument extraction", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.6703237891197205}, {"text": "Non-Explicit sense classification", "start_pos": 100, "end_pos": 133, "type": "TASK", "confidence": 0.6309296588102976}]}, {"text": "Argument extraction is a task to identify two argument spans fora given discourse relation.", "labels": [], "entities": [{"text": "Argument extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7113677263259888}]}, {"text": "Although the reported scores were relatively low for these components this is partially because of the \"quite harsh\" evaluation  introduction of anew evaluation criterion based on partial argument matching in the CoNLL-2016 Shared Task.", "labels": [], "entities": [{"text": "CoNLL-2016 Shared Task", "start_pos": 213, "end_pos": 235, "type": "DATASET", "confidence": 0.8007059892018636}]}, {"text": "On the other hand, the sense classification components, which assign a sense to each discourse relation, continue to perform poorly.", "labels": [], "entities": [{"text": "sense classification", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7099205851554871}]}, {"text": "In particular, Non-Explicit sense classification is a difficult task, and even the best system achieved an F 1 score of only 0.42 given the gold standard argument pairs without error propagation (.", "labels": [], "entities": [{"text": "Non-Explicit sense classification", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.6650050580501556}, {"text": "F 1 score", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9909698963165283}]}, {"text": "In response to this situation, Discourse Relation Sense Classification has become a separate task in the.", "labels": [], "entities": [{"text": "Discourse Relation Sense Classification", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.8822962194681168}]}, {"text": "In this task, participants implement a system that takes gold standard argument pairs and assigns a sense to each of them.", "labels": [], "entities": []}, {"text": "To tackle this task, we first analyzed the characteristics of the discourse relation data.", "labels": [], "entities": []}, {"text": "We then implemented a classification system based on the analysis.", "labels": [], "entities": []}, {"text": "One of the distinctive points of our system is that, compared to existing systems, it uses smaller number of features, which enables the source code to be quite short and clear, and the training time to be fast.", "labels": [], "entities": []}, {"text": "The performance is nonetheless competitive, and its potential for improvement is also promising owing to the short program.", "labels": [], "entities": []}, {"text": "This paper aims to reorganize the ideas about what this task actually involves, and to show the future direction for improvement.", "labels": [], "entities": []}, {"text": "It is organized as follows: Section 2 presents the data analysis.", "labels": [], "entities": []}, {"text": "Then the implementation of the system we submitted is described in Section 3.", "labels": [], "entities": []}, {"text": "The experimental results and the conclusion are provided in Section 4 and 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained our system on the official training dataset of the CoNLL-2016 Shared Task, and evaluated it on several test datasets.", "labels": [], "entities": [{"text": "official training dataset of the CoNLL-2016 Shared Task", "start_pos": 29, "end_pos": 84, "type": "DATASET", "confidence": 0.7807851731777191}]}, {"text": "We implemented SVM classifiers, which are popular among various NLP tasks, and MaxEnt classifiers, which have been used in the previous studies.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7283442616462708}]}, {"text": "Both are implemented using scikit-learn (Pedregosa et al., 2011), with the default parameters except for the automated weight balancing between classes (class weight='balanced') in order to overcome the imbalance of the data distribution 2 . In the balanced mode, the weights of samples are automatically adjusted inversely proportional to class frequencies in the input data.", "labels": [], "entities": []}, {"text": "We: Experimental results using the two datasets.", "labels": [], "entities": []}, {"text": "F 1 scores are shown.", "labels": [], "entities": [{"text": "F 1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9276999533176422}]}, {"text": "\"Maj\" = majority classifier for Explicit and AltLex relations.", "labels": [], "entities": []}, {"text": "\"CS\" = substitution of unknown AltLex connectives.", "labels": [], "entities": []}, {"text": "\"IE\" = Implicit vs. EntRel classification before Implicit sense classification.", "labels": [], "entities": [{"text": "IE", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9746468663215637}, {"text": "Implicit sense classification", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.5462966561317444}]}, {"text": "\"CC\" = Concession vs. Contrast classification after Explicit and AltLex sense classification.", "labels": [], "entities": []}, {"text": "also attempted hyperparameter tuning using the development dataset, but the performance was almost the same.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.6902806311845779}]}, {"text": "As a baseline, the majority classifier described in Section 2.1 is used for Explicit and AltLex relations, and an SVM classifier is used for Implicit and EntRel relations.", "labels": [], "entities": []}, {"text": "The features for the SVM classifier were bag-of-words of Arg1 and Arg2 texts.", "labels": [], "entities": []}, {"text": "The system used in the official evaluation on TIRA was an old version because of deployment problems.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.7581754922866821}]}, {"text": "This means it is almost the same as the baseline system, except that the MPQA subjectivity lexicon is added as features.", "labels": [], "entities": [{"text": "MPQA subjectivity lexicon", "start_pos": 73, "end_pos": 98, "type": "DATASET", "confidence": 0.8399027784665426}]}, {"text": "The systems are evaluated using the script provided by the CoNLL-2016 Shared Task organizers.", "labels": [], "entities": [{"text": "CoNLL-2016 Shared Task organizers", "start_pos": 59, "end_pos": 92, "type": "DATASET", "confidence": 0.8888906687498093}]}, {"text": "The official evaluation is carried out on TIRA ().", "labels": [], "entities": [{"text": "TIRA", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6096502542495728}]}, {"text": "lists the F 1 scores our systems achieved in the evaluation using the test and blind-test datasets.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9761987527211508}]}, {"text": "In the first column, \"CS\" indicates the substitution of unknown AltLex connectives.", "labels": [], "entities": []}, {"text": "\"IE\" indicates that the Implicit vs EntRel classifier was used, and \"CC\" indicates the Concession vs.", "labels": [], "entities": [{"text": "IE", "start_pos": 1, "end_pos": 3, "type": "METRIC", "confidence": 0.9794552326202393}]}], "tableCaptions": [{"text": " Table 2: Experimental results using the two datasets. F 1 scores are shown. \"Maj\" = majority classifier for  Explicit and AltLex relations. \"CS\" = substitution of unknown AltLex connectives. \"IE\" = Implicit vs.  EntRel classification before Implicit sense classification. \"CC\" = Concession vs. Contrast classification  after Explicit and AltLex sense classification.", "labels": [], "entities": [{"text": "F 1", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.96793532371521}]}, {"text": " Table 3: Experimental results using different sets of the features. F 1 scores are shown. Feature 1 =  tokens in argument texts. Feature 2 = parse tree nodes of argument texts. Feature 3 = MPQA subjectivity  lexicon. All classifiers share these features, and they also use connective words as a feature.", "labels": [], "entities": [{"text": "F 1", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9728684425354004}]}, {"text": " Table 4: Preprocessing results on AltLex relations with unknown connective words.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9391608238220215}]}]}