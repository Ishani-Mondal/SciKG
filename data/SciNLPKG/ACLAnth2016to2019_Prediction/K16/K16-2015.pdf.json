{"title": [{"text": "IIT (BHU) Submission for the CoNLL-2016 Shared Task: Shallow Discourse Parsing using Semantic Lexicon", "labels": [], "entities": [{"text": "IIT (BHU)", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8009815216064453}]}], "abstractContent": [{"text": "This paper describes the Shallow Discourse Parser (SDP) submitted as apart of the Shared Task of CoNLL, 2016.", "labels": [], "entities": [{"text": "Shallow Discourse Parser (SDP)", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.7440517544746399}, {"text": "Shared Task of CoNLL, 2016", "start_pos": 82, "end_pos": 108, "type": "DATASET", "confidence": 0.5663313716650009}]}, {"text": "The discourse parser takes newswire text as input and outputs relations between various components of the text.", "labels": [], "entities": []}, {"text": "Our system is a pipeline of various sub-tasks which have been elaborated in the paper.", "labels": [], "entities": []}, {"text": "We choose a data driven approach for each task and put a special focus on utilizing the resources allowed by the organizers for creating novel features.", "labels": [], "entities": []}, {"text": "We also give details of various experiments with the dataset and the lexicon provided for the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Shallow Discourse Parsing (SDP) is a linguistic task that identifies semantic relations between a pair of lexical units in apiece of discourse.", "labels": [], "entities": [{"text": "Shallow Discourse Parsing (SDP)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7773379782835642}]}, {"text": "Discourse relation is defined by three entities: a connective, a pair of lexical units between which the relation exists and the type or sense of relation between them).", "labels": [], "entities": []}, {"text": "The discourse relations can be explicit, in which relations are expressed by certain words or phrases, or implicit, where words are not directly used to convey the relation, but instead, the meaning is implied.", "labels": [], "entities": []}, {"text": "These words or phrases which convey the existence of a discourse relation directly are called connectives.", "labels": [], "entities": []}, {"text": "The lexical units between which relation exists, could be a pair of clauses, a pair of sentences or even multiple sentences which can be adjacent or non-adjacent.", "labels": [], "entities": []}, {"text": "A discourse treebank called the Penn Discourse TreeBank or PDTB () serves as the gold standard for this task and is used as training data.", "labels": [], "entities": [{"text": "Penn Discourse TreeBank or PDTB", "start_pos": 32, "end_pos": 63, "type": "DATASET", "confidence": 0.9276208758354187}]}, {"text": "The output of our system follows the same format as PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9504572153091431}]}, {"text": "Development data is also provided to perform experiments on the system.", "labels": [], "entities": []}, {"text": "Phrase structure and dependency parses of both the training and development data have also been provided to assist in the task.", "labels": [], "entities": [{"text": "Phrase", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9326022267341614}]}, {"text": "Further details of the Shared Task can be found in the overview paper (.", "labels": [], "entities": [{"text": "Shared Task", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.8507667481899261}]}, {"text": "Final evaluation of the parser is on test and blind data sets through TIRA platform setup by).", "labels": [], "entities": []}, {"text": "Besides automating the submission and evaluation system, TIRA also has provision for plagiarism detection, author identification and author profiling.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.49958574771881104}, {"text": "plagiarism detection", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.7322729825973511}, {"text": "author identification", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.8159661889076233}, {"text": "author profiling", "start_pos": 133, "end_pos": 149, "type": "TASK", "confidence": 0.7085000872612}]}, {"text": "The SDP task can be broadly classified into two categories of explicit and non-explicit relation detection.", "labels": [], "entities": [{"text": "SDP task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.9201520383358002}, {"text": "relation detection", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.733967661857605}]}, {"text": "We discuss the pipeline for explicit parser in section 2 and non-explicit parser in Section 3.", "labels": [], "entities": []}, {"text": "Various results and experiments carried out are reported in the relevant sub-sections.", "labels": [], "entities": []}, {"text": "These results are based on individual stages without error propagation from previous stages, unless specified otherwise.", "labels": [], "entities": []}, {"text": "We report results on test and blind datasets and conclude our work in Section 4 and 5 respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a combination of the features described above to gauge their performance on sense classification task.", "labels": [], "entities": [{"text": "sense classification task", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.8579272031784058}]}, {"text": "VerbNet and Subjectivity features are known to perform well according to previous literature.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8801892995834351}]}, {"text": "Hence, we test the novel Word2Vec features on top of baseline features, Subjectivity strength and VerbNet classes.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9391718506813049}]}, {"text": "For this reason, we call the combination of baseline features, Subjectivity strength and VerbNet classes as baseline in.", "labels": [], "entities": []}, {"text": "The results reported the: Implicit sense feature experimentation on development dataset contains the results of our updated system on development, test and blind datasets.", "labels": [], "entities": []}, {"text": "In the updated system, we fixed a small bug in argument index alignment code which doubled our overall parser F1 score on the development data.", "labels": [], "entities": [{"text": "argument index alignment", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.585076371828715}, {"text": "F1 score", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.8843581676483154}]}, {"text": "Hence, we report the updated results in the paper.", "labels": [], "entities": []}, {"text": "We also used Word2Vec features in our updated system.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.9381492733955383}]}, {"text": "The Word2Vec features did not improve the F1 score of Implicit Sense Classification on development and test datasets.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9865592122077942}, {"text": "Implicit Sense Classification", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.5756734410921732}]}, {"text": "This is probably because of error propagation from previous stages.", "labels": [], "entities": []}, {"text": "Surprisingly, the updated Implicit Classifier performs better on blind dataset as compared to development and test dataset.", "labels": [], "entities": []}, {"text": "There are several weak links in our pipeline.", "labels": [], "entities": []}, {"text": "For instance, the PS-Explicit and Implicit argument extractors are naive and hard-coded.", "labels": [], "entities": [{"text": "Implicit argument extractors", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.5127390027046204}]}, {"text": "This is one major cause of low F1 scores as compared to Wang et al.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9757238924503326}]}, {"text": "We feel that by fixing these links, we can improve the result by a significant margin.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: System performance and comparison on development, test and blind datasets", "labels": [], "entities": []}]}