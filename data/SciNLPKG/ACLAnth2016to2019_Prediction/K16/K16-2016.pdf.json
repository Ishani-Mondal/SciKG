{"title": [{"text": "The Virginia Tech System at CoNLL-2016 Shared Task on Shallow Discourse Parsing", "labels": [], "entities": [{"text": "Virginia Tech System at CoNLL-2016", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.926924753189087}, {"text": "Shallow Discourse Parsing", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.6506431698799133}]}], "abstractContent": [{"text": "This paper presents the Virginia Tech system that participated in the CoNLL-2016 shared task on shallow discourse parsing.", "labels": [], "entities": [{"text": "Virginia Tech system", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.9305017987887064}, {"text": "shallow discourse parsing", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.5416828393936157}]}, {"text": "We describe our end-to-end discourse parser that builds on the methods shown to be successful in previous work.", "labels": [], "entities": []}, {"text": "The system consists of several components, such that each module performs a specific sub-task, and the components are organized in a pipeline fashion.", "labels": [], "entities": []}, {"text": "We also present our efforts to improve several components-explicit sense classification and argument boundary identification for explicit and implicit arguments-and present evaluation results.", "labels": [], "entities": [{"text": "argument boundary identification", "start_pos": 92, "end_pos": 124, "type": "TASK", "confidence": 0.7056257128715515}]}, {"text": "In the closed evaluation, our system obtained an F1 score of 20.27% on the blind test.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9882694780826569}]}], "introductionContent": [{"text": "The CoNLL-2016 shared task on shallow discourse parsing is an extension of last year's competition where participants built end-to-end discourse parsers.", "labels": [], "entities": [{"text": "shallow discourse parsing", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.5676110585530599}]}, {"text": "In this paper, we present the Virginia Tech system that participated in the CoNLL-2016 shared task.", "labels": [], "entities": [{"text": "Virginia Tech system", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9469950397809347}]}, {"text": "Our system is based on the methods and approaches introduced in earlier work that focused on developing individual components of an end-to-end shallow discourse parsing system, as well as the overall architecture ideas that were introduced and proved to be successful in the competition last year.", "labels": [], "entities": [{"text": "shallow discourse parsing system", "start_pos": 143, "end_pos": 175, "type": "TASK", "confidence": 0.8032915741205215}]}, {"text": "Our discourse parser consists of multiple components that are organized using a pipeline architecture.", "labels": [], "entities": []}, {"text": "We also present novel features -for the explicit sense classifier and argument extractorsthat show improvement over the respective components of state-of-the-art systems submitted last year.", "labels": [], "entities": []}, {"text": "In the closed evaluation track, our system achieved an F1 score of 20.27% on the official blind test set.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.989940881729126}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the shared task.", "labels": [], "entities": []}, {"text": "In Section 3, we present our system architecture.", "labels": [], "entities": []}, {"text": "In Section 4, each component is described in detail.", "labels": [], "entities": []}, {"text": "The official evaluation results are presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation in the shared task is conducted using anew web service called TIRA ().", "labels": [], "entities": [{"text": "TIRA", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.8226364254951477}]}, {"text": "We first evaluate the contribution of new features in individual components in 5.1.", "labels": [], "entities": []}, {"text": "In 5.2, we report performance of all components of the final system on the development set using gold.", "labels": [], "entities": []}, {"text": "Finally, in 5.3, we show official results on the development, test, and blind test sets.", "labels": [], "entities": []}, {"text": "Since the system is implemented as a pipeline, each component contributes errors.", "labels": [], "entities": []}, {"text": "We refer to the results as no error propagation (EP) when gold predictions are used, or with EP when automatic predictions generated from previous steps are employed.", "labels": [], "entities": [{"text": "no error propagation (EP)", "start_pos": 27, "end_pos": 52, "type": "METRIC", "confidence": 0.7590304762125015}, {"text": "EP", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9427810311317444}]}, {"text": "The components of our final system are trained as follows: connective, position classifier, SS Arg1/Arg2 extractor and implicit Arg2 extractor (Maximum Entropy); explicit sense, PS Arg1, PS Arg2 extractors, Implicit Arg1 extractor (Averaged Perceptron); non-explicit sense (Na\u00a8\u0131veNa\u00a8\u0131ve Bayes).", "labels": [], "entities": []}, {"text": "The choice of the learning algorithms was primarily motivated by prior work.", "labels": [], "entities": []}, {"text": "Additional experiments on argument extractors and explicit.", "labels": [], "entities": [{"text": "argument extractors", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7251827716827393}]}, {"text": "The new set of features is presented in Section 3.", "labels": [], "entities": []}, {"text": "Evaluation using gold connectives and argument boundaries (no EP).: PS Arg1 extractor, no EP.", "labels": [], "entities": []}, {"text": "Baseline denotes taking the entire sentence as argument span.", "labels": [], "entities": []}, {"text": "The overall system results on the three data sets -development, test, and blind test -are shown in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Explicit sense classifier. Base refers  to features described in", "labels": [], "entities": []}, {"text": " Table 3: PS Arg1 extractor, no EP. Baseline de- notes taking the entire sentence as argument span.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.7103911638259888}]}, {"text": " Table 4: PS Arg2 extractor, no EP. Baseline de- notes taking the entire sentence, without the con- nective words, as argument span. Base features  refer to features used in Wang and Lan (2015).", "labels": [], "entities": []}, {"text": " Table 5: Implicit Arg1 extractor, no EP. Base- line denotes taking the entire sentence as argument  span.", "labels": [], "entities": [{"text": "Implicit Arg1 extractor", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.5635398228963217}]}, {"text": " Table 6: Evaluation of each component on the  development set (no EP).", "labels": [], "entities": []}, {"text": " Table 7: Official results on the development set.", "labels": [], "entities": []}, {"text": " Table 8: Official results on the test set.", "labels": [], "entities": []}, {"text": " Table 9: Official results on the blind test set.", "labels": [], "entities": []}]}