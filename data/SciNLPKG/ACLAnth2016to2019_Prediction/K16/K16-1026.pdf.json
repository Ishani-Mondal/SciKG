{"title": [{"text": "Entity Disambiguation by Knowledge and Text Jointly Embedding", "labels": [], "entities": [{"text": "Entity Disambiguation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8958932757377625}]}], "abstractContent": [{"text": "For most entity disambiguation systems, the secret recipes are feature representations for mentions and entities, most of which are based on Bag-of-Words (BoW) representations.", "labels": [], "entities": []}, {"text": "Commonly, BoW has several drawbacks: (1) It ignores the intrinsic meaning of words/entities; (2) It often results in high-dimension vector spaces and expensive computation; (3) For different applications, methods of designing handcrafted representations maybe quite different, lacking of a general guideline.", "labels": [], "entities": []}, {"text": "In this paper, we propose a different approach named EDKate.", "labels": [], "entities": []}, {"text": "We first learn low-dimensional continuous vector representations for entities and words by jointly embedding knowledge base and text in the same vector space.", "labels": [], "entities": []}, {"text": "Then we utilize these embeddings to design simple but effective features and build a two-layer disam-biguation model.", "labels": [], "entities": []}, {"text": "Extensive experiments on real-world data sets show that (1) The embedding-based features are very effective.", "labels": [], "entities": []}, {"text": "Even a single one embedding-based feature can beat the combination of several BoW-based features.", "labels": [], "entities": [{"text": "BoW-based", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.9539998173713684}]}, {"text": "(2) The superiority is even more promising in a difficult set where the mention-entity prior cannot work well.", "labels": [], "entities": []}, {"text": "(3) The proposed embedding method is much better than trivial implementations of some off-the-shelf embedding algorithms.", "labels": [], "entities": []}, {"text": "(4) We compared our EDKate with existing methods/systems and the results are also positive.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity disambiguation is the task of linking entity mentions in unstructured text to the corresponding entities in a knowledge base.", "labels": [], "entities": [{"text": "Entity disambiguation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8278146684169769}]}, {"text": "For example, in the sentence \"Michael Jordan is newly elected as AAAI fellow\", the mention \"Michael Jordan\" should be linked to \"Michael I. Jordan\" (Berkeley Professor) rather than \"Michael Jordan\" (NBA Player).", "labels": [], "entities": []}, {"text": "Formally, given a set of mentions M = {m 1 , m 2 , ..., m k } (specified or detected automatically) in a document d, for each mention mi \u2208 M , the task is to find the correct entity e i in the knowledge base (KB) K to which the mention mi refers.", "labels": [], "entities": []}, {"text": "There are various methods proposed for the problem in the past decades.", "labels": [], "entities": []}, {"text": "But generally speaking, an entity disambiguation method is commonly composed of three stages/components.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7292343378067017}]}, {"text": "(1) Constructing representations for mentions/entities from raw data, often as the form of sparse vectors.", "labels": [], "entities": []}, {"text": "(2) Extracting features for disambiguation models based on the representations of mentions and entities constructed in stage (1).", "labels": [], "entities": [{"text": "Extracting", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9222235083580017}]}, {"text": "(3) Optimizing the disambiguation model by empirically setting or learning weights on the extracted features, e.g., by training a classifier/ranker.", "labels": [], "entities": []}, {"text": "There exist few features directly defined by heuristics, skipping the first stage.", "labels": [], "entities": []}, {"text": "For example, string similarity or edit distance between a mention surface and an entity's canonical form, and the prior probability of a mention surface being some entity, etc.", "labels": [], "entities": []}, {"text": "However, they are the minority as it is difficult for human to design such features.", "labels": [], "entities": []}, {"text": "Almost all the existing methods focus on the second or the third stages while the importance of the first stage is often overlooked.", "labels": [], "entities": []}, {"text": "The common practice to deal with the first stage of representa-260 tions is defining handcrafted BoW representations.", "labels": [], "entities": []}, {"text": "For example, an entity is often represented by a sparse vector of weights on the n-grams contained in the description text of the entity, i.e., the standard Bag-of-Words (BoW) representation.", "labels": [], "entities": []}, {"text": "TF-IDF is often used to set the weights.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5330723524093628}]}, {"text": "There are several variants for this way, e.g., using selected key phrases or Wikipedia in-links/out-links instead of all n-grams as the dimensions of the vectors).", "labels": [], "entities": []}, {"text": "The problem is more challenging when representing a mention.", "labels": [], "entities": []}, {"text": "The common choice is using the n-gram vector of the surrounding text.", "labels": [], "entities": []}, {"text": "Obviously the information of the local text window is too limited to well represent a mention.", "labels": [], "entities": []}, {"text": "In practice, there is another constraint, the representations of entities and mentions should be in the same space, i.e., the dimensions of the vectors should be shared.", "labels": [], "entities": []}, {"text": "This constraint makes the representation design more difficult.", "labels": [], "entities": []}, {"text": "How to define such representations and the features based on them almost become the secrete sauce of a disambiguation system.", "labels": [], "entities": []}, {"text": "For example, uses Wikipedia anchor surfaces and \"Category\" values as dimensions and designed complex mechanisms to represent words, mentions and entities as sparse vectors on those dimensions.", "labels": [], "entities": []}, {"text": "BoW representations have several intrinsic drawbacks: First, the semantic meaning of a dimension is largely ignored.", "labels": [], "entities": []}, {"text": "For example, \"cat\", \"cats\" and \"tree\" are equally distant under onehot BoW representations.", "labels": [], "entities": []}, {"text": "Second, BoW representations often introduce high dimension vector spaces and lead to expensive computation.", "labels": [], "entities": []}, {"text": "Third, for different applications, methods of designing handcrafted representations maybe quite different, lacking of a general guideline.", "labels": [], "entities": []}, {"text": "The intuitive questions like \"why using n-grams, Wikipedia links or category values as dimensions\" and \"why using TF-IDF as weights\" are hinting us it is very likely these handcrafted representations are not the best and there should be some better representations.", "labels": [], "entities": []}, {"text": "In this paper we focus on the first stage, the problem of representations.", "labels": [], "entities": []}, {"text": "Inspired by the recent works on word embedding (), knowledge embedding () and joint embedding KBs and texts (, we propose to learn representations for entity disambiguation.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 151, "end_pos": 172, "type": "TASK", "confidence": 0.7361307740211487}]}, {"text": "Specifically, from KBs and texts, we jointly embed entities and words into the same low-dimensional continuous vector space.", "labels": [], "entities": []}, {"text": "The embeddings are obtained by optimizing a global objective considering all the information in the KBs and texts thus the intrinsic semantics of words and entities are believed to be preserved during the embedding.", "labels": [], "entities": []}, {"text": "Then we design simple but effective features based on embeddings and build a two-layer disambiguation model.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments on real-word data sets and exhibit the effectiveness of our words and entities' representation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics for each corpus", "labels": [], "entities": []}, {"text": " Table 2: Comparison between Embedding-based  Feature and BoW-based Feature", "labels": [], "entities": [{"text": "BoW-based", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8626994490623474}]}, {"text": " Table 3: Comparison between Linear Disam- biguation Model and Two-layer Model", "labels": [], "entities": []}, {"text": " Table 4: Comparison between Different Embed- dings", "labels": [], "entities": []}, {"text": " Table 5: Comparison with other reported results  on KBP 2010", "labels": [], "entities": []}, {"text": " Table 6: Comparison with other Wikification sys- tems in BoT F1 metric", "labels": [], "entities": [{"text": "BoT F1", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.913176029920578}]}]}