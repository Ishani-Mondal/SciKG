{"title": [{"text": "OPT: Oslo-Potsdam-Teesside Pipelining Rules, Rankers, and Classifier Ensembles for Shallow Discourse Parsing", "labels": [], "entities": [{"text": "OPT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.948759913444519}, {"text": "Shallow Discourse Parsing", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.5883040924866995}]}], "abstractContent": [{"text": "The OPT submission to the Shared Task of the 2016 Conference on Natural Language Learning (CoNLL) implements a 'classic' pipeline architecture, combining binary classification of (candidate) explicit connectives, heuristic rules for non-explicit discourse relations, ranking and 'editing' of syntactic constituents for argument identification , and an ensemble of classifiers to assign discourse senses.", "labels": [], "entities": [{"text": "Shared Task of the 2016 Conference on Natural Language Learning (CoNLL)", "start_pos": 26, "end_pos": 97, "type": "TASK", "confidence": 0.5312182926214658}, {"text": "argument identification", "start_pos": 319, "end_pos": 342, "type": "TASK", "confidence": 0.7205523997545242}]}, {"text": "With an end-to-end performance of 27.77 F 1 on the En-glish 'blind' test data, our system advances the previous state of the art (Wang & Lan, 2015) by close to four F 1 points, with particularly good results for the argument identification sub-tasks.", "labels": [], "entities": [{"text": "F 1", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9825624227523804}, {"text": "En-glish 'blind' test data", "start_pos": 51, "end_pos": 77, "type": "DATASET", "confidence": 0.653644065062205}, {"text": "F 1", "start_pos": 165, "end_pos": 168, "type": "METRIC", "confidence": 0.9697752296924591}, {"text": "argument identification", "start_pos": 216, "end_pos": 239, "type": "TASK", "confidence": 0.764249324798584}]}], "introductionContent": [{"text": "Being able to recognize aspects of discourse structure has recently been shown to be relevant for tasks as diverse as machine translation, questionanswering, text summarization, and sentiment analysis.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7708525657653809}, {"text": "text summarization", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7510257959365845}, {"text": "sentiment analysis", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.9530421495437622}]}, {"text": "For many of these applications, a 'shallow' approach as embodied in the PDTB can be effective.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9273869395256042}]}, {"text": "It is shallow in the sense of making only very few commitments to an overall account of discourse structure and of having annotation decisions concentrate on the individual instances of discourse relations, rather than on their interactions.", "labels": [], "entities": []}, {"text": "Previous work on this task has usually broken it down into a set of sub-problems, which are solved in a pipeline architecture (roughly: identify connectives, then arguments, then discourse senses;).", "labels": [], "entities": []}, {"text": "While adopting a similar pipeline approach, the OPT discourse parser also builds on and extends a method that has previously achieved state-of-the-art results for the detection of speculation and negation (.", "labels": [], "entities": [{"text": "OPT discourse parser", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7149265011151632}, {"text": "detection of speculation and negation", "start_pos": 167, "end_pos": 204, "type": "TASK", "confidence": 0.824109947681427}]}, {"text": "It is interesting to observe that an abstractly similar pipeline-disambiguating trigger expressions and then resolving their in-text 'scope'-yields strong performance across linguistically diverse tasks.", "labels": [], "entities": []}, {"text": "At the same time, the original system has been substantially augmented for discourse parsing as outlined below.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7690156400203705}]}, {"text": "There is no closely corresponding sub-problem to assigning discourse senses in the analysis of negation and speculation; thus, our sense classifier described has been developed specifically for OPT.", "labels": [], "entities": []}], "datasetContent": [{"text": "Overall Results summarizes OPT system performance in terms of the metrics computed by the official scorer for the Shared Task, against both the WSJ and 'blind' test sets.", "labels": [], "entities": [{"text": "OPT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8636801838874817}, {"text": "WSJ and 'blind' test sets", "start_pos": 144, "end_pos": 169, "type": "DATASET", "confidence": 0.6351295198713031}]}, {"text": "To compare against the previous state of the art, we include results for the top-performing systems from the 2015 and 2016 competitions (as reported by.", "labels": [], "entities": []}, {"text": "Where applicable, best results (when comparing F 1 ) are highlighted for each sub-task and -metric.", "labels": [], "entities": []}, {"text": "The highlighting makes it evident that the OPT system is competitive to the state of the art across the board, but particularly soon the argument identification sub-task and on the 'blind' test data: In terms of the WSJ test data, OPT would have ranked second in the 2015 competition, but on the 'blind' data it outperforms the previous state of the art on all but one metric for which contrastive results are provided by Xue et al..", "labels": [], "entities": [{"text": "argument identification", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.7294135242700577}, {"text": "WSJ test data", "start_pos": 216, "end_pos": 229, "type": "DATASET", "confidence": 0.9549787243207296}, {"text": "OPT", "start_pos": 231, "end_pos": 234, "type": "DATASET", "confidence": 0.8053238987922668}]}, {"text": "Where earlier systems tend to drop by several F 1 points when evaluated on the non-WSJ data, this 'out-of-domain' effect is much smaller for OPT.", "labels": [], "entities": [{"text": "F 1", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9521247446537018}]}, {"text": "For comparison, we also include the top scores for each submodule achieved by any system in the 2016 Shared Task.", "labels": [], "entities": []}, {"text": "Non-Explicit Relations In isolation, the stipulation of non-explicit relations achieves an F 1 of 93.2 on the WSJ test set (P = 89.9, R = 96.8).", "labels": [], "entities": [{"text": "F 1", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9965759515762329}, {"text": "WSJ test set", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.9709604978561401}]}, {"text": "Since this sub-module does not specify full argument spans, we match gold and predicted relations based on the sentence identifiers of the arguments only.", "labels": [], "entities": []}, {"text": "False positives include NoRel and missing relations.", "labels": [], "entities": [{"text": "NoRel", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8398920893669128}]}, {"text": "About half of the false negatives are relations within the same sentence (across a semicolon).: Isolated argument extraction results (PS refers to the immediately preceding sentence only).", "labels": [], "entities": [{"text": "Isolated argument extraction", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.5807475646336874}]}], "tableCaptions": [{"text": " Table 1: Observations of arguments in the training  data. Alignment rates are with respect to all argu- ments that do not span sentence boundaries and are  located in SS or PS, while the upper-bound is with  respect to all arguments.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9532542824745178}]}, {"text": " Table 3: Per-component breakdown of system performance, compared to top performers in 2015/16.", "labels": [], "entities": []}, {"text": " Table 4: Isolated argument extraction results (PS  refers to the immediately preceding sentence only).", "labels": [], "entities": [{"text": "Isolated argument extraction", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6675843596458435}]}, {"text": " Table 5: Isolated results for sense classification (the  bottom  *  model was not part of the submission).", "labels": [], "entities": [{"text": "sense classification", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9162608981132507}]}]}