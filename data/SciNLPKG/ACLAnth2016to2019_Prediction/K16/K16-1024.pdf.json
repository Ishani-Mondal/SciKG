{"title": [{"text": "Event Linking with Sentential Features from Convolutional Neural Networks", "labels": [], "entities": [{"text": "Event Linking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7351117581129074}]}], "abstractContent": [{"text": "Coreference resolution for event mentions enables extraction systems to process document-level information.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9103192090988159}]}, {"text": "Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available.", "labels": [], "entities": []}, {"text": "We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks.", "labels": [], "entities": []}, {"text": "Two such networks first process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant fora linking decision.", "labels": [], "entities": []}, {"text": "These representations are augmented with lexical-level and pairwise features, and serve as input to a trainable similarity function producing a coreference score.", "labels": [], "entities": []}, {"text": "Our model achieves state-of-the-art performance on two datasets, one of which is publicly available.", "labels": [], "entities": []}, {"text": "An error analysis points out directions for further research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Event extraction aims at detecting mentions of realworld events and their arguments in text documents of different domains, e.g., news articles.", "labels": [], "entities": [{"text": "Event extraction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7608809173107147}, {"text": "detecting mentions of realworld events and their arguments in text documents", "start_pos": 25, "end_pos": 101, "type": "TASK", "confidence": 0.7866687666286122}]}, {"text": "The subsequent task of event linking is concerned with resolving coreferences between recognized event mentions in a document, and is the focus of this paper.", "labels": [], "entities": [{"text": "event linking", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7601772546768188}, {"text": "resolving coreferences between recognized event mentions in a document", "start_pos": 55, "end_pos": 125, "type": "TASK", "confidence": 0.7872049543592665}]}, {"text": "Several studies investigate event linking and related problems such as relation mentions spanning multiple sentences.", "labels": [], "entities": [{"text": "event linking", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.7171080857515335}, {"text": "relation mentions spanning multiple sentences", "start_pos": 71, "end_pos": 116, "type": "TASK", "confidence": 0.9406669497489929}]}, {"text": "find that 28.5 % of binary relation mentions in the MUC 6 dataset are affected, as are 9.4 % of relation mentions in the ACE corpus from 2003.", "labels": [], "entities": [{"text": "MUC 6 dataset", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.9508796334266663}, {"text": "ACE corpus from 2003", "start_pos": 121, "end_pos": 141, "type": "DATASET", "confidence": 0.9683517217636108}]}, {"text": "estimate that 15 % of slot fills in the training data for the \"TAC 2010 KBP Slot Filling\" task require cross-sentential inference.", "labels": [], "entities": [{"text": "TAC 2010 KBP Slot Filling\" task", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.7833385212080819}]}, {"text": "To confirm these numbers, we analyzed the event annotation of the ACE 2005 corpus and found that approximately 23 % of the event mentions lack arguments which are present in other mentions of the same event instance in the respective document.", "labels": [], "entities": [{"text": "ACE 2005 corpus", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.9644735256830851}]}, {"text": "These numbers suggest that event linking is an important task.", "labels": [], "entities": [{"text": "event linking", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.8247630000114441}]}, {"text": "Previous approaches for modeling event mentions in context of coreference resolution) make either use of external feature sources with limited cross-domain availability like WordNet) and FrameNet (), or show low performance.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9500222504138947}, {"text": "WordNet", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.9641273617744446}]}, {"text": "At the same time, recent literature proposes anew kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (.", "labels": [], "entities": []}, {"text": "Their promising results motivate our work.", "labels": [], "entities": []}, {"text": "We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.8544519146283468}]}, {"text": "Our contributions in this paper are as follows: We design a neural approach to event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features.", "labels": [], "entities": [{"text": "event linking", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.7380163222551346}]}, {"text": "In the next step, our model learns to make coreference decisions for pairs of event mentions based on the previously generated representations.", "labels": [], "entities": []}, {"text": "This approach does not rely on external semantic features, but rather employs a combination of local and sentential features to describe individual event mentions, and combines these intermediate event representations with standard pairwise features for the coreference decision.", "labels": [], "entities": []}, {"text": "The model achieves state-of-the-art performance in our experiments on two datasets, one of which is publicly available.", "labels": [], "entities": []}, {"text": "Furthermore, we present an analysis of the system errors to identify directions for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented our model using the TensorFlow framework (   , we experimented with different convolution window sizes n, activation functions for the similarityfunction layer in model part (b), whether to use the dual pooling and final hidden layer in model part (a), whether to apply regularization with 2 penalties or Dropout, and parameters to Adam (\u03b7, \u03b2 1 , \u03b2 2 , ).", "labels": [], "entities": []}, {"text": "We started our exploration of this space of possibilities from previously reported hyperparameter values (Zhang and Wallace, 2015; Chen et al., 2015) and followed a combined strategy of random sampling from the hyperparameter space (180 points) and line search.", "labels": [], "entities": []}, {"text": "Optimization was done by training on ACE ++ train and evaluating on ACE ++ valid . The final settings we used for all following experiments are listed in  This section elaborates on the conducted experiments.", "labels": [], "entities": [{"text": "Optimization", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9505335688591003}, {"text": "ACE ++ train", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9326413869857788}, {"text": "ACE ++ valid", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9271401166915894}]}, {"text": "First, we compare our approach to state-ofart systems on dataset ACE, after which we report experiments on ACE ++ , where we contrast variations of our model to gain insights about the impact of the utilized feature classes.", "labels": [], "entities": []}, {"text": "We conclude this section with an error analysis.) has the highest validity, as it balances the impact of positive and negative event-mention links in a document.", "labels": [], "entities": [{"text": "validity", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9937999844551086}]}, {"text": "Negative links and consequently singleton event mentions are more common in this dataset (more than 90 % of links are negative).", "labels": [], "entities": []}, {"text": "As Recasens and Hovy (2011) point out, the informativeness of metrics like MUC (, B-CUBED (, and the naive positivelink metric suffers from such imbalance.", "labels": [], "entities": [{"text": "MUC", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.7317628264427185}, {"text": "B-CUBED", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9388144016265869}]}, {"text": "We still add these metrics for completeness, and because BLANC scores are not available for all systems.", "labels": [], "entities": [{"text": "completeness", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.8441476821899414}, {"text": "BLANC", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9656370878219604}]}, {"text": "Unfortunately, there are two caveats to this comparison.", "labels": [], "entities": []}, {"text": "First, while a 9:1 train/test split is the commonly accepted way of using ACE, the exact documents in the partitions vary from system to system.", "labels": [], "entities": []}, {"text": "We are not aware of any publicized split from previous work on event linking, which is why we create our own and announce the list of documents in ACE valid /ACE test at https://git.io/vwEEP.", "labels": [], "entities": [{"text": "event linking", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.7184025347232819}, {"text": "ACE valid /ACE test", "start_pos": 147, "end_pos": 166, "type": "DATASET", "confidence": 0.7772969722747802}]}, {"text": "Second, published methods follow different strategies regarding preprocessing components.", "labels": [], "entities": []}, {"text": "While all systems in use gold-annotated eventmention triggers, Bejan and Harabagiu (2010) and  use a semantic-role labeler and other tools instead of gold-argument information.", "labels": [], "entities": []}, {"text": "We argue that using full gold-annotated event mentions is reasonable in order to mitigate error propagation along the extraction pipeline and make performance values for the task at hand more informative.", "labels": [], "entities": []}, {"text": "We now look at several aspects of the model performance to gain further insights about it's behavior.", "labels": [], "entities": []}, {"text": "shows the impact of increasing the amount of training data (ACE \u2192 ACE ++ ).", "labels": [], "entities": []}, {"text": "This increase (rows 1, 3) leads to a boost in recall, from 75.16% to 83.21%, at the cost of a small decrease in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9997482895851135}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9992030262947083}]}, {"text": "This indicates that the model can generalize much better using this additional training data.", "labels": [], "entities": []}, {"text": "Looking into the use of the alternative clustering strategy BestLink recommended by , we can make the expected observation of a precision improvement (row 1 vs. 2; row 3 vs. 4), due to fewer positive links being used before the transitive-closure clustering takes place.", "labels": [], "entities": [{"text": "BestLink", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.981543779373169}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9992033839225769}]}, {"text": "This is however outweighed by a large decline in recall, resulting in a lower F1 score (73.31 \u2192 72.19; 76.90 \u2192 71.09).", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9996696710586548}, {"text": "F1 score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9877099096775055}]}, {"text": "The better performance of BestLink in Liu et al.'s model suggests that our model already weeds out many low confidence links in the classification step, which makes a downstream filtering unnecessary in terms of precision, and even counter-productive in terms of recall.: Event-linking performance of our model against naive baselines.", "labels": [], "entities": [{"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9973666071891785}, {"text": "recall.", "start_pos": 263, "end_pos": 270, "type": "METRIC", "confidence": 0.9977405071258545}]}, {"text": "Impact of feature classes shows our model's performance when particular feature classes are removed from the model (with retraining), with row 3 corresponding to the full model as described in Section 3.", "labels": [], "entities": []}, {"text": "Unsurprisingly, classifying examples with just pairwise features (row 1) results in the worst performance, and adding first trigger-local lexical features (row 2), then sentential features (row 3) subsequently raises both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 222, "end_pos": 231, "type": "METRIC", "confidence": 0.9995157718658447}, {"text": "recall", "start_pos": 236, "end_pos": 242, "type": "METRIC", "confidence": 0.9987949132919312}]}, {"text": "Just using pairwise features and sentential ones (row 4), boosts precision, which is counter-intuitive at first, but maybe explained by a different utilization of the sententialfeature part of the model during training.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9994706511497498}]}, {"text": "This part is then adapted to focus more on the trigger-word aspect, meaning the sentential features degrade to trigger-local features.", "labels": [], "entities": []}, {"text": "While this allows to reach higher precision (recall that Section 3 finds that more than fifty percent of positive examples have trigger-word agreement), it substantially limits the model's ability to learn other coreference-relevant aspects of event-mention pairs, leading to low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9987165927886963}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.989446759223938}, {"text": "recall", "start_pos": 280, "end_pos": 286, "type": "METRIC", "confidence": 0.9984526634216309}]}, {"text": "Further considering rows 5 & 6, we can conclude that all feature classes indeed positively contribute to the overall model performance.", "labels": [], "entities": []}, {"text": "Baselines The result of applying three naive baselines to ACE ++ is shown in.", "labels": [], "entities": []}, {"text": "The all singletons/one instance baselines predict every input link to be negative/positive, respectively.", "labels": [], "entities": []}, {"text": "In particular the all-singletons baseline performs well, due to the large fraction of singleton event mentions in the dataset.", "labels": [], "entities": []}, {"text": "The third baseline, same type, predicts a positive link whenever there is agreement on the event type, namely, it ignores the possibility that there could be multiple event mentions of the same type in a document which do not refer to the same real-world event, e.g., referring to different terrorist attacks.", "labels": [], "entities": []}, {"text": "This baseline also performs quite well, in particular in terms of recall, but shows low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9990779161453247}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9984586238861084}]}, {"text": "Error analysis We manually investigated a sample of 100 false positives and 100 false negatives from ACE ++ in order to get an understanding of system errors.", "labels": [], "entities": []}, {"text": "It turns out that a significant portion of the false negatives would involve the resolution of a pronoun to a previous event mention, a very hard and yet unsolved problem.", "labels": [], "entities": [{"text": "resolution of a pronoun to a previous event mention", "start_pos": 81, "end_pos": 132, "type": "TASK", "confidence": 0.801852179898156}]}, {"text": "Consider the following examples: \u2022 \"It's crazy that we're bombing Iraq.", "labels": [], "entities": []}, {"text": "\u2022 \"Some of the slogans sought to rebut war supporters' arguments that the protests are unpatriotic.", "labels": [], "entities": []}, {"text": "Nobody questions whether this is right or not.", "labels": [], "entities": []}, {"text": "In both examples, the event mentions (trigger words in bold font) are gold-annotated as coreferential, but our model failed to recognize this.", "labels": [], "entities": []}, {"text": "Another observation is that for 17 false negatives, we found analogous cases among the sampled false positives where annotators made a different annotation decision.", "labels": [], "entities": []}, {"text": "Consider these examples: \u2022 The 1860 Presidential Election.", "labels": [], "entities": []}, {"text": "Lincoln won a plurality with about 40% of the vote.", "labels": [], "entities": []}, {"text": "\u2022 She lost her seat in the 1997 election.", "labels": [], "entities": []}, {"text": "Each bullet point has two event mentions (in bold font) taken from the same document and referring to the same event type, i.e., Personnel.Elect.", "labels": [], "entities": []}, {"text": "While in the first example, the annotators identified the mentions as coreferential, the second pair of mentions is not annotated as such.", "labels": [], "entities": []}, {"text": "Analogously, 22 out of the 100 analyzed false positives were cases where the misclassification of the system was plausible to a human rater.", "labels": [], "entities": []}, {"text": "This exemplifies that this task has many boundary cases were a positive/negative decision is hard to make even for expert annotators, thus putting the overall performance of all models in in perspective.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Event-linking performance of our model & competitors on ACE. Best value per metric in bold.", "labels": [], "entities": [{"text": "ACE", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.7600598931312561}]}, {"text": " Table 4: Impact of data amount and clustering.", "labels": [], "entities": []}, {"text": " Table 5: Impact of feature classes; \"Pw\" is short  for pairwise features, \"Loc\" refers to trigger-local  lexical features, \"Sen\" corresponds to sentential  features.", "labels": [], "entities": []}, {"text": " Table 6: Event-linking performance of our model  against naive baselines.", "labels": [], "entities": []}]}