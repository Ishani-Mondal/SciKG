{"title": [{"text": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation", "labels": [], "entities": [{"text": "Named Entity Disambiguation", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6412079632282257}]}], "abstractContent": [{"text": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia).", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7826509525378545}]}, {"text": "In this paper, we propose a novel embedding method specifically designed for NED.", "labels": [], "entities": []}, {"text": "The proposed method jointly maps words and entities into the same continuous vector space.", "labels": [], "entities": []}, {"text": "We extend the skip-gram model by using two models.", "labels": [], "entities": []}, {"text": "The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words.", "labels": [], "entities": []}, {"text": "By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9991668462753296}, {"text": "CoNLL dataset", "start_pos": 145, "end_pos": 158, "type": "DATASET", "confidence": 0.9594843983650208}, {"text": "TAC 2010 dataset", "start_pos": 176, "end_pos": 192, "type": "DATASET", "confidence": 0.9751838445663452}]}], "introductionContent": [{"text": "Named Entity Disambiguation (NED) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base (KB) (e.g., Wikipedia).", "labels": [], "entities": [{"text": "Named Entity Disambiguation (NED)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7707868864138921}, {"text": "resolving ambiguous mentions of entities to their referent entities in a knowledge base (KB)", "start_pos": 49, "end_pos": 141, "type": "TASK", "confidence": 0.6096106600016356}]}, {"text": "NED has lately been extensively studied) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population, and semantic search ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.8527714610099792}, {"text": "semantic search", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.7464645802974701}]}, {"text": "We use Wikipedia as KB in this paper.", "labels": [], "entities": [{"text": "Wikipedia as KB", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.8614713748296102}]}, {"text": "The main difficulty in NED is ambiguity in the meaning of entity mentions.", "labels": [], "entities": []}, {"text": "For example, the mention \"Washington\" in a document can refer to various entities, such as the state, or the capital of the US, the actor Denzel Washington, the first US president George Washington, and soon.", "labels": [], "entities": []}, {"text": "In order to resolve these ambiguous mentions into references to the correct entities, early approaches focused on modeling textual context, such as the similarity between contextual words and encyclopedic descriptions of a candidate entity ().", "labels": [], "entities": []}, {"text": "Most state-of-the-art methods use more sophisticated global approaches, where all mentions in a document are simultaneously disambiguated based on global coherence among disambiguation decisions.", "labels": [], "entities": []}, {"text": "Word embedding methods are also becoming increasingly popular ().", "labels": [], "entities": []}, {"text": "These involve learning continuous vector representations of words from large, unstructured text corpora.", "labels": [], "entities": []}, {"text": "The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively lowdimensional vector space.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space.", "labels": [], "entities": []}, {"text": "In this model, similar words and entities are placed close to one another in a vector space.", "labels": [], "entities": []}, {"text": "Hence, we can measure the similarity between any pair of items (i.e., words, entities, and a word and an entity) by simply computing their cosine similarity.", "labels": [], "entities": []}, {"text": "This enables us to easily measure the contextual information for NED, such as the similarity between a context word and a candidate entity, and the relatedness of entities required to model coherence.", "labels": [], "entities": []}, {"text": "Our model is based on the skip-gram model (), a recently proposed embedding model that learns to predict each context word given the target word.", "labels": [], "entities": []}, {"text": "Our model consists of the following three models based on the skip-gram model: 1) the conventional skip-gram model that learns to predict neighboring words given the target word in text corpora, 2) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB, and 3) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB.", "labels": [], "entities": []}, {"text": "By jointly optimizing these models, our method simultaneously learns the embedding of words and entities.", "labels": [], "entities": []}, {"text": "Based on our proposed embedding, we also develop a straightforward NED method that computes two contexts using the proposed embedding: textual context similarity, and coherence.", "labels": [], "entities": []}, {"text": "Textual context similarity is measured according to vector similarity between an entity and words in a document.", "labels": [], "entities": []}, {"text": "Coherence is measured based on the relatedness between the target entity and other entities in a document.", "labels": [], "entities": []}, {"text": "Our NED method combines these contexts with several standard features (e.g., prior probability) using supervised machine learning.", "labels": [], "entities": []}, {"text": "We tested the proposed method using two standard NED datasets: the CoNLL dataset and the TAC 2010 dataset.", "labels": [], "entities": [{"text": "NED datasets", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.8252679705619812}, {"text": "CoNLL dataset", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9804366230964661}, {"text": "TAC 2010 dataset", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.9621230165163676}]}, {"text": "Experimental results revealed that our method outperforms state-of-the-art methods on both datasets by significant margins.", "labels": [], "entities": []}, {"text": "Moreover, we conducted experiments to separately assess the quality of the vector representation of entities using an entity relatedness dataset, and discovered that our method successfully learns the quality representations of entities.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the setup and results of our experiments.", "labels": [], "entities": []}, {"text": "In addition to experiments on the NED task, we separately assessed the quality of pairwise entity relatedness in order to test the 3 http://scikit-learn.org/ effectiveness of our method in capturing pairwise similarity between pairs of entities.", "labels": [], "entities": [{"text": "NED task", "start_pos": 34, "end_pos": 42, "type": "TASK", "confidence": 0.8842851221561432}]}, {"text": "We first describe the details of the training of the embedding and then present the experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the entity relatedness task.", "labels": [], "entities": [{"text": "entity relatedness task", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7899521986643473}]}, {"text": " Table 2: Experimental results of our proposed  NED method.", "labels": [], "entities": [{"text": "NED", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.967898428440094}]}, {"text": " Table 3: Accuracy scores of the proposed method  and the state-of-the-art methods.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987359642982483}]}]}