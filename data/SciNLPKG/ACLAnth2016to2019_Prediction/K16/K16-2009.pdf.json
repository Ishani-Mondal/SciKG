{"title": [{"text": "SoNLP-DP System for ConLL-2016 English Shallow Discourse Parsing", "labels": [], "entities": [{"text": "ConLL-2016 English Shallow Discourse Parsing", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.8131013512611389}]}], "abstractContent": [{"text": "This paper describes the submitted En-glish shallow discourse parsing system from the natural language processing (NLP) group of Soochow university (SoNLP-DP) to the CoNLL-2016 shared task.", "labels": [], "entities": [{"text": "En-glish shallow discourse parsing", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.6020476594567299}, {"text": "Soochow university (SoNLP-DP)", "start_pos": 129, "end_pos": 158, "type": "DATASET", "confidence": 0.9336231589317322}, {"text": "CoNLL-2016 shared task", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.6239708860715231}]}, {"text": "Our System classifies discourse relations into explicit and non-explicit relations and uses a pipeline platform to conduct every subtask to form an end-to-end shallow discourse parser in the Penn Discourse Treebank (PDTB).", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 191, "end_pos": 221, "type": "DATASET", "confidence": 0.9531691471735636}]}, {"text": "Our system is evaluated on the CoNLL-2016 Shared Task closed track and achieves the 24.31% and 28.78% in F1-measure on the official blind test set and test set, respectively.", "labels": [], "entities": [{"text": "CoNLL-2016 Shared Task closed track", "start_pos": 31, "end_pos": 66, "type": "DATASET", "confidence": 0.8816905498504639}, {"text": "F1-measure", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9988011121749878}, {"text": "official blind test set and test set", "start_pos": 123, "end_pos": 159, "type": "DATASET", "confidence": 0.7340671462672097}]}], "introductionContent": [{"text": "Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (), text summarization (, and statistical machine translation.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7828091979026794}, {"text": "natural language understanding", "start_pos": 156, "end_pos": 186, "type": "TASK", "confidence": 0.6472201546033224}, {"text": "coherence modeling", "start_pos": 267, "end_pos": 285, "type": "TASK", "confidence": 0.6927400976419449}, {"text": "text summarization", "start_pos": 290, "end_pos": 308, "type": "TASK", "confidence": 0.8022128641605377}, {"text": "statistical machine translation", "start_pos": 316, "end_pos": 347, "type": "TASK", "confidence": 0.733106772104899}]}, {"text": "As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus) and has been attracting more and more attention recently (.", "labels": [], "entities": [{"text": "Penn Discourse TreeBank (PDTB) corpus", "start_pos": 37, "end_pos": 74, "type": "DATASET", "confidence": 0.94681145463671}, {"text": "Penn TreeBank (PTB) corpus", "start_pos": 132, "end_pos": 158, "type": "DATASET", "confidence": 0.9660552442073822}]}, {"text": "Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory(RST) Treebank corpus", "start_pos": 52, "end_pos": 100, "type": "TASK", "confidence": 0.7149692848324776}]}, {"text": "This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring) or hierarchial structuring.", "labels": [], "entities": []}, {"text": "Although much research work has been conducted for certain subtasks since the release of the PDTB corpus, there is still little work on constructing an end-to-end shallow discourse parser.", "labels": [], "entities": [{"text": "PDTB corpus", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.9245851039886475}]}, {"text": "The CoNLL 2016 shared task evaluates endto-end shallow discourse parsing systems for determining and classifying both explicit and nonexplicit discourse relations.", "labels": [], "entities": [{"text": "CoNLL 2016 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8691649287939072}, {"text": "endto-end shallow discourse parsing", "start_pos": 37, "end_pos": 72, "type": "TASK", "confidence": 0.6175966560840607}]}, {"text": "A participant system needs to (1)locate all explicit (e.g., \"because\", \"however\", \"and\".) discourse connectives in the text, (2)identify the spans of text that serve as the two arguments for each discourse connective, and (3) predict the sense of the discourse relations (e.g., \"Cause\", \"Condition\", \"Contrast\").", "labels": [], "entities": []}, {"text": "In this paper, we describe the system submission from the NLP group of Soochow university (SoNLP-DP).", "labels": [], "entities": [{"text": "Soochow university (SoNLP-DP)", "start_pos": 71, "end_pos": 100, "type": "DATASET", "confidence": 0.8555643320083618}]}, {"text": "Our shallow discourse parser consists of multiple components in a pipeline architecture, including a connective classifier, argument labeler, explicit classifier, non-explicit classifier.", "labels": [], "entities": []}, {"text": "Our system is evaluated on the CoNLL-2016 Shared Task closed track and achieves the 24.31% and 28.78% in F1-measure on the official blind test set and test set, respectively.", "labels": [], "entities": [{"text": "CoNLL-2016 Shared Task closed track", "start_pos": 31, "end_pos": 66, "type": "DATASET", "confidence": 0.8816901922225953}, {"text": "F1-measure", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9988011121749878}, {"text": "official blind test set and test set", "start_pos": 123, "end_pos": 159, "type": "DATASET", "confidence": 0.7340671632971082}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents our shallow discourse parsing system.", "labels": [], "entities": [{"text": "shallow discourse parsing", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6437643766403198}]}, {"text": "The experimental results are described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our system on the corpora provided in the CoNLL-2016 Shared Task and evaluate our system on the CoNLL-2016 Shared Task closed track.", "labels": [], "entities": [{"text": "CoNLL-2016 Shared Task", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.9003347555796305}, {"text": "CoNLL-2016 Shared Task closed track", "start_pos": 105, "end_pos": 140, "type": "DATASET", "confidence": 0.9325323700904846}]}, {"text": "All our classifiers are trained using the OpenNLP maximum entropy package 4 with the default pa-rameters (i.e. without smoothing and with 100 iterations).", "labels": [], "entities": []}, {"text": "We firstly report the official score on the CoNLL-2016 shared task on development, test and blind test sets.", "labels": [], "entities": [{"text": "CoNLL-2016 shared task", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.8514182170232137}]}, {"text": "Then, the supplementary results provided by the shared task organizes are reported.", "labels": [], "entities": []}, {"text": "46.37 91.86 24.00: the official F1 score of our system.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9725446403026581}]}, {"text": "In, we present the official results of our system performances on the CoNLL-2016 development, test and blind test sets, respectively.", "labels": [], "entities": [{"text": "CoNLL-2016 development, test and blind test sets", "start_pos": 70, "end_pos": 118, "type": "DATASET", "confidence": 0.6902166455984116}]}, {"text": "In the blind test, our parser achieve a better result than the best system of last year (: the supplementary F1 score of our system.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9386362135410309}]}, {"text": "In, we reported the supplementary results provided by the shared task organizes on the development, test and blind test sets.", "labels": [], "entities": []}, {"text": "These additional experiments investigate the performance of our shallow discourse parsing for explicit and non-explicit relations separately.", "labels": [], "entities": [{"text": "shallow discourse parsing", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7462581197420756}]}, {"text": "From the results, we can find that the sense classification for both explicit and non-explicit discourse relations are the biggest obstacles to the overall performance of discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7202845513820648}]}, {"text": "Further, we reports all the official performance in on the development, test and blind test set in detail.", "labels": [], "entities": []}, {"text": "From the table, we observe: \u2022 For argument recognition of explicit discourse relations, the performance of Arg2 is much better than that of Arg1 on all the three datasets.", "labels": [], "entities": [{"text": "argument recognition of explicit discourse relations", "start_pos": 34, "end_pos": 86, "type": "TASK", "confidence": 0.8715073466300964}, {"text": "Arg2", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.898513674736023}]}, {"text": "So the performance of Arg1 & Arg2 recognition mainly depends on the performance of Arg1 recognition.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.7833917737007141}, {"text": "Arg2 recognition", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7902337908744812}, {"text": "Arg1 recognition", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.6644215881824493}]}, {"text": "With respect to non-explicit discourse relations, the performance gap of argument recognition on Arg1 and Arg2 is very small.", "labels": [], "entities": [{"text": "argument recognition", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7583707869052887}]}, {"text": "\u2022 With respect to explicit discourse relations, the sense classification works almost perfectly on development data.", "labels": [], "entities": [{"text": "sense classification", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7279280126094818}]}, {"text": "It also works well on the test and blind test sets.", "labels": [], "entities": []}, {"text": "With respect to non-explicit discourse relations, the sense classification works much worse than that of explicit sense classification.", "labels": [], "entities": [{"text": "sense classification", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7319718599319458}]}, {"text": "The performance gap caused by non-explicit sense classification reaches 15% 16%.", "labels": [], "entities": [{"text": "sense classification", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.6700236648321152}]}], "tableCaptions": [{"text": " Table 1: the official F1 score of our system.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9596264362335205}]}, {"text": " Table 2: the supplementary F1 score of our sys- tem.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.927051305770874}]}, {"text": " Table 3: Official results (%) of our parser on development, test and blind test sets. Group Explicit  indicates the performance with respect to explicit discourse relations; group Non-Explicit indicates the  performance with respect to non-explicit discourse relations, and group all indicates the performance  with respect to all discourse relations, including both explicit and non-explicit ones.", "labels": [], "entities": []}]}