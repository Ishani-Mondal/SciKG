{"title": [{"text": "Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.8391132056713104}]}], "abstractContent": [{"text": "Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly.", "labels": [], "entities": [{"text": "Cross lingual projection of linguistic annotation", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7463003446658453}]}, {"text": "In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7660479843616486}]}, {"text": "This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags.", "labels": [], "entities": []}, {"text": "We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data.", "labels": [], "entities": []}, {"text": "Our system equals or exceeds the state-of-the-art on eight simulated low-resource settings, as well as two real low-resource languages, Malagasy and Kin-yarwanda.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech (POS) tagging is a critical task for natural language processing (NLP) applications, providing lexical syntactic information.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5874536752700805}]}, {"text": "Automatic POS tagging has been extremely successful for many rich resource languages through the use of supervised learning overlarge training corpora (.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7858567237854004}]}, {"text": "However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.7898776829242706}]}, {"text": "For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (.", "labels": [], "entities": []}, {"text": "A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy \"distant\" data.", "labels": [], "entities": []}, {"text": "A popular method for distant supervision is to use parallel data between a low-resource language and a rich-resource language.", "labels": [], "entities": []}, {"text": "Although annotated data in low-resource languages is difficult to obtain, bilingual resources are more plentiful.", "labels": [], "entities": []}, {"text": "For example parallel translations into English are often available, in the form of news reports, novels or the Bible.", "labels": [], "entities": []}, {"text": "Parallel data allows annotation from a high-resource language to be projected across alignments to the low-resource language, which has been shown to be effective for several language processing tasks including POS tagging (, named entity recognition () and dependency parsing . Although cross-lingual POS projection is popular it has several problems, including errors from poor word alignments and cross-lingual syntactic divergence . Previous work has proposed heuristics or constraints to clean the projected tag before or during learning.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 211, "end_pos": 222, "type": "TASK", "confidence": 0.8334037363529205}, {"text": "named entity recognition", "start_pos": 226, "end_pos": 250, "type": "TASK", "confidence": 0.6499068140983582}, {"text": "dependency parsing", "start_pos": 258, "end_pos": 276, "type": "TASK", "confidence": 0.8299158811569214}, {"text": "POS projection", "start_pos": 302, "end_pos": 316, "type": "TASK", "confidence": 0.68935726583004}, {"text": "cross-lingual syntactic divergence", "start_pos": 400, "end_pos": 434, "type": "TASK", "confidence": 0.6178921163082123}]}, {"text": "In contrast, we consider compensating for these problems explicitly, by learning a bias transformation to encode the mapping between 'clean' tags and the kinds of tags produced from projection.", "labels": [], "entities": []}, {"text": "We propose anew neural network model for sequence tagging in a low-resource language, suitable for training with both a tiny gold standard annotated corpus, as well as distant supervision using cross-lingual tag projection.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7341722846031189}, {"text": "cross-lingual tag projection", "start_pos": 194, "end_pos": 222, "type": "TASK", "confidence": 0.6388824383417765}]}, {"text": "Our model uses a bidirectional Long Short-Term Memory (BiL-STM), which produces two types of output: gold tags generated directly from the hidden states of a neural network, and uncertain projected tags generated after applying a further linear transformation.", "labels": [], "entities": []}, {"text": "This transformation, encodes the mapping between the projected tags from the high-resource language, and the gold tags in the target low-resource language, and learns when and how much to trust the projected data.", "labels": [], "entities": []}, {"text": "For example, for languages without determiners, the model can learn to map projected determiner tags to nouns, or if verbs are often poorly aligned, the model can learn to effectively ignore the projected verb tag, by associating all tags with verbs.", "labels": [], "entities": []}, {"text": "Our model is trained jointly on gold and distant projected annotations, and can be trained end-to-end with backpropagation.", "labels": [], "entities": []}, {"text": "Our approach captures the relations among tokens, noisy projected POS tags and ground truth POS tags.", "labels": [], "entities": []}, {"text": "Our work differs in the use of projection, in that we explicitly model the transformation between tagsets as part of a more expressive deep learning neural network.", "labels": [], "entities": []}, {"text": "We make three main contributions.", "labels": [], "entities": []}, {"text": "First, we study the noise of projected data in word alignments and describe it with an additional transformation layer in the model.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7116145938634872}]}, {"text": "Second, we integrate the model into a deep neural network and jointly train the model on both annotated and projected data to make the model learn from better supervision.", "labels": [], "entities": []}, {"text": "Finally, evaluating on eight simulated and two real-world low-resource languages, experimental results demonstrate that our approach uniformly equals or exceeds existing methods on simulated languages, and achieves 86.7% accuracy for Malagasy and 82.6% on Kinyarwanda, exceeding the state-of-the-art results of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9978373646736145}]}], "datasetContent": [{"text": "We evaluate our algorithm using two kinds of experimental setups, simulation experiments and real-world experiments.", "labels": [], "entities": []}, {"text": "For the simulation experiments, we use the following eight European languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv).", "labels": [], "entities": []}, {"text": "These languages are obviously not low-resource languages, however we can use this data to simulate the low-resource setting by only using a small 1,000 tokens of the gold annotations for training.", "labels": [], "entities": []}, {"text": "This evaluation technique is widely used in previous work, and allows us to compare our results with prior stateof-the-art algorithms.", "labels": [], "entities": []}, {"text": "For the real-world experiments, we use the following two low-resource languages: Malagasy, an Austronesian language spoken in Madagascar, and Kinyarwanda, a NigerCongo language spoken in Rwanda.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The POS tagging accuracy for various models in eight languages: Danish (da), Dutch (nl),  German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv). The top results of the  second part are taken from", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7021481692790985}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9408522844314575}]}, {"text": " Table 2: The POS tagging accuracy for various  models in Malagasy and Kinyarwanda. The top  results of the second part are taken from Duong et  al. (2014), evaluated on the same data split.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.7179256975650787}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9592389464378357}]}]}