{"title": [{"text": "Leveraging Cognitive Features for Sentiment Analysis", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9792110621929169}]}], "abstractContent": [{"text": "Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels.", "labels": [], "entities": [{"text": "Sentiments expressed in user-generated short text and sentences", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8294866979122162}]}, {"text": "To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9796035289764404}, {"text": "sarcasm detection", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.9186509847640991}]}, {"text": "Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features.", "labels": [], "entities": [{"text": "Statistical classification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7080747187137604}, {"text": "F-score)", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9766659140586853}, {"text": "polarity detection", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7579558491706848}]}, {"text": "We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment ana-lyzers to handle complex constructs.", "labels": [], "entities": [{"text": "feature significance analysis", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.705605020125707}]}], "introductionContent": [{"text": "This paper addresses the task of Sentiment Analysis (SA) -automatic detection of the sentiment polarity as positive versus negative -of usergenerated short texts and sentences.", "labels": [], "entities": [{"text": "Sentiment Analysis (SA)", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8148726463317871}]}, {"text": "Several sentiment analyzers exist in literature today (.", "labels": [], "entities": [{"text": "sentiment analyzers", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.9284689128398895}]}, {"text": "Recent works, such as, and, attempt to conduct such analyses on user-generated content.", "labels": [], "entities": []}, {"text": "Sentiment analysis remains a hard problem, due to the challenges it poses at the various levels, as summarized below.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9752863049507141}]}], "datasetContent": [{"text": "We use two publicly available datasets for our experiments.", "labels": [], "entities": []}, {"text": "Dataset 1 has been released by which they use for the task of sarcasm understandability prediction.", "labels": [], "entities": [{"text": "sarcasm understandability prediction", "start_pos": 62, "end_pos": 98, "type": "TASK", "confidence": 0.964776893456777}]}, {"text": "Dataset 2 has been used by  for the task of sentiment annotation complexity prediction.", "labels": [], "entities": [{"text": "sentiment annotation complexity prediction", "start_pos": 44, "end_pos": 86, "type": "TASK", "confidence": 0.9603181630373001}]}, {"text": "These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting.", "labels": [], "entities": []}, {"text": "We describe the datasets below.", "labels": [], "entities": []}, {"text": "It contains 994 text snippets with 383 positive and 611 negative examples.", "labels": [], "entities": []}, {"text": "Out of this, 350 are sarcastic or have other forms of irony.", "labels": [], "entities": []}, {"text": "The snippets area collection of reviews, normalized-tweets and quotes.", "labels": [], "entities": []}, {"text": "Each snippet is annotated by seven participants with binary positive/negative polarity labels.", "labels": [], "entities": []}, {"text": "Their eye-movement patterns are recorded with a high quality SR-Research Eyelink-1000 eyetracker (sampling rate 500Hz).", "labels": [], "entities": [{"text": "SR-Research Eyelink-1000 eyetracker", "start_pos": 61, "end_pos": 96, "type": "DATASET", "confidence": 0.9336030681927999}]}, {"text": "The annotation accuracy varies from 70%\u221290% with a Fleiss kappa inter-rater agreement of 0.62.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9251505136489868}, {"text": "Fleiss kappa inter-rater agreement", "start_pos": 51, "end_pos": 85, "type": "METRIC", "confidence": 0.7609690129756927}]}, {"text": "This dataset consists of 1059 snippets comprising movie reviews and normalized tweets.", "labels": [], "entities": []}, {"text": "Each snippet is annotated by five participants with positive, negative and objective labels.", "labels": [], "entities": []}, {"text": "Eye-tracking is done using a low quality Tobii T120 eye-tracker (sampling rate 120Hz).", "labels": [], "entities": [{"text": "Tobii T120 eye-tracker", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.9168696800867716}]}, {"text": "The annotation accuracy varies from 75% \u2212 85% with a Fleiss kappa inter-rater agreement of 0.68.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9246995449066162}, {"text": "Fleiss kappa inter-rater agreement", "start_pos": 53, "end_pos": 87, "type": "METRIC", "confidence": 0.7713336497545242}]}, {"text": "We rule out the objective ones and consider 843 snippets out of which 443 are positive and 400 are negative.", "labels": [], "entities": []}, {"text": "It is essential to check whether our selected datasets really pose challenges to existing sentiment analyzers or not.", "labels": [], "entities": [{"text": "sentiment analyzers", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7969248592853546}]}, {"text": "For this, we implement two statistical classifiers and a rule based classifier to check the test accuracy of Dataset 1 and Dataset 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9801334142684937}]}, {"text": "The statistical classifiers are based on Support Vector Machine (SVM) and N\u00e4ive Bayes (NB) implemented using Weka () and Lib-SVM (Chang and Lin, 2011) APIs.", "labels": [], "entities": [{"text": "Weka", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.9262772798538208}]}, {"text": "These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by and Sentiment 140 (http://www.sentiment140.com/).", "labels": [], "entities": []}, {"text": "The feature-set comprises traditional features for SA reported in a number of papers.", "labels": [], "entities": [{"text": "SA", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.98493492603302}]}, {"text": "They are discussed in section 4 under the category of Sentiment Features.", "labels": [], "entities": [{"text": "Sentiment Features", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8830451369285583}]}, {"text": "The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon).", "labels": [], "entities": [{"text": "MPQA lexicon", "start_pos": 159, "end_pos": 171, "type": "DATASET", "confidence": 0.9407891631126404}]}, {"text": "It also considers negators as ex-plained by and intensifiers as explained by. presents the accuracy of the three systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9991224408149719}]}, {"text": "The F-scores are not very high for all the systems (especially for dataset 1 that contains more sarcastic/ironic texts), possibly indicating that the snippets in our dataset pose challenges for existing sentiment analyzers.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9981095790863037}, {"text": "sentiment analyzers", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.8773019909858704}]}, {"text": "Hence, the selected datasets are ideal for our current experimentation that involves cognitive features.", "labels": [], "entities": []}, {"text": "We test the effectiveness of the enhanced featureset by implementing three classifiers viz., SVM (with linear kernel), NB and Multi-layered Neural Network.", "labels": [], "entities": []}, {"text": "These systems are implemented using  The classification accuracy is reported in Table 2.", "labels": [], "entities": [{"text": "classification", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9370989203453064}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9212226271629333}]}, {"text": "We observe the maximum accuracy with the complete feature-set comprising Sentiment, Sarcasm and Thwarting, and Cognitive features derived from gaze data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994320273399353}]}, {"text": "For this combination, SVM outperforms the other classifiers.", "labels": [], "entities": []}, {"text": "The novelty of our feature design lies in (a) First augmenting sarcasm and thwarting based features (Sr) with sentiment features (Sn), which shoots up the accuracy by 3.1% for Dataset1 and 7.8% for Dataset2 (b) Augmenting gaze features with Sn+Sr, which further increases the accuracy by 0.6% and 1.5% for Dataset 1 and 2 respectively, amounting to an overall improvement of 3.7% and 9.3% respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9992318153381348}, {"text": "accuracy", "start_pos": 276, "end_pos": 284, "type": "METRIC", "confidence": 0.9991022348403931}]}, {"text": "It maybe noted that the addition of gaze features may seem to bring meager improvements in the classification accuracy but the improvements are consistent across datasets and several classifiers.", "labels": [], "entities": [{"text": "classification", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.9512386322021484}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9627528190612793}]}, {"text": "Still, we speculate that aggregating various eye-tracking parameters to extract the cognitive features may have caused loss of information, thereby limiting the improvements.", "labels": [], "entities": []}, {"text": "For example, the graph based features are computed for each participant and eventually averaged to get the graph features fora sentence, thereby not leveraging the power of individual eye-movement patterns.", "labels": [], "entities": []}, {"text": "We intend to address this issue in future.", "labels": [], "entities": []}, {"text": "Since the best (Sn + Sr + Gz) and the second best feature (Sn + Sr) combinations are close in terms of accuracy (difference of 0.6% for dataset 1 and 1.5% for dataset 2), we perform a statistical significance test using McNemar test (\u03b1 = 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9993102550506592}]}, {"text": "The difference in the F-scores turns out to be strongly significant with p = 0.0060 (The odds ratio is 0.489, with a 95% confidence interval).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.997680127620697}]}, {"text": "However, the difference in the F-scores is not statistically significant (p = 0.21) for dataset 2 for the best and second best feature combinations.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9951013922691345}]}], "tableCaptions": [{"text": " Table 1: Classification results for different SA systems for dataset 1 (D1) and dataset 2 (D2). P\u2192  Precision, R\u2192 Recall, F\u2192 F\u02d9score", "labels": [], "entities": []}, {"text": " Table 2: Results for different feature combinations. (P,R,F)\u2192 Precision, Recall, F-score. Feature labels  Uni\u2192Unigram features, Sn\u2192Sentiment features, Sr\u2192Sarcasm features and Gz\u2192Gaze features along  with features related to reading difficulty", "labels": [], "entities": [{"text": "Recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9698476195335388}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9405967593193054}]}, {"text": " Table 3: Features as per their ranking for both  Dataset 1 and Dataset 2. Integer values N in  NGRAM PCA N and IMPLICIT PCA N repre- sent the N th principal component.", "labels": [], "entities": []}, {"text": " Table 4: F-scores on held-out dataset for Com- plex Constructs (Irony), Simple Constructs (Non- irony)", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9965152740478516}]}, {"text": " Table 5: Example test-cases from the heldout dataset. Labels Ex\u2192Existing classifier, Sn\u2192Sentiment  features, Sr\u2192Sarcasm features and Gz\u2192Gaze features. Values (-1,1,0)\u2192 (negative,positive,undefined)", "labels": [], "entities": []}]}