{"title": [{"text": "UniTN End-to-End Discourse Parser for CoNLL 2016 Shared Task", "labels": [], "entities": [{"text": "CoNLL 2016 Shared Task", "start_pos": 38, "end_pos": 60, "type": "DATASET", "confidence": 0.8836278021335602}]}], "abstractContent": [{"text": "Penn Discourse Treebank style discourse parsing is a composite task of detecting explicit and non-explicit discourse relations, their connective and argument spans, and assigning a sense to these relations.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.9510154525438944}, {"text": "discourse parsing", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8037051260471344}]}, {"text": "Due to the composite nature of the task, the end-to-end performance is greatly affected by the error propagation.", "labels": [], "entities": []}, {"text": "This paper describes the end-to-end discourse parser for English submitted to the CoNLL 2016 Shared Task on Shallow Discourse Parsing with the main focus of the parser being on argument spans and the reduction of global error through model selection.", "labels": [], "entities": [{"text": "CoNLL 2016 Shared Task on Shallow Discourse Parsing", "start_pos": 82, "end_pos": 133, "type": "TASK", "confidence": 0.8311547264456749}]}, {"text": "In the end-to-end closed-track evaluation the parser achieves F-measure of 0.2510 out-performing the best system of the previous year.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9995959401130676}]}], "introductionContent": [{"text": "Discourse parsing is a Natural Language Processing (NLP) task with the potential utility for many other Natural Language Processing tasks).", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8529321849346161}]}, {"text": "However, as was illustrated by the CoNLL 2015 Shared Task on Shallow Discourse Parsing (, the task of Penn Discourse Treebank (PDTB) () style discourse parsing is very challenging as the best system achieved the end-to-end parsing performance of F 1 = 0.24.", "labels": [], "entities": [{"text": "CoNLL 2015 Shared Task", "start_pos": 35, "end_pos": 57, "type": "DATASET", "confidence": 0.8622976541519165}, {"text": "Shallow Discourse Parsing", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6464034418265024}, {"text": "Penn Discourse Treebank (PDTB)", "start_pos": 102, "end_pos": 132, "type": "DATASET", "confidence": 0.9382518331209818}, {"text": "discourse parsing", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.6122532039880753}, {"text": "F 1", "start_pos": 246, "end_pos": 249, "type": "METRIC", "confidence": 0.9641983807086945}]}, {"text": "The main reason for the low performance is the composite nature of the task and the error propagation through the long pipeline.", "labels": [], "entities": []}, {"text": "In PDTB discourse relations are binary: a discourse connective and its two arguments.", "labels": [], "entities": []}, {"text": "The arguments are defined syntactically such that Argument 2 is syntactically attached to the connective, and Argument 1 is the other argument.", "labels": [], "entities": [{"text": "Argument", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9720845818519592}]}, {"text": "A discourse relation is assigned a particular sense from the predefined sense hierarchy.", "labels": [], "entities": []}, {"text": "Discourse connective, a member of the closed class, signals the presence of an explicit relation.", "labels": [], "entities": []}, {"text": "Besides explicit discourse relations there are non-explicit relations: implicit relations where a connective is implied and can be inserted, alternative lexicalizations (AltLex) where a connective cannot be inserted and a relation is signaled by a phrase not in the list of discourse connectives, and entity relations (EntRel) where two arguments share the same entity.", "labels": [], "entities": []}, {"text": "Such definition of discourse relations naturally suggests at least two pipelines for the parsing: for explicit and non-explicit relations.", "labels": [], "entities": []}, {"text": "Moreover, since in PDTB non-explicit relations are annotated only in the absence of explicit relations, explicit relation parsing pipeline precedes the non-explicit one.", "labels": [], "entities": [{"text": "explicit relation parsing", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.6477151811122894}]}, {"text": "While detection of discourse connectives is only required for the explicit relations, for both relation types parsing requires identification of argument spans and relation senses.", "labels": [], "entities": []}, {"text": "Consequently, PDTB-style discourse parsing is partitioned into several sub-tasks: (1) explicit discourse connective detection, (2) argument span extraction (with labeling for Argument 1 and 2), and (3) sense classification.", "labels": [], "entities": [{"text": "PDTB-style discourse parsing", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6996306876341502}, {"text": "explicit discourse connective detection", "start_pos": 86, "end_pos": 125, "type": "TASK", "confidence": 0.6674787700176239}, {"text": "argument span extraction", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.6359579066435496}, {"text": "sense classification", "start_pos": 202, "end_pos": 222, "type": "TASK", "confidence": 0.803302675485611}]}, {"text": "The tasks are often conditioned on the type of a relation (explicit or non-explicit) and argument positions (intra-or inter-sentential).", "labels": [], "entities": []}, {"text": "In this paper we describe the end-to-end discourse parser submitted to CoNLL 2016 Shared Task on Shallow Discourse Parsing (.", "labels": [], "entities": [{"text": "CoNLL 2016 Shared Task", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.8316334187984467}, {"text": "Shallow Discourse Parsing", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6887299617131551}]}, {"text": "The parser makes use of token-level sequence labeling with Conditional Random Fields () for the identification of connective and argument spans; and classification for the identification of relation senses and argument positions.", "labels": [], "entities": [{"text": "token-level sequence labeling", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.6159751911958059}]}, {"text": "The main focus of the parser is on argument spans.", "labels": [], "entities": []}, {"text": "For the end-to-end parsing task the models are selected with respect to the global parsing score.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.8725862503051758}]}, {"text": "The overall parser architecture is described in Section 1.", "labels": [], "entities": []}, {"text": "The token-level features used for sequence labeling and argument and relation-level features used for sense classification are described in Section 3.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.677325963973999}, {"text": "sense classification", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8109699487686157}]}, {"text": "The individual discourse parsing subtasks are described in Section 4.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7796525359153748}]}, {"text": "Section 5 describes the official CoNLL 2016 Shared Task evaluation results, and in Section 6 we compare the system to the best systems of the preceding shared task on discourse parsing (.", "labels": [], "entities": [{"text": "CoNLL 2016 Shared Task evaluation", "start_pos": 33, "end_pos": 66, "type": "DATASET", "confidence": 0.8836170434951782}, {"text": "discourse parsing", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.7017027139663696}]}, {"text": "Section 7 provides concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "The official end-to-end parsing evaluation of the CoNLL 2016 Shared Task on Shallow Discourse Parsing carried on TIRA platform () is on a per-discourse relation basis.", "labels": [], "entities": [{"text": "CoNLL 2016 Shared Task", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.8713699132204056}, {"text": "Shallow Discourse Parsing", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.7396531701087952}, {"text": "TIRA platform", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.8040105998516083}]}, {"text": "A relation is considered to be predicted correctly only in case the parser correctly predicts (1) discourse connective head, (2) exact spans and labels of both arguments, and (3) sense of a relation.", "labels": [], "entities": []}, {"text": "The official evaluation is reported for the PDTB development and test sets (sections 22 and 23, respectively) and a blind test set.", "labels": [], "entities": [{"text": "PDTB development and test sets", "start_pos": 44, "end_pos": 74, "type": "DATASET", "confidence": 0.8196911334991455}]}, {"text": "The reported evaluation metrics are (1) explicit discourse connective, (2-4) Argument 1 and Argument 2 spans individually and together, and the sense of a relation.", "labels": [], "entities": [{"text": "Argument", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9693543910980225}]}, {"text": "The reported micro-F 1 measure of the sense classification is equivalent to the end-to-end parsing performance as it considers the error propagation from the upstream tasks.", "labels": [], "entities": [{"text": "micro-F 1 measure", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.8421903252601624}, {"text": "sense classification", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.6833968162536621}]}, {"text": "The metrics are reported for explicit and non-explicit relations individually and jointly.", "labels": [], "entities": []}, {"text": "The performance of the submitted system on all the metrics is reported in.", "labels": [], "entities": []}, {"text": "On the closed-track evaluation, the system achieves end-to-end parsing F 1 of 0.3246, 0.2789 and 0.2510 on the development, test and blind test sets respectively.", "labels": [], "entities": [{"text": "end-to-end parsing F 1", "start_pos": 52, "end_pos": 74, "type": "METRIC", "confidence": 0.7105941027402878}]}], "tableCaptions": [{"text": " Table 2: Task-level and end-to-end F 1 -measures of the discourse parser on the development, test, and  blind test sets for explicit and non-explicit relations individually and jointly for all relations. The task- level performances are reported with the error propagation. Thus, the sense classification performances  are equivalent to the end-to-end parser performances.", "labels": [], "entities": [{"text": "end-to-end F 1", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.6493248840173086}]}, {"text": " Table 3: Precision (P), recall (R) and F 1 (F) of the  end-to-end discourse parsing on the blind test set  for the best CoNLL 2015 Shared Task systems and  the current submission.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9368464201688766}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9480001479387283}, {"text": "F 1 (F)", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9793373346328735}, {"text": "end-to-end discourse parsing", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6749760111172994}]}, {"text": " Table 4: F 1 for the non-explicit argument extrac- tion and parsing.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9792511463165283}, {"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.9704229831695557}]}, {"text": " Table 5: F 1 for the Discourse Connective Detec- tion and explicit intra-sentential Argument 2 span  extraction.", "labels": [], "entities": [{"text": "Argument 2 span  extraction", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.5440758541226387}]}]}