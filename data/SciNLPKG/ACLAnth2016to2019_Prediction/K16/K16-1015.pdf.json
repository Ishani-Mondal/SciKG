{"title": [{"text": "Harnessing Sequence Labeling for Sarcasm Detection in Dialogue from TV Series 'Friends'", "labels": [], "entities": [{"text": "Harnessing Sequence Labeling", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7308112581570944}, {"text": "Sarcasm Detection in Dialogue from TV Series 'Friends'", "start_pos": 33, "end_pos": 87, "type": "TASK", "confidence": 0.8299792607625326}]}], "abstractContent": [{"text": "This paper is a novel study that views sarcasm detection in dialogue as a sequence labeling task, where a dialogue is made up of a sequence of utterances.", "labels": [], "entities": [{"text": "sarcasm detection in dialogue", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.8548726886510849}, {"text": "sequence labeling task", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.773773709932963}]}, {"text": "We create a manually-labeled dataset of dialogue from TV series 'Friends' annotated with sarcasm.", "labels": [], "entities": []}, {"text": "Our goal is to predict sarcasm in each utterance, using sequential nature of a scene.", "labels": [], "entities": [{"text": "predict sarcasm", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7792917191982269}]}, {"text": "We show performance gain using sequence labeling as compared to classification-based approaches.", "labels": [], "entities": []}, {"text": "Our experiments are based on three sets of features, one is derived from information in our dataset, the other two are from past works.", "labels": [], "entities": []}, {"text": "Two sequence labeling algorithms (SVM-HMM and SEARN) outperform three classification algorithms (SVM, Naive Bayes) for all these feature sets, with an increase in F-score of around 4%.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9283997416496277}, {"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9991274476051331}]}, {"text": "Our observations highlight the viability of sequence labeling techniques for sarcasm detection of dialogue.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7179972529411316}, {"text": "sarcasm detection of dialogue", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.9112558662891388}]}], "introductionContent": [{"text": "Sarcasm is defined as 'the use of irony to mock or convey contempt' . An example of a sarcastic sentence is 'Being stranded in traffic is the best way to start the week'.", "labels": [], "entities": []}, {"text": "In this case, the positive word 'best' together with the undesirable situation 'being stranded in traffic' conveys the sarcasm.", "labels": [], "entities": []}, {"text": "Because sarcasm has an implied sentiment (negative) that is different from surface sentiment (positive due to presence of 'best'), it poses a challenge to sentiment analysis systems that aim to determine polarity in text.", "labels": [], "entities": []}, {"text": "Some sarcastic expressions maybe more difficult to detect.", "labels": [], "entities": []}, {"text": "Consider the possibly sarcastic statement 'I absolutely love this restaurant'.", "labels": [], "entities": []}, {"text": "Unlike in the traffic example above, sarcasm in this sentence, if any, can be understood using context which is 'external' to the sentence i.e., beyond common world knowledge.", "labels": [], "entities": []}, {"text": "This external context maybe available in the conversation that this sentence is apart of.", "labels": [], "entities": []}, {"text": "For example, the conversational context maybe situational: the speaker discovers a fly in her soup, then looks at her date and says, 'I absolutely love this restaurant'.", "labels": [], "entities": []}, {"text": "The conversational context may also be verbal: her date says, 'They've taken 40 minutes to bring our appetizers' to which the speaker responds 'I absolutely love this restaurant'.", "labels": [], "entities": []}, {"text": "Both these examples point to the intuition that for dialogue (i.e., data where more than one speaker participates in a discourse), conversational context is often a clue for sarcasm.", "labels": [], "entities": []}, {"text": "For such dialogue, prior work in sarcasm detection (determining whether a text is sarcastic or not) captures context in the form of classifier features such as the topic's probability of evoking sarcasm, or the author's tendency to use sarcasm (.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.7857307493686676}]}, {"text": "In this paper, we present an alternative hypothesis: sarcasm detection of dialogue is better formulated as a sequence labeling task, instead of classification task.", "labels": [], "entities": [{"text": "sarcasm detection of dialogue", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.9389738440513611}, {"text": "sequence labeling task", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.7553435862064362}]}, {"text": "The central message of our work is the efficacy of using sequence labeling as a learning mechanism for sarcasm detection in dialogue, and not in the set of features that we propose for sarcasm detectionalthough we experiment with three feature sets.", "labels": [], "entities": [{"text": "sarcasm detection in dialogue", "start_pos": 103, "end_pos": 132, "type": "TASK", "confidence": 0.8466483354568481}, {"text": "sarcasm detectionalthough", "start_pos": 185, "end_pos": 210, "type": "TASK", "confidence": 0.7753044962882996}]}, {"text": "For our experiments, we create a manually labeled dataset of dialogues from TV series 'Friends'.", "labels": [], "entities": []}, {"text": "Each dialogue is considered to be a sequence of utterances, and every utterance is annotated as sarcastic or non-sarcastic (Details in Section 3).", "labels": [], "entities": []}, {"text": "It maybe argued that a TV series episode is dramatized and hence does not reflect realworld conversations.", "labels": [], "entities": []}, {"text": "However, although the script of 'Friends' is dramatized to suit the situational comedy genre, it takes away nothing from its relevance to reallife conversations except for the volume of sarcastic sentences.", "labels": [], "entities": []}, {"text": "Therefore, our findings from this work can, in theory, be reliably extended to work for any real-life utterances.", "labels": [], "entities": []}, {"text": "Also, such datasets that are not based on real-world conversations have been used in prior work: emotion detection of children stories in  and speech transcripts of a MTV show in.", "labels": [], "entities": [{"text": "emotion detection of children stories", "start_pos": 97, "end_pos": 134, "type": "TASK", "confidence": 0.8264074087142944}]}, {"text": "As a first step in the direction of using sequence labeling, our dataset is a good 'controlled experiment' environment (The details are discussed in Section 2).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6270567923784256}]}, {"text": "In fact, use of a dataset in anew genre (TV series transcripts, specifically) has potential for future work in sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.9281288981437683}]}, {"text": "Our dataset without the actual dialogues from the show (owing to copyright restrictions) maybe available on request.", "labels": [], "entities": []}, {"text": "Based on information available in our dataset (names of speakers, etc.), we present new features.", "labels": [], "entities": []}, {"text": "We then compare two sequence labelers (SEARN and SVM-HMM) with three classifiers (SVM with oversampled and undersampled data, and Na\u00a8\u0131veNa\u00a8\u0131ve Bayes), for this set of features and also for features from two prior works.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9605658650398254}]}, {"text": "In case of our novel features as well as features reported in prior work, sequence labeling algorithms outperform classification algorithms.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.6938773691654205}]}, {"text": "There is an improvement of 3-4% in F-score when sequence labelers are used, as compared to classifiers, for sarcasm detection in our dialogue dataset.", "labels": [], "entities": [{"text": "F-score", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9981991648674011}, {"text": "sarcasm detection", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.9198349714279175}]}, {"text": "Since many datasets such as tweet conversations, chat transcripts, etc. are currently available, our findings will be useful to obtain additional contexts in future work.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 motivates the approach and presents our hypothesis.", "labels": [], "entities": []}, {"text": "Section 3 describes our dataset, while Section 4 presents the features we use (this includes three configurations: novel features based on our dataset, and features from past work).", "labels": [], "entities": []}, {"text": "Experiment setup is in Section 5 and results are given in Section 6.", "labels": [], "entities": []}, {"text": "We present a discussion on which types of sarcasm are handled better by sequence labeling and an error analysis in Section 7, and describe related work in Section 8.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "We design our dataset-derived features based on information available in our dataset.", "labels": [], "entities": []}, {"text": "An utterance consists of three parts: 1.", "labels": [], "entities": []}, {"text": "Speaker: The name of the speaker is the first word of an utterance, and is followed by a colon.", "labels": [], "entities": []}, {"text": "In case of the second utterance in, the speaker is 'Ross' while in the third, the speaker is 'Chandler'.", "labels": [], "entities": []}, {"text": "3. Action words: Actions that a speaker performs while speaking the utterance are indicated in parentheses.", "labels": [], "entities": []}, {"text": "These are useful clues that form additional context.", "labels": [], "entities": []}, {"text": "Unlike speaker and spoken words, action words mayor may not be present.", "labels": [], "entities": []}, {"text": "In the second utterance in, there are no action words while in the third utterance, 'action Chandler does while reading this' are action words.", "labels": [], "entities": []}, {"text": "Based on this information, we design three categories of features (listed in).", "labels": [], "entities": []}, {"text": "Lexical Features: These are unigrams in the spoken words.", "labels": [], "entities": []}, {"text": "We experimented with both count and boolean representations, and the results are comparable.", "labels": [], "entities": []}, {"text": "We report values for boolean representation.", "labels": [], "entities": []}, {"text": "2. Conversational Context Features: In order to capture conversational context, we use three kinds of features.", "labels": [], "entities": []}, {"text": "Action words are unigrams indicated within parentheses.", "labels": [], "entities": []}, {"text": "The intuition is that a character 'raising her eyebrows' (action) is different from saying \"raising her eyebrows\".", "labels": [], "entities": []}, {"text": "As the next feature, we use sentiment score of this utterance.", "labels": [], "entities": [{"text": "sentiment score", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.8316595256328583}]}, {"text": "These are two values: positive and negative scores.", "labels": [], "entities": []}, {"text": "These scores are the positive and negative words present in an utterance.", "labels": [], "entities": []}, {"text": "The third kind of conversational context features is the sentiment score of the previous utterance.", "labels": [], "entities": []}, {"text": "This captures phenomena such as a negative remark from one character eliciting sarcasm from another.", "labels": [], "entities": []}, {"text": "This is similar to the situation described in . Thus, for the third utterance in, the sentiment score of Chandler's utterance forms the Sentiment score feature, while that of Ross's utterance forms Sentiment score of previous utterance.", "labels": [], "entities": []}, {"text": "We experiment with three classification techniques and two sequence labeling techniques: \u2022 SVM (Oversampled) i.e., SVM (O): Sarcastic utterances are duplicated to match the count of non-sarcastic utterances.", "labels": [], "entities": []}, {"text": "\u2022 SVM (Undersampled) i.e., SVM (U): Random non-sarcastic utterances are dropped to match the count of sarcastic utterances.", "labels": [], "entities": []}, {"text": "The two prior works are chosen based on what information was available in our dataset for the purpose of reimplementation.", "labels": [], "entities": []}, {"text": "For example, approaches that use the Twitter profile information or the follower/friends structure in the Twitter, cannot be computed for our dataset.", "labels": [], "entities": []}, {"text": "We also observe the same.", "labels": [], "entities": []}, {"text": "We report weighted average values of precision, recall and F-score computed using five-fold crossvalidation for all experiments, and class-wise precision, recall, F-score wherever necessary.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9996274709701538}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9987450838088989}, {"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9907634854316711}, {"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9682561159133911}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.998916506767273}, {"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9943879246711731}]}, {"text": "The folds are created on the basis of sequences and not utterances.", "labels": [], "entities": []}, {"text": "This means that a sequence does not get split across different folds.", "labels": [], "entities": []}, {"text": "It maybe argued that the benefit in case of sequence labeling is due to our features, and is not a benefit of the sequence labeling formulation itself.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.6335560381412506}]}, {"text": "Hence, we ran these five techniques with all possible combinations of features.", "labels": [], "entities": []}, {"text": "shows the best performance obtained by each of the classifiers, and the corresponding (best) feature combinations.", "labels": [], "entities": []}, {"text": "The table can be read as: SVM (O) obtains a F-score of 81.2% when spoken words, speaker, speaker-listener and sentiment score are used as features.", "labels": [], "entities": [{"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9992757439613342}]}, {"text": "The table shows that even if we consider the best performance of each of the techniques (with different feature sets), classifiers are notable to perform as well as sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7033205628395081}]}, {"text": "The best sequence labeling algorithm (SVM-HMM) gives a Fscore of 84.4% while the best classifier (SVM(O)) has an F-score of 81.2%.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9996887445449829}, {"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9985145926475525}]}, {"text": "We emphasize that both SVM-HMM and SEARN have higher recall values than the three classification techniques.", "labels": [], "entities": [{"text": "SEARN", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9415955543518066}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9992741942405701}]}, {"text": "These findings show that for our novel set of dataset-derived features, sequence labeling works better than classification.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6384641975164413}]}], "tableCaptions": [{"text": " Table 2: Dataset statistics related to: (a) percentage of sarcastic utterances for six lead characters, (b) average  surface positive and negative scores for the two classes, (c) percentage of sarcastic and non-sarcastic utterances  with actions", "labels": [], "entities": []}, {"text": " Table 4: Comparison of sequence labeling techniques  with classification techniques, for features reported in  dataset-derived features", "labels": [], "entities": []}, {"text": " Table 5: Class-wise precision/recall values for all techniques using our dataset-derived features", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9846414923667908}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9499428272247314}]}, {"text": " Table 6: Feature Combinations for which different  techniques exhibit their best performance for dataset- derived features", "labels": [], "entities": []}, {"text": " Table 7: Comparison of sequence labeling techniques  with classification techniques, for features reported in  Gonzalez-Ibanez et al. (2011)", "labels": [], "entities": []}, {"text": " Table 8: Comparison of sequence labeling techniques  with classification techniques, for features reported in  Buschmeier et al. (2014)", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.5977239459753036}]}, {"text": " Table 9: Proportion of utterances of different types of  sarcasm that were correctly labeled by sequence label- ing but incorrectly labeled by classification techniques", "labels": [], "entities": []}]}