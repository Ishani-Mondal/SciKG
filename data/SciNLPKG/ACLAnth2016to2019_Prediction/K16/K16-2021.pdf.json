{"title": [{"text": "Finding Arguments as Sequence Labeling in Discourse Parsing", "labels": [], "entities": [{"text": "Sequence Labeling in Discourse Parsing", "start_pos": 21, "end_pos": 59, "type": "TASK", "confidence": 0.7072009444236755}]}], "abstractContent": [{"text": "This paper describes our system for the CoNLL-2016 Shared Task on Shallow Discourse Parsing on English.", "labels": [], "entities": [{"text": "CoNLL-2016 Shared Task on Shallow Discourse Parsing on English", "start_pos": 40, "end_pos": 102, "type": "TASK", "confidence": 0.6100294258859422}]}, {"text": "We adopt a cascaded framework consisting of nine components, among which six are casted as sequence labeling tasks and the remaining three are treated as classification problems.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.6896131833394369}]}, {"text": "All our sequence labeling and classification models are implemented based on linear models with averaged perceptron training.", "labels": [], "entities": [{"text": "sequence labeling and classification", "start_pos": 8, "end_pos": 44, "type": "TASK", "confidence": 0.6040035486221313}]}, {"text": "Our feature sets are mostly borrowed from previous works.", "labels": [], "entities": []}, {"text": "The main focus of our effort is to recall cases when Arg1 locates at sentences far before the connective phrase, with some yet limited success.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9621607065200806}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data statistics of English.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of instances in terms of dis- tance between the sentence containing Arg1 and  the sentence containing CP, where \"0\" means that  Arg1 locates at the same sentence containing CP,  \"1\" means that Arg1 is in the previous sentence of  the sentence containing CP, and so on. We throw  instances in which Arg1 or Arg2 locates at multi- ple sentences.", "labels": [], "entities": []}, {"text": " Table 4: Distribution of instances in terms of the  number of sentences that one Arg1 locates at,  where the numbers in parenthesis mean the case  when the sentences are discontinuous.", "labels": [], "entities": []}, {"text": " Table 5: Results of different Explicit-Arg1 sentence locators on dev data.", "labels": [], "entities": [{"text": "Explicit-Arg1 sentence locators", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.548773854970932}]}, {"text": " Table 6: Result analysis of different Explicit-Arg1 sentence locators on dev data. We report the distribu- tion of the outputs of each model in terms of distance between the predicted sentence containing Arg1 and  the sentence with CP, where numbers in parenthesis count correct prediction according to gold-standard  answers.", "labels": [], "entities": []}, {"text": " Table 7: Distribution of adjacent sentences having  non-explicit relation.", "labels": [], "entities": []}, {"text": " Table 8: Official results of our system on the dev, test, and blind test datasets. \"All\" means both explicit  and non-explicit relations.", "labels": [], "entities": []}]}