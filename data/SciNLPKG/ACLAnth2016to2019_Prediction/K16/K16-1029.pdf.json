{"title": [{"text": "Compression of Neural Machine Translation Models via Pruning", "labels": [], "entities": [{"text": "Compression of Neural Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5982354164123536}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8631523748238882}]}, {"text": "This paper examines three simple magnitude-based pruning schemes to compress NMT models , namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture.", "labels": [], "entities": []}, {"text": "We demonstrate the efficacy of weight pruning as a compression technique fora state-of-the-art NMT system.", "labels": [], "entities": []}, {"text": "We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task.", "labels": [], "entities": [{"text": "WMT'14 English-German translation task", "start_pos": 133, "end_pos": 171, "type": "TASK", "confidence": 0.6698526293039322}]}, {"text": "This sheds light on the distribution of redundancy in the NMT architecture.", "labels": [], "entities": []}, {"text": "Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.797717014948527}]}, {"text": "NMT is a single deep neural network that is trained end-to-end, holding several advantages such as the ability to capture long-range dependencies in sentences, and generalization to unseen texts.", "labels": [], "entities": [{"text": "generalization to unseen texts", "start_pos": 164, "end_pos": 194, "type": "TASK", "confidence": 0.7950549423694611}]}, {"text": "Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs including EnglishFrench (), English-German (; Luong and * Both authors contributed equally.", "labels": [], "entities": [{"text": "NMT", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9032523036003113}]}, {"text": "Manning, 2015;, EnglishTurkish (, and English-Czech (.", "labels": [], "entities": [{"text": "EnglishTurkish", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.9272620677947998}]}, {"text": "gives an example of an NMT system.", "labels": [], "entities": []}, {"text": "While NMT has a significantly smaller memory footprint than traditional phrase-based approaches (which need to store gigantic phrase-tables and language models), the model size of NMT is still prohibitively large for mobile devices.", "labels": [], "entities": []}, {"text": "For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes ().", "labels": [], "entities": []}, {"text": "Though the trend for bigger and deeper neural networks has brought great progress, it has also introduced over-parameterization, resulting in long running times, overfitting, and the storage size issue discussed above.", "labels": [], "entities": []}, {"text": "A solution to the overparameterization problem could potentially aid all three issues, though the first (long running times) is outside the scope of this paper.", "labels": [], "entities": []}, {"text": "In this paper we investigate the efficacy of weight pruning for NMT as a means of compression.", "labels": [], "entities": []}, {"text": "We show that despite its simplicity, magnitude-based pruning with retraining is highly effective, and we compare three magnitude-based pruning schemes -class-blind, class-uniform and class-distribution.", "labels": [], "entities": []}, {"text": "Though recent work has chosen to use the latter two, we find the first and simplest scheme -class-blind -the most successful.", "labels": [], "entities": []}, {"text": "We are able to prune 40% of the weights of a state-of-the-art NMT system with negligible performance loss, and by adding a retraining phase after pruning, we can prune 80% with no performance loss.", "labels": [], "entities": []}, {"text": "Our pruning experiments also reveal some patterns in the distribution of redundancy in NMT.", "labels": [], "entities": []}, {"text": "In particular we find that higher layers, attention and softmax weights are the most important, while lower layers and the embedding weights hold a lot of redundancy.", "labels": [], "entities": []}, {"text": "For the Long Short-Term Memory (LSTM) architecture, we find that at lower layers the parameters for the input are most crucial, but at higher layers the parameters for the gates also become important.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the effectiveness of our pruning approaches on a state-of-the-art NMT model.", "labels": [], "entities": []}, {"text": "Specifically, an attention-based English-German NMT system from () is considered.", "labels": [], "entities": []}, {"text": "Training data was obtained from WMT'14 consisting of 4.5M sentence pairs (116M English words, 110M German words).", "labels": [], "entities": [{"text": "WMT'14", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.9433479905128479}]}, {"text": "For more details on training hyperparameters, we refer readers to Section 4.1 of ().", "labels": [], "entities": []}, {"text": "All models are tested on newstest2014 (2737 sentences).", "labels": [], "entities": [{"text": "newstest2014", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9555222392082214}]}, {"text": "The model achieves a perplexity of 6.1 and a BLEU score of 20.5 (after unknown word replacement).", "labels": [], "entities": [{"text": "perplexity", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9806148409843445}, {"text": "BLEU score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.98948535323143}, {"text": "word replacement", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.6738100945949554}]}, {"text": "When retraining pruned NMT systems, we use the following settings: (a) we start with a smaller learning rate of 0.5 (the original model uses a learning rate of 1.0), (b) we train for fewer epochs, 4 instead of 12, using plain SGD, (c) a simple learning rate schedule is employed; after 2 epochs, we begin to halve the learning rate every half an epoch, and (d) all other hyperparameters are the We thank the authors of (Luong et al., 2015a) for providing their trained models and assistance in using the codebase at https://github.com/lmthang/nmt.matlab.", "labels": [], "entities": []}, {"text": "The performance of this model is reported under row global (dot) in of ().", "labels": [], "entities": []}, {"text": "same, such as mini-batch size 128, maximum gradient norm 5, and dropout with probability 0.2.", "labels": [], "entities": []}], "tableCaptions": []}