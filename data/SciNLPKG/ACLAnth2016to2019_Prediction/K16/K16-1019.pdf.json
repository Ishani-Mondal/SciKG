{"title": [{"text": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a transition-based parser that jointly produces syntactic and semantic dependencies.", "labels": [], "entities": []}, {"text": "It learns a representation of the entire algorithm state, using stack long short-term memories.", "labels": [], "entities": []}, {"text": "Our greedy inference algorithm has linear time, including feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7044205516576767}]}, {"text": "On the CoNLL 2008-9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.", "labels": [], "entities": [{"text": "CoNLL 2008-9 English shared tasks", "start_pos": 7, "end_pos": 40, "type": "DATASET", "confidence": 0.9236476182937622}]}], "introductionContent": [{"text": "We introduce anew joint syntactic and semantic dependency parser.", "labels": [], "entities": [{"text": "semantic dependency parser", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6407176355520884}]}, {"text": "Our parser draws from the algorithmic insights of the incremental structure building approach of, with two key differences.", "labels": [], "entities": []}, {"text": "First, it learns representations for the parser's entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all.", "labels": [], "entities": []}, {"text": "Second, it uses entirely greedy inference rather than beam search.", "labels": [], "entities": []}, {"text": "We find that it outperforms all previous joint parsing models, including Hender- Joint models like ours have frequently been proposed as away to avoid cascading errors in NLP pipelines; varying degrees of success have been attained fora range of joint syntactic-semantic analysis tasks.", "labels": [], "entities": []}, {"text": "One reason pipelines often dominate is that they make available the complete syntactic parse tree, and arbitrarily-scoped syntactic features-such as the \"path\" between predicate and argument, proposed by-for semantic analysis.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 208, "end_pos": 225, "type": "TASK", "confidence": 0.7797802686691284}]}, {"text": "Such features area mainstay of highperformance semantic role labeling (SRL) systems (), but they are expensive to extract.", "labels": [], "entities": [{"text": "highperformance semantic role labeling (SRL)", "start_pos": 31, "end_pos": 75, "type": "TASK", "confidence": 0.756415707724435}]}, {"text": "This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.9672327041625977}]}, {"text": "The specific advance we employ is the stack LSTM ( ), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser's state is conventionally encoded.", "labels": [], "entities": []}, {"text": "Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6600606739521027}, {"text": "joint syntactic-semantic dependency parsing", "start_pos": 123, "end_pos": 166, "type": "TASK", "confidence": 0.6660540699958801}]}, {"text": "We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design.", "labels": [], "entities": []}, {"text": "Our system's performance does not match that of the top expert-crafted feature-based systems (, systems which perform optimal decoding), or of systems that exploit additional, differently-annotated datasets ().", "labels": [], "entities": []}, {"text": "Many advances in those systems are orthogonal to our model, and we expect future work to achieve further gains by integrating them.", "labels": [], "entities": []}, {"text": "Because our system is very fast-with an end-to-end runtime of 177.6\u00b118 seconds to parse the CoNLL 2009 English test data on a single core-we believe it will be useful in practical set-all are expected to reopen soon expect.01 reopen.", "labels": [], "entities": [{"text": "CoNLL 2009 English test data", "start_pos": 92, "end_pos": 120, "type": "DATASET", "confidence": 0.9788675427436828}]}, {"text": "AM-TMP A1: Example of a joint parse.", "labels": [], "entities": []}, {"text": "Syntactic dependencies are shown by arcs above the sentence and semantic dependencies below; predicates are marked in boldface.", "labels": [], "entities": []}, {"text": "C-denotes continuation of argument A1.", "labels": [], "entities": []}, {"text": "Correspondences between dependencies might be close (between expected and to) or not (between reopen and all). tings.", "labels": [], "entities": []}, {"text": "Our open-source implementation has been released.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model is evaluated on the CoNLL shared tasks on joint syntactic and semantic dependency parsing in 2008 ().", "labels": [], "entities": [{"text": "joint syntactic and semantic dependency parsing", "start_pos": 52, "end_pos": 99, "type": "TASK", "confidence": 0.4998254378636678}]}, {"text": "The standard training, development and test splits of all datasets were used.", "labels": [], "entities": []}, {"text": "Per the shared task guidelines, automatically predicted POS tags and lemmas provided in the datasets were used for all experiments.", "labels": [], "entities": []}, {"text": "As a preprocessing step, pseudo-projectivization of the syntactic trees () was used, which allowed an accurate conversion of even the non-projective syntactic trees into syntactic transitions.", "labels": [], "entities": []}, {"text": "However, the oracle conversion of semantic parses into transitions is not perfect despite using the M-SWAP action, due to the presence of multiple crossing arcs.", "labels": [], "entities": [{"text": "oracle conversion of semantic parses", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.6803903996944427}]}, {"text": "7 The standard evaluation metrics include the syntactic labeled attachment score (LAS), the semantic F 1 score on both in-domain (WSJ) and outof-domain (Brown corpus) data, and their macro average (Macro F 1 ) to score joint systems.", "labels": [], "entities": [{"text": "syntactic labeled attachment score (LAS)", "start_pos": 46, "end_pos": 86, "type": "METRIC", "confidence": 0.7556333839893341}, {"text": "semantic F 1 score", "start_pos": 92, "end_pos": 110, "type": "METRIC", "confidence": 0.7481022104620934}, {"text": "Brown corpus) data", "start_pos": 153, "end_pos": 171, "type": "DATASET", "confidence": 0.8473744988441467}, {"text": "macro average (Macro F 1 )", "start_pos": 183, "end_pos": 209, "type": "METRIC", "confidence": 0.6871357943330493}]}, {"text": "Because the task was defined somewhat differently in each year, each dataset is considered in turn.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Joint parsers: comparison on the CoNLL  2008 test (WSJ+Brown) set.", "labels": [], "entities": [{"text": "CoNLL  2008 test (WSJ+Brown) set", "start_pos": 43, "end_pos": 75, "type": "DATASET", "confidence": 0.9276804195510017}]}, {"text": " Table 3: Comparison on the CoNLL 2009 English  test set. The first block presents results of other  models evaluated for both syntax and semantics on  the CoNLL 2009 task. The second block presents  our models. The third block presents the best pub- lished models, each using its own syntactic pre- processing.", "labels": [], "entities": [{"text": "CoNLL 2009 English  test set", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.9701548337936401}, {"text": "CoNLL 2009 task", "start_pos": 156, "end_pos": 171, "type": "DATASET", "confidence": 0.8961007396380106}]}, {"text": " Table 4: Comparison of macro F 1 scores on the  multilingual CoNLL 2009 test set.", "labels": [], "entities": [{"text": "macro F 1 scores", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.6590774059295654}, {"text": "CoNLL 2009 test set", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.9475811868906021}]}]}