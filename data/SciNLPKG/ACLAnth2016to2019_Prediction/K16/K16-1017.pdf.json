{"title": [{"text": "Modelling Context with User Embeddings for Sarcasm Detection in Social Media", "labels": [], "entities": [{"text": "Sarcasm Detection", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8912061452865601}]}], "abstractContent": [{"text": "We introduce a deep neural network for automated sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8175507187843323}]}, {"text": "Recent work has emphasized the need for models to capitalize on contextual features, beyond lexical and syntactic cues present in utterances.", "labels": [], "entities": []}, {"text": "For example, different speakers will tend to employ sarcasm regarding different subjects and, thus, sarcasm detection models ought to encode such speaker information.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.8354303240776062}]}, {"text": "Current methods have achieved this byway of laborious feature engineering.", "labels": [], "entities": []}, {"text": "By contrast, we propose to automatically learn and then exploit user embeddings, to be used in concert with lexical signals to recognize sarcasm.", "labels": [], "entities": []}, {"text": "Our approach does not require elaborate feature engineering (and concomi-tant data scraping); fitting user embed-dings requires only the text from their previous posts.", "labels": [], "entities": []}, {"text": "The experimental results show that the our model outperforms a state-of-the-art approach leveraging an extensive set of carefully crafted features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Existing social media analysis systems are hampered by their inability to accurately detect and interpret figurative language.", "labels": [], "entities": []}, {"text": "This is particularly relevant in domains like the social sciences and politics, in which the use of figurative communication devices such as verbal irony (roughly, sarcasm) is common.", "labels": [], "entities": []}, {"text": "Sarcasm is often used by individuals to express opinions on complex matters and regarding specific targets (.", "labels": [], "entities": []}, {"text": "Early computational models for verbal irony and sarcasm detection tended to rely on shallow methods exploiting conditional token count regularities.", "labels": [], "entities": [{"text": "verbal irony", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.7219459414482117}, {"text": "sarcasm detection", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8849863409996033}]}, {"text": "But lexical clues alone are insufficient to discern ironic intent.", "labels": [], "entities": [{"text": "discern ironic intent", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7877328793207804}]}, {"text": "Appreciating the context of utterances is critical for this; even for humans ().", "labels": [], "entities": []}, {"text": "Indeed, the exact same sentence can be interpreted as literal or sarcastic, depending on the speaker.", "labels": [], "entities": []}, {"text": "Consider the sarcastic tweet in (ignoring for the moment the attached #sarcasm hashtag).", "labels": [], "entities": []}, {"text": "Without knowing the author's political leanings, it would be difficult to conclude with certainty whether the remark was intended sarcastically or in earnest.", "labels": [], "entities": []}, {"text": "Recent work in sarcasm detection on social media has tried to incorporate contextual information by exploiting the preceding messages of a user, to e.g., detect contrasts in sentiments expressed towards named entities (, infer behavioural traits ( and capture the relationship between authors and the audience (.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8994190692901611}]}, {"text": "However, all of these approaches require the design and implementation of complex features that explicitly encode the content and (relevant) context of messages to be classified.", "labels": [], "entities": []}, {"text": "This feature engineering is labor intensive, and depends on external tools and resources.", "labels": [], "entities": []}, {"text": "Therefore, deploying such systems in practice is expensive, time-consuming and unwieldy.", "labels": [], "entities": []}, {"text": "We propose a novel approach to sarcasm detection on social media that does not require extensive manual feature engineering.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9818346500396729}]}, {"text": "Instead, we develop a neural model that learns to represent and exploit embeddings of both content and context.", "labels": [], "entities": []}, {"text": "For the former, we induce vector lexical repre-sentations via a convolutional layer; for the latter, our model learns user embeddings.", "labels": [], "entities": []}, {"text": "Inference concerning whether an utterance (tweet) was intended ironically (or not) is then modelled as a joint function of lexical representations and corresponding author embeddings.", "labels": [], "entities": [{"text": "Inference concerning whether an utterance (tweet) was intended ironically", "start_pos": 0, "end_pos": 73, "type": "TASK", "confidence": 0.6567203023216941}]}, {"text": "The main contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "(i) We propose a novel convolutional neural network based model that explicitly learns and exploits user embeddings in conjunction with features derived from utterances.", "labels": [], "entities": []}, {"text": "(ii) We show that this model outperforms the strong baseline recently proposed by by more than 2% in absolute accuracy, while obviating the need to manually engineer features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9250205159187317}]}, {"text": "(iii) We show that the learned user embeddings can capture relevant user attributes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We replicated experimental setup using (a subset of) the same Twitter corpus.", "labels": [], "entities": []}, {"text": "The labels were inferred from selfdeclarations of sarcasm, i.e., a tweet is considered sarcastic if it contains the hashtag #sarcasm or #sarcastic and deemed non-sarcastic otherwise.", "labels": [], "entities": []}, {"text": "To comply with Twitter terms of service, we were given only the tweet ids along with the corresponding labels and had to retrieve the messages ourselves.", "labels": [], "entities": []}, {"text": "By the time we tried to retrieve the messages, some of them were not available.", "labels": [], "entities": []}, {"text": "We also did not have access to the historical user tweets used by Bamman and Smith, hence, for each author and mentioned user, we scraped additional tweets from their Twitter feed.", "labels": [], "entities": []}, {"text": "Due to restrictions in the Twitter API, we were only able to crawl at most 1000 historical tweets per user.", "labels": [], "entities": []}, {"text": "Furthermore, we were unable to collect historical tweets fora significant proportion of the users, thus, we discarded messages for which no contextual information was available, resulting in a corpus of 11, 541 tweets involving 12, 500 unique users (authors and mentioned users).", "labels": [], "entities": []}, {"text": "It should also be noted that our historical tweets were posted after the ones in the corpus used for the experiments.", "labels": [], "entities": []}, {"text": "Evaluation was performed via 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "For each split, we fit models to 80% of the data, tuned them on 10% and tested on the remaining, held-out 10%.", "labels": [], "entities": []}, {"text": "These data splits were kept fixed in all the experiments.", "labels": [], "entities": []}, {"text": "For the linear classifiers, in each split, the regularization constant was selected with a linear search over the (a) Performance of the linear classifier baselines.", "labels": [], "entities": []}, {"text": "We include the results reported by as a reference.", "labels": [], "entities": []}, {"text": "Discrepancies between their reported results and those we achieved with our re-implementation reflect the fact that their experiments were performed using a significantly larger training set and more historical tweets than we had access to.", "labels": [], "entities": []}, {"text": "(b) Performance of the proposed neural models.", "labels": [], "entities": []}, {"text": "We compare simple neural models that only consider the lexical content of a message (first 3 bars) with architectures that explicitly model the context.", "labels": [], "entities": []}, {"text": "Bars 4 and 5 show the gains obtained by pretraining the user embeddings.", "labels": [], "entities": []}, {"text": "The last 2 bars compare different negative sampling procedures for the user embedding pre-training.", "labels": [], "entities": []}, {"text": "range C = [1e \u22124 , 1e \u22123 , 1e \u22122 , 1e \u22121 , 1, 10] using the training set to fit the model and evaluating on the tuning set.", "labels": [], "entities": []}, {"text": "After selecting the best regularization constant, the model was re-trained on the union of the train and tune sets, and evaluated on the test set.", "labels": [], "entities": []}, {"text": "To train our neural model, we first had to choose a suitable architecture and hyperparameter set.", "labels": [], "entities": []}, {"text": "However, selecting the optimal network parametrization would require an extensive search over a large configuration space.", "labels": [], "entities": []}, {"text": "Therefore, in these experiments, we followed the recommendations in We performed random search by sampling without replacement over half of the possible configurations.", "labels": [], "entities": []}, {"text": "For each data split, 20% of the training set was reserved for early stopping.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.8284754157066345}]}, {"text": "We compared the sampled configurations by fitting the model on the remaining training data and testing on the tune set.", "labels": [], "entities": []}, {"text": "After choosing the best configuration, we retrained the model on the union of the train and tune set (again reserving 20% of the data for early stopping) and evaluated on the test set.", "labels": [], "entities": []}, {"text": "The model was trained by minimizing the crossentropy error between the predictions and true labels, the gradients w.r.t to the network parameters were computed with backpropagation) and the model weights were updated with the AdaDelta rule).", "labels": [], "entities": []}, {"text": "presents the main experimental results.", "labels": [], "entities": []}, {"text": "In, we show the performance of linear classifiers with the manually engineered feature sets proposed by.", "labels": [], "entities": []}, {"text": "Our results differ slightly from those originally reported.", "labels": [], "entities": []}, {"text": "Nonetheless, we observe the same general trends: namely, that including contextual features significantly improves the performance, and that the biggest gains are attributable to features encoding information about the authors of tweets.", "labels": [], "entities": []}], "tableCaptions": []}