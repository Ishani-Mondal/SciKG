{"title": [{"text": "Discourse Relation Sense Classification Using Cross-argument Semantic Similarity Based on Word Embeddings", "labels": [], "entities": [{"text": "Discourse Relation Sense Classification", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8711151778697968}]}], "abstractContent": [{"text": "This paper describes our system for the CoNLL 2016 Shared Task's supplementary task on Discourse Relation Sense Classification.", "labels": [], "entities": [{"text": "CoNLL 2016 Shared Task", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.7888420820236206}, {"text": "Discourse Relation Sense Classification", "start_pos": 87, "end_pos": 126, "type": "TASK", "confidence": 0.868111789226532}]}, {"text": "Our official submission employs a Logistic Regression classifier with several cross-argument similarity features based on word embeddings and performs with overall F-scores of 64.13 for the Dev set, 63.31 for the Test set and 54.69 for the Blind set, ranking first in the Overall ranking for the task.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9975838661193848}]}, {"text": "We compare the feature-based Logistic Regression classifier to different Convolutional Neu-ral Network architectures.", "labels": [], "entities": [{"text": "Logistic Regression classifier", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.759154220422109}]}, {"text": "After the official submission we enriched our model for Non-Explicit relations by including similarities of explicit connectives with the relation arguments, and part of speech similarities based on modal verbs.", "labels": [], "entities": []}, {"text": "This improved our Non-Explicit result by 1.46 points on the Dev set and by 0.36 points on the Blind set.", "labels": [], "entities": [{"text": "Dev set", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9158667027950287}, {"text": "Blind set", "start_pos": 94, "end_pos": 103, "type": "DATASET", "confidence": 0.6944177150726318}]}], "introductionContent": [{"text": "The CoNLL 2016 Shared Task on Shallow Discourse Parsing () focuses on identifying individual discourse relations presented in text.", "labels": [], "entities": [{"text": "Shallow Discourse Parsing", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6888587474822998}]}, {"text": "This year the shared task has a main track that requires end-to-end discourse relation parsing and a supplementary task that is restricted to discourse relation sense classification.", "labels": [], "entities": [{"text": "discourse relation parsing", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.6307270129521688}, {"text": "discourse relation sense classification", "start_pos": 142, "end_pos": 181, "type": "TASK", "confidence": 0.6866947039961815}]}, {"text": "For the main task, systems are required to build a system that given a raw text as input can identify arguments Arg1 and Arg2 that are related in the discourse, and also classify the type of the relation, which can be Explicit, Implicit, AltLex or EntRel.", "labels": [], "entities": []}, {"text": "A further attribute to be detected is the relation Sense, which can be one of 15 classes organized hierarchically in 4 parent classes.", "labels": [], "entities": []}, {"text": "With this work we participate in the Supplementary Task on Discourse Relation Sense Classification in English.", "labels": [], "entities": [{"text": "Supplementary Task on Discourse Relation Sense Classification", "start_pos": 37, "end_pos": 98, "type": "TASK", "confidence": 0.7644457689353398}]}, {"text": "The task is to predict the discourse relation sense when the arguments Arg1, Arg2 are given, as well as the Discourse Connective in case of explicit marking.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9045445919036865}, {"text": "Arg2", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.8447705507278442}]}, {"text": "In our contribution we compare different approaches including a Logistic Regression classifier using similarity features based on word embeddings, and two Convolutional Neural Network architectures.", "labels": [], "entities": [{"text": "Logistic Regression classifier", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7819427649180094}]}, {"text": "We show that an approach using only word embeddings retrieved from word2vec () and cross-argument similarity features is simple and fast, and yields results that rank first in the Overall, second in the Explicit and forth in the Non-Explicit sense classification task.", "labels": [], "entities": [{"text": "Non-Explicit sense classification task", "start_pos": 229, "end_pos": 267, "type": "TASK", "confidence": 0.6964807212352753}]}, {"text": "Our system's code is publicly accessible 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In we compare different models for NonExplicit relation sense classification trained on the Train and evaluated on the Dev set.", "labels": [], "entities": [{"text": "NonExplicit relation sense classification", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.6130829527974129}]}, {"text": "The first three columns show the results obtained with three approaches that use only features based on word embeddings.", "labels": [], "entities": []}, {"text": "We use word2vec word embeddings.", "labels": [], "entities": []}, {"text": "We also experimented with pre-trained dependency-based word embeddings (, but this yielded slightly worse results on the Dev set.", "labels": [], "entities": [{"text": "Dev set", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.8735065758228302}]}, {"text": "Logistic Regression (LR).", "labels": [], "entities": [{"text": "Logistic Regression (LR)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7800600171089173}]}, {"text": "The LR column shows the results from a Logistic Regression classifier that uses only the concatenated features from the centroid representations built from the words of Arg1 and Arg2.: Evaluation of our official submission system, trained on Train 2016 and evaluated on Dev, Test and Blind sets.", "labels": [], "entities": []}, {"text": "Comparison with our official system and our improved system with the official results of CoNLL 2015 Shared task's best system ( and CoNLL 2016 Shared Task best systems in Explicit (Jain, 2016) and Non-Explicit (.", "labels": [], "entities": []}, {"text": "system parameters as proposed in: filter windows with size 3,4,5 with 100 feature maps each, dropout probability 0.5 and mini-batch of size 50.", "labels": [], "entities": []}, {"text": "We train the model with 50 epochs.", "labels": [], "entities": []}, {"text": "The CNN ARC-1M column shows results from our modification of ARC-1 CNN for sentence matching (see Section 3.3) fed with Arg1 and Arg2 word tokens' vector representations from the Word2Vec word embeddings.", "labels": [], "entities": [{"text": "sentence matching", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7309225648641586}, {"text": "Word2Vec word embeddings", "start_pos": 179, "end_pos": 203, "type": "DATASET", "confidence": 0.896551787853241}]}, {"text": "We use filter windows with size 3,4,5 with 100 feature maps each, shared between the two argument convolutions, dropout probability 0.5 and mini-batch of size 50 as proposed in.", "labels": [], "entities": []}, {"text": "We train the model with 50 epochs.", "labels": [], "entities": []}, {"text": "Comparing LR, CNN and CNN ARC-1M according to their ability to classify different classes we observe that CNN ARC-1M performs best in detecting Contingency.Cause.Reason and Contingency.Cause.Result with a substantial margin over the other two models.", "labels": [], "entities": [{"text": "CNN", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.9089148640632629}, {"text": "CNN ARC-1M", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.861734926700592}]}, {"text": "The CNN model outperforms the LR and CNN-ARC1M for Comparison.Contrast, EntRel, Expansion.Conjunction and Expansion.Instantiation but cannot capture any Expansion.Restatement which leads to worse overall results compared to the others.", "labels": [], "entities": [{"text": "EntRel", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9465225338935852}]}, {"text": "These insights show that the Neural Network models are able to capture some dependencies between the relation arguments.", "labels": [], "entities": []}, {"text": "For Contingency.Cause.Results, CNN ARC-1M even clearly outperforms the LR models enhanced with similarity features (discussed below).", "labels": [], "entities": [{"text": "CNN ARC-1M", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.810796320438385}]}, {"text": "We also implemented a modified version of the CNN ARC-2 architecture of, which uses a cross-argument convolution layer, but it yielded much worse results.", "labels": [], "entities": [{"text": "CNN ARC-2 architecture", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.8499780495961508}]}, {"text": "LR with Embeddings + Features The last three columns in show the results of our featurebased Logistic Regression approach with different feature groups on top of the embedding representations of the arguments.", "labels": [], "entities": []}, {"text": "Column E+Sim shows the results from our official submission and the other two columns show results for additional features that we added after the submission deadline.", "labels": [], "entities": []}, {"text": "Adding the cross-argument similarity features (without the POS modal verbs similarities) improves the overall result of the embeddings-only Logistic Regression (LR) baseline significantly from F-score 35.54 to 40.32.", "labels": [], "entities": [{"text": "embeddings-only Logistic Regression (LR) baseline", "start_pos": 124, "end_pos": 173, "type": "METRIC", "confidence": 0.6627570986747742}, {"text": "F-score", "start_pos": 193, "end_pos": 200, "type": "METRIC", "confidence": 0.9476816654205322}]}, {"text": "It also improves the result on almost all senses individually.", "labels": [], "entities": []}, {"text": "Adding Explicit connective similarities features improves the All result by 0.67 points (E+Sim+Conn).", "labels": [], "entities": [{"text": "All", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9989343285560608}]}, {"text": "It also improves the performance on Tem-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of our official submission system, trained on Train 2016 and evaluated on Dev, Test  and Blind sets. Comparison with our official system and our improved system with the official results  of CoNLL 2015 Shared task's best system (", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of different systems and feature configurations for Non-Explicit relation sense clas- sification, trained on Train 2016 and evaluated on Dev. F-score is presented.", "labels": [], "entities": [{"text": "Dev", "start_pos": 158, "end_pos": 161, "type": "DATASET", "confidence": 0.9421089887619019}, {"text": "F-score", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9963280558586121}]}]}