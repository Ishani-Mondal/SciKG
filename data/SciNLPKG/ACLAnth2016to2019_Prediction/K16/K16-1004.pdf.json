{"title": [{"text": "Semi-supervised Clustering for Short Text via Deep Representation Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we propose a semi-supervised method for short text clustering , where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering.", "labels": [], "entities": [{"text": "short text clustering", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6295183499654134}]}, {"text": "We design a novel objective to combine the representation learning process and the k-means clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) re-estimate the cluster cen-troids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping cen-troids and cluster assignments fixed.", "labels": [], "entities": []}, {"text": "Experimental results on four datasets show that our method works significantly better than several other text clustering methods.", "labels": [], "entities": [{"text": "text clustering", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.7344428598880768}]}], "introductionContent": [{"text": "Text clustering is a fundamental problem in text mining and information retrieval.", "labels": [], "entities": [{"text": "Text clustering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7942601144313812}, {"text": "text mining", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.815252274274826}, {"text": "information retrieval", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7819564938545227}]}, {"text": "Its task is to group similar texts together such that texts within a cluster are more similar to texts in other clusters.", "labels": [], "entities": []}, {"text": "Usually, a text is represented as a bag-ofwords or term frequency-inverse document frequency (TF-IDF) vector, and then the k-means algorithm) is performed to partition a set of texts into homogeneous groups.", "labels": [], "entities": [{"text": "term frequency-inverse document frequency (TF-IDF) vector", "start_pos": 51, "end_pos": 108, "type": "METRIC", "confidence": 0.7962331846356392}]}, {"text": "However, when dealing with short texts, the characteristics of short text and clustering task raise several issues for the conventional unsupervised clustering algorithms.", "labels": [], "entities": []}, {"text": "First, the number of uniqe words in each short text is small, as a re-(a) What's the color of apples?", "labels": [], "entities": []}, {"text": "(b) When will this apple be ripe?", "labels": [], "entities": []}, {"text": "(c) Do you like apples?", "labels": [], "entities": []}, {"text": "(d) What's the color of oranges?", "labels": [], "entities": []}, {"text": "(e) When will this orange be ripe?", "labels": [], "entities": []}, {"text": "(f) Do you like oranges?: Examples for short text clustering.", "labels": [], "entities": [{"text": "short text clustering", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.6129769682884216}]}, {"text": "sult, the lexcical sparsity issue usually leads to poor clustering quality.", "labels": [], "entities": []}, {"text": "Second, fora specific short text clustering task, we have prior knowledge or particular intentions before clustering, while fully unsupervised approaches may learn some classes the other way around.", "labels": [], "entities": [{"text": "short text clustering task", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.7328587174415588}]}, {"text": "Take the sentences in for example, those sentences can be clustered into different partitions based on different intentions: apple {a, b, c} and orange {d, e, f} with a fruit type intention, or what-question {a, d}, when-question {b, e}, and yes/no-question cluster {c, f} with a question type intension.", "labels": [], "entities": []}, {"text": "To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia ( or an ontology.", "labels": [], "entities": []}, {"text": "But this approach requires the annotated knowledge, which is also language dependent.", "labels": [], "entities": []}, {"text": "So the other direction, which directly encode texts into distributed vectors with neural networks, becomes more interesting.", "labels": [], "entities": []}, {"text": "To tackle the second problem, semi-supervised approaches (e.g. () have gained significant popularity in the past decades.", "labels": [], "entities": []}, {"text": "Our question is can we have a unified model to integrate neural networks into the semi-supervised framework?", "labels": [], "entities": []}, {"text": "In this paper, we propose a unified framework for the short text clustering task.", "labels": [], "entities": [{"text": "short text clustering task", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.7188712581992149}]}, {"text": "We employ a deep neural network model to represent short sentences, and integrate it into a semi-supervised algorithm.", "labels": [], "entities": []}, {"text": "Concretely, we extend the objective in the classical unsupervised k-means algorithm by adding a penalty term from labeled data.", "labels": [], "entities": []}, {"text": "Thus, the new objective covers three key groups of parameters: centroids of clusters, the cluster assignment for each text, and the parameters within deep neural networks.", "labels": [], "entities": []}, {"text": "In the training procedure, we start from random initialization of centroids and neural networks, and then optimize the objective iteratively through three steps until converge: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) re-estimate cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.", "labels": [], "entities": []}, {"text": "Experimental results on four different datasets show that our method achieves significant improvements over several other text clustering methods.", "labels": [], "entities": [{"text": "text clustering", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7429542541503906}]}, {"text": "In following parts, we first describe our neural network models for text representation (Section 2).", "labels": [], "entities": [{"text": "text representation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8263454735279083}]}, {"text": "Then we introduce our semi-supervised clustering method and the learning algorithm (Section 3).", "labels": [], "entities": []}, {"text": "Finally, we evaluate our method on four different datasets (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method on four short text datasets.", "labels": [], "entities": []}, {"text": "(1) question type is the TREC question dataset (, where all the questions are classified into 6 categories: abbreviation, description, entity, human, location and numeric.", "labels": [], "entities": [{"text": "TREC question dataset", "start_pos": 25, "end_pos": 46, "type": "DATASET", "confidence": 0.8641844391822815}]}, {"text": "(2) ag news dataset contains short texts extracted from the AG's news corpus, where all the texts are classified into 4 categories: World, Sports, Business, and Sci/Tech (Zhang and LeCun, 2015).", "labels": [], "entities": [{"text": "ag news dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.6102397342522939}, {"text": "AG's news corpus", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.924841582775116}]}, {"text": "(3) dbpedia is the DBpedia ontology dataset, which is constructed by picking 14 non-overlapping classes from DBpedia 2014 ( summarizes the statistics of these datasets.", "labels": [], "entities": [{"text": "DBpedia ontology dataset", "start_pos": 19, "end_pos": 43, "type": "DATASET", "confidence": 0.8922497828801473}, {"text": "DBpedia 2014", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.925558865070343}]}, {"text": "In all experiments, we set the size of word vector dimension as d=300 2 , and pre-train the word vectors with the word2vec toolkit () on the English Gigaword (LDC2011T07).", "labels": [], "entities": [{"text": "English Gigaword (LDC2011T07)", "start_pos": 141, "end_pos": 170, "type": "DATASET", "confidence": 0.8975541591644287}]}, {"text": "The number of clusters is set to be the same number of labels in the dataset.", "labels": [], "entities": []}, {"text": "The clustering performance is evaluated with two metrics: Adjusted Mutual Information (AMI) () and accuracy (ACC).", "labels": [], "entities": [{"text": "Adjusted Mutual Information (AMI)", "start_pos": 58, "end_pos": 91, "type": "METRIC", "confidence": 0.8863151669502258}, {"text": "accuracy (ACC)", "start_pos": 99, "end_pos": 113, "type": "METRIC", "confidence": 0.9611723721027374}]}, {"text": "In order to show the statistical significance, the performance of each experiment is the average of 10 trials.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics for the short text datasets", "labels": [], "entities": [{"text": "short text datasets", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.6861554781595866}]}, {"text": " Table 4: Performance of all systems on each dataset.", "labels": [], "entities": []}]}