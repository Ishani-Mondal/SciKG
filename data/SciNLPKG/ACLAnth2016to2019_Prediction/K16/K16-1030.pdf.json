{"title": [{"text": "Modeling the Usage of Discourse Connectives as Rational Speech Acts", "labels": [], "entities": []}], "abstractContent": [{"text": "Discourse relations can either be implicit or explicitly expressed by markers, such as 'therefore' and 'but'.", "labels": [], "entities": []}, {"text": "How a speaker makes this choice is a question that is not well understood.", "labels": [], "entities": []}, {"text": "We propose a psy-cholinguistic model that predicts whether a speaker will produce an explicit marker given the discourse relation s/he wishes to express.", "labels": [], "entities": []}, {"text": "Based on the framework of the Rational Speech Acts model, we quantify the utility of producing a marker based on the information-theoretic measure of surprisal, the cost of production, and a bias to maintain uniform information density throughout the utterance.", "labels": [], "entities": []}, {"text": "Experiments based on the Penn Discourse Treebank show that our approach outperforms state-of-the-art approaches, while giving an explanatory account of the speaker's choice.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.9846136768658956}]}], "introductionContent": [{"text": "Speakers or authors 1 produce informative utterances, such that the listeners or readers can understand his/her message.", "labels": [], "entities": []}, {"text": "Grice's Maxim of Quantity states that human speakers communicate by being as informative as required, but no more.", "labels": [], "entities": []}, {"text": "If a speaker always tries to provide as much information as possible, the resulting utterance could become excessively long and tedious.", "labels": [], "entities": []}, {"text": "Such utterance is not only effort consuming for the speaker to produce, but also contains redundant information that is not necessary for the listener.", "labels": [], "entities": []}, {"text": "In this work, we model how speakers plan the presentation of discourse structure optimally in terms of informativeness.", "labels": [], "entities": []}, {"text": "Specifically, we propose a model that predicts whether the speaker will use or omit a discourse connective, given the sense of discourse relation s/he wants to convey.", "labels": [], "entities": []}, {"text": "Discourse relations are relations between unit of texts (known as arguments) that make a document coherent.", "labels": [], "entities": []}, {"text": "These relations can be marked in the surface text or inferred by the readers, as shown in the below examples.", "labels": [], "entities": []}, {"text": "1. It was a great movie, but I did not like it.", "labels": [], "entities": []}, {"text": "2. It was a great movie, therefore I liked it.", "labels": [], "entities": []}, {"text": "3. It was a great movie.", "labels": [], "entities": []}, {"text": "The word 'but' indicates a Concession relation in Example (1), and 'therefore' indicates a Result relation in Example (2).", "labels": [], "entities": [{"text": "Result", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9261853694915771}]}, {"text": "We call 'but' and 'therefore' explicit discourse connectives (DCs).", "labels": [], "entities": [{"text": "explicit discourse connectives (DCs)", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.7847356696923574}]}, {"text": "In Example (3), DCs are absent but a Result relation can be inferred.", "labels": [], "entities": [{"text": "Result", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9570717811584473}]}, {"text": "We say the DC is implicit in this case.", "labels": [], "entities": [{"text": "DC", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9386995434761047}]}, {"text": "Explicit DCs are highly informative cues to identify discourse relations ( while implicit DCs are more ambiguous.", "labels": [], "entities": []}, {"text": "For example, 'I liked it' can also be read as a Justification for the first sentence in Example (3).", "labels": [], "entities": []}, {"text": "Marking a discourse relation or not is subject to ambiguity and redundancy.", "labels": [], "entities": [{"text": "Marking a discourse relation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8775238990783691}]}, {"text": "On one hand, using an explicit DC avoids ambiguity.", "labels": [], "entities": []}, {"text": "For example, if the DC 'but' is omitted in Example (1), readers may have problems in inferring the Concession sense.", "labels": [], "entities": [{"text": "Example", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8846731781959534}]}, {"text": "On the other hand, if the intended discourse sense is highly predictable, it is verbose or redundant to insert an explicit DC in the utterance, such as the DC 'therefore' in Example (2).", "labels": [], "entities": [{"text": "Example", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.8783367872238159}]}, {"text": "A model that predicts the markedness of discourse relations not only contributes to a better understanding of the human language production mechanism, but is also important in generating natural, humanlike texts and dialogues.", "labels": [], "entities": []}, {"text": "In particular, the degree of markedness in discourse relations differs cross-lingually.", "labels": [], "entities": []}, {"text": "analyze the manual alignments of explicit and implicit DCs in a Chinese-English translation corpus and find that 30% of implicit DCs in Chinese are translated to explicit DCs in English.", "labels": [], "entities": []}, {"text": "It remains a challenge for machine translation systems to explicitate or implicitate discourse relations in the source texts as human translators do, since the markedness of the translation is subject to the discourse planning of the target text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7391034364700317}]}, {"text": "In order to explain how human speakers choose the optimal level of markedness in his utterance, we model how speakers rationally balance between ambiguity and redundancy.", "labels": [], "entities": []}, {"text": "In particular, we use the Rational Speech Acts (RSA) model) to predict how speakers reason about the ambiguity of an utterance.", "labels": [], "entities": [{"text": "Rational Speech Acts (RSA)", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6778955658276876}]}, {"text": "In addition, we model how speakers adjust the redundancy of the utterance following the Uniform Information Density (UID) principle ().", "labels": [], "entities": [{"text": "Uniform Information Density (UID)", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.6185199866692225}]}, {"text": "We apply the framework to predict whether an explicit or implicit DC is used in corpus data, given the two arguments of the discourse relations and the discourse sense to be conveyed.", "labels": [], "entities": []}, {"text": "Our model not only achieves higher accuracy comparing with previous work, but also provides an interpretable account of various cognitive factors behind the predicted decision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9988417029380798}]}, {"text": "We start by a review of related work in Section 2, followed by the descriptions of our model in Section 3 and experiments in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We apply the model to simulate speaker's choice of explicit or implicit DC for discourse relations in the PDTB corpus.", "labels": [], "entities": [{"text": "PDTB corpus", "start_pos": 106, "end_pos": 117, "type": "DATASET", "confidence": 0.8742600083351135}]}, {"text": "The aim of the experiment is to answer two questions:  We first describe the details of the data we use in the experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sense distribution of explicit and implicit  DCs in screened data set.", "labels": [], "entities": []}, {"text": " Table 2: Accuracies and F1 scores of predicted DC markedness. The best values are bolded.  + / ++ :significant improvement over baseline (BL) accuracy at p < 0.05 and p < 0.001 respectively;  * :significant improvement over state-of-the-art (SOA) accuracy at p < 0.03 (by Pearson's \u03c7 2 test)  (refer to Section 3.2 for abbreviations of discourse context C.)", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9977879524230957}, {"text": "F1", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9993340373039246}, {"text": "baseline (BL) accuracy", "start_pos": 129, "end_pos": 151, "type": "METRIC", "confidence": 0.739987576007843}, {"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9108145833015442}, {"text": "Pearson's \u03c7 2 test", "start_pos": 273, "end_pos": 291, "type": "DATASET", "confidence": 0.8155287623405456}]}]}