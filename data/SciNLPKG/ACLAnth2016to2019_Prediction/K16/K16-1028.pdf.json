{"title": [{"text": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond", "labels": [], "entities": [{"text": "Abstractive Text Summarization", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5878088772296906}]}], "abstractContent": [{"text": "In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.6138908863067627}]}, {"text": "We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling keywords , capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time.", "labels": [], "entities": [{"text": "summarization", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9807979464530945}]}, {"text": "Our work shows that many of our proposed models contribute to further improvement in performance.", "labels": [], "entities": []}, {"text": "We also propose anew dataset consisting of multi-sentence summaries , and establish performance benchmarks for further research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstractive text summarization is the task of generating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage.", "labels": [], "entities": [{"text": "Abstractive text summarization is the task of generating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage", "start_pos": 0, "end_pos": 175, "type": "Description", "confidence": 0.8135110884904861}]}, {"text": "We use the adjective 'abstractive' to denote a summary that is not a mere selection of a few existing passages or sentences extracted from the source, but a compressed paraphrasing of the main contents of the document, potentially using vocabulary unseen in the source document.", "labels": [], "entities": []}, {"text": "This task can also be naturally cast as mapping an input sequence of words in a source document to a target sequence of words called summary.", "labels": [], "entities": []}, {"text": "In the recent past, deep-learning based models that map an input sequence into another output sequence, called sequence-to-sequence models, have been successful in many problems such as machine translation (), speech recognition () and video captioning ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.819610595703125}, {"text": "speech recognition", "start_pos": 210, "end_pos": 228, "type": "TASK", "confidence": 0.7860850989818573}, {"text": "video captioning", "start_pos": 236, "end_pos": 252, "type": "TASK", "confidence": 0.7065090090036392}]}, {"text": "In the framework of sequence-to-sequence models, a very relevant model to our task is the attentional Recurrent Neural Network (RNN) encoderdecoder model proposed in, which has produced state-of-the-art performance in machine translation (MT), which is also a natural language task.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 218, "end_pos": 242, "type": "TASK", "confidence": 0.8746130466461182}]}, {"text": "Despite the similarities, abstractive summarization is a very different problem from MT.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.6053220927715302}, {"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9708837270736694}]}, {"text": "Unlike in MT, the target (summary) is typically very short and does not depend very much on the length of the source (document) in summarization.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.97370445728302}]}, {"text": "Additionally, a key challenge in summarization is to optimally compress the original document in a lossy manner such that the key concepts in the original document are preserved, whereas in MT, the translation is expected to be loss-less.", "labels": [], "entities": [{"text": "summarization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9896677732467651}, {"text": "MT", "start_pos": 190, "end_pos": 192, "type": "TASK", "confidence": 0.9144064784049988}]}, {"text": "In translation, there is a strong notion of almost one-to-one wordlevel alignment between source and target, but in summarization, it is less obvious.", "labels": [], "entities": [{"text": "translation", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9663326144218445}, {"text": "summarization", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9753351807594299}]}, {"text": "We make the following main contributions in this work: (i) We apply the off-the-shelf attentional encoder-decoder RNN that was originally developed for machine translation to summarization, and show that it already outperforms stateof-the-art systems on two different English corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7616950869560242}, {"text": "summarization", "start_pos": 175, "end_pos": 188, "type": "TASK", "confidence": 0.9873201847076416}]}, {"text": "(ii) Motivated by concrete problems in summarization that are not sufficiently addressed by the machine translation based model, we propose novel models and show that they provide additional improvement in performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9867269396781921}]}, {"text": "(iii) We propose anew dataset for the task of abstractive summarization of a document into multiple sentences and establish benchmarks.", "labels": [], "entities": [{"text": "abstractive summarization of a document into multiple sentences", "start_pos": 46, "end_pos": 109, "type": "TASK", "confidence": 0.8486803621053696}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we describe each specific problem in abstractive summarization that we aim to solve, and present a novel model that addresses it.", "labels": [], "entities": []}, {"text": "Sec-tion 3 contextualizes our models with respect to closely related work on the topic of abstractive text summarization.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.6111404200394949}]}, {"text": "We present the results of our experiments on three different data sets in Section 4.", "labels": [], "entities": []}, {"text": "We also present some qualitative analysis of the output from our models in Section 5 before concluding the paper with remarks on our future direction in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance comparison of various models. '*' indicates statistical significance of the corresponding model with", "labels": [], "entities": []}, {"text": " Table 2. We note that although our model", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of our models using the limited-length", "labels": [], "entities": []}, {"text": " Table 3: Performance of various models on CNN/Daily", "labels": [], "entities": [{"text": "CNN/Daily", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.9833622574806213}]}]}