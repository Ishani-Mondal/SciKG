{"title": [{"text": "Neighborhood Mixture Model for Knowledge Base Completion", "labels": [], "entities": [{"text": "Neighborhood Mixture", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6498329788446426}]}], "abstractContent": [{"text": "Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7247689664363861}]}, {"text": "In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE-a well-known embedding model for knowledge base completion.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 178, "end_pos": 203, "type": "TASK", "confidence": 0.6176874339580536}]}, {"text": "Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.", "labels": [], "entities": [{"text": "triple classification", "start_pos": 237, "end_pos": 258, "type": "TASK", "confidence": 0.7663752734661102}, {"text": "entity prediction", "start_pos": 260, "end_pos": 277, "type": "TASK", "confidence": 0.7488360404968262}, {"text": "relation prediction tasks", "start_pos": 282, "end_pos": 307, "type": "TASK", "confidence": 0.8290108640988668}]}], "introductionContent": [{"text": "Knowledge bases (KBs), such as WordNet,), Freebase () and DBpedia (, represent relationships between entities as triples (head entity, relation, tail entity).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9717479944229126}, {"text": "Freebase", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9624735713005066}]}, {"text": "Even very large knowledge bases are still far from complete ().", "labels": [], "entities": []}, {"text": "Knowledge base completion or link prediction systems () predict which triples not in a knowledge base are likely to be true ().", "labels": [], "entities": [{"text": "Knowledge base completion", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6833150386810303}, {"text": "link prediction", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.6627538800239563}]}, {"text": "Embedding models for KB completion associate entities and/or relations with dense feature vectors or matrices.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.8793512284755707}]}, {"text": "Such models obtain state-of-the-art performance () and generalize to large.", "labels": [], "entities": []}, {"text": "Most embedding models for KB completion learn only from triples and by doing so, ignore lots of information implicitly provided by the structure of the knowledge graph.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.895453006029129}]}, {"text": "Recently, several authors have addressed this issue by incorporating relation path information into model learning) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 218, "end_pos": 243, "type": "TASK", "confidence": 0.6441910862922668}]}, {"text": "For instance, a three-relation path (head, born in hospital/r 1 , e 1 ) \u21d2(e 1 , hospital located in city/r 2 , e 2 ) \u21d2(e 2 , city in country/r 3 , tail) is likely to indicate that the fact (head, nationality, tail) could be true, so the relation path here p = {r 1 , r 2 , r 3 } is useful for predicting the relationship \"nationality\" between the head and tail entities.", "labels": [], "entities": []}, {"text": "Besides the relation paths, there could be other useful information implicitly presented in the knowledge base that could be exploited for better KB completion.", "labels": [], "entities": [{"text": "KB completion", "start_pos": 146, "end_pos": 159, "type": "TASK", "confidence": 0.8792680501937866}]}, {"text": "For instance, the whole neighborhood of entities could provide lots of useful information for predicting the relationship between two entities.", "labels": [], "entities": []}, {"text": "Consider for example a KB fragment given in.", "labels": [], "entities": []}, {"text": "If we know that Ben Affleck has won an Oscar award and Ben Affleck lives in Los Angeles, then this can help us to predict that Ben Affleck is an actor or a filmmaker, rather than a lecturer or a doctor.", "labels": [], "entities": []}, {"text": "If we additionally know that Ben Affleck's gender is male then there is a higher chance for him to be a filmmaker.", "labels": [], "entities": []}, {"text": "This intuition can be formalized by representing an entity vector as a relation-specific mixture of its neighborhood as follows: Ben Affleck = \u03c9 r,1 (Violet Anne, child of) + \u03c9 r,2 (male, gender \u22121 ) + \u03c9 r,3 (Los Angeles, lives in \u22121 ) + \u03c9 r,4 (Oscar award, won \u22121 ), where \u03c9 r,i are the mixing weights that indicate how important each neighboring relation is for predicting the relation r.", "labels": [], "entities": []}, {"text": "For example, for predicting the occupation relationship, the knowledge about the child of relationship might not be that informative and thus the corresponding mixing coefficient can be close to zero, whereas it could be relevant for predicting some other relationship, such as parent or spouse, in which case the relation-specific mixing coefficient for the child of relationship could be high.", "labels": [], "entities": [{"text": "mixing coefficient", "start_pos": 160, "end_pos": 178, "type": "METRIC", "confidence": 0.9551395773887634}]}, {"text": "The primary contribution of this paper is introducing and formalizing the neighborhood mixture model.", "labels": [], "entities": []}, {"text": "We demonstrate its usefulness by applying it to the well-known TransE model (.", "labels": [], "entities": []}, {"text": "However, it could be applied to other embedding models as well, such as Bilinear models ( and.", "labels": [], "entities": []}, {"text": "While relation path models exploit extra information using longer paths existing in the KB, the neighborhood mixture model effectively incorporates information about many paths simultaneously.", "labels": [], "entities": []}, {"text": "Our extensive experiments on three benchmark datasets show that it achieves superior performance over competitive baselines in three KB completion tasks: triple classification, entity prediction and relation prediction.", "labels": [], "entities": [{"text": "triple classification", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.6988303959369659}, {"text": "entity prediction", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.7786754667758942}, {"text": "relation prediction", "start_pos": 199, "end_pos": 218, "type": "TASK", "confidence": 0.8258389234542847}]}], "datasetContent": [{"text": "To investigate the usefulness of the neighbor mixtures, we compare the performance of the TransE-NMM against the results of the baseline TransE and other state-of-the-art embedding models on the triple classification, entity prediction and relation prediction tasks.", "labels": [], "entities": [{"text": "entity prediction and relation prediction", "start_pos": 218, "end_pos": 259, "type": "TASK", "confidence": 0.694332218170166}]}, {"text": "We conduct experiments using three publicly available datasets WN11, FB13 and NELL186.", "labels": [], "entities": [{"text": "WN11", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9293789863586426}, {"text": "FB13", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.5550412535667419}, {"text": "NELL186", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.7907593250274658}]}, {"text": "For all of them, the validation and test sets containing both correct and incorrect triples have already been constructed.", "labels": [], "entities": []}, {"text": "Statistical information about these datasets is given in.", "labels": [], "entities": []}, {"text": "The two benchmark datasets 1 , WN11 and FB13, were produced by for triple classification.", "labels": [], "entities": [{"text": "WN11", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9356101155281067}, {"text": "FB13", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.5650055408477783}, {"text": "triple classification", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.7731668949127197}]}, {"text": "WN11 is derived from the large lexical KB WordNet   We evaluate our model on three commonly used benchmark tasks: triple classification, entity prediction and relation prediction.", "labels": [], "entities": [{"text": "WN11", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9419927000999451}, {"text": "triple classification", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.7098042368888855}, {"text": "entity prediction", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.785881370306015}, {"text": "relation prediction", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7981351315975189}]}, {"text": "This subsection describes those tasks in detail.", "labels": [], "entities": []}, {"text": "Triple classification: The triple classification task was first introduced by, and since then it has been used to evaluate various embedding models.", "labels": [], "entities": [{"text": "Triple classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7631166577339172}, {"text": "triple classification task", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7669024666150411}]}, {"text": "The aim of the task is to predict whether a triple (h, r, t) is corrector not.", "labels": [], "entities": []}, {"text": "For classification, we set a relation-specific threshold \u03b8 r for each relation type r.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9698739051818848}]}, {"text": "If the implausibility score of an unseen test triple (h, r, t) is smaller than \u03b8 r then the triple will be classified as correct, otherwise incorrect.", "labels": [], "entities": []}, {"text": "Following, the relation-specific thresholds are determined by maximizing the micro-averaged accuracy, which is a per-triple average, on the validation set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.7972476482391357}]}, {"text": "We also report the macro-averaged accuracy, which is a per-relation average.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8777159452438354}]}, {"text": "Entity prediction: The entity prediction task () predicts the head or the tail entity given the relation type and the other entity, i.e. predicting h given (?, r, t) or predicting t given (h, r, ?) where ? denotes the missing element.", "labels": [], "entities": [{"text": "Entity prediction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7339589595794678}, {"text": "entity prediction task", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7766604026158651}]}, {"text": "The results are evaluated using a ranking induced by the function f (h, r, t) on test triples.", "labels": [], "entities": []}, {"text": "Note that the incorrect triples in the validation and test sets are not used for evaluating the entity prediction task nor the relation prediction task.", "labels": [], "entities": [{"text": "entity prediction task", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7779049475987753}, {"text": "relation prediction task", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.8504120508829752}]}, {"text": "Each correct test triple (h, r, t) is corrupted by replacing either its head or tail entity by each of the possible entities in turn, and then we rank these candidates in ascending order of their implausibility score.", "labels": [], "entities": []}, {"text": "This is called as the \"Raw\" setting protocol.", "labels": [], "entities": []}, {"text": "For the \"Filtered\" setting protocol described in, we also filter out before ranking any corrupted triples that appear in the KB.", "labels": [], "entities": []}, {"text": "Ranking a corrupted triple appearing in the KB (i.e. a correct triple) higher than the original test triple is also correct, but is penalized by the \"Raw\" score, thus the \"Filtered\" setting provides a clearer view on the ranking performance.", "labels": [], "entities": []}, {"text": "In addition to the mean rank and the Hits@10 (i.e., the proportion of test triples for which the target entity was ranked in the top 10 predictions), which were originally used in the entity prediction task (, we also report the mean reciprocal rank (MRR), which is commonly used in information retrieval.", "labels": [], "entities": [{"text": "Hits@10", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9319734970728556}, {"text": "entity prediction task", "start_pos": 184, "end_pos": 206, "type": "TASK", "confidence": 0.7856062451998392}, {"text": "mean reciprocal rank (MRR)", "start_pos": 229, "end_pos": 255, "type": "METRIC", "confidence": 0.7999764879544576}, {"text": "information retrieval", "start_pos": 283, "end_pos": 304, "type": "TASK", "confidence": 0.7804680466651917}]}, {"text": "In both \"Raw\" and \"Filtered\" settings, mean rank is always greater or equal to 1 and lower mean rank indicates better entity prediction performance.", "labels": [], "entities": [{"text": "mean rank", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9558025300502777}, {"text": "entity prediction", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.6459309756755829}]}, {"text": "The MRR and Hits@10 scores always range from 0.0 to 1.0, and higher score reflects better prediction result.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9249840378761292}]}, {"text": "Relation prediction: The relation prediction task () predicts the relation type given the head and tail entities, i.e. predicting r given (h, ?, t) where ? denotes the missing element.", "labels": [], "entities": [{"text": "Relation prediction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7793810367584229}, {"text": "relation prediction", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8575201034545898}]}, {"text": "We corrupt each correct test triple (h, r, t) by replacing its relation r by each possible relation type in turn, and then rank these candidates in ascending order of their implausibility score.", "labels": [], "entities": []}, {"text": "Just as in the entity prediction task, we use two setting protocols, \"Raw\" and \"Filtered\", and evaluate on mean rank, MRR and Hits@10.", "labels": [], "entities": [{"text": "entity prediction task", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.8307081262270609}, {"text": "Filtered", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9631897211074829}, {"text": "MRR", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9667966961860657}]}], "tableCaptions": [{"text": " Table 2: Statistics of the experimental datasets  used in this study (and previous works). #E is  the number of entities, #R is the number of rela- tion types, and #Train, #Valid and #Test are the  numbers of correct triples in the training, valida- tion and test sets, respectively. Each validation and  test set also contains the same number of incorrect  triples as the number of correct triples.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results of TransE (i.e. TransE-NMM with \u03c4 = 0) and TransE-NMM with \u03c4 = 10.", "labels": [], "entities": []}, {"text": " Table 4: Micro-averaged accuracy results (in %)  for triple classification on WN11 (labeled as W11)  and FB13 (labeled as F13) test sets. Scores in bold  and underline are the best and second best scores,  respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8875803351402283}, {"text": "triple classification", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6989374458789825}, {"text": "WN11", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9745409488677979}, {"text": "FB13 (labeled as F13) test sets", "start_pos": 106, "end_pos": 137, "type": "DATASET", "confidence": 0.6506263576447964}]}]}