{"title": [{"text": "CoNLL 2016 Shared Task on Multilingual Shallow Discourse Parsing", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8578630685806274}, {"text": "Multilingual Shallow Discourse Parsing", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.5975081324577332}]}], "abstractContent": [{"text": "The CoNLL-2016 Shared Task is the second edition of the CoNLL-2015 Shared Task, now on Multilingual Shallow discourse parsing.", "labels": [], "entities": [{"text": "Multilingual Shallow discourse parsing", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.6309117078781128}]}, {"text": "Similar to the 2015 task, the goal of the shared task is to identify individual discourse relations that are present in natural language text.", "labels": [], "entities": []}, {"text": "Given a natural language text, participating teams are asked to locate the discourse connectives (ex-plicit or implicit) and their arguments as well as predicting the sense of the discourse connectives.", "labels": [], "entities": []}, {"text": "Based on the success of the previous year, we continued to ask participants to deploy their systems on TIRA, a web-based platform on which participants can run their systems on the test data for evaluation.", "labels": [], "entities": []}, {"text": "This evaluation methodology preserves the integrity of the shared task.", "labels": [], "entities": []}, {"text": "We have also made a few changes and additions in the 2016 shared task based on the feedback from 2015.", "labels": [], "entities": []}, {"text": "The first is that teams could choose to carryout the task on Chinese texts, or En-glish texts, or both.", "labels": [], "entities": []}, {"text": "We have also allowed participants to focus on parts of the shared task (rather than the whole thing) as atypical system requires substantial investment of effort.", "labels": [], "entities": []}, {"text": "Finally, we have modified the scorer so that it can report results based on partial matches of the arguments.", "labels": [], "entities": []}, {"text": "23 teams participated in this year's shared task, using a wide variety of approaches.", "labels": [], "entities": []}, {"text": "In this overview paper, we present the task definition, the training and test sets, and the evaluation protocol and metric used during this shared task.", "labels": [], "entities": []}, {"text": "We also summarize the different approaches adopted by the participating teams, and present the evaluation results.", "labels": [], "entities": []}, {"text": "The evaluation data sets and the scorer will serve as a benchmark for future research on shallow discourse parsing .", "labels": [], "entities": [{"text": "shallow discourse parsing", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7115335265795389}]}], "introductionContent": [{"text": "The shared task for the Twentieth Conference on Computational Natural Language Learning) is a follow-on to the CoNLL-2015 shared task, and it is on Multilingual Shallow Discourse Parsing (SDP).", "labels": [], "entities": [{"text": "Twentieth Conference on Computational Natural Language Learning)", "start_pos": 24, "end_pos": 88, "type": "TASK", "confidence": 0.6082664206624031}, {"text": "Multilingual Shallow Discourse Parsing (SDP)", "start_pos": 148, "end_pos": 192, "type": "TASK", "confidence": 0.7401914511408124}]}, {"text": "While the 2015 task focused on newswire text data in English, this year we added anew language, Chinese.", "labels": [], "entities": []}, {"text": "Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text.", "labels": [], "entities": []}, {"text": "The conceptual framework of the Shallow Discourse Parsing task is that of the Penn Discourse TreeBank (PDTB) (, where a discourse relation is viewed as a predicate that takes two abstract objects as arguments.", "labels": [], "entities": [{"text": "Shallow Discourse Parsing task", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.7586650550365448}, {"text": "Penn Discourse TreeBank (PDTB)", "start_pos": 78, "end_pos": 108, "type": "DATASET", "confidence": 0.9368071059385935}]}, {"text": "The two arguments maybe realized as clauses or sentences, or occasionally phrases.", "labels": [], "entities": []}, {"text": "It is \"shallow\" in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized.", "labels": [], "entities": []}, {"text": "As such, it differs from analyses according to either Rhetorical Structure ( or Segmented Discourse Representation Theory (SDRT).", "labels": [], "entities": [{"text": "Segmented Discourse Representation Theory (SDRT)", "start_pos": 80, "end_pos": 128, "type": "TASK", "confidence": 0.7662560428891864}]}, {"text": "The rest of this overview paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we provide a concise definition of the shared task.", "labels": [], "entities": []}, {"text": "We describe how the training and test data are prepared in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we present the evaluation protocol, metric and scorer.", "labels": [], "entities": [{"text": "scorer", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9838640689849854}]}, {"text": "The different approaches that participants took in the shared task are summarized in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6, we present the ranking of participating systems and analyze the evaluation results.", "labels": [], "entities": []}, {"text": "We present our conclusions in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The scorer that computes all of the available evaluation metrics is open-source with some contribution from the participants during the task period 6 .  End-to-end discourse parsing Relation between a supposed condition and a supposed result Conjunction Relation between two equal-status statements serving a common communicative function Contrast Relation between two statements, co-occurrence of which seems contradictory, counter-intuitive, out-of-ordinary, etc.", "labels": [], "entities": [{"text": "End-to-end discourse parsing Relation between a supposed condition", "start_pos": 153, "end_pos": 219, "type": "TASK", "confidence": 0.7939956039190292}]}, {"text": "Expansion Relation in which one argument is an elaboration or restatement of another Purpose Relation between an action and the intention behind it Temporal Relation that is temporal in nature, expressing temporal precedence, etc.", "labels": [], "entities": [{"text": "Temporal Relation", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.79628786444664}]}, {"text": "Progression Relation in which one argument represents a progression from the other, in extent, intensity, scale, etc.", "labels": [], "entities": [{"text": "Progression Relation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6509031653404236}]}, {"text": "Although the submissions are ranked based on the end-to-end F 1 score, discourse relation sense classification subtask has gained much attention from the community within the past years including some participants from last year.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.8895275592803955}, {"text": "discourse relation sense classification subtask", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.7615849614143372}]}, {"text": "We provide the data and evaluation setup for participants who are only interested in the discourse relation sense classification subtask and for those who want to evaluate their system without the error propagation from argument extraction.", "labels": [], "entities": [{"text": "discourse relation sense classification subtask", "start_pos": 89, "end_pos": 136, "type": "TASK", "confidence": 0.7018575608730316}]}, {"text": "In this supplementary evaluation, the input is gold-standard argument pairs and their corresponding explicit discourse connectives if applicable.", "labels": [], "entities": []}, {"text": "The goal is to fill in the senses including EntRel.", "labels": [], "entities": []}, {"text": "The results from this evaluation are shown in  For analytical purposes, the scorer also provides component-wise evaluation with error propagation and a breakdown of the discourse parser performance for explicit and non-explicit discourse relations.", "labels": [], "entities": []}, {"text": "The scorer computes the precision, recall, and F 1 for the following tasks: \u2022 Explicit discourse connective identification.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9998088479042053}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9994956254959106}, {"text": "F 1", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9930509626865387}, {"text": "Explicit discourse connective identification", "start_pos": 78, "end_pos": 122, "type": "TASK", "confidence": 0.6143721714615822}]}, {"text": "\u2022 Arg1 and Arg2 identification.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9886530041694641}, {"text": "Arg2 identification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8839671313762665}]}, {"text": "\u2022 Sense classification with error propagation from discourse connective and argument identification.", "labels": [], "entities": [{"text": "Sense classification", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.8679463565349579}, {"text": "argument identification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.746769517660141}]}, {"text": "For purposes of evaluation, an explicit discourse connective predicted by a parser is considered correct if and only if the predicted raw connective includes the gold raw connective head, while allowing for the tokens of the predicted connective to be a subset of the tokens in the gold raw connective.", "labels": [], "entities": []}, {"text": "We provide a function that maps discourse connectives to their corresponding heads.", "labels": [], "entities": []}, {"text": "The notion of discourse connective head is not the same as its syntactic head.", "labels": [], "entities": []}, {"text": "Rather, it is thought of as the part of the connective conveying its core meaning.", "labels": [], "entities": []}, {"text": "For example, the head of the discourse connective \"At least not when\" is \"when\", and the head of \"five minutes before\" is \"before\".", "labels": [], "entities": []}, {"text": "The non-head part of the connective serves to semantically restrict the interpretation of the connective.", "labels": [], "entities": []}, {"text": "Although Implicit discourse relations are annotated with an implicit connective inserted between adjacent sentences, participants are not required to provide the inserted connective.", "labels": [], "entities": []}, {"text": "They only need to output the sense of the discourse relation.", "labels": [], "entities": []}, {"text": "Similarly, for AltLex relations, which are also annotated between adjacent sentences, participants are not required to output the text span of the AltLex expression, but only the sense.", "labels": [], "entities": []}, {"text": "The EntRel relation is included as a sense in the shared task, and here, systems are required to correctly label the EntRel relation between adjacent sentence pairs.", "labels": [], "entities": []}, {"text": "We also provide partial evaluation to assess how well a system does when we relax the criteria.", "labels": [], "entities": []}, {"text": "The official full evaluation metric produces low scores due to error propagation from argument extraction.", "labels": [], "entities": []}, {"text": "Partial evaluation instead allows 'fuzzy matching' in arguments.", "labels": [], "entities": []}, {"text": "The extracted Arg1 and Arg2 are correct if and only if the average of F 1 score of the extracted Arg1 and Arg2 is greater than 0.7.", "labels": [], "entities": [{"text": "Arg1", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9646568894386292}, {"text": "Arg2", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9392132759094238}, {"text": "F 1 score", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9898786544799805}, {"text": "Arg2", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8991526961326599}]}, {"text": "This allows us to evaluate the sense classification of that relation even if the argument extraction is not perfect.", "labels": [], "entities": []}, {"text": "The evaluation is also done for both explicit and non-explicit relations separately and together).", "labels": [], "entities": []}, {"text": "We use anew web service called TIRA as the platform for system evaluation ().", "labels": [], "entities": []}, {"text": "Traditionally, participating teams have been asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation.", "labels": [], "entities": []}, {"text": "Starting with the 2015 shared task, however, we shifted this evaluation paradigm, asking participants to deploy their systems on a remote virtual machine, and to use the TIRA web platform (tira.io) to run their systems on the test sets without actually seeing them.", "labels": [], "entities": []}, {"text": "The organizers would then inspect the evaluation results, and verify that participating systems yielded acceptable output.", "labels": [], "entities": []}, {"text": "This evaluation protocol allowed us to maintain the integrity of the blind test set and reduce the organizational overhead.", "labels": [], "entities": []}, {"text": "On TIRA, the blind test set can only be accessed in the evaluation environment, and the evaluation results are automatically collected.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.557738184928894}]}, {"text": "Participants cannot see any part of the test sets and hence cannot do iterative development based on the test set performance, which preserves the integrity of the evaluation.", "labels": [], "entities": []}, {"text": "Most importantly, this evaluation platform promotes replicability, which is crucial for proper evaluation of scientific progress.", "labels": [], "entities": [{"text": "replicability", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9205343723297119}]}, {"text": "Reproducing all of the results is just a matter of a button click on TIRA.", "labels": [], "entities": [{"text": "TIRA", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.6575079560279846}]}, {"text": "All of the results presented in this paper, along with the trained models and the software, are archived and available for distribution upon request to the organizers and upon the permission of the participating team, who holds the copyrights to the software.", "labels": [], "entities": []}, {"text": "Replicability also helps speedup the research and development in discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7282380163669586}]}, {"text": "Anyone wanting to extend or apply any of the approaches proposed by a shared task participant does not have to re-implement the model from scratch.", "labels": [], "entities": []}, {"text": "They can request a clone of the virtual machine where the participating system is deployed, and then implement their extension based off the original source code.", "labels": [], "entities": []}, {"text": "Any extension effort also benefits from the precise evaluation of the progress and improvement since the system is based off the exact same implementation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The distribution of discourse relation  types in the Chinese data", "labels": [], "entities": []}, {"text": " Table 5: Scoreboard for the CoNLL-2016 shared task showing performance (strict scoring)  across the subtasks and the three data partitions-blind test, standard test and development  set for both English and Chinese.", "labels": [], "entities": []}, {"text": " Table 6: Scoreboard for the CoNLL-2016 shared task showing performance (partial scoring)  across the subtasks and the three data partitions-blind test, standard test and development  set for both English and Chinese.", "labels": [], "entities": []}, {"text": " Table 7: F-score (strict scoring) of all subtasks separated by Explicit and Implicit discourse  relations across the three data partitions-blind test, standard test and development set for  both English and Chinese.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.998482882976532}, {"text": "Explicit", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9764137864112854}]}, {"text": " Table 8: F-score (partial scoring) of all subtasks separated by Explicit and Implicitdiscourse  relations across the three data partitions-blind test, standard test and development set for  both English and Chinese.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9983789920806885}, {"text": "Explicit", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9922389984130859}]}, {"text": " Table 9: Discourse relation sense classification evaluation results (Supplementary evaluation).  All participants are given gold standard discourse connectives and argument pairs.", "labels": [], "entities": [{"text": "Discourse relation sense classification evaluation", "start_pos": 10, "end_pos": 60, "type": "TASK", "confidence": 0.7430483877658844}]}]}