{"title": [{"text": "Generating Sentences from a Continuous Space", "labels": [], "entities": [{"text": "Generating Sentences from a Continuous Space", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.846580704053243}]}], "abstractContent": [{"text": "The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does notwork from an explicit global sentence representation.", "labels": [], "entities": []}, {"text": "In this work, we introduce and study an rnn-based variational au-toencoder generative model that incorporates distributed latent representations of entire sentences.", "labels": [], "entities": []}, {"text": "This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features.", "labels": [], "entities": []}, {"text": "Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple de-terministic decoding.", "labels": [], "entities": []}, {"text": "By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences.", "labels": [], "entities": []}, {"text": "We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent neural network language models (rnnlms,) represent the state of the art in unsupervised generative modeling for natural language sentences.", "labels": [], "entities": [{"text": "generative modeling for natural language sentences", "start_pos": 98, "end_pos": 148, "type": "TASK", "confidence": 0.8402144213517507}]}, {"text": "In supervised settings, rnnlm decoders conditioned on taskspecific features are the state of the art in tasks like machine translation) and image captioning ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7220595628023148}, {"text": "image captioning", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.7088735103607178}]}, {"text": "The rnnlm generates sentences word-by-word based on an evolving distributed state representation, which makes it a probabilistic model with no significant independence * First two authors contributed equally.", "labels": [], "entities": []}, {"text": "Work was done when all authors were at Google, Inc.", "labels": [], "entities": []}, {"text": "i went to the store to buy some groceries . i store to buy some groceries . i were to buy any groceries . horses are to buy any groceries . horses are to buy any animal . horses the favorite any animal . horses the favorite favorite animal . horses are my favorite animal . assumptions, and makes it capable of modeling complex distributions over sequences, including those with long-term dependencies.", "labels": [], "entities": []}, {"text": "However, by breaking the model structure down into a series of next-step predictions, the rnnlm does not expose an interpretable representation of global features like topic or of high-level syntactic properties.", "labels": [], "entities": []}, {"text": "We propose an extension of the rnnlm that is designed to explicitly capture such global features in a continuous latent variable.", "labels": [], "entities": []}, {"text": "Naively, maximum likelihood learning in such a model presents an intractable inference problem.", "labels": [], "entities": []}, {"text": "Drawing inspiration from recent successes in modeling images (, handwriting, and natural speech (, our model circumvents these difficulties using the architecture of a variational autoencoder and takes advantage of recent advances in variational inference () that introduce a practical training technique for powerful neural network generative models with latent variables.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: We propose a variational autoencoder architecture for text and discuss some of the obstacles to training it as well as our proposed solutions.", "labels": [], "entities": []}, {"text": "We find that on a standard language modeling evaluation where a global variable is not explicitly needed, this model yields similar performance to existing rnnlms.", "labels": [], "entities": []}, {"text": "We also evaluate our model using a larger corpus on the task of imputing missing words.", "labels": [], "entities": []}, {"text": "For this task, we introduce a novel evaluation strategy using an adversarial classifier, sidestepping the issue of intractable likelihood computations by drawing inspiration from work on non-parametric two-sample tests and adversarial training.", "labels": [], "entities": []}, {"text": "In this setting, our model's global latent variable allows it to do well where simpler models fail.", "labels": [], "entities": []}, {"text": "We finally introduce several qualitative techniques for analyzing the ability of our model to learn high level features of sentences.", "labels": [], "entities": []}, {"text": "We find that they can produce diverse, coherent sentences through purely deterministic decoding and that they can interpolate smoothly between sentences.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Penn Treebank language modeling results, reported as negative log likelihoods (nll) and as  perplexities (ppl). Lower is better for both metrics. For the vae, the kl term of the likelihood is shown  in parentheses alongside the total likelihood.", "labels": [], "entities": [{"text": "Penn Treebank language", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.973577598730723}]}, {"text": " Table 3: Examples of using beam search to impute missing words within sentences. Since we decode from  right to left, note the stereotypical completions given by the rnnlm, compared to the vae completions  that often use topic data and more varied vocabulary.", "labels": [], "entities": []}, {"text": " Table 6: Greedily decoded sentences from a model with 75% word keep probability, sampling from  lower-likelihood areas of the latent space. Note the consistent topics and vocabulary usage.", "labels": [], "entities": [{"text": "word keep probability", "start_pos": 59, "end_pos": 80, "type": "METRIC", "confidence": 0.7055527071158091}]}, {"text": " Table 9: Results for the msr Paraphrase Corpus.", "labels": [], "entities": [{"text": "msr Paraphrase Corpus", "start_pos": 26, "end_pos": 47, "type": "DATASET", "confidence": 0.791187564531962}]}, {"text": " Table 10: Results for TREC Question Classifica- tion.", "labels": [], "entities": [{"text": "TREC Question Classifica- tion", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.5256526529788971}]}, {"text": " Table 11: Automatically selected hyperparameter  values used for the models used in the Penn Tree- bank language modeling experiments. kr is the  keep rate for word dropout.", "labels": [], "entities": [{"text": "Penn Tree- bank language modeling experiments", "start_pos": 89, "end_pos": 134, "type": "DATASET", "confidence": 0.9502165487834385}, {"text": "keep rate", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9851382672786713}]}]}