{"title": [{"text": "Adapting Event Embedding for Implicit Discourse Relation Recognition", "labels": [], "entities": [{"text": "Adapting Event Embedding", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8929492632548014}, {"text": "Implicit Discourse Relation Recognition", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.7800146639347076}]}], "abstractContent": [{"text": "Predicting the sense of a discourse relation is particularly challenging when connective markers are missing.", "labels": [], "entities": []}, {"text": "To address this challenge, we propose a simple deep neu-ral network approach that replaces manual feature extraction by introducing event vectors as an alternative representation, which can be pre-trained using a very large corpus, without explicit annotation.", "labels": [], "entities": []}, {"text": "We model discourse arguments as a combination of word and event vectors.", "labels": [], "entities": []}, {"text": "Event information is aggregated with word vectors and a Multi-Layer Neural Network is used to classify discourse senses.", "labels": [], "entities": []}, {"text": "This work was submitted as part of the CoNLL 2016 shared task on Discourse Parsing.", "labels": [], "entities": [{"text": "CoNLL 2016 shared task", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.7419988512992859}, {"text": "Discourse Parsing", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7290377020835876}]}, {"text": "We obtain competitive results, reaching an accuracy of 38%, 34% and 34% for the development, test and blind test datasets, competitive with the best performing system on CoNLL 2015.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9994211196899414}, {"text": "CoNLL 2015", "start_pos": 170, "end_pos": 180, "type": "DATASET", "confidence": 0.9605412483215332}]}], "introductionContent": [{"text": "The CoNLL 2016 shared task focuses on Discourse Parsing.", "labels": [], "entities": [{"text": "CoNLL 2016 shared task", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.896620512008667}, {"text": "Discourse Parsing", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.8765709698200226}]}, {"text": "Building on the CoNLL 2015 task, this year teams were able to focus on a supplementary task, limited to sense classification of discourse relations, given their (gold) arguments.", "labels": [], "entities": [{"text": "CoNLL 2015 task", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.8124960859616598}, {"text": "sense classification of discourse relations", "start_pos": 104, "end_pos": 147, "type": "TASK", "confidence": 0.8650812625885009}]}, {"text": "Identifying the sense is particularly challenging in the case of implicit relations, where explicit connective words (e.g., however, but, because) are not present.", "labels": [], "entities": [{"text": "Identifying the sense", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8894810080528259}]}, {"text": "Last year, most submitted systems used algorithms traditionally applied for this task, such as Support Vector Machine (SVM) and Maximum Entropy classifiers learned over binary features as input representation.", "labels": [], "entities": []}, {"text": "This included the best performing system, which reached an accuracy of 34.45 in the test data and an accuracy of 36.29 in the blind test data for implicit relations;.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9990519881248474}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9984898567199707}]}, {"text": "We followed the intuition that obtaining a significant increase in performance using traditional classifiers and feature engineering would be difficult given the effort that was previously spent on such systems.", "labels": [], "entities": []}, {"text": "Neural-network-based classifiers present a different and less explored approach to the discourse sense problem, which can potentially lead to considerable improvement.", "labels": [], "entities": []}, {"text": "Our system, described in this paper, takes a step in this direction.", "labels": [], "entities": []}, {"text": "We explore different input representation types and introduce event vectors for this task.", "labels": [], "entities": []}, {"text": "Following the work of (Chambers and Jurafsky, 2009), we look into event chains as away to represent structure in the discourse arguments.", "labels": [], "entities": []}, {"text": "Then, we adapt the skip-gram approach originally used to learn word vectors from sentences () to learn event vector representations from event sequences.", "labels": [], "entities": []}, {"text": "To do so, we draw a clear analogy between words and events, as well as between sentences and event chains.", "labels": [], "entities": []}, {"text": "Finally, each input relation is represented with the pre-trained event and word vectors of its arguments and a multi-layer neural network is used to classify senses.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since our main focus is implicit relations, we carried out a series of experiments to test the three different input representations in the implicit sense identification task.", "labels": [], "entities": [{"text": "implicit sense identification task", "start_pos": 140, "end_pos": 174, "type": "TASK", "confidence": 0.7201237007975578}]}, {"text": "In all these experiments, we used a neural network architecture, and used as a baseline a simple lexical classifier based on word pairs.", "labels": [], "entities": []}, {"text": "Since during the development of the system we only had direct access to the train and development folds, most of our experiments were performed on the development data set alone.", "labels": [], "entities": []}, {"text": "Word pairs have been widely used for implicit sense classification (, and most systems submitted to CoNLL 2015 shared task incorporated word pairs as a fundamental part of their feature set.", "labels": [], "entities": [{"text": "implicit sense classification", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.7262843251228333}, {"text": "CoNLL 2015 shared task", "start_pos": 100, "end_pos": 122, "type": "DATASET", "confidence": 0.8270276784896851}]}, {"text": "we can seethe aggregated results for this simple approach using Support Vector Machines on the development dataset.", "labels": [], "entities": []}, {"text": "For this test, the top 500 word pairs ranked by information gain were used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy in the development data (Unofficial) by sense classes using different input represen- tations: words, events and words + events", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9854733347892761}]}, {"text": " Table 2: F1 score (Unofficial) by sense classes for both implicit and explicit classifier.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9811280965805054}]}, {"text": " Table 3: Performance metrics on the development  data for the implicit classifier", "labels": [], "entities": []}, {"text": " Table 4: Official TIRA F1 score for both implicit and explicit classifier.", "labels": [], "entities": [{"text": "Official TIRA F1 score", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.7081871777772903}]}]}