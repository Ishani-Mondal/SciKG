{"title": [{"text": "Exploring Prediction Uncertainty in Machine Translation Quality Estimation", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.8408451676368713}]}], "abstractContent": [{"text": "Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments.", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8273509591817856}]}, {"text": "Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty.", "labels": [], "entities": []}, {"text": "However , models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account.", "labels": [], "entities": []}, {"text": "We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions.", "labels": [], "entities": [{"text": "Quality Estimation", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7447833120822906}]}, {"text": "We also show how this posterior information can be useful in an asym-metric risk scenario, which aims to capture typical situations in translation workflows.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality Estimation (QE) () models aim at predicting the quality of automatically translated text segments.", "labels": [], "entities": [{"text": "Quality Estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5763967871665955}]}, {"text": "Traditionally, these models provide point estimates and are evaluated using metrics like Mean Absolute Error (MAE), Root-Mean-Square Error (RMSE) and Pearson's r correlation coefficient.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 89, "end_pos": 114, "type": "METRIC", "confidence": 0.9681537946065267}, {"text": "Root-Mean-Square Error (RMSE)", "start_pos": 116, "end_pos": 145, "type": "METRIC", "confidence": 0.7370370984077453}, {"text": "Pearson's r correlation coefficient", "start_pos": 150, "end_pos": 185, "type": "METRIC", "confidence": 0.6725367724895477}]}, {"text": "However, in practice QE models are built for use in decision making in large workflows involving Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.8440001606941223}]}, {"text": "In these settings, relying on point estimates would mean that only very accurate prediction models can be useful in practice.", "labels": [], "entities": []}, {"text": "A way to improve decision making based on quality predictions is to explore uncertainty estimates.", "labels": [], "entities": []}, {"text": "Consider for example a post-editing scenario where professional translators use MT in an effort to speed-up the translation process.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9130323529243469}, {"text": "translation process", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8909231722354889}]}, {"text": "A QE model can be used to determine if an MT segment is good enough for post-editing or should be discarded and translated from scratch.", "labels": [], "entities": [{"text": "MT segment", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.8729654550552368}]}, {"text": "But since QE models are not perfect they can end up allowing bad MT segments to go through for postediting because of a prediction error.", "labels": [], "entities": [{"text": "MT segments", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.8764650523662567}]}, {"text": "In such a scenario, having an uncertainty estimate for the prediction can provide additional information for the filtering decision.", "labels": [], "entities": []}, {"text": "For instance, in order to ensure good user experience for the human translator and maximise translation productivity, an MT segment could be forwarded for post-editing only if a QE model assigns a high quality score with low uncertainty (high confidence).", "labels": [], "entities": [{"text": "MT segment", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.8629047274589539}]}, {"text": "Such a decision process is not possible with point estimates only.", "labels": [], "entities": []}, {"text": "Good uncertainty estimates can be acquired from well-calibrated probability distributions over the quality predictions.", "labels": [], "entities": []}, {"text": "In QE, arguably the most successful probabilistic models are Gaussian Processes (GPs) since they considered the state-ofthe-art for regression, especially in the low-data regimes typical for this task.", "labels": [], "entities": []}, {"text": "We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions.", "labels": [], "entities": []}, {"text": "Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by, which defined new evaluation metrics that take into account probability distributions over predictions.", "labels": [], "entities": []}, {"text": "The remaining of this paper is organised as follows: \u2022 In Section 2 we further motivate the use of GPs for uncertainty modelling in QE and revisit their underlying theory.", "labels": [], "entities": [{"text": "uncertainty modelling", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.6679559499025345}]}, {"text": "We also propose some model extensions previously developed in the GP literature and argue they are more appropriate for the task.", "labels": [], "entities": []}, {"text": "\u2022 We intrinsically evaluate our proposed models in terms of their posterior distributions on training and test data in Section 3.", "labels": [], "entities": []}, {"text": "Specifically, we show that differences in uncertainty modelling are not captured by the usual point estimate metrics commonly used for this task.", "labels": [], "entities": [{"text": "uncertainty modelling", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.69516721367836}]}, {"text": "\u2022 As an example of an application for predicitive distributions, in Section 4 we show how they can be useful in scenarios with asymmetric risk and how the proposed models can provide better performance in this case.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 5 and give conclusions and avenues for future work in Section 6.", "labels": [], "entities": []}, {"text": "While we focus on QE as application, the methods we explore in this paper can be applied to any text regression task where modelling predictive uncertainty is useful, either inhuman decision making or by propagating this information for further computational processing.", "labels": [], "entities": [{"text": "QE", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.8750694394111633}, {"text": "text regression task", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.7707996964454651}]}], "datasetContent": [{"text": "Given a set of different probabilistic QE models, we are interested in evaluating the performance of these models, while also taking their uncertainty into account, particularly to distinguish among models with seemingly same or similar performance.", "labels": [], "entities": []}, {"text": "A straightforward way to measure the performance of a probabilistic model is to inspect its negative (log) marginal likelihood.", "labels": [], "entities": []}, {"text": "This measure, however, does not capture if a model overfit the training data.", "labels": [], "entities": []}, {"text": "We can have a better generalisation measure by calculating the likelihood on test data instead.", "labels": [], "entities": []}, {"text": "This was proposed in previous work and it is called Negative Log Predictive Density (NLPD)): wher\u00ea y is a set of test predictions, y is the set of true labels and n is the test set size.", "labels": [], "entities": [{"text": "Negative Log Predictive Density (NLPD))", "start_pos": 52, "end_pos": 91, "type": "TASK", "confidence": 0.715613956962313}]}, {"text": "This metric has since been largely adopted by the ML community when evaluating GPs and other probabilistic models for regression (see Section 5 for some examples).", "labels": [], "entities": [{"text": "ML", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.956800639629364}]}, {"text": "As with other error metrics, lower values are better.", "labels": [], "entities": []}, {"text": "Intuitively, if two models produce equally incorrect predictions but they have different uncertainty estimates, NLPD will penalise the overconfident model more than the underconfident one.", "labels": [], "entities": [{"text": "NLPD", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.7219853401184082}]}, {"text": "On the other hand, if predictions are close to the true value then NLPD will penalise the underconfident model instead.", "labels": [], "entities": [{"text": "NLPD", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8223300576210022}]}, {"text": "In our first set of experiments we evaluate models proposed in Section 2 according to their negative log likelihood (NLL) and the NLPD on test data.", "labels": [], "entities": [{"text": "negative log likelihood (NLL)", "start_pos": 92, "end_pos": 121, "type": "METRIC", "confidence": 0.8869264622529348}]}, {"text": "We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson's r, which has recently proposed by as a more robust alternative.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 56, "end_pos": 81, "type": "METRIC", "confidence": 0.9585960308710734}, {"text": "Pearson's r", "start_pos": 135, "end_pos": 146, "type": "METRIC", "confidence": 0.9156927863756815}]}, {"text": "Here we assess the models and datasets used in Section 3.1 in terms of their performance in the asymmetric setting.", "labels": [], "entities": []}, {"text": "Following the explanation in the previous Section, we do not perform any retraining: we collect the predictions obtained using the 10-fold cross-validation protocol and apply different Bayes estimators corresponding to the asymmetric losses.", "labels": [], "entities": []}, {"text": "Evaluation is performed using the same loss employed in the estimator (for instance, when using the linex estimator with w = 0.75 we report the results using the linex loss with same w) and averaged over the 10 folds.", "labels": [], "entities": []}, {"text": "To simulate both pessimistic and optimistic scenarios, we use w \u2208 {3, 1/3} for the AL loss and w \u2208 {\u22120.75, 0.75} for the linex loss.", "labels": [], "entities": []}, {"text": "The only exception is the en-de dataset, where we report results for w \u2208 \u22120.25, 0.75 for linex . We also report results only for models using the Mat\u00e8rn52 kernel.", "labels": [], "entities": [{"text": "Mat\u00e8rn52 kernel", "start_pos": 146, "end_pos": 161, "type": "DATASET", "confidence": 0.905176430940628}]}, {"text": "While we did experiment with different kernels and weighting schemes 4 our findings showed similar trends so we omit them for the sake of clarity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Intrinsic evaluation results. The first three  rows in each table correspond to standard GP mod- els, while the remaining rows are Warped GP mod- els with different warping functions. The number", "labels": [], "entities": []}, {"text": " Table 2: Asymmetric loss experiments results.  The first line in each table corresponds to a stan- dard GP while the others are Warped GPs with  different warping functions. All models use the", "labels": [], "entities": []}]}