{"title": [{"text": "Coreference in Wikipedia: Main Concept Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "Wikipedia is a resource of choice exploited in many NLP applications, yet we are not aware of recent attempts to adapt coreference resolution to this resource.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.9639300107955933}]}, {"text": "In this work, we revisit a seldom studied task which consists in identifying in a Wikipedia article all the mentions of the main concept being described.", "labels": [], "entities": []}, {"text": "We show that by exploiting the Wikipedia markup of a document, as well as links to external knowledge bases such as Freebase, we can acquire useful information on entities that helps to classify mentions as coreferent or not.", "labels": [], "entities": []}, {"text": "We designed a classifier which drastically outperforms fair baselines built on top of state-of-the-art coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.8289781212806702}]}, {"text": "We also measure the benefits of this classifier in a full coreference resolution pipeline applied to Wikipedia texts.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7520523965358734}]}], "introductionContent": [{"text": "Coreference Resolution (CR) is the task of identifying all mentions of entities in a document and grouping them into equivalence classes.", "labels": [], "entities": [{"text": "Coreference Resolution (CR)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9209656476974487}]}, {"text": "CR is a prerequisite for many NLP tasks.", "labels": [], "entities": [{"text": "CR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9196852445602417}]}, {"text": "For example, in Open Information Extraction (OIE) (, one acquires subject-predicate-object relations, many of which (e.g., <the foundation stone, was laid by, the Queens daughter>) are useless because the subject or the object contains material coreferring to other mentions in the text being mined.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.7551560550928116}]}, {"text": "Most CR systems, including state-of-the-art ones are essentially adapted to news-like texts.", "labels": [], "entities": []}, {"text": "This is basically imputable to the availability of large datasets where this text genre is dominant.", "labels": [], "entities": []}, {"text": "This includes resources developed within the Message Understanding Conferences or the Automatic Content Extraction (ACE) program (), as well as resources developed within the collaborative annotation project OntoNotes (.", "labels": [], "entities": [{"text": "Message Understanding Conferences", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.8138615091641744}, {"text": "Automatic Content Extraction (ACE)", "start_pos": 86, "end_pos": 120, "type": "TASK", "confidence": 0.7216121554374695}]}, {"text": "It is now widely accepted that coreference resolution systems trained on newswire data performs poorly when tested on other text genres, including Wikipedia texts, as we shall see in our experiments.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9442558586597443}]}, {"text": "Wikipedia is a large, multilingual, highly structured, multi-domain encyclopedia, providing an increasingly large wealth of knowledge.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9194778800010681}]}, {"text": "It is known to contain well-formed, grammatical and meaningful sentences, compared to say, ordinary internet documents.", "labels": [], "entities": []}, {"text": "It is therefore a resource of choice in many NLP systems, see) fora review of some pioneering works.", "labels": [], "entities": []}, {"text": "While being a ubiquitous resource in the NLP community, we are not aware of much work conducted to adapt CR to this text genre.", "labels": [], "entities": []}, {"text": "Two notable exceptions are and, two studies dedicated to extract tuples from Wikipedia articles.", "labels": [], "entities": []}, {"text": "Both studies demonstrate that the design of a dedicated rulebased CR system leads to improved extraction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9437728524208069}]}, {"text": "The focus of those studies being information extraction, the authors did not spend much efforts in designing a fully-fledged CR designed for Wikipedia, neither did they evaluate it on a coreference resolution task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8173865377902985}, {"text": "coreference resolution task", "start_pos": 186, "end_pos": 213, "type": "TASK", "confidence": 0.9082690080006918}]}, {"text": "Our main contribution in this work is to revisit the task initially discussed in which consists in identifying in a Wikipedia article all the mentions of the concept being described by this article.", "labels": [], "entities": []}, {"text": "We refer to this concept as the \"main concept\" (MC) henceforth.", "labels": [], "entities": []}, {"text": "For instance, within the article Chilly Gonzales, the task is to find all proper (e.g. Gonzales, Beck), nominal (e.g. the performer) and pronominal (e.g. he) mentions that refer to the MC \"Chilly Gonzales\".", "labels": [], "entities": []}, {"text": "More specifically, we frame this task as a binary classification problem, where one has to decide whether a detected mention refers to the MC.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7205329835414886}]}, {"text": "Our classifier exploits carefully designed features extracted from Wikipedia markup and characteristics, as well as from Freebase; many of which we borrowed from the related literature.", "labels": [], "entities": []}, {"text": "We show that our approach outperforms stateof-the-art generic coreference resolution engines on this task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.8534602522850037}]}, {"text": "We further demonstrate that the integration of our classifier into the state-of-the-art rule-based coreference system of improves the detection of coreference chains in Wikipedia articles.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We discuss related works in Section 2.", "labels": [], "entities": []}, {"text": "We describe in Section 3 the Wikipedia-based dataset we exploited in this study, and show the drop in performance of state-of-the-art coreference resolution systems when faced to this corpus.", "labels": [], "entities": [{"text": "Wikipedia-based dataset", "start_pos": 29, "end_pos": 52, "type": "DATASET", "confidence": 0.8354099690914154}, {"text": "coreference resolution", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.890734076499939}]}, {"text": "We describe in Section 4 the baselines we built on top of two state-ofthe-art coreference resolution systems, and present our approach in Section 5.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.748665064573288}]}, {"text": "We report on experiments we conducted in section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "As our approach is dedicated to Wikipedia articles, we used the freely 1 available resource called WikiCoref (.", "labels": [], "entities": [{"text": "WikiCoref", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.949276864528656}]}, {"text": "This ressource consists in 30 English Wikipedia articles manually coreference-annotated.", "labels": [], "entities": []}, {"text": "It comprises 60k tokens annotated with the OntoNotes project guidelines (.", "labels": [], "entities": [{"text": "OntoNotes project guidelines", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.9283138314882914}]}, {"text": "Each mention is annotated with three attributes: the mention type (named-entity, noun phrase, or pronominal), the coreference type (identity, attributive or copular) and the equivalent Freebase entity if it exists.", "labels": [], "entities": []}, {"text": "The resource contains roughly 7 000 non singleton mentions, among which 1 800 refer to the main concept, which is to say that 30 chains out of 1 469 makeup for 25% of the mentions annotated.", "labels": [], "entities": []}, {"text": "In this section, we first describe the data preparation we conducted (section 6.1), and provide details on the classifier we trained (section 6.2).", "labels": [], "entities": []}, {"text": "Then, we report experiments we carried out on the task of identifying the mentions co-referent (positive class) to the main concept of an article (section 6.3).", "labels": [], "entities": []}, {"text": "We compare our approach to the baselines described in section 4, and analyze the impact of the families of features described in section 5.", "labels": [], "entities": []}, {"text": "We also investigate a simple extension of Dcoref which takes advantage of our classifier for improving coreference resolution (section 6.4).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.9552498161792755}]}], "tableCaptions": [{"text": " Table 1: CoNLL F1 score of recent state-of-the- art systems on the WikiCoref dataset, and the 2012  OntoNotes test data for predicted mentions.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.41042381525039673}, {"text": "F1 score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8910198509693146}, {"text": "WikiCoref dataset", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9595554172992706}, {"text": "OntoNotes test data", "start_pos": 101, "end_pos": 120, "type": "DATASET", "confidence": 0.8126187523206075}]}, {"text": " Table 2: Performance of the baselines on the task of identifying all MC coreferent mentions.", "labels": [], "entities": []}, {"text": " Table 3: Performance of our approach on the  pronominal mentions, as a function of the features.", "labels": [], "entities": []}, {"text": " Table 4: Performance of our approach on the non- pronominal mentions, as a function of the features.", "labels": [], "entities": []}]}