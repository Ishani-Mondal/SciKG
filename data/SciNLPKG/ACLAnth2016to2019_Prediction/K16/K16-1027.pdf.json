{"title": [{"text": "Substring-based unsupervised transliteration with phonetic and contextual knowledge", "labels": [], "entities": [{"text": "Substring-based unsupervised transliteration", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8062819242477417}]}], "abstractContent": [{"text": "We propose an unsupervised approach for substring-based transliteration which incorporates two new sources of knowledge in the learning process: (i) context by learning substring mappings, as opposed to single character mappings, and (ii) pho-netic features which capture cross-lingual character similarity via prior distributions.", "labels": [], "entities": []}, {"text": "Our approach is a two-stage iterative, boot-strapping solution, which vastly outperforms Ravi and Knight (2009)'s state-of-the-art unsupervised translitera-tion method and outperforms a rule-based baseline by up to 50% for top-1 accuracy on multiple language pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9722522497177124}]}, {"text": "We show that substring-based models are superior to character-based models, and observe that their top-10 accuracy is comparable to the top-1 accuracy of supervised systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9933684468269348}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9360105395317078}]}, {"text": "Our method only requires a phonemic representation of the words.", "labels": [], "entities": []}, {"text": "This is possible for many language-script combinations which have a high grapheme-to-phoneme correspondence e.g. scripts of Indian languages derived from the Brahmi script.", "labels": [], "entities": []}, {"text": "Hence, Indian languages were the focus of our experiments.", "labels": [], "entities": []}, {"text": "For other languages, a grapheme-to-phoneme converter would be required.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transliteration is a key building block for multilingual and cross-lingual NLP since it is useful for user-friendly input methods and applications like machine translation and cross-lingual information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.8273357450962067}, {"text": "cross-lingual information retrieval", "start_pos": 176, "end_pos": 211, "type": "TASK", "confidence": 0.6603600680828094}]}, {"text": "The best performing solutions are supervised, discriminative learning methods which learn transliteration models from parallel transliteration corpora.", "labels": [], "entities": []}, {"text": "However, such corpora are available only for some language pairs.", "labels": [], "entities": []}, {"text": "It is also expensive and time-consuming to build a parallel corpus.", "labels": [], "entities": []}, {"text": "This limitation can be addressed in three ways: (i) train a transliteration model on mined parallel transliterations.", "labels": [], "entities": []}, {"text": "The transliterations can be mined from monolingual comparable corpora (Jagarlamudi and Daum\u00e9 III, 2012) or parallel translation corpora ().", "labels": [], "entities": []}, {"text": "However, it may not be possible to mine enough transliteration pairs to train a system for most languages.", "labels": [], "entities": []}, {"text": "(ii) transliterate via abridge language () when transliteration corpora involving bridge languages is available.", "labels": [], "entities": []}, {"text": "(iii) learn transliteration models in an unsupervised setting using only monolingual word lists.", "labels": [], "entities": []}, {"text": "Unsupervised transliteration can be defined as: Learn a transliteration model (T X ) from the source language (F) to the target (E) language given their respective monolingual word lists, W F and W E respectively.", "labels": [], "entities": []}, {"text": "We explore this direction in the present work, addressing shortcomings in the previous work.", "labels": [], "entities": []}, {"text": "Our work addresses two major limitations in existing unsupervised transliteration approaches: (i) lack of linguistic signals to drive the learning, and (ii) limited use of context since their model is character-based.", "labels": [], "entities": []}, {"text": "Due to this knowledge-lite approach, these model performs poorly.", "labels": [], "entities": []}, {"text": "Our primary contributions are novel methods to incorporate two knowledge sources, phonetic and contextual, in the training process.", "labels": [], "entities": []}, {"text": "These knowledge sources are critical since statistical co-occurrence signals used in supervised learning are not available for unsupervised learning.", "labels": [], "entities": []}, {"text": "Unlike transliteration mining, our approach can learn effectively even if the source and target corpus do not have any transliteration pairs in common.", "labels": [], "entities": [{"text": "transliteration mining", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8505004346370697}]}, {"text": "We propose a two-stage iterative, bootstrapping approach for learning unsupervised transliteration models.", "labels": [], "entities": []}, {"text": "In the first stage, a character-based model is learnt which is used to bootstrap and learn a series of improved substring-based models in the second stage.", "labels": [], "entities": []}, {"text": "The first stage incorporates two linguistic signals to drive the learning process: phonemic correspondence and phonetic similarity.", "labels": [], "entities": []}, {"text": "This means we make the model aware that two characters represent either the same phoneme (\u0915 in Hindi and \u0995 in Bengali ) or similar phonemes (\u0915 in Hindi and \u0996 [IPA: k h ] in Bengali -which differ only in aspiration).", "labels": [], "entities": []}, {"text": "We achieve this by incorporating phonetic information as prior distributions in our EM-MAP approach to characterbased unsupervised learning.", "labels": [], "entities": []}, {"text": "We show that these linguistic signals can improve top-1 accuracy by 20%-100% over a baseline rule-based system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9764070510864258}]}, {"text": "It is also vastly superior to knowledge-lite unsupervised methods.", "labels": [], "entities": []}, {"text": "The second stage incorporates contextual knowledge by unsupervised learning of a substring-based transliteration model viz.", "labels": [], "entities": []}, {"text": "learning mappings from substring in one language to another, as opposed to learning single character mappings.", "labels": [], "entities": []}, {"text": "In other words, along with learning mappings of the form (k hindi \u2192 k bengali ), we also try to learn mappings of the form (kaa hindi \u2192 kaa bengali ).", "labels": [], "entities": []}, {"text": "It is known that substring-based transliteration outperforms character-based transliteration in a supervised setting due to the additional context information.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, ours is the first unsupervised approach for substring-based transliteration.", "labels": [], "entities": []}, {"text": "It outperforms a character-based model by up to 11% in terms of top-1 accuracy and 27% in terms of top-10 accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9779738783836365}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9815014004707336}]}, {"text": "The top-10 accuracy of our unsupervised system is comparable to the top-1 accuracy of a supervised system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.993536651134491}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9802072644233704}]}, {"text": "Hence, the unsupervised system maybe a reasonable substitute for supervised systems in applications which require transliteration (e.g. handling untranslated words in MT) and can disambiguate from the top-k transliterations with information available to the application (e.g. LM in MT systems).", "labels": [], "entities": []}, {"text": "The focus of our work was Indian languages using scripts descended from the ancient Brahmi script.", "labels": [], "entities": []}, {"text": "We show that our methods can be applied to these languages without requiring phoneme dictionaries or grapheme-to-phoneme converters.", "labels": [], "entities": []}, {"text": "We achieve this by using scriptural properties and similarity across scripts to capture phonemic correspondence and phonetic similarity, and show results on 4 languages using 4 different scripts.", "labels": [], "entities": []}, {"text": "At least 19 of the Indian subcontinent's top 30 and 9 of the top 10 most spoken languages use Brahmiderived scripts.", "labels": [], "entities": []}, {"text": "Each of these languages have more than a million speakers with an aggregate speaker population of about 900 million, so our method is widely applicable.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: We experimented on the following Indian language pairs representing two language families: Bengali\u2192Hindi, Kannada\u2192Hindi, Hindi\u2192Kannada and Tamil\u2192Kannada.", "labels": [], "entities": []}, {"text": "Bengali (bn) and Hindi (hi) are Indo-Aryan languages, while Kannada (kn) and Tamil (ta) are Dravidian languages.", "labels": [], "entities": []}, {"text": "We used 10k source language names as training corpus, which were collected from various sources.", "labels": [], "entities": []}, {"text": "We evaluated our systems on the NEWS 2015 Indic dataset.", "labels": [], "entities": [{"text": "NEWS 2015 Indic dataset", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.9284070581197739}]}, {"text": "We created this set from the English to Indian language training corpora of the NEWS 2015 shared task ( by mining name pairs which have English names in common.", "labels": [], "entities": [{"text": "NEWS 2015 shared task", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.859019547700882}]}, {"text": "1500 words were selected at random to create the testset.", "labels": [], "entities": []}, {"text": "The remaining pairs are used to train and tune a skyline supervised transliteration system for comparison.", "labels": [], "entities": []}, {"text": "The training sets are small, the number of name pairs being: 2556 (bn-hi), 4022 (knhi), 3586 (hi-kn) and 3230 (ta-kn).", "labels": [], "entities": []}, {"text": "Experimental Setup: We trained the character level unsupervised transliteration systems with source language word lists using a custom implementation . We set the the value of the scaling factors (\u03b2, \u03b3 c , \u03b3 s ) to 100.", "labels": [], "entities": []}, {"text": "Viterbi decoding was done with a bigram character language model, followed by re-ranking with a 5-gram character language model.", "labels": [], "entities": []}, {"text": "We trained the substring level discriminative transliteration models as well as a skyline su-: Results for character-based model (% scores) pervised transliteration system using the Moses () machine translation system with default parameters.", "labels": [], "entities": []}, {"text": "Batch MIRA (Cherry and Foster, 2012) was used to tune the Stage 2 systems with 1000 name pairs and supervised systems with 500 name pairs.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.8864355087280273}]}, {"text": "The tuning set for the Stage 2 systems were drawn from the the top-1 transliterations in the synthesized, pseudo-parallel corpus; no true parallel corpus is used.", "labels": [], "entities": []}, {"text": "We used a 5-gram character language model trained with Witten-Bell smoothing on 40k names for all target languages.", "labels": [], "entities": []}, {"text": "We ran Stage 2 for 5 iterations.", "labels": [], "entities": []}, {"text": "For a rule-based baseline, we used the script conversion method implemented in the Indic NLP Library 2 ( which is based on phonemic correspondences.", "labels": [], "entities": [{"text": "script conversion", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.6936517506837845}, {"text": "Indic NLP Library 2", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.7268505990505219}]}, {"text": "Evaluation: We used top-1 accuracy based on exact match (A 1 ) and Mean F-score (F 1 ) at the character level as defined in the NEWS shared tasks as our evaluation metrics ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9716155529022217}, {"text": "exact match (A 1 )", "start_pos": 44, "end_pos": 62, "type": "METRIC", "confidence": 0.9456718564033508}, {"text": "Mean F-score (F 1 )", "start_pos": 67, "end_pos": 86, "type": "METRIC", "confidence": 0.9132356146971384}, {"text": "NEWS shared tasks", "start_pos": 128, "end_pos": 145, "type": "DATASET", "confidence": 0.8200331528981527}]}, {"text": "We also used top-10 accuracy as an evaluation metric (A 10 ), since applications like MT and IR can further disambiguate with context information available to these applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9468156695365906}, {"text": "A 10 )", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9727572004000345}, {"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9513534903526306}]}, {"text": "shows the results for the rule-based system and various character-based unsupervised models.", "labels": [], "entities": []}, {"text": "shows results for substring-level models bootstrapped from different character-based models.", "labels": [], "entities": []}, {"text": "Results of supervised transliteration on a small training set are also shown in both tables.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for substring-based model (% scores)", "labels": [], "entities": []}]}