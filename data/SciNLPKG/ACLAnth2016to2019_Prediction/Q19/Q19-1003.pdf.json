{"title": [{"text": "Joint Transition-Based Models for Morpho-Syntactic Parsing: Parsing Strategies for MRLs and a Case Study from Modern Hebrew", "labels": [], "entities": [{"text": "MRLs", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.9659455418586731}]}], "abstractContent": [{"text": "In standard NLP pipelines, morphological analysis and disambiguation (MA&D) precedes syntactic and semantic downstream tasks.", "labels": [], "entities": [{"text": "morphological analysis and disambiguation (MA&D)", "start_pos": 27, "end_pos": 75, "type": "TASK", "confidence": 0.7287696169482337}]}, {"text": "However, for languages with complex and ambiguous word-internal structure, known as morphologically rich languages (MRLs), it has been hypothesized that syntactic context maybe crucial for accurate MA&D, and vice versa.", "labels": [], "entities": [{"text": "MA&D", "start_pos": 198, "end_pos": 202, "type": "TASK", "confidence": 0.8363303542137146}]}, {"text": "In this work we empirically confirm this hypothesis for Modern Hebrew, an MRL with complex morphology and severe word-level ambiguity, in a novel transition-based framework.", "labels": [], "entities": []}, {"text": "Specifically, we propose a joint morphosyntactic transition-based framework which formally unifies two distinct transition systems, morphological and syntactic, into a single transition-based system with joint training and joint inference.", "labels": [], "entities": []}, {"text": "We empirically show that MA&D results obtained in the joint settings outperform MA&D results obtained by the respective standalone components, and that end-to-end parsing results obtained by our joint system present anew state of the art for Hebrew dependency parsing.", "labels": [], "entities": [{"text": "Hebrew dependency parsing", "start_pos": 242, "end_pos": 267, "type": "TASK", "confidence": 0.733139157295227}]}], "introductionContent": [{"text": "NLP research in recent years has shown increasing interest in parsing typologically different languages, as evident, for instance, by the universal dependencies 1 initiative ().", "labels": [], "entities": [{"text": "parsing typologically different languages", "start_pos": 62, "end_pos": 103, "type": "TASK", "confidence": 0.8907203525304794}]}, {"text": "In particular, much attention is drawn to parsing morphologically rich languages (MRLs), which differ significantly from English in their structure and characteristics.", "labels": [], "entities": [{"text": "parsing morphologically rich languages (MRLs)", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.897540305341993}]}, {"text": "In MRLs, grammatical information, typically expressed using word order in English, is often manifested in the internally complex structure of the words.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.9690398573875427}]}, {"text": "Words in MRLs may carry, in addition to lexical content, functional affixes and clitics that correspond to additional pieces of information.", "labels": [], "entities": []}, {"text": "In Modern Hebrew, for example, the inflected verb ''ahbtih'' 2 (loved + 1pers.singular.past + 3pers.feminine.singular) corresponds to three different grammatical functions: the subject ''I,'' the predicate ''loved,'' and the direct object ''her.'' Similarly, Spanish d\u00e1melo corresponds to a predicate, an indirect object, and a direct object, as in ''give it to me.'' Thus, in MRLs, morphological analysis (MA) which translates raw space-delimited tokens to syntactically relevant ''word'' units is a necessary condition for any syntactic or semantic downstream task.", "labels": [], "entities": []}, {"text": "However, raw space-delimited tokens in MRLs are often highly ambiguous.", "labels": [], "entities": [{"text": "MRLs", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.8193739056587219}]}, {"text": "In Hebrew, Arabic, and other Semitic languages, this situation is further complicated by fact that written texts lack diacritics.", "labels": [], "entities": []}, {"text": "The Hebrew token ''fmn,'' for instance, maybe read as the noun ''oil,'' the adjective ''fat,'' the verb ''lubricated,'' the sequence ''that''+''of,'' or the phrase ''their''+''name,'' only one of which is relevant in context.", "labels": [], "entities": []}, {"text": "This has clear ramifications for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.9140199720859528}]}, {"text": "shows a lattice that captures all possible analyses of the Hebrew phrase ''bclm hneim,'' literally: ''in-the-shadow-of-them the-pleasant,'' translated ''in their pleasant shadow.'' Each lattice arc corresponds to a potential node in a dependency tree.", "labels": [], "entities": []}, {"text": "Dark circles mark morpheme boundaries, double circles mark token boundaries.", "labels": [], "entities": []}, {"text": "The top tree depicts a correct syntactic analysis.", "labels": [], "entities": []}, {"text": "In the bottom tree, incorrectly disambiguated tokens lead to a wrong syntactic analysis.", "labels": [], "entities": []}, {"text": "Previous dependency parsing evaluation campaigns ( assumed that the correct morphological analysis and disambiguation (MA&D) of the input stream is known in advance.", "labels": [], "entities": [{"text": "dependency parsing evaluation", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.8695735732714335}, {"text": "correct morphological analysis and disambiguation (MA&D)", "start_pos": 68, "end_pos": 124, "type": "METRIC", "confidence": 0.6165424942970276}]}, {"text": "In realistic end-toend parsing scenarios, however, this is of course not so.", "labels": [], "entities": [{"text": "end-toend parsing", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6165197491645813}]}, {"text": "To overcome this, pipeline architectures where MA&D precedes parsing have been setup.", "labels": [], "entities": [{"text": "MA&D precedes parsing", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6634850203990936}]}, {"text": "These pipelines are suboptimal since they suffer from error propagation, and since local linear context available for automatic MA&D maybe insufficient for accurate morphological disambiguation.", "labels": [], "entities": [{"text": "MA&D", "start_pos": 128, "end_pos": 132, "type": "TASK", "confidence": 0.8356581727663676}]}, {"text": "For this, actual syntactic context maybe required).", "labels": [], "entities": []}, {"text": "To resolve this apparent loop, where morphological analysis is required for syntactic parsing and syntactic analysis is required for morphological disambiguation, hypothesised that joint morphosyntactic parsing, where morphological information may assist syntactic disambiguation and vice versa, maybe better suited.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7825085520744324}, {"text": "joint morphosyntactic parsing", "start_pos": 181, "end_pos": 210, "type": "TASK", "confidence": 0.6241668164730072}, {"text": "syntactic disambiguation", "start_pos": 255, "end_pos": 279, "type": "TASK", "confidence": 0.7289517670869827}]}, {"text": "This joint morphosyntactic hypothesis has been taken up and successfully confirmed in the context of phrase-structure parsing for Semitic languages.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7307622134685516}]}, {"text": "For dependency parsing, and present language-agnostic transition-based frameworks for jointly parsing and tagging input words, though without addressing the complex issue of retokenizing ambiguous input tokens.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8512422144412994}, {"text": "parsing and tagging input words", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.7420147240161896}]}, {"text": "More recently, presented a graph-based framework for lattice parsing of Turkish also covering morphological segmentation.", "labels": [], "entities": [{"text": "lattice parsing", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.6665221601724625}]}, {"text": "Their system takes a ''product of experts'' approach wherein the morphological paths and dependency trees are handled via two distinct models (a linear model over bigrams for MD and an arc-factor model for dependencies), reaching agreement via a dual decomposition setup.", "labels": [], "entities": []}, {"text": "In this work, we present a novel, languageagnostic, transition-based framework for end-toend morphosyntactic dependency parsing.", "labels": [], "entities": [{"text": "end-toend morphosyntactic dependency parsing", "start_pos": 83, "end_pos": 127, "type": "TASK", "confidence": 0.6109189987182617}]}, {"text": "The framework unifies a morphological and a syntactic component into a joint parser encompassing a single transition system, a single objective function, joint learning, and joint decoding.", "labels": [], "entities": []}, {"text": "We apply this system to parsing Modern Hebrew and empirically confirm that predicting MA&D in the joint settings improves upon standalone MA&D, and upon recently reported Hebrew MA&D results.", "labels": [], "entities": [{"text": "parsing Modern Hebrew", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7500318288803101}, {"text": "predicting MA&D", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.8872432559728622}, {"text": "MA&D", "start_pos": 138, "end_pos": 142, "type": "TASK", "confidence": 0.6240789095560709}]}, {"text": "Our system further improves endto-end dependency parsing results in comparison to existing state-of-the-art parsers in pipeline scenarios, it significantly outperforms the joint parser of, and it substantially outperforms the dependency parser of, so far considered the de facto standard for Hebrew dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.652297243475914}, {"text": "Hebrew dependency parsing", "start_pos": 292, "end_pos": 317, "type": "TASK", "confidence": 0.5957149068514506}]}, {"text": "The contribution of this paper is thus threefold.", "labels": [], "entities": []}, {"text": "First, we define a language-agnostic joint morphosyntactic parser in a transition-based framework.", "labels": [], "entities": []}, {"text": "Secondly, we empirically confirm that MA&D benefits from syntactic parsing, and in realistic end-to-end parsing scenarios, also vice versa.", "labels": [], "entities": [{"text": "MA&D", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.8823564648628235}, {"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.6775434613227844}]}, {"text": "Finally, we present anew set of strong Hebrew end-to-end parsing results and deliver an open-source, language agnostic implementation of the joint parser, for further investigating joint morphosyntactic parsing strategies.", "labels": [], "entities": [{"text": "joint morphosyntactic parsing", "start_pos": 181, "end_pos": 210, "type": "TASK", "confidence": 0.6852904558181763}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present our formal framework (2.1), morphological model (2.2), syntactic model (2.3), and joint framework (2.4).", "labels": [], "entities": []}, {"text": "Sections 3 and 4 present our experiments and analysis, respectively.", "labels": [], "entities": []}, {"text": "Section 5 discusses related and future work, and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "Goal: We aim to test the hypothesis that joint syntactic and morphological disambiguation is better than a pipeline by empirically comparing the Pipeline, MDFirst and ArcGreedy 3 parsing strategies in our unified transition-based morphosyntactic framework.", "labels": [], "entities": [{"text": "joint syntactic and morphological disambiguation", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.6249048769474029}, {"text": "Pipeline", "start_pos": 145, "end_pos": 153, "type": "DATASET", "confidence": 0.9560753703117371}, {"text": "ArcGreedy 3 parsing", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.7151143153508505}]}, {"text": "12 Data: We use the Modern Hebrew section of the SPMRL shared task (), derived from the Hebrew Unified-SD version of Tsarfaty (2013).", "labels": [], "entities": [{"text": "SPMRL shared task", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.743253211180369}]}, {"text": "For the purpose of this work, we harmonized the treebank annotation scheme We set k = 3 because some features of Zhang and Nivre (2011) require three morphemes in the buffer. with the annotation scheme of the lexical resources of, and in particular the HEBLEX lexicon of.", "labels": [], "entities": [{"text": "HEBLEX lexicon", "start_pos": 253, "end_pos": 267, "type": "DATASET", "confidence": 0.9039671123027802}]}, {"text": "We use the standard train/dev/test sets split, train on the train set (5,000 sentences) with a detailed investigation on dev (500), and confirm our results on test (716).", "labels": [], "entities": []}, {"text": "Implementation: We implemented from scratch a fully integrated, transition-based, multilingual natural language processor, written in Go.", "labels": [], "entities": []}, {"text": "Our implementation uses a general purpose morphological analyzer, which for Hebrew is backed by the BGU HEBLEX lexicon).", "labels": [], "entities": [{"text": "BGU HEBLEX lexicon", "start_pos": 100, "end_pos": 118, "type": "DATASET", "confidence": 0.7169606586297353}]}, {"text": "We implemented the morphological disambiguator, dependency parser, and joint integration strategies defined herein.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7982065975666046}]}, {"text": "We implemented and experimented with both the Arc Standard and Arc ZEager transition systems.", "labels": [], "entities": [{"text": "Arc Standard", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9129123091697693}, {"text": "Arc ZEager transition", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.847257137298584}]}, {"text": "14 Scenarios: In MRLs, out-of-vocabulary (OOV) tokens pose a great challenge to parsing.", "labels": [], "entities": []}, {"text": "A raw token may have not been observed during training, even though all its morphemes have been observed in other contexts.", "labels": [], "entities": []}, {"text": "To gauge the effect of such OOV items on the quality of Hebrew parses, we evaluate the system in two different scenarios.", "labels": [], "entities": []}, {"text": "In the first, infused scenario, we verify that each lattice contains the gold morphological analysis.", "labels": [], "entities": []}, {"text": "That is, if the gold path is not present in L = MA(x) (hence, an OOV), we automatically infuse the gold path into L.", "labels": [], "entities": [{"text": "MA", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9395313262939453}, {"text": "OOV", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9767242670059204}]}, {"text": "We contrast this with uninfused scenarios, where we use a realistic morphological analyzer with its (incomplete) lexical coverage as is, compliant with.", "labels": [], "entities": []}, {"text": "Settings: In all experiments, we used abeam of size 64, which, in our preliminary experiments on dev, gave better results for the joint models than abeam of 32, and in any event no worse results than abeam of 128.", "labels": [], "entities": []}, {"text": "To avoid both overfitting and underfitting, we define a stopping condition for the training procedure, which we test in each training iteration.", "labels": [], "entities": []}, {"text": "During training, we use a sliding window of three iterations and select the first model that precedes two sequential scores-drop on dev.", "labels": [], "entities": []}, {"text": "For pipeline models, we test distinct stopping conditions for the morphological and the syntactic models, each based on its own standalone scores.", "labels": [], "entities": []}, {"text": "For joint models, we test the stopping condition with respect to a single overall dependency F 1 score, which we define shortly.", "labels": [], "entities": []}, {"text": "Evaluating Morphology: To evaluate morphological disambiguation (MD) results, we report the F 1 scores on the set of predicted morphemes versus gold-standard morphemes in the sentence.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9802443583806356}]}, {"text": "Formally, let M p , Mg be sets of predicted and gold morphemes of the sentence, respectively.", "labels": [], "entities": []}, {"text": "We define precision, recall, and F 1 -scores as: We report two different scores for each MA&D run, one for full MD including segmentation, tagging and morphological features (MD Full), and one for segmentation and tags only (MD POS).", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9992164373397827}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9984639883041382}, {"text": "F 1 -scores", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9867102652788162}]}, {"text": "Evaluating Dependencies: Evaluating joint morpho-syntactic dependency parsing performance is non-trivial, because the gold and parse trees may have a different number of nodes, which precludes the application of standard attachment scores; it suffices that an incorrect segmentation occurs early in the sequence, then off-by-one indices in the remainder of the sentence deem the rest of the arcs incorrect ().", "labels": [], "entities": [{"text": "Evaluating joint morpho-syntactic dependency parsing", "start_pos": 25, "end_pos": 77, "type": "TASK", "confidence": 0.6778928637504578}]}, {"text": "Let us illustrate this effect.", "labels": [], "entities": []}, {"text": "Consider the Hebrew phrase ''bbit'' (translated ''in the house'') that appears as a single space-delimited token.", "labels": [], "entities": []}, {"text": "Now consider the two following MD alternatives, with and without the Hebrew covert definite article.", "labels": [], "entities": []}, {"text": "We also include here the indices of the disambiguated morphemes in their linear order: Gold MD: 1.b(''in'') 2.h(''the'') 3.bit(''house'') Predicted MD: 1.b(''in'') 2.bit(''house'').", "labels": [], "entities": []}, {"text": "Further assume that both the Gold and Predicted dependency trees contain the correct dependency arc between b (''in'') and bit (''house'') labeled pobj.", "labels": [], "entities": []}, {"text": "In simple LAS terms, the arcs that would be compared for the purpose of evaluation are: So the pobj predicted arc will be considered an error, even though the relation between forms is correct, and accordingly both UAS and LAS will be 0.", "labels": [], "entities": [{"text": "UAS", "start_pos": 215, "end_pos": 218, "type": "METRIC", "confidence": 0.6093937158584595}, {"text": "LAS", "start_pos": 223, "end_pos": 226, "type": "METRIC", "confidence": 0.8597649335861206}]}, {"text": "To address this issue, we define an F 1 accuracy measure with respect to the forms of arc edges, rather than their node indices.", "labels": [], "entities": [{"text": "F 1 accuracy measure", "start_pos": 36, "end_pos": 56, "type": "METRIC", "confidence": 0.80349300801754}]}, {"text": "Formally, let M p be the predicted morphological disambiguation of x, and let A p be the predicted dependency tree over M p . Likewise, let Mg , Ag be the gold-standard morphological disambiguation and dependency tree of x.", "labels": [], "entities": []}, {"text": "We now replace the index of each node in the arcs of A p , Ag with the form of the corresponding morpheme in M p and Mg . Let J p , J g be the form-based (rather than index-based) arcs of the predicted and gold representations of x.", "labels": [], "entities": []}, {"text": "We report both labeled and unlabeled F 1 as: 15 In our example, the revised arcs will now be: Now, the parser will be credited for identifying the pobj arc correctly, as desired, and the dependency scores will be: Pr = 1, Re = 0.5, and F 1 = 0.67.", "labels": [], "entities": [{"text": "Pr", "start_pos": 214, "end_pos": 216, "type": "METRIC", "confidence": 0.9881735444068909}, {"text": "Re", "start_pos": 222, "end_pos": 224, "type": "METRIC", "confidence": 0.9787904620170593}, {"text": "F 1", "start_pos": 236, "end_pos": 239, "type": "METRIC", "confidence": 0.9860270321369171}]}, {"text": "Results: present our morphosyntactic parsing results for each of our different systems in all, pipeline and joint, strategies.", "labels": [], "entities": []}, {"text": "We report F 1 scores, both MD Full and MD POS for morphological disambiguation (MD), and both unlabeled and labeled F 1 scores for the dependency trees (Dep).", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9645685156186422}, {"text": "MD Full and MD POS", "start_pos": 27, "end_pos": 45, "type": "METRIC", "confidence": 0.754554009437561}, {"text": "F 1 scores", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9020315806070963}]}, {"text": "present results on the Modern Hebrew dev set, and confirm our results on the test set.", "labels": [], "entities": [{"text": "Modern Hebrew dev set", "start_pos": 23, "end_pos": 44, "type": "DATASET", "confidence": 0.7870266288518906}]}, {"text": "presents parsing results for infused morphological lattices; that is, ambiguous MA lattices that are guaranteed to also include the correct MD path in them.", "labels": [], "entities": []}, {"text": "In these experiments, we see that MD results in joint parsing strategies (MDFirst, ArcGreedy) always improve upon the MD standalone/pipeline results.", "labels": [], "entities": []}, {"text": "In particular, all MD results across the joint strategies are very close.", "labels": [], "entities": [{"text": "MD", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9937514662742615}]}, {"text": "We observe only a minor advantage for ArcZeager over Arc-Standard for both joint strategies.", "labels": [], "entities": [{"text": "ArcZeager", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9297786355018616}, {"text": "Arc-Standard", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.921261191368103}]}, {"text": "This increase in MD accuracy unfortunately comes at the expense of syntax, where we observe a slight drop (up to 0.5 point inlabeled F 1 ) when switching from pipeline to joint strategies.", "labels": [], "entities": [{"text": "MD", "start_pos": 17, "end_pos": 19, "type": "METRIC", "confidence": 0.7934266328811646}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.6893907785415649}, {"text": "F 1 )", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9463062286376953}]}, {"text": "We confirm this trend on the test set in  joint results are better than the respective pipelines (although now Arc-Standard slightly improves upon Arc-Zeager in the ArcGreedy strategy), while dependency parsing results drop in joint scenarios (a slightly larger drop than on dev).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.7916988730430603}]}, {"text": "present parsing results for the more interesting scenario, a realistic parsing scenario where we use uninfused lattices-ambigous lattices obtained by an existing broad-coverage morphological analyzer, which are not (and cannot be) guaranteed to always also include the correct path.", "labels": [], "entities": []}, {"text": "As expected, on both the dev set and test set, the results drop relative to the respective infused scenarios, respectively), as some elements from the correct path and tree are no longer reachable within the search space.", "labels": [], "entities": []}, {"text": "At the same time, it is interesting to observe that for both dev and test, all MD scores (Full/POS) as well as dependency scores (un/labeled) are better in joint parsing.", "labels": [], "entities": [{"text": "MD scores (Full/POS)", "start_pos": 79, "end_pos": 99, "type": "METRIC", "confidence": 0.8760560665811811}]}, {"text": "The specific differences between the joint strategies and transition systems do not matter very muchthe robust empirical trend is that switching from pipeline to joint improves both MD and dependency parsing performance.", "labels": [], "entities": [{"text": "MD", "start_pos": 182, "end_pos": 184, "type": "METRIC", "confidence": 0.9878608584403992}, {"text": "dependency parsing", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.7067584991455078}]}, {"text": "It is interesting to inquire why in the infused scenario, on both dev and test, dependency parsing results in the joint strategies drop relative to the respective pipelines.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.6343955248594284}]}, {"text": "At it turns out, in case the   correct analysis of a rare (OOV) token has been injected artificially into the lattice, training on these lattices may turnout to be misleading.", "labels": [], "entities": []}, {"text": "Injecting a correct but rare MSR may lead to an artificial ''certainty'' as to its appropriate syntactic context.", "labels": [], "entities": [{"text": "certainty", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9841549396514893}]}, {"text": "Then, if the parser does not apply robust statistics on the general behavior of rare/OOV items in different syntactic contexts (as would be the casein joint uninfused scenarios), selecting the injected MD may lead to a wrong syntactic decision.", "labels": [], "entities": []}, {"text": "The main message coming out of our experiments is that joint morphological disambiguation and syntactic parsing in this transition-based framework is preferred to pipeline settings, inline with the hypothesis that syntactic information aids morphological disambiguation.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 94, "end_pos": 111, "type": "TASK", "confidence": 0.7571467757225037}]}, {"text": "Furthermore, it is reassuring to observe that when parsing uninfused lattices, as in the more realistic scenario, dependency parsing results improve upon pipeline scenarios, corroborating the findings of Seeker and Centinoglu (2015) in graph-based frameworks and of and in phrase-structure parsing.", "labels": [], "entities": [{"text": "parsing uninfused lattices", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.8925252755482992}, {"text": "dependency parsing", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7922098934650421}, {"text": "phrase-structure parsing", "start_pos": 273, "end_pos": 297, "type": "TASK", "confidence": 0.7574336230754852}]}, {"text": "End-to-End Parsing Performance: To put our end-to-end system performance in context,    the test set, respectively.", "labels": [], "entities": []}, {"text": "We compare these results with studies that parsed the same data sets.", "labels": [], "entities": []}, {"text": "As shows, our parser significantly outperforms the state-of-the-art parser by, so far considered the de facto standard for Hebrew parsing.", "labels": [], "entities": []}, {"text": "As shown in, the parser also outperforms the results reported by most (though not all) SPMRL shared tasks participants, using the same data and same split.", "labels": [], "entities": [{"text": "SPMRL shared tasks", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8274123668670654}]}, {"text": "Such gold morphology settings are of course not suited for realistic parsing scenarios.", "labels": [], "entities": []}, {"text": "So, in we compare our best end-to-end parsing results to the most recent dependency parsing results in realistic scenarios on the same data (by.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6973758935928345}]}, {"text": "Here our best pipeline and joint systems outperform the previously reported pipeline and joint results, thus presenting anew state of the art for Hebrew dependency parsing.", "labels": [], "entities": [{"text": "Hebrew dependency parsing", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.678504467010498}]}, {"text": "Moreover, these results are obtained within a unified formal framework in a single ''allincluded'' implementation, providing a further practical advantage of not having to maintain and train separate standalone components.", "labels": [], "entities": []}, {"text": "16 report only UAS, only dev.", "labels": [], "entities": [{"text": "UAS", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.894196093082428}]}, {"text": "Our implementation, models, and data are publicly available via https://github.com/OnlpLab/yap.", "labels": [], "entities": []}, {"text": "We also provide a web demo of Hebrew raw-to-dependency parsing http://onlp.openu.org.il/.", "labels": [], "entities": [{"text": "Hebrew raw-to-dependency parsing", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.565181831518809}]}], "tableCaptions": [{"text": " Table 1: Joint morpho-syntactic parsing of the Modern  Hebrew dev set with infused morphological lattices.", "labels": [], "entities": []}, {"text": " Table 2: Joint morpho-syntactic parsing of the Modern  Hebrew test set with infused morphological lattices.", "labels": [], "entities": [{"text": "Modern  Hebrew test set", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.707131527364254}]}, {"text": " Table 3: Joint morpho-syntactic parsing of the Modern  Hebrew dev set with uninfused (realistic) lattices.", "labels": [], "entities": []}, {"text": " Table 4: Joint morpho-syntactic parsing of the Modern  Hebrew test set with uninfused (realistic) lattices.", "labels": [], "entities": [{"text": "Modern  Hebrew test set", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.7148953378200531}]}, {"text": " Table 8: Qualitative error analysis: The number (percentage) of error patterns of Arc-Zeager in pipeline/joint  scenarios, on a sample of 100 sentences from the dev set, in uninfused lattices settings.", "labels": [], "entities": [{"text": "Qualitative error analysis", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.5646200180053711}]}]}