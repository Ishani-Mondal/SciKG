{"title": [{"text": "Rotational Unit of Memory: A Novel Representation Unit for RNNs with Scalable Applications", "labels": [], "entities": []}], "abstractContent": [{"text": "Stacking long short-term memory (LSTM) cells or gated recurrent units (GRUs) as part of a recurrent neural network (RNN) has become a standard approach to solving a number of tasks ranging from language modeling to text summarization.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.7574017345905304}, {"text": "text summarization", "start_pos": 215, "end_pos": 233, "type": "TASK", "confidence": 0.811652272939682}]}, {"text": "Although LSTMs and GRUs were designed to model long-range dependencies more accurately than conventional RNNs, they nevertheless have problems copying or recalling information from the long distant past.", "labels": [], "entities": []}, {"text": "Here, we derive a phase-coded representation of the memory state, Rotatio-nal Unit of Memory (RUM), that unifies the concepts of unitary learning and associative memory.", "labels": [], "entities": [{"text": "Rotatio-nal Unit of Memory (RUM)", "start_pos": 66, "end_pos": 98, "type": "METRIC", "confidence": 0.8464551738330296}]}, {"text": "We show experimentally that RNNs based on RUMs can solve basic sequential tasks such as memory copying and memory recall much better than LSTMs/GRUs.", "labels": [], "entities": [{"text": "memory copying", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.6682892888784409}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.7817639708518982}]}, {"text": "We further demonstrate that by replacing LSTM/GRU with RUM units we can apply neural networks to real-world problems such as language modeling and text summarization, yielding results comparable to the state of the art.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7320821732282639}, {"text": "text summarization", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7733087837696075}]}], "introductionContent": [{"text": "An important element of the ongoing neural revolution in Natural Language Processing (NLP) is the rise of Recurrent Neural Networks (RNNs), which have become a standard tool for addressing a number of tasks ranging from language modeling, part-of-speech tagging and named entity recognition to neural machine translation, text summarization, question answering, and building chatbots/ dialog systems.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.7546439369519552}, {"text": "language modeling", "start_pos": 220, "end_pos": 237, "type": "TASK", "confidence": 0.729898989200592}, {"text": "part-of-speech tagging", "start_pos": 239, "end_pos": 261, "type": "TASK", "confidence": 0.7339942455291748}, {"text": "named entity recognition", "start_pos": 266, "end_pos": 290, "type": "TASK", "confidence": 0.6124753554662069}, {"text": "neural machine translation", "start_pos": 294, "end_pos": 320, "type": "TASK", "confidence": 0.6714569628238678}, {"text": "text summarization", "start_pos": 322, "end_pos": 340, "type": "TASK", "confidence": 0.7743366658687592}, {"text": "question answering", "start_pos": 342, "end_pos": 360, "type": "TASK", "confidence": 0.8876355588436127}]}, {"text": "As standard RNNs suffer from exploding/ vanishing gradient problems, alternatives such as long short-term memory (LSTM)) or gated recurrent units (GRUs) () have been proposed and have now become standard.", "labels": [], "entities": []}, {"text": "Nevertheless, LSTMs and GRUs fail to demonstrate really long-term memory capabilities or efficient recall on synthetic tasks (see).", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.992238461971283}]}, {"text": "shows that when RNN units are fed along string (e.g., emojis in(a)), they struggle to represent the input in their memory, which results in recall or copy mistakes.", "labels": [], "entities": [{"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9955123066902161}]}, {"text": "The origins of these issues are two-fold: (i) a single hidden state cannot memorize complicated sequential dynamics and (ii) the hidden state is not manipulated well, resulting in information loss.", "labels": [], "entities": []}, {"text": "Typically, these are addressed separately: by using external memory for (i), and gated mechanisms for (ii).", "labels": [], "entities": []}, {"text": "Here, we solve (i) and (ii) jointly by proposing a novel RNN unit, Rotational Unit of Memory (RUM), that manipulates the hidden state by rotating it in an Euclidean space, resulting in a better information flow.", "labels": [], "entities": []}, {"text": "This remedy to (ii) affects (i) to the extent that the external memory is less needed.", "labels": [], "entities": []}, {"text": "As a proof of concept, in(a), RUM recalls correctly a faraway emoji.", "labels": [], "entities": [{"text": "RUM", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.6106159090995789}]}, {"text": "We further show that RUM is fit for real-world NLP tasks.", "labels": [], "entities": []}, {"text": "In(b), a RUM-based seq2seq model produces a better summary than what a standard LSTM-based seq2seq model yields.", "labels": [], "entities": []}, {"text": "In this particular example, LSTM falls into the wellknown trap of repeating information close to the end, whereas RUM avoids it.", "labels": [], "entities": []}, {"text": "Thus, RUM can be seen as a more ''well-rounded'' alternative to LSTM.", "labels": [], "entities": [{"text": "RUM", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.5422674417495728}, {"text": "LSTM", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7989229559898376}]}, {"text": "Given the example from, we ask the following questions: Does the long-term memory's 0 1 2 3 4 5 6 7 8 9 10 11 12 ? @ 1", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe two kinds of experiments based (i) on synthetic and (ii) on real-world tasks.", "labels": [], "entities": []}, {"text": "The former test the representational power of RUMs vs. LSTMs/GRUs, and the latter test whether RUMs also perform well for real-world NLP problems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Associative recall results. T is the input  length. Note that line 8 still learns the task completely  for T = 50, but it needs more than 100k training  steps. Moreover, varying the activations or removing  the update gate does not change the result in the last  line.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.7884707450866699}, {"text": "T", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9875200986862183}]}, {"text": " Table 2: Question answering results. Accuracy aver- aged over the 20 bAbI tasks. Using tanh is worse than  ReLU (line 13 vs. 15). RUM 150 \u03bb = 0 without an  update gate drops by 1.7% compared with line 13.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9030539691448212}, {"text": "Accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.997348427772522}, {"text": "ReLU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9252767562866211}, {"text": "RUM 150 \u03bb", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9609912832578024}]}, {"text": " Table 3: Character-level language modeling results.  BPC score on the PTB test split. Using tanh is slightly  better than ReLU (lines 2-3). Removing the update  gate in line 1 is worse than line 2. Phase-inspired reg- ularization may improve lines 1-3, 6-8, 9-10, and 16.", "labels": [], "entities": [{"text": "Character-level language modeling", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.7147018512090048}, {"text": "BPC", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9960224032402039}, {"text": "PTB test split", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.9789978265762329}, {"text": "ReLU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9319076538085938}]}, {"text": " Table 4: Text summarization results. Shown are  ROUGE F-{1,2,L} scores on the test split for the  CNN / Daily Mail and the Science Daily datasets.  Some settings are different from ours: lines 8-9 show  results when training and testing on an anonymized  data set, and lines 12-14 use reinforcement learning.  The ROUGE scores have a 95% confidence interval  ranging within \u00b10.25 points absolute. For lines 2  and 7, the maximum decoder steps during testing is  100. In lines 15-18, L/dR stands for LEAD/decRUM.  Replacing ReLU with tanh or removing the update  gate in decRUM line 17 yields a drop in ROUGE  of 0.01/0.09/0.25 and 0.36/0.39/0.42 points absolute,  respectively.", "labels": [], "entities": [{"text": "ROUGE F-{1,2,L", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.8176879286766052}, {"text": "CNN / Daily Mail and the Science Daily datasets", "start_pos": 99, "end_pos": 146, "type": "DATASET", "confidence": 0.8123152587148879}, {"text": "LEAD", "start_pos": 500, "end_pos": 504, "type": "METRIC", "confidence": 0.9643116593360901}, {"text": "ReLU", "start_pos": 524, "end_pos": 528, "type": "METRIC", "confidence": 0.9242213368415833}, {"text": "ROUGE", "start_pos": 603, "end_pos": 608, "type": "METRIC", "confidence": 0.9743521809577942}]}, {"text": " Table 5: RUM modeling ingredients: Tasks (A-E).", "labels": [], "entities": [{"text": "RUM modeling", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9553880095481873}]}]}