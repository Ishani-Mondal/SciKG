{"title": [{"text": "Calculating the Optimal Step in Shift-Reduce Dependency Parsing: From Cubic to Linear Time", "labels": [], "entities": [{"text": "Shift-Reduce Dependency Parsing", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.6587133407592773}]}], "abstractContent": [{"text": "We present anew cubic-time algorithm to calculate the optimal next step in shift-reduce dependency parsing, relative to ground truth, commonly referred to as dynamic oracle.", "labels": [], "entities": [{"text": "shift-reduce dependency parsing", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.5891355872154236}]}, {"text": "Unlike existing algorithms, it is applicable if the training corpus contains non-projective structures.", "labels": [], "entities": []}, {"text": "We then show that fora projective training corpus, the time complexity can be improved from cubic to linear.", "labels": [], "entities": []}], "introductionContent": [{"text": "A deterministic parser may rely on a classifier that predicts the next step, given features extracted from the present configuration).", "labels": [], "entities": []}, {"text": "It was found that accuracy improves if the classifier is trained not just on configurations that correspond to the ground-truth, or ''gold'', tree, but also on configurations that a parser would typically reach when a classifier strays from the optimal predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9991258978843689}]}, {"text": "This is known as a dynamic oracle.", "labels": [], "entities": []}, {"text": "The effective calculation of the optimal step for some kinds of parsing relies on 'arc-decomposibility', as in the case of.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9680405855178833}]}, {"text": "This generally requires a projective training corpus; an attempt to extend this to non-projective training corpora had to resort to an approximation (.", "labels": [], "entities": []}, {"text": "It is known how to calculate the optimal step fora number of non-1 A term we avoid here, as dynamic oracles are neither oracles nor dynamic, especially in our formulation, which allows gold trees to be non-projective.", "labels": [], "entities": []}, {"text": "Following, for example,, an oracle informs a parser whether a step may lead to the correct parse.", "labels": [], "entities": []}, {"text": "If the gold tree is non-projective and the parsing strategy only allows projective trees, then there are no steps that lead to the correct parse.", "labels": [], "entities": []}, {"text": "At best, there is an optimal step, by some definition of optimality.", "labels": [], "entities": []}, {"text": "An algorithm to compute the optimal step, fora given configuration, would typically not changeover time, and therefore is not dynamic in any generally accepted sense of the word.", "labels": [], "entities": []}, {"text": "projective parsing algorithms, however); see also.", "labels": [], "entities": [{"text": "projective parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6276365518569946}]}, {"text": "Ordinary shift-reduce dependency parsing is known at least since; see also.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6258657723665237}]}, {"text": "calls it ''arc-standard parsing.'' For shift-reduce dependency parsing, calculation of the optimal step is regarded to be difficult.", "labels": [], "entities": [{"text": "shift-reduce dependency parsing", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.6053928633530935}]}, {"text": "The best known algorithm is cubic and is only applicable if the training corpus is projective ().", "labels": [], "entities": []}, {"text": "We present anew cubic-time algorithm that is also applicable to non-projective training corpora.", "labels": [], "entities": []}, {"text": "Moreover, its architecture is modular, expressible as a generic tabular algorithm for dependency parsing plus a context-free grammar that expresses the allowable transitions of the parsing strategy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.821616142988205}]}, {"text": "This differs from approaches that require specialized tabular algorithms for different kinds of parsing ().", "labels": [], "entities": []}, {"text": "The generic tabular algorithm is interesting in its own right, and can be used to determine the optimal projectivization of a non-projective tree.", "labels": [], "entities": []}, {"text": "This is not to be confused with pseudo-projectivization (), which generally has a different architecture and is used fora different purpose, namely, to allow a projective parser to produce non-projective structures, by encoding non-projectivity into projective structures before training, and then reconstructing potential non-projectivity after parsing.", "labels": [], "entities": []}, {"text": "A presentational difference with earlier work is that we do not define optimality in terms of ''loss'' or ''cost'' functions but directly in terms of attainable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9788955450057983}]}, {"text": "This perspective is shared by, who also relate accuracies of competing steps, albeit by means of actual parser output and not in terms of best attainable accuracies.", "labels": [], "entities": []}, {"text": "We further show that if the training corpus is projective, then the time complexity can be reduced to linear.", "labels": [], "entities": []}, {"text": "To achieve this, we develop anew approach of excluding computations whose accuracies are guaranteed not to exceed the accuracies of the remaining computations.", "labels": [], "entities": []}, {"text": "The main theoretical conclusion is that arc-decomposibility is not a necessary requirement for efficient calculation of the optimal step.", "labels": [], "entities": []}, {"text": "Despite advances in unrestricted non-projective parsing, as, for example,, many state-ofthe-art dependency parsers are projective, as, for example,.", "labels": [], "entities": []}, {"text": "One main practical contribution of the current paper is that it introduces new ways to train projective parsers using non-projective trees, thereby enlarging the portion of trees from a corpus that is available for training.", "labels": [], "entities": []}, {"text": "This can be done either after applying optimal projectivization, or by computing optimal steps directly for non-projective trees.", "labels": [], "entities": []}, {"text": "This can be expected to lead to more accurate parsers, especially if a training corpus is small and a large proportion of it is non-projective.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were run on a laptop with an Intel i7-7500U processor (4 cores, 2.70 GHz) with 8 GB of RAM.", "labels": [], "entities": []}, {"text": "The implementation language is Java, with DL4J 2 for the classifier, realized as a neural network with a single layer of 256 hidden nodes.", "labels": [], "entities": []}, {"text": "Training is with batch size 100, and 20 epochs.", "labels": [], "entities": []}, {"text": "Features are the (gold) parts of speech and length-100 word2vec representations of the word forms of the top-most three stack elements, as well as of the left-most three elements of the remaining input, and the left-most and right-most dependency relations in the top-most two stack elements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. By step  we mean the application of a transition on a  particular configuration. By computation we mean  a series of steps, the formal notation of which  uses  *  , the reflexive, transition closure of . If  (0, 1 2 \u00b7 \u00b7 \u00b7 n, \u2205)  *  (0, \u03b5, T ), then T represents a  tree, with 0 as root element, and T is projective,  which means that for each node, the set of its  descendants (including that node itself) is of the  form {a, a + 1, . . . , a \u2212 1, a }, for some a and a .  In general, a dependency tree is any tree of nodes  labelled 0, 1, . . . , n, with 0 being the root.", "labels": [], "entities": []}, {"text": " Table 6: Accuracy (LAS or UAS, which here  are identical) of pseudo-projectivization and of  optimal projectivization.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971837401390076}, {"text": "LAS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8532761931419373}, {"text": "UAS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.7898435592651367}]}, {"text": " Table 7: Accuracies, with percentage of trees that  are non-projective, and number of tokens. Only  gold computations are considered in a single pass", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9967844486236572}]}]}