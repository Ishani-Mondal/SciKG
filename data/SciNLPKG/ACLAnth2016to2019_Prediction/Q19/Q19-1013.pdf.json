{"title": [{"text": "Categorical Metadata Representation for Customized Text Classification", "labels": [], "entities": [{"text": "Categorical Metadata Representation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6723048090934753}, {"text": "Customized Text Classification", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.707780639330546}]}], "abstractContent": [{"text": "Several tasks in argumentation mining and debating, question-answering, and natural language inference involve classifying a sequence in the context of another sequence (referred as bi-sequence classification).", "labels": [], "entities": [{"text": "argumentation mining and debating", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.7391826063394547}, {"text": "natural language inference", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.6581448515256246}, {"text": "bi-sequence classification", "start_pos": 182, "end_pos": 208, "type": "TASK", "confidence": 0.6559563726186752}]}, {"text": "For several single sequence classification tasks, the current state-of-the-art approaches are based on recurrent and convolutional neural networks.", "labels": [], "entities": [{"text": "single sequence classification tasks", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.7117304503917694}]}, {"text": "On the other hand, for bi-sequence classification problems, there is not much understanding as to the best deep learning architecture.: Example texts from the AAPR data set (upper) and Political Media data set (lower) with a variable category label (research field and political bias) that changes the classification label.", "labels": [], "entities": [{"text": "AAPR data set", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.9649254480997721}, {"text": "Political Media data set", "start_pos": 185, "end_pos": 209, "type": "DATASET", "confidence": 0.7098073959350586}]}, {"text": "We finally examine the performance of our models when data contain cold-start entities (i.e., users/products may have zero or very few reviews) using the Sparse80, subset of the Yelp 2013 data set provided in.", "labels": [], "entities": [{"text": "Sparse80", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.883305013179779}, {"text": "Yelp 2013 data set", "start_pos": 178, "end_pos": 196, "type": "DATASET", "confidence": 0.9834258854389191}]}, {"text": "We compare our models with three competing models: NSC (, which uses a hierarchical LSTM encoder coupled with customization on the attention mechanism, BiLSTM+CSAA (), which uses a BiLSTM encoder with customization on a CSAA mechanism, and HCSC (), which is", "labels": [], "entities": []}], "introductionContent": [{"text": "Text classification is the backbone of most NLP tasks: review classification in sentiment analysis), paper classification in scientific data discovery, and question classification in question answering (), to name a few.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7435478419065475}, {"text": "review classification in sentiment analysis", "start_pos": 55, "end_pos": 98, "type": "TASK", "confidence": 0.7196430206298828}, {"text": "paper classification in scientific data discovery", "start_pos": 101, "end_pos": 150, "type": "TASK", "confidence": 0.6202323486407598}, {"text": "question classification in question answering", "start_pos": 156, "end_pos": 201, "type": "TASK", "confidence": 0.8171366810798645}]}, {"text": "While prior methods require intensive feature engineering, recent methods enjoy automatic extraction of features from text using neural-based models) by encoding texts into low-dimensional dense feature vectors.", "labels": [], "entities": []}, {"text": "This paper discusses customized text classification, generalized from personalized text classification (), where we customize classifiers based on possibly multiple different known categorical metadata information (e.g., user/product information for sentiment classification) instead of just the user information.", "labels": [], "entities": [{"text": "customized text classification", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6606508592764536}, {"text": "personalized text classification", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.6345243056615194}, {"text": "sentiment classification)", "start_pos": 250, "end_pos": 275, "type": "TASK", "confidence": 0.8202689091364542}]}, {"text": "As shown in, in addition to the text, a customizable text classifier is given a list of categories specific to the text to predict its class.", "labels": [], "entities": []}, {"text": "Existing works applied metadata information to improve the performance of a model, such as user and product) information in sentiment classification, and author) and publication () information in paper classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 124, "end_pos": 148, "type": "TASK", "confidence": 0.8239060640335083}, {"text": "paper classification", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.7092250287532806}]}, {"text": "Towards our goal, we are inspired by the advancement in neural-based models, incorporating categorical information ''as is'' and injecting it on various parts of the model such as in the word embeddings (, attention mechanism () and memory networks.", "labels": [], "entities": []}, {"text": "Indeed, these methods theoretically make use of combined features from both textual and categorical features, which make them more powerful than disconnected features.", "labels": [], "entities": []}, {"text": "However, metadata is generated for human understanding, and thus we claim that these categories need to be carefully represented for machine use to Figure 1: A high-level framework of models for the Customized Text Classification Task that inputs a text with n tokens (e.g., review) and m categories (e.g., users, products) and outputs a class (e.g., positive/negative).", "labels": [], "entities": [{"text": "Customized Text Classification Task", "start_pos": 199, "end_pos": 234, "type": "TASK", "confidence": 0.7494976297020912}]}, {"text": "Example tasks are shown in the left of the improve the performance of the text classifier effectively.", "labels": [], "entities": []}, {"text": "First, we empirically invalidate the results from previous studies by showing in our experiments on multiple data sets that popular methods using metadata categories ''as is'' perform worse than a simple concatenation of textual and categorical feature vectors.", "labels": [], "entities": []}, {"text": "We argue that this is because of the difficulties of the model in learning optimized dense vector representation of the categorical features to be used by the classification model.", "labels": [], "entities": []}, {"text": "The reasons are two-fold: (a) categorical features do not have direct context and thus rely solely on classification labels when training the feature vectors, and (b) there are categorical information that are sparse and thus cannot effectively learn optimal feature vectors.", "labels": [], "entities": []}, {"text": "Second, we suggest an alternative representation, using low-dimensional basis vectors to mitigate the optimization problems of categorical feature vectors.", "labels": [], "entities": []}, {"text": "Basis vectors have nice properties that can solve the issues presented here because they (a) transform multiple categories into useful combinations, which serve as mutual context to all categories, and (b) intelligently initialize vectors, especially of sparse categorical information, to a suboptimal location to efficiently train them further.", "labels": [], "entities": []}, {"text": "Furthermore, our method reduces the number of trainable parameters and thus is flexible for any kinds and any number of available categories.", "labels": [], "entities": []}, {"text": "We experiment on multiple classification tasks with different properties and kinds of categories available.", "labels": [], "entities": []}, {"text": "Our experiments show that while customization methods using categorical information ''as is'' do not perform as well as the naive concatenation method, applying our proposed basis-customization method makes them much more effective than the naive method.", "labels": [], "entities": []}, {"text": "Our method also enables the use of categorical metadata to customize other parts of the model, such as the encoder weights, that are previously unexplored due to their high space complexity and weak performance.", "labels": [], "entities": []}, {"text": "We show that this unexplored use of customization outperform popular and conventional methods such as attention mechanism when our proposed basis-customization method is used.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment on three data sets for different tasks: \u2022 audience \u2022 political bias The data set has more categories.", "labels": [], "entities": []}, {"text": "Categories with binary labels may not be diverse enough to be useful.", "labels": [], "entities": []}, {"text": "2012) with l 2 constraint of 3.", "labels": [], "entities": []}, {"text": "We do early stopping using the accuracy of the development set.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7667812705039978}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9991151690483093}]}, {"text": "We perform 10-fold cross-validation on the training set when the development set is not available.", "labels": [], "entities": []}, {"text": "Data set-specific settings are described in their corresponding sections.", "labels": [], "entities": []}, {"text": "We compare the performance of the following competing models: the base classifier BiLSTM with no customization, the five versions (i.e., bias, linear, attention, encoder, embedding) of Customized BiLSTM, and our proposed basiscustomized versions.", "labels": [], "entities": []}, {"text": "We report the accuracy and the number of parameters of all models, and additionally report the root mean square error (RMSE) values for the sentiment classification task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996286630630493}, {"text": "root mean square error (RMSE)", "start_pos": 95, "end_pos": 124, "type": "METRIC", "confidence": 0.8389124189104352}, {"text": "sentiment classification task", "start_pos": 140, "end_pos": 169, "type": "TASK", "confidence": 0.9396895964940389}]}, {"text": "We also compare with results from previous papers whenever available.", "labels": [], "entities": []}, {"text": "Results are shown in, and further discussion is provided the following sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The data sets, the split sizes (train, dev, test), and the available categories and their properties. Numbers  inside parentheses are the number of unique category labels.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy, RMSE, and parameter values of competing models for all data sets. An asterisk (*) indicates  customization methods first introduced in this paper. A dash (-) indicates the model is too big to be trained in  an NVIDIA 1080 Ti GPU. Boldface indicates that the performance of basis-customization is significantly better  (p < 0.05) than that of a simple customization. Values colored red are performance weaker than that of the  BiLSTM model, thus customization hurts the performance in those cases.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9935000538825989}, {"text": "RMSE", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.731679379940033}]}, {"text": " Table 3: Performance comparison of previous and our best models in the Yelp 2013 dataset. Our best models  perform better, even though we only use a single BiLSTM encoder. Boldface correspond to the best values for  each block. Underlined values correspond to the best values across the board.", "labels": [], "entities": [{"text": "Yelp 2013 dataset", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9822291930516561}]}, {"text": " Table 4: Performance comparison of models using  full texts and our implemented models using paper  abstracts (and authors and research areas as categories  for basis-customized models) as inputs in the AAPR  data set.", "labels": [], "entities": [{"text": "AAPR  data set", "start_pos": 204, "end_pos": 218, "type": "DATASET", "confidence": 0.9534796476364136}]}]}