{"title": [], "abstractContent": [{"text": "Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.8531213303407034}]}, {"text": "However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs.", "labels": [], "entities": []}, {"text": "In this paper, we introduce asynchronous bidirectional-neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time.", "labels": [], "entities": [{"text": "bidirectional-neural machine translation", "start_pos": 41, "end_pos": 81, "type": "TASK", "confidence": 0.741945207118988}]}, {"text": "Specifically, we first propose anew algorithm that enables synchronous bidirectional decoding in a single model.", "labels": [], "entities": []}, {"text": "Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding.", "labels": [], "entities": []}, {"text": "We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese-English, WMT14 English-German, and WMT18 Russian-English translation tasks.", "labels": [], "entities": [{"text": "NIST Chinese-English", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.8627174198627472}, {"text": "WMT14", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.8706766366958618}, {"text": "WMT18 Russian-English translation tasks", "start_pos": 113, "end_pos": 152, "type": "TASK", "confidence": 0.7989017218351364}]}, {"text": "Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively , and obtains the state-of-the-art per-* Corresponding author.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.998887836933136}]}, {"text": "formance on Chinese-English and English-German translation tasks.", "labels": [], "entities": [{"text": "English-German translation tasks", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7085662682851156}]}], "introductionContent": [{"text": "Neural machine translation has significantly improved the quality of machine translation in recent years (.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6278035342693329}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7395814061164856}]}, {"text": "Recent approaches to sequence-to-sequence learning typically leverage recurrence (), convolution, or attention ( as basic building blocks.", "labels": [], "entities": []}, {"text": "Typically, NMT adopts the encoder-decoder architecture and generates the target translation from left to right.", "labels": [], "entities": []}, {"text": "Despite their remarkable success, NMT models suffer from several weaknesses (.", "labels": [], "entities": []}, {"text": "One of the most prominent issues is the problem of unbalanced outputs in which the translation prefixes are better predicted than the suffixes ( . We analyze translation accuracy of the first and last 4 tokens for left-to-right (L2R) and right-toleft (R2L) directions, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9240982532501221}]}, {"text": "As shown in, the statistical results show that L2R performs better in the first 4 tokens, whereas R2L translates better in terms of the last 4 tokens.", "labels": [], "entities": []}, {"text": "This problem is mainly caused by the left-toright unidirectional decoding, which conditions each output word on previously generated outputs only, but leaving the future information from target-side contexts unexploited during translation.", "labels": [], "entities": []}, {"text": "The future context is commonly used in reading and writing inhuman cognitive process", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed model on three translation datasets with different sizes, including NIST Chinese-English, WMT14 English-German, and WMT18 Russian-English translations.", "labels": [], "entities": [{"text": "NIST Chinese-English", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.9093804955482483}, {"text": "WMT14", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.8962149024009705}, {"text": "WMT18", "start_pos": 141, "end_pos": 146, "type": "DATASET", "confidence": 0.9191790223121643}]}, {"text": "For Chinese-English, our training data includes about 2.0 million sentence pairs extracted from the LDC corpus.", "labels": [], "entities": [{"text": "LDC corpus", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.9111714959144592}]}, {"text": "We use the NIST 2002 (MT02) Chinese-English dataset as the validation set and NIST (MT03-06) as our test sets.", "labels": [], "entities": [{"text": "NIST 2002 (MT02) Chinese-English dataset", "start_pos": 11, "end_pos": 51, "type": "DATASET", "confidence": 0.9321691819599697}, {"text": "NIST (MT03-06)", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.8607728779315948}]}, {"text": "We use BPE () to encode Chinese and English, respectively.", "labels": [], "entities": [{"text": "BPE", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.925491452217102}]}, {"text": "We learn 30K merge operations and limit the source and target vocabularies to the most frequent 30K tokens.", "labels": [], "entities": []}, {"text": "For English-German translation, the training set consists of about 4.5 million bilingual sentence pairs from WMT 2014.", "labels": [], "entities": [{"text": "WMT 2014", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.930433601140976}]}, {"text": "We use newstest2013 as the validation set and newstest2014 as the test set.", "labels": [], "entities": []}, {"text": "Sentences are encoded using BPE, which has a shared vocabulary of about 37,000 tokens.", "labels": [], "entities": [{"text": "BPE", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.48352864384651184}]}, {"text": "To evaluate the models, we compute the BLEU metric () on tokenized, truecase output.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9820740222930908}]}, {"text": "For Russian-English translation, we use the following resources from the WMT parallel data 7 : ParaCrawl corpus, Common Crawl corpus, News Commentary v13, and Yandex Corpus.", "labels": [], "entities": [{"text": "Russian-English translation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.588985025882721}, {"text": "WMT parallel data 7", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.909960150718689}, {"text": "ParaCrawl corpus", "start_pos": 95, "end_pos": 111, "type": "DATASET", "confidence": 0.8346840441226959}, {"text": "Common Crawl corpus", "start_pos": 113, "end_pos": 132, "type": "DATASET", "confidence": 0.8644681572914124}, {"text": "Yandex Corpus", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.9401205778121948}]}, {"text": "We do not use Wiki Headlines and UN Parallel Corpus V1.0.", "labels": [], "entities": [{"text": "Wiki Headlines", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.8834065794944763}, {"text": "UN Parallel Corpus V1.0", "start_pos": 33, "end_pos": 56, "type": "DATASET", "confidence": 0.8412015438079834}]}, {"text": "The training corpus consists of 14M sentence pairs.", "labels": [], "entities": []}, {"text": "We employ the Moses Tokenizer 8 for preprocessing.", "labels": [], "entities": []}, {"text": "For subword segmentation, we use 50,000 joint BPE operations and choose the most frequent 52,000 tokens as vocabularies.", "labels": [], "entities": [{"text": "subword segmentation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.8149794936180115}]}, {"text": "We use newstest2017 as the development set and the newtest2018 as the test set.", "labels": [], "entities": []}, {"text": "The corpora includes LDC2000T50, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, and LDC2004T07.", "labels": [], "entities": []}, {"text": "Following previous work, we also use case-insensitive tokenized BLEU to evaluate ChineseEnglish which have been segmented by Stanford word segmentation and Moses Tokenizer, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9938791990280151}, {"text": "ChineseEnglish", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.9158884882926941}, {"text": "Stanford word segmentation", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.5699427127838135}]}, {"text": "5 http://www.statmt.org/wmt14/translation-task.html.", "labels": [], "entities": []}, {"text": "All preprocessed datasets and vocab can be directly download in tensor2tensor website https://drive.google.com/ open?id=0B_bZck-ksdkpM25jRUN2X2UxMm8.", "labels": [], "entities": []}, {"text": "This procedure is used in the literature to which we compare ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experiment results on the development set  using different fusion mechanism with different \u03bbs.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of translation quality for Chinese-English translation tasks using case-insensitive BLEU  scores. All results of our model are significantly better than Transformer and Transformer (R2L) (p < 0.01).", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.7247995932896932}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9889987111091614}]}, {"text": " Table 4: Results of WMT14 English-German transla- tion using case-sensitive BLEU. Results with  \u2021 mark  are taken from the corresponding papers.", "labels": [], "entities": [{"text": "WMT14 English-German transla- tion", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.6849865674972534}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9688482880592346}]}, {"text": " Table 5: Results of WMT18 Russian-English transla- tion using case-insensitive tokenized BLEU.", "labels": [], "entities": [{"text": "WMT18 Russian-English transla- tion", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.6242417395114899}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9829714894294739}]}, {"text": " Table 6: Statistics of parameters, training, and testing  speeds. Train denotes the number of global training  steps processed per second at the same batch-size  sentences; Test indicates the amount of translated  sentences in 1 second.", "labels": [], "entities": [{"text": "Train", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9833511710166931}]}]}