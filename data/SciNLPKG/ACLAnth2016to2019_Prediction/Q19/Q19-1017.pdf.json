{"title": [{"text": "What You Say and How You Say it: Joint Modeling of Topics and Discourse in Microblog Conversations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an unsupervised framework for jointly modeling topic content and discourse behavior in microblog conversations.", "labels": [], "entities": []}, {"text": "Concretely, we propose a neural model to discover word clusters indicating what a conversation concerns (i.e., topics) and those reflecting how participants voice their opinions (i.e., discourse).", "labels": [], "entities": []}, {"text": "1 Extensive experiments show that our model can yield both coherent topics and meaningful discourse behavior.", "labels": [], "entities": []}, {"text": "Further study shows that our topic and discourse representations can benefit the classification of microblog messages, especially when they are jointly trained with the classifier.", "labels": [], "entities": [{"text": "classification of microblog messages", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.8092086017131805}]}], "introductionContent": [{"text": "The last decade has witnessed the revolution of communication, where the ''kitchen table conversations'' have been expanded to public discussions on online platforms.", "labels": [], "entities": []}, {"text": "As a consequence, in our daily life, the exposure to new information and the exchange of personal opinions have been mediated through microblogs, one popular online platform genre (.", "labels": [], "entities": []}, {"text": "The flourish of microblogs has also led to the sheer quantity of user-created conversations emerging everyday, exposing individuals to superfluous information.", "labels": [], "entities": []}, {"text": "Facing such an unprecedented number of conversations relative to limited attention of individuals, how shall we automatically extract the critical points and make sense of these microblog conversations?", "labels": [], "entities": []}, {"text": "* This work was partially conducted in Jichuan Zeng's internship in Tencent AI Lab.", "labels": [], "entities": []}, {"text": "Corresponding author: Jing Li.", "labels": [], "entities": []}, {"text": "Our data sets and code are available at: http:// github.com/zengjichuan/Topic_Disc.", "labels": [], "entities": []}, {"text": "Toward key focus understanding of a conversation, previous work has shown the benefits of discourse structure (, which shapes how messages interact with each other, forming the discussion flow, and can usefully reflect salient topics raised in the discussion process.", "labels": [], "entities": []}, {"text": "After all, the topical content of a message naturally occurs in context of the conversation discourse and hence should not be modeled in isolation.", "labels": [], "entities": []}, {"text": "Conversely, the extracted topics can reveal the purpose of participants and further facilitate the understanding of their discourse behavior.", "labels": [], "entities": []}, {"text": "Further, the joint effects of topics and discourse will contribute to better understanding of social media conversations, benefiting downstream tasks such as the management of discussion topics and discourse behavior of social chatbots ( and the prediction of user engagements for conversation recommendation (.", "labels": [], "entities": []}, {"text": "To illustrate how the topics and discourse interplay in a conversation, displays a snippet of Twitter conversation.", "labels": [], "entities": []}, {"text": "As can be seen, the content words reflecting the discussion topics (such as ''supreme court'' and ''gun rights'') appear in context of the discourse flow, where participants carry the conversation forward via making a statement, giving a comment, asking a question, and so forth.", "labels": [], "entities": []}, {"text": "Motivated by such an observation, we assume that a microblog conversation can be decomposed into two crucially different components: one for topical content and the other for discourse behavior.", "labels": [], "entities": []}, {"text": "Here, the topic components indicate what a conversation is centered around and reflect the important discussion points put forward in the conversation process.", "labels": [], "entities": []}, {"text": "The discourse components signal the discourse roles of messages, such as making a statement, asking a question, and other dialogue acts (, which further shape the discourse structure of a conversation.", "labels": [], "entities": []}, {"text": "To distinguish the above two components, we examine the conversation contexts and identify two types of words: topic words, indicating what a conversation focuses on, and discourse words, reflecting how the opinion is voiced in each message.", "labels": [], "entities": []}, {"text": "For example, in, the topic words ''gun'' and ''control'' indicate the conversation topic while the discourse word ''what'' and ''?'' signal the question in M 3 . Concretely, we propose a neural framework built upon topic models, enabling the joint exploration of word clusters to represent topic and discourse in microblog conversations.", "labels": [], "entities": []}, {"text": "Different from the prior models trained on annotated data (, our model is fully unsupervised, not dependent on annotations for either topics or discourse, which ensures its immediate applicability in any domain or language.", "labels": [], "entities": []}, {"text": "Moreover, taking advantages of the recent advances in neural topic models, we are able to approximate Bayesian variational inference without requiring model-specific derivations, whereas most existing work () require expertise involved to customize model inference algorithms.", "labels": [], "entities": []}, {"text": "In addition, our neural nature enables In this paper, the discourse role refers to a certain type of dialogue act (e.g., statement or question) for each message.", "labels": [], "entities": []}, {"text": "And the discourse structure refers to some combination of discourse roles in a conversation.", "labels": [], "entities": []}, {"text": "end-to-end training of topic and discourse representation learning with other neural models for diverse tasks.", "labels": [], "entities": [{"text": "topic and discourse representation learning", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.6826039969921112}]}, {"text": "For model evaluation, we conduct an extensive empirical study on two large-scale Twitter data sets.", "labels": [], "entities": [{"text": "model evaluation", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7098757028579712}, {"text": "Twitter data sets", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.8154579599698385}]}, {"text": "The intrinsic results show that our model can produce latent topics and discourse roles with better interpretability than the state-of-theart models from previous studies.", "labels": [], "entities": []}, {"text": "The extrinsic evaluations on a tweet classification task exhibit the model's ability to capture useful representations for microblog messages.", "labels": [], "entities": [{"text": "tweet classification task", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.8058735926946005}]}, {"text": "Particularly, our model enables an easy combination with existing neural models for end-to-end training, such as convolutional neural networks, which is shown to perform better in classification than the pipeline approach without joint training.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we collected two microblog conversation data sets from Twitter.", "labels": [], "entities": []}, {"text": "One is released by the TREC 2011 microblog track (henceforth TREC), containing conversations concerning a wide range of topics.", "labels": [], "entities": [{"text": "TREC 2011 microblog track (henceforth TREC)", "start_pos": 23, "end_pos": 66, "type": "DATASET", "confidence": 0.8112628199160099}]}, {"text": "5 The distributions in Equation 4 are all conditional probability distributions given the target message x and its conversation c.", "labels": [], "entities": []}, {"text": "We omit the conditions for simplicity.", "labels": [], "entities": []}, {"text": "To smooth the gradients in implementation, for z \u223c N (\u00b5, \u03c3), we apply the reparameterization on z (, and ford \u223c M ulti(\u03c0), we adopt the Gumbel-Softmax trick ( Data Preprocessing.", "labels": [], "entities": []}, {"text": "We preprocessed the data with the following steps.", "labels": [], "entities": []}, {"text": "First, non-English tweets were filtered out.", "labels": [], "entities": []}, {"text": "Then, hashtags, mentions (@username), and links were replaced with generic tags ''HASH'', ''MENT'', and ''URL'', respectively.", "labels": [], "entities": [{"text": "HASH", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.976608395576477}, {"text": "MENT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8982399106025696}]}, {"text": "Next, the natural languge toolkit was applied for tweet tokenization.", "labels": [], "entities": []}, {"text": "8 After that, all letters were normalized to lower cases.", "labels": [], "entities": []}, {"text": "Finally, words that occurred fewer than 20 times were filtered out from the data.", "labels": [], "entities": []}, {"text": "To ensure comparable results with (the prior work focusing on the same task as ours), in the topic coherence evaluation, we follow their setup to report the results under two sets of K (the number of topics): K = 50 and K = 100, and with the number of discourse roles (D) set to 10.", "labels": [], "entities": []}, {"text": "The analysis for the effects of K and D will be further presented in Section 5.5.", "labels": [], "entities": []}, {"text": "For all the other hyperparameters, we tuned them on development set by grid search.", "labels": [], "entities": []}, {"text": "The trade-off parameter \u03bb (defined in Equation 6), balancing the MI loss and the other objective functions, is set to 0.01.", "labels": [], "entities": [{"text": "trade-off parameter \u03bb", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.746669073899587}, {"text": "MI loss", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9766001403331757}]}, {"text": "In model training, we use the Adam optimizer) and run 100 epochs with early stop strategy adopted.", "labels": [], "entities": []}, {"text": "In topic modeling experiments, we consider the five topic model baselines treating each tweet as a document: LDA (), BTM (), LF-LDA, LF-DMM (, and NTM ().", "labels": [], "entities": []}, {"text": "In particular, BTM and LF-DMM are the state-of-the-art topic models for short texts.", "labels": [], "entities": []}, {"text": "BTM explores the topics of all word pairs (biterms) in each message to alleviate data sparsity in short texts.", "labels": [], "entities": [{"text": "BTM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.701331615447998}]}, {"text": "LF-DMM incorporates word embeddings pre-trained on external data to expand semantic meanings of words, so does LF-LDA.", "labels": [], "entities": [{"text": "LF-DMM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8774817585945129}]}, {"text": "In, LF-DMM, based on one-topic-perdocument Dirichlet Multinomial Mixture (DMM) (), was reported to perform better than LF-LDA, based on LDA.", "labels": [], "entities": []}, {"text": "For LF-LDA and LF-DMM, we use GloVe Twitter embeddings () as the pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "For the discourse modeling experiments, we compare our results with LAED (), a VAE-based representation learning model for conversation discourse.", "labels": [], "entities": [{"text": "discourse modeling", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7132849991321564}, {"text": "LAED", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9487518668174744}]}, {"text": "In addition, for both topic and discourse evaluation, we compare with, a recently proposed model for microblog conversations, where topics and discourse are jointly explored with a non-neural framework.", "labels": [], "entities": []}, {"text": "Besides the existing models from previous studies, we also compare with the variants of our model that only models topics (henceforth TOPIC ONLY) or discourse (henceforth DISC ONLY).", "labels": [], "entities": [{"text": "TOPIC ONLY", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.7913713753223419}]}, {"text": "10 Our joint model of topics and discourse is referred to as TOPIC+DISC.", "labels": [], "entities": [{"text": "TOPIC", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.6928927898406982}]}, {"text": "In the preprocessing procedure for the baselines, we removed stop words and punctuation for topic models unable to learn discourse representations following the common practice in previous work).", "labels": [], "entities": []}, {"text": "For the other models, stop words and punctuation were retained in the vocabulary, considering their usefulness as discourse indicators (  In this section, we first report the topic coherence results in Section 5.1, followed by a discussion in Section 5.2 comparing the latent discourse roles discovered by our model with the manually annotated dialogue acts.", "labels": [], "entities": []}, {"text": "Then, we study whether we can capture useful representations for microblog messages in a tweet classification task (in Section 5.3).", "labels": [], "entities": [{"text": "tweet classification task", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.8092115918795267}]}, {"text": "A qualitative analysis, showing some example topics and discourse roles, is further provided in Section 5.4.", "labels": [], "entities": []}, {"text": "Finally, in Section 5.5, we provide more discussions on our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the two data sets containing  Twitter conversations.", "labels": [], "entities": []}, {"text": " Table 2: C v coherence scores for latent topics  produced by different models. The best result in  each column is highlighted in bold. Our joint  model TOPIC+DISC achieves significantly better  coherence scores than all the baselines (p < 0.01,  paired test).", "labels": [], "entities": []}, {"text": " Table 3: The purity, homogeneity, and variation  of information (VI) scores for the latent discourse  roles measured against the human-annotated dia- logue acts. For purity and homogeneity, higher  scores indicate better performance, while for VI  scores, lower is better. In each column, the best  results are in boldface. Our joint model TOPIC+DISC  significantly outperforms all the baselines (p <  0.01, paired t-test).", "labels": [], "entities": [{"text": "purity", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.991912841796875}, {"text": "purity", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9605520963668823}]}, {"text": " Table 4: Evaluation of tweet classification re- sults in accuracy (Acc) and average F1 (Avg F1).  Representations learned by various models serve  as the classification features. For our model, both  the topic and discourse representations are fed  into the classifier.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7153194695711136}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9995152950286865}, {"text": "Acc", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8777636289596558}, {"text": "F1 (Avg F1)", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.8402003526687623}]}, {"text": " Table 7: Accuracy (Acc) and average F1 (Avg F1)  on tweet classification (hashtags as labels). CNN  only: CNN without using our representations.  Seperate-Train: CNN fed with our pre-trained  representations. Joint-Train: Joint training CNN  and our model.", "labels": [], "entities": [{"text": "Accuracy (Acc)", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8208016455173492}, {"text": "F1 (Avg F1)", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9075519919395447}]}]}