{"title": [{"text": "Semantic Neural Machine Translation Using AMR", "labels": [], "entities": [{"text": "Semantic Neural Machine Translation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6890189871191978}, {"text": "AMR", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.5834513902664185}]}], "abstractContent": [{"text": "It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7950737178325653}, {"text": "meaning preservation", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.7529326677322388}]}, {"text": "On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT).", "labels": [], "entities": [{"text": "leveraging semantics for neural machine translation (NMT)", "start_pos": 48, "end_pos": 105, "type": "TASK", "confidence": 0.7964464426040649}]}, {"text": "In this work, we study the usefulness of AMR (abstract meaning representation) on NMT.", "labels": [], "entities": [{"text": "abstract meaning representation", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6234739224116007}]}, {"text": "Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model.", "labels": [], "entities": [{"text": "attention-based sequence-to-sequence neural translation", "start_pos": 139, "end_pos": 194, "type": "TASK", "confidence": 0.5849933102726936}]}], "introductionContent": [{"text": "It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8075387477874756}]}, {"text": "Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well.", "labels": [], "entities": [{"text": "symbolic machine translation", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.6709111134211222}]}, {"text": "Leveraging syntax for neural machine translation (NMT) has been an active research topic ().", "labels": [], "entities": [{"text": "Leveraging syntax for neural machine translation (NMT)", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7066994276311662}]}, {"text": "On the other hand, exploring semantics for NMT has so far received relatively little attention.", "labels": [], "entities": []}, {"text": "Recently, exploited semantic role labeling (SRL) for NMT, showing that the predicate-argument information from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ''argument switching'' problem, 1 one frequent and severe issue faced by NMT systems.(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.7637212773164114}]}, {"text": "Other important information, such as the relation between John and wife, cannot be incorporated.", "labels": [], "entities": []}, {"text": "In this paper, we explore the usefulness of abstract meaning representation (AMR) () as a semantic representation for NMT.", "labels": [], "entities": [{"text": "abstract meaning representation (AMR)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.7642829418182373}]}, {"text": "AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph.(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the concepts and edges (such as :ARG0 and :ARG1) represent the relations between concepts they connect.", "labels": [], "entities": []}, {"text": "Comparing with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines).", "labels": [], "entities": [{"text": "AMRs", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9788699150085449}]}, {"text": "In addition, AMRs directly capture entity relations and abstract away inflections and function words.", "labels": [], "entities": []}, {"text": "As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.740205705165863}]}, {"text": "Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training.", "labels": [], "entities": []}, {"text": "Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance; Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence.), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering, summarization (, and event detection (.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9779573976993561}, {"text": "question answering", "start_pos": 296, "end_pos": 314, "type": "TASK", "confidence": 0.86726513504982}, {"text": "summarization", "start_pos": 316, "end_pos": 329, "type": "TASK", "confidence": 0.9890841841697693}, {"text": "event detection", "start_pos": 337, "end_pos": 352, "type": "TASK", "confidence": 0.7330366969108582}]}, {"text": "However, to our knowledge, no existing work has exploited AMR for enhancing NMT.", "labels": [], "entities": [{"text": "AMR", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.6040894389152527}, {"text": "NMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.7562636733055115}]}, {"text": "We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to.", "labels": [], "entities": []}, {"text": "To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (  as the AMR encoder.", "labels": [], "entities": []}, {"text": "In particular, a full AMR graph is considered as a single state, with nodes in the graph being its substates.", "labels": [], "entities": []}, {"text": "State transitions are performed on the graph recurrently, allowing substates to exchange information through edges.", "labels": [], "entities": []}, {"text": "At each recurrent step, each node advances its current state by receiving information from the current states of its adjacent nodes.", "labels": [], "entities": []}, {"text": "Thus, with increasing numbers of recurrent steps, each word receives information from a larger context.", "labels": [], "entities": []}, {"text": "shows the recurrent transition, where each node works simultaneously.", "labels": [], "entities": []}, {"text": "Compared with other methods for encoding AMRs (, GRN keeps the original graph structure, and thus no information is lost ( . For the decoding stage, two separate attention mechanisms are adopted in the AMR encoder and sequential encoder, respectively.", "labels": [], "entities": [{"text": "GRN", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.7831897735595703}]}, {"text": "Experiments on WMT16 English-to-German data show that adopting AMR significantly improves a strong attention-based sequenceto-sequence baseline (25.5 vs 23.7 BLEU).", "labels": [], "entities": [{"text": "WMT16 English-to-German data", "start_pos": 15, "end_pos": 43, "type": "DATASET", "confidence": 0.9403191407521566}, {"text": "AMR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.526350200176239}, {"text": "attention-based sequenceto-sequence baseline", "start_pos": 99, "end_pos": 143, "type": "METRIC", "confidence": 0.7221720516681671}, {"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9976041913032532}]}, {"text": "When trained with small-scale (226K) data, the improvement increases (19.2 vs 16.0 BLEU), which shows that the structural information from AMR can alleviate data sparsity when training data are not sufficient.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9975937008857727}]}, {"text": "To our knowledge, we are the first to investigate AMR for NMT.", "labels": [], "entities": [{"text": "AMR", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.87095707654953}, {"text": "NMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8095484375953674}]}, {"text": "Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt.", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically investigate the effectiveness of AMR for English-to-German translation.", "labels": [], "entities": [{"text": "English-to-German translation", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.6425849497318268}]}], "tableCaptions": [{"text": " Table 1: Statistics of the dataset. Numbers of tokens  are after BPE processing.", "labels": [], "entities": [{"text": "BPE", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.5457469820976257}]}, {"text": " Table 2: Sizes of vocabularies. EN-ori represents  original English sentences without BPE.", "labels": [], "entities": [{"text": "BPE", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.7566161751747131}]}, {"text": " Table 3: TEST performance. NC-v11 represents training only with the NC-v11 data, while Full means using the  full training data. * represents significant (Koehn, 2004) result (p < 0.01) over Seq2seq. \u2193 indicates the lower  the better.", "labels": [], "entities": [{"text": "TEST", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6733753085136414}, {"text": "NC-v11 data", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9516107738018036}]}]}