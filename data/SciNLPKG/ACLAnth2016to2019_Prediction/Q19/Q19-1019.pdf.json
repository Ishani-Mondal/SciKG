{"title": [{"text": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.743857741355896}]}, {"text": "To capture structural information associated with graphs, we investigate the problem of encoding graphs using graph convolutional networks (GCNs).", "labels": [], "entities": []}, {"text": "Unlike various existing approaches where shallow architectures were used for capturing local structural information only, we introduce a dense connection strategy, proposing a novel Densely Connected Graph Convolutional Network (DCGCN).", "labels": [], "entities": []}, {"text": "Such a deep architecture is able to integrate both local and non-local features to learn a better structural representation of a graph.", "labels": [], "entities": []}, {"text": "Our model outperforms the state-of-the-art neural models significantly on AMR-to-text generation and syntax-based neural machine translation.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.9498656690120697}, {"text": "syntax-based neural machine translation", "start_pos": 101, "end_pos": 140, "type": "TASK", "confidence": 0.605832539498806}]}], "introductionContent": [{"text": "Graphs play an important role in natural language processing (NLP) as they are able to capture richer structural information than sequences and trees.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.8237596750259399}]}, {"text": "Generally, semantics of sentences can be encoded as graphs.", "labels": [], "entities": []}, {"text": "For example, the abstract meaning representation (AMR) () is a directed, labeled graph as shown in, where nodes in the graph denote semantic concepts and edges denote relations between concepts.", "labels": [], "entities": [{"text": "abstract meaning representation (AMR)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.706005205710729}]}, {"text": "Such graph representations can capture rich semanticlevel structural information, and are attractive representations useful for semantics-related tasks such as semantic parsing and natural language generation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.7163503617048264}, {"text": "natural language generation", "start_pos": 181, "end_pos": 208, "type": "TASK", "confidence": 0.6694942712783813}]}, {"text": "In this paper, we focus on the graph-to-sequence * Contributed equally.", "labels": [], "entities": []}, {"text": "learning tasks, where we aim to learn representations for graphs that are useful for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7421572506427765}]}, {"text": "Graph convolutional networks (GCNs)) are variants of convolutional neural networks (CNNs) that operate directly on graphs, where the representation of each node is iteratively updated based on those of its adjacent nodes in the graph through an information propagation scheme.", "labels": [], "entities": []}, {"text": "For example, the first layer of GCNs can only capture the graph's adjacency information between immediate neighbors, while with the second layer one will be able to capture second-order proximity information (neighborhood information two hops away from one node) as shown in.", "labels": [], "entities": []}, {"text": "Formally, L layers will be needed in order to capture neighborhood information that is L hops away.", "labels": [], "entities": []}, {"text": "GCNs have been successfully applied to many NLP tasks ().", "labels": [], "entities": []}, {"text": "Interestingly, although deeper GCNs with more layers will be able to capture richer neighborhood information of a graph, empirically it has been observed that the best performance is achieved with a 2-layer model ( . Therefore, recent efforts that leverage recurrencebased graph neural networks have been explored as the alternatives to encode the structural information of graphs.", "labels": [], "entities": []}, {"text": "Examples include graph-state long short-term memory (LSTM) networks () and gated graph neural networks (GGNNs).", "labels": [], "entities": []}, {"text": "Deep architectures based on such recurrence-based models have been successfully built for tasks such as language generation, where rich neighborhood information captured was shown useful.", "labels": [], "entities": [{"text": "language generation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7228214740753174}]}, {"text": "Compared with recurrent neural networks, convolutional architectures are highly parallelizable and are more amenable to hardware acceleration (.", "labels": [], "entities": []}, {"text": "It is therefore worthwhile to explore the possibility of applying deeper GCNs that are able to capture more non-local information associated with the graph for graphto-sequence learning.", "labels": [], "entities": []}, {"text": "Prior efforts have tried to train deep GCNs by incorporating residual connections (.", "labels": [], "entities": []}, {"text": "show that vanilla residual connections proposed by are not effective for graph neural networks.", "labels": [], "entities": []}, {"text": "They next attempt to resolve this issue by adding additional recurrent layers on top of graph convolutional layers.", "labels": [], "entities": []}, {"text": "However, they are still confined to relatively shallow GCNs architectures (at most 6 layers in their experiments), which may not be able to capture the rich nonlocal interactions for larger graphs.", "labels": [], "entities": []}, {"text": "In this paper, to better address the issue of learning deeper GCNs, we introduce dense connectivity to GCNs and propose the novel densely connected graph convolutional networks (DCGCNs), inspired by DenseNets () that distill insights from residual connections.", "labels": [], "entities": []}, {"text": "The dense connectivity strategy is illustrated in schematically.", "labels": [], "entities": []}, {"text": "Direct connections are introduced from any layer to all its preceding layers.", "labels": [], "entities": []}, {"text": "For example, the third layer receives the outputs of the first layer and the second layer, capturing the first-order, the second-order, and the third-order neighborhood information.", "labels": [], "entities": []}, {"text": "With the help of dense connections, we are able to train multi-layer GCN models with a large depth, allowing rich local and non-local information to be captured for learning a better graph representation than those learned from the shallower GCN models.", "labels": [], "entities": []}, {"text": "Experiments show that our model is able to achieve better performance for graph-to-sequence learning tasks.", "labels": [], "entities": []}, {"text": "For the AMR-to-text generation task, our model surpasses the current state-ofthe-art neural models trained on LDC2015E86 and LDC2017T10 by 2 and 4.3 BLEU points, respectively.", "labels": [], "entities": [{"text": "AMR-to-text generation task", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.9666491746902466}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.999024510383606}]}, {"text": "For the syntax-based neural machine translation task, our model is also consistently better than others, showing the effectiveness of the model on a large training set.", "labels": [], "entities": [{"text": "syntax-based neural machine translation task", "start_pos": 8, "end_pos": 52, "type": "TASK", "confidence": 0.6652178227901459}]}, {"text": "Our code is available at https://github.com/Cartus/ DCGCN.", "labels": [], "entities": [{"text": "Cartus/ DCGCN", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.7047641078631083}]}], "datasetContent": [{"text": "We assess the effectiveness of our models on two typical graph-to-sequence learning tasks, including AMR-to-text generation and syntaxbased neural machine translation (NMT).", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.8975485861301422}, {"text": "syntaxbased neural machine translation (NMT)", "start_pos": 128, "end_pos": 172, "type": "TASK", "confidence": 0.7107330518109458}]}, {"text": "For the AMR-to-text generation task, we use two benchmarks-the LDC2015E86 dataset (AMR15) and the LDC2017T10 dataset (AMR17).", "labels": [], "entities": [{"text": "AMR-to-text generation task", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.9574886560440063}, {"text": "LDC2015E86 dataset (AMR15", "start_pos": 63, "end_pos": 88, "type": "DATASET", "confidence": 0.7842321246862411}, {"text": "LDC2017T10 dataset", "start_pos": 98, "end_pos": 116, "type": "DATASET", "confidence": 0.9203664660453796}, {"text": "AMR17", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.5302098989486694}]}, {"text": "In these datasets, each instance contains a sentence and an AMR graph.", "labels": [], "entities": []}, {"text": "We follow to apply entity simplification in the preprocessing steps.", "labels": [], "entities": []}, {"text": "We then transform each preprocessed AMR graph into its extended Levi graph as described in Section 3.2.", "labels": [], "entities": []}, {"text": "For the syntax-based NMT task, we evaluate our model on both the En-De and the En-Cs News Commentary v11 dataset from the WMT16 translation task.", "labels": [], "entities": [{"text": "En-Cs News Commentary v11 dataset", "start_pos": 79, "end_pos": 112, "type": "DATASET", "confidence": 0.7693836510181427}, {"text": "WMT16 translation task", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.7453487316767374}]}, {"text": "We parse English sentences after tokenization to generate the dependency trees on the source side using SyntaxNet (.", "labels": [], "entities": []}, {"text": "We tokenize Czech and German using the Moses tokenizer.", "labels": [], "entities": [{"text": "tokenize Czech", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.8351389169692993}]}, {"text": "On the target side, we use byte-pair encodings (Sennrich et al., 2016) with 8,000 merge operations to obtain subwords.", "labels": [], "entities": []}, {"text": "We transform the labelled dependency trees into their corresponding extended Levi graphs as described in Section 3.2.) to perform default ensemble decoding.", "labels": [], "entities": []}, {"text": "performance increases when we gradually enlarge n and m.", "labels": [], "entities": []}, {"text": "For example, when n = 1 and m = 1, the BLEU score is 17.6; when n = 6 and m = 6, the BLEU score becomes 22.0.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9833477139472961}, {"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9810645580291748}]}, {"text": "We observe that the three settings (n = 6, m = 3), (n = 3, m = 6), and (n = 6, m = 6) give similar results for both 1 DCGCN block and 2 DCGCN blocks.", "labels": [], "entities": []}, {"text": "Because the first two settings contain fewer parameters than the third setting, it is reasonable to choose either (n = 6, m = 3) or (n = 3, m = 6).", "labels": [], "entities": []}, {"text": "For later experiments, we use (n = 6, m = 3).", "labels": [], "entities": []}, {"text": "The first block in shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA).", "labels": [], "entities": []}, {"text": "In general, increasing +RC 16.8 48.1 +RC+LA (2) 18.3 47.9 +RC 18.4 49.6 +RC+LA (4) 18.0 51.1 +RC 19.9 49.7 +RC+LA (6) 21.3 50.8 +RC 21.1 50.5 +RC+LA (9) 22.0 52.6 +RC (10) 20.7 50.7 +RC+LA (10) 21.2 52.9 DCGCN1   the number of GCN layers from 2 to 9 boosts the model performance.", "labels": [], "entities": []}, {"text": "However, when the layer number exceeds 10, the performance of both baseline models start to drop.", "labels": [], "entities": []}, {"text": "For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9).", "labels": [], "entities": [{"text": "GCN", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.7911790609359741}, {"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9872824847698212}]}, {"text": "In preliminary experiments, we cannot manage to train very deep GCN+RC and GCN+RC+LA models.", "labels": [], "entities": []}, {"text": "In contrast, our DCGCN models can be trained using a large number of layers.", "labels": [], "entities": []}, {"text": "For example, DCGCN4 contains 36 layers.", "labels": [], "entities": [{"text": "DCGCN4", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9495655298233032}]}, {"text": "When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on the AMR15 development set.", "labels": [], "entities": [{"text": "AMR15 development set", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.970929483572642}]}, {"text": "We therefore choose DCGCN4 for the AMR experiments.", "labels": [], "entities": [{"text": "DCGCN4", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.9293259382247925}, {"text": "AMR", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.537580132484436}]}, {"text": "Using a similar method, DCGCN2 is selected for the NMT tasks.", "labels": [], "entities": [{"text": "NMT tasks", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.5489251613616943}]}, {"text": "When the layer numbers are 9, DCGCN1 is better than GCN+RC in term of B/C scores (21.7/51.5 vs. 21.1/50.5).", "labels": [], "entities": [{"text": "GCN+RC", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.7827749053637186}, {"text": "B/C scores", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.8775163292884827}]}, {"text": "GCN+RC+LA (9) is sightly better than DCGCN1.", "labels": [], "entities": [{"text": "GCN+RC+LA (9", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8867113164493016}, {"text": "DCGCN1", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.934338390827179}]}, {"text": "However, when we set the number to 18, GCN+RC+LA achieves a BLEU score of 19.4, which is significantly worse than the BLEU score obtained by DCGCN2 (23.3).", "labels": [], "entities": [{"text": "GCN+RC+LA", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.7119529008865356}, {"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9840803742408752}, {"text": "BLEU", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9993878602981567}, {"text": "DCGCN2", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.9230250716209412}]}, {"text": "We also try GCN+RC+LA, but it does not converge.", "labels": [], "entities": [{"text": "LA", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.6770291924476624}]}, {"text": "In conclusion, these results show the robustness and effectiveness of our DCGCN models.", "labels": [], "entities": []}, {"text": "We also evaluate the performance of DCGCN model against different number of parameters on the AMR generation task.", "labels": [], "entities": [{"text": "AMR generation task", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.9077409307161967}]}, {"text": "Specifically, we try four parameter budgets, including 11.8M, 14.0M, 16.2M, and 18.4M.", "labels": [], "entities": []}, {"text": "These numbers correspond to the model size (in terms of number of parameters) of DCGCN1, DCGCN2, DCGCN3, and DCGCN4, respectively.", "labels": [], "entities": [{"text": "DCGCN1", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.9418540596961975}, {"text": "DCGCN2", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.8302409052848816}, {"text": "DCGCN4", "start_pos": 109, "end_pos": 115, "type": "DATASET", "confidence": 0.9256082773208618}]}, {"text": "For each budget, we vary both the depth of GCN models and the hidden vector dimensions of each node in GCNs in order to exhaust the entire budget.", "labels": [], "entities": []}, {"text": "For example, GCN (2) \u2212 512,  Performance vs. Layers.", "labels": [], "entities": [{"text": "GCN (2) \u2212 512", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.7383817533651987}]}, {"text": "We compare DCGCN models with different layers under the same parameter budget.", "labels": [], "entities": []}, {"text": "For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.999051034450531}]}, {"text": "Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters, DCGCN4 outperforms DCGCN3 by 1 BLEU point with a slightly smaller model.", "labels": [], "entities": [{"text": "DCGCN3", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9233617186546326}, {"text": "DCGCN4", "start_pos": 27, "end_pos": 33, "type": "DATASET", "confidence": 0.8928672075271606}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9986618757247925}]}, {"text": "In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones.", "labels": [], "entities": []}, {"text": "shows the ablation study of the level of density of our model.", "labels": [], "entities": [{"text": "ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9694457054138184}]}, {"text": "We use DCGCNs with 4 dense blocks as the full model.", "labels": [], "entities": []}, {"text": "Then we remove dense connections gradually from the last block to the first block.", "labels": [], "entities": []}, {"text": "In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections.", "labels": [], "entities": []}, {"text": "The full model gives 25.5 BLEU points on the AMR15 dev set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9991557598114014}, {"text": "AMR15 dev set", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9699793060620626}]}, {"text": "After removing the dense connections in the last block, the BLEU score becomes 24.8.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9782706797122955}]}, {"text": "Without using the dense connections in the last two blocks, the score drops to 23.8.", "labels": [], "entities": []}, {"text": "Furthermore, excluding the dense connections in the last three blocks only gives 23.2 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9992306232452393}]}, {"text": "Although these four      producing more expressive graph representations.", "labels": [], "entities": []}, {"text": "Results also show the linear combination is more effective than the global node.", "labels": [], "entities": []}, {"text": "Considering them together further enhances the model performance.", "labels": [], "entities": []}, {"text": "After removing the graph attention module, our model gives 24.9 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9990560412406921}]}, {"text": "Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9989847540855408}]}, {"text": "The coverage mechanism is also effective in our models.", "labels": [], "entities": []}, {"text": "Without the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.", "labels": [], "entities": [{"text": "coverage", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.981622576713562}, {"text": "B", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.9576536417007446}]}], "tableCaptions": [{"text": " Table 1: The number of sentences in four datasets.", "labels": [], "entities": []}, {"text": " Table 3.  Note that GraphLSTM uses char-level neural  representations and pretrained word embeddings,  whereas our model solely relies on word-level  representations with random initializations. This  empirically shows that compared with recurrent  graph encoders, DCGCNs can learn better rep- resentations for graphs.", "labels": [], "entities": []}, {"text": " Table 4: Main results on English-German and English-Czech datasets.", "labels": [], "entities": [{"text": "English-Czech datasets", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.7017160058021545}]}, {"text": " Table 6: Comparisons with baselines. +RC denotes  GCNs with residual connections. +RC+LA refers  to GCNs with both residual connections and layer  aggregations. DCGCNi represents our model  with i blocks, containing i \u00d7 (n + m) layers.  The number of layers for each model is shown  in parentheses.", "labels": [], "entities": []}, {"text": " Table 7: Comparisons of different DCGCN  models under almost the same parameter budget.", "labels": [], "entities": []}, {"text": " Table 8: Ablation study for density of  connections on the dev set of AMR15. -{i} dense  block denotes removing the dense connections  in the i-th block.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9726384878158569}, {"text": "AMR15", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9131473302841187}]}, {"text": " Table 9: Ablation study for modules used in the  graph encoder and the LSTM decoder.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9783953428268433}, {"text": "LSTM decoder", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.8547060787677765}]}]}