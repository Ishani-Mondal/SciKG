{"title": [{"text": "GILE: A Generalized Input-Label Embedding for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7549733519554138}]}], "abstractContent": [{"text": "Neural text classification models typically treat output labels as categorical variables that lack description and semantics.", "labels": [], "entities": [{"text": "Neural text classification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7523512045542399}]}, {"text": "This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones.", "labels": [], "entities": []}, {"text": "Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels.", "labels": [], "entities": []}, {"text": "The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance.", "labels": [], "entities": []}, {"text": "We evaluate models on full-resource and low-or zero-resource text classification of multilingual news and biomedical text with a large label set.", "labels": [], "entities": [{"text": "text classification of multilingual news and biomedical text", "start_pos": 61, "end_pos": 121, "type": "TASK", "confidence": 0.782159723341465}]}, {"text": "Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text classification is a fundamental NLP task with numerous real-world applications such as topic recognition, sentiment analysis (, and question answering).", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8371264040470123}, {"text": "topic recognition", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.9111632108688354}, {"text": "sentiment analysis", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.9746654629707336}, {"text": "question answering", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.8829242587089539}]}, {"text": "Classification also appears as a sub-task for sequence prediction tasks such as neural machine translation () and summarization (.", "labels": [], "entities": [{"text": "Classification", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9427178502082825}, {"text": "sequence prediction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7022672295570374}, {"text": "neural machine translation", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.6375571489334106}, {"text": "summarization", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9879913926124573}]}, {"text": "Despite numerous studies, existing models are trained on a fixed label set using k-hot vectors, and therefore treat target labels as mere atomic symbols without any particular structure to the space of labels, ignoring potential linguistic knowledge about the words used to describe the output labels.", "labels": [], "entities": []}, {"text": "Given that semantic representations of words have been shown to be useful for representing the input, it is reasonable to expect that they are going to be useful for representing the labels as well.", "labels": [], "entities": []}, {"text": "Previous work has leveraged knowledge from the label texts through a joint input-label space, initially for image classification).", "labels": [], "entities": [{"text": "image classification", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7511523962020874}]}, {"text": "Such models generalize to labels both seen and unseen during training, and scale well on very large label sets.", "labels": [], "entities": []}, {"text": "However, as we explain in Section 2, existing input-label models for text) have the following limitations: (i) their embedding does not capture complex label relationships due to its bilinear form, (ii) their output layer parametrization is rigid because it depends on the dimensionality of the encoded text and labels, and (iii) they are outperformed on seen labels by classification baselines trained with cross-entropy loss (.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew joint inputlabel model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels (see).", "labels": [], "entities": []}, {"text": "The proposed model is composed of a joint nonlinear input-label embedding with controllable capacity and a joint-spacedependent classification unit which is trained with cross-entropy loss to optimize classification performance.", "labels": [], "entities": []}, {"text": "The need for capturing complex label relationships is addressed by two nonlinear transformations that have the same target joint space dimensionality.", "labels": [], "entities": []}, {"text": "The parametrization of the output layer is not constrained by the dimensionality of the input or label encoding, but is instead flexible with a capacity that can be easily controlled by choosing the dimensionality of the joint space.", "labels": [], "entities": []}, {"text": "Training is performed with crossentropy loss, which is a suitable surrogate loss for classification problems, as opposed to a ranking loss such as WARP loss, which is more suitable for ranking problems.", "labels": [], "entities": [{"text": "WARP loss", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.6108168810606003}, {"text": "ranking", "start_pos": 185, "end_pos": 192, "type": "TASK", "confidence": 0.9605993032455444}]}, {"text": "Evaluation is performed on full-resource and low-or zero-resource scenarios of two text classification tasks, namely, on biomedical semantic indexing () and on multilingual news classification , against several competitive baselines.", "labels": [], "entities": [{"text": "text classification", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7257469296455383}, {"text": "biomedical semantic indexing", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.628410150607427}, {"text": "multilingual news classification", "start_pos": 160, "end_pos": 192, "type": "TASK", "confidence": 0.6355288724104563}]}, {"text": "In both scenarios, we provide a comprehensive ablation analysis that highlights the importance of each model component and the difference with previous embedding formulations when using the same type of architecture and loss function.", "labels": [], "entities": []}, {"text": "Our main contributions are the following: (i) We identify key theoretical and practical limitations of existing joint input-label models.", "labels": [], "entities": []}, {"text": "(ii) We propose a novel joint input-label embedding with flexible parametrization that generalizes over the previous such models and addresses their limitations.", "labels": [], "entities": []}, {"text": "(iii) We provide empirical evidence of the superiority of our model over monolingual and multilingual models that ignore label semantics, and over previous joint input-label models on both seen and unseen labels.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides background knowledge and explains limitations of existing models.", "labels": [], "entities": []}, {"text": "Section 3 describes the model components, training, and relation to previous formulations.", "labels": [], "entities": []}, {"text": "Section 4 describes our evaluation results and analysis, while Section 5 provides an overview of previous work and Section 6 concludes the paper and provides future research directions.", "labels": [], "entities": []}, {"text": "Our code is available at: github.com/idiap/gile.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation is performed on large-scale biomedical semantic indexing using the BioASQ data set, obtained by, and on multilingual news classification using the DW corpus, which consists of eight language data sets obtained by Pappas and Popescu-Belis (2017).", "labels": [], "entities": [{"text": "biomedical semantic indexing", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6536659399668375}, {"text": "BioASQ data set", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.9105265537897745}, {"text": "multilingual news classification", "start_pos": 119, "end_pos": 151, "type": "TASK", "confidence": 0.6454911530017853}, {"text": "DW corpus", "start_pos": 162, "end_pos": 171, "type": "DATASET", "confidence": 0.9458091557025909}]}, {"text": "The statistics of these data sets are listed in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data set statistics. #count is the number of  documents, #words are the number of unique words in  the vocabulary V, \u00af  w d and \u00af  w l are the average number  of words per document and label, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Biomedical semantic indexing results computed over labels seen and unseen during training, i.e., the  full-resource versus zero-resource settings. Best scores among the competing models are marked in bold.", "labels": [], "entities": [{"text": "Biomedical semantic indexing", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8807564576466879}]}, {"text": " Table 4: Full-resource classification results on general (Y g ) topic labels with DENSE and GRU encoders. Reported  are also the average number of parameters per language (n l ) and the average F 1 per language (f l ).", "labels": [], "entities": [{"text": "Full-resource classification", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6863894760608673}, {"text": "DENSE", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.8176938891410828}, {"text": "F 1", "start_pos": 195, "end_pos": 198, "type": "METRIC", "confidence": 0.9687315821647644}]}, {"text": " Table 5: Direct comparison with previous bilinear input-label models, namely, BIL [YH15] and BIL", "labels": [], "entities": [{"text": "BIL", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.930160403251648}, {"text": "BIL", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9504293203353882}]}, {"text": " Table 6: Multilingual learning results. The columns  are the average number of parameters per language  (n l ), average F 1 per language (f l ).", "labels": [], "entities": []}, {"text": " Table 7: Low-resource classification results with  various sizes of training data using the general labels.", "labels": [], "entities": [{"text": "Low-resource classification", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7393718659877777}]}]}