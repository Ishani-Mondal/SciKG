{"title": [{"text": "Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation", "labels": [], "entities": [{"text": "End-to-End Speech Translation", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.6512607236703237}]}], "abstractContent": [{"text": "Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts.", "labels": [], "entities": [{"text": "Speech translation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8363936543464661}]}, {"text": "Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech.", "labels": [], "entities": []}, {"text": "However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage.", "labels": [], "entities": [{"text": "direct speech translation", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.607908695936203}]}, {"text": "As a remedy, we propose the use of end-to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment.", "labels": [], "entities": []}, {"text": "We show that such models naturally decompose into multi-task-trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages.", "labels": [], "entities": [{"text": "multi-task-trainable recognition and translation", "start_pos": 50, "end_pos": 98, "type": "TASK", "confidence": 0.7355485260486603}]}, {"text": "Our proposed model outper-forms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech translation takes audio signals of speech as input and produces text translations as output.", "labels": [], "entities": [{"text": "Speech translation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7848926782608032}]}, {"text": "Although traditionally realized by cascading an automatic speech recognition (ASR) and a machine translation (MT) component, recent work has shown that it is feasible to use a single sequence-to-sequence model instead ().", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8489792943000793}, {"text": "machine translation (MT)", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.835002064704895}]}, {"text": "An appealing property of such direct models is that we no longer suffer from propagation of errors, where the speech recognizer passes an erroneous source text to the machine translation component, potentially leading to compounding follow-up errors.", "labels": [], "entities": []}, {"text": "Another advantage is the ability to train all model parameters jointly.", "labels": [], "entities": []}, {"text": "Despite these obvious advantages, two problems persist: (1) Reports on whether direct models outperform cascaded models) are inconclusive, with some work in favor of direct models (, some work in favor of cascaded models (, and one work in favor of direct models for two out of the three examined language pairs.", "labels": [], "entities": []}, {"text": "Cascaded and direct models have been compared under identical data situations, but this is an unrealistic assumption: In practice, cascaded models can be trained on much more abundant independent ASR and MT corpora, whereas end-to-end models require hard-to-acquire end-to-end corpora of speech utterances paired with textual translations.", "labels": [], "entities": []}, {"text": "Our first contribution is a closer investigation of these two issues.", "labels": [], "entities": []}, {"text": "Regarding the question of whether direct models or cascaded models are generally stronger, we hypothesize that direct models require more data to work well, due to the more complex mapping between inputs (source speech) and outputs (target text).", "labels": [], "entities": []}, {"text": "This would imply that direct models outperform cascades when enough data are available, but underperform in low-data scenarios.", "labels": [], "entities": []}, {"text": "We conduct experiments and present empirical evidence in favor of this hypothesis.", "labels": [], "entities": []}, {"text": "Next, fora more realistic comparison with regard to data conditions, we train a direct speech translation model using more auxiliary ASR and MT training data than end-to-end data.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7509740889072418}]}, {"text": "This can be implemented through multi-task training ().", "labels": [], "entities": []}, {"text": "Our results show that the auxiliary data are beneficial only to a limited extent, and that direct multitask models are still heavily dependent on the end-to-end data.", "labels": [], "entities": []}, {"text": "As our second contribution, we apply a two-stage model ( as an alternative solution to our problem, hoping that such models may overcome the data efficiency shortcoming of the direct model.", "labels": [], "entities": []}, {"text": "Twostage models consist of a first-stage attentional sequence-to-sequence model that performs speech recognition and then passes the decoder states as input to a second attentional model that performs translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7619029581546783}]}, {"text": "This architecture is closer to cascaded translation while maintaining end-toend trainability.", "labels": [], "entities": [{"text": "cascaded translation", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.699032187461853}]}, {"text": "Introducing supervision from the source-side transcripts midway through the model creates inductive bias that guides the complex transformation between source speech and target text through a reasonable intermediate representation closely tied to the source text.", "labels": [], "entities": []}, {"text": "The architecture has been proposed by to realize a reconstruction objective, and a similar model was also applied to speech translation ( to ease trainability, although no experiments under varying data conditions have been conducted.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.8091848790645599}]}, {"text": "We hypothesize that such a model may help to address the identified data efficiency issue: Unlike multi-task training for the direct model that trains auxiliary models on additional data but then discards many of the additionally learned parameters, the two-stage model uses all parameters of sub-models in the final end-to-end model.", "labels": [], "entities": []}, {"text": "Empirical results confirm that the twostage model is indeed successful at improving data efficiency, but suffers from some degradation in translation accuracy under high data conditions compared with the direct model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.8736958503723145}]}, {"text": "One reason for this degradation is that this model re-introduces the problem of error propagation, because the second stage of the model depends on the decoder states of the first model stage which often contain errors.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.6978720128536224}]}, {"text": "Our third contribution, therefore, is an attentionpassing variant of the two-stage model that, rather than passing on possibly erroneous decoder states from the first to the second stage, passes on only the computed attention context vectors.", "labels": [], "entities": []}, {"text": "We can view this approach as replacing the early decision on a source-side transcript by an early decision only on the attention scores needed to compute the same transcript, where the attention scores are expectedly more robust to errors in source text decoding.", "labels": [], "entities": []}, {"text": "We explore several variants of this model and show that it outperforms both the direct model and the vanilla two-stage model, while maintaining the improved data efficiency of the latter.", "labels": [], "entities": []}, {"text": "Through an analysis, we further observe a trade-off between sensitivity to error propagation and data efficiency.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on the Fisher and Callhome Spanish-English Speech Translation Corpus (, a corpus of Spanish telephone conversations that includes audio, transcriptions, and translations into English.", "labels": [], "entities": [{"text": "Fisher and Callhome Spanish-English Speech Translation", "start_pos": 30, "end_pos": 84, "type": "TASK", "confidence": 0.6614316403865814}]}, {"text": "We use the Fisher portion that consists of telephone conversations between strangers.", "labels": [], "entities": []}, {"text": "The training data size is 138,819 sentences, corresponding to 162 hours of speech.", "labels": [], "entities": []}, {"text": "ASR word error rates on this dataset are usually relatively high because of the spontaneous speaking style and challenging acoustics.", "labels": [], "entities": [{"text": "ASR word error rates", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.7692989856004715}]}, {"text": "From a translation viewpoint, the data can be considered as relatively easy with regard to both the topical domain and particular language pair.", "labels": [], "entities": [{"text": "translation", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9647433161735535}]}, {"text": "Our implementation is based on the xnmt toolkit.", "labels": [], "entities": []}, {"text": "We use the speech recognition recipe as a starting point, which has previously been shown to achieve competitive ASR results (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7731789350509644}, {"text": "ASR", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.9541581273078918}]}, {"text": "The vocabulary consists of the common characters appearing in English and Spanish, apostrophe, whitespace, and special start-of-sequence and unknown-character tokens.", "labels": [], "entities": []}, {"text": "The same vocabulary is used on both encoder (for the MT auxiliary task) and decoder sides.", "labels": [], "entities": [{"text": "MT auxiliary task)", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8824352622032166}]}, {"text": "We set the batch size dynamically depending on the input sequence size such that the average batch size is 24 sentences.", "labels": [], "entities": []}, {"text": "We use Adam () with initial learning rate of 0.0005, decayed by 0.5 when the validation BLEU score did not improve over 10 check points initially and 5 check points after the first decay.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.712373157342275}, {"text": "BLEU score", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.961498349905014}]}, {"text": "We initialize attention-passing models using weights from a basic two-stage model trained on the same data.", "labels": [], "entities": []}, {"text": "Following, we lowercase texts and remove punctuation.", "labels": [], "entities": []}, {"text": "As speech features, we use 40-dimensional Mel filter bank features with per-speaker mean and variance normalization.", "labels": [], "entities": []}, {"text": "We exclude a small number of utterances longer than 1500 frames from training to avoid running out of memory.", "labels": [], "entities": []}, {"text": "The encoder-decoder attention is MLP-based, and the decoder uses a single LSTM layer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores (4 references) on the Fisher/  Test for various amounts of training data. The  direct (multi-task) model performs best in the full  data condition, but the cascaded model is best in  all reduced conditions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991087317466736}, {"text": "Fisher/  Test", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8217379450798035}]}, {"text": " Table 2: Results for cascaded and multi-task  models under full training data conditions.", "labels": [], "entities": []}, {"text": " Table 3: Adding auxiliary OpenSubtitles MT data  to the training. The two-stage models benefit  much more strongly than the direct model, with  our proposed model yielding the strongest overall  results.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.8422685265541077}]}, {"text": " Table 4: Effect of altering the ASR labels for  different models as a measure for robustness against  error propagation. We compare results for the  cascade, the basic two-stage model (B2S), and APM  without and with cross connections. Percentages  are relative to the results for unaltered (decoded)  ASR labels.", "labels": [], "entities": [{"text": "ASR labels", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.8501631319522858}]}]}