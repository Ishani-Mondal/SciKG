{"title": [{"text": "CoQA: A Conversational Question Answering Challenge", "labels": [], "entities": [{"text": "Conversational Question Answering Challenge", "start_pos": 8, "end_pos": 51, "type": "TASK", "confidence": 0.7024122551083565}]}], "abstractContent": [{"text": "Humans gather information through conversations involving a series of interconnected questions and answers.", "labels": [], "entities": []}, {"text": "For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions.", "labels": [], "entities": [{"text": "information gathering", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7436032295227051}]}, {"text": "We introduce CoQA, a novel data-set for building Conversational Question Answering systems.", "labels": [], "entities": [{"text": "Conversational Question Answering", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7728150089581808}]}, {"text": "Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains.", "labels": [], "entities": []}, {"text": "The questions are conversational , and the answers are free-form text with their corresponding evidence highlighted in the passage.", "labels": [], "entities": []}, {"text": "We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coref-erence and pragmatic reasoning).", "labels": [], "entities": []}, {"text": "We evaluate strong dialogue and reading comprehension models on CoQA.", "labels": [], "entities": []}, {"text": "The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.991629034280777}]}, {"text": "We present CoQA as a challenge to the community at https://stanfordnlp.github.", "labels": [], "entities": []}], "introductionContent": [{"text": "We ask other people a question to either seek or test their knowledge about a subject.", "labels": [], "entities": []}, {"text": "Depending on their answer, we followup with another question and their second answer builds on what has already been discussed.", "labels": [], "entities": []}, {"text": "This incremental aspect makes human conversations succinct.", "labels": [], "entities": []}, {"text": "An inability to build and maintain common ground in this way is part of why virtual assistants usually don't seem like competent conversational partners.", "labels": [], "entities": []}, {"text": "In this * The first two authors contributed equally.", "labels": [], "entities": []}, {"text": "paper, we introduce CoQA, 1 a Conversational Question Answering dataset for measuring the ability of machines to participate in a questionanswering style conversation.", "labels": [], "entities": [{"text": "Conversational Question Answering", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.6058910588423411}]}, {"text": "In CoQA, a machine has to understand a text passage and answer a series of questions that appear in a conversation.", "labels": [], "entities": []}, {"text": "We develop CoQA with three main goals in mind.", "labels": [], "entities": []}, {"text": "The first concerns the nature of questions in a human conversation.", "labels": [], "entities": []}, {"text": "shows a conversation between two humans who are reading a passage, one acting as a questioner and the other as an answerer.", "labels": [], "entities": []}, {"text": "In this conversation, every question after the first is dependent on the conversation history.", "labels": [], "entities": []}, {"text": "For instance, Q 5 (Who?) is only a single word and is impossible to answer without knowing what has already been said.", "labels": [], "entities": []}, {"text": "Posing short questions is an effective human conversation strategy, but such questions are difficult for machines to parse.", "labels": [], "entities": []}, {"text": "As is well known, state-of-the-art models rely heavily on lexical similarity between a question and a passage.", "labels": [], "entities": []}, {"text": "At present, there are no largescale reading comprehension datasets that contain questions that depend on a conversation history (see) and this is what CoQA is mainly developed for.", "labels": [], "entities": []}, {"text": "The second goal of CoQA is to ensure the naturalness of answers in a conversation.", "labels": [], "entities": []}, {"text": "Many existing QA datasets restrict answers to contiguous text spans in a given passage.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.7502241134643555}]}, {"text": "Such answers are not always natural-for example, there is no span-based answer to Q 4 (How many?) in.", "labels": [], "entities": []}, {"text": "In CoQA, we propose that the answers can be free-form text, while for each answer, we also provide a text span from the passage as a rationale to the answer.", "labels": [], "entities": []}, {"text": "Therefore, the answer to Q 4 is simply Three and its rationale spans CoQA is pronounced as coca.", "labels": [], "entities": []}, {"text": "Concurrent with our work, also created a conversational dataset with a similar goal, but it differs in many aspects.", "labels": [], "entities": []}, {"text": "We discuss the details in Section 7.", "labels": [], "entities": []}, {"text": "Free-form answers have been studied in previous reading comprehension datasets for example, MS MARCO ( and NarrativeQA, and metrics such as BLEU or ROUGE are used for evaluation due to the high variance of possible answers.", "labels": [], "entities": [{"text": "MS", "start_pos": 92, "end_pos": 94, "type": "DATASET", "confidence": 0.9498893022537231}, {"text": "MARCO", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.43288570642471313}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.998512327671051}, {"text": "ROUGE", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.9826282262802124}]}, {"text": "One key difference in our setting is that we require answerers to first select a text span as the rationale and then edit it to obtain a free-form answer.", "labels": [], "entities": []}, {"text": "Our method strikes a balance between naturalness of answers and reliable automatic evaluation, and it results in a high human agreement (88.8% F1 word overlap among human annotators).", "labels": [], "entities": [{"text": "agreement", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.87403404712677}, {"text": "F1 word overlap", "start_pos": 143, "end_pos": 158, "type": "METRIC", "confidence": 0.9127922256787618}]}, {"text": "The third goal of CoQA is to enable building QA systems that perform robustly across domains.", "labels": [], "entities": []}, {"text": "The current QA datasets mainly focus on a single domain, which makes it hard to test the generalization ability of existing models.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8007895052433014}]}, {"text": "Hence we collect our dataset from seven different domainschildren's stories, literature, middle and high school English exams, news, Wikipedia, Reddit, and science.", "labels": [], "entities": []}, {"text": "The last two are used for out-of-domain evaluation.", "labels": [], "entities": []}, {"text": "To summarize, CoQA has the following key characteristics: \u2022 It consists of 127k conversation turns collected from 8k conversations over text passages.", "labels": [], "entities": []}, {"text": "The average conversation length is 15 turns, and each turn consists of a question and an answer.", "labels": [], "entities": []}, {"text": "\u2022 It contains free-form answers and each answer has a span-based rationale highlighted in the passage.", "labels": [], "entities": []}, {"text": "\u2022 Its text passages are collected from seven diverse domains: five are used for in-domain evaluation and two are used for out-ofdomain evaluation.", "labels": [], "entities": []}, {"text": "Almost half of CoQA questions refer back to conversational history using anaphors, and a large portion require pragmatic reasoning, making it challenging for models that rely on lexical cues alone.", "labels": [], "entities": []}, {"text": "We benchmark several deep neural network models, building on top of state-ofthe-art conversational and reading comprehension models (Section 5).", "labels": [], "entities": []}, {"text": "The best-performing system achieves an F1 score of 65.4%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9875385761260986}]}, {"text": "In contrast, humans achieve 88.8% F1, 23.4% F1 higher, indicating that there is a considerable room for improvement.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9980608820915222}, {"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9993712306022644}]}], "datasetContent": [{"text": "For each conversation, we use two annotators, a questioner and an answerer.", "labels": [], "entities": []}, {"text": "This setup has several advantages over using a single annotator to act both as a questioner and an answerer: 1) when two annotators chat about a passage, their dialogue flow is natural; 2) when one annotator responds with a vague question or an incorrect answer, the other can raise a flag, which we use to identify bad workers; and 3) the two annotators can discuss guidelines (through a separate chat window) when they have disagreements.", "labels": [], "entities": []}, {"text": "These measures help to prevent spam and to obtain high agreement data.", "labels": [], "entities": []}, {"text": "We use Amazon Mechanical Turk to pair workers on a passage through the ParlAI MTurk API ().", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.882385273774465}, {"text": "ParlAI MTurk API", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.8527329564094543}]}, {"text": "What makes the CoQA dataset conversational compared to existing reading comprehension datasets like SQuAD?", "labels": [], "entities": [{"text": "CoQA dataset conversational", "start_pos": 15, "end_pos": 42, "type": "DATASET", "confidence": 0.8895685076713562}]}, {"text": "What linguistic phenomena do the questions in CoQA exhibit?", "labels": [], "entities": []}, {"text": "How does the conversation flow from one turn to the next?", "labels": [], "entities": []}, {"text": "We answer these questions in this section.", "labels": [], "entities": []}, {"text": "Following SQuAD, we use macro-average F1 score of word overlap as our main evaluation metric.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9512124955654144}]}, {"text": "We use the gold answers of history to predict the next answer.", "labels": [], "entities": []}, {"text": "In SQuAD, for computing a model's performance, each individual prediction is compared against n human answers resulting inn F1 scores, the maximum of which is chosen as the prediction's F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.993615984916687}, {"text": "F1", "start_pos": 186, "end_pos": 188, "type": "METRIC", "confidence": 0.9969839453697205}]}, {"text": "For each question, we average out F1 across these n sets, both for humans and models.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9992772936820984}]}, {"text": "In our final evaluation, we use n = 4 human answers for every question (the original answer and 3 additionally collected answers).", "labels": [], "entities": []}, {"text": "The articles a, an, and the and punctuations are excluded in evaluation.", "labels": [], "entities": []}, {"text": "For all the experiments of seq2seq and PGNet, we use the OpenNMT toolkit () and its default settings: 2-layers of LSTMs with 500 hidden units for both the encoder and the decoder.", "labels": [], "entities": []}, {"text": "The models are optimized using SGD, with an initial learning rate of 1.0 and a decay rate of 0.5.", "labels": [], "entities": []}, {"text": "A dropout rate of 0.3 is applied to all layers.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9720647037029266}]}, {"text": "For the DrQA experiments, we use the implementation from the original paper.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9551513195037842}]}, {"text": "We tune the hyperparameters on the development data: the number of turns to use from the conversation history, the number of layers, number of each hidden units per layer, and dropout rate.", "labels": [], "entities": []}, {"text": "The best configuration we find is 3 layers of LSTMs with 300 hidden units for each layer.", "labels": [], "entities": []}, {"text": "A dropout rate of 0.4 is applied to all LSTM layers and a dropout rate of 0.5 is applied to word embeddings.", "labels": [], "entities": []}, {"text": "We used Adam to optimize DrQA models.", "labels": [], "entities": [{"text": "DrQA", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8414716124534607}]}, {"text": "We initialized the word projection matrix with GloVe () for conversational models and fastText for reading comprehension models, based on empirical performance.", "labels": [], "entities": [{"text": "word projection", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.749823808670044}, {"text": "GloVe", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9754173755645752}, {"text": "fastText", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.8216962814331055}]}, {"text": "We update the projection matrix during training in order to learn embeddings for delimiters such as <q>.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of domains in CoQA.", "labels": [], "entities": []}, {"text": " Table 3: Average number of words in passage,  question, and answer in SQuAD and CoQA.", "labels": [], "entities": []}, {"text": " Table 4: Distribution of answer types in SQuAD  and CoQA.", "labels": [], "entities": []}, {"text": " Table 7: Models and human performance (F1 score) on the development and the test data.", "labels": [], "entities": [{"text": "F1 score)", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9791499773661295}]}, {"text": " Table 8: Fine-grained results of different question and answer types in the development set. For the  question type results, we only analyze 150 questions as described in Section 4.2.", "labels": [], "entities": []}, {"text": " Table 9: Results on the development set with  different history sizes. History size indicates the  number of previous turns prepended to the current  question. Each turn contains a question and its  answer.", "labels": [], "entities": []}, {"text": " Table 10: Error analysis of questions with answers  that do not overlap with the text passage.", "labels": [], "entities": []}]}