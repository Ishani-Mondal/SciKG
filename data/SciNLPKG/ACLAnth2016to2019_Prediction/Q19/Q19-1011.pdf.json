{"title": [{"text": "SECTOR: A Neural Model for Coherent Topic Segmentation and Classification", "labels": [], "entities": [{"text": "SECTOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6730393171310425}, {"text": "Coherent Topic Segmentation and Classification", "start_pos": 27, "end_pos": 73, "type": "TASK", "confidence": 0.654035747051239}]}], "abstractContent": [{"text": "When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention.", "labels": [], "entities": []}, {"text": "However , the high variance of document structure complicates the identification of the salient topic of a given section at a glance.", "labels": [], "entities": []}, {"text": "To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section.", "labels": [], "entities": []}, {"text": "Our deep neural network architecture learns a latent topic embedding over the course of a document.", "labels": [], "entities": []}, {"text": "This can be leveraged to classify local topics from plain text and segment a document at topic shifts.", "labels": [], "entities": []}, {"text": "In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities.", "labels": [], "entities": []}, {"text": "From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation.", "labels": [], "entities": [{"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9998794794082642}, {"text": "segmentation and classification of 30 topics", "start_pos": 97, "end_pos": 141, "type": "TASK", "confidence": 0.7656379242738088}, {"text": "English city domain", "start_pos": 151, "end_pos": 170, "type": "DATASET", "confidence": 0.7536820967992147}, {"text": "SECTOR", "start_pos": 186, "end_pos": 192, "type": "DATASET", "confidence": 0.6734048128128052}]}, {"text": "This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.", "labels": [], "entities": [{"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9990562796592712}]}], "introductionContent": [{"text": "Today's systems for natural language understanding are composed of building blocks that extract semantic information from the text, such as named entities, relations, topics, or discourse structure.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6819833715756735}]}, {"text": "In traditional natural language processing (NLP), these extractors are typically applied to bags of words or full sentences (.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.7494643926620483}]}, {"text": "Recent neural architectures build upon pretrained word or sentence embeddings (, which focus on semantic relations that can be learned from large sets of paradigmatic examples, even from long ranges (.", "labels": [], "entities": []}, {"text": "From a human perspective, however, it is mostly the authors themselves who help best to understand a text.", "labels": [], "entities": []}, {"text": "Especially in long documents, an author thoughtfully designs a readable structure and guides the reader through the text by arranging topics into coherent passages.", "labels": [], "entities": []}, {"text": "In many cases, this structure is not formally expressed as section headings (e.g., in news articles, reviews, discussion forums) or it is structured according to domain-specific aspects (e.g., health reports, research papers, insurance documents).", "labels": [], "entities": []}, {"text": "Ideally, systems for text analytics, such as topic detection and tracking (TDT)), text summarization (, information retrieval (IR) (, or question answering (QA) , could access a document representation that is aware of both topical (i.e., latent semantic content) and structural information (i.e., segmentation) in the text ().", "labels": [], "entities": [{"text": "topic detection and tracking (TDT))", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.877528897353581}, {"text": "text summarization", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7047268152236938}, {"text": "information retrieval (IR)", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.7519304871559143}, {"text": "question answering (QA)", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.838221275806427}]}, {"text": "The challenge in building such a representation is to combine these two dimensions that are strongly interwoven in the author's mind.", "labels": [], "entities": []}, {"text": "It is therefore important to understand topic segmentation and classification as a mutual task that requires encoding both topic information and document structure coherently.", "labels": [], "entities": [{"text": "topic segmentation and classification", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.7156428098678589}]}, {"text": "In this paper, we present SECTOR, an end-to-end model that learns an embedding of latent topics from potentially ambiguous headings and can be applied to entire documents to predict local topics on sentence level.", "labels": [], "entities": [{"text": "SECTOR", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.7853959202766418}]}, {"text": "Our model encodes topical information on a vertical dimension and structural information on a horizontal dimension.", "labels": [], "entities": []}, {"text": "We show that the resulting embedding can be leveraged in a downstream pipeline to segment a document into coherent sections and classify the sections into one of up to 30 topic categories reaching 71.6% F 1 -or alternatively, attach up to 2.8k topic labels with 71.1% mean average precision (MAP).", "labels": [], "entities": [{"text": "F 1", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.9870290756225586}, {"text": "mean average precision (MAP)", "start_pos": 268, "end_pos": 296, "type": "METRIC", "confidence": 0.8512119352817535}]}, {"text": "We further show that segmentation performance of our bidirectional long short-term memory (LSTM) architecture is comparable to specialized state-of-the-art segmentation methods on various real-world data sets.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, the combined task of segmentation and classification has not been approached on the full document level before.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9770445227622986}]}, {"text": "There exist a large number of data sets for text segmentation, but most of them do not reflect real-world topic drifts, do not include topic labels, or are heavily normalized and too small to be used for training neural networks.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7550037801265717}]}, {"text": "We can utilize a generic segmentation data set derived from Wikipedia that includes headings (), but there is also a need in IR and QA for supervised structural topic labels, different languages and more specific domains, such as clinical or biomedical research, and news-based TDT (.", "labels": [], "entities": []}, {"text": "Therefore we introduce WIKISECTION, 2 a large novel data set of 38k articles from the English and German Wikipedia labeled with 242k sections, original headings, and normalized topic labels for up to 30 topics from two domains: diseases and cities.", "labels": [], "entities": [{"text": "WIKISECTION", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.6473942995071411}]}, {"text": "We chose these subsets to cover both clinical/biomedical aspects (e.g., symptoms, treatments, complications) and news-based topics (e.g., history, politics, economy, climate).", "labels": [], "entities": []}, {"text": "Both article types are reasonably well-structured according to Wikipedia guidelines (), but we show that they are also comple- The data set is available under the CC BY-SA 3.0 license at https://github.com/sebastianarnold/ WikiSection.", "labels": [], "entities": [{"text": "WikiSection", "start_pos": 223, "end_pos": 234, "type": "DATASET", "confidence": 0.935997486114502}]}, {"text": "mentary: Diseases is atypical scientific domain with low entropy (i.e., very narrow topics, precise language, and low word ambiguity).", "labels": [], "entities": []}, {"text": "In contrast, cities resembles a diversified domain, with high entropy (i.e., broader topics, common language, and higher word ambiguity) and will be more applicable to for example, news, risk reports, or travel reviews.", "labels": [], "entities": []}, {"text": "We compare SECTOR to existing segmentation and classification methods based on latent Dirichlet allocation (LDA), paragraph embeddings, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).", "labels": [], "entities": [{"text": "SECTOR", "start_pos": 11, "end_pos": 17, "type": "TASK", "confidence": 0.8919877409934998}, {"text": "segmentation and classification", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.8732720812161764}]}, {"text": "We show that SECTOR significantly improves these methods in a combined task by up to 29.5 points F 1 when applied to plain text with no given segmentation.", "labels": [], "entities": [{"text": "SECTOR", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.6969411373138428}]}, {"text": "The rest of this paper is structured as follows: We introduce related work in Section 2.", "labels": [], "entities": []}, {"text": "Next, we describe the task and data set creation process in Section 3.", "labels": [], "entities": []}, {"text": "We formalize our model in Section 4.", "labels": [], "entities": []}, {"text": "We report results and insights from the evaluation in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct three experiments to evaluate the segmentation and classification task introduced in Section 3.", "labels": [], "entities": [{"text": "segmentation and classification", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.8328626354535421}]}, {"text": "The WIKISECTION-topics experiment constitutes segmentation and classification of each section with a single topic label out of a small number of clean labels (25-30 topics).", "labels": [], "entities": []}, {"text": "The WIKISECTION-headings experiment extends the classification task to multi-label per section with a larger target vocabulary (1.0k-2.8k words).", "labels": [], "entities": []}, {"text": "This is important, because often there are no clean topic labels available for training or evaluation.", "labels": [], "entities": []}, {"text": "Finally, we conduct a third experiment to see how SECTOR performs across existing segmentation data sets.", "labels": [], "entities": [{"text": "SECTOR", "start_pos": 50, "end_pos": 56, "type": "TASK", "confidence": 0.9394537210464478}]}, {"text": "For the first two experiments we use the WIKISECTION data sets introduced in Section 3.1, which contain documents about diseases and cities in both English and German.", "labels": [], "entities": [{"text": "WIKISECTION data sets", "start_pos": 41, "end_pos": 62, "type": "DATASET", "confidence": 0.9081846276919047}]}, {"text": "The subsections are retained with full granularity.", "labels": [], "entities": []}, {"text": "For the third experiment, text segmentation results are often reported on artificial data sets.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8083756268024445}]}, {"text": "It was shown that this scenario is hardly applicable to topic-based segmentation (), so we restrict our evaluation to real-world data sets that are publicly available.", "labels": [], "entities": [{"text": "topic-based segmentation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6772717237472534}]}, {"text": "The Wiki-727k data set by contains Wikipedia articles with abroad range of topics and their top-level sections.", "labels": [], "entities": [{"text": "Wiki-727k data set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9556230306625366}]}, {"text": "However, it is too large to compare exhaustively, so we use the smaller Wiki-50 subset.", "labels": [], "entities": [{"text": "Wiki-50 subset", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.9064335227012634}]}, {"text": "We further use the Cities and Elements data sets introduced by, which also provide headings.", "labels": [], "entities": [{"text": "Cities and Elements data sets", "start_pos": 19, "end_pos": 48, "type": "DATASET", "confidence": 0.7712177634239197}]}, {"text": "These sets are typically used for word-level segmentation, so they don't contain any punctuation and are lowercased.", "labels": [], "entities": [{"text": "word-level segmentation", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7446688413619995}]}, {"text": "Finally, we use the Clinical Textbook chapters introduced by, which do not supply headings.", "labels": [], "entities": [{"text": "Clinical Textbook chapters", "start_pos": 20, "end_pos": 46, "type": "DATASET", "confidence": 0.8887537519137064}]}, {"text": "We compare SEC-TOR to common text segmentation methods as baseline, C99 (Choi, 2000) and TopicTiling () and the state-of-the-art TextSeg segmenter (.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7275355756282806}, {"text": "TextSeg segmenter", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.752631276845932}]}, {"text": "In the third experiment we report numbers for BayesSeg (Eisenstein and Barzilay, 2008) (configured to predict with unknown number of segments) and).", "labels": [], "entities": []}, {"text": "We compare SECTOR to existing models for single and multi-label sentence classification.", "labels": [], "entities": [{"text": "SECTOR", "start_pos": 11, "end_pos": 17, "type": "TASK", "confidence": 0.8037075400352478}, {"text": "multi-label sentence classification", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.6550314128398895}]}, {"text": "Because we are not aware of any existing method for combined segmentation and classification, we first compare all methods using given prior segmentation from newlines in the text (NL) and then additionally apply our own segmentation strategies for plain text input: maximum label (max), embedding deviation (emd) and bidirectional embedding deviation (bemd).", "labels": [], "entities": []}, {"text": "For the experiments, we train a Paragraph Vectors (PV) model () using all sections of the training sets.", "labels": [], "entities": []}, {"text": "We utilize this model for single-label topic classification (depicted as PV>T) by assigning the given topic labels as paragraph IDs.", "labels": [], "entities": [{"text": "single-label topic classification", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.6592381199200948}]}, {"text": "Multi-label classification is not possible with this model.", "labels": [], "entities": [{"text": "Multi-label classification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7655580937862396}]}, {"text": "We use the paragraph embedding for our own segmentation strategies.", "labels": [], "entities": []}, {"text": "We set the layer size to 256, window size to 7, and trained for 10 epochs using a batch size of 512 sentences and a learning rate of 0.025.", "labels": [], "entities": []}, {"text": "We further use an implementation of CNN) with our pre-trained word vectors as input for single-label topics (CNN>T) and multi-label headings (CNN>H).", "labels": [], "entities": []}, {"text": "We configured the models using the hyperparameters given in the paper and trained the model using a batch size of 256 sentences for 20 epochs with learning rate 0.01.", "labels": [], "entities": []}, {"text": "We evaluate the various configurations of our model discussed in prior sections.", "labels": [], "entities": []}, {"text": "SEC>T depicts the single-label topic classification model which uses a softmax activation output layer, SEC>H is the multilabel variant with a larger output and sigmoid activations.", "labels": [], "entities": [{"text": "single-label topic classification", "start_pos": 18, "end_pos": 51, "type": "TASK", "confidence": 0.6577731370925903}]}, {"text": "Other options are: bag-of-words sentence encoding (+bow), Bloom filter encoding (+bloom) and sentence embeddings (+emb); multi-class cross-entropy loss (as default) and ranking loss (+rank).", "labels": [], "entities": [{"text": "Bloom filter encoding", "start_pos": 58, "end_pos": 79, "type": "METRIC", "confidence": 0.9096326033274332}]}, {"text": "We have chosen network hyperparameters using grid search on the en disease validation set and keep them fixed overall evaluation runs.", "labels": [], "entities": [{"text": "en disease validation set", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.8511247038841248}]}, {"text": "For all configurations, we set LSTM layer size to 256, topic embeddings dimension to 128.", "labels": [], "entities": []}, {"text": "Models are trained on the complete train splits with a batch size of 16 documents (reduced to 8 for bag-of-words), 0.01 learning rate, 0.5 dropout, and ADAM optimization.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.6317512392997742}]}, {"text": "We used early stopping after 10 epochs without MAP improvement on the validation data sets.", "labels": [], "entities": [{"text": "MAP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.942081868648529}, {"text": "validation data sets", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.8426204919815063}]}, {"text": "We pretrained word embeddings with 256 dimensions for the specific tasks using word2vec on lowercase English and German Wikipedia documents using a window size of 7.", "labels": [], "entities": []}, {"text": "All tests are implemented in Deeplearning4j and run on a Tesla P100 GPU with 16GB memory.", "labels": [], "entities": []}, {"text": "Training a SEC+bloom model on en city takes roughly 5 hours, inference on CPU takes on average 0.36 seconds per document.", "labels": [], "entities": []}, {"text": "In addition, we trained a SEC>H@fullwiki model with raw headings from a complete English Wikipedia dump, 8 and use this model for cross-data set evaluation.", "labels": [], "entities": []}, {"text": "We measure text segmentation at sentence level using the probabilistic P k error score), which calculates the probability of a false boundary in a window of size k, lower numbers mean better segmentation.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7174235135316849}, {"text": "probabilistic P k error score", "start_pos": 57, "end_pos": 86, "type": "METRIC", "confidence": 0.7011962056159973}]}, {"text": "As relevant section boundaries we consider all section breaks where the topic label changes.", "labels": [], "entities": []}, {"text": "We set k to half of the average segment length.", "labels": [], "entities": []}, {"text": "We measure classification performance on section level by comparing the topic labels of all ground truth sections with predicted sections.", "labels": [], "entities": []}, {"text": "We Excluding all documents contained in the test sets.", "labels": [], "entities": []}, {"text": "select the pairs by matching their positions using maximum boundary overlap.", "labels": [], "entities": []}, {"text": "We report microaveraged F 1 score for single-label or Precision@1 for multi-label classification.", "labels": [], "entities": [{"text": "microaveraged F 1 score", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.7223193347454071}, {"text": "Precision@1", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.9307127396265665}, {"text": "multi-label classification", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.640746459364891}]}, {"text": "Additionally, we measure Mean Average Precision (MAP), which evaluates the average fraction of true labels ranked above a particular label ().", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 25, "end_pos": 53, "type": "METRIC", "confidence": 0.9777021706104279}]}, {"text": "shows the evaluation results of the WIKISECTION-topics single-label classification task, contains the corresponding numbers for multi-label classification.", "labels": [], "entities": [{"text": "WIKISECTION-topics single-label classification task", "start_pos": 36, "end_pos": 87, "type": "TASK", "confidence": 0.7115404233336449}, {"text": "multi-label classification", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.7700602412223816}]}, {"text": "shows results for topic segmentation across different data sets.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7811363041400909}]}], "tableCaptions": [{"text": " Table 1: Data set characteristics for disease (German:  Krankheit) and city (German: Stadt). Headings denotes  the number of distinct section and subsection headings  among the documents. Topics stands for the number of  topic labels after synset clustering. Coverage denotes  the proportion of headings covered by topics; the  remaining headings are labeled as other.", "labels": [], "entities": []}, {"text": " Table 2: Frequency and entropy (H) of top-3 head and  randomly selected torso and tail headings for category  diseases in the English Wikipedia.", "labels": [], "entities": [{"text": "Frequency and entropy (H)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8666221002737681}, {"text": "English Wikipedia", "start_pos": 127, "end_pos": 144, "type": "DATASET", "confidence": 0.8834237158298492}]}, {"text": " Table 3: Results for topic segmentation and single-label classification on four WIKISECTION data sets. n = 718  / 464 / 3, 907 / 2, 507 documents. Numbers are given as P k on sentence level, micro-averaged F 1 and MAP at  segment-level. For methods without segmentation, we used newlines as segment boundaries (NL) and merged  sections of same classes after prediction. Models marked with * are based on pre-trained distributional embeddings.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7244988083839417}, {"text": "single-label classification", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6974850744009018}, {"text": "WIKISECTION data sets", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.9350095987319946}, {"text": "micro-averaged F 1", "start_pos": 192, "end_pos": 210, "type": "METRIC", "confidence": 0.6919270753860474}, {"text": "MAP", "start_pos": 215, "end_pos": 218, "type": "METRIC", "confidence": 0.9701727032661438}]}, {"text": " Table 4: Results for segmentation and multi-label classification trained with raw Wikipedia headings. Here, the  task is to segment the document and predict multi-word topics from a large ambiguous target vocabulary.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.982475996017456}, {"text": "multi-label classification", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.8109819293022156}]}, {"text": " Table 5: Results for cross-data set evaluation on existing data sets. Numbers marked with * are generated by  models trained specifically for this data set. A value of 'n/a' indicates that a model is not applicable to this problem.", "labels": [], "entities": []}]}