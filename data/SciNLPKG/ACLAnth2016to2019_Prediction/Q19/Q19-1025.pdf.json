{"title": [{"text": "No Word is an Island-A Transformation Weighting Model for Semantic Composition", "labels": [], "entities": [{"text": "No Word is an Island-A Transformation Weighting", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.5541628343718392}, {"text": "Semantic Composition", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7441577017307281}]}], "abstractContent": [{"text": "Composition models of distributional semantics are used to construct phrase representations from the representations of their words.", "labels": [], "entities": []}, {"text": "Composition models are typically situated on two ends of a spectrum.", "labels": [], "entities": []}, {"text": "They either have a small number of parameters but compose all phrases in the same way, or they perform word-specific compositions at the cost of afar larger number of parameters.", "labels": [], "entities": []}, {"text": "In this paper we propose transformation weighting (TransWeight), a composition model that consistently outper-forms existing models on nominal compounds, adjective-noun phrases, and adverb-adjective phrases in English, German, and Dutch.", "labels": [], "entities": []}, {"text": "Trans-Weight drastically reduces the number of parameters needed compared with the best model in the literature by composing similar words in the same way.", "labels": [], "entities": []}], "introductionContent": [{"text": "The phrases black car and purple car are very similar-except for their frequency.", "labels": [], "entities": [{"text": "frequency", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9734660387039185}]}, {"text": "Ina large corpus, described in more detail in Section 4.1, the phrase black car occurs 131 times, making it possible to model the whole phrase distributionally.", "labels": [], "entities": []}, {"text": "But purple car is far less frequent, with only five occurrences, and therefore out of reach for distributional models based on co-occurrence counts.", "labels": [], "entities": []}, {"text": "Distributional word representations derived from large, unannotated corpora) capture information about individual words like purple and car and are able to express, in vector space, different types of word similarity (the similarity between color adjectives like black and purple, between car and truck, etc.).", "labels": [], "entities": []}, {"text": "Creating phrasal representations, and in particular representing low-frequency phrases like purple car, is a task for composition models of distributional semantics.", "labels": [], "entities": []}, {"text": "A composition model is a function f that combines the vectors of individual words, for example, u for black and v for car into a phrase representation p. p is the result of applying the composition function f to the word vectors u, v: p = f (u, v).", "labels": [], "entities": []}, {"text": "Our goal is to find the function f that learns how to compose phrase representations by training on a set of phrases.", "labels": [], "entities": []}, {"text": "The target representation of the whole phrase, \u02dc p, is learned directly from the corpus, if needed by concatenating first the word pairs of interest, for example, black car.", "labels": [], "entities": []}, {"text": "The function f seeks to maximize the cosine similarity of the composed representation p and the target representatio\u00f1 p, arg max p\u00b7\u02dcpp\u00b7\u02dcp \ud97b\udf59p\ud97b\udf59 2 \ud97b\udf59\u02dcp\ud97b\udf59\ud97b\udf59\u02dcp\ud97b\udf59 2 . The contributions of this paper are as follows: \u2022 We provide an extensive review and evaluation of existing composition models.", "labels": [], "entities": []}, {"text": "The evaluation is carried out in parallel on three syntactic constructions using English, German, and Dutch treebanks: adjective-noun phrases (black car, schwarz Auto, zwart auto), nominal compounds (apple tree, Apfelbaum, appelboom), and adverb-adjective phrases (very large, sehr gro\u00df, zeer groot).", "labels": [], "entities": []}, {"text": "These constructions serve as case studies in analyzing the generalization potential of composition models.", "labels": [], "entities": []}, {"text": "The evaluation of existing approaches shows that state-of-the-art composition models lie at opposite ends of the spectrum.", "labels": [], "entities": []}, {"text": "On one end, they use the same matrix transformation) for all words, leading to a very general composition.", "labels": [], "entities": []}, {"text": "On the other end, there are composition models that use individual vectors) or matrices () for each dictionary word, leading to word-specific compositions with a multitude of parameters.", "labels": [], "entities": []}, {"text": "Although the latter perform better, they require training data for each word.", "labels": [], "entities": []}, {"text": "These lexicalized composition models suffer from the curse of dimensionality (): The individual word transformations are learned exclusively from examples containing those words, and do not benefit from the information provided by training examples involving different, but semantically related words.", "labels": [], "entities": []}, {"text": "In the example, the vectors/matrices for black and purple would be trained independently, despite the similarity of the words and their corresponding vector representations.", "labels": [], "entities": []}, {"text": "\u2022 We propose transformation weighting-a new composition model where 'no word is an island'.", "labels": [], "entities": [{"text": "transformation weighting-a", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.8968574404716492}]}, {"text": "The model draws on the similarity of the word vectors to induce similar transformations.", "labels": [], "entities": []}, {"text": "Its formulation, presented in Section 3, makes it possible to tailor the composition function even for examples that were not seen during training: For example, even if purple car is infrequent, the composition can still produce a suitable representation based on other phrases containing color adjectives that occur in training.", "labels": [], "entities": []}, {"text": "\u2022 We correct an error in the rank evaluation methodology proposed by and subsequently used as an evaluation standard in other publications (.", "labels": [], "entities": []}, {"text": "The corrected methodology, described in Section 4.4, uses the original phrase representations as a reference point instead of the composed representations.", "labels": [], "entities": []}, {"text": "It makes a fair comparison between the results of different composition models possible.", "labels": [], "entities": []}, {"text": "\u2022 We provide reference TensorFlow () implementations of all the composition models investigated in this paper 1 and composition data sets for English and German compounds, adjective-noun phrases, and adverb-adjective phrases and for Dutch adjective-noun and adverb-adjective phrases.", "labels": [], "entities": []}], "datasetContent": [{"text": "Baroni and Zamparelli (2010) introduced the idea of using a rank evaluation to assess the performance of different composition models.", "labels": [], "entities": []}, {"text": "illustrates the process of evaluating two composed representations, p 1 and p 2 , of the compound apple tree, produced by two distinct composition functions, f 1 and f 2 . In the simplified setup of, the original vectors, depicted using solid blue arrows, are v, the vector of tree, and\u02dcpand\u02dc and\u02dcp, the vector of apple tree.", "labels": [], "entities": []}, {"text": "p 1 and p 2 , the representations composed using f 1 and f 2 , are depicted using dashed orange arrows.", "labels": [], "entities": []}, {"text": "p 1 's evaluation proceeds as follows: First, p 1 is compared, in terms of cosine similarity, to the original representations of all words and compounds in the dictionary.", "labels": [], "entities": []}, {"text": "The original vectors are then sorted such that the most similar vectors are first.", "labels": [], "entities": []}, {"text": "In, v, the vector of tree, is closer top 1 tha\u00f1 p, the original vector of apple tree.", "labels": [], "entities": []}, {"text": "The rank assigned to a composed representation is the position of the corresponding original vector in the similarity-sorted list.", "labels": [], "entities": []}, {"text": "In p 1 's case, the rank is 2 because the original representation of apple tree, \u02dc p, was second in the ordering.", "labels": [], "entities": []}, {"text": "The same procedure is then performed for p 2 . p 2 is compared to the original vectors\u02dcpvectors\u02dc vectors\u02dcp and v and assigned the rank 1, because\u02dcpbecause\u02dc because\u02dcp, the original vector for apple tree, is its nearest neighbor.", "labels": [], "entities": []}, {"text": "Herein lies the problem: Although p 1 is closer to the original representatio\u00f1 p than p 2 , p 1 's rank is worse.", "labels": [], "entities": []}, {"text": "This is because the vector of reference is the composed representation, which changes from one composition function to the other.", "labels": [], "entities": []}, {"text": "formulation of the rank assignment procedure can lead to situations where composed representations rank better even as the distance between the composed and the original vector increases, as illustrated in.", "labels": [], "entities": [{"text": "rank assignment", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7003771811723709}]}, {"text": "We propose a simple fix for this issue: We compute all the cosine similarities with respect t\u00f5t\u00f5 p, the original representation of each compound or phrase.", "labels": [], "entities": []}, {"text": "Having a fixed reference point makes it possible to correctly assess and compare the performance of different composition models.", "labels": [], "entities": []}, {"text": "In the new formulation p 1 is correctly judged to be the better composed representation and assigned rank 1, whereas p 2 is assigned rank 2.", "labels": [], "entities": []}, {"text": "Composition models are evaluated on the test split of each data set.", "labels": [], "entities": []}, {"text": "First, the rank of each composed representation is determined using the procedure described above.", "labels": [], "entities": []}, {"text": "Then the ranks of all the test set entries are sorted.", "labels": [], "entities": []}, {"text": "We report the first (Q 1 ), second (Q 2 ), and third (Q 3 ) quartile, where Q 2 is the median of the sorted rank list, Q 1 is the median of the first half and Q 3 is the median of the second half of the list.", "labels": [], "entities": []}, {"text": "We report two additional performance metrics: the average cosine distance (cos-d) between the composed and the original representations of each test set entry and the percentage of test compounds with rank \u22645.", "labels": [], "entities": [{"text": "cosine distance (cos-d)", "start_pos": 58, "end_pos": 81, "type": "METRIC", "confidence": 0.8153066635131836}]}, {"text": "Typically, a composed representation can have close neighbors that are different but semantically similar to the composed phrase.", "labels": [], "entities": []}, {"text": "A rank \u22645 indicates a well-built representation, compatible with the neighborhood of the original phrase.", "labels": [], "entities": []}, {"text": "Another particularity of our evaluation is that the ranks are computed against the full vocabulary of each embedding set.", "labels": [], "entities": []}, {"text": "This is in contrast to previous evaluations ( where the rank was computed against a restricted dictionary containing only the words and phrases in the data set.", "labels": [], "entities": []}, {"text": "A restricted dictionary makes the evaluation easier because many similar words are excluded.", "labels": [], "entities": []}, {"text": "In our case, similar words from the entire original vector space can become foils for the composition model, even if they are not part of the data set.", "labels": [], "entities": []}, {"text": "For example, the English compounds data set has a restricted vocabulary of 25,807 words, whereas the full vocabulary contains 270,941 words.: Different weighting variations evaluated on the compounds data set (32,246 nominal compounds).", "labels": [], "entities": [{"text": "English compounds data set", "start_pos": 17, "end_pos": 43, "type": "DATASET", "confidence": 0.8347924500703812}, {"text": "compounds data set", "start_pos": 190, "end_pos": 208, "type": "DATASET", "confidence": 0.8012743592262268}]}, {"text": "All variations use t = 100 transformations, word representations with n = 200 dimensions and the dropout rate that was observed to work best on the dev dataset (see for details).", "labels": [], "entities": []}, {"text": "Results on the 6,442 compounds in the test set of the German compounds data set.", "labels": [], "entities": [{"text": "German compounds data set", "start_pos": 54, "end_pos": 79, "type": "DATASET", "confidence": 0.9455915540456772}]}], "tableCaptions": [{"text": " Table 2: Overview of the composition data sets evaluated in this paper. Reports the total number of  phrases (Phrases), the number of unique words in the first (w1) and second (w2) position, as well as  the number of unique words in either position (w1 & w2) for each language, phrase type pair.", "labels": [], "entities": []}, {"text": " Table 3: Different weighting variations evaluated on the compounds data set (32,246 nominal  compounds). All variations use t = 100 transformations, word representations with n = 200  dimensions and the dropout rate that was observed to work best on the dev dataset (see Appendix 6  for details). Results on the 6,442 compounds in the test set of the German compounds data set.", "labels": [], "entities": [{"text": "compounds data set", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.8160162369410197}, {"text": "German compounds data set", "start_pos": 352, "end_pos": 377, "type": "DATASET", "confidence": 0.9438402205705643}]}, {"text": " Table 4: Results for English, German, and Dutch on the composition of nominal compounds, adjective- noun phrases, and adverb-adjective phrases.", "labels": [], "entities": []}, {"text": " Table 5: Results using the", "labels": [], "entities": []}]}