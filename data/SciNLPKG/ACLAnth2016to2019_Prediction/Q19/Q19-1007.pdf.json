{"title": [{"text": "Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a novel geometric approach for learning bilingual mappings given monolin-gual embeddings and a bilingual dictionary.", "labels": [], "entities": []}, {"text": "Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings.", "labels": [], "entities": []}, {"text": "Overall , we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds.", "labels": [], "entities": []}, {"text": "Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks.", "labels": [], "entities": [{"text": "cross-lingual word similarity tasks", "start_pos": 97, "end_pos": 132, "type": "TASK", "confidence": 0.6712121218442917}]}, {"text": "We next generalize our framework to represent multiple languages in a common latent space.", "labels": [], "entities": []}, {"text": "Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs.", "labels": [], "entities": []}, {"text": "We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting.", "labels": [], "entities": [{"text": "word translation", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7775427103042603}]}], "introductionContent": [{"text": "Bilingual word embeddings area useful tool in natural language processing (NLP) that has attracted a lot of interest lately due to a fundamental property: similar concepts/words across * This work was carried out during the author's internship at Microsoft, India.", "labels": [], "entities": [{"text": "Bilingual word embeddings", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7287112474441528}, {"text": "natural language processing (NLP)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7678278783957163}]}, {"text": "different languages are mapped close to each other in a common embedding space.", "labels": [], "entities": []}, {"text": "Hence, they are useful for joint/transfer learning and sharing annotated data across languages in different NLP applications such as machine translation (, building bilingual dictionaries (), mining parallel corpora (), text classification (), sentiment analysis (, and dependency parsing (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.824412077665329}, {"text": "text classification", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.7980151176452637}, {"text": "sentiment analysis", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.9690904319286346}, {"text": "dependency parsing", "start_pos": 270, "end_pos": 288, "type": "TASK", "confidence": 0.8333511650562286}]}, {"text": "empirically show that a linear transformation of embeddings from one language to another preserves the geometric arrangement of word embeddings.", "labels": [], "entities": []}, {"text": "Ina supervised setting, the transformation matrix, W, is learned given a small bilingual dictionary and their corresponding monolingual embeddings.", "labels": [], "entities": []}, {"text": "Subsequently, many refinements to the bilingual mapping framework have been proposed.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel geometric approach for learning bilingual embeddings.", "labels": [], "entities": []}, {"text": "We rotate the source and target language embeddings from their original vector spaces to a common latent space via language-specific orthogonal transformations.", "labels": [], "entities": []}, {"text": "Furthermore, we define a similarity metric, the Mahalanobis metric, in this common space to refine the notion of similarity between a pair of embeddings.", "labels": [], "entities": []}, {"text": "We achieve the above by learning the transformation matrix as follows: W = Ut BU s , where Ut and U s are the orthogonal transformations for target and source language embeddings, respectively, and B is a positive definite matrix representing the Mahalanobis metric.", "labels": [], "entities": []}, {"text": "The proposed formulation has the following benefits: \u2022 The learned similarity metric allows fora more effective similarity comparison of embeddings based on evidence from the data.", "labels": [], "entities": []}, {"text": "\u2022 A common latent space decouples the source and target language transformations, and naturally enables representation of word embeddings from both languages in a single vector space.", "labels": [], "entities": []}, {"text": "\u2022 We also show that the proposed method can be easily generalized to jointly learn multilingual embeddings, given bilingual dictionaries of multiple language pairs.", "labels": [], "entities": []}, {"text": "We map multiple languages into a single vector space by learning the characteristics common across languages (the similarity metric) as well as language-specific attributes (the orthogonal transformations).", "labels": [], "entities": []}, {"text": "The optimization problem resulting from our formulation involves orthogonal constraints on language-specific transformations (U i for language i) as well as the symmetric positivedefinite constraint on the metric B.", "labels": [], "entities": []}, {"text": "Instead of solving the optimization problem in the Euclidean space with constraints, we view it as an optimization problem in smooth Riemannian manifolds, which are well-studied topological spaces.", "labels": [], "entities": []}, {"text": "The Riemannian optimization framework embeds the given constraints into the search space and conceptually views the problem as an unconstrained optimization problem over the manifold.", "labels": [], "entities": []}, {"text": "We evaluate our approach on different bilingual as well as multilingual tasks across multiple languages and datasets.", "labels": [], "entities": []}, {"text": "The following is a summary of our findings: \u2022 Our approach outperforms state-of-the-art supervised and unsupervised bilingual mapping methods on the bilingual lexicon induction as well as the cross-lingual word similarity tasks.", "labels": [], "entities": [{"text": "cross-lingual word similarity tasks", "start_pos": 192, "end_pos": 227, "type": "TASK", "confidence": 0.681005522608757}]}, {"text": "\u2022 An ablation analysis reveals that the following contribute to our model's improved performance: (a) aligning the embedding spaces of different languages, (b) learning a similarity metric which induces a latent space, (c) performing inference in the induced latent space, and (d) formulating the tasks as a classification problem.", "labels": [], "entities": [{"text": "formulating", "start_pos": 281, "end_pos": 292, "type": "TASK", "confidence": 0.9741607308387756}]}, {"text": "\u2022 We evaluate our multilingual model on an indirect word translation task: translation between a language pair that does not have a bilingual dictionary, but the source and target languages each possess a bilingual dictionary with a third, common pivot language.", "labels": [], "entities": [{"text": "word translation", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7542327642440796}]}, {"text": "Our multilingual model outperforms a strong unsupervised baseline as well as methods based on adapting bilingual methods for this indirect translation task.", "labels": [], "entities": [{"text": "indirect translation task", "start_pos": 130, "end_pos": 155, "type": "TASK", "confidence": 0.7501121759414673}]}, {"text": "\u2022 Lastly, we propose a semi-supervised extension of our approach that further improves performance over the supervised approaches.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "The proposed framework, including problem formulations for bilingual and multilingual mappings, is presented in Section 3.", "labels": [], "entities": []}, {"text": "The proposed Riemannian optimization algorithm is described in Section 4.", "labels": [], "entities": [{"text": "Riemannian optimization", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7102918922901154}]}, {"text": "In Section 5, we discuss our experimental setup.", "labels": [], "entities": []}, {"text": "Section 6 presents the results of experiments on direct translation with our algorithms and analyzes the results.", "labels": [], "entities": [{"text": "direct translation", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.6828802824020386}]}, {"text": "Section 7 presents experiments on indirect translation using our generalized multilingual algorithm.", "labels": [], "entities": [{"text": "indirect translation", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7041678726673126}]}, {"text": "We discuss a semi-supervised extension to our framework in Section 8.", "labels": [], "entities": []}, {"text": "Section 9 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with the following one-hop translation cases: (a) fr-it-pt, (b) it-de-es, and (c) Method en-es es-en en-fr fr-en en-de de-en en-ru ru-en en-zh zh-en en-it it-en avg.", "labels": [], "entities": []}, {"text": "shows the results of the one-hop translation experiments.", "labels": [], "entities": [{"text": "one-hop translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6346041560173035}]}, {"text": "We observe that GeoMM multi outperforms pivoting methods (cmp and pip) built on top of MSF and Procrustes for all language pairs.", "labels": [], "entities": [{"text": "MSF", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9273043870925903}, {"text": "Procrustes", "start_pos": 95, "end_pos": 105, "type": "DATASET", "confidence": 0.8920815587043762}]}, {"text": "It should be noted that pivoting may lead to cascading of errors in the solution, whereas learning a common embedding space jointly mitigates this disadvantage.", "labels": [], "entities": []}, {"text": "This is reaffirmed by ourobservation that GeoMM multi performs significantly better than GeoMM (cmp) and GeoMM (pip).", "labels": [], "entities": []}, {"text": "Since unsupervised methods have been shown to be competitive with supervised methods, they can bean alternative to pivoting.", "labels": [], "entities": []}, {"text": "Indeed, we observe that the unsupervised method SL-unsup is better than the pivoting methods, although it used no bilingual dictionaries.", "labels": [], "entities": []}, {"text": "On the other hand, GeoMM multi is better than the unsupervised methods too.", "labels": [], "entities": []}, {"text": "It should be noted that the unsupervised methods use much larger vocabulary than GeoMM multi during the training stage.", "labels": [], "entities": []}, {"text": "We consider the BLI task from language L src to language L tgt in the absence of a bilingual lexicon between them.", "labels": [], "entities": [{"text": "BLI", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.5812174677848816}]}, {"text": "We, however, assume the Method fr-it-pt it-de-es es-pt-fr avg.", "labels": [], "entities": []}, {"text": "As discussed in Section 3.4, our framework allows the flexibility to jointly learn the common latent space of multiple languages, given bilingual dictionaries of multiple language pairs.", "labels": [], "entities": []}, {"text": "Our multilingual approach, GeoMM multi , views this setting as a graph with three nodes {L src , L tgt , L pvt } and two edges {L src -L pvt , L pvt -L tgt } (dictionaries).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Precision@1 for BLI on the VecMap  dataset. The results of MSF-ISF, SL-unsup, CCA- NN (Faruqui and Dyer, 2014), and Adv-Refine  are reported by Artetxe et al. (2018b). CCA-NN  employs nearest neighbor retrieval procedure. The  results of GPA are reported by Kementchedjhieva  et al. (2018).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9712381362915039}, {"text": "BLI", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9603761434555054}, {"text": "VecMap  dataset", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9675942957401276}, {"text": "GPA", "start_pos": 248, "end_pos": 251, "type": "METRIC", "confidence": 0.8076224327087402}]}, {"text": " Table 3: Ablation test results: Precision@1 for BLI  on the VecMap dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9950934648513794}, {"text": "Precision@1", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9601344267527262}, {"text": "BLI", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9878168702125549}, {"text": "VecMap dataset", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.9763914048671722}]}, {"text": " Table 4: Pearson correlation coefficient for the  SemEval 2017 cross-lingual word similarity task.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.9321554104487101}, {"text": "SemEval 2017 cross-lingual word similarity task", "start_pos": 51, "end_pos": 98, "type": "TASK", "confidence": 0.8425170083840688}]}, {"text": " Table 5: Results on the monolingual word analogy  task.", "labels": [], "entities": [{"text": "monolingual word analogy  task", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6887999698519707}]}, {"text": " Table 6: Indirect translation: Precision@1 for BLI.", "labels": [], "entities": [{"text": "BLI", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.8058266043663025}]}, {"text": " Table 7: Comparison of GeoMM and GeoMM semi with RCSLS (Joulin et al., 2018). Precision@1 for  BLI is reported. The results of RCSLS are reported in the original paper. The results of language pairs  en-it and it-en are on the VecMap dataset, while others are on the MUSE dataset.", "labels": [], "entities": [{"text": "Precision@1", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9543452660242716}, {"text": "BLI", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.8587135076522827}, {"text": "VecMap dataset", "start_pos": 228, "end_pos": 242, "type": "DATASET", "confidence": 0.9841546714305878}, {"text": "MUSE dataset", "start_pos": 268, "end_pos": 280, "type": "DATASET", "confidence": 0.9646930396556854}]}, {"text": " Table 8: Precision@1 for BLI on the VecMap  dataset.", "labels": [], "entities": [{"text": "Precision@1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9597074389457703}, {"text": "BLI", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9803709983825684}, {"text": "VecMap  dataset", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9744678735733032}]}]}