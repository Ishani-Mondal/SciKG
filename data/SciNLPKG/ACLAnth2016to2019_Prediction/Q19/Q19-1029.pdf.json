{"title": [{"text": "Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8204955160617828}]}], "abstractContent": [{"text": "Adversarial evaluation stress-tests a model's understanding of natural language.", "labels": [], "entities": [{"text": "Adversarial evaluation stress-tests a model's understanding of natural language", "start_pos": 0, "end_pos": 79, "type": "TASK", "confidence": 0.7535814821720124}]}, {"text": "Because past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity.", "labels": [], "entities": []}, {"text": "We propose human-in-the-loop adversarial generation, where human authors are guided to break models.", "labels": [], "entities": [{"text": "human-in-the-loop adversarial generation", "start_pos": 11, "end_pos": 51, "type": "TASK", "confidence": 0.6644207239151001}]}, {"text": "We aid the authors with interpretations of model predictions through an interactive user interface.", "labels": [], "entities": [{"text": "interpretations of model predictions", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.7458100765943527}]}, {"text": "We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.81766676902771}]}, {"text": "The resulting questions are validated via live human-computer matches: Although the questions appear ordinary to humans, they systematically stump neural and information retrieval models.", "labels": [], "entities": []}, {"text": "The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7258383184671402}]}], "introductionContent": [{"text": "Proponents of machine learning claim human parity on tasks like reading comprehension () and commonsense inference.", "labels": [], "entities": [{"text": "commonsense inference", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.8526166677474976}]}, {"text": "Despite these successes, many evaluations neglect that computers solve natural language processing (NLP) tasks in a fundamentally different way than humans.", "labels": [], "entities": [{"text": "solve natural language processing (NLP) tasks", "start_pos": 65, "end_pos": 110, "type": "TASK", "confidence": 0.725599467754364}]}, {"text": "Models can succeed without developing ''true'' language understanding, instead learning superficial patterns from crawled ( or manually annotated data sets.", "labels": [], "entities": []}, {"text": "Thus, recent work stress-tests models via adversarial evaluation: elucidating a system's capabilities by exploiting its weaknesses (.", "labels": [], "entities": []}, {"text": "Unfortunately, whereas adversarial evaluation reveals simplistic model failures (, exploring more complex failure patterns requires human involvement): Automatically modifying natural language examples without invalidating them is difficult.", "labels": [], "entities": []}, {"text": "Hence, the diversity of adversarial examples is often severely restricted.", "labels": [], "entities": []}, {"text": "Instead, our human-computer hybrid approach uses human creativity to generate adversarial examples.", "labels": [], "entities": []}, {"text": "A user interface presents model interpretations and helps users craft model-breaking examples (Section 3).", "labels": [], "entities": []}, {"text": "We apply this to a question answering (QA) task called Quizbowl, where trivia enthusiasts-who write questions for academic competitions-create diverse examples that stump existing QA models.", "labels": [], "entities": [{"text": "question answering (QA) task", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.8674657742182413}]}, {"text": "The adversarially authored test set is nonetheless as easy as regular questions for humans (Section 4), but the relative accuracy of strong QA models drops as much as 40% (Section 5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9990411400794983}]}, {"text": "We also host live human vs. computer matches-where models typically defeat top human teams-but observe spectacular model failures on adversarial questions.", "labels": [], "entities": []}, {"text": "Analyzing the adversarial edits uncovers phenomena that humans can solve but computers cannot (Section 6), validating that our framework uncovers creative, targeted adversarial edits (Section 7).", "labels": [], "entities": []}, {"text": "Our resulting adversarial data set presents a fun, challenging, and diverse resource for future QA research: A system that masters it will demonstrate more robust language understanding.: Adversarial evaluation in NLP typically focuses on a specific phenomenon (e.g., word replacements) and then generates the corresponding examples (top).", "labels": [], "entities": [{"text": "word replacements)", "start_pos": 268, "end_pos": 286, "type": "TASK", "confidence": 0.7891695400079092}]}, {"text": "Consequently, adversarial examples are limited to the diversity of what the underlying generative model or perturbation rule can produce-and also require downstream human evaluation to ensure validity.", "labels": [], "entities": []}, {"text": "Our setup (bottom) instead has human-authored examples, using human-computer collaboration to craft adversarial examples with greater diversity.", "labels": [], "entities": []}], "datasetContent": [{"text": "Adversarial examples () often reveal model failures better than traditional test sets.", "labels": [], "entities": []}, {"text": "However, automatic adversarial generation is tricky for NLP (e.g., by replacing words) without changing an example's meaning or invalidating it.", "labels": [], "entities": [{"text": "automatic adversarial generation", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.6762822469075521}]}, {"text": "Recent work sidesteps this by focusing on simple transformations that preserve meaning.", "labels": [], "entities": []}, {"text": "For instance, generate adversarial perturbations such as replacing What has \u2192 What's.", "labels": [], "entities": []}, {"text": "Other minor perturbations such as typos, adding distractor sentences (, or character replacements ( preserve meaning while degrading model performance.", "labels": [], "entities": [{"text": "character replacements", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.6989893615245819}]}, {"text": "Generative models can discover more adversarial perturbations but require post hoc human verification of the examples.", "labels": [], "entities": []}, {"text": "For example, neural paraphrase or language models can generate syntax modifications (, plausible captions (, or NLI premises ( . These methods improve examplelevel diversity but mainly target a specific phenomenon, (e.g., rewriting question syntax).", "labels": [], "entities": []}, {"text": "Furthermore, existing adversarial perturbations are restricted to sentences-not the paragraph inputs of Quizbowl and other tasks-due to challenges in long-text generation.", "labels": [], "entities": [{"text": "long-text generation", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.7963134050369263}]}, {"text": "For instance, syntax paraphrase networks () applied to Quizbowl only yield valid paraphrases 3% of the time (Appendix A).", "labels": [], "entities": [{"text": "Quizbowl", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.9194990992546082}, {"text": "Appendix", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9726477265357971}]}, {"text": "This section evaluates QA systems on the adversarially authored questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9751278758049011}]}, {"text": "We test three models: the IR and RNN models shown in the interface, as well as a Deep Averaging Network ( to evaluate the transferability of the adversarial questions.", "labels": [], "entities": []}, {"text": "We break our study into two rounds.", "labels": [], "entities": []}, {"text": "The first round consists of adversarially authored questions written against the IR system Finally, we also hold live competitions that pit the state-of-the-art Studio Ousia model () against human teams (Section 5.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The topical diversity of the questions  in the adversarially authored data set based on a  random sample of 100 questions.", "labels": [], "entities": []}, {"text": " Table 2: The adversarially authored questions  have similar n-gram overlap to the regular test  questions. However, the overlap of the named enti- ties (NE) decreases for IR Adversarial questions.", "labels": [], "entities": [{"text": "overlap", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9644210934638977}, {"text": "enti- ties (NE)", "start_pos": 142, "end_pos": 157, "type": "METRIC", "confidence": 0.9330583612124125}, {"text": "IR Adversarial questions", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.8627902269363403}]}, {"text": " Table 3: A breakdown of the phenomena in the  adversarially authored data set.", "labels": [], "entities": []}]}