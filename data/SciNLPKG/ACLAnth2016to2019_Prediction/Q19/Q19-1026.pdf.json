{"title": [{"text": "Natural Questions: A Benchmark for Question Answering Research", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.79249507188797}]}], "abstractContent": [{"text": "We present the Natural Questions corpus, a question answering data set.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7158426940441132}]}, {"text": "Questions consist of real anonymized, aggregated queries issued to the Google search engine.", "labels": [], "entities": []}, {"text": "An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates along answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present.", "labels": [], "entities": []}, {"text": "The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data.", "labels": [], "entities": []}, {"text": "We present experiments validating quality of the data.", "labels": [], "entities": []}, {"text": "We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task.", "labels": [], "entities": []}, {"text": "We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.", "labels": [], "entities": [{"text": "question answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8348194062709808}]}], "introductionContent": [{"text": "In recent years there has been dramatic progress in machine learning approaches to problems such as machine translation, speech recognition, and image recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.817713588476181}, {"text": "speech recognition", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7857609391212463}, {"text": "image recognition", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.8051040768623352}]}, {"text": "One major factor in these successes has been the development of neural methods that far exceed the performance of previous approaches.", "labels": [], "entities": []}, {"text": "A second major factor has been the existence of large quantities of training data for these systems.", "labels": [], "entities": []}, {"text": "Open-domain question answering (QA) is a benchmark task in natural language understanding (NLU), which has significant utility to users, and in addition is potentially a challenge task that can drive the development of methods for NLU.", "labels": [], "entities": [{"text": "Open-domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7655123919248581}, {"text": "natural language understanding (NLU)", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.8314155538876852}]}, {"text": "Several pieces of recent work have introduced QA data sets (e.g.,.", "labels": [], "entities": []}, {"text": "However, in contrast to tasks where it is relatively easy to gather naturally occurring examples, 1 the definition of a suitable QA task, and the development of a methodology for annotation and evaluation, is challenging.", "labels": [], "entities": []}, {"text": "Key issues include the methods and sources used to obtain questions; the methods used to annotate and collect answers; the methods used to measure and ensure annotation quality; and the metrics used for evaluation.", "labels": [], "entities": []}, {"text": "For more discussion of the limitations of previous work with respect to these issues, see Section 2 of this paper.", "labels": [], "entities": []}, {"text": "This paper introduces Natural Questions 2 (NQ), anew data set for QA research, along with methods for QA system evaluation.", "labels": [], "entities": [{"text": "QA research", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.8708052933216095}, {"text": "QA system evaluation", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8317567507425944}]}, {"text": "Our goals are three-fold: 1) To provide large-scale end-to-end training data for the QA problem.", "labels": [], "entities": [{"text": "QA problem", "start_pos": 85, "end_pos": 95, "type": "TASK", "confidence": 0.9100234508514404}]}, {"text": "2) To provide a data set that drives research in natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.6525155504544576}]}, {"text": "3) To study human performance in providing QA annotations for naturally occurring questions.", "labels": [], "entities": [{"text": "QA annotations for naturally occurring questions", "start_pos": 43, "end_pos": 91, "type": "TASK", "confidence": 0.7882635593414307}]}, {"text": "In brief, our annotation process is as follows.", "labels": [], "entities": []}, {"text": "An annotator is presented with a (question, Wikipedia page) pair.", "labels": [], "entities": []}, {"text": "The annotator returns a (long answer, short answer) pair.", "labels": [], "entities": []}, {"text": "The long answer (l) can bean HTML bounding box on the Wikipedia page-typically a paragraph or table-that contains the information required to answer the question.", "labels": [], "entities": []}, {"text": "Alternatively, the annotator can return l = NULL if there is no answer on the page, or if the information required to answer the question is spread across many paragraphs.", "labels": [], "entities": [{"text": "NULL", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9739258885383606}]}, {"text": "The short answer (s) can be a span or set of spans (typically entities) within l that answer the question, a boolean yes or no answer, or NULL.", "labels": [], "entities": []}, {"text": "If l = NULL then s = NULL, necessarily.", "labels": [], "entities": [{"text": "NULL", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8680484890937805}]}, {"text": "Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine.", "labels": [], "entities": []}, {"text": "Simple heuristics are used to filter questions from the query stream.", "labels": [], "entities": []}, {"text": "Thus the questions are ''natural'' in that they represent real queries from people seeking information.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes evaluation of the quality of the human annotations in our data.", "labels": [], "entities": []}, {"text": "We use a combination of two methods: 1) post hoc evaluation of correctness of non-null answers, under consensus judgments from four ''experts'';  We now describe the process for deriving ''expert'' judgments of answer correctness.", "labels": [], "entities": []}, {"text": "We used four experts for these judgments.", "labels": [], "entities": []}, {"text": "These experts had prepared the guidelines for the annotation process.", "labels": [], "entities": []}, {"text": "Ina first phase each of the four experts independently annotated examples for correctness.", "labels": [], "entities": []}, {"text": "Ina second phase the four experts met to discuss disagreements in judgments, and to reach a single consensus judgment for each example.", "labels": [], "entities": []}, {"text": "A key step is to define the criteria used to determine correctness of an example.", "labels": [], "entities": []}, {"text": "Given a triple (l, q, d), we extracted the passage l \ud97b\udf59 corresponding to l on the page d.", "labels": [], "entities": []}, {"text": "The pair (q, l \ud97b\udf59 ) was then presented to the expert.", "labels": [], "entities": []}, {"text": "Experts categorized (q, l \ud97b\udf59 ) pairs into the following three categories: Correct (C): It is clear beyond a reasonable doubt that the answer is correct.", "labels": [], "entities": [{"text": "Correct (C)", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9359600394964218}]}, {"text": "The first four authors of this paper.", "labels": [], "entities": []}, {"text": "Correct (but debatable) (C d ): A reasonable person could be satisfied by the answer; however, a reasonable person could raise a reasonable doubt about the answer.", "labels": [], "entities": []}, {"text": "Wrong (W): There is not convincing evidence that the answer is correct.", "labels": [], "entities": [{"text": "Wrong (W)", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7958895415067673}]}, {"text": "We introduced the intermediate Cd category after observing that many (q, l \ud97b\udf59 ) pairs are high quality answers, but raise some small doubt or quibble about whether they fully answer the question.", "labels": [], "entities": []}, {"text": "The use of the word ''debatable'' is intended to be literal: (q, l \ud97b\udf59 ) pairs falling into the Cd category could literally lead to some debate between reasonable people as to whether they fully answer the question or not.", "labels": [], "entities": []}, {"text": "Given this background, we will make the following assumption: Answers in the Cd category should be very useful to a user interacting with a QA system, and should be considered to be high-quality answers; however, an annotator would be justified in either annotating or not annotating the example.", "labels": [], "entities": []}, {"text": "For these cases there is often disagreement between annotators as to whether the page contains an answer or not: We will see evidence of this when we consider the 25-way annotations.", "labels": [], "entities": []}, {"text": "NQ includes 5-way annotations on 7,830 items for development data, and we will sequester a further 7,842 items, 5-way annotated, for test data.", "labels": [], "entities": [{"text": "NQ", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9715424180030823}]}, {"text": "This section describes evaluation metrics using this data, and gives justification for these metrics.", "labels": [], "entities": []}, {"text": "We choose 5-way annotations for the following reasons: First, we have evidence that aggregating annotations from 5 annotators is likely to be much more robust than relying on a single annotator (see Section 4).", "labels": [], "entities": []}, {"text": "Second, 5 annotators is a small enough number that the cost of annotating thousands of development and test items is not prohibitive.", "labels": [], "entities": []}, {"text": "Assume that we have a model f \u03b8 with parameters \u03b8 that maps an input to along answer l = f \u03b8 (q, d j is the output from the j'th annotator, and can be a paragraph ind (i) , or can be NULL.", "labels": [], "entities": []}, {"text": "The five annotators are chosen uniformly at random from a pool of annotators.", "labels": [], "entities": []}, {"text": "We define an evaluation measure based on the five way annotations as follows.", "labels": [], "entities": []}, {"text": "If at least two out of five annotators have given a non-null long answer on the example, then the system is required to output a non-null answer that is seen at least once in the five annotations; conversely, if fewer than two annotators give a non-null long answer, the system is required to return NULL as its output.", "labels": [], "entities": [{"text": "NULL", "start_pos": 300, "end_pos": 304, "type": "METRIC", "confidence": 0.9665055274963379}]}, {"text": "To make this more formal, define the function g(a (i) ) to be the number of annotations in a (i) that are non-null.", "labels": [], "entities": []}, {"text": "Define a function h \u03b2 (a, l) that judges the correctness of label l given annotations a = a 1 . .", "labels": [], "entities": []}, {"text": "a 5 . This function is parameterized by an integer \u03b2.", "labels": [], "entities": []}, {"text": "The function returns 1 if the label l is judged to be correct, and 0 otherwise: We used \u03b2 = 2 in our experiments.", "labels": [], "entities": []}, {"text": "The accuracy of a model is then The value for A \u03b2 is an estimate of accuracy with respect to the underlying distribution, which we define as \u00af Here the expectation is taken with respect hence the annotations a 1 . .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999144434928894}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9991390705108643}]}, {"text": "a 5 are assumed to be drawn IID from p(l|q, d).", "labels": [], "entities": [{"text": "IID", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9117172360420227}]}, {"text": "We discuss this measure at length in this section.", "labels": [], "entities": []}, {"text": "First, however, we make the following critical point: It is possible fora model trained on (l (i) , q (i) , d ) triples drawn IID from p(l, q, d) to exceed the performance of a single annotator on this measure.", "labels": [], "entities": []}, {"text": "In particular, if we have a model p(l|q, d; \u03b8), trained on (l, q, d) triples, which is a good approximation to p(l|q, d), it is then possible to use p(l|q, d; \u03b8) to make predictions that outperform a single random draw from p(l|q, d).", "labels": [], "entities": []}, {"text": "The Bayes optimal hypothesis (see for , is a function of the posterior distribution p(\u00b7|q, d), and will generally exceed the performance of a single random annotation, We also show this empirically, by constructing an approximation to p(l|q, d) from 20-way annotations, then using this approximation to make predictions that significantly outperform a single annotator.", "labels": [], "entities": []}, {"text": "for \u03bc (i) < 0.4 over 35% (11/17 annotations) are in the W category.", "labels": [], "entities": []}, {"text": "10 This isn't quite accurate as the annotators are sampled without replacement; however, it simplifies the analysis.", "labels": [], "entities": []}, {"text": "Specifically, for an input , and to output NULL otherwise.", "labels": [], "entities": [{"text": "NULL", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.8660517930984497}]}, {"text": "Implementation of this strategy is straightforward if \u03b3 and \u00af \u03b3 are known; this strategy will in general give a higher accuracy value than taking a single sample l from p(l|q, d) and using this sample as the prediction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9984546899795532}]}, {"text": "In principle a model p(l|q, d; \u03b8) trained on (l, q, d) triples can converge to a good estimate of \u03b3 and \u00af \u03b3.", "labels": [], "entities": []}, {"text": "Note that for the special case \u03b3 + \u00af \u03b3 = 1 we have It follows that the Bayes optimal hypothesis is to predict l * if \u03b3 \u2265 \u03b1 where \u03b1 \u2248 0.31381, and to predict NULL otherwise.", "labels": [], "entities": []}, {"text": "Precision and Recall During evaluation, it is often beneficial to separately measure false positives (incorrectly predicting an answer), and false negatives (failing to predict an answer).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9890718460083008}, {"text": "Recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.988466739654541}]}, {"text": "We define the precision (P ) and recall (R) off \u03b8 :", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 14, "end_pos": 28, "type": "METRIC", "confidence": 0.954552948474884}, {"text": "recall (R)", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9579467475414276}]}], "tableCaptions": [{"text": " Table 3: Precision (P), recall (R), and the harmonic mean of these (F1) of all baselines, a single annotator, and", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9518211036920547}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9626806974411011}, {"text": "harmonic mean of these (F1)", "start_pos": 45, "end_pos": 72, "type": "METRIC", "confidence": 0.6904566330569131}]}]}