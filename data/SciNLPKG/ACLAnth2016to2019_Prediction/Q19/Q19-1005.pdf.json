{"title": [], "abstractContent": [{"text": "Lexicalized parsing models are based on the assumptions that (i) constituents are organized around a lexical head and (ii) bilexical statistics are crucial to solve ambiguities.", "labels": [], "entities": [{"text": "Lexicalized parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.770218014717102}]}, {"text": "In this paper, we introduce an unlexicalized transition-based parser for discontinuous constituency structures , based on a structure-label transition system and a bi-LSTM scoring system.", "labels": [], "entities": []}, {"text": "We compare it with lexicalized parsing models in order to address the question of lexical-ization in the context of discontinuous constituency parsing.", "labels": [], "entities": [{"text": "discontinuous constituency parsing", "start_pos": 116, "end_pos": 150, "type": "TASK", "confidence": 0.6377077500025431}]}, {"text": "Our experiments show that unlexicalized models systematically achieve higher results than lexicalized models, and provide additional empirical evidence that lex-icalization is not necessary to achieve strong parsing results.", "labels": [], "entities": []}, {"text": "Our best unlexicalized model sets anew state of the art on English and German discontinuous constituency treebanks.", "labels": [], "entities": [{"text": "English and German discontinuous constituency treebanks", "start_pos": 59, "end_pos": 114, "type": "DATASET", "confidence": 0.5617967297633489}]}, {"text": "We further provide a per-phenomenon analysis of its errors on discontinuous constituents.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper introduces an unlexicalized parsing model and addresses the question of lexicalization, as a parser design choice, in the context of transition-based discontinuous constituency parsing.", "labels": [], "entities": [{"text": "transition-based discontinuous constituency parsing", "start_pos": 144, "end_pos": 195, "type": "TASK", "confidence": 0.5899591743946075}]}, {"text": "Discontinuous constituency trees are constituency trees where crossing arcs are allowed in order to represent long-distance dependencies, and in general phenomena related to word order variations (e.g., the left dislocation in).", "labels": [], "entities": []}, {"text": "Lexicalized parsing models) are based on the assumptions that (i) constituents are organized around a lexical * Work partly done at Universit\u00e9 Paris Diderot.", "labels": [], "entities": [{"text": "Lexicalized parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7722237706184387}]}, {"text": "head and (ii) bilexical statistics are crucial to solve ambiguities.", "labels": [], "entities": []}, {"text": "Ina lexicalized Probabilistic ContextFree Grammar (PCFG), grammar rules involve nonterminals annotated with a terminal element that represents their lexical head, for example: The probability of such a rule models the likelihood that telescope is a suitable modifier for saw.", "labels": [], "entities": []}, {"text": "In contrast, unlexicalized parsing models renounce modeling bilexical statistics, based on the assumptions that they are too sparse to be estimated reliably.", "labels": [], "entities": []}, {"text": "Indeed, observed that removing bilexical statistics from Collins' (1997) model lead to at most a 0.5 drop in F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 109, "end_pos": 116, "type": "METRIC", "confidence": 0.9951061010360718}]}, {"text": "Furthermore, showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs.", "labels": [], "entities": []}, {"text": "Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (.", "labels": [], "entities": []}, {"text": "They sometimes rely on structural information, such as the boundaries of constituents (.", "labels": [], "entities": []}, {"text": "Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers.", "labels": [], "entities": []}, {"text": "We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions.", "labels": [], "entities": []}, {"text": "Head assignment is typically performed with REDUCE-RIGHT and REDUCE-LEFT actions.", "labels": [], "entities": [{"text": "Head assignment", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8765362501144409}, {"text": "REDUCE-RIGHT", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.9577307105064392}, {"text": "REDUCE-LEFT", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9328786134719849}]}, {"text": "Most proposals in transition-based constituency parsing since have used a lexicalized transition system, and features involving heads to score actions (, among others), including proposals for discontinuous constituency parsing.", "labels": [], "entities": [{"text": "transition-based constituency parsing", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.580968846877416}, {"text": "discontinuous constituency parsing", "start_pos": 193, "end_pos": 227, "type": "TASK", "confidence": 0.636672576268514}]}, {"text": "A few recent proposals use an unlexicalized model (.", "labels": [], "entities": []}, {"text": "Interestingly, these latter models all use recurrent neural networks (RNN) to compute constituent representations.", "labels": [], "entities": []}, {"text": "Our contributions are the following.", "labels": [], "entities": []}, {"text": "We introduce an unlexicalized discontinuous parsing model, as well as its lexicalized counterpart.", "labels": [], "entities": []}, {"text": "We evaluate them in identical experimental conditions.", "labels": [], "entities": []}, {"text": "Our main finding is that, in our experiments, unlexicalized models consistently outperform lexicalized models.", "labels": [], "entities": []}, {"text": "We assess the robustness of this result by performing the comparison of unlexicalized and lexicalized models with a second pair of transition systems.", "labels": [], "entities": []}, {"text": "We further analyze the empirical properties of the systems in order to better understand the reasons for this performance difference.", "labels": [], "entities": []}, {"text": "We find that the unlexicalized system oracle produces shorter, more incremental derivations.", "labels": [], "entities": []}, {"text": "Finally, we provide a per-phenomenon error analysis of our best model and identify which types of discontinuous constituents are hard to predict.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments we performed aim at assessing the role of lexicalization in transition-based constituency parsing.", "labels": [], "entities": [{"text": "transition-based constituency parsing", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.6280541121959686}]}, {"text": "We describe the data (Section 5.1) and the optimization protocol (Section 5.2).", "labels": [], "entities": []}, {"text": "Then, we discuss empirical runtime efficiency (Section 5.3), before presenting the results of our experiments (Section 5.4).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Discontinuous parsing results on the development sets.", "labels": [], "entities": [{"text": "Discontinuous parsing", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5841671526432037}]}, {"text": " Table 5: Running times on development sets of the Tiger and the DPTB, reported in tokens per second (tok/s) and  sentences per second (sent/s). Runtimes are only indicative; they are not comparable with those reported by other  authors, since they use different hardware.", "labels": [], "entities": [{"text": "DPTB", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.931696355342865}]}, {"text": " Table 6: Morphological analysis results on develop- ment sets.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9169925153255463}]}, {"text": " Table 6. For each  morphological attribute, we report an accuracy  score computed over every token. However, most  morphological attributes are only relevant for  specific part-of-speech tags. For instance, TENSE  is only a feature of verbs. The accuracy metric  is somewhat misleading, since the fact that the  tagger predicts correctly that a token does not  have an attribute is considered a correct answer.  Therefore, if only 5% of tokens bore a specific  morphological attribute, a 95% accuracy is a most", "labels": [], "entities": [{"text": "accuracy  score", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.9768859148025513}, {"text": "TENSE", "start_pos": 208, "end_pos": 213, "type": "METRIC", "confidence": 0.9924968481063843}, {"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9940477609634399}, {"text": "accuracy", "start_pos": 493, "end_pos": 501, "type": "METRIC", "confidence": 0.9929186701774597}]}, {"text": " Table 7: Discontinuous parsing results on the test sets.   *  Neural scoring system.  \u2020 Does not discount root symbols and punctuation.", "labels": [], "entities": []}, {"text": " Table 8: Incrementality measured by the average size of  the stack during derivations. The average is calculated  across all configurations (not across all sentences).", "labels": [], "entities": []}, {"text": " Table 9: GAP action statistics in training sets.", "labels": [], "entities": [{"text": "GAP action", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8875322341918945}]}, {"text": " Table 10: Evaluation statistics per phenomenon. G: gold occurrences; PfM: perfect match; PaM: partial match;  FN: false negatives; FP: false positives.", "labels": [], "entities": [{"text": "FN: false negatives", "start_pos": 111, "end_pos": 130, "type": "METRIC", "confidence": 0.840078204870224}, {"text": "FP", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.8673250079154968}]}]}