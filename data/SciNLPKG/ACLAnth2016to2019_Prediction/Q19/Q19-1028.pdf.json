{"title": [{"text": "Multiattentive Recurrent Neural Network Architecture for Multilingual Readability Assessment", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a multiattentive recurrent neural network architecture for automatic multilingual readability assessment.", "labels": [], "entities": [{"text": "automatic multilingual readability assessment", "start_pos": 70, "end_pos": 115, "type": "TASK", "confidence": 0.542214073240757}]}, {"text": "This architecture considers raw words as its main input, but internally captures text structure and informs its word attention process using other syntax-and morphology-related datapoints, known to be of great importance to readability.", "labels": [], "entities": []}, {"text": "This is achieved by a multiattentive strategy that allows the neural network to focus on specific parts of a text for predicting its reading level.", "labels": [], "entities": []}, {"text": "We conducted an exhaustive evaluation using data sets targeting multiple languages and prediction task types, to compare the proposed model with traditional, state-of-the-art, and other neural network strategies.", "labels": [], "entities": []}], "introductionContent": [{"text": "For decades, readability assessment has been used by diverse stakeholders-from educators to public institutions-for determining the complexity of texts.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7504333257675171}]}, {"text": "Traditional formulas do so by focusing only on superficial linguistic features (e.g., average length of sentences or syllables per word).", "labels": [], "entities": []}, {"text": "This leads to criticism, as these formulas do not explore deeper levels of text processing and thus yield rough estimates of complexity (i.e., difficulty) that often lack accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9944527745246887}]}, {"text": "In fact, traditional formulas can label a text as ''easy to read'' even if its content is completely nonsensical.", "labels": [], "entities": []}, {"text": "To improve the quality of automatic readability assessment, researchers turned to more sophisticated techniques that go beyond examining shallow features.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7727240920066833}]}, {"text": "These techniques, typically based on supervised machine learning, incorporate hundreds (even thousands) of features that describe a text from multiple perspectives: syntax, morphology, cohesion, discourse structure, and subject matter.", "labels": [], "entities": []}, {"text": "The dependency on these numerous features, however, has made readability assessment tools too complex to deploy and apply to languages beyond the one for which they were originally designed.", "labels": [], "entities": []}, {"text": "Furthermore, feature and language dependency, along with lack of homogeneity in terms of readability scales, often prevent researchers from comparing new strategies with state-of-the-art counterparts, preventing community consensus on which features are the most beneficial for capturing text complexity.", "labels": [], "entities": []}, {"text": "Existing literature reflects the fact that applications that leverage text complexity analysis, including book recommendation or categorization), Web result summarization (, and accessibility in the health domain, still favor less precise but easier to implement alternatives, with Flesch as the most accepted choice.", "labels": [], "entities": [{"text": "Web result summarization", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.5776302019755045}]}, {"text": "We argue that this is caused by the uncertainty induced by the lack of uniformity of readability scales, adaptability among readability assessment tools, and benchmarks.", "labels": [], "entities": []}, {"text": "Areas of study that were historically heavily dependent on feature engineering, including sentiment analysis or image processing), have made their way towards alternatives that do not involve manually developing features, and instead favor deep learning (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.949219673871994}, {"text": "image processing", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.6837999373674393}]}, {"text": "This resulted in more reproducible strategies-easily portable to other domains or languages, as they only require implementing the structure of a specific neural network and just rely on core components of resources, such as words, signals, or pixels, rather than features specifically designed fora domain or language.", "labels": [], "entities": []}, {"text": "Issues pertaining to readability assessment are not limited to performance and adaptability.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.9336755573749542}]}, {"text": "As stated by, a teacher should never use a readability score blindly when giving a text to a student, as specifics of the difficulties of the reader and the text should always be considered in this process.", "labels": [], "entities": []}, {"text": "For this pairing to be successful, it is imperative for readability assessment tools to provide information beyond a single score.", "labels": [], "entities": []}, {"text": "The explainability issue has been addressed in systems like by showing users the individual values of the features incorporated in the system.", "labels": [], "entities": []}, {"text": "This strategy, however, has been criticized by the education community as most features presented are not straightforward to understand for people without background in both computation and linguistics).", "labels": [], "entities": []}, {"text": "More intuitive explanations could greatly ease the use of readability tools.", "labels": [], "entities": []}, {"text": "In this paper, we present a multilingual automatic readability assessment strategy based on deep learning: Vec2Read.", "labels": [], "entities": [{"text": "Vec2Read", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.861511766910553}]}, {"text": "We still follow the premise of words being the core components fora neural network that deals with text.", "labels": [], "entities": []}, {"text": "However, in order to avoid the aforementioned domain dependency issue and adapt the architecture to the readability task, we inform our model with part of speech (POS) and morphological tags.", "labels": [], "entities": []}, {"text": "This is done by a multiattentive structure that allows the network to filter important words that influence the final complexity level estimation of a text.", "labels": [], "entities": []}, {"text": "Apart from informing the network, the multiattentive structure can also be used to offer users further insights on which parts of a text have the most influence for determining its reading level.", "labels": [], "entities": []}, {"text": "Our research contributions include the following: \u2022 We propose a multiattentive recurrent deep learning architecture specifically oriented to the readability assessment task.", "labels": [], "entities": []}, {"text": "\u2022 The proposed strategy is, to the best of our knowledge, the first capable of estimating readability in more than two languages.", "labels": [], "entities": []}, {"text": "\u2022 We incorporate an attention structure that allows a model to use multiple focuses of attention (with different degrees of importance) to inform word selection.", "labels": [], "entities": [{"text": "word selection", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.8228069841861725}]}, {"text": "\u2022 We conduct an exhaustive evaluation based on different languages, readability-measuring scales, and data sets of varied sizes, in order to compare the performance of Vec2Read with existing baselines, a comparison that is rarely done in this area due to lack of benchmarks.", "labels": [], "entities": []}, {"text": "\u2022 We present an initial analysis on the use of attention mechanisms as a potential alternative for providing explanations for readability.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first describe model configuration.", "labels": [], "entities": []}, {"text": "We then outline data sets and baselines considered for evaluation purposes.", "labels": [], "entities": []}, {"text": "Lastly, we discuss the results of the analysis conducted to verify the overall performance of Vec2Read and showcase the validity of its attention mechanism.", "labels": [], "entities": [{"text": "Vec2Read", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.8427799940109253}]}, {"text": "We followed a 10-cross-fold validation framework for measuring the performance of each strategy considered.", "labels": [], "entities": []}, {"text": "A disjoint stratified 10% of data in SimpleWiki (includes both simple and complex) was excluded from the experiments and used for developmental and hyper-parameter tuning purposes.", "labels": [], "entities": []}, {"text": "Note that to abide by the adaptability premise intended for our model, we only tuned hyper-parameters for English.", "labels": [], "entities": []}, {"text": "Doing so allows us to understand to what extent the model can directly transfer to other languages without language-specific tuning, thus simulating a realworld scenario for tool adaptation.", "labels": [], "entities": [{"text": "tool adaptation", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.7221280783414841}]}, {"text": "To conduct fair comparisons, we used the same cross-validation folds across experiments (when possible, we used the folds made publicly available; otherwise we re-run strategies using our data and folds).", "labels": [], "entities": []}, {"text": "The only exception are experiments related to S1, for which we could only access the original data set.", "labels": [], "entities": []}, {"text": "Consequently, we compare our results with respect to those published in De.: Performance comparison among traditional, state-of-the-art, ablation strategies, and Vec2Read on different data sets.", "labels": [], "entities": [{"text": "Vec2Read", "start_pos": 162, "end_pos": 170, "type": "DATASET", "confidence": 0.8315339088439941}]}, {"text": "'*' denotes statistically significant improvement over counterparts (Flesch, S1, S2, S3, Vec2Read).", "labels": [], "entities": [{"text": "Flesch", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.893470823764801}]}, {"text": "Accuracy (higher is better) is reported for all data sets except for MTDE, where RMSE (lower is better) is used in order to be able to compare with S1.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9946269392967224}, {"text": "MTDE", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.907825767993927}, {"text": "RMSE", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9924600720405579}]}, {"text": "Cells marked with '-' denote that the strategy is not applicable to the data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the data sets considered in our assessment, where S and C stand for Simple and  Complex, respectively. When data sets are multilingual, texts from all languages are considered for  computing average. * Given that ground truth scores for MTDE are continuous, for illustration purposes  we reported statistics grouped in 4 levels, i.e., 0-25, 26-50, 51-75, 76-100 (original values preserved in  the experiments).", "labels": [], "entities": [{"text": "MTDE", "start_pos": 261, "end_pos": 265, "type": "TASK", "confidence": 0.7901584506034851}]}, {"text": " Table 2: Performance comparison among traditional, state-of-the-art, ablation strategies, and Vec2Read  on different data sets. '*' denotes statistically significant improvement over counterparts (Flesch, S1,  S2, S3, Vec2Read). Accuracy (higher is better) is reported for all data sets except for MTDE, where  RMSE (lower is better) is used in order to be able to compare with S1. Cells marked with '-' denote  that the strategy is not applicable to the data set.", "labels": [], "entities": [{"text": "Flesch", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.7206902503967285}, {"text": "Accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.998884379863739}, {"text": "MTDE", "start_pos": 299, "end_pos": 303, "type": "DATASET", "confidence": 0.8547578454017639}, {"text": "RMSE", "start_pos": 312, "end_pos": 316, "type": "METRIC", "confidence": 0.9770070314407349}]}, {"text": " Table 4: Weights learned by Vec2Read for each of the data sets considered.", "labels": [], "entities": [{"text": "Weights", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9744638800621033}, {"text": "Vec2Read", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.8459198474884033}]}]}