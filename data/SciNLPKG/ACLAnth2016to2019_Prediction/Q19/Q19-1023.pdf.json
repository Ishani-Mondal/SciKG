{"title": [{"text": "A Generative Model for Punctuation in Dependency Trees", "labels": [], "entities": []}], "abstractContent": [{"text": "Treebanks traditionally treat punctuation marks as ordinary words, but linguists have suggested that a tree's \"true\" punctuation marks are not observed (Nunberg, 1990).", "labels": [], "entities": []}, {"text": "These latent \"underlying\" marks serve to delimit or separate constituents in the syntax tree.", "labels": [], "entities": []}, {"text": "When the tree's yield is rendered as a written sentence, a string rewriting mechanism transduces the underlying marks into \"surface\" marks, which are part of the observed (surface) string but should not be regarded as part of the tree.", "labels": [], "entities": []}, {"text": "We formalize this idea in a generative model of punctuation that admits efficient dynamic programming.", "labels": [], "entities": []}, {"text": "We train it without observing the underlying marks, by locally maximizing the incomplete data likelihood (simi-larly to the EM algorithm).", "labels": [], "entities": []}, {"text": "When we use the trained model to reconstruct the tree's underlying punctuation, the results appear plausible across 5 languages, and in particular are consistent with Nunberg's analysis of English.", "labels": [], "entities": []}, {"text": "We show that our gener-ative model can be used to beat baselines on punctuation restoration.", "labels": [], "entities": [{"text": "punctuation restoration", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8417297303676605}]}, {"text": "Also, our reconstruction of a sentence's underlying punctuation lets us appropriately render the surface punctuation (via our trained underlying-to-surface mechanism) when we syntactically transform the sentence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Punctuation enriches the expressiveness of written language.", "labels": [], "entities": []}, {"text": "When converting from spoken to written language, punctuation indicates pauses or pitches; expresses propositional attitude; and is conventionally associated with certain syntactic constructions such as apposition, parenthesis, quotation, and conjunction.", "labels": [], "entities": []}, {"text": "In this paper, we present a latent-variable model of punctuation usage, inspired by the rulebased approach to English punctuation of.", "labels": [], "entities": [{"text": "punctuation usage", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8306980133056641}]}, {"text": "Training our model on English data \u21e4 Equal contribution.", "labels": [], "entities": []}, {"text": "learns rules that are consistent with Nunberg's hand-crafted rules.", "labels": [], "entities": []}, {"text": "Our system is automatic, so we use it to obtain rules for Arabic, Chinese, Spanish, and Hindi as well.", "labels": [], "entities": []}, {"text": "Moreover, our rules are stochastic, which allows us to reason probabilistically about ambiguous or missing punctuation.", "labels": [], "entities": []}, {"text": "Across the 5 languages, our model predicts surface punctuation better than baselines, as measured both by perplexity ( \u00a74) and by accuracy on a punctuation restoration task ( \u00a7 6.1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9990600943565369}, {"text": "punctuation restoration task", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.7867811322212219}]}, {"text": "We also use our model to correct the punctuation of non-native writers of English ( \u00a7 6.2), and to maintain natural punctuation style when syntactically transforming English sentences.", "labels": [], "entities": []}, {"text": "In principle, our model could also be used within a generative parser, allowing the parser to evaluate whether a candidate tree truly explains the punctuation observed in the input sentence ( \u00a78).", "labels": [], "entities": []}, {"text": "Punctuation is interesting In The Linguistics of Punctuation, argues that punctuation (in English) is more than a visual counterpart of spoken-language prosody, but forms a linguistic system that involves \"interactions of point indicators (i.e. commas, semicolons, colons, periods and dashes).\"", "labels": [], "entities": []}, {"text": "He proposes that much as in phonology, a grammar generates underlying punctuation which then transforms into the observed surface punctuation.", "labels": [], "entities": []}, {"text": "Consider generating a sentence from a syntactic grammar as follows: Hail the king [, Arthur [, who wields ,] . Although the full tree is not depicted here, some of the constituents are indicated with brackets.", "labels": [], "entities": []}, {"text": "In this underlying generated tree, each appositive NP is surrounded by commas.", "labels": [], "entities": []}, {"text": "On the surface, however, the two adjacent commas after Pendragon will now be collapsed into one, and the final comma will be absorbed into the adjacent period.", "labels": [], "entities": [{"text": "Pendragon", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.9812362194061279}]}, {"text": "Furthermore, in American English, the typographic convention is to move the final punctuation inside the quotation marks.", "labels": [], "entities": []}, {"text": "Thus a reader sees only this modified surface form of the sentence: Hail the king, Arthur Pendragon, who wields \"Excalibur.\"", "labels": [], "entities": []}, {"text": "Note that these modifications are string transformations that do not see or change the tree.", "labels": [], "entities": []}, {"text": "The resulting surface punctuation marks maybe clues to the parse tree, but (contrary to NLP convention) they should not be included as nodes in the parse tree.", "labels": [], "entities": []}, {"text": "Only the underlying marks play that role.", "labels": [], "entities": []}, {"text": "Punctuation is meaningful use question and exclamation marks as clues to sentiment.", "labels": [], "entities": []}, {"text": "Similarly, quotation marks maybe used to mark titles, quotations, reported speech, or dubious terminology (University of.", "labels": [], "entities": []}, {"text": "Because of examples like this, methods for determining the similarity or meaning of syntax trees, such as a tree kernel) or a recursive neural network, should ideally be able to consider where the underlying punctuation marks attach.", "labels": [], "entities": []}, {"text": "Punctuation is helpful Surface punctuation remains correlated with syntactic phrase structure.", "labels": [], "entities": []}, {"text": "NLP systems for generating or editing text must be able to deploy surface punctuation as human writers do.", "labels": [], "entities": []}, {"text": "Parsers and grammar induction systems benefit from the presence of surface punctuation marks).", "labels": [], "entities": []}, {"text": "It is plausible that they could do better with a linguistically informed model that explains exactly why the surface punctuation appears where it does.", "labels": [], "entities": []}, {"text": "Patterns of punctuation usage can also help identify the writer's native language (.", "labels": [], "entities": []}, {"text": "Punctuation is neglected Work on syntax and parsing tends to treat punctuation as an afterthought rather than a phenomenon governed by its own linguistic principles.", "labels": [], "entities": []}, {"text": "Treebank annotation guidelines for punctuation tend to adopt simple heuristics like \"attach to the highest possible node that preserves projectivity\" (.", "labels": [], "entities": []}, {"text": "Many dependency parsing works exclude punctuation from evaluation (, although some others retain punctuation ( \" Dale \" means \" river valley \" . \" Dale \" means \" river valley . \" root.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.8011481165885925}]}, {"text": "nsubj \" \" dobj \" \" Figure 1: The generative story of a sentence.", "labels": [], "entities": [{"text": "generative story of a sentence", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.8907486200332642}]}, {"text": "Given an unpunctuated tree T at top, at each node w 2 T , the ATTACH process stochastically attaches a left puncteme land aright puncteme r, which maybe empty.", "labels": [], "entities": [{"text": "ATTACH", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9160317182540894}]}, {"text": "The resulting tree T 0 has underlying punctuation u.", "labels": [], "entities": []}, {"text": "Each slot's punctuation u i 2 u is rewritten to xi 2 x by NOISYCHANNEL.", "labels": [], "entities": [{"text": "NOISYCHANNEL", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9324054718017578}]}, {"text": "In tasks such as word embedding induction () and machine translation (), punctuation marks are usually either removed or treated as ordinary words).", "labels": [], "entities": [{"text": "word embedding induction", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.7978268067042033}, {"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8206378817558289}]}, {"text": "Yet to us, building a parse tree on a surface sentence seems as inappropriate as morphologically segmenting a surface word.", "labels": [], "entities": []}, {"text": "In both cases, one should instead analyze the latent underlying form, jointly with recovering that form.", "labels": [], "entities": []}, {"text": "For example, the proper segmentation of English hoping is not hop-ing but hope-ing (with underlying e), and the proper segmentation of stopping is neither stopp-ing nor stop-ping but stop-ing (with only one underlying p).", "labels": [], "entities": []}, {"text": "get this right for morphology.", "labels": [], "entities": [{"text": "morphology", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.8977077603340149}]}, {"text": "We attempt to do the same for punctuation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Throughout \u00a7 \u00a74-6, we will examine the punctuation model on a subset of the Universal Dependencies (UD) version 1.4 (Nivre et al., 2016)-a collection of dependency treebanks across 47 languages with unified POS-tag and dependency label sets.", "labels": [], "entities": []}, {"text": "Each treebank has designated training, development, and test portions.", "labels": [], "entities": []}, {"text": "We experiment on Arabic, English, Chinese, Hindi, and Spanish-languages with diverse punctuation vocabularies and punctuation interaction rules, not to mention script directionality.", "labels": [], "entities": []}, {"text": "For each treebank, we use the tokenization provided by UD, and take the punctuation tokens (which maybe multi-character, such as ...) to be the tokens with the PUNCT tag.", "labels": [], "entities": [{"text": "UD", "start_pos": 55, "end_pos": 57, "type": "DATASET", "confidence": 0.81458979845047}]}, {"text": "We replace each straight double quotation mark \" with either \" or \" as appropriate, and similarly for single quotation marks.", "labels": [], "entities": []}, {"text": "We split each non-punctuation token that ends in . (such as etc.) into a shorter non-punctuation token (etc) followed by a special punctuation token called the \"abbreviation dot\" (which is distinct from a period).", "labels": [], "entities": []}, {"text": "We prepend a special punctuation mark\u02c6tomark\u02c6mark\u02c6to every sentence \u00af x, which can serve to absorb an initial comma, for example.", "labels": [], "entities": []}, {"text": "We then replace each token with the special symbol UNK if its type appeared fewer than 5 times in the training portion.", "labels": [], "entities": [{"text": "UNK", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.4653668999671936}]}, {"text": "This gives the surface sentences.", "labels": [], "entities": []}, {"text": "To estimate the vocabulary V of underlying punctemes, we simply collect all surface token sequences xi that appear at any slot in the training portion of the processed treebank.", "labels": [], "entities": []}, {"text": "This is a generous estimate.", "labels": [], "entities": []}, {"text": "Similarly, we estimate W d ( \u00a7 2.1) as all pairs (l, r) 2 V 2 that flank any d constituent.", "labels": [], "entities": []}, {"text": "Recall that our model generates surface punctuation given an unpunctuated dependency tree.", "labels": [], "entities": []}, {"text": "We train it on each of the 5 languages independently.", "labels": [], "entities": []}, {"text": "We evaluate on conditional perplexity, which will below if the trained model successfully assigns a high probability to the actual surface punctuation in a held-out corpus of the same language.", "labels": [], "entities": []}, {"text": "For en and en_esl, \" and \" are distinguished by language-specific part-of-speech tags.", "labels": [], "entities": []}, {"text": "For the other 4 languages, we identify two \" dependents of the same head word,: Statistics of our datasets.", "labels": [], "entities": []}, {"text": "\"Treebank\" is the UD treebank identifier, \"#Token\" is the number of tokens, \"%Punct\" is the percentage of punctuation tokens, \"#Omit\" is the small number of sentences containing non-leaf punctuation tokens (see footnote 19), and \"#Type\" is the number of punctuation types after preprocessing.", "labels": [], "entities": [{"text": "UD treebank identifier", "start_pos": 18, "end_pos": 40, "type": "DATASET", "confidence": 0.8885540763537089}]}, {"text": "(Recall from \u00a74 that preprocessing distinguishes between left and right quotation mark types, and between abbreviation dot and period dot types.)", "labels": [], "entities": []}, {"text": "We compare our model against three baselines to show that its complexity is necessary.", "labels": [], "entities": []}, {"text": "Our first baseline is an ablation study that does not use latent underlying punctuation, but generates the surface punctuation directly from the tree.", "labels": [], "entities": []}, {"text": "(To implement this, we fix the parameters of the noisy channel so that the surface punctuation equals the underlying with probability 1.)", "labels": [], "entities": []}, {"text": "If our full model performs significantly better, it will demonstrate the importance of a distinct underlying layer.", "labels": [], "entities": []}, {"text": "Our other two baselines ignore the tree structure, so if our full model performs significantly better, it will demonstrate that conditioning on explicit syntactic structure is useful.", "labels": [], "entities": []}, {"text": "These baselines are based on previously published approaches that reduce the problem to tagging: use a BiLSTM-CRF tagger with bigram topology; Tilk and Alum\u00e4e (2016) use a BiGRU tagger with attention.", "labels": [], "entities": []}, {"text": "In both approaches, the model is trained to tag each slot i with the correct string xi 2 V \u21e4 (possibly \u270f or\u02c6)or\u02c6).", "labels": [], "entities": []}, {"text": "These are discriminative probabilistic models (in contrast to our generative one).", "labels": [], "entities": []}, {"text": "Each gives a probability distribution over the taggings (conditioned on the unpunctuated sentence), so we can evaluate their perplexity.", "labels": [], "entities": []}, {"text": "As shown in, our full model beats the baselines in perplexity in all 5 languages.", "labels": [], "entities": []}, {"text": "Also, in 4 of 5 languages, allowing a trained NOISYCHANNEL (rather than the identity map) replacing the left one with \" and the right one with \".", "labels": [], "entities": [{"text": "NOISYCHANNEL", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.9691224694252014}]}, {"text": "13 For symmetry, we should also have added a final mark.", "labels": [], "entities": [{"text": "symmetry", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.6496091485023499}, {"text": "final mark", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9444709122180939}]}, {"text": "These methods learn word embeddings that optimize conditional log-likelihood on the punctuation restoration training data.", "labels": [], "entities": [{"text": "punctuation restoration training", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.7685740888118744}]}, {"text": "They might do better if these embeddings were shared with other tasks, as multi-task learning might lead them to discover syntactic categories of words.: Results of the conditional perplexity experiment ( \u00a74), reported as perplexity per punctuation slot, where an unpunctuated sentence of n words has n + 1 slots.", "labels": [], "entities": []}, {"text": "Column \"Attn.\" is the BiGRU tagger with attention, and \"CRF\" stands for the BiLSTM-CRF tagger.", "labels": [], "entities": []}, {"text": "\"ATTACH\" is the ablated version of our model where surface punctuation is directly attached to the nodes.", "labels": [], "entities": [{"text": "ATTACH", "start_pos": 1, "end_pos": 7, "type": "METRIC", "confidence": 0.9276984930038452}]}, {"text": "Our full model \"+NC\" adds NOISYCHANNEL to transduce the attached punctuation into surface punctuation.", "labels": [], "entities": [{"text": "NOISYCHANNEL", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9978173971176147}]}, {"text": "DIR is the learned direction ( \u00a7 2.2) of our full model's noisy channel PFST: Left-to-right or Right-to-left.", "labels": [], "entities": [{"text": "DIR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5665383338928223}]}, {"text": "Our models are given oracle parse trees T . The best perplexity is boldfaced, along with all results that are not significantly worse (paired permutation test, p < 0.05).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of our datasets. \"Treebank\" is the  UD treebank identifier, \"#Token\" is the number of to- kens, \"%Punct\" is the percentage of punctuation to- kens, \"#Omit\" is the small number of sentences con- taining non-leaf punctuation tokens (see footnote 19),  and \"#Type\" is the number of punctuation types after  preprocessing. (Recall from  \u00a74 that preprocessing dis- tinguishes between left and right quotation mark types,  and between abbreviation dot and period dot types.)", "labels": [], "entities": [{"text": "UD treebank identifier", "start_pos": 57, "end_pos": 79, "type": "DATASET", "confidence": 0.9327277143796285}]}, {"text": " Table 3: Results of the conditional perplexity experi- ment ( \u00a74), reported as perplexity per punctuation slot,  where an unpunctuated sentence of n words has n + 1  slots. Column \"Attn.\" is the BiGRU tagger with atten- tion, and \"CRF\" stands for the BiLSTM-CRF tagger.  \"ATTACH\" is the ablated version of our model where  surface punctuation is directly attached to the nodes.  Our full model \"+NC\" adds NOISYCHANNEL to trans- duce the attached punctuation into surface punctuation.  DIR is the learned direction ( \u00a7 2.2) of our full model's  noisy channel PFST: Left-to-right or Right-to-left. Our  models are given oracle parse trees T . The best per- plexity is boldfaced, along with all results that are not  significantly worse (paired permutation test, p < 0.05).", "labels": [], "entities": [{"text": "ATTACH", "start_pos": 273, "end_pos": 279, "type": "METRIC", "confidence": 0.8159440159797668}, {"text": "NOISYCHANNEL", "start_pos": 406, "end_pos": 418, "type": "METRIC", "confidence": 0.9901973009109497}]}, {"text": " Table 1. That is, of the  16 2 = 256 possible underlying punctuation bi- grams ab, 3  4 are supposed to undergo absorption  or transposition. Our method achieves fairly high  recall, in the sense that when Nunberg proposes  ab7 !\ud97b\udf59, our learned p(\ud97b\udf59 | ab) usually ranks highly  among all probabilities of the form p(\ud97b\udf59 0 | ab). 75  of Nunberg's rules got rank 1, 48 got rank 2, and  the remaining 69 got rank > 2. The mean recipro- cal rank was 0.621. Recall is quite high when we  restrict to those Nunberg rules ab 7 ! \ud97b\udf59 for which  our model is confident how to rewrite ab, in the  sense that some p(\ud97b\udf59 0 | ab) > 0.5. (This tends  to eliminate rare ab: see footnote 5.) Of these 55  Nunberg rules, 38 rules got rank 1, 15 got rank 2,  and only 2 got rank worse than 2. The mean recip- rocal rank was 0.836.", "labels": [], "entities": [{"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9992609620094299}, {"text": "Recall", "start_pos": 450, "end_pos": 456, "type": "METRIC", "confidence": 0.9995197057723999}]}, {"text": " Table 3: the trivial deterministic method, the BiLSTM- CRF, and the ATTACH ablation baseline that attaches  the surface punctuation directly to the tree. Column 4  is our method that incorporates a noisy channel, and  column 5 (in gray) is our method using oracle (gold)  trees. We boldface the best non-oracle result as well as  all that are not significantly worse (paired permutation  test, p < 0.05). The curves show how our method's  AED (on dev data) varies with the labeled attachment  score (LAS) of the trees, where --a at x = 100 uses  the oracle (gold) trees, a--at x < 100 uses trees from  our parser trained on 100% of the training data, and the  #--points at x \u2327 100 use increasingly worse parsers.  The p and 8 at the right of the graph show the AED of  the trivial deterministic baseline and the BiLSTM-CRF  baseline, which do not use trees.", "labels": [], "entities": [{"text": "ATTACH", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9487293362617493}, {"text": "AED", "start_pos": 440, "end_pos": 443, "type": "METRIC", "confidence": 0.9955320358276367}, {"text": "labeled attachment  score (LAS)", "start_pos": 474, "end_pos": 505, "type": "METRIC", "confidence": 0.8463439444700877}, {"text": "AED", "start_pos": 762, "end_pos": 765, "type": "METRIC", "confidence": 0.9930873513221741}]}, {"text": " Table 5: AED and F 0.5 results on the test split of  English-ESL data. Lower AED is better; higher F 0.5  is better. The first three columns (markers corre- spond to", "labels": [], "entities": [{"text": "AED", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9546163082122803}, {"text": "F", "start_pos": 18, "end_pos": 19, "type": "METRIC", "confidence": 0.9871679544448853}, {"text": "AED", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9959849119186401}]}, {"text": " Table 6: Perplexity (evaluated on the train split to  avoid evaluating generalization) of a trigram language  model trained (with add-0.001 smoothing) on differ- ent versions of rephrased training sentences. \"Punc- tuation\" only evaluates perplexity on the trigrams that  have punctuation. \"All\" evaluates on all the tri- grams. \"Base\" permutes all surface dependents includ- ing punctuation (", "labels": [], "entities": []}]}