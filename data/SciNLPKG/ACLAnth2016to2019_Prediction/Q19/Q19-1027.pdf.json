{"title": [{"text": "Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition", "labels": [], "entities": [{"text": "Evaluating Text Representations on Lexical Composition", "start_pos": 26, "end_pos": 80, "type": "TASK", "confidence": 0.7186512450377146}]}], "abstractContent": [{"text": "Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings.", "labels": [], "entities": []}, {"text": "Lexical composition can shift the meanings of the constituent words and introduce implicit information.", "labels": [], "entities": [{"text": "Lexical composition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8742966055870056}]}, {"text": "We tested abroad range of textual representations for their capacity to address these issues.", "labels": [], "entities": []}, {"text": "We found that, as expected , contextualized word representations perform better than static word embeddings, more soon detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans.", "labels": [], "entities": [{"text": "detecting meaning shift", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.7584787408510844}]}, {"text": "Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modeling the meaning of phrases involves addressing semantic phenomena that pose nontrivial challenges for common text representations, which derive a phrase representation from those of its constituent words.", "labels": [], "entities": []}, {"text": "One such phenomenon is meaning shift, which happens when the meaning of the phrase departs from the meanings of its constituent words.", "labels": [], "entities": [{"text": "meaning shift", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7297074198722839}]}, {"text": "This is especially common among verb-particle constructions (carry on), idiomatic noun compounds (guilt trip), and other multi-word expressions (MWE, lexical units that form a distinct concept), making them ''a pain in the neck'' for NLP applications (.", "labels": [], "entities": []}, {"text": "A second phenomenon is common for both MWEs and free word combinations such as noun compounds and adjective-noun compositions.", "labels": [], "entities": []}, {"text": "It happens when the composition introduces an implicit meaning that often requires world knowledge to uncover.", "labels": [], "entities": []}, {"text": "For example, that hot refers to the temperature of tea but to the manner of debate, or that olive oil is made of olives while baby oil is made for babies.", "labels": [], "entities": []}, {"text": "There has been a line of attempts to learn compositional phrase representations (e.g.,, but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context.", "labels": [], "entities": []}, {"text": "Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks).", "labels": [], "entities": []}, {"text": "Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift.", "labels": [], "entities": []}, {"text": "These models were shown to capture some world knowledge (e.g.,, which may potentially help with uncovering implicit information.", "labels": [], "entities": []}, {"text": "In this paper we test how well various text representations address these composition-related phenomena.", "labels": [], "entities": []}, {"text": "Methodologically, we follow recent work that applied ''black-box'' testing to assess various capacities of distributed representations (e.g.,.", "labels": [], "entities": []}, {"text": "We construct an evaluation suite with six tasks related to the above two phenomena, as shown in, and develop generic models that rely on pre-trained representations.", "labels": [], "entities": []}, {"text": "We test six representations, including static word embeddings () and contextualized word embeddings ().", "labels": [], "entities": []}, {"text": "Our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We created a unified framework that tests the capacity of representations to address lexical composition via classification tasks, focusing on detecting meaning shift and recovering implicit meaning.", "labels": [], "entities": [{"text": "detecting meaning shift", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.7856525778770447}]}, {"text": "We test six representations and provide an in depth analysis of the results.", "labels": [], "entities": []}, {"text": "2. We relied on existing annotated data sets used in various tasks, and recast them to fit our classification framework.", "labels": [], "entities": []}, {"text": "We additionally annotated a sample from each test set to confirm the data validity and estimate the human performance on each task.", "labels": [], "entities": []}, {"text": "3. We provide the classification framework, including data and code (available at https:// github.com/vered1986/lexcomp), which would allow testing future models for their capacity to address lexical composition.", "labels": [], "entities": []}, {"text": "Our results confirm that the contextualized word embeddings perform better than the static ones.", "labels": [], "entities": []}, {"text": "In particular, we show that modeling context indeed contributes to recognizing meaning shift: On such tasks, contextualized models performed on par with humans.", "labels": [], "entities": [{"text": "recognizing meaning shift", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.8061632911364237}]}, {"text": "Conversely, despite hopes of filling missing information with world knowledge provided by the contextualized representations, the signal they yield for recovering implicit information is much weaker, and the gap between the best performing model and the human performance on such tasks remains substantial.", "labels": [], "entities": []}, {"text": "We expect that improving the ability of such representations to reveal implicit meaning would require more than a language model training objective.", "labels": [], "entities": []}, {"text": "In particular, one future direction is a richer training objective that simultaneously models multiple co-occurrences of the constituent words across different texts, as is commonly done in noun compound interpretation (e.g.,).", "labels": [], "entities": [{"text": "noun compound interpretation", "start_pos": 190, "end_pos": 218, "type": "TASK", "confidence": 0.7874284187952677}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Summary of the best performance of each family of representations on the various tasks. The  evaluation metric is accuracy except for the phrase type task in which we report span-based F 1 score,  excluding O tags.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9993979930877686}, {"text": "span-based F 1 score", "start_pos": 184, "end_pos": 204, "type": "METRIC", "confidence": 0.7791695445775986}]}, {"text": " Table 7: Top substitutes for a target word in literal (left) and non-literal (right) contexts, along with  model scores. Bold words are words judged reasonable (not necessarily meaning preserving) in the  given context, and underlined words are suitable substitutes for the entire noun compound, but not for  a single constituent.", "labels": [], "entities": []}, {"text": " Table 8: Accuracy scores of ablations of the  phrase, context sentence, and both features  from the best models in the NC Relations  and AN Attributes tasks (ELMo+Top+biLM and  BERT+All+None, respectively).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9867517948150635}, {"text": "BERT", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9939250349998474}]}]}