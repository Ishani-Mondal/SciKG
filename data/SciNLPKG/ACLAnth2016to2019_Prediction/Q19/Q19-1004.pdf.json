{"title": [{"text": "Analysis Methods in Neural Language Processing: A Survey", "labels": [], "entities": [{"text": "Neural Language Processing", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.7028298377990723}]}], "abstractContent": [{"text": "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6592775881290436}]}, {"text": "A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts.", "labels": [], "entities": []}, {"text": "This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways.", "labels": [], "entities": []}, {"text": "In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.", "labels": [], "entities": [{"text": "neural language processing", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6619705855846405}]}], "introductionContent": [{"text": "The rise of deep learning has transformed the field of natural language processing (NLP) in recent years.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.8217156529426575}]}, {"text": "Models based on neural networks have obtained impressive improvements in various tasks, including language modeling (, syntactic parsing (, machine translation (MT) (, and many other tasks; see for example success stories.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7824113368988037}, {"text": "syntactic parsing", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7201901227235794}, {"text": "machine translation (MT)", "start_pos": 140, "end_pos": 164, "type": "TASK", "confidence": 0.8193529486656189}]}, {"text": "This progress has been accompanied by a myriad of new neural network architectures.", "labels": [], "entities": []}, {"text": "In many cases, traditional feature-rich systems are being replaced by end-to-end neural networks that aim to map input text to some output prediction.", "labels": [], "entities": []}, {"text": "As end-to-end systems are gaining prevalence, one may point to two trends.", "labels": [], "entities": []}, {"text": "First, some push back against the abandonment of linguistic knowledge and call for incorporating it inside the networks in different ways.", "labels": [], "entities": []}, {"text": "Others strive to better understand how NLP models work.", "labels": [], "entities": []}, {"text": "This theme of analyzing neural networks has connections to the broader work on interpretability in machine learning, along with specific characteristics of the NLP field.", "labels": [], "entities": []}, {"text": "Why should we analyze our neural NLP models?", "labels": [], "entities": []}, {"text": "To some extent, this question falls into the larger question of interpretability in machine learning, which has been the subject of much debate in recent years.", "labels": [], "entities": []}, {"text": "Arguments in favor of interpretability in machine learning usually mention goals like accountability, trust, fairness, safety, and reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9674164056777954}]}, {"text": "Arguments against interpretability typically stress performance as the most important desideratum.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.9632879495620728}]}, {"text": "All these arguments naturally apply to machine learning applications in NLP.", "labels": [], "entities": []}, {"text": "In the context of NLP, this question needs to be understood in light of earlier NLP work, often referred to as feature-rich or feature-engineered systems.", "labels": [], "entities": []}, {"text": "In some of these systems, features are more easily understood by humans-they can be morphological properties, lexical classes, syntactic categories, semantic relations, etc.", "labels": [], "entities": []}, {"text": "In theory, one could observe the importance assigned by statistical NLP models to such features in order to gain a better understanding of the model.", "labels": [], "entities": []}, {"text": "Nevertheless, one could question how feasible such an analysis is; consider, for example, interpreting support vectors in high-dimensional support vector machines (SVMs).", "labels": [], "entities": [{"text": "interpreting support vectors in high-dimensional support vector machines (SVMs)", "start_pos": 90, "end_pos": 169, "type": "TASK", "confidence": 0.7581020593643188}]}, {"text": "contrast, it is more difficult to understand what happens in an end-to-end neural network model that takes input (say, word embeddings) and generates an output (say, a sentence classification).", "labels": [], "entities": []}, {"text": "Much of the analysis work thus aims to understand how linguistic concepts that were common as features in NLP systems are captured in neural networks.", "labels": [], "entities": []}, {"text": "As the analysis of neural networks for language is becoming more and more prevalent, neural networks in various NLP tasks are being analyzed; different network architectures and components are being compared, and a variety of new analysis methods are being developed.", "labels": [], "entities": []}, {"text": "This survey aims to review and summarize this body of work, highlight current trends, and point to existing lacunae.", "labels": [], "entities": []}, {"text": "It organizes the literature into several themes.", "labels": [], "entities": []}, {"text": "Section 2 reviews work that targets a fundamental question: What kind of linguistic information is captured in neural networks?", "labels": [], "entities": []}, {"text": "We also point to limitations in current methods for answering this question.", "labels": [], "entities": []}, {"text": "Section 3 discusses visualization methods, and emphasizes the difficulty in evaluating visualization work.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss the compilation of challenge sets, or test suites, for fine-grained evaluation, a methodology that has old roots in NLP.", "labels": [], "entities": []}, {"text": "Section 5 deals with the generation and use of adversarial examples to probe weaknesses of neural networks.", "labels": [], "entities": []}, {"text": "We point to unique characteristics of dealing with text as a discrete input and how different studies handle them.", "labels": [], "entities": []}, {"text": "Section 6 summarizes work on explaining model predictions, an important goal of interpretability research.", "labels": [], "entities": [{"text": "explaining model predictions", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8486217061678568}]}, {"text": "This is a relatively underexplored area, and we call for more work in this direction.", "labels": [], "entities": []}, {"text": "Section 7 mentions a few other methods that do not fall neatly into one of the above themes.", "labels": [], "entities": []}, {"text": "In the conclusion, we summarize the main gaps and potential research directions for the field.", "labels": [], "entities": []}, {"text": "The paper is accompanied by online supplementary materials that contain detailed references for studies corresponding to Sections 2, 4, and 5, and SM3, respectively), available at https://boknilev.github.io/ nlp-analysis-methods.", "labels": [], "entities": []}, {"text": "Before proceeding, we briefly mention some earlier work of a similar spirit.", "labels": [], "entities": []}, {"text": "A Historical Note Reviewing the vast literature on neural networks for language is beyond our scope.", "labels": [], "entities": []}, {"text": "However, we mention here a few representative studies that focused on analyzing such networks in order to illustrate how recent trends have roots that go back to before the recent deep learning revival.", "labels": [], "entities": []}, {"text": "built a feedforward neural network for learning the English past tense and analyzed its performance on a variety of examples and conditions.", "labels": [], "entities": []}, {"text": "They were especially concerned with the performance over the course of training, as their goal was to model the past form acquisition in children.", "labels": [], "entities": [{"text": "past form acquisition", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.68439914782842}]}, {"text": "They also analyzed a scaled-down version having eight input units and eight output units, which allowed them to describe it exhaustively and examine how certain rules manifest in network weights.", "labels": [], "entities": []}, {"text": "In his seminal work on recurrent neural networks (RNNs), Elman trained networks on synthetic sentences in a language prediction task.", "labels": [], "entities": [{"text": "language prediction task", "start_pos": 108, "end_pos": 132, "type": "TASK", "confidence": 0.8064208030700684}]}, {"text": "Through extensive analyses, he showed how networks discover the notion of a word when predicting characters; capture syntactic structures like number agreement; and acquire word representations that reflect lexical and syntactic categories.", "labels": [], "entities": []}, {"text": "Similar analyses were later applied to other networks and tasks.", "labels": [], "entities": []}, {"text": "While Elman's work was limited in some ways, such as evaluating generalization or various linguistic phenomena-as Elman himself recognized-it introduced methods that are still relevant today: from visualizing network activations in time, through clustering words by hidden state activations, to projecting representations to dimensions that emerge as capturing properties like sentence number or verb valency.", "labels": [], "entities": []}, {"text": "The sections on visualization (Section 3) and identifying linguistic information (Section 2) contain many examples for these kinds of analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "Systems are typically evaluated by their performance on the challenge set examples, either with the same metric used for evaluating the system in the first place, or via a proxy, as in the contrastive pairs evaluation of.", "labels": [], "entities": []}, {"text": "Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale.", "labels": [], "entities": []}, {"text": "However, they may miss certain aspects.", "labels": [], "entities": []}, {"text": "Thus a few studies report human evaluation on their challenge sets, such as in MT (.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.5937402248382568}]}, {"text": "We note here also that judging the quality of a model by its performance on a challenge set can be tricky.", "labels": [], "entities": []}, {"text": "Some authors emphasize their wish to test systems on extreme or difficult cases, ''beyond normal operational capacity'' (.", "labels": [], "entities": []}, {"text": "However, whether one should expect systems to perform well on specially chosen cases (as opposed to the average case) may depend on one's goals.", "labels": [], "entities": []}, {"text": "To put results in perspective, one may compare model performance to human performance on the same task (.", "labels": [], "entities": []}], "tableCaptions": []}