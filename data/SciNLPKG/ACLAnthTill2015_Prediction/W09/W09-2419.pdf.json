{"title": [{"text": "SemEval-2010 Task 14: Evaluation Setting for Word Sense Induction & Disambiguation Systems", "labels": [], "entities": [{"text": "SemEval-2010 Task 14", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8490952650705973}, {"text": "Word Sense Induction & Disambiguation", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.7129415392875671}]}], "abstractContent": [{"text": "This paper presents the evaluation setting for the SemEval-2010 Word Sense Induction (WSI) task.", "labels": [], "entities": [{"text": "SemEval-2010 Word Sense Induction (WSI) task", "start_pos": 51, "end_pos": 95, "type": "TASK", "confidence": 0.8949901387095451}]}, {"text": "The setting of the SemEval-2007 WSI task consists of two evaluation schemes, i.e. unsupervised evaluation and supervised evaluation.", "labels": [], "entities": [{"text": "SemEval-2007 WSI task", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7952025731404623}]}, {"text": "The first one evaluates WSI methods in a similar fashion to Information Retrieval exercises using F-Score.", "labels": [], "entities": [{"text": "WSI", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9403764009475708}, {"text": "Information Retrieval", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.840606302022934}]}, {"text": "However, F-Score suffers from the matching problem which does not allow: (1) the assessment of the entire membership of clusters, and (2) the evaluation of all clusters in a given solution.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.5813697576522827}]}, {"text": "In this paper, we present the use of V-measure as a measure of objectively assessing WSI methods in an unsupervised setting, and we also suggest a small modification on the supervised evaluation.", "labels": [], "entities": [{"text": "WSI", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.8645774126052856}]}], "introductionContent": [{"text": "WSI is the task of identifying the different senses (uses) of a target word in a given text.", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9305276274681091}, {"text": "identifying the different senses (uses) of a target word in a given text", "start_pos": 19, "end_pos": 91, "type": "TASK", "confidence": 0.6167687952518464}]}, {"text": "WSI is afield of significant value, because it aims to overcome the limitations originated by representing word senses as a fixed-list of dictionary definitions.", "labels": [], "entities": []}, {"text": "These limitations of hand-crafted lexicons include the use of general sense definitions, the lack of explicit semantic and topical relations between concepts (, and the inability to reflect the exact content of the context in which a target word appears.", "labels": [], "entities": []}, {"text": "Given the significance of WSI, the objective assessment and comparison of WSI methods is crucial.", "labels": [], "entities": [{"text": "WSI", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.6767756938934326}]}, {"text": "The first effort to evaluate WSI methods under a common framework (evaluation schemes & dataset) was undertaken in the SemEval-2007 WSI task (SWSI), where two separate evaluation schemes were employed.", "labels": [], "entities": [{"text": "WSI", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9480997920036316}, {"text": "SemEval-2007 WSI task (SWSI)", "start_pos": 119, "end_pos": 147, "type": "TASK", "confidence": 0.6568052967389425}]}, {"text": "The first one, unsupervised evaluation, treats the WSI results as clusters of target word contexts and Gold Standard (GS) senses as classes.", "labels": [], "entities": [{"text": "WSI", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8831201195716858}]}, {"text": "The traditional clustering measure of F-Score () is used to assess the performance of WSI systems.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9843013882637024}, {"text": "WSI", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9420662522315979}]}, {"text": "The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses.", "labels": [], "entities": []}, {"text": "In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.6979297399520874}]}, {"text": "A significant limitation of F-Score is that it does not evaluate the makeup of clusters beyond the majority class.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.875500500202179}]}, {"text": "Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9580003023147583}]}, {"text": "These two limitations define the matching problem of F-Score ( which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.7089532017707825}]}, {"text": "The supervised evaluation scheme employs a method in order to map the automatically induced clusters to GS senses.", "labels": [], "entities": []}, {"text": "As a result, this process might change the distribution of clusters by mapping more than one clusters to the same GS sense.", "labels": [], "entities": []}, {"text": "The outcome of this process might be more helpful for systems that produce a large number of clusters.", "labels": [], "entities": []}, {"text": "In this paper, we focus on analysing the SemEval-2007 WSI evaluation schemes showing their deficiencies.", "labels": [], "entities": [{"text": "SemEval-2007 WSI evaluation schemes", "start_pos": 41, "end_pos": 76, "type": "DATASET", "confidence": 0.6022232845425606}]}, {"text": "Subsequently, we present the use of V-measure ( as an evaluation measure that can overcome the current limitations of F-Score.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.7300148010253906}]}, {"text": "Finally, we also suggest a small modification on the supervised evaluation scheme, which will possibly allow fora more reliable estimation of WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9569604396820068}]}, {"text": "The proposed evaluation setting will be applied in the SemEval-2010 WSI task.", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.6596017877260844}]}], "datasetContent": [{"text": "The evaluates WSI systems on 35 nouns and 65 verbs.", "labels": [], "entities": [{"text": "WSI", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.6615958213806152}]}, {"text": "The corpus consists of texts of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses ().", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.9746771305799484}]}, {"text": "For each target word tw, the task consists of firstly identifying the senses of tw (e.g. as clusters of target word instances, cooccurring words, etc.), and secondly tagging the instances of the target word using the automatically induced clusters.", "labels": [], "entities": []}, {"text": "In the next sections, we describe and review the two evaluation schemes.", "labels": [], "entities": []}, {"text": "Let us assume that given a target word tw, a WSI method has produced 3 clusters which have tagged 2100 instances of tw. shows the number of tagged instances for each cluster, as well as the common instances between each cluster and each gold standard sense.", "labels": [], "entities": []}, {"text": "F-Score is used in a similar fashion to Information Retrieval exercises.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8518357276916504}, {"text": "Information Retrieval", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.850884348154068}]}, {"text": "The F-Score of class gs i , F (gs i ), is the maximum F (gs i , c j ) value attained at any cluster.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9926049709320068}, {"text": "F", "start_pos": 28, "end_pos": 29, "type": "METRIC", "confidence": 0.9666779637336731}, {"text": "F", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9929391145706177}]}, {"text": "Finally, the F-Score of the entire clustering solution is defined as the weighted average of the F-Scores of each GS sense (Formula 1), where q is the number of GS senses and N is the total number of target word ings 1 gs 2 gs 3 cl 1 500 100 100 cl 2 100 500 100 cl 3 100 100 500 stances.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9922900795936584}, {"text": "F-Scores", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9756256937980652}]}, {"text": "If the clustering is identical to the original classes in the datasets, F-Score will be equal to one.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.998291552066803}]}, {"text": "In the example of, F-Score is equal to 0.714.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9981141090393066}]}, {"text": "As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9751111268997192}]}, {"text": "Homogeneity refers to the degree that each cluster consists of data points, which primarily belong to a single GS class.", "labels": [], "entities": []}, {"text": "On the other hand, completeness refers to the degree that each GS class consists of data points, which have primarily been assigned to a single cluster.", "labels": [], "entities": []}, {"text": "A perfect homogeneity would result in a precision equal to 1, while a perfect completeness would result in a recall equal to 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.998961329460144}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9990391731262207}]}, {"text": "Purity and entropy () are also used in SWSI as complementary measures.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.945342481136322}, {"text": "SWSI", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.8927887082099915}]}, {"text": "However, both of them evaluate only the homogeneity of a clustering solution disregarding completeness.", "labels": [], "entities": []}, {"text": "In supervised evaluation, the target word corpus is split into a testing and a training part.", "labels": [], "entities": []}, {"text": "The training part is used to map the automatically induced clusters to GS senses.", "labels": [], "entities": []}, {"text": "In the next step, the testing corpus is used to evaluate WSI methods in a WSD setting.", "labels": [], "entities": [{"text": "WSI", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9414637684822083}]}, {"text": "Let us consider the example shown in and assume that this matrix has been created by using the training part of our corpus.", "labels": [], "entities": []}, {"text": "shows that cl 1 is more likely to be associated with gs 1 , cl 2 is more likely to be associated with gs 2 , and cl 3 is more likely to be associated with gs 3 . This information from the training part is utilised to map the clusters to GS senses.", "labels": [], "entities": []}, {"text": "Particularly, the matrix shown in is normalised to produce a matrix M , in which each entry depicts the conditional probability P (gs i |cl j ).", "labels": [], "entities": []}, {"text": "Given an instance I of tw from the testing corpus, a row cluster vector IC is created, in which System F-Sc.  each entry k corresponds to the score assigned to cl k to be the winning cluster for instance I.", "labels": [], "entities": []}, {"text": "The product of IC and M provides a row sense vector, IG, in which the highest scoring entry a denotes that gs a is the winning sense for instance I.", "labels": [], "entities": [{"text": "IG", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9276543855667114}]}, {"text": "For example, if we produce the row cluster vector , and multiply it with the normalised matrix of, then we would get a row sense vector in which gs 1 would be the winning sense with a score equal to 0.6.", "labels": [], "entities": []}, {"text": "shows the unsupervised and supervised performance of systems participating in SWSI.", "labels": [], "entities": [{"text": "SWSI", "start_pos": 78, "end_pos": 82, "type": "TASK", "confidence": 0.9362033009529114}]}, {"text": "As far as the baselines is concerned, the 1c1w baseline groups all instances of a target word into a single cluster, while the 1c1inst creates anew cluster for each instance of a target word.", "labels": [], "entities": []}, {"text": "Note that the 1c1w baseline is equivalent to the MFS in the supervised evaluation.", "labels": [], "entities": []}, {"text": "As it can be observed, a system with low entropy (high purity) does not necessarily achieve high F-Score.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9973360896110535}]}, {"text": "This is due to the fact that entropy and purity only measure the homogeneity of a clustering solution.", "labels": [], "entities": [{"text": "purity", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9952279329299927}]}, {"text": "For that reason, the 1c1inst baseline achieves a perfect entropy and purity, although its clustering solution is far from ideal.", "labels": [], "entities": []}, {"text": "On the contrary, F-Score has a significant advantage over purity and entropy, since it measures both homogeneity (precision) and completeness (recall) of a clustering solution.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9950730204582214}, {"text": "purity", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9959115982055664}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9861754775047302}, {"text": "recall)", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9141634106636047}]}, {"text": "However, F-Score suffers from the matching problem, which manifests itself either by not evaluating the entire membership of a cluster, or by not evaluating every cluster.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.743201732635498}]}, {"text": "The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9579325914382935}]}, {"text": "For example, in the F-Score of the clustering sogs 1 gs 2 gs 3 cl 1 500 0 200 cl 2 200 500 0 cl 3 0 200 500 lution is 0.714 and equal to the F-Score of the clustering solution shown in, although these are two significantly different clustering solutions.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.997865617275238}, {"text": "F-Score", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.9945433139801025}]}, {"text": "In fact, the clustering shown in should have a better homogeneity than the clustering shown in, since intuitively speaking each cluster contains fewer classes.", "labels": [], "entities": []}, {"text": "Moreover, the second clustering should also have a better completeness, since each GS class contains fewer clusters.", "labels": [], "entities": []}, {"text": "An additional instance of the matching problem manifests itself, when F-Score fails to evaluate the quality of smaller clusters.", "labels": [], "entities": []}, {"text": "For example, if we add in one more cluster (cl 4 ), which only tags 50 additional instances of gs 1 , then we will be able to observe that this cluster will not be matched to any of the GS senses, since cl 1 is matched togs 1 . Although F-Score will decrease since the recall of gs 1 will decrease, the evaluation setting ignores the perfect homogeneity of this small cluster.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 237, "end_pos": 244, "type": "METRIC", "confidence": 0.9975271821022034}, {"text": "recall", "start_pos": 269, "end_pos": 275, "type": "METRIC", "confidence": 0.9986200332641602}]}, {"text": "Let us assume that the dataset of a target word tw comprises of N instances (data points).", "labels": [], "entities": []}, {"text": "These data points are divided into two partitions, i.e. a set of automatically generated clusters C = {c j |j = 1 . .", "labels": [], "entities": []}, {"text": "n} and a set of gold standard classes GS = {gs i |gs = 1 . .", "labels": [], "entities": [{"text": "GS", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.7812346816062927}]}, {"text": "Moreover, let a ij be the number of data points, which are members of class gs i and elements of cluster c j . V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness.", "labels": [], "entities": []}, {"text": "Recall that homogeneity refers to the degree that each cluster consists of data points which primarily belong to a single GS class.", "labels": [], "entities": []}, {"text": "V-measure assesses homogeneity by examining the conditional entropy of the class distribution given the proposed clustering, i.e. H(GS|C).", "labels": [], "entities": []}, {"text": "H(GS|C) quantifies the remaining entropy (uncertainty) of the class distribution given that the proposed clustering is known.", "labels": [], "entities": [{"text": "entropy (uncertainty)", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.8959372043609619}]}, {"text": "As a result, when H(GS|C) is 0, we have the perfectly homogeneous solution, since each cluster contains only those data points that are members of a single class.", "labels": [], "entities": []}, {"text": "However in an imperfect situation, H(GS|C) depends on the size of the dataset and the distribution of class sizes.", "labels": [], "entities": []}, {"text": "As a result, instead of taking the raw conditional entropy, V-measure normalises it by the maximum reduction in entropy the clustering information could provide, i.e. H(GS).", "labels": [], "entities": []}, {"text": "Formulas 2 and 3 define H(GS) and H(GS|C).", "labels": [], "entities": []}, {"text": "When there is only a single class (H(GS) = 0), any clustering would produce a perfectly homogeneous solution.", "labels": [], "entities": []}, {"text": "In the worst case, the class distribution within each cluster is equal to the overall class distribution (H(GS|C) = H(GS)), i.e. clustering provides no new information.", "labels": [], "entities": []}, {"text": "Overall, in accordance with the convention of 1 being desirable and 0 undesirable, the homogeneity (h) of a clustering solution is 1 if there is only a single class, and 1\u2212 H(GS|C) H(GS) in any other case.", "labels": [], "entities": []}, {"text": "Symmetrically to homogeneity, completeness refers to the degree that each GS class consists of data points, which have primarily been assigned to a single cluster.", "labels": [], "entities": []}, {"text": "To evaluate completeness, V-measure examines the distribution of cluster assignments within each class.", "labels": [], "entities": []}, {"text": "The conditional entropy of the cluster given the class distribution, H(C|GS), quantifies the remaining entropy (uncertainty) of the cluster given that the class distribution is known.", "labels": [], "entities": []}, {"text": "Consequently, when H(C|GS) is 0, we have the perfectly complete solution, since all the data points of a class belong to the same cluster.", "labels": [], "entities": []}, {"text": "Therefore, symmetrically to homogeneity, the completeness c of a clustering solution is 1 if there is only a single cluster (H(C) = 0), and 1 \u2212 H(C|GS) H in any other case.", "labels": [], "entities": []}, {"text": "In the worst case, completeness will be equal to 0, particularly when H(C|GS) is maximal and equal to H(C).", "labels": [], "entities": []}, {"text": "This happens when each GS class is included in all clusters with a distribution equal to the distribution of sizes.", "labels": [], "entities": []}, {"text": "Formulas 4 and 5 define H(C) and H(C|GS).", "labels": [], "entities": []}, {"text": "Finally hand c can be combined and produce V-measure, which is the harmonic mean of homogeneity and completeness.", "labels": [], "entities": []}, {"text": "Returning to our clustering example in, its V-measure is equal to 0.275.", "labels": [], "entities": []}, {"text": "In section 2.3, we also presented an additional clustering, which had the same F-Score as the clustering in Table 1, despite the fact that it intuitively had a better completeness and homogeneity.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9984045624732971}]}, {"text": "The V-measure of the second clustering solution is equal to 0.45, and higher than the V-measure of the first clustering.", "labels": [], "entities": [{"text": "V-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9744250774383545}]}, {"text": "This result shows that V-measure is able to discriminate between these two clusterings by considering the make-up of the clusters beyond the majority class.", "labels": [], "entities": []}, {"text": "Furthermore, it is straightforward from the description in this section, that V-measure evaluates each cluster in terms of homogeneity and completeness, unlike F-Score which relies on a post-hoc matching.", "labels": [], "entities": []}, {"text": "shows the performance of SWSI participating systems according to V-measure.", "labels": [], "entities": [{"text": "V-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9141252040863037}]}, {"text": "The last four columns of show the weighted average homogeneity and completeness for nouns and verbs.", "labels": [], "entities": []}, {"text": "Note that the homogeneity and completeness columns are weighted averages overall nouns or verbs, and are not used for the calculation of the weighted average V-measure (second column).", "labels": [], "entities": [{"text": "completeness", "start_pos": 30, "end_pos": 42, "type": "METRIC", "confidence": 0.9593238830566406}]}, {"text": "The latter is calculated by measuring for each target word's clustering solution the harmonic mean of homogeneity and completeness separately, and then producing the weighted average.", "labels": [], "entities": []}, {"text": "As it can be observed in, all WSI systems have outperformed the random baseline which means that they have learned useful information.", "labels": [], "entities": [{"text": "WSI", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8380502462387085}]}, {"text": "Moreover, shows that on average all systems have outperformed the 1c1w baseline, which groups the instances of a target word to a single cluster.", "labels": [], "entities": []}, {"text": "The completeness of the 1c1w baseline is equal to 1 by definition, since all instances of GS classes are grouped to a single cluster.", "labels": [], "entities": []}, {"text": "However, this solution is as inhomogeneous as possible and causes a homogeneity equal to 0 in the case of nouns.", "labels": [], "entities": []}, {"text": "In the verb dataset however, some verbs appear with only one sense, in effect causing the 1c1w homogeneity to be equal to 1 in some cases, and the average Vmeasure greater than 0.", "labels": [], "entities": [{"text": "Vmeasure", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9463028907775879}]}, {"text": "In section 2.3, we mentioned that supervised evaluation might favor methods which produce many  clusters, since the mapping step can artificially increase completeness.", "labels": [], "entities": []}, {"text": "Furthermore, we have shown that generating a large number of clusters might lead to an unreliable mapping of clusters to GS senses due to the lack of adequate training data.", "labels": [], "entities": []}, {"text": "Despite that, the supervised evaluation can be considered as an application-oriented evaluation, since it allows the transformation of unsupervised WSI systems to semi-supervised WSD ones.", "labels": [], "entities": []}, {"text": "Given the great difficulty of unsupervised WSD systems to outperform the MFS baseline as well as the SWSI results, which show that some systems outperform the MFS by a significant amount in nouns, we believe that this evaluation scheme should be used to compare against supervised WSD methods.", "labels": [], "entities": [{"text": "SWSI", "start_pos": 101, "end_pos": 105, "type": "DATASET", "confidence": 0.49504056572914124}]}, {"text": "In section 2.3, we also mentioned that the supervised evaluation on two different test/train splits provided a different ranking of methods, and more importantly a different ranking with regard to the MFS.", "labels": [], "entities": [{"text": "MFS", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.7358096837997437}]}, {"text": "To deal with that problem, we believe that it would be reasonable to perform k-fold cross validation in order to collect statistically significant information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: SWSI Unsupervised & supervised evaluation.", "labels": [], "entities": [{"text": "SWSI", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9466282725334167}]}, {"text": " Table 4: V-Measure, homogeneity and completeness of SemEval-2007 WSI systems. The range of V-measure, homo- geneity & completeness is 0-100.", "labels": [], "entities": []}]}