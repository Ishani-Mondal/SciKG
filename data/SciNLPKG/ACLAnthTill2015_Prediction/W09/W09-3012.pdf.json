{"title": [], "abstractContent": [{"text": "We present a preliminary pilot study of belief annotation and automatic tagging.", "labels": [], "entities": [{"text": "belief annotation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7739580869674683}, {"text": "automatic tagging", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.6425866186618805}]}, {"text": "Our objective is to explore semantic meaning beyond surface propositions.", "labels": [], "entities": []}, {"text": "We aim to model people's cognitive states, namely their beliefs as expressed through linguistic means.", "labels": [], "entities": []}, {"text": "We model the strength of their beliefs and their (the human) degree of commitment to their utterance.", "labels": [], "entities": []}, {"text": "We explore only the perspective of the author of a text.", "labels": [], "entities": []}, {"text": "We classify predicates into one of three possibilities: committed belief, non committed belief, or not applicable.", "labels": [], "entities": []}, {"text": "We proceed to manually annotate data to that end, then we build a supervised framework to test the feasibility of automatically predicting these belief states.", "labels": [], "entities": []}, {"text": "Even though the data is relatively small, we show that automatic prediction of a belief class is a feasible task.", "labels": [], "entities": [{"text": "automatic prediction of a belief class", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.7554325858751932}]}, {"text": "Using syntactic features, we are able to obtain significant improvements over a simple baseline of 23% F-measure absolute points.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9803056716918945}]}, {"text": "The best performing automatic tagging condition is where we use POS tag, word type feature AlphaNumeric, and shallow syntactic chunk information CHUNK.", "labels": [], "entities": []}, {"text": "Our best overall performance is 53.97% F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9995366334915161}]}], "introductionContent": [{"text": "As access to large amounts of textual information increases, there is a strong realization that searches and processing purely based on surface words is highly limiting.", "labels": [], "entities": []}, {"text": "Researchers in information retrieval and natural language processing (NLP) have long used morphological and (in a more limited way) syntactic analysis to improve access and processing of text; recently, interest has grown in relating text to more abstract representations of its propositional meaning, as witnessed by work on semantic role labeling, word sense disambiguation, and textual entailment.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.738493949174881}, {"text": "natural language processing (NLP)", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.8164577384789785}, {"text": "semantic role labeling", "start_pos": 326, "end_pos": 348, "type": "TASK", "confidence": 0.6425780653953552}, {"text": "word sense disambiguation", "start_pos": 350, "end_pos": 375, "type": "TASK", "confidence": 0.6890769799550375}, {"text": "textual entailment", "start_pos": 381, "end_pos": 399, "type": "TASK", "confidence": 0.7034886479377747}]}, {"text": "However, there are more levels to \"meaning\" than just propositional content.", "labels": [], "entities": []}, {"text": "Consider the following examples, and suppose we find these sentences in the New York Times: (1) a.", "labels": [], "entities": [{"text": "New York Times", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.768088161945343}]}, {"text": "GM will layoff workers. b. A spokesman for GM said GM will layoff workers. c. GM may layoff workers. d. The politician claimed that GM will layoff workers. e. Some wish GM would lay of workers. f. Will GM layoff workers?", "labels": [], "entities": []}, {"text": "g. Many wonder if GM will layoff workers.", "labels": [], "entities": [{"text": "GM", "start_pos": 18, "end_pos": 20, "type": "DATASET", "confidence": 0.8422444462776184}]}, {"text": "If we are searching text to find out whether GM will layoff workers, all of the sentences in (1) con-tain the proposition LAYOFF(GM,WORKERS).", "labels": [], "entities": [{"text": "LAYOFF", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9021915793418884}]}, {"text": "However, the six sentences clearly allow us very different inferences about whether GM will layoff workers or not.", "labels": [], "entities": []}, {"text": "Supposing we consider the Times a trustworthy news source, we would be fairly certain with (1a) and (1b).", "labels": [], "entities": []}, {"text": "(1c) suggests the Times is not certain about the layoffs, but considers them possible.", "labels": [], "entities": [{"text": "Times", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.9474735856056213}]}, {"text": "When reading (1d), we know that someone else thinks that GM will layoff workers, but that the Times does not necessarily share this belief.", "labels": [], "entities": []}, {"text": "(1e), (1f), and (1g) do not tell us anything about whether anyone believes whether GM will layoff workers.", "labels": [], "entities": []}, {"text": "In order to tease apart what is happening, we need to refine a simple IR-ish view of text as a repository of propositions about the world.", "labels": [], "entities": []}, {"text": "We use two theories to aid us.", "labels": [], "entities": []}, {"text": "The first theory is that in addition to facts about the world (GM will or will not layoff workers), we have facts about people's cognitive states, and these cognitive states relate their bearer to the facts in the world.", "labels": [], "entities": []}, {"text": "(Though perhaps there are only cognitive states, and no facts about the world.)", "labels": [], "entities": []}, {"text": "Following the literature in Artificial Intelligence), we can model cognitive state as beliefs, desires, and intentions.", "labels": [], "entities": []}, {"text": "In this paper, we are only interested in beliefs (and in distinguishing them from desires and intentions).", "labels": [], "entities": []}, {"text": "The second theory is that communication is intention-driven, and understanding text actually means understanding the communicative intention of the writer.", "labels": [], "entities": []}, {"text": "Furthermore, communicative intentions are intentions to affect the reader's cognitive state -his or her beliefs, desires, and/or intentions.", "labels": [], "entities": []}, {"text": "This view has been worked out in the text generation and dialog community more than in the text understanding community.", "labels": [], "entities": [{"text": "text generation", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8013437688350677}, {"text": "text understanding", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8214518129825592}]}, {"text": "In this paper we are interested in exploring the following: we would like to recognize what the text wants to make us believe about various people's cognitive states, including the speaker's.", "labels": [], "entities": []}, {"text": "As mentioned, we are only interested in people's belief.", "labels": [], "entities": []}, {"text": "In this view, the result of text processing is not a list of facts about the world, but a list of facts about different people's cognitive states.", "labels": [], "entities": []}, {"text": "This paper is part of an on-going research effort.", "labels": [], "entities": []}, {"text": "The goals of this paper are to summarize a pilot annotation effort, and to present the results of initial experiments in automatically extracting facts about people's beliefs from open domain running text.", "labels": [], "entities": [{"text": "automatically extracting facts about people's beliefs from open domain running text", "start_pos": 121, "end_pos": 204, "type": "TASK", "confidence": 0.7899184599518776}]}], "datasetContent": [{"text": "We use F \u03b2=1 (F-measure) as the harmonic mean between (P)recision and (R)ecall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.909917950630188}]}, {"text": "All the presented results are the F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9915424585342407}]}, {"text": "We report the results separately for the three classes CB, NCB, and NA as well as the overall global F measure for anyone condition averaged over the 5 folds of the TEST data set.", "labels": [], "entities": [{"text": "F measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9761250615119934}, {"text": "TEST data set", "start_pos": 165, "end_pos": 178, "type": "DATASET", "confidence": 0.9579807718594869}]}], "tableCaptions": [{"text": " Table 1: Final results averaged over 5 folds of test data using different features and their combinations:  NG is NGRAM, AN is AlphaNumeric, VT is verbtype", "labels": [], "entities": [{"text": "verbtype", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.6009713411331177}]}]}