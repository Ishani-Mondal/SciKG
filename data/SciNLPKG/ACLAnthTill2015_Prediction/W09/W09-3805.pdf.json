{"title": [{"text": "Empirical lower bounds on translation unit error rate for the full class of inversion transduction grammars", "labels": [], "entities": [{"text": "translation unit error rate", "start_pos": 26, "end_pos": 53, "type": "METRIC", "confidence": 0.6783743500709534}]}], "abstractContent": [{"text": "Empirical lower bounds studies in which the frequency of alignment configurations that cannot be induced by a particular formalism is estimated, have been important for the development of syntax-based machine translation formalisms.", "labels": [], "entities": [{"text": "machine translation formalisms", "start_pos": 201, "end_pos": 231, "type": "TASK", "confidence": 0.7405101160208384}]}, {"text": "The formalism that has received most attention has been inversion transduction grammars (ITGs) (Wu, 1997).", "labels": [], "entities": [{"text": "inversion transduction grammars (ITGs)", "start_pos": 56, "end_pos": 94, "type": "TASK", "confidence": 0.8027494847774506}]}, {"text": "All previous work on the coverage of ITGs, however, concerns parse failure rates (PFRs) or sentence level coverage, which is not directly related to any of the evaluation measures used in machine translation.", "labels": [], "entities": [{"text": "coverage of ITGs", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7640111049016317}, {"text": "parse failure rates (PFRs)", "start_pos": 61, "end_pos": 87, "type": "METRIC", "confidence": 0.8673989276091257}, {"text": "machine translation", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.755176842212677}]}, {"text": "S\u00f8gaard and Kuhn (2009) induce lower bounds on translation unit error rates (TUERs) fora number of formalisms, incl.", "labels": [], "entities": [{"text": "translation unit error rates (TUERs)", "start_pos": 47, "end_pos": 83, "type": "METRIC", "confidence": 0.8078495221478599}]}, {"text": "normal form ITGs, but not for the full class of ITGs.", "labels": [], "entities": []}, {"text": "Many of the alignment configurations that cannot be induced by normal form ITGs can be induced by unrestricted ITGs, however.", "labels": [], "entities": []}, {"text": "This paper estimates the difference and shows that the average reduction in lower bounds on TUER is 2.48 in absolute difference (16.01 in average parse failure rate).", "labels": [], "entities": [{"text": "TUER", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8262219429016113}, {"text": "absolute difference", "start_pos": 108, "end_pos": 127, "type": "METRIC", "confidence": 0.9511730670928955}, {"text": "parse failure rate", "start_pos": 146, "end_pos": 164, "type": "METRIC", "confidence": 0.9047375917434692}]}], "introductionContent": [{"text": "The first stage in training a machine translation system is typically that of aligning bilingual text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7287181913852692}]}, {"text": "The quality of alignments is in that case of vital importance to the quality of the induced translation rules used by the system in subsequent stages.", "labels": [], "entities": []}, {"text": "In string-based statistical machine translation, the alignment space is typically restricted by the n-grams considered in the underlying language model, but in syntax-based machine translation the alignment space is restricted by very different and less transparent structural contraints.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.6445291737715403}]}, {"text": "While it is easy to estimate the consequences of restrictions to n-grams of limited size, it is less trivial to estimate the consequences of the structural constraints imposed by syntax-based machine translation formalisms.", "labels": [], "entities": [{"text": "syntax-based machine translation formalisms", "start_pos": 179, "end_pos": 222, "type": "TASK", "confidence": 0.7074395418167114}]}, {"text": "Consequently, much work has been devoted to this task (.", "labels": [], "entities": []}, {"text": "The task of estimating the consequences of the structural constraints imposed by a particular syntax-based formalism consists in finding what is often called \"empirical lower bounds\" on the coverage of the formalism (.", "labels": [], "entities": []}, {"text": "Gold standard alignments are constructed and queried in someway as to identify complex alignment configurations, or they are parsed by an all-accepting grammar such that a parse failure indicates that no alignment could be induced by the formalism.", "labels": [], "entities": []}, {"text": "The assumption in this and related work that enables us to introduce a meaningful notion of alignment capacity is that simultaneously recognized words are aligned.", "labels": [], "entities": []}, {"text": "As noted by, this definition of alignment has the advantageous consequence that candidate alignments can be singled out by mere inspection of the grammar rules.", "labels": [], "entities": []}, {"text": "It also has the consequence that alignments are transitive (), since simultaneity is transitive.", "labels": [], "entities": []}, {"text": "While previous work) has estimated empirical lower bounds for normal form ITGs at the level of translation units (TUER), or cepts (), defined as maximally connected subgraphs in alignments, nobody has done this for the full class of ITGs.", "labels": [], "entities": [{"text": "translation units (TUER)", "start_pos": 95, "end_pos": 119, "type": "METRIC", "confidence": 0.7322715520858765}]}, {"text": "What is important to understand is that while normal form ITGs can induce the same class of translations as the full class of ITGs, they do not induce the same class of alignments.", "labels": [], "entities": []}, {"text": "They do not, for ex-ample, induce discontinuous translation units (see Sect. 3).", "labels": [], "entities": []}, {"text": "2 briefly presents some related results in the literature.", "labels": [], "entities": []}, {"text": "Some knowledge about formalisms used in machine translation is assumed.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7947669327259064}]}, {"text": "showed that 4-ary synchronous context-free grammars (SCFGs) could not be binarized, and showed that the hiearchy of SCFGs beyond ternary ones does not collapse; they also showed that the complexity of the universal recognition problem for SCFGs is NP-complete.", "labels": [], "entities": [{"text": "universal recognition problem", "start_pos": 205, "end_pos": 234, "type": "TASK", "confidence": 0.7253664831320444}]}, {"text": "ITGs on the other hand has a O(|G|n 6 ) solvable universal recognition problem, which coincides with the unrestricted alignment problem.", "labels": [], "entities": [{"text": "ITGs", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9234199523925781}, {"text": "O", "start_pos": 29, "end_pos": 30, "type": "METRIC", "confidence": 0.9686251878738403}, {"text": "universal recognition", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6110261231660843}]}, {"text": "The result extends to decoding in conjunction with a bigram language model ().", "labels": [], "entities": []}], "datasetContent": [{"text": "The advantage of (a) and (b) is that they are guaranteed to find the highest possible lower bound on the gold standard data, whereas (c) is more modular (formalismindependent) and actually tells us what configurations cause trouble.", "labels": [], "entities": []}, {"text": "(i) In this study we use hand-aligned gold standard data.", "labels": [], "entities": []}, {"text": "It should be obvious why this is preferable to automatically aligned data.", "labels": [], "entities": []}, {"text": "The only reason that some previous studies used automatically aligned data is that hand-aligned data are hard to come by.", "labels": [], "entities": []}, {"text": "This study uses the data also used by, which to the best of our knowledge uses the largest collection of handaligned parallel corpora used in any of these studies.", "labels": [], "entities": []}, {"text": "(ii) Failures are counted at the level of translation units as argued for in the above, but supplemented by parse failure rates for completeness.", "labels": [], "entities": []}, {"text": "(iii) Since we count failures at the level of translation units, it is natural to interpret them conjunctively.", "labels": [], "entities": []}, {"text": "Otherwise we would in reality count failures at the level of alignments.", "labels": [], "entities": []}, {"text": "(iv) We use (c).", "labels": [], "entities": []}, {"text": "The conjunctive interpretation of translation units was also adopted by and is motivated by the importance of translation units and discontinuous ones in particular to machine translation in general (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7535616755485535}]}, {"text": "In brief, where GU are the translation units in the gold standard, and S U the translation units produced by the system.", "labels": [], "entities": [{"text": "GU", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9157335162162781}]}, {"text": "This evaluation measure is related to consistent phrase error rate (CPER) introduced in, except that it does not only consider contiguous phrases.", "labels": [], "entities": [{"text": "consistent phrase error rate (CPER)", "start_pos": 38, "end_pos": 73, "type": "METRIC", "confidence": 0.7838771598679679}]}], "tableCaptions": []}