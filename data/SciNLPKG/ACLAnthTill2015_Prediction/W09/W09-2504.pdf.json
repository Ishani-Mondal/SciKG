{"title": [{"text": "Augmenting WordNet-based Inference with Argument Mapping", "labels": [], "entities": [{"text": "Augmenting WordNet-based Inference", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7848539153734843}]}], "abstractContent": [{"text": "WordNet is a useful resource for lexical inference in applications.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9547348618507385}]}, {"text": "Inference over predicates, however, often requires a change in argument positions, which is not specified in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9693614840507507}]}, {"text": "We propose a novel framework for augmenting WordNet-based inferences over predicates with corresponding argument mappings.", "labels": [], "entities": []}, {"text": "We further present a concrete implementation of this framework, which yields substantial improvement to WordNet-based inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "WordNet, a manually constructed lexical database, is probably the mostly used resource for lexical inference in NLP tasks, such as Question Answering (QA), Information Extraction (IE), Information Retrieval and Textual Entailment (RTE);.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.961712658405304}, {"text": "Question Answering (QA)", "start_pos": 131, "end_pos": 154, "type": "TASK", "confidence": 0.8406099259853363}, {"text": "Information Extraction (IE)", "start_pos": 156, "end_pos": 183, "type": "TASK", "confidence": 0.7723686575889588}, {"text": "Information Retrieval and Textual Entailment (RTE)", "start_pos": 185, "end_pos": 235, "type": "TASK", "confidence": 0.7882921360433102}]}, {"text": "Inference using WordNet typically involves lexical substitutions for words in text based on WordNet relations, a process known as lexical chains ().", "labels": [], "entities": []}, {"text": "For example, the answer to \"From which country was Louisiana acquired?\" can be inferred from \"The United States bought up Louisiana from France\" using the chains 'France \u21d2 European country \u21d2 country' and 'buy up \u21d2 buy \u21d2 acquire'.", "labels": [], "entities": []}, {"text": "When performing inference between predicates there is an additional complexity on top of lexical substitution: the syntactic relationship between the predicate and its arguments may change as well.", "labels": [], "entities": []}, {"text": "For example, 'X buy Y for Z \u21d2 X pay Z for Y '.", "labels": [], "entities": []}, {"text": "Currently, argument mappings are not specified for WordNet's relations.", "labels": [], "entities": []}, {"text": "Therefore, correct WordNet inference chains over predicates can be performed only for substitution relations (mainly synonyms and hypernyms, e.g. 'buy \u21d2 acquire'), for which argument positions do not change.", "labels": [], "entities": []}, {"text": "Other relation types that maybe used for inference cannot be utilized when the predicate arguments need to be traced as well.", "labels": [], "entities": []}, {"text": "Examples include the WordNet 'entailment' relation (e.g. 'buy \u21d2 pay') and relations between morphologically derived words (e.g. 'acquire \u21d4 acquisition').", "labels": [], "entities": []}, {"text": "Our goal is to obtain argument mappings for WordNet relations that are often used for inference.", "labels": [], "entities": []}, {"text": "In this paper we address several prominent WordNet relations, including verb-noun derivations and the verb-verb 'entailment' and 'cause' relations, referred henceforth as inferential relations.", "labels": [], "entities": []}, {"text": "Under the Textual Entailment paradigm, all these relations can be viewed as expressing entailment.", "labels": [], "entities": []}, {"text": "Accordingly, we propose a novel framework, called Argument-mapped WordNet (AmWN), that represents argument mappings for inferential relations as entailment rules.", "labels": [], "entities": []}, {"text": "These rules are augmented with subcategorization frames and functional roles, which are proposed as a generally-needed extension for predicative entailment rules.", "labels": [], "entities": []}, {"text": "Following our new representation scheme, we present a concrete implementation of AmWN fora large number of WordNet's relations.", "labels": [], "entities": [{"text": "AmWN", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.901580810546875}]}, {"text": "The mappings for these relations are populated by combining information from manual and corpus-based resources, which provides broader coverage compared to prior work and more accurate mappings.", "labels": [], "entities": []}, {"text": "Arguments are subscripted with functional roles, e.g. subject (subj) and indirect-object (ind-obj).", "labels": [], "entities": []}, {"text": "For brevity, predicate frames are omitted.", "labels": [], "entities": []}, {"text": "To further improve WordNet-based inference for NLP applications, we address the phenomena of rare WordNet senses.", "labels": [], "entities": []}, {"text": "Rules generated for such senses might hurt inference accuracy since they are often applied incorrectly to texts when matched against inappropriate, but more frequent senses of the rule words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9705016613006592}]}, {"text": "Since word sense disambiguation (WSD) solutions are typically not sufficiently robust yet, most applications do not currently apply WSD methods.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.8106710016727448}]}, {"text": "Hence, we propose to optionally filter out such rules using a novel corpus-based validation algorithm.", "labels": [], "entities": []}, {"text": "We tested both WordNet and AmWN on a test set derived from a standard IE benchmark.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.9542089104652405}, {"text": "AmWN", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8732607364654541}, {"text": "IE benchmark", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.7176915854215622}]}, {"text": "The results show that AmWN substantially improves WordNet-based inference in terms of both recall and precision 1 .", "labels": [], "entities": [{"text": "AmWN", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.691920816898346}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9993206262588501}, {"text": "precision 1", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9837920665740967}]}], "datasetContent": [{"text": "We follow here the experimental setup presented in , testing the generated rules on the ACE 2005 event dataset . This standard IE benchmark includes 33 types of event predicates such as Injure, Sue and Divorce 7 . The ACE guidelines specify for each event its possible arguments.", "labels": [], "entities": [{"text": "ACE 2005 event dataset", "start_pos": 88, "end_pos": 110, "type": "DATASET", "confidence": 0.969512864947319}, {"text": "ACE", "start_pos": 218, "end_pos": 221, "type": "DATASET", "confidence": 0.9339462518692017}]}, {"text": "For example, some of the Injure event arguments are Agent and Victim.", "labels": [], "entities": []}, {"text": "All event mentions, including their instantiated arguments, are annotated in a corpus collected from various sources (newswire articles, blogs, etc.).", "labels": [], "entities": []}, {"text": "To utilize the ACE dataset for evaluating rule applications, each ACE event predicate was represented by a set of unary seed templates, one for each event argument.", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.9547071754932404}]}, {"text": "Example seed templates for Injure are 'A injure' and 'injure V '.", "labels": [], "entities": []}, {"text": "Each event argument is mapped to the corresponding seed template variable, e.g. 'Agent' to A and 'Victim' to V in the above example.", "labels": [], "entities": []}, {"text": "We manually annotated each seed template with a subcategorization frame and an argument functional role, e.g. 'injure {trans} V obj '.", "labels": [], "entities": []}, {"text": "We also included relevant WordNet synset-ids, so only rules fitting the target meaning of the event will be extracted.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9659134745597839}]}, {"text": "In this experiment, we focused only on the core semantic arguments.", "labels": [], "entities": []}, {"text": "Adjuncts (time and  place) were ignored since they typically don't require argument mapping, the main target for our assessment.", "labels": [], "entities": []}, {"text": "The ACE corpus was dependency-parsed with Minipar ( and annotated with functional roles and frames for each predicate mention.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9380980432033539}]}, {"text": "The functional roles fora verb mention were taken directly from the corresponding dependency tree relations.", "labels": [], "entities": []}, {"text": "Its frame was chosen to be the largest WordNet frame of that verb that matched the mention's roles.", "labels": [], "entities": []}, {"text": "Nominalization frames and functional roles in the text were annotated using our extended Nomlex-plus database.", "labels": [], "entities": [{"text": "Nomlex-plus database", "start_pos": 89, "end_pos": 109, "type": "DATASET", "confidence": 0.9322843551635742}]}, {"text": "For each nominal mention, we found the largest Nomlex frame whose syntactic argument positions matched those of the mention's arguments.", "labels": [], "entities": []}, {"text": "The arguments were then annotated with the specified roles of the chosen frame.", "labels": [], "entities": []}, {"text": "Ambiguous cases, where the same argument position could match multiple roles, were left unannotated, as discussed in Section 2.", "labels": [], "entities": []}, {"text": "Argument mentions for events were found in the annotated corpus by matching either the seed templates or the templates entailing them in some rules.", "labels": [], "entities": []}, {"text": "The matching procedure follows the one described in Section 2.", "labels": [], "entities": []}, {"text": "Templates are matched using a syntactic matcher that handles simple syntactic variations such as passive-form and conjunctions.", "labels": [], "entities": []}, {"text": "For example, 'wound {trans} V obj \u21d2 injure {trans} V obj ' was matched in the text \"Hagel obj was wounded trans in Vietnam\".", "labels": [], "entities": []}, {"text": "A rule application is considered correct if the matched argument is annotated in the corpus with the corresponding ACE role.", "labels": [], "entities": []}, {"text": "We note that our system performance on the ACE task as such is limited.", "labels": [], "entities": [{"text": "ACE task", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.6694855391979218}]}, {"text": "First, WordNet does not provide all types of needed rules.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9798088669776917}]}, {"text": "Second, the system of our experimental setting is rather basic, with limited matching capabilities and without a WSD module.", "labels": [], "entities": [{"text": "WSD", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.6490252614021301}]}, {"text": "However, this test-set is still very useful for relative comparison of WordNet and our proposed AmWN.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9713190793991089}, {"text": "AmWN", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.9842707514762878}]}], "tableCaptions": [{"text": " Table 3: Recall (R), Precision (P) and F1 results  for the different tested configurations.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9497048258781433}, {"text": "Precision (P)", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.965520054101944}, {"text": "F1", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9945119619369507}]}, {"text": " Table 4: Distribution of reasons for false positives  (incorrect argument extractions).", "labels": [], "entities": [{"text": "argument extractions", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7380579710006714}]}, {"text": " Table 5: Distribution of reasons for false negatives  (missed argument mentions).", "labels": [], "entities": []}, {"text": " Table 6: The Recall (R), Precision (P) and F1 re- sults for ablation tests.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.964689090847969}, {"text": "Precision (P)", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.9672368615865707}, {"text": "F1 re- sults", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.9288430660963058}]}]}