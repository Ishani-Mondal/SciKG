{"title": [{"text": "Modular resource development and diagnostic evaluation framework for fast NLP system improvement", "labels": [], "entities": []}], "abstractContent": [{"text": "Natural Language Processing systems are large-scale softwares, whose development involves many man-years of work, in terms of both coding and resource development.", "labels": [], "entities": []}, {"text": "Given a dictionary of 110k lemmas, a few hundred syntactic analysis rules, 20k ngrams matrices and other resources, what will be the impact on a syntactic analyzer of adding anew possible category to a given verb?", "labels": [], "entities": []}, {"text": "What will be the consequences of anew syntactic rules ad-dition?", "labels": [], "entities": []}, {"text": "Any modification may imply, besides what was expected, unforeseeable side-effects and the complexity of the system makes it difficult to guess the overall impact of even small changes.", "labels": [], "entities": []}, {"text": "We present here a framework designed to effectively and iteratively improve the accuracy of our linguistic analyzer LIMA by iterative refinements of its linguistic resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9986748695373535}, {"text": "linguistic analyzer LIMA", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.6713812748591105}]}, {"text": "These improvements are continuously assessed by evaluating the analyzer performance against a reference corpus.", "labels": [], "entities": []}, {"text": "Our first results show that this framework is really helpful towards this goal.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In Natural Language Processing (NLP), robustness and reliability of linguistic analyzers becomes an everyday more addressed issue, given the increasing size of resources and the amount of code implied by the implementation of such systems.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7510092159112295}]}, {"text": "Beyond choosing a sound technology, one must now have efficients and user-friendly tools around the system itself, for evaluating its accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9905114769935608}]}, {"text": "As shown by, where developers receive daily reports of system's performance for improving their system, systematic evaluation with regression testing has shown to be gainful to accelerate grammar engineering.", "labels": [], "entities": [{"text": "grammar engineering", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.7997138500213623}]}, {"text": "Evaluation campaigns, where several participants evaluate their system's performance on a specific task against other systems, area good mean to search for directions in which a system maybe able to improve its performance.", "labels": [], "entities": []}, {"text": "Often, these evaluation campaigns also give possibility for participants to run their analyzer on test data and retrieve evaluation results.", "labels": [], "entities": []}, {"text": "In this context, parsers authors may rely on evaluation campaigns to provide performance results, but they should also be able to continuously evaluate and improve their analyzers between evaluation campaigns.", "labels": [], "entities": []}, {"text": "We aim at providing such a generic evaluation tool, using evaluation data to assess systems accuracy, this software will be referenced as the \"Benchmarking Tool\".", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9949113726615906}]}, {"text": "Approaches concerning Natural Language Processing involve everyday more and more resource data for analyzing texts.", "labels": [], "entities": []}, {"text": "These resources have grown enough (in terms of volume and diversity), that it now becomes a challenge to manipulate them, even for experienced users.", "labels": [], "entities": []}, {"text": "Moreover, it is needed to have non-developers being able to work on these resources: it is necessary to develop accessible tools through intuitive graphical user interfaces.", "labels": [], "entities": []}, {"text": "Such a resource editing GUI tool represent the second part of our contribution, called the \"Resource Tool\".", "labels": [], "entities": []}, {"text": "The overall picture is to build a diagnostic framework enabling a language specialist, such as a linguist, to status, almost in real-time, how modifica-tions impact our analyzer on as much test data as possible.", "labels": [], "entities": []}, {"text": "For analyzers, each resource may have an effect on the final accuracy of the analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9927217364311218}]}, {"text": "It is often needed to iterate over tests before understanding what resource, what part of the code needs to be improved.", "labels": [], "entities": []}, {"text": "This is especially the case with grammar engineering, where it is difficult to predict the consequences of modifying a single rule.", "labels": [], "entities": []}, {"text": "Ideally, our framework would allow the manipulator to slightly alter a resource, trigger an evaluation and, almost instantaneously, view results and interpret them.", "labels": [], "entities": []}, {"text": "With this framework, we expect a large acceleration in the process of improving our analyzer.", "labels": [], "entities": []}, {"text": "In the remaining of this introduction, we will describe our analyzer and Passage, a collaborative project including an evaluation campaign and the production of a reference treebank for French through a voting procedure.", "labels": [], "entities": [{"text": "Passage", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9709091186523438}]}, {"text": "Section 2 will describe our evaluation framework; its architecture, its two main modules and our first results using it.", "labels": [], "entities": []}, {"text": "Section 3 describes some related works.", "labels": [], "entities": []}, {"text": "We conclude in section 4 by describing the next steps of our work.", "labels": [], "entities": []}, {"text": "We are constantly recalled that evaluation metrics and methodologies evolve and are subject to intense research and innovation).", "labels": [], "entities": []}, {"text": "Discussing these metrics is not in the scope of this paper, we only need to be able to workout as many metrics as possible on the entire corpus or on any part of it.", "labels": [], "entities": []}, {"text": "The evaluation is supposed, for each document d and for each type (of chunk or of dependency) t within all types set T , to return following counts: \u2022 Number of items found and correct -f c(d, t) With this approach, we are able to compute common Information Retrieval (IR) metrics: precision, recall, f-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 282, "end_pos": 291, "type": "METRIC", "confidence": 0.9993076324462891}, {"text": "recall", "start_pos": 293, "end_pos": 299, "type": "METRIC", "confidence": 0.997880220413208}]}, {"text": "We also introduce anew metric that gives us indications about what types are the most lowering overall performance, called \"Type error ratio\": This metric counts the number of errors and misses fora given type reported to the total number of errors and misses.", "labels": [], "entities": [{"text": "Type error ratio", "start_pos": 124, "end_pos": 140, "type": "METRIC", "confidence": 0.814084788163503}]}, {"text": "It allows us to quantify how much an improvement on a given type will improve the overall score.", "labels": [], "entities": []}, {"text": "In our case, scores are computed for chunks on the one hand, and for dependencies on the other hand.", "labels": [], "entities": []}, {"text": "For instance, we have notices that GN errors represent 34.6% of the chunks errors, whereas PV only represent 2.2%: we are thus much more interested in improving detection of GN than PV regarding current evaluation campaign.", "labels": [], "entities": []}, {"text": "We need our framework to be portable and to be implemented using an agile approach: each new version should be fully functional while adding some more features.", "labels": [], "entities": []}, {"text": "It also must be user-friendly, allowing to easily add eye-candy features.", "labels": [], "entities": []}, {"text": "Consequently, we have chosen to implement these tools in C++, using the Qt 4.5 library 2 . This library satisfies our requirements and will allow to rely on stable and open source (LGPL) tools, making it feasible for us to possibly deliver our framework as a free software.", "labels": [], "entities": []}, {"text": "This approach allows us to quickly deliver working software while continuously testing and developing it.", "labels": [], "entities": []}, {"text": "Iterations of this process are still occurring but the current version, with its core functions, already succeeded in running benchmarks and in beginning the improvement of our linguistic resources while regularly delivering upgraded versions of our framework.", "labels": [], "entities": []}, {"text": "First results of this work will be presented below in this paper.", "labels": [], "entities": []}, {"text": "The open architecture we have chosen implies to use externals tools, for analysis and evaluation on the one hand, for compiling and installing resources on the other hand.", "labels": [], "entities": []}, {"text": "These tools may then be considered as black boxes, being externals commands called with convenient parameters.", "labels": [], "entities": []}, {"text": "In particular, the Benchmarking Tool relies on two commands: the analyzer command, receiving input file as a parameter and producing the analyzed file, the evaluation command, receiving the analyzed file and the reference file as parameters and outputting counts of found, correct, found and correct items for each dimension.", "labels": [], "entities": []}, {"text": "This allows, for example, to replace our analyzer with another one, by just wrapping the latter in a thin conversion layer to convert its inputs and its outputs.", "labels": [], "entities": []}], "tableCaptions": []}