{"title": [{"text": "The Role of Interactivity in Human-Machine Conversation for Automatic Word Acquisition", "labels": [], "entities": [{"text": "Automatic Word Acquisition", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.5967234571774801}]}], "abstractContent": [{"text": "Motivated by the psycholinguistic finding that human eye gaze is tightly linked to speech production, previous work has applied naturally occurring eye gaze for automatic vocabulary acquisition.", "labels": [], "entities": [{"text": "automatic vocabulary acquisition", "start_pos": 161, "end_pos": 193, "type": "TASK", "confidence": 0.6507070859273275}]}, {"text": "However, unlike in the typical settings for psycholin-guistic studies, eye gaze can serve different functions in human-machine conversation.", "labels": [], "entities": []}, {"text": "Some gaze streams do not link to the content of the spoken utterances and thus can be potentially detrimental to word acquisition.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.7952462732791901}]}, {"text": "To address this problem , this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.7893864512443542}]}, {"text": "Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.7433260977268219}]}], "introductionContent": [{"text": "Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (), intelligent space station control, and automated training and education ().", "labels": [], "entities": []}, {"text": "As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation.", "labels": [], "entities": [{"text": "robust language interpretation", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.7313940127690634}]}, {"text": "To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (;) to improve interpretation.", "labels": [], "entities": []}, {"text": "Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 110, "end_pos": 132, "type": "TASK", "confidence": 0.7015273571014404}, {"text": "reference resolution", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.8762633800506592}]}, {"text": "Recent work has also shown that what users look at on the interface (e.g., natural scenes or generated graphic displays) during speech production provides unique opportunities for word acquisition, namely automatically acquiring semantic meanings of spoken words by grounding them to visual entities ( or domain concepts (.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 180, "end_pos": 196, "type": "TASK", "confidence": 0.7765139639377594}]}, {"text": "Psycholinguistic studies have shown that eye gaze indicates a person's attention, and eye movement can facilitate spoken language comprehension ( . It has been found that users' eyes move to the mentioned object directly before speaking a word).", "labels": [], "entities": []}, {"text": "This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.7870706617832184}]}, {"text": "However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies.", "labels": [], "entities": []}, {"text": "There are different types of eye movements.", "labels": [], "entities": []}, {"text": "The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking ().", "labels": [], "entities": [{"text": "turn taking", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.7094254344701767}]}, {"text": "Furthermore, while interacting with a graphic display, a user could be talking about objects that were previously seen on the display or something completely unrelated to any object the user is looking at.", "labels": [], "entities": []}, {"text": "Therefore using every speechgaze pair for word acquisition can be detrimental.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.767180472612381}]}, {"text": "The type of gaze that is mostly useful for word acquisition is the kind that reflects the underlying attention and tightly links to the content of the cooccurring speech.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.8090046942234039}]}, {"text": "Thus, one important question is how to identify the closely coupled speech and gaze streams to improve word acquisition.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.7916470170021057}]}, {"text": "To address this question, we develop an approach that incorporates interactivity (e.g., speech, user activity, conversation context) with eye gaze to identify closely coupled speech and gaze streams.", "labels": [], "entities": []}, {"text": "We further use the identified speech and gaze streams to acquire words with a translation model.", "labels": [], "entities": []}, {"text": "Our empirical evaluation demonstrates that automatic identification of closely coupled gaze-speech streams can lead to significantly better word acquisition performance.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.7578932046890259}]}], "datasetContent": [{"text": "Given the extracted features and the \"closely coupled\" label of each instance in the training set, we train a logistic regression classifier (le to predict whether an instance is a closely coupled gaze-speech instance.", "labels": [], "entities": []}, {"text": "Since the goal of identifying closely coupled gaze-speech instances is to improve word acquisition and we are only interested in acquiring nouns and adjectives, only the instances with recognized nouns/adjectives are used for training the logistic regression classifier.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.7551098763942719}]}, {"text": "Among the 2969 instances with recognized nouns/adjectives and gaze fixations, 2002 (67.4%) instances are labeled as \"closely coupled\".", "labels": [], "entities": []}, {"text": "The prediction is evaluated by a 10-fold cross validation.: Gaze-speech prediction performance for the instances with 1-best speech recognition shows the prediction precision and recall when different sets of features are used.", "labels": [], "entities": [{"text": "Gaze-speech", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.9775661826133728}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.886713981628418}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9988901019096375}]}, {"text": "As seen in the table, as more features are used, the prediction precision goes up and the recall goes down.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.5543045997619629}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9997066855430603}]}, {"text": "It is important to note that prediction precision is more critical than recall for word acquisition when sufficient amount data is available.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9404926896095276}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9983487129211426}, {"text": "word acquisition", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7891364991664886}]}, {"text": "Noisy instances where the gaze is not coupled with the speech content will only hurt word acquisition since they will guide the translation models to ground words to the wrong entities.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.745677575469017}]}, {"text": "Although higher recall can be helpful, its effect is expected to be reduced when more data becomes available.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9994495511054993}]}, {"text": "Every conversational system has an initial vocabulary where words are associated with domain concepts of entities.", "labels": [], "entities": []}, {"text": "In our evaluation, we assume that the system's vocabulary has one default word for each entity that indicates the semantic type of the entity.", "labels": [], "entities": []}, {"text": "For example, the word \"barrel\" is the default word for the entity barrel.", "labels": [], "entities": []}, {"text": "For each entity, we only evaluate those new words that are not in the system's vocabulary.", "labels": [], "entities": []}, {"text": "The acquired words are evaluated against the \"gold standard\" words that were manually compiled for each entity and its properties based on all users' speech transcripts.", "labels": [], "entities": []}, {"text": "For the 115 entities in our domain, each entity has 1 to 20 \"gold standard\" words.", "labels": [], "entities": []}, {"text": "The average number of \"gold standard\" words for an entity is 6.7.", "labels": [], "entities": []}, {"text": "We evaluate the n-best acquired words (words grounded to domain concepts of entities) using precision, recall, and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.999584972858429}, {"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.998853325843811}, {"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9976617097854614}]}, {"text": "When a different n is chosen, we will have different precision, recall, and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9997555613517761}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9997604489326477}, {"text": "F-measure", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9981362819671631}]}, {"text": "We also evaluate the whole ranked candidate word list on Mean Reciprocal Rank Rate (MRRR) as in (: where Ne is the number of all \"gold standard\" words {w i e } for entity e, index(w i e ) is the index of word w i e in the ranked list of candidate words for entity e.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank Rate (MRRR)", "start_pos": 57, "end_pos": 89, "type": "METRIC", "confidence": 0.9611266255378723}]}, {"text": "MRRR measures how close the ranks of the \"gold standard\" words in the candidate word lists are to the best-case scenario where the top Ne words are the \"gold standard\" words fore.", "labels": [], "entities": [{"text": "MRRR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6529874801635742}]}, {"text": "The higher the MRRR, the better is the acquisition performance.", "labels": [], "entities": [{"text": "MRRR", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9363768696784973}, {"text": "acquisition", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9084030389785767}]}, {"text": "We evaluate the effect of the closely coupled gazespeech instances on word acquisition from the 1-best speech recognition and speech transcript.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7754978537559509}, {"text": "speech recognition", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7244157493114471}]}, {"text": "The predicted closely coupled gaze-speech instances are generated by a 10-fold cross validation with the logistic regression classifier.", "labels": [], "entities": []}, {"text": "shows the precision, recall, and Fmeasure of the n-best words acquired from 1-best speech recognition by Model-2t using all instances (all), predicted coupled instances (predicted), and true (manually labeled) coupled instances (true).", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995473027229309}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9990311861038208}, {"text": "Fmeasure", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9997267127037048}, {"text": "speech recognition", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.7354726195335388}]}, {"text": "As shown in the figure, using predicted coupled instances achieves consistently better performance than using all instances.", "labels": [], "entities": []}, {"text": "These results show that the identification of coupled gaze-speech prediction helps word acquisition.", "labels": [], "entities": [{"text": "coupled gaze-speech prediction", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.640146404504776}, {"text": "word acquisition", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.808527797460556}]}, {"text": "When the true coupled instances are used, the performance is further improved.", "labels": [], "entities": []}, {"text": "This means that reliable identification of coupled gaze-speech instances can lead to better word acquisition.", "labels": [], "entities": [{"text": "word acquisition", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7703806161880493}]}, {"text": "shows the precision, recall, and Fmeasure of the n-best words acquired from speech transcript by Model-2t using all instances, predicted coupled instances, and true coupled instances.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995868802070618}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9992031455039978}, {"text": "Fmeasure", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.99973064661026}]}, {"text": "Consistent with the performance based on the 1-best speech recognition, we can observe that automatic identification of coupled instances results in better word acquisition performance and using the true coupled instances results in even better performance.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7062162309885025}, {"text": "word acquisition", "start_pos": 156, "end_pos": 172, "type": "TASK", "confidence": 0.7326306700706482}]}, {"text": "presents the MRRRs achieved by Model-2t when words are acquired from different speech input (speech transcript, 1-best recognition) with different set of instances (all instances, predicted coupled instances, true coupled instances).", "labels": [], "entities": []}, {"text": "These results also show the consistent behavior.", "labels": [], "entities": []}, {"text": "Using predicted coupled instances achieves significantly better MRRR than using all instances no matter the words are acquired from 1-best speech recognition (t = 2.59, p < 0.006) or speech transcript(t = 3.15, p < 0.002).", "labels": [], "entities": [{"text": "MRRR", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.707904577255249}]}, {"text": "When the true coupled instances are used, the performances are further improved for both 1-best recognition (t = 2.29, p < 0.013) and speech transcript (t = 5.21, p < 0.001) compared to using predicted coupled instances.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Gaze-speech prediction performance for  the instances with 1-best speech recognition", "labels": [], "entities": [{"text": "Gaze-speech prediction", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8318431973457336}]}]}