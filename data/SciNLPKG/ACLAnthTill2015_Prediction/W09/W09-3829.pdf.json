{"title": [{"text": "Clustering Words by Syntactic Similarity Improves Dependency Parsing of Predicate-Argument Structures", "labels": [], "entities": [{"text": "Syntactic Similarity Improves Dependency Parsing of Predicate-Argument Structures", "start_pos": 20, "end_pos": 101, "type": "TASK", "confidence": 0.7811895869672298}]}], "abstractContent": [{"text": "We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntactic contexts.", "labels": [], "entities": [{"text": "syntactic word clusters from parsed text", "start_pos": 36, "end_pos": 76, "type": "TASK", "confidence": 0.7304907639821371}]}, {"text": "We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.998327910900116}]}, {"text": "In our experiments , we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9960772395133972}]}], "introductionContent": [{"text": "Syntactic parsing of natural language has advanced greatly in recent years, in large part due to data-driven techniques) coupled with the availability of large treebanks.", "labels": [], "entities": [{"text": "Syntactic parsing of natural language", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8976974844932556}]}, {"text": "Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available.", "labels": [], "entities": []}, {"text": "Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting () and stacking, and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (;.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9943225383758545}]}, {"text": "In this paper we explore anew way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure.", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9623175859451294}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9290650486946106}]}, {"text": "In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (, we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we use to produce syntactically derived clusters that are then used to improve a transition-based parser that outputs dependency graphs that reflect predicate-argument structure where words maybe dependents of more than one parent.", "labels": [], "entities": []}, {"text": "This type of representation is more general than dependency trees, and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement).", "labels": [], "entities": []}, {"text": "The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context ( or by immediate dependency context.", "labels": [], "entities": [{"text": "syntactic word clusters from parsed text", "start_pos": 69, "end_pos": 109, "type": "TASK", "confidence": 0.7428302864233652}]}, {"text": "Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior.", "labels": [], "entities": []}, {"text": "This way, we attempt to generate clusters that are more conceptually similar to part-of-speech tags or supertags), but organized hierarchically to provide tagsets with varying levels of granularity.", "labels": [], "entities": []}, {"text": "Our second contribution is a methodology for leveraging a high-accuracy parser to improve the accuracy of a parser that uses a different formalism (that represents different structural information), without the need to process the input with both parsers at run-time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9980877041816711}]}, {"text": "In our experiments, we show that we can improve the accuracy of a fast dependency parser for predicate-argument structures by using a corpus which was previously automatically annotated using a highly accurate but considerably slower phrase-structure tree parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9992647767066956}]}, {"text": "This is accomplished by using the slower parser only to parse the data used to create the syntactic word clusters.", "labels": [], "entities": []}, {"text": "During run-time, the dependency parser uses these clusters, which encapsulate syntactic knowledge from the phrase-structure parser.", "labels": [], "entities": []}, {"text": "Although our experiments focus on the use of phrase-structure and dependency parsers, the same framework can be easily applied to data-driven parsing using other syntactic formalisms, such as CCG or HPSG.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.9039396643638611}]}], "datasetContent": [{"text": "Following previous experiments with Penn Treebank WSJ data, or annotations derived from it, we used sections 02-21 of the HPSG Treebank as training material, section 22 for development, and section 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank WSJ data", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.9869457930326462}, {"text": "HPSG Treebank", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.9913508296012878}]}, {"text": "Only the predicateargument dependencies were used, not the phrase structures or other information from the HPSG analyses.", "labels": [], "entities": [{"text": "HPSG analyses", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9294274747371674}]}, {"text": "For all experiments described here, part-of-speech tagging was done separately using a CRF tagger with accuracy of 97.3% on sections 22-24.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7927893698215485}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.999397873878479}]}, {"text": "Our evaluation is based on labeled precision and recall of predicate-argument dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9821082353591919}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9982061386108398}]}, {"text": "Although accuracy is commonly used for evaluation of dependency parsers, in our task the parser is not restricted to output a fixed number of dependencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9988001585006714}]}, {"text": "Labeled precision and recall of predicate-argument pairs are also the standard evaluation metrics for data-driven HPSG and CCG parsers (although the predicate-argument pairs extracted from the HPSG Treebank and the CCGBank are specific to their formalisms and not quantitatively comparable).", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9911578297615051}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.998151957988739}, {"text": "HPSG Treebank", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.9838259220123291}]}, {"text": "We started by eliminating cycles from the dependency graphs extracted from the HPSG Treebank by using the arc reversal transform in the following way: for each cycle detected in the data, the shortest arc in the cycle was reversed until no cycles remained.", "labels": [], "entities": [{"text": "HPSG Treebank", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9876774549484253}]}, {"text": "We then applied pseudo-projective transformation to create data that can be used to train our parser, described in section 3.", "labels": [], "entities": []}, {"text": "By detransforming the projective graphs generated from gold-standard dependencies, we obtain labeled precision of 98.1% and labeled recall of 97.7%, which is below the accuracy expected for detransformation of syntactic dependency trees.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.5605113506317139}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.8652063608169556}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9943050742149353}]}, {"text": "This is expected, since arc crossing occurs more frequently in predicateargument graphs in the HPSG Treebank than in surface syntactic dependencies.", "labels": [], "entities": [{"text": "HPSG Treebank", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.987527072429657}]}, {"text": "We first trained a parsing model without cluster-based features, using only the baseline set of features, which was the product of experimentation using the development set.", "labels": [], "entities": []}, {"text": "On the test set, this baseline model has labeled precision and recall of 88.7 and 88.2, respectively, slightly below the precision and recall obtained by Sagae and Tsujii on the same data (89.0 precision and 88.5 recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.964288592338562}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9987309575080872}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9990456700325012}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9946854114532471}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.913776695728302}, {"text": "recall", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.8837911486625671}]}, {"text": "We then used the development set to explore the effects of cluster sets with different levels of granularity.", "labels": [], "entities": []}, {"text": "The baseline model has precision and recall of 88.6 and 88.0 on the development set.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9998559951782227}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9997710585594177}]}, {"text": "We found that by slicing the cluster tree relatively close to the root, resulting in a set of 50 to 100 distinct cluster labels (corresponding to relatively coarse clusters), we obtain small (0.3 to 0.4), but statistically significant (p < 0.005) improvements on precision and recall over the baseline model on the development set.", "labels": [], "entities": [{"text": "precision", "start_pos": 263, "end_pos": 272, "type": "METRIC", "confidence": 0.9993798732757568}, {"text": "recall", "start_pos": 277, "end_pos": 283, "type": "METRIC", "confidence": 0.9994213581085205}]}, {"text": "By increasing the number of cluster labels (making the distinctions among members of different clusters more fine-grained) in steps of 100, we observed improvements in precision and recall until the point where there were 600 distinct cluster labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9993022680282593}, {"text": "recall", "start_pos": 182, "end_pos": 188, "type": "METRIC", "confidence": 0.9991199374198914}]}, {"text": "This set of 600 cluster labels produced the highest values of precision and recall (89.5 and 89.0) that we obtained for the development set using only one set of cluster labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9996453523635864}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9992669224739075}]}, {"text": "shows how precision, recall and F-score on the development set varied with the number of cluster labels used.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995558857917786}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9996330738067627}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9988518953323364}]}, {"text": "Following, we also experimented with using two sets of cluster labels with different levels of granularity.", "labels": [], "entities": []}, {"text": "We found that using the set of 600 labels and an additional set with fewer than 600 labels did not improve or hurt precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9993890523910522}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9979841709136963}]}, {"text": "Finer grained clusters with more than 1,000 labels (combined with the set of 600 labels) improved results further.", "labels": [], "entities": []}, {"text": "The highest precision and recall figures of 90.1 and 89.6 were obtained with the sets of 600 and 1,400 labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9995985627174377}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9996215105056763}]}, {"text": "We parsed the test set using the best configuration of cluster-based features as determined using the development set (the sets with 600 and 1,400 cluster labels) and obtained 90.2 precision, 89.8 recall and 90.0 f-score, a 13.8% reduction in error over a strong baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.9894801378250122}, {"text": "recall", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.9872481226921082}, {"text": "f-score", "start_pos": 213, "end_pos": 220, "type": "METRIC", "confidence": 0.9875582456588745}, {"text": "error", "start_pos": 243, "end_pos": 248, "type": "METRIC", "confidence": 0.974745512008667}]}, {"text": "summarizes our results on the test set.", "labels": [], "entities": []}, {"text": "For comparison, we also shows results published by, to our knowledge the highest f-score reported for this test set, and, who first reported results on this data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results obtained on the test set us- ing our baseline model and our best cluster- based features. The results in the bottom two  rows are from", "labels": [], "entities": []}]}