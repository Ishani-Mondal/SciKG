{"title": [{"text": "The GREC Main Subject Reference Generation Challenge 2009: Overview and Evaluation Results", "labels": [], "entities": [{"text": "GREC Main Subject Reference Generation Challenge 2009", "start_pos": 4, "end_pos": 57, "type": "TASK", "confidence": 0.6490962590490069}]}], "abstractContent": [{"text": "The GREC-MSR Task at Generation Challenges 2009 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia.", "labels": [], "entities": [{"text": "GREC-MSR Task at Generation Challenges 2009", "start_pos": 4, "end_pos": 47, "type": "DATASET", "confidence": 0.698017011086146}]}, {"text": "Three teams submitted one system each, and we additionally created four baseline systems.", "labels": [], "entities": []}, {"text": "Systems were tested automatically using existing intrinsic metrics.", "labels": [], "entities": []}, {"text": "We also evaluated systems extrinsically by applying corefer-ence resolution tools to the outputs and measuring the success of the tools.", "labels": [], "entities": []}, {"text": "In addition , systems were tested in an intrinsic evaluation involving human judges.", "labels": [], "entities": []}, {"text": "This report describes the GREC-MSR Task and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results.", "labels": [], "entities": [{"text": "GREC-MSR Task", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.5931218564510345}]}], "introductionContent": [{"text": "The GREC-MSR Task is about how to generate appropriate references to an entity in the context of apiece of discourse longer than a sentence.", "labels": [], "entities": [{"text": "GREC-MSR", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.5388781428337097}]}, {"text": "Rather than requiring participants to generate referring expressions from scratch, the GREC-MSR data provides sets of possible referring expressions for selection.", "labels": [], "entities": [{"text": "GREC-MSR data", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9108063280582428}]}, {"text": "This was the second time we ran a shared task using the GREC-MSR data (following a first run in 2008).", "labels": [], "entities": [{"text": "GREC-MSR data", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9030935764312744}]}, {"text": "The task definition was again kept fairly simple, but in the 2009 round the main aim for participating systems was to select an appropriate word string to serve as a referring expression, whereas in 2008 it was to select an appropriate type of referring expression (name, common noun, pronoun, or empty reference).", "labels": [], "entities": []}, {"text": "The immediate motivating application context for the GREC-MSR Task is the improvement of referential clarity and coherence in extractive summaries by regenerating referring expressions in them.", "labels": [], "entities": [{"text": "GREC-MSR Task", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.4643021523952484}]}, {"text": "There has recently been a small flurry of work in this area (.", "labels": [], "entities": []}, {"text": "In the longer term, the GREC-MSR Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context.", "labels": [], "entities": []}, {"text": "The GREC-MSR data is an extension of the GREC 1.0 Corpus which had about 1,000 texts in the subdomains of cities, countries, rivers and people ().", "labels": [], "entities": [{"text": "GREC-MSR data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9421639442443848}, {"text": "GREC 1.0 Corpus", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9288725654284159}]}, {"text": "For the purpose of the GREC-MSR shared task, an additional 1,000 texts in the new subdomain of mountain texts were obtained and anew XML annotation scheme (Section 2.2) was developed.", "labels": [], "entities": [{"text": "GREC-MSR shared task", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.5941128929456075}]}], "datasetContent": [{"text": "Accuracy of REFEX word strings: when computed against test sets (C-1, Land P), Word String Accuracy is simply the proportion of REFEX word strings selected by a participating system that are identical to the one in the corpus.", "labels": [], "entities": []}, {"text": "When computed against test set C-2, which has three versions of each text, Word String Accuracy is computed as follows: first the number of correct REFEX word strings is computed at the text level for each of the three versions of a text and the maximum of these is determined; then the maximum text-level numbers are summed and divided by the total number of REFs in all the texts, which gives the global Word String Accuracy score.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.7930038571357727}, {"text": "Word String Accuracy score", "start_pos": 406, "end_pos": 432, "type": "METRIC", "confidence": 0.7069129794836044}]}, {"text": "The rationale behind computing the Word String Accuracy scores in this way for multiple-RE test sets (maximising scores on RE chains rather than individual REs) is that an RE is not good or bad in its own right, but depends on other MSREs in the same text.", "labels": [], "entities": []}, {"text": "Accuracy of REG08-Type: similarly to Word String Accuracy above, when computed against test sets C-1, Land P, REG08-Type Accuracy is the proportion of REFEXs selected by a participating system that have a REG08-TYPE value identical to the one in the corpus.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9806367754936218}]}, {"text": "When computed against test set C-2, first the number of correct REG08-Types is computed at the text level for each of the three versions of a corpus text and the maximum of these is determined; then the maximum text-level numbers are summed and divided by the total number of REFs in all the texts, which gives the global REG08-Type Accuracy score.", "labels": [], "entities": [{"text": "REG08-Type Accuracy score", "start_pos": 322, "end_pos": 347, "type": "METRIC", "confidence": 0.705196738243103}]}, {"text": "String-edit distance metrics: String-edit distance (SE) is straightforward Levenshtein distance with a substitution cost of 2 and insertion/deletion cost of 1.", "labels": [], "entities": [{"text": "String-edit distance (SE)", "start_pos": 30, "end_pos": 55, "type": "METRIC", "confidence": 0.6454275608062744}]}, {"text": "We also used a length-normalised version of string-edit distance (denoted 'norm. SE' in results tables below).", "labels": [], "entities": [{"text": "SE", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.5199900269508362}]}, {"text": "For test sets C-1, Land P, the global score is simply the mean of all RE-level scores.", "labels": [], "entities": [{"text": "RE-level", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8921716213226318}]}, {"text": "For Test Set C-2, the global score is the mean of the mean of the three text-level scores.", "labels": [], "entities": [{"text": "Test Set C-2", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8277124365170797}]}, {"text": "Other metrics: BLEU is a precision metric from machine translation that assesses peer translations in terms of the proportion of word n-grams (n \u2264 4 is standard) they share with several reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9986349940299988}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9939956068992615}, {"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7350091487169266}]}, {"text": "We used BLEU-3 rather than the more standard BLEU-4 because most REs in the corpus are less than 4 tokens long.", "labels": [], "entities": [{"text": "BLEU-3", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9936649799346924}, {"text": "BLEU-4", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9771541953086853}]}, {"text": "We also used the NIST version of BLEU which weights in favour of less frequent n-grams.", "labels": [], "entities": [{"text": "NIST", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.9139583110809326}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9971902966499329}]}, {"text": "In both cases, we assessed just the MSREs selected by peer systems (leaving out the surrounding text), and computed scores globally (rather than averaging over RE-level scores), as this is standard for these metrics.", "labels": [], "entities": []}, {"text": "BLEU, and NIST are designed to work with one or multiple reference texts, so we did not need to use a different method for Test Set C-2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9606759548187256}, {"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.9471744894981384}]}, {"text": "As in GREC-MSR'08, we used an automatic extrinsic evaluation method based on coreference resolution performance.", "labels": [], "entities": [{"text": "GREC-MSR'08", "start_pos": 6, "end_pos": 17, "type": "DATASET", "confidence": 0.8338413834571838}, {"text": "coreference resolution", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.856721967458725}]}, {"text": "The basic idea is that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.874531626701355}]}, {"text": "To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolversthose included in LingPipe and OpenNLP)-and averaged results.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7575281858444214}, {"text": "LingPipe", "start_pos": 163, "end_pos": 171, "type": "DATASET", "confidence": 0.9596962332725525}, {"text": "OpenNLP", "start_pos": 176, "end_pos": 183, "type": "DATASET", "confidence": 0.7834722399711609}]}, {"text": "There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC-6 (, CEAF, and B-CUBED (, which seem to be the most widely accepted metrics.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.9610624611377716}, {"text": "MUC-6", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.5770575404167175}, {"text": "CEAF", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.5292860269546509}, {"text": "B-CUBED", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.9831417798995972}]}, {"text": "All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains.", "labels": [], "entities": [{"text": "Recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9696652293205261}, {"text": "Precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9777923822402954}, {"text": "F-Scores", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.965480387210846}]}, {"text": "They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores.", "labels": [], "entities": []}, {"text": "Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9959419369697571}]}, {"text": "The intrinsic human evaluation involved 24 randomly selected items from Test Set C and outputs for these produced by peer and basline systems as However, for GREC'09 we overhauled the tool; the current version no longer uses JavaRAP, and uses the most recent versions of the other resolvers; the GREC-MSR'08 and GREC-MSR'09 results for this method are not entirely comparable for this reason.", "labels": [], "entities": [{"text": "GREC'09", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.8689703345298767}, {"text": "GREC-MSR'08", "start_pos": 296, "end_pos": 307, "type": "DATASET", "confidence": 0.8252727389335632}, {"text": "GREC-MSR'09", "start_pos": 312, "end_pos": 323, "type": "DATASET", "confidence": 0.7238414287567139}]}, {"text": "well as those found in the original corpus texts (8 systems in total).", "labels": [], "entities": []}, {"text": "We used a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from each system and for each test set item.", "labels": [], "entities": []}, {"text": "There were three 8x8 squares, and a total of 576 individual judgments in this evaluation (72 per system: 3 criteria x 3 articles x 8 evaluators).", "labels": [], "entities": []}, {"text": "We recruited 8 native speakers of English from among post-graduate students currently doing a linguistics-related degree at University College London (UCL) and University of Sussex.", "labels": [], "entities": []}, {"text": "Following detailed instructions, subjects did two practice examples, followed by the 24 texts to be evaluated, in random order.", "labels": [], "entities": []}, {"text": "Subjects carried out the evaluation over the internet, at a time and place of their choosing.", "labels": [], "entities": []}, {"text": "They were allowed to interrupt and resume the experiment (though discorouged from doing so).", "labels": [], "entities": []}, {"text": "According to self-reported timings, subjects took between 25 and 45 minutes to complete the evaluation (not counting breaks).", "labels": [], "entities": []}, {"text": "shows what subjects saw during the evaluation of an individual text.", "labels": [], "entities": []}, {"text": "All references to the MS are highlighted in yellow, and the task is to evaluate the quality of the REs in terms of three criteria which were explained in the introduction as follows (the wording of the explanations of Criteria 1 and 3 were taken from the DUC evaluations): Subjects selected evaluation scores by moving sliders (see) along scales ranging from 1 to 5.", "labels": [], "entities": [{"text": "DUC evaluations", "start_pos": 255, "end_pos": 270, "type": "DATASET", "confidence": 0.8999677896499634}]}, {"text": "Slider pointers started out in the middle of the scale (3).", "labels": [], "entities": [{"text": "Slider pointers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7503420114517212}]}, {"text": "These were continuous scales and we recorded scores with one decimal place (e.g. 3.2).", "labels": [], "entities": []}, {"text": "The meaning of the numbers was explained in terms of integer scores (1=very poor, 2=poor, 3=neither poor nor good, 4=good, 5=very good).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Self-reported evaluation scores for devel- opment set.", "labels": [], "entities": []}, {"text": " Table 3: Word String Accuracy scores against Test Sets C-1, L and P; homogeneous subsets (Tukey HSD,  alpha = .05) for each test set (systems that do not share a letter are significantly different).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8894149661064148}, {"text": "Tukey HSD", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.7320423424243927}]}, {"text": " Table 4: Word String Accuracy scores against Test Set C-2 for complete set and for subdomains; homo- geneous subsets (Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are  significantly different).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9216103553771973}, {"text": "Tukey HSD", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.8544908463954926}]}, {"text": " Table 5: REG08-Type Accuracy, BLEU, NIST and string-edit scores, computed on test set C-2 (systems  in order of REG08-Type Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for REG08-Type  Accuracy only (systems that do not share a letter are significantly different).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9986029267311096}, {"text": "Tukey HSD", "start_pos": 156, "end_pos": 165, "type": "DATASET", "confidence": 0.7605441510677338}]}, {"text": " Table 7: MUC, CEAF and B-CUBED F-Scores for  all systems; homogeneous subsets (Tukey HSD),  alpha = .05, for mean of F-Scores.", "labels": [], "entities": [{"text": "MUC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7836761474609375}, {"text": "CEAF", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8164443373680115}, {"text": "B-CUBED F-Scores", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.8198305666446686}, {"text": "F-Scores", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.8908413052558899}]}]}