{"title": [{"text": "HPSG Supertagging: A Sequence Labeling View", "labels": [], "entities": [{"text": "HPSG Supertagging", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.7742738723754883}, {"text": "Sequence Labeling", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9101196229457855}]}], "abstractContent": [{"text": "Supertagging is a widely used speed-up technique for deep parsing.", "labels": [], "entities": [{"text": "deep parsing", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.6294591128826141}]}, {"text": "In another aspect, supertagging has been exploited in other NLP tasks than parsing for utilizing the rich syntactic information given by the supertags.", "labels": [], "entities": []}, {"text": "However, the performance of supertagger is still a bottleneck for such applications.", "labels": [], "entities": []}, {"text": "In this paper, we investigated the relationship between supertagging and parsing, not just to speedup the deep parser; We started from a sequence labeling view of HPSG supertagging, examining how well a supertagger can do when separated from parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.975792407989502}]}, {"text": "Comparison of two types of supertagging model, point-wise model and sequential model, showed that the former model works competitively well despite its simplicity, which indicates the true dependency among supertag assignments is far more complex than the crude first-order approximation made in the sequential model.", "labels": [], "entities": []}, {"text": "We then analyzed the limitation of separated supertagging by using a CFG-filter.", "labels": [], "entities": [{"text": "CFG-filter", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9636306166648865}]}, {"text": "The results showed that big gains could be acquired by resorting to a lightweight parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supertagging is an important part of lexicalized grammar parsing.", "labels": [], "entities": [{"text": "lexicalized grammar parsing", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6562474568684896}]}, {"text": "A high performance supertagger greatly reduces the load of a parser and accelerates its speed.", "labels": [], "entities": []}, {"text": "A supertag represents a linguistic word category, which encodes syntactic behavior of the word.", "labels": [], "entities": []}, {"text": "The concept of supertagging was first proposed for lexicalized tree adjoining grammar (LTAG) () and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG)) and Head-driven phrase structure grammar (HPSG) ().", "labels": [], "entities": [{"text": "Head-driven phrase structure grammar (HPSG)", "start_pos": 195, "end_pos": 238, "type": "TASK", "confidence": 0.7352779677936009}]}, {"text": "Recently, syntactic information in supertags has been exploited for NLP tasks besides parsing, such as NP chunking, semantic role labeling and machine translation (.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.8257845044136047}, {"text": "semantic role labeling", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.6576569080352783}, {"text": "machine translation", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.8047261536121368}]}, {"text": "Supertagging serves there as an implicit and convenient way to incorporate rich syntactic information in those tasks.", "labels": [], "entities": []}, {"text": "Improving the performance of supertagging can thus benefit these two aspects: as a preprocessor for deep parsing and as an independent, alternative technique for \"almost\" parsing.", "labels": [], "entities": []}, {"text": "However, supertags are derived from a grammar and thus have a strong connection to parsing.", "labels": [], "entities": []}, {"text": "To further improve the supertagging accuracy, the relation between supertagging and parsing is crucial.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.989477813243866}, {"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9800721406936646}]}, {"text": "With this motivation, we investigate how well a sequence labeling model can do when it is separated from a parser, and to what extent the ignorance of long distance dependencies in the sequence labeling formulation affects the supertagging results.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.6778596043586731}]}, {"text": "Specifically, we evaluated two different types of supertagging model, point-wise model and sequential model, for HPSG supertagging.", "labels": [], "entities": []}, {"text": "CFGfilter was then used to empirically evaluate the effect of long distance dependencies in supertagging.", "labels": [], "entities": [{"text": "CFGfilter", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9688526391983032}]}, {"text": "The point-wise model achieved competitive result of 92.53% accuracy on WSJ-HPSG treebank with fast training speed, while the sequential model augmented with supertag edge features did not give much further improvement over the point-wise model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9992550015449524}, {"text": "WSJ-HPSG treebank", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9682668447494507}]}, {"text": "Big gains acquired by using CFG-filter indicates that further improvement maybe achieved by resorting to a light-weight parser.", "labels": [], "entities": [{"text": "CFG-filter", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.8858687877655029}]}], "datasetContent": [{"text": "We conducted experiments on WSJ-HPSG treebank corpus), which was semiautomatically converted from the WSJ portion of PennTreebank.", "labels": [], "entities": [{"text": "WSJ-HPSG treebank corpus", "start_pos": 28, "end_pos": 52, "type": "DATASET", "confidence": 0.9724776943524679}, {"text": "WSJ portion of PennTreebank", "start_pos": 102, "end_pos": 129, "type": "DATASET", "confidence": 0.8921835273504257}]}, {"text": "The number of training iterations was set to 5 for all models.", "labels": [], "entities": []}, {"text": "Gold-standard POS tags are used as input.", "labels": [], "entities": []}, {"text": "The performance is evaluated by accuracy 1 and speed of supertagging on an AMD Opteron 2.4GHz server.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9996892213821411}]}, {"text": "shows the averaged results of 10-fold cross-validation of averaged perceptron (AP) models 2 on section 02-21.", "labels": [], "entities": []}, {"text": "We can seethe difference between point-wise AP model and sequential AP model is small (0.24%).", "labels": [], "entities": []}, {"text": "It becomes even smaller after CFG-filtering (0.11%).", "labels": [], "entities": [{"text": "CFG-filtering", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.7522180080413818}]}, {"text": "shows the supertagging accuracy on section 22 based on BPM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.967333197593689}, {"text": "BPM", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.7269151210784912}]}, {"text": "Although not statistically significantly different from previous ME model, point-wise model (PW-BPM) achieved competitive result 92.53% with faster training.", "labels": [], "entities": []}, {"text": "In addition, 0.27% and 0.29% gains were brought by using BPM from PW-AP (92.26%) and PW-SEQ (92.54%) with P-values less than 0.05.", "labels": [], "entities": [{"text": "BPM", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9183387756347656}, {"text": "PW-AP", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8991395831108093}, {"text": "PW-SEQ", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9079599976539612}]}, {"text": "The improvement by using sequential models (PW-AP\u2192SEQ-AP: 0.24%, PW-BPM\u2192SEQ-BPM: 0.3%, statistically significantly different),  compared to point-wise models, were not so large, but the training time was around 6 times longer.", "labels": [], "entities": []}, {"text": "We think the reason is twofold.", "labels": [], "entities": []}, {"text": "First, as previous research showed, POS sequence is very informative in supertagging.", "labels": [], "entities": [{"text": "POS sequence", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.8200344443321228}]}, {"text": "A large amount of local syntactic information can be captured in POS tags of surrounding words, although a few long-range dependencies are of course not.", "labels": [], "entities": []}, {"text": "Second, the number of supertags is large and the supertag edge features used in sequential model are inevitably suffered from data sparseness.", "labels": [], "entities": []}, {"text": "To alleviate this, we extracted sub-structure from lexical templates (i.e., lexical items corresponding to supertags) to augment the supertag edge features, but only got 0.16% improvement (SEQ-BPM+SUB).", "labels": [], "entities": [{"text": "SEQ-BPM+SUB", "start_pos": 189, "end_pos": 200, "type": "METRIC", "confidence": 0.6186075011889139}]}, {"text": "Furthermore, we also got 0.15% gains with Pvalue less than 0.05 by incorporating the substructure features into point-wise model (PW-BPM+SUB).", "labels": [], "entities": [{"text": "Pvalue", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9537993669509888}]}, {"text": "We hence conclude that the contribution of the first-order edge features is not large in sequence modeling for HPSG supertagging.", "labels": [], "entities": []}, {"text": "As we explained in Section 3.3, sequence labeling models have inherent limitation in the ability to capture long distance dependencies between supertags.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7395320236682892}]}, {"text": "This kind of ambiguity could be easier to solve in a parser.", "labels": [], "entities": []}, {"text": "To examine this, we added CFGfilter which works as an approximation of a full HPSG parser, after the sequence labeling model.", "labels": [], "entities": [{"text": "CFGfilter", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.8816460967063904}]}, {"text": "As expected, there came big gains of 1.26% (from PW-AP to PW-AP+CFG) and 1.15% (from PW-BPM to PW-BPM+CFG).", "labels": [], "entities": [{"text": "PW-AP", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9205601215362549}]}, {"text": "Even for the sequential model we also got 1.15% (from SEQ-AP to SEQ-AP+CFG) and 0.87% (from SEQ-BPM to SEQ-BPM+CFG) respectively.", "labels": [], "entities": []}, {"text": "All these models were statistically significantly different from orig-inal ones.", "labels": [], "entities": []}, {"text": "We also gave error analysis on test results.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.9247235357761383}]}, {"text": "Comparing SEQ-AP with SEQ-AP+CFG, one of the most frequent types of \"correct supertag\" by the CFG-filter was for word \"and\", wherein a supertag for NP-coordination (\"NP and NP\") was corrected to one for VP-coordination (\"VP and VP\" or \"S and S\").", "labels": [], "entities": [{"text": "CFG", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.8691747188568115}, {"text": "CFG-filter", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.913955807685852}]}, {"text": "It means the disambiguation between the two coordination type is difficult for supertaggers, presumably because they looks very similar with a limited length of context since the sequence of the NP-object of left conjunct, \"and\", the NP subject of right conjunct looks very similar to a NP coordination.", "labels": [], "entities": []}, {"text": "The different assignments by SEQ-AP+CFG from SEQ-AP include 725 right corrections, while it changes 298 correct predictions by SEQ-AP to wrong assignments.", "labels": [], "entities": [{"text": "SEQ-AP+CFG", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.5105326473712921}]}, {"text": "One possible reason for some of \"wrong correction\" is related to the approximation of grammar.", "labels": [], "entities": [{"text": "approximation of grammar", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6775711377461752}]}, {"text": "But this gives clue that for supertagging task: just using sequence labeling models is limited, and we can resort to use some light-weight parser to handle long distance dependencies.", "labels": [], "entities": []}, {"text": "Although some of the ambiguous supertags could be left for deep parsing, like multi-tagging technique), we also consider the tasks where supertags can be used while conducting deep parsing is too computationally costly.", "labels": [], "entities": []}, {"text": "Alternatively, focusing on supertagging, we could treat it as a sequence labeling task, while a consequent light-weight parser is a disambiguator with long distance constraint.", "labels": [], "entities": [{"text": "sequence labeling task", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7214298049608866}]}], "tableCaptions": [{"text": " Table 2: Averaged 10-cross validation of averaged  perceptron on Section 02-21.", "labels": [], "entities": [{"text": "Section 02-21", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9749190509319305}]}, {"text": " Table 3: Supertagging accuracy and training&  testing speed on section 22. ( \u2021) Test time was cal- culated on totally 1648 sentences.", "labels": [], "entities": [{"text": "Supertagging", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9032004475593567}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.987727165222168}]}]}