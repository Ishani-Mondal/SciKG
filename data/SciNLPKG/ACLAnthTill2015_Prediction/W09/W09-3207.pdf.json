{"title": [{"text": "Measuring semantic relatedness with vector space models and random walks", "labels": [], "entities": []}], "abstractContent": [{"text": "Both vector space models and graph random walk models can be used to determine similarity between concepts.", "labels": [], "entities": []}, {"text": "Noting that vectors can be regarded as local views of a graph, we directly compare vector space models and graph random walk models on standard tasks of predicting human similarity ratings, concept categorization, and semantic priming, varying the size of the dataset from which vector space and graph are extracted.", "labels": [], "entities": [{"text": "predicting human similarity ratings", "start_pos": 153, "end_pos": 188, "type": "TASK", "confidence": 0.7874851673841476}]}], "introductionContent": [{"text": "Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks.", "labels": [], "entities": []}, {"text": "Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (.", "labels": [], "entities": []}, {"text": "We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs.", "labels": [], "entities": []}, {"text": "The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity.", "labels": [], "entities": []}, {"text": "In particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second-order and even higher-order vectors.", "labels": [], "entities": []}, {"text": "This view leads to the second focal point of this paper: We investigate whether random walk models can simulate the smoothing effects obtained by methods like Singular Value Decomposition (SVD).", "labels": [], "entities": []}, {"text": "To answer this question, we compute models on reduced (downsampled) versions of our dataset and evaluate the robustness of random walk models, a classic vector-based model, and SVD-based models against data sparseness.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we report the results for all tasks obtained on the full data-set and then proceed with the comparison of different models on differing graph sizes to seethe robustness of the models against data sparseness.", "labels": [], "entities": []}, {"text": "Human similarity ratings: We use the dataset of, consisting of averages of subject similarity ratings for 65 noun pairs.", "labels": [], "entities": []}, {"text": "We use the Pearson's coefficient between estimates and human judgments as our performance measure.", "labels": [], "entities": [{"text": "Pearson's coefficient", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.9487641255060831}]}, {"text": "The results obtained for  the full graph are in, line 1.", "labels": [], "entities": []}, {"text": "The SVD model clearly outperforms the pure-vector based approach and the graph-based approaches.", "labels": [], "entities": []}, {"text": "Its performance is above that of previous models trained on the same corpus ().", "labels": [], "entities": []}, {"text": "The best model that we report is based on web search engine results).", "labels": [], "entities": []}, {"text": "Among the graph-based random walk models, flexible walk with parameter 0.5 and fixed 1-step walk with indirect relatedness measures using dot product similarity achieve the highest performance.", "labels": [], "entities": []}, {"text": "Concept categorization: Almuhareb (2006) proposed a set of 402 nouns to be categorized into 21 classes of both concrete (animals, fruit.", "labels": [], "entities": []}, {"text": ") and abstract (feelings, times.", "labels": [], "entities": []}, {"text": "Our results on this clustering task are given in.", "labels": [], "entities": [{"text": "clustering task", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.8944937288761139}]}, {"text": "The difference between SVD and pure-vector models is negligible and they both obtain the best performance in terms of both cluster entropy (not shown in the table) and purity.", "labels": [], "entities": []}, {"text": "Both models' performances are comparable with the previously reported studies, and above that of random walks.", "labels": [], "entities": []}, {"text": "Semantic priming: The next dataset comes from and it is of interest since it requires capturing different forms of semantic relatedness between prime-target pairs: synonyms (synonym), coordinates (coord), antonyms (antonym), free association pairs (conass), superand subordinate pairs (supersub) and phrasal associates (phrasacc).", "labels": [], "entities": []}, {"text": "Following previous simulations of this data-set (Pad\u00f3 and Lapata, 2007), we measure the similarity of each related target-prime pair, and we compare it to the average similarity of the target to all the other primes instantiating the same relation, treating the latter quantity as our surrogate of an unrelated target-prime pair.", "labels": [], "entities": []}, {"text": "We report results in terms of differences between unrelated and related pairs, normalized to t-scores, marking significance according to twotailed paired t-tests for the relevant degrees of freedom.", "labels": [], "entities": []}, {"text": "Even though the SVD-based and pure-vector models are among the top achievers in general, we see that in different tasks different random walk models achieve comparable or even better performances.", "labels": [], "entities": []}, {"text": "In particular, for phrasal associates and conceptual associates, the best results are obtained by random walks based on direct measures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: All datasets. * (**) indicates significance level p < 0.01 (p < 0.001). BL: (Baroni and Lenci,  2009), CLW: (Chen et al., 2006), AP: (Almuhareb, 2006), RS: (Rothenh\u00e4usler and Sch\u00fctze, 2009)", "labels": [], "entities": [{"text": "significance level p", "start_pos": 41, "end_pos": 61, "type": "METRIC", "confidence": 0.9329289197921753}, {"text": "BL", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9969096779823303}, {"text": "AP", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9960674047470093}, {"text": "RS", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9764459133148193}]}, {"text": " Table 2: Each cell contains the ratio of the performance of the corresponding model for the corresponding  downsampling ratio to the performance of the same model on the full graph. The higher ratio means the  less deterioration due to data sparseness.", "labels": [], "entities": []}]}