{"title": [{"text": "A Deep Learning Approach to Machine Transliteration", "labels": [], "entities": [{"text": "Machine Transliteration", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6896215975284576}]}], "abstractContent": [{"text": "In this paper we present a novel translit-eration technique which is based on deep belief networks.", "labels": [], "entities": []}, {"text": "Common approaches use finite state machines or other methods similar to conventional machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7905199825763702}]}, {"text": "Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems.", "labels": [], "entities": []}, {"text": "We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task.", "labels": [], "entities": [{"text": "translation", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.9750571846961975}]}], "introductionContent": [{"text": "Transliteration, i.e. the transcription of words such as proper nouns from one language into another or, more commonly from one alphabet into another, is an important subtask of machine translation (MT) in order to obtain high quality output.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.8343937277793885}]}, {"text": "We present anew technique for transliteration which is based on deep belief networks (DBNs), a well studied approach in machine learning.", "labels": [], "entities": []}, {"text": "Transliteration can in principle be considered to be a small-scale translation problem and, thus, some ideas presented here can be transferred to the machine translation domain as well.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7074595838785172}]}, {"text": "Transliteration has been in use in machine translation systems, e.g. Russian-English, since the existence of the field of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7251336723566055}, {"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7572464048862457}]}, {"text": "However, to our knowledge it was first studied as a machine learning problem by using probabilistic finite-state transducers.", "labels": [], "entities": []}, {"text": "Subsequently, the performance of this system was greatly improved by combining different spelling and phonetic models).", "labels": [], "entities": []}, {"text": "construct a probabilistic Chinese-English edit model as part of a larger alignment solution using a heuristic bootstrapped procedure.", "labels": [], "entities": []}, {"text": "propose a technique which combines conventional MT methods with a single layer perceptron.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9881832003593445}]}, {"text": "In contrast to these methods which strongly build on top of well-established natural language processing (NLP) techniques, we propose an alternative model.", "labels": [], "entities": []}, {"text": "Our new model is based on deep belief networks which have been shown to work well in other machine learning and pattern recognition areas (cf. Section 2).", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.6701004058122635}]}, {"text": "Since translation and transliteration are closely related and transliteration can be considered a translation problem on the character level, we discuss various methods from both domains which are related to the proposed approach in the following.", "labels": [], "entities": []}, {"text": "Neural networks have been used in NLP in the past, e.g. for machine translation) and constituent parsing.", "labels": [], "entities": [{"text": "machine translation)", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8485130071640015}, {"text": "constituent parsing", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7366635501384735}]}, {"text": "However, it might not be straight-forward to obtain good results using neural networks in this domain.", "labels": [], "entities": []}, {"text": "In general, when training a neural network, one has to choose the structure of the neural network which involves certain trade-offs.", "labels": [], "entities": []}, {"text": "If a small network with no hidden layer is chosen, it can be efficiently trained but has very limited representational power, and maybe unable to learn the relationships between the source and the target language.", "labels": [], "entities": []}, {"text": "The DBN approach alleviates some of the problems that commonly occur when working with neural networks: 1.", "labels": [], "entities": []}, {"text": "they allow for efficient training due to a good initialisation of the individual layers.", "labels": [], "entities": []}, {"text": "2. Overfitting problems are addressed by creating generative models which are later refined discriminatively.", "labels": [], "entities": []}, {"text": "3. The network structure is clearly defined and only a few structure parameters have to beset.", "labels": [], "entities": []}, {"text": "4. DBNs can be interpreted as Bayesian probabilistic generative models.", "labels": [], "entities": []}, {"text": "Recently, proposed a technique which applies a convolutional DBN to a multi-task learning NLP problem.", "labels": [], "entities": []}, {"text": "Their approach is able to address POS tagging, chunking, named entity tagging, semantic role and similar word identification in one model.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8606154322624207}, {"text": "named entity tagging", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.688467542330424}, {"text": "word identification", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7531853318214417}]}, {"text": "Our model is similar to this approach in that it uses the same machine learning techniques but the encoding and the processing is done differently.", "labels": [], "entities": []}, {"text": "First, we learn two independent generative models, one for the source input and one for the target output.", "labels": [], "entities": []}, {"text": "Then, these two models are combined into a source-to-target encoding/decoding system (cf. Section 2).", "labels": [], "entities": []}, {"text": "Regarding that the target is generated and not searched in a space of hypotheses (e.g. in a word graph), our approach is similar to the approach presented by who present an MT system where the set of words of the target sentence is generated based on the full source sentence and then a finite-state approach is used to reorder the words.", "labels": [], "entities": [{"text": "MT", "start_pos": 173, "end_pos": 175, "type": "TASK", "confidence": 0.9680018424987793}]}, {"text": "Opposed to this approach we do not only generate the letters/words in the target sentence but we generate the full sentence with ordering.", "labels": [], "entities": []}, {"text": "We evaluate the proposed methods on an Arabic-English transliteration task where Arabic city names have to be transcribed into the equivalent English spelling.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present experimental results for an Arabic-English transliteration task.", "labels": [], "entities": []}, {"text": "For evaluation we use the character error rate (CER) which is the commonly used word error rate (WER) on character level.", "labels": [], "entities": [{"text": "character error rate (CER)", "start_pos": 26, "end_pos": 52, "type": "METRIC", "confidence": 0.8954181174437205}, {"text": "word error rate (WER)", "start_pos": 80, "end_pos": 101, "type": "METRIC", "confidence": 0.9028419057528178}]}, {"text": "We use a corpus of 10,084 personal names in Arabic and their transliterated English ASCII representation (LDC corpus LDC2005G02).", "labels": [], "entities": [{"text": "LDC corpus LDC2005G02)", "start_pos": 106, "end_pos": 128, "type": "DATASET", "confidence": 0.786737710237503}]}, {"text": "The Arabic names are written in the usual way, i.e. lacking vowels and diacritics.", "labels": [], "entities": []}, {"text": "1,000 names were randomly sampled for development and evaluation, respectively.", "labels": [], "entities": []}, {"text": "The vocabulary of the source language is 33 and the target language has 30 different characters (including the padding character).", "labels": [], "entities": []}, {"text": "The longest word on both sides consists of 14 characters, thus the feature vector on the source side is 462-dimensional and the feature vector on the target side is 420-dimensional.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Transliteration experiments using differ- ent network structures.  number of nodes  CER [%]", "labels": [], "entities": [{"text": "CER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9376532435417175}]}, {"text": " Table 2: Results for the Arabic-English translit- eration task depending on the network size and a  comparison with state of the art results using con- ventional phrase-based machine translation tech- niques  network  CER [%]  size train dev eval", "labels": [], "entities": [{"text": "translit- eration task", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.722807914018631}, {"text": "con- ventional phrase-based machine translation tech- niques  network  CER", "start_pos": 148, "end_pos": 222, "type": "TASK", "confidence": 0.645030059597709}]}, {"text": " Table 3: Oracle character error rates on 10-best  lists.  sampling layer oracle CER [%]  dev  eval", "labels": [], "entities": [{"text": "CER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9254437685012817}]}, {"text": " Table 4: Results from the rescoring experiments  and fusion with the phrase-based MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.8512402176856995}]}, {"text": " Table 5: Results from the individual methods in- vestigated versus ROVER combination.", "labels": [], "entities": [{"text": "ROVER", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9604920148849487}]}]}