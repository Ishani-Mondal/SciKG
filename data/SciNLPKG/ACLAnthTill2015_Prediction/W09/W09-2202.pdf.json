{"title": [{"text": "Surrogate Learning - From Feature Independence to Semi-Supervised Classification", "labels": [], "entities": [{"text": "Surrogate Learning", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9187575280666351}, {"text": "Feature Independence", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.6712673753499985}, {"text": "Classification", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7178710699081421}]}], "abstractContent": [{"text": "We consider the task of learning a classi-fier from the feature space X to the set of classes Y = {0, 1}, when the features can be partitioned into class-conditionally independent feature sets X 1 and X 2.", "labels": [], "entities": []}, {"text": "We show that the class-conditional independence can be used to represent the original learning task in terms of 1) learning a classifier from X 2 to X 1 (in the sense of estimating the probability P (x 1 |x 2))and 2) learning the class-conditional distribution of the feature set X 1.", "labels": [], "entities": []}, {"text": "This fact can be exploited for semi-supervised learning because the former task can be accomplished purely from unlabeled samples.", "labels": [], "entities": []}, {"text": "We present experimental evaluation of the idea in two real world applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semi-supervised learning is said to occur when the learner exploits (a presumably large quantity of) unlabeled data to supplement a relatively small labeled sample, for accurate induction.", "labels": [], "entities": []}, {"text": "The high cost of labeled data and the simultaneous plenitude of unlabeled data in many application domains, has led to considerable interest in semi-supervised learning in recent years ().", "labels": [], "entities": []}, {"text": "We show a somewhat surprising consequence of class-conditional feature independence that leads to a principled and easily implementable semisupervised learning algorithm.", "labels": [], "entities": []}, {"text": "When the feature set can be partitioned into two class-conditionally independent sets, we show that the original learning problem can be reformulated in terms of the problem of learning a first predictor from one of the partitions to the other, plus a second predictor from the latter partition to class label.", "labels": [], "entities": []}, {"text": "That is, the latter partition acts as a surrogate for the class variable.", "labels": [], "entities": []}, {"text": "Assuming that the second predictor can be learned from a relatively small labeled sample this results in an effective semi-supervised algorithm, since the first predictor can be learned from only unlabeled samples.", "labels": [], "entities": []}, {"text": "In the next section we present the simple yet interesting result on which our semi-supervised learning algorithm (which we call surrogate learning) is based.", "labels": [], "entities": []}, {"text": "We present examples to clarify the intuition behind the approach and present a special case of our approach that is used in the applications section.", "labels": [], "entities": []}, {"text": "We then examine related ideas in previous work and situate our algorithm among previous approaches to semi-supervised learning.", "labels": [], "entities": []}, {"text": "We present empirical evaluation on two real world applications where the required assumptions of our algorithm are satisfied.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision and Recall for record linkage.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9977768063545227}, {"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9957000017166138}]}, {"text": " Table 2: Patterns used as seeds and the number of source  sentences matching each seed.", "labels": [], "entities": []}, {"text": " Table 3: Precision/Recall of surrogate learning on the  MA paraphrase problem for various thresholds. The  baseline of using all the targets as paraphrases for MA  has a precision of 66% and a recall of 100%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9875390529632568}, {"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9053385257720947}, {"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.9983760118484497}, {"text": "recall", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.999670147895813}]}, {"text": " Table 4: Number of sentences found by surrogate learn- ing matching each of the remaining seed patterns, when  only one of the patterns was used as a seed. Each column  is for one experiment with the corresponding pattern used  as the seed. For example, when only the first pattern was  used as the seed, we obtained 18 sentences that match the  fourth pattern.", "labels": [], "entities": []}]}