{"title": [], "abstractContent": [{"text": "This paper discusses the performance difference of wide-coverage parsers on small-domain speech transcripts.", "labels": [], "entities": []}, {"text": "Two parsers (C&C CCG and RASP) are tested on the speech transcripts of two different domains (parent-child language, and picture descriptions).", "labels": [], "entities": [{"text": "RASP", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.6822602152824402}]}, {"text": "The performance difference between the domain-independent parsers and two domain-trained parsers (MSTParser and MEGRASP) is substantial, with a difference of at least 30 percent point inaccuracy.", "labels": [], "entities": []}, {"text": "Despite this gap, some of the grammatical relations can still be recovered reliably.", "labels": [], "entities": []}], "introductionContent": [{"text": "Even though wide-coverage, domainindependent 1 parser systems may perform sufficiently well for the task at hand, obtaining highly accurate parses of sentences in a particular domain usually requires the parser to be domain-trained.", "labels": [], "entities": []}, {"text": "Training a parser requires a sufficient amount of labelled data (a gold standard), something that is only available for very few domains.", "labels": [], "entities": []}, {"text": "When accurate parses of sentences in anew domain are desired, there are several ways to proceed.", "labels": [], "entities": []}, {"text": "Hand labelling all data in the new domain is a consideration, but is usually unfeasible as manual annotation is a costly activity.", "labels": [], "entities": [{"text": "Hand labelling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7991533875465393}]}, {"text": "Another possibility is to minimise the amount of annotation effort required to achieve good performance by resorting to semi-automatic annotation or domain adaptation methods.", "labels": [], "entities": []}, {"text": "In any case, dedicated effort is still required to obtain highly accurate parses, even with recent automated domain adaptation methods (.", "labels": [], "entities": []}, {"text": "Work that requires parsing in anew domain as basis of further study or as part of a larger natural language processing system usually involves a domain-independent parser with the expectation that parses are sufficiently accurate for the specific purpose.", "labels": [], "entities": []}, {"text": "For instance, use a wide-coverage CCG-parser) to generate semantic representations for recognising textual entailment.", "labels": [], "entities": []}, {"text": "Geertzen (2009) uses a HPSG-based dependency parser () to obtain the semantic content of utterances.", "labels": [], "entities": [{"text": "HPSG-based dependency parser", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6872580448786417}]}, {"text": "And in the study of child language acquisition, use RASP, a wide-coverage dependency parser ( ), to look at lexical acquisition.", "labels": [], "entities": [{"text": "child language acquisition", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.6152296960353851}, {"text": "RASP", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.7608801126480103}]}, {"text": "The goal of this paper is to give an indication of wide-coverage, domain-independent parser performance on specific domains.", "labels": [], "entities": []}, {"text": "Additionally, the study gives insight into RASP's performance on CHILDES, allowing to factor in parsing performance in the syntax-based study of.", "labels": [], "entities": [{"text": "RASP", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.8993856906890869}]}], "datasetContent": [{"text": "System performance is reported with accuracy measures for labelled and unlabelled dependencies resulting from 15-fold cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9993112087249756}]}, {"text": "The performance on each grammatical relation is expressed by precision, recall, and F 1 -score.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9997217059135437}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9995841383934021}, {"text": "F 1 -score", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9869719743728638}]}], "tableCaptions": [{"text": " Table 1: Parsing accuracy scores.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8334786295890808}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.979955792427063}]}]}