{"title": [{"text": "Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric", "labels": [], "entities": [{"text": "HTER", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.7867261171340942}]}], "abstractContent": [{"text": "Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance.", "labels": [], "entities": [{"text": "Automatic Machine Translation (MT) evaluation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.795686445065907}, {"text": "MT output", "start_pos": 136, "end_pos": 145, "type": "TASK", "confidence": 0.8937110304832458}]}, {"text": "Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9869099259376526}, {"text": "Adequacy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9424266219139099}, {"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9937574863433838}, {"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9787634015083313}]}, {"text": "We explore these differences through the use of anew tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.9093510508537292}, {"text": "TER-Plus", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9610536694526672}]}, {"text": "TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spear-man correlation.", "labels": [], "entities": [{"text": "TER-Plus", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9075173139572144}, {"text": "NIST's Metrics MATR 2008 Challenge", "start_pos": 51, "end_pos": 85, "type": "DATASET", "confidence": 0.7563526829083761}, {"text": "Pearson and Spear-man correlation", "start_pos": 131, "end_pos": 164, "type": "METRIC", "confidence": 0.8374684005975723}]}, {"text": "Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the introduction of the BLEU metric), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9865553379058838}, {"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.7412029504776001}]}, {"text": "These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality.", "labels": [], "entities": [{"text": "MT output", "start_pos": 89, "end_pos": 98, "type": "TASK", "confidence": 0.9156256020069122}]}, {"text": "Numerous methods of judging MT output by humans have been used, including Fluency, Adequacy, and, more recently, Human-mediated Translation Edit Rate (HTER)).", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9301279187202454}, {"text": "Fluency", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9989056587219238}, {"text": "Adequacy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9905893802642822}, {"text": "Human-mediated Translation Edit Rate (HTER", "start_pos": 113, "end_pos": 155, "type": "METRIC", "confidence": 0.6696868638197581}]}, {"text": "Fluency measures whether a translation is fluent, regardless of the correct meaning, while Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9846006035804749}, {"text": "Adequacy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9924321174621582}]}, {"text": "Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9304913282394409}, {"text": "Adequacy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9348366260528564}]}, {"text": "HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate anew reference translation that is closer to the MT output but retains the fluency and meaning of the original reference.", "labels": [], "entities": [{"text": "HTER", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6431590914726257}]}, {"text": "This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER)) or when used with other automatic metrics such as BLEU or ME-TEOR ().", "labels": [], "entities": [{"text": "MT", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.9022660255432129}, {"text": "Translation Edit Rate (TER))", "start_pos": 103, "end_pos": 131, "type": "METRIC", "confidence": 0.8710141181945801}, {"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9968888163566589}]}, {"text": "One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent.", "labels": [], "entities": [{"text": "TER", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9980219602584839}]}, {"text": "In this way, only true errors in the MT output are counted.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9497131109237671}]}, {"text": "While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy, it is much more time consuming and taxing on human annotators than other types of human judgments, making it difficult and expensive to use.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9834144711494446}]}, {"text": "In addition, because HTER treats all edits equally, no distinction is made between serious errors (errors in names or missing subjects) and minor edits (such as a difference in verb agreement or a missing determinator).", "labels": [], "entities": []}, {"text": "Different types of translation errors vary in importance depending on the type of human judgment being used to evaluate the translation.", "labels": [], "entities": []}, {"text": "For example, errors intense might barely affect the adequacy of a translation but might cause the translation be scored as less fluent.", "labels": [], "entities": []}, {"text": "On the other hand, deletion of content words might not lower the fluency of a translation but the adequacy would suffer.", "labels": [], "entities": []}, {"text": "In this paper, we examine these differences by taking an automatic evaluation metric and tuning it to these these human judgments and examining the resulting differences in the parameterization of the metric.", "labels": [], "entities": []}, {"text": "To study this we introduce anew evaluation metric, TER-Plus (TERp) 1 that improves over the existing Translation Edit Rate (TER) metric), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.", "labels": [], "entities": [{"text": "TER-Plus (TERp) 1", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.9136799693107605}, {"text": "Translation Edit Rate (TER) metric", "start_pos": 101, "end_pos": 135, "type": "METRIC", "confidence": 0.7983303623540061}, {"text": "interpretation of the differences between human judgments", "start_pos": 265, "end_pos": 322, "type": "TASK", "confidence": 0.6476067560059684}]}, {"text": "Section 2 summarizes the TER metric and discusses how TERp improves on it.", "labels": [], "entities": [{"text": "TER metric", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9451236724853516}, {"text": "TERp", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9232187867164612}]}, {"text": "Correlation results with human judgments, including independent results from the 2008 NIST Metrics MATR evaluation, where TERp was consistently one of the top metrics, are presented in Section 3 to show the utility of TERp as an evaluation metric.", "labels": [], "entities": [{"text": "NIST Metrics MATR evaluation", "start_pos": 86, "end_pos": 114, "type": "DATASET", "confidence": 0.8694968223571777}, {"text": "TERp", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9828160405158997}]}, {"text": "The generation of paraphrases, as well as the effect of varying the source of paraphrases, is discussed in Section 4.", "labels": [], "entities": [{"text": "generation of paraphrases", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7710321942965189}]}, {"text": "Section 5 discusses the results of tuning TERp to Fluency, Adequacy and HTER, and how this affects the weights of various edit types.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9921054840087891}, {"text": "HTER", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.685445249080658}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Optimized TERp Edit Costs", "labels": [], "entities": [{"text": "TERp Edit Costs", "start_pos": 20, "end_pos": 35, "type": "METRIC", "confidence": 0.7739721536636353}]}, {"text": " Table 2: Optimization & Test Set Pearson Correlation Results", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.822917252779007}]}, {"text": " Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results", "labels": [], "entities": [{"text": "MT06 Dev", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.7720469832420349}]}, {"text": " Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results", "labels": [], "entities": [{"text": "Average Metric Rank", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9176546136538187}, {"text": "NIST Metrics MATR 2008 Official", "start_pos": 33, "end_pos": 64, "type": "DATASET", "confidence": 0.8846551537513733}]}, {"text": " Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.", "labels": [], "entities": [{"text": "NIST MATR 2008 test set", "start_pos": 25, "end_pos": 48, "type": "DATASET", "confidence": 0.8948514938354493}, {"text": "paraphrase usage", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.8663890361785889}]}, {"text": " Table 6: Optimized Edit Costs", "labels": [], "entities": [{"text": "Optimized Edit Costs", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5460754235585531}]}]}