{"title": [{"text": "Complex Linguistic Annotation -No Easy Way Out! A Case from Bangla and Hindi POS Labeling Tasks", "labels": [], "entities": [{"text": "POS Labeling Tasks", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.6972300906976064}]}], "abstractContent": [{"text": "Alternative paths to linguistic annotation, such as those utilizing games or exploiting the web users, are becoming popular in recent times owing to their very high benefit-to-cost ratios.", "labels": [], "entities": [{"text": "linguistic annotation", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7230682969093323}]}, {"text": "In this paper, however, we report a case study on POS annotation for Bangla and Hindi, where we observe that reliable linguistic annotation requires not only expert anno-tators, but also a great deal of supervision.", "labels": [], "entities": [{"text": "POS annotation", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.8498651683330536}]}, {"text": "For our hierarchical POS annotation scheme, we find that close supervision and training is necessary at every level of the hierarchy, or equivalently, complexity of the tagset.", "labels": [], "entities": []}, {"text": "Nevertheless , an intelligent annotation tool can significantly accelerate the annotation process and increase the inter-annotator agreement for both expert and non-expert annotators.", "labels": [], "entities": []}, {"text": "These findings lead us to believe that reliable annotation requiring deep linguistic knowledge (e.g., POS, chunking, Treebank, semantic role labeling) requires expertise and supervision.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.6273554762204488}]}, {"text": "The focus, therefore, should be on design and development of appropriate annotation tools equipped with machine learning based predictive modules that can significantly boost the productivity of the annota-tors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Access to reliable annotated data is the first hurdle encountered inmost NLP tasks be it at the level of Parts-of-Speech (POS) tagging or a more complex discourse level annotation.", "labels": [], "entities": [{"text": "Parts-of-Speech (POS) tagging", "start_pos": 105, "end_pos": 134, "type": "TASK", "confidence": 0.6078707993030548}]}, {"text": "The performance of the machine learning approaches which have become de rigueur for most NLP tasks are dependent on accurately annotated large datasets.", "labels": [], "entities": []}, {"text": "Creation of such databases is, hence, a highly resource intensive task both in terms of time and expertise.", "labels": [], "entities": []}, {"text": "While the cost of an annotation task can be characterized by the number of man-hours and the level of expertise required, the productivity or the benefit can be measured in terms of the reliability and usability of the end-product, i.e., the annotated dataset.", "labels": [], "entities": []}, {"text": "It is thus no surprise that considerable effort has gone into developing techniques and tools that can effectively boost the benefit-to-cost ratio of the annotation process.", "labels": [], "entities": []}, {"text": "Methods exploiting the web-users for linguistic annotation are particularly popular these days, presumably because of the success of the ESPGame (von) and its successors in image annotation.", "labels": [], "entities": [{"text": "linguistic annotation", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7182727307081223}, {"text": "image annotation", "start_pos": 173, "end_pos": 189, "type": "TASK", "confidence": 0.7677885890007019}]}, {"text": "A more recent study by shows that annotated data obtained from non-expert anonymous web-users is as good as those obtained from experts.", "labels": [], "entities": []}, {"text": "However, unlike the game model, here the task is distributed among non-experts through an Internet portal such as Amazon Mechanical Turk, and the users are paid for their annotations.", "labels": [], "entities": []}, {"text": "This might lead to an impression that the expert knowledge is dispensable for NLP annotation tasks.", "labels": [], "entities": [{"text": "NLP annotation tasks", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7656181454658508}]}, {"text": "However, while these approaches may work for more simple tasks like those described in (, most NLP related annotation tasks such as POS tagging, chunking, semantic role labeling, Treebank annotation and discourse level tagging, require expertise in the relevant linguistic area.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 132, "end_pos": 143, "type": "TASK", "confidence": 0.7480399012565613}, {"text": "semantic role labeling", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.5964582761128744}, {"text": "discourse level tagging", "start_pos": 203, "end_pos": 226, "type": "TASK", "confidence": 0.6414946019649506}]}, {"text": "In this work, we present a case study of POS annotation in Bangla and Hindi using a hierarchical tagset, where we observe that reliable linguistic annotation requires not only expert annotators, but also a great deal of supervision.", "labels": [], "entities": [{"text": "POS annotation", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.9063019156455994}]}, {"text": "A generic user interface for facilitating the task of hierarchical word level linguistic annotation was designed and experiments conducted to measure the inter-annotator agreement (IA) and annotation time.", "labels": [], "entities": [{"text": "hierarchical word level linguistic annotation", "start_pos": 54, "end_pos": 99, "type": "TASK", "confidence": 0.5802022337913513}, {"text": "inter-annotator agreement (IA)", "start_pos": 154, "end_pos": 184, "type": "METRIC", "confidence": 0.8206994295120239}]}, {"text": "It is observed that the tool can significantly accelerate the annotation process and increase the IA.", "labels": [], "entities": [{"text": "IA", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9207349419593811}]}, {"text": "The productivity of the annotation process is further enhanced through bootstrapping, whereby a little amount of manually annotated data is used to train an automatic POS tagger.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 167, "end_pos": 177, "type": "TASK", "confidence": 0.6700028330087662}]}, {"text": "The annotators are then asked to edit the data already tagged by the automatic tagger using an appropriate user interface.", "labels": [], "entities": []}, {"text": "However, the most significant observation to emerge from these experiments is that irrespective of the complexity of the annotation task (see Sec.", "labels": [], "entities": []}, {"text": "2 for definition), language, design of the user interface and the accuracy of the automatic POS tagger used during bootstrapping, the productivity and reliability of the expert annotators working under close supervision of the dataset designer is higher than that of non-experts or those working without expert-supervision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9995642304420471}]}, {"text": "This leads us to believe that among the four aforementioned approaches for improving the benefit-tocost ratio of the annotation tasks, solution (a) does not seem to be the right choice for involved linguistic annotations; rather, approaches (b), (c) and (d) show more promise.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 provides a brief introduction to IL-POST -a hierarchical POS Tag framework for Indian Languages which is used for defining the specific annotation tasks used for the experiments.", "labels": [], "entities": []}, {"text": "The design and features of the data annotation tool are described in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiments conducted for POS labeling task of Bangla and Hindi while the results of these experiments are discussed in Section 5.", "labels": [], "entities": [{"text": "POS labeling task", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.9208451509475708}, {"text": "Bangla and Hindi", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.860946794350942}]}, {"text": "The conclusions are presented in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The objective of the current work is to study the cognitive load associated with the task of linguistic annotation, more specifically, POS annotation.", "labels": [], "entities": [{"text": "POS annotation", "start_pos": 135, "end_pos": 149, "type": "TASK", "confidence": 0.7804244756698608}]}, {"text": "Cognitive load relates to the higher level of processing required by the working memory of an annotator when more learning is to be done in a shorter time.", "labels": [], "entities": []}, {"text": "Hence, a higher cognitive load implies more time required for annotation and higher error rates.", "labels": [], "entities": [{"text": "error rates", "start_pos": 84, "end_pos": 95, "type": "METRIC", "confidence": 0.9663791954517365}]}, {"text": "The time required for annotation can be readily measured by keeping track of the time taken by the annotators while tagging a sentence.", "labels": [], "entities": []}, {"text": "The timer facility provided with the annotation tool helps us keep track of the annotation time.", "labels": [], "entities": [{"text": "timer", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9634649753570557}]}, {"text": "Measuring the error rate is slightly trickier as we do not have any ground truth (gold standard) against which we can measure the accuracy of the manual annotators.", "labels": [], "entities": [{"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9711278975009918}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9988453388214111}]}, {"text": "Therefore, we measure the IA, which should be high if the error rate is low.", "labels": [], "entities": [{"text": "IA", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.996845006942749}, {"text": "error rate", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9651804864406586}]}, {"text": "Details of the evaluation metrics are discussed in the next section.", "labels": [], "entities": []}, {"text": "The cognitive load of the annotation task is dependent on the complexity of the tagset, (un)availability of an appropriate annotation tool and bootstrapping facility.", "labels": [], "entities": []}, {"text": "Therefore, in order to quantify the effect of these factors on the annotation task, annotation experiments are conducted under eight different settings.", "labels": [], "entities": []}, {"text": "Four experiments are done for annotation at the Category+Type (CT) level.", "labels": [], "entities": []}, {"text": "These are: \u2022 CT-AT: without using annotation tool, i.e., using any standard text editor 2 . \u2022 CT+AT: with the help of the basic annotation tool.", "labels": [], "entities": [{"text": "AT", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.46548202633857727}]}, {"text": "\u2022 CT+ATL: with the help of the annotation tool in the edit mode, where the POS tagger used has low accuracy.", "labels": [], "entities": [{"text": "CT+ATL", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.7738679846127828}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9966925382614136}]}, {"text": "\u2022 CT+ATH: in the edit mode where the POS tagger used has a high accuracy.", "labels": [], "entities": [{"text": "CT+", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.930474191904068}, {"text": "ATH", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.5429967045783997}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9988518953323364}]}, {"text": "Similarly, four experiments are conducted at the Category+Type+Attribute (CTA) level, which are named following the same convention: CTA-AT, CTA+AT, CTA+ATL, CTA+ATH.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagging accuracy in % for Bangla and  Hindi", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9717016220092773}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9812055826187134}]}, {"text": " Table 2: Mean annotation time for Bangla ex- periments (%reduction in time with respect to - AT is given within parentheses).", "labels": [], "entities": [{"text": "Mean annotation time", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8230253259340922}, {"text": "AT", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.8986066579818726}]}, {"text": " Table 3: Average IA for Bangla experiments  (%increase in IA with respect to -AT is given  within parentheses).", "labels": [], "entities": [{"text": "IA", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.7334113717079163}, {"text": "IA", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.9983530044555664}, {"text": "AT", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9040573239326477}]}, {"text": " Table 4: Percentage agreement between the edit  and the normal mode annotations (for Bangla).", "labels": [], "entities": []}]}