{"title": [{"text": "Combining Syntactic Co-occurrences and Nearest Neighbours in Distributional Methods to Remedy Data Sparseness", "labels": [], "entities": []}], "abstractContent": [{"text": "The task of automatically acquiring semantically related words have led people to study distributional similarity.", "labels": [], "entities": []}, {"text": "The distributional hypothesis states that words that are similar share similar contexts.", "labels": [], "entities": []}, {"text": "In this paper we present a technique that aims at improving the performance of a syntax-based distribu-tional method by augmenting the original input of the system (syntactic co-occurrences) with the output of the system (nearest neighbours).", "labels": [], "entities": []}, {"text": "This technique is based on the idea of the transitivity of similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "The approach described in this paper builds on the DISTRIBUTIONAL HYPOTHESIS, the idea that semantically related words are distributed similarly over contexts.", "labels": [], "entities": []}, {"text": "claims that, 'the meaning of entities and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.'", "labels": [], "entities": []}, {"text": "In other words, you can grasp the meaning of a word by looking at its context.", "labels": [], "entities": []}, {"text": "Context can be defined in many ways.", "labels": [], "entities": []}, {"text": "In this paper we look at the syntactic contexts a word is found in.", "labels": [], "entities": []}, {"text": "For example, the verbs that are in a object relation with a particular noun form apart of its context.", "labels": [], "entities": []}, {"text": "In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words.", "labels": [], "entities": [{"text": "Firthian tradition", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.9658685028553009}]}, {"text": "For instance, words that occur in a object relation with the verb drink have something in common: they are liquid.", "labels": [], "entities": []}, {"text": "We will refer to words linked by a syntactic relation, such as drink -OBJbeer, as SYNTACTIC CO-OCCURRENCES.", "labels": [], "entities": []}, {"text": "Syntactic co-occurrences have often been used in work on lexical acquisition).", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7237181663513184}]}, {"text": "Distributional methods for automatic acquisition of semantically related words suffer from data sparseness.", "labels": [], "entities": [{"text": "automatic acquisition of semantically related words", "start_pos": 27, "end_pos": 78, "type": "TASK", "confidence": 0.8064509183168411}]}, {"text": "They generally perform less well on low-frequency words.", "labels": [], "entities": []}, {"text": "This is a pity because the available resources for semantically related words usually cover the frequent words rather well.", "labels": [], "entities": []}, {"text": "It is for the low-frequency words that automatic methods would be most welcome.", "labels": [], "entities": []}, {"text": "This paper tries to find away to improve the performance on the words that are most wanted: the middle to very-low-frequency words.", "labels": [], "entities": []}, {"text": "At the basis of the proposed technique lies the intuition that semantic similarity between concepts is transitive: if A is like B and B is like C \u2192 A is like C.", "labels": [], "entities": []}, {"text": "As explained in the second paragraph of this section, the fact that both milk and water are found in object relation with the verb to drink tells us that they might be similar.", "labels": [], "entities": []}, {"text": "However, even if we had never seen lemonade in the same syntactic contexts as water, we could still infer that lemonade and water are similar because we have found evidence that both water and lemonade are similar to milk.", "labels": [], "entities": []}, {"text": "In an ideal world we would be able to infer that milk and water are related from the syntactic cooccurrences alone, however, because of data sparseness we might not always encounter this evidence directly.", "labels": [], "entities": []}, {"text": "We hope that nearest neighbours are able to account for the missing information.", "labels": [], "entities": []}, {"text": "Nearest neighbours such as milk and water, and water and lemonade are the output of our system.", "labels": [], "entities": []}, {"text": "We used the nearest neighbours (the output of our system) as input to our system that normally takes syntactic co-45 occurrences as input.", "labels": [], "entities": []}, {"text": "Thus it uses the output of the system as input in a second round to smooth the syntactic co-occurrences.", "labels": [], "entities": []}, {"text": "discusses the difference between FIRST-AND SECOND-ORDER AFFINITIES.", "labels": [], "entities": [{"text": "FIRST-AND SECOND-ORDER AFFINITIES", "start_pos": 33, "end_pos": 66, "type": "METRIC", "confidence": 0.6598548889160156}]}, {"text": "There exists a first-order affinity between words if they often appear in the same context, i.e., if they are often found in the vicinity of each other.", "labels": [], "entities": []}, {"text": "Words that co-occur frequently such as orange and squeezed have a first-order affinity.", "labels": [], "entities": []}, {"text": "There exists a secondorder affinity between words if they share many firstorder affinities.", "labels": [], "entities": []}, {"text": "These words need not appear together themselves, but their contexts are similar.", "labels": [], "entities": []}, {"text": "Orange and lemon appear often in similar contexts such as being the object of squeezed, or being modified by juicy.", "labels": [], "entities": []}, {"text": "In this paper we will use second-order affinities as input to the distributional system.", "labels": [], "entities": []}, {"text": "We are thus computing THIRD-ORDER AFFINITIES.", "labels": [], "entities": [{"text": "THIRD-ORDER", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9218475222587585}, {"text": "AFFINITIES", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.5407130122184753}]}, {"text": "There exists a third-order affinity between words, if they share many second-order affinities.", "labels": [], "entities": []}, {"text": "If pear and watermelon are similar and orange and watermelon are similar, then pear and orange have a third-order affinity.", "labels": [], "entities": []}, {"text": "We will refer to traditional approaches that compute second-order affinities as second-order techniques.", "labels": [], "entities": []}, {"text": "In this paper we will compare a secondorder technique with a third-order technique, a technique that computes third-order affinities.", "labels": [], "entities": []}, {"text": "In addition we use a combined technique that combines both second-order and third-order techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following subsections we will first explain how we determined the semantic similarity of the retrieved nearest neighbours (subsection 5.1) and then we will describe the test sets used (subsection 5.2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Syntactic co-occurrence vector for kat", "labels": [], "entities": []}, {"text": " Table 2: EWN similarity several values of k for the four  test sets", "labels": [], "entities": [{"text": "EWN", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6095145344734192}]}, {"text": " Table 3: Number of synonyms at several values of k for  the four test sets", "labels": [], "entities": []}]}