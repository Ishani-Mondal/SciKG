{"title": [{"text": "Learning Stochastic Bracketing Inversion Transduction Grammars with a Cubic Time Biparsing Algorithm", "labels": [], "entities": [{"text": "Stochastic Bracketing Inversion Transduction Grammars", "start_pos": 9, "end_pos": 62, "type": "TASK", "confidence": 0.609419584274292}]}], "abstractContent": [{"text": "We present a biparsing algorithm for Stochastic Bracketing Inversion Transduc-tion Grammars that runs in O(bn 3) time instead of O(n 6).", "labels": [], "entities": [{"text": "Stochastic Bracketing Inversion Transduc-tion Grammars", "start_pos": 37, "end_pos": 91, "type": "TASK", "confidence": 0.6884719491004944}]}, {"text": "Transduction grammars learned via an EM estimation procedure based on this biparsing algorithm are evaluated directly on the translation task, by building a phrase-based statistical MT system on top of the alignments dictated by Viterbi parses under the induced bi-grammars.", "labels": [], "entities": []}, {"text": "Translation quality at different levels of pruning are compared, showing improvements over a conventional word aligner even at heavy pruning levels.", "labels": [], "entities": []}], "introductionContent": [{"text": "As demonstrated by there is something to be gained by applying structural models such as Inversion Transduction Grammars (ITG) to the problem of word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.7709982097148895}]}, {"text": "One issue is that na\u00a8\u0131vena\u00a8\u0131ve methods for inducing ITGs from parallel data can be very time consuming.", "labels": [], "entities": []}, {"text": "We introduce a parsing algorithm for inducing Stochastic Bracketing ITGs from parallel data in O(bn 3 ) time instead of O(n 6 ), where b is a pruning parameter (lower = tighter pruning).", "labels": [], "entities": []}, {"text": "We tryout different values for b, and evaluate the results on a translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8844464719295502}]}, {"text": "In section 2 we summarize the ITG framework; in section 3 we present our algorithm, whose time complexity is analyzed in section 4.", "labels": [], "entities": []}, {"text": "In section 5 we describe how the algorithm is evaluated, and in section 6, the empirical results are given.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the parser on a translation task (WMT'08 shared task 3 ).", "labels": [], "entities": [{"text": "translation task", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.8991877734661102}, {"text": "WMT'08 shared task 3", "start_pos": 46, "end_pos": 66, "type": "DATASET", "confidence": 0.7669604420661926}]}, {"text": "In order to evaluate on a translation task, a translation system has to be built.", "labels": [], "entities": [{"text": "translation task", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.9190569519996643}]}, {"text": "We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++.", "labels": [], "entities": [{"text": "Viterbi parses of the training corpus", "start_pos": 31, "end_pos": 68, "type": "DATASET", "confidence": 0.7394826312859853}]}, {"text": "This is the same approach as taken in.", "labels": [], "entities": []}, {"text": "We will evaluate the resulting translations with two automatic 3 http://www.statmt.org/wmt08/ metrics: BLEU () and NIST).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9968704581260681}, {"text": "NIST", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9094993472099304}]}], "tableCaptions": [{"text": " Table 1: Results. Time measures are approximate time per iteration.", "labels": [], "entities": [{"text": "Time", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9710088968276978}]}]}