{"title": [{"text": "A Handsome Set of Metrics to Measure Utterance Classification Performance in Spoken Dialog Systems", "labels": [], "entities": [{"text": "Measure Utterance Classification", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.6733454863230387}]}], "abstractContent": [{"text": "We present a set of metrics describing classification performance for individual contexts of a spoken dialog system as well as for the entire system.", "labels": [], "entities": []}, {"text": "We show how these metrics can be used to train and tune system components and how they are related to Caller Experience, a subjective measure describing how well a caller was treated by the dialog system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most of the speech recognition contexts in commercial spoken dialog systems aim at mapping the caller input to one out of a set of context-specific semantic classes ().", "labels": [], "entities": [{"text": "speech recognition contexts", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7862877547740936}]}, {"text": "This is done by providing a grammar to the speech recognizer at a given recognition context.", "labels": [], "entities": []}, {"text": "A grammar serves two purposes: \u2022 It constraints the lexical content the recognizer is able to recognize in this context (the language model) and \u2022 It assigns one out of a set of possible classes to the recognition hypothesis (the classifier).", "labels": [], "entities": []}, {"text": "This basic concept is independent of the nature of a grammar: it can be a rule-based one, manually or automatically generated; it can comprise a statistical language model and a classifier; it can consist of sets of grammars, language models, or classifiers; or it can be a holistic grammar, i.e., a statistical model combining a language model and a classification model in one large search tree.", "labels": [], "entities": []}, {"text": "Most commercial dialog systems utilize grammars that return a semantic parse in one of these contexts: \u2022 directed dialogs (e.g., yes/no contexts, menus with several choices, collection of information out of a restricted set [Which type of modem do you have?]-usually, less than 50 classes) \u2022 open-ended prompts (e.g. for call routing, problem capture; likewise to collect information out of a restricted set [Tell me what * Patent pending.", "labels": [], "entities": [{"text": "call routing", "start_pos": 321, "end_pos": 333, "type": "TASK", "confidence": 0.8099178373813629}, {"text": "problem capture", "start_pos": 335, "end_pos": 350, "type": "TASK", "confidence": 0.7793677449226379}]}, {"text": "you are calling about today]-possibly several hundred classes () \u2022 information collection out of a huge (or infinite) set of classes (e.g., collection of phone numbers, dates, names, etc.)", "labels": [], "entities": []}, {"text": "When the performance of spoken dialog systems is to be measured, there is a multitude of objective metrics to do so, many of which feature major disadvantages.", "labels": [], "entities": []}, {"text": "Examples include \u2022 Completion rate is calculated as the number of completed calls divided by the total number of calls.", "labels": [], "entities": [{"text": "Completion rate", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.9710787236690521}]}, {"text": "The main disadvantage of this metric is that it is influenced by many factors out of the system's control, such as caller hang-ups, opt-outs, or call reasons that fallout of the system's scope.", "labels": [], "entities": []}, {"text": "Furthermore, there are several system characteristics that impact this metric, such as recognition performance, dialog design, technical stability, availability of back-end integration, etc.", "labels": [], "entities": []}, {"text": "As experience shows, all of these factors can have unpredictable influence on the completion rate.", "labels": [], "entities": [{"text": "completion rate", "start_pos": 82, "end_pos": 97, "type": "METRIC", "confidence": 0.9521050155162811}]}, {"text": "On the one hand, a simple wording change in the introduction prompt of a system can make this rate improve significantly, whereas, on the other hand, major improvement of the open-ended speech recognition grammar following this very prompt may not have any impact.", "labels": [], "entities": [{"text": "open-ended speech recognition grammar", "start_pos": 175, "end_pos": 212, "type": "TASK", "confidence": 0.6926670745015144}]}, {"text": "\u2022 Average holding time is a common term for the average call duration.", "labels": [], "entities": [{"text": "Average holding time", "start_pos": 2, "end_pos": 22, "type": "METRIC", "confidence": 0.9328978061676025}]}, {"text": "This metric is often considered to be quite controversial since it is unclear whether longer calls are preferred or dispreferred.", "labels": [], "entities": []}, {"text": "Consider the following two incongruous behaviors resulting in longer call duration: -The system fails to appropriately treat callers, asking too many questions, performing redundant operations, acting unintelligently because of missing backend integration, or letting the caller wait in never-ending wait music loops.", "labels": [], "entities": []}, {"text": "-The system is so well-designed that it engages callers to interact with the system longer.", "labels": [], "entities": []}, {"text": "\u2022 Hang-up and opt-out rates.", "labels": [], "entities": []}, {"text": "These metrics try to encapsulate how many callers choose not to use the dialog system, either because they hangup or because they request to speak with a human operator.", "labels": [], "entities": []}, {"text": "However, it is unclear how such events are related to dialog system performance.", "labels": [], "entities": []}, {"text": "Certainly, many callers may have a prejudice against speaking with automated systems and may hangup or request a human regardless of how well-performing the dialog system is with cooperative users.", "labels": [], "entities": []}, {"text": "Furthermore, callers who hangup may do so because they are unable to get their problem solved or they may hangup precisely because their problem was solved (instead of waiting for the more felicitous post-problem-solving dialog modules).", "labels": [], "entities": []}, {"text": "\u2022 Retry rate is calculated as the average number of times that the system has to re-prompt for caller input because the caller's previous utterance was determined to be Out-ofGrammar.", "labels": [], "entities": [{"text": "Retry rate", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9506106972694397}]}, {"text": "The intuition behind this metric is that the lower the retry rate, the better the system.", "labels": [], "entities": [{"text": "retry rate", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.7631312310695648}]}, {"text": "However, this metric is problematic because it is tied to grammar performance itself.", "labels": [], "entities": []}, {"text": "Consider a well-performing grammar that correctly accepts In-Grammar utterances and rejects Out-of-Grammar utterances.", "labels": [], "entities": []}, {"text": "This grammar will cause the system to produce retries for all Out-of-Grammar utterances.", "labels": [], "entities": []}, {"text": "Consider a poorly designed grammar that accepts everything (incorrectly), even background noise.", "labels": [], "entities": []}, {"text": "This grammar would decrease the retry rate but would not be indicative of a well-performing dialog system.", "labels": [], "entities": [{"text": "retry", "start_pos": 32, "end_pos": 37, "type": "TASK", "confidence": 0.8233713507652283}]}, {"text": "As opposed to these objective measures, there is a subjective measure directly related to the system performance as perceived by the user: \u2022 Caller Experience.", "labels": [], "entities": []}, {"text": "This metric is used to describe how well the caller is treated by the system according to its design.", "labels": [], "entities": []}, {"text": "Caller Experience is measured on a scale between 1 (bad) and 5 (excellent).", "labels": [], "entities": []}, {"text": "This is the only subjective measure in this list and is usually estimated based on averaging scores given by multiple voice user interface experts which listen to multiple full calls.", "labels": [], "entities": []}, {"text": "Although this metric directly represents the ultimate design goal for spoken dialog systems-i.e., to achieve highest possible user experience-it is very expensive to be repeatedly produced and not suitable to be generated on-the-fly.", "labels": [], "entities": []}, {"text": "Our former research has suggested, however, that it maybe possible to automatically estimate Caller Experience based on several objective measures ().", "labels": [], "entities": []}, {"text": "These measures include the overall number of nomatches and substitutions in a call, operator requests, hang-ups, non-heard speech, the fact whether the call reason could be successfully captured and whether the call reason was finally satisfied.", "labels": [], "entities": []}, {"text": "Initial experiments showed a near-human accuracy of the automatic predictor trained on several hundred calls with available manual Caller Experience scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9995437264442444}]}, {"text": "The most powerful objective metric turned out to be the overall number of no-matches and substitutions, indicating a high correlation between the latter and Caller Experience.", "labels": [], "entities": []}, {"text": "No-matches and substitutions are objective metrics defined in the scope of semantic classification of caller utterances.", "labels": [], "entities": [{"text": "semantic classification of caller utterances", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.7479148387908936}]}, {"text": "They are part of a larger set of semantic classification metrics which we systematically demonstrate in Section 2.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7819058001041412}]}, {"text": "The remainder of the paper examines three case studies exploring the usefulness and interplay of different evaluation metrics, including: \u2022 the correlation between True Total (one of the introduced metrics) and Caller Experience in Section 3, \u2022 the estimation of speech recognition and classification parameters based on True Total and True Confirm Total (another metric) in Section 4, and \u2022 the tuning of large-scale spoken dialog systems to maximize True Total and its effect on Caller Experience in Section 5.", "labels": [], "entities": [{"text": "speech recognition and classification", "start_pos": 263, "end_pos": 300, "type": "TASK", "confidence": 0.7886920273303986}]}], "datasetContent": [], "tableCaptions": []}