{"title": [], "abstractContent": [{"text": "Books and other text-based learning material contain implicit information which can aid the learner but which usually can only be accessed through a semantic analysis of the text.", "labels": [], "entities": []}, {"text": "Definitions of new concepts appearing in the text are one such instance.", "labels": [], "entities": []}, {"text": "If extracted and presented to the learner inform of a glossary, they can provide an excellent reference for the study of the main text.", "labels": [], "entities": []}, {"text": "One way of extracting definitions is by reading through the text and annotating definitions manually-a tedious and boring job.", "labels": [], "entities": [{"text": "extracting definitions", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7655530869960785}]}, {"text": "In this paper, we explore the use of machine learning to extract definitions from non-technical texts, reducing human expert input to a minimum.", "labels": [], "entities": []}, {"text": "We report on experiments we have conducted on the use of genetic programming to learn the typical linguistic forms of definitions and a genetic algorithm to learn the relative importance of these forms.", "labels": [], "entities": []}, {"text": "Results are very positive , showing the feasibility of exploring further the use of these techniques in definition extraction.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 104, "end_pos": 125, "type": "TASK", "confidence": 0.9802592098712921}]}, {"text": "The genetic program is able to learn similar rules derived by a human linguistic expert, and the genetic algorithm is able to rank candidate definitions in an order of confidence.", "labels": [], "entities": []}], "introductionContent": [{"text": "Definitions provide the meaning of terms, giving information which could be useful in several scenarios.", "labels": [], "entities": []}, {"text": "In an eLearning context, definitions could be used by a student to assimilate knowledge, and if collected in a glossary, they enable the student to rapidly refer to definitions of keywords and the context in which they can be found.", "labels": [], "entities": []}, {"text": "Unfortunately, identifying definitions manually in a large text is along and tedious job, and should ideally be automated.", "labels": [], "entities": [{"text": "identifying definitions manually in a large text", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.8629852533340454}]}, {"text": "In texts with strong structuring (stylistic or otherwise), such as technical or medical texts, the automatic identification of definitions is possible through the use of the structure and possibly cue words.", "labels": [], "entities": [{"text": "automatic identification of definitions", "start_pos": 99, "end_pos": 138, "type": "TASK", "confidence": 0.7097994536161423}]}, {"text": "For instance, inmost mathematical textbooks, definitions are explicitly marked in the text, and usually follow a regular form.", "labels": [], "entities": []}, {"text": "In less structured texts, such as programming tutorials, identifying the sentences which are definitions can be much more challenging, since they are typically expressed in a linguistically freer form.", "labels": [], "entities": []}, {"text": "In such cases, humans have to comb through the whole text manually to tag definitional sentences.", "labels": [], "entities": []}, {"text": "One way of automating definition extraction is to consult human linguistic experts to identify linguistic forms definitions conform to, usually using either lexical patterns or through specific keywords or cuephrases contained in the sentence.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8295285999774933}]}, {"text": "Once such rules are identified, automatic tools can be applied to find the sentences matching one or more of these forms.", "labels": [], "entities": []}, {"text": "This approach has been shown to work with varying results.", "labels": [], "entities": []}, {"text": "Technical texts fare better than non-technical ones, where results are usually not of a satisfactory level.", "labels": [], "entities": []}, {"text": "Two issues which limit the success of these results are (i) the relative importance of the different linguistic forms is difficult to assess by human experts, and is thus usually ignored; and (ii) coming up with effective linguistic forms which tread the fine line between accepting most of the actual definitions, but not accepting non-definitions, requires time and expertise and can be extremely difficult.", "labels": [], "entities": []}, {"text": "Since there typically is a numeric imbalance between definitions and non-definitions in a text, having a slightly overliberal rule can result in tensor hundreds of wrong positives (non-definitions proposed as definitions), which is clearly undesirable.", "labels": [], "entities": []}, {"text": "In the approach we propose, we give a degree of importance (weight) to each linguistic form.", "labels": [], "entities": []}, {"text": "Through this technique, one could go further than simple human-engineered linguistic forms -by being able to rank the sentences by how probable the system thinks they are actual definitions.", "labels": [], "entities": []}, {"text": "The more a sentence matches against the more important forms, the higher the degree of confidence in its classification as a definition.", "labels": [], "entities": []}, {"text": "In this paper, we explore the use of machine learning techniques, in particular evolutionary algorithms, to enable the learning of sentence classifiers, separating definitions from non-definitions.", "labels": [], "entities": []}, {"text": "We have used two separate algorithms for two distinct tasks: \u2022 Relative importance of linguistic forms: Given a number of predetermined linguistic forms which definitions may (or usually) conform to, we have used a genetic algorithm to learn their relative importance.", "labels": [], "entities": []}, {"text": "Through this technique we enable a more fine-grained filter to select definitions, taking into account multiple rules, but at the same time assigning them different weights before performing the final judgement.", "labels": [], "entities": []}, {"text": "We thus benefit from having a ranking mechanism which would indicate a level of confidence in the classification of the definitions.", "labels": [], "entities": []}, {"text": "Ina semi-automated scenario, it would make the system more usable since a human expert would be presented with the best re-sults first, and results are grouped by 'quality' of the definition.", "labels": [], "entities": []}, {"text": "\u2022 Learning the linguistic forms: The previous technique assumes that we start off with linguistic forms which are able to match definitions -a task which would typically require human expert input.", "labels": [], "entities": []}, {"text": "We incorporated genetic programming techniques to learn such forms automatically by generating different rules in the classification task.", "labels": [], "entities": []}, {"text": "Within such a setup it is possible to explore new linguistic structures and test their worthiness automatically against the training data.", "labels": [], "entities": []}, {"text": "Rule which are found to be useful in classifying definitions are kept and improved upon to evolve to a better solution.", "labels": [], "entities": [{"text": "classifying definitions", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.9136945605278015}]}, {"text": "These two separate techniques are then combined to provide us with a fully automated definition extraction system by first identifying a number of linguistic forms through the use of genetic programming, and then using the genetic algorithm to assign to each rule a degree of importance.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.9219779670238495}]}, {"text": "The resulting features and their associated weights can then be used by a definition extraction tool which will not only extract candidate definitional sentences, but also rank them according to a level of confidence.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.8884681165218353}]}, {"text": "The results achieved when combining these two techniques are very promising, and encourage further investigation of these techniques in the task of automatic definition extraction.", "labels": [], "entities": [{"text": "automatic definition extraction", "start_pos": 148, "end_pos": 179, "type": "TASK", "confidence": 0.6905748248100281}]}, {"text": "In section 2 we give a short overview of definition extraction and the setup of our experiments.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.9360972344875336}]}, {"text": "In section 3 we describe the results of the genetic algorithm experiment, while in section 4 we present the genetic programming experiments and results achieved.", "labels": [], "entities": []}, {"text": "In section 5 we discuss how these two components can be merged into one complete definition extractor, and compare the results to other related work in this area in section 6.", "labels": [], "entities": [{"text": "definition extractor", "start_pos": 81, "end_pos": 101, "type": "TASK", "confidence": 0.7078423202037811}]}, {"text": "We then conclude and discuss future directions in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments using the GP delved into the three categories identified in section 2, that is the is-a, verb and punctuation categories.", "labels": [], "entities": []}, {"text": "For each category, several experiments were run, each of which resulting in different rules (albeit at times quite similar).", "labels": [], "entities": []}, {"text": "Experiments also tested the inclusion of different linguistic objects by either focusing on specific POS tags, or by generalising the particular category, say to include all nouns.", "labels": [], "entities": []}, {"text": "In the case of the verb category, some of the experiments focused on the POS tags, while others included certain words such as 'known', 'define' and similar words typically found in definitions in this category.", "labels": [], "entities": []}, {"text": "shows a summary of the best results achieved in the different categories where the GP was applied.", "labels": [], "entities": [{"text": "GP", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9423220157623291}]}, {"text": "The experiments were run with different population size ranging from 200 to 1,000 individuals.", "labels": [], "entities": []}, {"text": "Most of the experiments converged within 100 generations, and at times as early as 30 generations.", "labels": [], "entities": []}, {"text": "The selection of the individuals to survive to the next generation used elitism (which copies the best individuals of the population into the next generation as is), and selecting the remaining individuals for crossover using the stochastic universal sampling algorithm.", "labels": [], "entities": []}, {"text": "Further details about the experimental setup can be found in.", "labels": [], "entities": []}, {"text": "The GP was able to learn at times rather simple rules such as noun\u00b7is\u00b7a\u00b7noun.", "labels": [], "entities": []}, {"text": "The rules learnt for each category by the different experiments were usually similar in structure and content.", "labels": [], "entities": []}, {"text": "However in certain runs the rules represented by the individuals gave better results.", "labels": [], "entities": []}, {"text": "In the is-a category, the average f- measure obtained was around 25%, with one run managing to produce a slightly different rule achieving 28% f-measure.", "labels": [], "entities": [{"text": "f- measure", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9639642437299093}]}, {"text": "In the verb category it was noticed using part-of-speech categories was not sufficient, and that the use of keywords, such as 'know', 'define', and 'call', was necessary to achieve good results.", "labels": [], "entities": []}, {"text": "In the punctuation category we observed that results were achieved easily primarily due to a smaller search space when compared to the other categories.", "labels": [], "entities": []}, {"text": "The two experiments described above were so far isolated, each one with a particular purpose.", "labels": [], "entities": []}, {"text": "The challenge towards which we worked is to have a fully automated definition extraction tool which is easily adaptable to different domains and which ranks candidate definitions according to some level of confidence.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.8963951170444489}]}, {"text": "In this section we describe how these two separate experiments were combined together towards a fully automated definition extraction tool.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.9538367688655853}]}, {"text": "In we seethe different phases of the definition extraction process.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.9509679079055786}]}, {"text": "Phase one is the creation of an annotated training set and is not dealt within this work.", "labels": [], "entities": []}, {"text": "Given an annotated corpus with definitions, one can then move onto phase two where the GP is applied to learn useful simple features which can be used to distinguish definitions from non-definitions.", "labels": [], "entities": []}, {"text": "In phase three the GA is then used to learn weights for the rules learnt by the GP.", "labels": [], "entities": [{"text": "GA", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9736143946647644}]}, {"text": "Using the rules and weights, one can incorporate all this in a definition classification tool in phase four.", "labels": [], "entities": [{"text": "definition classification", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.9073942303657532}]}, {"text": "In this section we present the results achieved from combining phase two and three together.", "labels": [], "entities": []}, {"text": "For the purpose of combining the two phases, we used the best rules learnt by ten different GP experiments in the is-a category.", "labels": [], "entities": []}, {"text": "These individuals were used by the GA to learn their respective weights.", "labels": [], "entities": []}, {"text": "The setup is shown in where the final result is a set of rules in the is-a category together with their allocated weights indicating the level of effectiveness each weight has.", "labels": [], "entities": []}, {"text": "As shown in the previous section, the rules the GP learnt without the application of the weights resulted at best in f-measure being 28%.", "labels": [], "entities": [{"text": "f-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9953492283821106}]}, {"text": "Once weights were learnt and applied to the definition extraction tool, this increased to 68% f-measure.", "labels": [], "entities": [{"text": "definition extraction", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.9377448856830597}, {"text": "f-measure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9738667607307434}]}, {"text": "This improvement shows that learning weights is useful to the classification task since it does matter which rule is actually carrying out the classification of sentences.", "labels": [], "entities": [{"text": "classification task", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8950885832309723}]}, {"text": "Further analysis show that the f-measure is resulting from a 100% precision and a 51% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9901449680328369}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9990870952606201}]}, {"text": "This means that by combining the rules learnt and their associated weights, we succeeded in classifying just over half of the annotated definitions, without classifying any incorrect definitions.", "labels": [], "entities": []}, {"text": "There are several factors behind these results: 1.", "labels": [], "entities": []}, {"text": "This experiment was carried out on only one corpus, so the rules learnt together with their respective weights, were specific to the corpus used.", "labels": [], "entities": []}, {"text": "Achieving such a good result is only indicative that as in any machine learning process, the two algorithms were able to learn rules and weights specific to our corpus.", "labels": [], "entities": []}, {"text": "2. The recall of 51% represents definitions for which the genetic program did not learn rules for.", "labels": [], "entities": [{"text": "recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9992400407791138}]}, {"text": "Since these algorithms are searching for solutions in an automatic manner without expert feedback, it is the case that not all possible rules are explored.", "labels": [], "entities": []}, {"text": "This can be tackled by including rules from more experiments or by having direct feedback from a linguistic expert (say, injection of good humanly crafted rules into the population).", "labels": [], "entities": []}, {"text": "Notwithstanding the conditions under which they were achieved, the results are very promising.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for best experiments", "labels": [], "entities": []}, {"text": " Table 2: Summary of results  Category  F-measure Precision Recall  Is-a  0.28  0.22  0.39  Verb  0.20  0.14  0.33  Punctuation  0.30  0.25  0.36", "labels": [], "entities": [{"text": "Precision Recall  Is-a  0.28  0.22  0.39  Verb  0.20  0.14  0.33  Punctuation  0.30  0.25  0.36", "start_pos": 50, "end_pos": 145, "type": "METRIC", "confidence": 0.8582261545317513}]}]}