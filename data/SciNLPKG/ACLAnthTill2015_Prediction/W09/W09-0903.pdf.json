{"title": [{"text": "What's in a Message?", "labels": [], "entities": [{"text": "What's in a Message?", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5826539347569147}]}], "abstractContent": [{"text": "In this paper we present the first step in a larger series of experiments for the induction of pred-icate/argument structures.", "labels": [], "entities": []}, {"text": "The structures that we are inducing are very similar to the conceptual structures that are used in Frame Semantics (such as FrameNet).", "labels": [], "entities": []}, {"text": "Those structures are called messages and they were previously used in the context of a multi-document sum-marization system of evolving events.", "labels": [], "entities": []}, {"text": "The series of experiments that we are proposing are essentially composed from two stages.", "labels": [], "entities": []}, {"text": "In the first stage we are trying to extract a representative vocabulary of words.", "labels": [], "entities": []}, {"text": "This vocabulary is later used in the second stage, during which we apply to it various clustering approaches in order to identify the clusters of predicates and arguments-or frames and semantic roles, to use the jargon of Frame Semantics.", "labels": [], "entities": []}, {"text": "This paper presents in detail and evaluates the first stage.", "labels": [], "entities": []}], "introductionContent": [{"text": "Take a sentence, any sentence for that matter; step back fora while and try to perceive that sentence in its most abstract form.", "labels": [], "entities": []}, {"text": "What you will notice is that once you try to abstract away sentences, several regularities between them will start to emerge.", "labels": [], "entities": []}, {"text": "To start with, there is almost always an action that is performed.", "labels": [], "entities": []}, {"text": "Then, there is most of the times an agent that is performing this action and a patient or a benefactor that is receiving this action, and it could be the case that this action is performed with the aid of a certain instrument.", "labels": [], "entities": []}, {"text": "In other words, within a sentence-and in respect to its actiondenoting word, or predicate in linguistic terms-there will be several entities that are associated with the predicate, playing each time a specific semantic role.", "labels": [], "entities": []}, {"text": "The notion of semantic roles can be traced back to theory of Frame Semantics.", "labels": [], "entities": []}, {"text": "According to this theory then, a frame is a conceptual structure which tries to describe a stereotypical situation, event or object along with its participants and props.", "labels": [], "entities": []}, {"text": "Each frame takes a name (e.g. COMMERCIAL TRANSAC-TION) and contains a list of Lexical Units (LUs) which actually evoke this frame.", "labels": [], "entities": []}, {"text": "An LU is nothing else than a specific word or a specific meaning of a word in the case of polysemous words.", "labels": [], "entities": []}, {"text": "To continue the previous example, some LUs that evoke the frame of COMMER-CIAL TRANSACTION could be the verbs buy, sell, etc.", "labels": [], "entities": []}, {"text": "Finally, the frames contain several frame elements or semantic roles which actually denote the abstract conceptual entities that are involved with the particular frame.", "labels": [], "entities": []}, {"text": "Research in semantic roles can be distinguished into two major branches.", "labels": [], "entities": [{"text": "semantic roles", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.8024317920207977}]}, {"text": "The first branch of research consists in defining an ontology of semantic roles, the frames in which the semantic roles are found as well as defining the LUs that evoke those frames.", "labels": [], "entities": []}, {"text": "The second branch of research, on the other hand, stipulates the existence of a set of frames, including semantic roles and LUs; its goal then, is the creation of an algorithm that given such a set of frames containing the semantic roles, will be able to label the appropriate portions of a sentence with the corresponding semantic roles.", "labels": [], "entities": []}, {"text": "This second branch of research is known as semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7986831068992615}]}, {"text": "Most of the research concerning the definition of the semantic roles has been carried out by linguists who are manually examining a certain amount of frames before finally defining the semantic roles and the frames that contain those semantic roles.", "labels": [], "entities": [{"text": "definition of the semantic roles", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.80445796251297}]}, {"text": "Two such projects that are widely known are the FrameNet () and PropBank/NomBank 2 ().", "labels": [], "entities": [{"text": "PropBank/NomBank 2", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.840519905090332}]}, {"text": "Due to the fact that the aforementioned projects are accompanied by a large amount of annotated data, computer scientists have started creating algorithms, mostly based on statistics () in order to automatically label the semantic roles in a sentence.", "labels": [], "entities": []}, {"text": "Those algorithms take as input the frame that We would like to note here that although the two approaches (FrameNet and PropBank/NomBank) share many common elements, they have several differences as well.", "labels": [], "entities": []}, {"text": "Two major differences, for example, are the fact that the Linguistic Units (FrameNet) are referred to as Relations (PropBank/NomBank), and that for the definition of the semantic roles in the case of PropBank/NomBank there is no reference ontology.", "labels": [], "entities": []}, {"text": "A detailed analysis of the differences between FrameNet and PropBank/NomBank would be out of the scope of this paper.", "labels": [], "entities": [{"text": "PropBank/NomBank", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.7901326616605123}]}, {"text": "contains the roles as well as the predicate 3 of the sentence.", "labels": [], "entities": []}, {"text": "Despite the fact that during the last years we have seen an increasing interest concerning semantic role labeling, we have not seen many advancements concerning the issue of automatically inducing semantic roles from raw textual corpora.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.648034522930781}]}, {"text": "Such a process of induction would involve, firstly the identification of the words that would serve as predicates and secondly the creation of the appropriate clusters of word sequences, within the limits of a sentence, that behave similarly in relation to the given predicates.", "labels": [], "entities": []}, {"text": "Although those clusters of word sequences could not actually be said to serve in themselves as the semantic roles, 5 they can nevertheless be viewed as containing characteristic word sequences of specific semantic roles.", "labels": [], "entities": []}, {"text": "The last point has the implication that if one is looking fora human intuitive naming of the semantic role that is implied by the cluster then one should look elsewhere.", "labels": [], "entities": []}, {"text": "This is actually reminiscent of the approach that is carried out by PropBank/NomBank in which each semantic role is labeled as Arg1 through Arg5 with the semantics given aside in a human readable natural language sentence.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to contribute to the research problem of frame induction, that is of the creation of frames, including their associated semantic roles, given as input only a set of textual documents.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.8253642022609711}]}, {"text": "More specifically we propose a general methodology to accomplish this task, and we test its first stage which includes the use of corpus statistics for the creation of a subset of words, from the initial universe of initial words that are present in the corpus.", "labels": [], "entities": []}, {"text": "This subset will later be used for the identification of the predicates as well as the semantic roles.", "labels": [], "entities": []}, {"text": "Knowing that the problem of frame induction is very difficult in the general case, we limit ourselves to a specific genre and domain trying to exploit the characteristics that exist in that domain.", "labels": [], "entities": [{"text": "frame induction", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.8253734111785889}]}, {"text": "The domain that we have chosen is that of the terroristic incidents which involve hostages.", "labels": [], "entities": []}, {"text": "Nevertheless, the same methodology could be applied to other domains.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2 we describe the data on which we have applied our methodology, which itself is described in detail in section 3.", "labels": [], "entities": []}, {"text": "Section 4 describes the actual experiments that we have performed and the results obtained, while a discussion of those results follows in section 5.", "labels": [], "entities": []}, {"text": "Finally, section 6 contains a description of the related work while we present our future work and conclusions in section 7.", "labels": [], "entities": []}, {"text": "The annotated data that we have used in order to perform our experiments come from a previous work on automatic multi-document summarization of events that evolve through time ().", "labels": [], "entities": [{"text": "multi-document summarization of events that evolve", "start_pos": 112, "end_pos": 162, "type": "TASK", "confidence": 0.722509374221166}]}, {"text": "The methodology that is followed is based on the identification of similarities and differences-between documents that describe the evolution of an event-synchronically as well as diachronically.", "labels": [], "entities": []}, {"text": "In order to do so, the notion of Synchronic and Diachronic cross document Relations (SDRs), was introduced.", "labels": [], "entities": [{"text": "Synchronic and Diachronic cross document Relations (SDRs)", "start_pos": 33, "end_pos": 90, "type": "TASK", "confidence": 0.6465185516410403}]}, {"text": "SDRs connect not the documents themselves but some semantic structures that were called messages.", "labels": [], "entities": []}, {"text": "The connection of the messages with the SDRs resulted in the creation of a semantic graph that was then fed to a Natural Language Generation (NLG) system in order to produce the final summary.", "labels": [], "entities": []}, {"text": "Although the notion of messages was originally inspired by the notion of messages as used in the area of NLG, for example during the stage of Content Determination as described in, and in general they do follow the spirit of the initial definition by Reiter & Dale, in the following section we would like to make it clear what the notion of messages represents for us.", "labels": [], "entities": [{"text": "Content Determination", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.6738089770078659}]}, {"text": "In the rest of the paper, when we refer to the notion of messages, it will be in the context of the discussion that follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpus that we have consists of 163 journalistic articles which describe the evolution of five different terroristic incidents that involved hostages.", "labels": [], "entities": []}, {"text": "The corpus was initially used in the context of training a multidocument summarization system.", "labels": [], "entities": []}, {"text": "Out of the 3,027 sentences that the corpus contains, about one third (1,017 sentences) were annotated with the 48 message types that were mentioned in section 2.1.", "labels": [], "entities": []}, {"text": "The corpus contained 7,185 distinct verbs and nouns, which actually constitute the U of the formula (1) above.", "labels": [], "entities": []}, {"text": "Out of those 7,185 distinct verbs and nouns 2,426 appear in the sentences that have been annotated with the messages.", "labels": [], "entities": []}, {"text": "Our goal was to create this set that approached as much as possible to the set of 2,426 distinct verbs and nouns that are found in the messages.", "labels": [], "entities": []}, {"text": "Using the four different statistical measures presented in the previous section, we tried to reconstruct that set.", "labels": [], "entities": []}, {"text": "In order to understand how the statistical measures behaved, we varied for each one of them the value of the threshold used.", "labels": [], "entities": []}, {"text": "For each statistical measure used, the threshold represents something different.", "labels": [], "entities": []}, {"text": "For the Collection Frequency measure the threshold represents the n% most frequent words that appear in the corpus.", "labels": [], "entities": []}, {"text": "For the Document Frequency it represents the n% most frequent words that appear in each document separately.", "labels": [], "entities": [{"text": "Document Frequency", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.4933892786502838}]}, {"text": "For tf.idf it represents the words with the highest n% tf.idf score in each document.", "labels": [], "entities": []}, {"text": "Finally for the Inter-document Frequency the threshold represents the verbs and nouns that appear in at least n documents.", "labels": [], "entities": [{"text": "Inter-document Frequency", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.4965668320655823}]}, {"text": "Since for the first three measures the threshold represents a percentage, we varied it from 1 to 100 in order to study how this measure behaves.", "labels": [], "entities": []}, {"text": "For the case of the Inter-document Frequency, we varied the threshold from 1 to 73 which represents the maximum number of documents in which a word appeared.", "labels": [], "entities": [{"text": "Inter-document Frequency", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.5145442485809326}]}, {"text": "In order to measure the performance of the statistical measures employed, we used four different evaluation measures, often employed in the information retrieval The obtained graphs that combine the evaluation results for the four statistical measures presented in section 3 are shown in.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.8132631182670593}]}, {"text": "A first remark that we can make in respect to those graphs is that concerning the collection frequency, document frequency and tf.idf measures, for small threshold numbers we have more or less high precision values while the recall and fallout values are low.", "labels": [], "entities": [{"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9942593574523926}, {"text": "recall", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.9990494847297668}]}, {"text": "This implies that for smaller threshold values the obtained sets are rather small, in relation to M (and by consequence to U as well).", "labels": [], "entities": []}, {"text": "As the threshold increases we have the opposite situation, that is the precision falls while the recall and the fallout increases, implying that we get much bigger sets of verbs and nouns.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9996598958969116}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9995773434638977}]}, {"text": "In terms of absolute numbers now, the best Fmeasure is given by the Collection Frequency measure with a threshold value of 46%.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9875915050506592}, {"text": "Collection Frequency measure", "start_pos": 68, "end_pos": 96, "type": "METRIC", "confidence": 0.6844930251439413}]}, {"text": "In other words, the best results-in terms of F-measure-is given by the union of the 46% most frequent verbs and nouns that appear in the corpus.", "labels": [], "entities": [{"text": "F-measure-is", "start_pos": 45, "end_pos": 57, "type": "METRIC", "confidence": 0.997859537601471}]}, {"text": "For this threshold the Precision is 54.14%, the Recall is 72.18% and the F-measure is 61,87%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9989432692527771}, {"text": "Recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9864990711212158}, {"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9994590878486633}]}, {"text": "This high F-measure though comes at a certain cost since the Fallout is at 31.16%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9993646740913391}, {"text": "Fallout", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9570995569229126}]}, {"text": "This implies that although we get a rather satisfying score in terms of precision and recall, the number of false positives that we get is rather high in relation to our universe.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9995818734169006}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9993195533752441}]}, {"text": "As we have earlier said, a motivating factor of this paper is the automatic induction of the structures that we have called messages; the extracted lexicon of verbs and messages will later be used by an unsupervised clustering algorithm in order to create the classes of words which will correspond to the message types.", "labels": [], "entities": []}, {"text": "For this reason, although we prefer to have an F-measure as high as possible, we also want to have a fallout measure as low as possible, so that the number of false positives will not perturb the clustering algorithm.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.982725203037262}]}, {"text": "If, on the other hand, we examine the relation between the F-measure and Fallout, we notice that for the Inter-document Frequency with a threshold value of 4 we obtain a Precision of 71.60%, a recall of 43.86% and an F-measure of 54.40%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9680648446083069}, {"text": "Precision", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9993878602981567}, {"text": "recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9985978007316589}, {"text": "F-measure", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9961918592453003}]}, {"text": "Most importantly though we get a fallout measure of 8.86% which implies that the percentage of wrongly classified verbs and nouns compose a small percentage of the total universe of verbs and nouns.", "labels": [], "entities": []}, {"text": "This combination of high F-measure and very low Fallout is very important for later stages during the process of message induction.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9984390139579773}, {"text": "Fallout", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9957327246665955}, {"text": "message induction", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.915088027715683}]}], "tableCaptions": []}