{"title": [{"text": "The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages", "labels": [], "entities": [{"text": "Syntactic and Semantic Dependencies in Multiple Languages", "start_pos": 28, "end_pos": 85, "type": "TASK", "confidence": 0.7021168129784721}]}], "abstractContent": [{"text": "For the 11th straight year, the Conference on Computational Natural Language Learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting.", "labels": [], "entities": []}, {"text": "In 2009, the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages.", "labels": [], "entities": [{"text": "parsing of syntactic and semantic dependencies", "start_pos": 52, "end_pos": 98, "type": "TASK", "confidence": 0.7848389347394308}]}, {"text": "This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task.", "labels": [], "entities": []}, {"text": "In this paper, we define the shared task, describe how the data sets were created and show their quantitative properties, report the results and summarize the approaches of the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) launches a competitive, open \"Shared Task\".", "labels": [], "entities": []}, {"text": "A common (\"shared\") task is defined and datasets are provided for its participants.", "labels": [], "entities": []}, {"text": "In, the shared tasks were dedicated to semantic role labeling (SRL) in a monolingual setting.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7937971452871958}]}, {"text": "In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages.", "labels": [], "entities": [{"text": "parsing of syntactic dependencies", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.8638057559728622}]}, {"text": "In 2008, the shared task () used a unified dependencybased formalism, which modeled both syntactic dependencies and semantic roles for English.", "labels": [], "entities": []}, {"text": "The CoNLL-2009 Shared Task has built on the 2008 results by providing data for six more languages (Catalan, Chinese, Czech, German, Japanese and Spanish) in addition to the original English . It has thus naturally extended the path taken by the five most recent CoNLL shared tasks.", "labels": [], "entities": []}, {"text": "As in 2008, the CoNLL-2009 shared task combined dependency parsing and the task of identifying and labeling semantic arguments of verbs (and other parts of speech whenever available).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.823828935623169}]}, {"text": "Participants had to choose from two tasks: \u2022 Joint task (syntactic dependency parsing and semantic role labeling), or \u2022 SRL-only task (syntactic dependency parses have been provided by the organizers, using state-of-the art parsers for the individual languages).", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.7581523458162943}, {"text": "semantic role labeling", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6070756117502848}]}], "datasetContent": [{"text": "It was required that participants submit results in all seven languages in the chosen task and in any of (or both) the challenges.", "labels": [], "entities": []}, {"text": "Submission of out-of-domain data files has been optional.: Description of the fields (columns) in the data provided.", "labels": [], "entities": []}, {"text": "The values of columns 9, 11 and 14 and above are not provided in the evaluation data; for the Joint task, columns 9-12 are also empty in the evaluation data.", "labels": [], "entities": []}, {"text": "closed challenge, Macro F 1 score.", "labels": [], "entities": [{"text": "Macro F 1 score", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.6867772564291954}]}, {"text": "However, scores can also be computed fora number of other conditions: \u2022 Task: Joint or SRL-only \u2022 Challenge: open or closed \u2022 Domain: in-domain data (IDD, separated from training corpus) or out-of-domain data (OOD) Joint task participants are also evaluated separately on the syntactic dependency task (labeled attachment score, LAS).", "labels": [], "entities": []}, {"text": "Finally, systems competing in both tasks are compared on semantic role labeling alone, to assess the impact of the the joint parsing/SRL task compared to an SRL-only task on preparsed data.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6026327311992645}, {"text": "joint parsing/SRL", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.6067222058773041}]}, {"text": "Finally, as an explanatory measure, precision and recall of the semantic labeling task have been computed and tabulated.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9994639754295349}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.999359667301178}, {"text": "semantic labeling task", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.7069456577301025}]}, {"text": "We have decided to omit several evaluation figures that were reported in previous years, such as the percentage of completely correct sentences (\"Exact Match\"), unlabeled scores, etc.", "labels": [], "entities": [{"text": "Exact Match\")", "start_pos": 146, "end_pos": 159, "type": "METRIC", "confidence": 0.9210240046183268}]}, {"text": "With seven languages, two tasks (plus two challenges, and the IDD/OOD distinction), there are enough results to get lost even as it is.", "labels": [], "entities": [{"text": "OOD distinction", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.7011006772518158}]}], "tableCaptions": [{"text": " Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are  not provided in the evaluation data; for the Joint task, columns 9-12 are also empty in the evaluation data.", "labels": [], "entities": []}, {"text": " Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original  treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All  evaluation data statistics are derived from the in-domain evaluation data.", "labels": [], "entities": [{"text": "CoNLL-2009 Shared Task languages", "start_pos": 45, "end_pos": 77, "type": "DATASET", "confidence": 0.8354765474796295}]}, {"text": " Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009  Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.", "labels": [], "entities": [{"text": "Unigram probability", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9544691741466522}, {"text": "CoNLL-2009  Shared Task", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.7830701867739359}]}, {"text": " Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009  Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The \"Avg.\" line  shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by  FILLPRED='Y').", "labels": [], "entities": [{"text": "Unigram probability", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9448550939559937}, {"text": "CoNLL-2009  Shared Task", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.7971803545951843}, {"text": "Avg.\" line", "start_pos": 210, "end_pos": 220, "type": "METRIC", "confidence": 0.9351151386896769}, {"text": "FILLPRED='Y')", "start_pos": 333, "end_pos": 346, "type": "METRIC", "confidence": 0.9234021902084351}]}, {"text": " Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added  only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the  language-averaged macro F 1 score on the closed challenge Joint task. Bold numbers denote the best result for a given  language.", "labels": [], "entities": [{"text": "language-averaged macro F 1 score", "start_pos": 238, "end_pos": 271, "type": "METRIC", "confidence": 0.6324190139770508}]}, {"text": " Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name  (first name added only where needed) of the author who registered for the evaluation data. Results are sorted in  descending order of the semantic labeled F 1 score (closed challenge). Bold numbers denote the best result for a given  language. Separate ranking is provided for SRL-only systems.", "labels": [], "entities": [{"text": "SRL-only", "start_pos": 397, "end_pos": 405, "type": "TASK", "confidence": 0.9331207275390625}]}]}