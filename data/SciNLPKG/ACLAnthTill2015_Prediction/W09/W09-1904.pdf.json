{"title": [{"text": "Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria", "labels": [], "entities": []}], "abstractContent": [{"text": "Annotation acquisition is an essential step in training supervised classifiers.", "labels": [], "entities": [{"text": "Annotation acquisition", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9168290793895721}]}, {"text": "However, manual annotation is often time-consuming and expensive.", "labels": [], "entities": []}, {"text": "The possibility of recruiting anno-tators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates.", "labels": [], "entities": []}, {"text": "In this paper, we consider the difficult problem of classifying sentiment in political blog snippets.", "labels": [], "entities": [{"text": "classifying sentiment in political blog snippets", "start_pos": 52, "end_pos": 100, "type": "TASK", "confidence": 0.8996639947096506}]}, {"text": "Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined.", "labels": [], "entities": []}, {"text": "Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty.", "labels": [], "entities": []}, {"text": "Analysis confirm the utility of these criteria on improving data quality.", "labels": [], "entities": []}, {"text": "We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8364546000957489}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9493195414543152}]}], "introductionContent": [{"text": "Crowdsourcing) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models.", "labels": [], "entities": []}, {"text": "To sense the potential of crowdsourcing, consider an observation in von: a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days.", "labels": [], "entities": [{"text": "von", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9553377628326416}]}, {"text": "Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9140763878822327}]}, {"text": "With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings anew set of issues to the table.", "labels": [], "entities": []}, {"text": "These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations.", "labels": [], "entities": []}, {"text": "Consequently, the obtained annotations maybe noisy by nature, and might require additional validation or scrutiny.", "labels": [], "entities": []}, {"text": "Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How does one handle differences among workers in terms of the quality of annotations they provide?", "labels": [], "entities": []}, {"text": "How useful are noisy annotations for the end task of creating a model?", "labels": [], "entities": []}, {"text": "Is it possible to identify genuinely ambiguous examples via annotator disagreements?", "labels": [], "entities": []}, {"text": "How should these considerations be treated with respect to intrinsic informativeness of examples?", "labels": [], "entities": []}, {"text": "These questions also hint at a strong connection to active learning, with annotation quality as anew dimension to the problem.", "labels": [], "entities": []}, {"text": "As a challenging empirical testbed for these issues, we consider the problem of sentiment classification on political blogs.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.8731387555599213}]}, {"text": "Given a snippet drawn from apolitical blog post, the desired output is a polarity score that indicates whether the sentiment expressed is positive or negative.", "labels": [], "entities": []}, {"text": "Such an analysis provides a view of the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere.", "labels": [], "entities": []}, {"text": "Recently, sentiment analy-sis is emerging as a critical methodology for social media analytics.", "labels": [], "entities": [{"text": "social media analytics", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7012630899747213}]}, {"text": "Previous research has focused on classifying subjective-versus-objective expressions (), and also on accurate sentiment polarity assignment.", "labels": [], "entities": [{"text": "accurate sentiment polarity assignment", "start_pos": 101, "end_pos": 139, "type": "TASK", "confidence": 0.681484617292881}]}, {"text": "The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (), or quality annotation data for statistical training.", "labels": [], "entities": []}, {"text": "While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (), manual intervention in the annotation process is harder to avoid.", "labels": [], "entities": []}, {"text": "Moreover, the task of annotating blog-post snippets is challenging, particularly in a charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues.", "labels": [], "entities": []}, {"text": "The downside is that high-performance models are generally difficult to construct, but the upside is that annotation and data-quality issues are more clearly exposed.", "labels": [], "entities": []}, {"text": "In this paper we aim to provide an empirical basis for the use of data selection criteria in the context of sentiment analysis in political blogs.", "labels": [], "entities": [{"text": "sentiment analysis in political blogs", "start_pos": 108, "end_pos": 145, "type": "TASK", "confidence": 0.8643290519714355}]}, {"text": "Specifically, we highlight the need fora set of criteria that can be applied to screen untrustworthy annotators and select informative yet unambiguous examples for the end goal of predictive modeling.", "labels": [], "entities": [{"text": "predictive modeling", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.9607667326927185}]}, {"text": "In Section 2, we first examine annotation data obtained by both the expert and non-expert annotators to quantify the impact of including non-experts.", "labels": [], "entities": []}, {"text": "Then, in Section 3, we quantify criteria that can be used to select annotators and examples for selective sampling.", "labels": [], "entities": []}, {"text": "Next, in Section 4, we address the questions of whether the noisy annotations are still useful for this task and study the effect of the different selection criteria on the performance of this task.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we present conclusion and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "3 raises two important questions: (1) how useful are noisy annotations for sentiment analysis, and what is the effect of online annotation selection on improving sentiment polarity classification?", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.971299022436142}, {"text": "sentiment polarity classification", "start_pos": 162, "end_pos": 195, "type": "TASK", "confidence": 0.8165703217188517}]}], "tableCaptions": [{"text": " Table 1: Summary matrix for the three on-site annotators'  sentiment codings.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8051626682281494}, {"text": "sentiment codings", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8029484450817108}]}, {"text": " Table 2: Average prediction accuracy on gold standard (GS) using one-coder strategy and inter-coder agreement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9456550478935242}]}, {"text": " Table 3: Accuracy of sentiment classification methods.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9813151955604553}, {"text": "sentiment classification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.9296876192092896}]}, {"text": " Table 4: Effect of annotation selection on classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9597482681274414}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9559581279754639}]}]}