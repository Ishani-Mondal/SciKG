{"title": [{"text": "Unsupervised Induction of Sentence Compression Rules", "labels": [], "entities": [{"text": "Unsupervised Induction of Sentence Compression", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.739504051208496}]}], "abstractContent": [{"text": "In this paper, we propose anew unsu-pervised approach to sentence compression based on shallow linguistic processing.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7701193690299988}]}, {"text": "For that purpose, paraphrase extraction and alignment is performed over web news stories extracted automatically from the web on a daily basis to provide struc-tured data examples to the learning process.", "labels": [], "entities": [{"text": "paraphrase extraction and alignment", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.7375523597002029}]}, {"text": "Compression rules are then learned through the application of Inductive Logic Programming techniques.", "labels": [], "entities": []}, {"text": "Qualitative and quantitative evaluations suggests that this is a worth following approach, which might be even improved in the future.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression, simplification or summarization has been an active research subject during this decade.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9572715163230896}, {"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8767621517181396}]}, {"text": "A set of approaches involving machine learning algorithms and statistical models have been experimented and documented in the literature and several of these are described next.", "labels": [], "entities": []}], "datasetContent": [{"text": "A relevant issue, not very commonly discussed, is the Utility of a learned theory.", "labels": [], "entities": [{"text": "Utility", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.4986148178577423}]}, {"text": "In real life problems, people maybe more interested in the volume of data processed than the quality of the results.", "labels": [], "entities": []}, {"text": "Maybe, between a system which is 90% precise and processes only 10% of data, and a system with 70% precision, processing 50% of data, the user would prefer the last one.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9982560276985168}]}, {"text": "The Utility maybe a stronger than the Recall measure, used for the evaluation of supervised learning systems, because the later measures how many instances were well identified or processed from the test set only, and the former takes into account the whole universe.", "labels": [], "entities": [{"text": "Utility", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8761763572692871}, {"text": "Recall measure", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.9045667350292206}]}, {"text": "For example, in a sentence compression system, it is important to know how many sentences would be compressed, from the whole possible set of sentences encountered in electronic news papers, or in classical literature books, or both.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7514885365962982}]}, {"text": "This is what we mean hereby Utility.", "labels": [], "entities": [{"text": "Utility", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.7777147889137268}]}, {"text": "The Ngram-Simplification methodology is an automatic extrinsic test, performed to perceive how much a given sentence reduction ruleset would simplify sentences in terms of syntactical complexity.", "labels": [], "entities": []}, {"text": "The answer is not obvious at first sight, because even smaller sentences can contain more improbable syntactical subsequences than their uncompressed versions.", "labels": [], "entities": []}, {"text": "To evaluate the syntactical complexity of a sentence, we use a 4 \u2212 gram model and compute a relative 11 sequence probability as defined in Equation 12 where W = [t 1 , t 2 , ..., t m ] is the sequence of part-of-speech tags fora given sentence with size m.", "labels": [], "entities": []}, {"text": "The third evaluation is qualitative.", "labels": [], "entities": []}, {"text": "We measure the quality of the learned rules when applied to sentence reduction.", "labels": [], "entities": [{"text": "sentence reduction", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7313133031129837}]}, {"text": "The objective is to assess how correct is the application of the reduction rules.", "labels": [], "entities": []}, {"text": "This evaluation was made through manual annotation fora statistically representative random sample of compressed sentences.", "labels": [], "entities": []}, {"text": "A human judged the adequacy and Correctness of each compression rule to a given sentence segment, in a scale from 1 to 5, where 1 means that it is absolutely incorrect and inadequate, and 5 that the compression rule fits perfectly to the situation (sentence) being analyzed.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9987502098083496}]}, {"text": "To perform our evaluation, a sample of 300 sentences were randomly extracted, whereat least one compression rule had been applied.", "labels": [], "entities": []}, {"text": "This evaluation set maybe subdivided into three subsets, where 100 instances came from rules with Z (X) = 1 (BD1), 100 from rules with Z (X) = 2 (BD2), and the other 100 from rules with Z (X) = 3 (BD3).", "labels": [], "entities": []}, {"text": "Another random sample, also with 100 cases has been extracted to evaluate our base-line (BL) which consists in the direct application of the bubble set to make compressions.", "labels": [], "entities": [{"text": "BL)", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.8899789750576019}]}, {"text": "This means that no learning process is performed.", "labels": [], "entities": []}, {"text": "Instead, we store the complete bubble set as if they were rules by themselves (in the same manner as) do).", "labels": [], "entities": []}, {"text": "compiles the comparative results for Correctness, Precision, Utility and Ngramsimplification for all datasets.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9886815547943115}, {"text": "Precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9964337348937988}, {"text": "Utility", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9708338379859924}, {"text": "Ngramsimplification", "start_pos": 73, "end_pos": 92, "type": "METRIC", "confidence": 0.8905477523803711}]}, {"text": "In particular, Ngram-simplification in percentage is the proportion of test cases where P {reduced(  sidering the three experiences, BD1, BD2, and BD3, as a unique evaluation run, we obtained a mean Correctness quality of 3.867 (i.e. 77.33% Precision), a mean Utility of 48.45%, and a mean Ngram-simplification equal to 89.53%, which are significantly better than the baseline.", "labels": [], "entities": [{"text": "BD1", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.8491598963737488}, {"text": "BD2", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.7496432065963745}, {"text": "BD3", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.9610432386398315}, {"text": "Correctness quality", "start_pos": 199, "end_pos": 218, "type": "METRIC", "confidence": 0.9848804473876953}, {"text": "Precision", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.9959316849708557}, {"text": "Utility", "start_pos": 260, "end_pos": 267, "type": "METRIC", "confidence": 0.9447811841964722}]}, {"text": "Moreover, best results overall are obtained for BD2 with 80.6% Precision, 85.72% Utility and 90.03% Ngram-simplification which means that we can expect a reduction of two words with high quality fora great number of sentences.", "labels": [], "entities": [{"text": "Precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9971864819526672}, {"text": "Utility", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9977675676345825}]}, {"text": "In particular, shows examples of learned rules.", "labels": [], "entities": []}], "tableCaptions": []}