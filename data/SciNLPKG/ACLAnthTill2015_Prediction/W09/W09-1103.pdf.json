{"title": [{"text": "Sample Selection for Statistical Parsers: Cognitively Driven Algorithms and Evaluation Measures", "labels": [], "entities": []}], "abstractContent": [{"text": "Creating large amounts of manually annotated training data for statistical parsers imposes heavy cognitive load on the human annota-tor and is thus costly and error prone.", "labels": [], "entities": []}, {"text": "It is hence of high importance to decrease the human efforts involved in creating training data without harming parser performance.", "labels": [], "entities": []}, {"text": "For constituency parsers, these efforts are traditionally evaluated using the total number of constituents (TC) measure, assuming uniform cost for each annotated item.", "labels": [], "entities": [{"text": "constituency parsers", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7085404098033905}, {"text": "total number of constituents (TC) measure", "start_pos": 78, "end_pos": 119, "type": "METRIC", "confidence": 0.6664793565869331}]}, {"text": "In this paper, we introduce novel measures that quantify aspects of the cognitive efforts of the human annota-tor that are not reflected by the TC measure, and show that they are well established in the psycholinguistic literature.", "labels": [], "entities": []}, {"text": "We present a novel parameter based sample selection approach for creating good samples in terms of these measures.", "labels": [], "entities": []}, {"text": "We describe methods for global op-timisation of lexical parameters of the sample based on a novel optimisation problem, the constrained multiset multicover problem, and for cluster-based sampling according to syntactic parameters.", "labels": [], "entities": []}, {"text": "Our methods outperform previously suggested methods in terms of the new measures, while maintaining similar TC performance.", "labels": [], "entities": [{"text": "TC", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.7168384790420532}]}], "introductionContent": [{"text": "State of the art statistical parsers require large amounts of manually annotated data to achieve good performance.", "labels": [], "entities": []}, {"text": "Creating such data imposes heavy cognitive load on the human annotator and is thus costly and error prone.", "labels": [], "entities": []}, {"text": "Statistical parsers are major components in NLP applications such as QA (), MT () and SRL ().", "labels": [], "entities": [{"text": "Statistical parsers", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6575006246566772}]}, {"text": "These often operate over the highly variable Web, which consists of texts written in many languages and genres.", "labels": [], "entities": []}, {"text": "Since the performance of parsers markedly degrades when training and test data come from different domains (), large amounts of training data from each domain are required for using them effectively.", "labels": [], "entities": []}, {"text": "Thus, decreasing the human efforts involved in creating training data for parsers without harming their performance is of high importance.", "labels": [], "entities": []}, {"text": "In this paper we address this problem through sample selection: given a parsing algorithm and a large pool of unannotated sentences S, select a subset S 1 \u2282 S for human annotation such that the human efforts in annotating S 1 are minimized while the parser performance when trained with this sample is maximized.", "labels": [], "entities": []}, {"text": "Previous works addressing training sample size vs. parser performance for constituency parsers (Section 2) evaluated training sample size using the total number of constituents (TC).", "labels": [], "entities": []}, {"text": "Sentences differ in length and therefore in annotation efforts, and it has been argued (see, e.g,) that TC reflects the number of decisions the human annotator makes when syntactically annotating the sample, assuming uniform cost for each decision.", "labels": [], "entities": [{"text": "TC", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9551095962524414}]}, {"text": "In this paper we posit that important aspects of the efforts involved in annotating a sample are not reflected by the TC measure.", "labels": [], "entities": [{"text": "TC measure", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.8326509296894073}]}, {"text": "Since annotators analyze sentences rather than a bag of constituents, sentence structure has a major impact on their cognitive efforts.", "labels": [], "entities": []}, {"text": "Sizeable psycholinguistic literature points to the connection between nested structures in the syntactic structure of a sentence and its annotation efforts.", "labels": [], "entities": []}, {"text": "This has motivated us to introduce (Section 3) three sample size measures, the total and av-erage number of nested structures of degree kin the sample, and the average number of constituents per sentence in the sample.", "labels": [], "entities": []}, {"text": "Active learning algorithms for sample selection focus on sentences that are difficult for the parsing algorithm when trained with the available training data (Section 2).", "labels": [], "entities": [{"text": "sample selection", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7052427381277084}]}, {"text": "In Section 5 we show that active learning samples contain a high number of complex structures, much higher than their number in a randomly selected sample that achieves the same parser performance level.", "labels": [], "entities": []}, {"text": "To avoid that, we introduce (Section 4) a novel parameter based sample selection (PBS) approach which aims to select a sample that enables good estimation of the model parameters, without focusing on difficult sentences.", "labels": [], "entities": []}, {"text": "In Section 5 we show that the methods derived from our approach select substantially fewer complex structures than active learning methods and the random baseline.", "labels": [], "entities": []}, {"text": "We propose two different methods.", "labels": [], "entities": []}, {"text": "In cluster based sampling (CBS), we aim to select a sample in which the distribution of the model parameters is similar to their distribution in the whole unlabelled pool.", "labels": [], "entities": [{"text": "cluster based sampling (CBS)", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6989308794339498}]}, {"text": "To do that we build a vector representation for each sentence in the unlabelled pool reflecting the distribution of the model parameters in this sentence, and use a clustering algorithm to divide these vectors into clusters.", "labels": [], "entities": []}, {"text": "In the second method we use the fact that a sample containing many examples of a certain parameter yields better estimation of this parameter.", "labels": [], "entities": []}, {"text": "If this parameter is crucial for model performance and the selection process does not harm the distribution of other parameters, then the selected sample is of high quality.", "labels": [], "entities": []}, {"text": "To select such a sample we introduce a reduction between this selection problem and a variant of the NP-hard multiset-multicover problem.", "labels": [], "entities": []}, {"text": "We call this problem the constrained multiset multicover (CMM) problem, and present an algorithm to approximate it.", "labels": [], "entities": []}, {"text": "We experiment (Section 5) with the WSJ PennTreebank and Collins' generative parser, as in previous work.", "labels": [], "entities": [{"text": "WSJ PennTreebank", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.84890416264534}, {"text": "generative parser", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7567504644393921}]}, {"text": "We show that PBS algorithms achieve good results in terms of both the traditional TC measure (significantly better than the random selection baseline and similar to the results of the state of the art tree entropy (TE) method of) and our novel cognitively driven measures (where PBS algorithms significantly outperform both TE and the random baseline).", "labels": [], "entities": []}, {"text": "We thus argue that PBS provides away to select a sample that imposes reduced cognitive load on the human annotator.", "labels": [], "entities": []}], "datasetContent": [{"text": "While the resources, capabilities and constraints of the human parser have been the subject of extensive research, different theories predict different aspects of its observed performance.", "labels": [], "entities": []}, {"text": "We focus on structures that are widely agreed to impose a high cognitive load on the human annotator and on theories considering the cognitive resources required in parsing a complete sentence.", "labels": [], "entities": [{"text": "parsing a complete sentence", "start_pos": 165, "end_pos": 192, "type": "TASK", "confidence": 0.8441268801689148}]}, {"text": "Based on these, we derive measures for the cognitive load on the human parser when syntactically annotating a set of sentences.", "labels": [], "entities": []}, {"text": "A nested structure is a parse tree node representing a constituent created while another constituent is still being processed ('open').", "labels": [], "entities": []}, {"text": "The degree K of a nested structure is the number of such open constituents.", "labels": [], "entities": []}, {"text": "In this paper, we enumerate the constituents in a top-down left-right order, and thus when a constituent is created, only its ancestors are processed 2 . A constituent is processed 2 A good review on node enumeration of the human parser in given in (.", "labels": [], "entities": []}, {"text": "until the processing of its children is completed.", "labels": [], "entities": []}, {"text": "For example, in, when the constituent NP3 is created, it starts a nested structure of degree 2, since two levels of its ancestors (VP, S) are still processed.", "labels": [], "entities": []}, {"text": "Its parent (VP) starts a nested structure of degree 1.", "labels": [], "entities": []}, {"text": "The difficulty of deeply nested structures for the human parser is well established in the psycholinguistics literature.", "labels": [], "entities": []}, {"text": "We review here some of the various explanations of this phenomenon; fora comprehensive review see.", "labels": [], "entities": []}, {"text": "According to the classical stack overflow theory ( and its extension, the incomplete syntactic/thematic dependencies theory, the human parser should track the open structures in its short term memory.", "labels": [], "entities": []}, {"text": "When the number of these structures is too large or when the structures are nested too deeply, the short term memory fails to hold them and the sentence becomes uninterpretable.", "labels": [], "entities": []}, {"text": "According to the perspective shifts theory, processing deeply nested structures requires multiple shifts of the annotator perspective and is thus more difficult than processing shallow structures.", "labels": [], "entities": []}, {"text": "The difficulty of deeply nested structured has been demonstrated for many languages.", "labels": [], "entities": []}, {"text": "We thus propose the total number of nested structures of degree K in a sample (TNSK) as a measure of the cognitive efforts that its annotation requires.", "labels": [], "entities": []}, {"text": "The higher K is, the more demanding the structure.", "labels": [], "entities": []}, {"text": "In the psycholinguistic literature of sentence processing there are many theories describing the cognitive resources required during a complete sentence processing.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7411904335021973}]}, {"text": "These resources might be allocated during the processing of a certain word and are needed long after its constituent is closed.", "labels": [], "entities": []}, {"text": "We briefly discuss two lines of theory, focusing on their predictions that sentences consisting of a large number of structures (e.g., con-stituents or nested structures) require more cognitive resources for longer periods.", "labels": [], "entities": []}, {"text": "Levelt suggested a layered model of the mental lexicon organization, arguing that when one hears or reads a sentence s/he activates word forms (lexemes) that in turn activate lemma information.", "labels": [], "entities": []}, {"text": "The lemma information contains information about syntactic properties of the word (e.g., whether it is a noun or a verb) and about the possible sentence structures that can be generated given that word.", "labels": [], "entities": []}, {"text": "The process of reading words and retrieving their lemma information is incremental and the lemma information fora given word is used until its syntactic structure is completed.", "labels": [], "entities": []}, {"text": "The information about a word include all syntactic predictions, obligatory (e.g., the prediction of a noun following a determiner) and optional (e.g., optional arguments of a verb, modifier relationships).", "labels": [], "entities": []}, {"text": "This information might be relevant long after the constituents containing the word are closed, sometimes till the end of the sentence.", "labels": [], "entities": []}, {"text": "Another line of research focuses on working memory, emphasizing the activation decay principle.", "labels": [], "entities": []}, {"text": "It stresses that words and structures perceived during sentence processing are forgotten overtime.", "labels": [], "entities": []}, {"text": "As the distance between two related structures in a sentence grows, it is more demanding to reactivate one when seeing the other.", "labels": [], "entities": []}, {"text": "Indeed, supported by a variety of observations, many of the theories of the human parser (see () fora survey) predict that processing items towards the end of longer sentences should be harder, since they most often have to be integrated with items further back.", "labels": [], "entities": []}, {"text": "Thus, sentences with a large number of structures impose a special cognitive load on the annotator.", "labels": [], "entities": []}, {"text": "We thus propose to use the number of structures (constituents or nested structures) in a sentence as a measure of its difficulty for human annotation.", "labels": [], "entities": []}, {"text": "The measures we use fora sample (a sentence set) are the average number of constituents (AC) and the average number of nested structures of degree k (ANSK) per sentence in the set.", "labels": [], "entities": [{"text": "ANSK", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.9046874046325684}]}, {"text": "Higher AC or ANSK values of a set imply higher annotation requirements . Pschycolinguistics research makes finer observa- The correlation between the number of constituents and sentence length is very strong (e.g., correlation coefficient of 0.93 in WSJ section 0).", "labels": [], "entities": [{"text": "ANSK", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9028138518333435}, {"text": "correlation coefficient", "start_pos": 215, "end_pos": 238, "type": "METRIC", "confidence": 0.9508906900882721}, {"text": "WSJ section 0", "start_pos": 250, "end_pos": 263, "type": "DATASET", "confidence": 0.9232755104700724}]}, {"text": "We could use the number of words, but we prefer the number of structures since the latter better reflects the arguments made in the literature.", "labels": [], "entities": []}, {"text": "tions about the human parser than those described here.", "labels": [], "entities": []}, {"text": "A complete survey of that literature is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "We consider the proposed measures a good approximation of some of the human parser characteristics.", "labels": [], "entities": []}], "tableCaptions": []}