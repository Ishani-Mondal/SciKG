{"title": [{"text": "Wiktionary and NLP: Improving synonymy networks", "labels": [], "entities": [{"text": "Wiktionary and NLP", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8107593258221945}, {"text": "Improving synonymy", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.855796605348587}]}], "abstractContent": [{"text": "Wiktionary, a satellite of the Wikipedia initiative, can be seen as a potential resource for Natural Language Processing.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9416487216949463}, {"text": "Natural Language Processing", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.6266693671544393}]}, {"text": "It requires however to be processed before being used efficiently as an NLP resource.", "labels": [], "entities": []}, {"text": "After describing the relevant aspects of Wiktionary for our purposes, we focus on its structural properties.", "labels": [], "entities": []}, {"text": "Then, we describe how we extracted synonymy networks from this resource.", "labels": [], "entities": []}, {"text": "We provide an in-depth study of these synonymy networks and compare them to those extracted from traditional resources.", "labels": [], "entities": []}, {"text": "Finally , we describe two methods for semi-automatically improving this network by adding missing relations: (i) using a kind of semantic proximity measure; (ii) using translation relations of Wiktionary itself.", "labels": [], "entities": []}, {"text": "Note: The experiments of this paper are based on Wik-tionary's dumps downloaded in year 2008.", "labels": [], "entities": [{"text": "Wik-tionary's dumps downloaded in year 2008", "start_pos": 49, "end_pos": 92, "type": "DATASET", "confidence": 0.9555503640856061}]}, {"text": "Differences maybe observed with the current versions available online.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reliable and comprehensive lexical resources constitute a crucial prerequisite for various NLP tasks.", "labels": [], "entities": []}, {"text": "However their building cost keeps them rare.", "labels": [], "entities": []}, {"text": "In this context, the success of the Princeton WordNet (PWN) can be explained by the quality of the resource but also by the lack of serious competitors.", "labels": [], "entities": [{"text": "Princeton WordNet (PWN)", "start_pos": 36, "end_pos": 59, "type": "DATASET", "confidence": 0.9423009157180786}]}, {"text": "Widening this observation to more languages only makes this observation more acute.", "labels": [], "entities": []}, {"text": "In spite of various initiatives, costs make resource development extremely slow or/and result in non freely accessible resources.", "labels": [], "entities": []}, {"text": "Collaborative resources might bring an attractive solution to this difficult situation.", "labels": [], "entities": []}, {"text": "Among them Wiktionary seems to be the perfect resource for building computational mono-lingual and multi-lingual lexica.", "labels": [], "entities": []}, {"text": "This paper focuses therefore on Wiktionary, how to improve it, and on its exploitation for creating resources.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.8871416449546814}]}, {"text": "In next section, we present some relevant information about Wiktionary.", "labels": [], "entities": []}, {"text": "Section 3 presents the lexical graphs we are using and the way we build them.", "labels": [], "entities": []}, {"text": "Then we pay some attention to evaluation ( \u00a74) before exploring some tracks of improvement suggested by Wiktionary structure itself.", "labels": [], "entities": []}], "datasetContent": [{"text": "Coverage and global SW analysis By comparing tables 2 and 3, one can observe that: \u2022 The lexical coverage of Wiktionary-based synonyms graphs is always quantitatively lower than those of standard resources although this may change.", "labels": [], "entities": [{"text": "SW analysis", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9436995685100555}]}, {"text": "For example, to horn (in PWN), absent from Wiktionary in 2008, appeared in 2009.", "labels": [], "entities": [{"text": "Wiktionary in 2008", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.8683648109436035}]}, {"text": "At last, Wiktionary is more inclined to include some class of words such as to poo (childish) or to prefetch, to google (technical neologisms).", "labels": [], "entities": []}, {"text": "\u2022 The average number of synonyms for an entry of a Wiktionary-based resource is smaller than those of standard resources.", "labels": [], "entities": []}, {"text": "For example, common synonyms such as to act/to play appear in PWN and not in Wiktionary.", "labels": [], "entities": [{"text": "PWN", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9372568130493164}]}, {"text": "Nevertheless, some other appear (rightly) in Wiktionary: to reduce/to decrease, to cook/to microwave.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.8115634322166443}]}, {"text": "\u2022 The clustering rate of Wiktionary-based graphs is always smaller than those of standard resources.", "labels": [], "entities": []}, {"text": "This is particularly the case for English.", "labels": [], "entities": []}, {"text": "However, this specificity might be due to differences between the resources themselves (Dicosyn vs. PWN) rather than structural differences at the linguistic level.", "labels": [], "entities": []}, {"text": "Evaluation of synonymy In order to evaluate the quality of extracted synonymy graphs from Wiktionary, we use recall and precision measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9993199110031128}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9991100430488586}]}, {"text": "The objects we compare are not simple sets but graphs (G = (V ; E)), thus we should compare separately set of vertices (V ) and set of edges (E).", "labels": [], "entities": []}, {"text": "Vertices are words and edges are synonymy links.", "labels": [], "entities": [{"text": "Vertices", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9464201331138611}]}, {"text": "Vertices evaluation leads to measure the resource  coverage whereas edges evaluation leads to measure the quality of the synonymy links in Wiktionary resource.", "labels": [], "entities": []}, {"text": "First of all, the global picture (table 4) shows clearly that the lexical coverage is rather poor.", "labels": [], "entities": []}, {"text": "A lot of words included in standard resources are not included yet in the corresponding wiktionary resources.", "labels": [], "entities": []}, {"text": "Overall the lexical coverage is always lower than 50%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8075668811798096}]}, {"text": "This has to be kept in mind while looking at the evaluation of relations shown in table 5.", "labels": [], "entities": []}, {"text": "To compute the relations evaluation, each resource has been first restricted to the links between words being present in each resource.", "labels": [], "entities": []}, {"text": "About PWN, since every link added with method A will also be added with method B, the precision of Wiktionary-based graphs synonyms links will be always lower for \"method A graphs\" than for \"method B graphs\".", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9989182949066162}]}, {"text": "Precision is rather good while recall is very low.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9893606901168823}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9997128844261169}]}, {"text": "That means that a lot of synonymy links of the standard resources are missing within Wiktionary.", "labels": [], "entities": []}, {"text": "As for Dicosyn, the picture is similar with even better precision but very low recall.", "labels": [], "entities": [{"text": "Dicosyn", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.93526691198349}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9990247488021851}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9989259839057922}]}], "tableCaptions": [{"text": " Table 1: Ratio of \"relevant\" articles in wiktionaries  By \"relevant\", we mean an article about a word  in the wiktionary's own language (e.g. not an  article about a French word in the English Wik- tionary). Among the \"relevant\" articles, some  are empty and some do not contain any transla- tion nor synonym link. Therefore, before deciding  to use Wiktionary, it is necessary to compare the  amount of extracted information contribution and  the amount of work required to obtain it .", "labels": [], "entities": []}, {"text": " Table 3: Gold standard's synonymy graphs properties", "labels": [], "entities": [{"text": "Gold standard", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9598744809627533}]}, {"text": " Table 5: Wiktionary synonymy links precision & recall", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9987053871154785}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9966703057289124}]}]}