{"title": [{"text": "QAST: Question Answering System for Thai Wikipedia", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7518901526927948}, {"text": "Thai Wikipedia", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.7454803884029388}]}], "abstractContent": [{"text": "We propose an open-domain question answering system using Thai Wikipedia as the knowledge base.", "labels": [], "entities": [{"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7499664425849915}, {"text": "Thai Wikipedia", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8928306102752686}]}, {"text": "Two types of information are used for answering a question: (1) structured information extracted and stored in the form of Resource Description Framework (RDF), and (2) un-structured texts stored as a search index.", "labels": [], "entities": []}, {"text": "For the structured information, SPARQL transformed query is applied to retrieve a short answer from the RDF base.", "labels": [], "entities": []}, {"text": "For the unstructured information, keyword-based query is used to retrieve the shortest text span containing the questions's key terms.", "labels": [], "entities": []}, {"text": "From the experimental results, the system which integrates both approaches could achieve an average MRR of 0.47 based on 215 test questions.", "labels": [], "entities": [{"text": "MRR", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9968876242637634}]}], "introductionContent": [{"text": "Most keyword-based search engines available online do not support the retrieval of precise information.", "labels": [], "entities": [{"text": "retrieval of precise information", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.7889983505010605}]}, {"text": "They only return a list of URLs, each referring to a web page, sorted by relevancy to the user's query.", "labels": [], "entities": []}, {"text": "Users then have to manually scan those documents for needed information.", "labels": [], "entities": []}, {"text": "Due to this limitation, many techniques for implementing QA systems have been proposed in the past decades.", "labels": [], "entities": []}, {"text": "From the literature reviews, previous and existing QA systems can be broadly categorized into two types: 1.", "labels": [], "entities": []}, {"text": "Knowledge Intensive: Knowledge intensive systems focus on analyzing and understanding the input questions.", "labels": [], "entities": [{"text": "Knowledge Intensive", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7230623066425323}]}, {"text": "The system knows exactly what to be answered, and also what type the answer should be.", "labels": [], "entities": []}, {"text": "The analysis phase usually depends on an ontology or a semantic lexicon like WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9610841870307922}]}, {"text": "The answer is retrieved from a predefined organized knowledge base.", "labels": [], "entities": []}, {"text": "Natural Language Processing (NLP) techniques are heavily used in a knowledge intensive system.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6791485001643499}]}, {"text": "2. Data Intensive: Data intensive systems, which do not fully analyze the input questions, rely on the redundancy of huge amount of data ().", "labels": [], "entities": []}, {"text": "The idea is that if we have a huge amount of data, apiece of information is likely to be stated more than once in different forms.", "labels": [], "entities": []}, {"text": "As a result, the dataintensive QA systems are not required to perform many complex NLP techniques.", "labels": [], "entities": []}, {"text": "In this paper, we propose an open-domain QA system for Thai Wikipedia called QAST.", "labels": [], "entities": [{"text": "Thai Wikipedia", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.8670246303081512}]}, {"text": "The system supports five types of close-ended questions: person, organization, place, quantity, and date/-time.", "labels": [], "entities": []}, {"text": "Our system can be classified as a data intensive type with an additional support of structured information.", "labels": [], "entities": []}, {"text": "Structured information in Thai Wikipedia is extracted and represented in the form of RDF.", "labels": [], "entities": [{"text": "Thai Wikipedia", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8127175569534302}]}, {"text": "We use SPARQL to retrieve specific information from the RDF base.", "labels": [], "entities": []}, {"text": "If using SPARQL cannot answer a given question, the system will retrieve answer candidates from the pre-constructed search index using a technique based on Minimal Span Weighting).", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the system, 215 test questions (43 questions for each question type) and their correct answers were constructed based on the contents of random articles in Thai Wikipedia.", "labels": [], "entities": [{"text": "Thai Wikipedia", "start_pos": 168, "end_pos": 182, "type": "DATASET", "confidence": 0.8972300291061401}]}, {"text": "Mean Reciprocal Rank (MRR), the official measurement used for QA systems in TREC, is used as the performance measurement.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.9654493530591329}]}, {"text": "To evaluate the system, a question is said to be correctly answered only when at least one of the produced five ranked candidates contained the true answer with the right context.", "labels": [], "entities": []}, {"text": "Out-of-context candidate phrases which happen to contain the true answers are not counted.", "labels": [], "entities": []}, {"text": "If there is no correct answer in any candidate, the score for that question is equal to zero.", "labels": [], "entities": []}, {"text": "shows a comparison of the MRR values when using both index and RDF, and using only the index.", "labels": [], "entities": [{"text": "MRR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6666872501373291}, {"text": "RDF", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.5763773918151855}]}, {"text": "The approach of using only the index, the overall MRR is equal to 0.39 which is fairly high with respect to the answer retrieval methodology.", "labels": [], "entities": [{"text": "MRR", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9646952748298645}, {"text": "answer retrieval", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.7928734123706818}]}, {"text": "The index search approach simply relies on the fact that if the question keywords in a ranked candidate document occur close together and at least one occurrence of expected answer pattern exists, then there is a high chance that the term span contains an answer.", "labels": [], "entities": [{"text": "index search", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7749935686588287}]}, {"text": "The MRR significantly increases to 0.47 (20.5% improvement) when RDF (structured information) is used together with the index.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9724377989768982}, {"text": "RDF", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9061281085014343}]}, {"text": "A thorough analysis showed that out of 215 questions, 21 questions triggered the RDF base.", "labels": [], "entities": [{"text": "RDF base", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.6857404708862305}]}, {"text": "Among these, 18 questions were correctly answered.", "labels": [], "entities": []}, {"text": "Therefore, using the additional structured information helps answer the definitional and factoid questions.", "labels": [], "entities": []}, {"text": "We expect a higher improvement when more structured information is incorporated into the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: QAST's performance comparison be- tween (1) using both index and RDF and (2) using  only the index.", "labels": [], "entities": [{"text": "QAST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.7363843321800232}]}]}