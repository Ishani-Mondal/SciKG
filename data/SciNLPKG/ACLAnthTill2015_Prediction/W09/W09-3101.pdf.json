{"title": [{"text": "Repetition and Language Models and Comparable Corpora", "labels": [], "entities": [{"text": "Repetition and Language Models", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8376308977603912}]}], "abstractContent": [{"text": "I will discuss a couple of non-standard features that I believe could be useful for working with comparable corpora.", "labels": [], "entities": []}, {"text": "Dotplots have been used in biology to find interesting DNA sequences.", "labels": [], "entities": []}, {"text": "Biology is interested in ordered matches, which show up as (possibly broken) diagonals in dot-plots.", "labels": [], "entities": []}, {"text": "Information Retrieval is more interested in unordered matches (e.g., cosine similarity), which show up as squares in dotplots.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6561080664396286}]}, {"text": "Parallel corpora have both squares and diagonals multiplexed together.", "labels": [], "entities": []}, {"text": "The diagonals tell us what is a translation of what, and the squares tell us what is in the same language.", "labels": [], "entities": []}, {"text": "I would expect dotplots of comparable corpora would contain lots of diagonals and squares, though the diagonals would be shorter and more subtle in comparable corpora than in parallel corpora.", "labels": [], "entities": []}, {"text": "There is also an opportunity to take advantage of repetition in comparable corpora.", "labels": [], "entities": [{"text": "repetition", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9760821461677551}]}, {"text": "Standard bag-of-word models in Information Retrieval do not attempt to model discourse structure such as given/new.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.797562301158905}]}, {"text": "The first mention in a news article (e.g., \"Manuel Noriega, former President of Panama\") is different from subsequent mentions (e.g., \"Noriega\").", "labels": [], "entities": []}, {"text": "Adaptive language models were introduced in Speech Recognition to capture the fact that probabilities change or adapt.", "labels": [], "entities": [{"text": "Speech Recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7939698100090027}]}, {"text": "After we seethe first mention, we should expect a subsequent mention.", "labels": [], "entities": []}, {"text": "If the first mention has probability p, then under standard (bag-of-words) independence assumptions, two mentions ought to have probability p 2 , but we find the probability is actually closer to p/2.", "labels": [], "entities": []}, {"text": "Adaptation matters more for meaningful units of text.", "labels": [], "entities": [{"text": "Adaptation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9145029187202454}]}, {"text": "In Japanese, words (meaningful sequences of characters) are more likely to be repeated than fragments (meaningless sequences of characters from words that happen to be adjacent).", "labels": [], "entities": []}, {"text": "In newswire, we find more adaptation for content words (proper nouns, technical terminology and good keywords for information retrieval), and less adaptation for function words, clich\u00e9s and ordinary first names.", "labels": [], "entities": []}, {"text": "There is more to meaning than frequency.", "labels": [], "entities": []}, {"text": "Content words are not only low frequency, but likely to be repeated.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}