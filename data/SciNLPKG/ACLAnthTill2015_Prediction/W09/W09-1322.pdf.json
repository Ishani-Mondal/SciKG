{"title": [{"text": "Evaluation of the Clinical Que Evaluation of the Clinical Que Evaluation of the Clinical Que Evaluation of the Clinical Ques s s stion Answering Presentation tion Answering Presentation tion Answering Presentation tion Answering Presentation", "labels": [], "entities": [{"text": "Clinical Ques s s stion Answering Presentation tion Answering Presentation tion Answering Presentation tion Answering Presentation", "start_pos": 111, "end_pos": 241, "type": "TASK", "confidence": 0.826451025903225}]}], "abstractContent": [{"text": "Question answering is different from information retrieval in that it attempts to answer questions by providing summaries from numerous retrieved documents rather than by simply providing a list of documents that requires users to do additional work.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9187232553958893}, {"text": "information retrieval", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7384503781795502}]}, {"text": "However, the quality of answers that question answering provides has not been investigated extensively, and the practical approach to presenting question answers still needs more study.", "labels": [], "entities": [{"text": "question answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8274436891078949}]}, {"text": "In addition to fac-toid answering using phrases or entities, most question answering systems use a sentence based approach for generating answers.", "labels": [], "entities": [{"text": "fac-toid answering using phrases or entities", "start_pos": 15, "end_pos": 59, "type": "TASK", "confidence": 0.7759858965873718}, {"text": "question answering", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7564910352230072}]}, {"text": "However, many sentences are often only meaningful or understandable in their context, and a passage-based presentation can often provide richer, more coherent context.", "labels": [], "entities": []}, {"text": "However, passage-based presentations may introduce additional noise that places greater burden on users.", "labels": [], "entities": []}, {"text": "In this study, we performed a quantitative evaluation on the two kinds of presentation produced by our online clinical question answering system, AskHERMES (http://www.AskHERMES.org).", "labels": [], "entities": []}, {"text": "The overall finding is that, although irrelevant context can hurt the quality of an answer, the passage-based approach is generally more effective in that it provides richer context and matching across sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering is different from information retrieval in that it attempts to answer questions by providing summaries from numerous retrieved documents rather than by simply providing a list of documents for preparing the user to do even more exploration.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9202570915222168}, {"text": "information retrieval", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7380919754505157}]}, {"text": "The presentation of answers to questions is a key factor in its efficiently meeting the information needs of information users.", "labels": [], "entities": []}, {"text": "While different systems have adopted a variety of approaches for presenting the results of question answering, the efficacy of the use of these different approaches in extracting, summarizing, and presenting results from the biomedical literature has not been adequately investigated.", "labels": [], "entities": [{"text": "presenting the results of question answering", "start_pos": 65, "end_pos": 109, "type": "TASK", "confidence": 0.6229149103164673}, {"text": "extracting, summarizing", "start_pos": 168, "end_pos": 191, "type": "TASK", "confidence": 0.691884974638621}]}, {"text": "In this paper, we compare the sentence-based approach and the passage-based approach by using our own system, AskHERMES, which is designed to retrieve passages of text from the biomedical literature in response toad hoc clinical questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate whether the passage-based presentation improves question answering, we plugged two different approaches into our real system by making use of either the passage-based or the sentencebased ranking and presentation unit constructor.", "labels": [], "entities": [{"text": "question answering", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8526648581027985}]}, {"text": "Both of them share the same document retrieval component, and they share the same ranking and clustering strategies.", "labels": [], "entities": []}, {"text": "In our system, we used a density-based passage retrieval strategy () and a sequence sensitive ranking strategy similar to ROUGE (F. Liu and Y.).", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7993025779724121}, {"text": "ROUGE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9906302094459534}]}, {"text": "An in-house query-oriented clustering algorithm was used to construct the order and structure of the final hierarchical presentation.", "labels": [], "entities": []}, {"text": "The difference between the two approaches is the unit for ranking and presentation.", "labels": [], "entities": []}, {"text": "A passage-based approach takes the passage as its primary unit, with each passage consisting of one or more sentences.", "labels": [], "entities": []}, {"text": "Those sentences in the passage are extracted from the adjacent matching sentences in the original article.", "labels": [], "entities": []}, {"text": "To evaluate the difference between the passagebased presentation and sentence-based presentation, we randomly selected 20 questions from 4,653 clinical questions.", "labels": [], "entities": []}, {"text": "A physician (Dr. John Ely) was shown the corresponding passage-based and sentence-based outputs of every question and was then asked to judge the relevance of the output and which output had the higher quality answer.", "labels": [], "entities": []}, {"text": "Because physicians have little time in clinical settings to be sifting through data, we presented only the top five units (sentences or passages) of output for every question.", "labels": [], "entities": []}, {"text": "For answer extraction, we built a hierarchical weighted-keyword grouping model (.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9310272634029388}]}, {"text": "More specifically, in using this model we group units based on the presence of expanded query-term categories: keywords, keyword synonyms, UMLS concepts, UMLS synonyms, and original words, and we then prioritize the groups based on their ranking.", "labels": [], "entities": []}, {"text": "For example, units that incorporate keywords are grouped into the first cluster, followed by the cluster of units that incorporate keyword synonyms, UMLS concepts, etc.", "labels": [], "entities": []}, {"text": "The units that appear synonymous are in the clusters with the same parent cluster.", "labels": [], "entities": []}, {"text": "shows an example of the top branch of the clusters for the question \"What is the dose of sporanox?\" in which the answers are organized by sporanox and dose as well as their synonyms.", "labels": [], "entities": []}, {"text": "We classify physician evaluations as being of the following four types and plot their distribution in: The question is considered difficult because it is patient-specific or unclear (that is, it is a poorly formed question), e.g., \"Multiple small ulcers on ankles and buttocks.", "labels": [], "entities": []}, {"text": "I sent him fora complete blood count (cbc) and blood sugar but I don't know what these are.\"", "labels": [], "entities": [{"text": "complete blood count (cbc)", "start_pos": 16, "end_pos": 42, "type": "METRIC", "confidence": 0.830064465602239}, {"text": "blood sugar", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.8971674144268036}]}, {"text": "The evaluation data is shown in.", "labels": [], "entities": []}, {"text": "In our study, the score range is set from 0 to 5 with the value 0 referring to answers that are totally irrelevant to the question and the value 5 meaning there is enough information to fully answer the question.", "labels": [], "entities": []}, {"text": "Our results show that the passage-based approach is better than the sentence-based approach (p-value < 0.05).", "labels": [], "entities": []}, {"text": "Through further analysis of the results, we found that 70% of the sentences yielded by the sentencebased approach did not answer the question at all (the score is zero), while this was true for only 40% of the output of the passage-based approach.", "labels": [], "entities": []}, {"text": "This indicates that the passage-based approach provides more evidence for answering questions by providing richer context and matching across sentences.", "labels": [], "entities": []}, {"text": "On the other hand, if the question was too general and included a plethora of detail and little focus, both approaches failed.", "labels": [], "entities": [{"text": "detail", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.96028071641922}]}, {"text": "For example, in the question \"One year and 10-month-old boy removed from his home because of parental neglect.", "labels": [], "entities": []}, {"text": "Caretaker says he often cries like he's in pain, possibly abdominal pain.", "labels": [], "entities": []}, {"text": "Not eating, just drinking liquids, not sleeping.", "labels": [], "entities": []}, {"text": "The big question with him: \"is it something physical or all adjustment disorder?\"\" there is a great deal of description of the boy, and a variety of common symptoms are also provided.", "labels": [], "entities": []}, {"text": "AskHERMES found a passage containing all of the following extracted words: \"availability, because, before, between, changes, children, decrease, disorder/disorders, drug, eating, going, increase, indications/reasons, intake, laboratory, level, may, often, one, patient/patients, physical, recommended, routinely, specific, still, symptom/symptoms, two, urine, used, women, treat/treated/treating/therapy/treatment/treatments, and work.\"", "labels": [], "entities": []}, {"text": "But since these words are so commonly used in a variety of scenarios, the output passage is off-topic.", "labels": [], "entities": []}, {"text": "For very simple questions, the sentence-based approach works well for providing answers in a very concise form.", "labels": [], "entities": []}, {"text": "For example, the question \"what is the dose of zyrtec fora 3-year-old?\" can be answered by the dosage amount for the target age group, and the query resulted in this answer: \"\u2026children of both sexes aged between 2 to 6 years with allergy rhinitis (AR) were included in this study, who were randomly selected to be treated with Zyrtec (Cetirizine 2 HCL) drops 5 mg daily for 3 weeks.\"", "labels": [], "entities": []}, {"text": "From a literal view, this looks like an answer to the question because it discusses the dosage of Zyrtec for the specific age group; however, it actually describes an experiment and does not necessarily provide the suggested dosage that the user is seeking.", "labels": [], "entities": []}, {"text": "This leads to an interesting problem for clinical question answering: how should experimental data be distinguished from suggestion data for recommended daily usage?", "labels": [], "entities": [{"text": "clinical question answering", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6356635689735413}]}, {"text": "People tend to ask for the best answer instead of the possible answers.", "labels": [], "entities": []}, {"text": "This is one of the main reasons why in, there is no perfect score (5).", "labels": [], "entities": [{"text": "perfect score", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9797503352165222}]}, {"text": "Our result looks similar to the conclusion of Lin et al (Jimmy), whose study on opendomain factoid question answering indicates a preference among users for the answer-inparagraph approach rather than the three other types of presentation: exact-answer (that is, answer entity), answer-in-sentence, and answer-in-document.", "labels": [], "entities": [{"text": "opendomain factoid question answering", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.5428527742624283}]}, {"text": "The results of both Lin's research and our own indicate the usefulness of context, but Lin's work focuses on how surrounding context helps users to understand and become confident in answers retrieved by simple open-domain queries, while our research reveals that adjacent sentences can improve the quality of answers retrieved using complex clinical questions.", "labels": [], "entities": []}, {"text": "Our results also indicate that context is important for relevance ranking, which has not been thoroughly investigated in previous research.", "labels": [], "entities": [{"text": "relevance ranking", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.9174931645393372}]}, {"text": "Furthermore, our work places emphasis on proper passage extraction from the document or paragraph because irrelevant context can also be a burden to users, especially for physicians who have limited time for reading through irrelevant text.", "labels": [], "entities": [{"text": "passage extraction from the document or paragraph", "start_pos": 48, "end_pos": 97, "type": "TASK", "confidence": 0.8502238477979388}]}, {"text": "Our continuous sentence-based passage extraction method works well for our study, but other approaches should be investigated to improve the passage-based approach.", "labels": [], "entities": [{"text": "sentence-based passage extraction", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.5836514333883921}]}, {"text": "With respect to the quality of the answer, the content of the output is not the only important issue.", "labels": [], "entities": []}, {"text": "Rather, the question itself and the organization of content are also important issues to consider.", "labels": [], "entities": []}, {"text": "proposed an iterative user interface to capture the information needs of users to form structured queries with the assistance of a knowledge base, and this kind of approach guides users toward a clearer and more formal representation of their questions.", "labels": [], "entities": []}, {"text": "DynaCat (Pratt and Fagan 2000) also uses a knowledgebased approach to organize search results.", "labels": [], "entities": [{"text": "DynaCat (Pratt and Fagan 2000)", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.814998882157462}]}, {"text": "Thus, applying domain-specific knowledge is promising for improving the quality of an answer, but the difficulty of the knowledge-based approach is that building and updating such knowledge bases is human labor intensive, and furthermore, a knowledge-based approach restricts the usage of the system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. In our  study, the score range is set from 0 to 5 with the  value 0 referring to answers that are totally irrele- vant to the question and the value 5 meaning there  is enough information to fully answer the question.", "labels": [], "entities": []}, {"text": " Table 1. Quantitative measurement of the answers  generated by both approaches to the 20 questions", "labels": [], "entities": []}]}