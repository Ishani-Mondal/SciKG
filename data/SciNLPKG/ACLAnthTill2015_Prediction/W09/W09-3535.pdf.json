{"title": [{"text": "Tag Confidence Measure for Semi-Automatically Updating Named Entity Recognition", "labels": [], "entities": [{"text": "Semi-Automatically Updating Named Entity Recognition", "start_pos": 27, "end_pos": 79, "type": "TASK", "confidence": 0.7158351898193359}]}], "abstractContent": [{"text": "We present two techniques to reduce machine learning cost, i.e., cost of manually annotating unlabeled data, for adapting existing CRF-based named entity recognition (NER) systems to new texts or domains.", "labels": [], "entities": [{"text": "CRF-based named entity recognition (NER)", "start_pos": 131, "end_pos": 171, "type": "TASK", "confidence": 0.7477740347385406}]}, {"text": "We introduce the tag posterior probability as the tag confidence measure of an individual NE tag determined by the base model.", "labels": [], "entities": [{"text": "tag posterior probability", "start_pos": 17, "end_pos": 42, "type": "METRIC", "confidence": 0.6215978562831879}]}, {"text": "Dubious tags are automatically detected as recognition errors, and regarded as targets of manual correction.", "labels": [], "entities": []}, {"text": "Compared to entire sentence posterior probability, tag posterior probability has the advantage of minimizing system cost by focusing on those parts of the sentence that require manual correction.", "labels": [], "entities": [{"text": "tag posterior probability", "start_pos": 51, "end_pos": 76, "type": "METRIC", "confidence": 0.7338432868321737}]}, {"text": "Using the tag confidence measure, the first technique, known as active learning, asks the editor to assign correct NE tags only to those parts that the base model could not assign tags confidently.", "labels": [], "entities": []}, {"text": "Active learning reduces the learning cost by 66%, compared to the conventional method.", "labels": [], "entities": []}, {"text": "As the second technique, we propose bootstrapping NER, which semi-automatically corrects dubious tags and updates its model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning, especially supervised learning, has achieved great success in many natural language tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and parsing.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 117, "end_pos": 145, "type": "TASK", "confidence": 0.6411922931671142}, {"text": "named entity recognition (NER)", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.7872448315223058}]}, {"text": "This approach automatically encodes linguistic knowledge as statistical parameters (models) from large annotated corpora.", "labels": [], "entities": []}, {"text": "In the NER task, which is the focus of this paper, sequential tagging based on statistical models is Tags are assigned to each input unit (e.g., word) one by one.", "labels": [], "entities": [{"text": "NER task", "start_pos": 7, "end_pos": 15, "type": "TASK", "confidence": 0.9353054463863373}, {"text": "sequential tagging", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7117476165294647}]}, {"text": "similarly used; studies include Conditional Random Fields (CRFs;).", "labels": [], "entities": []}, {"text": "However, the manual costs incurred in creating annotated corpora are extremely high.", "labels": [], "entities": []}, {"text": "On the other hand, Consumer Generated Media (CGM) such as blog texts has attracted a lot of attention recently as an informative resource for information retrieval and information extraction tasks.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.7552438378334045}, {"text": "information extraction", "start_pos": 168, "end_pos": 190, "type": "TASK", "confidence": 0.7650680840015411}]}, {"text": "CGM has two distinctive features; enormous quantities of new texts are generated day after day, and new vocabularies and topics come and go rapidly.", "labels": [], "entities": [{"text": "CGM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8142459988594055}]}, {"text": "The most effective approach to keep up with new linguistic phenomena is creating new annotated corpora for model re-training at short intervals.", "labels": [], "entities": []}, {"text": "However, it is difficult to build new corpora expeditiously because of the high manual costs imposed by traditional schemes.", "labels": [], "entities": []}, {"text": "To reduce the manual labor and costs, various learning methods, such as active learning, semi-supervised learning and bootstrapping) have been proposed.", "labels": [], "entities": []}, {"text": "Active learning automatically selects effective texts to be annotated from huge raw-text corpora.", "labels": [], "entities": []}, {"text": "The correct answers are then manually annotated, and the model is re-trained.", "labels": [], "entities": []}, {"text": "In active learning, one major issue is data selection, namely, determining which sample data is most effective.", "labels": [], "entities": [{"text": "data selection", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7073809653520584}]}, {"text": "The data units used in conventional methods are sentences.", "labels": [], "entities": []}, {"text": "Automatically creating annotated corpora would dramatically decrease the manual costs.", "labels": [], "entities": []}, {"text": "In fact, there always are some recognition errors in any automatically annotated corpus and the editor has to correct errors one by one.", "labels": [], "entities": []}, {"text": "Since sentences are used as data units, the editor has to pay attention to all tags in the selected sentence because it is not obvious where the recognition error is.", "labels": [], "entities": []}, {"text": "However, it is a waste of manual effort to annotate all tags because most tags must be labeled correctly by the base model 2 . In this paper, we propose a confidence measure based on tag posterior probability for the NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 217, "end_pos": 225, "type": "TASK", "confidence": 0.9296650886535645}]}, {"text": "Our method does not use the confidence of a sentence, but instead computes the confidence of the tag assigned to each word.", "labels": [], "entities": []}, {"text": "The tag confidence measure allows the sentence to which the base model might assign an incorrect tag to be selected automatically.", "labels": [], "entities": []}, {"text": "Active learning becomes more efficient because we correct only those tags that have low confidence (cf. Sec. 4).", "labels": [], "entities": [{"text": "Active learning", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7618641555309296}]}, {"text": "We can realize the same effect as active learning if we can automatically correct the selected data based upon our tag confidence measure.", "labels": [], "entities": [{"text": "tag confidence measure", "start_pos": 115, "end_pos": 137, "type": "METRIC", "confidence": 0.8270495136578878}]}, {"text": "Our proposal \"Semi-Automatically Updating NER\" automatically corrects erroneous data by using a seed NE list generated from other information sources.", "labels": [], "entities": [{"text": "Semi-Automatically Updating NER", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.5214433670043945}]}, {"text": "Semi-Automatically Updating NER easily keeps up with new words because it enables us to update the model simply by providing anew NE list (cf. Sec. 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our Updating NER with a large amount of blog texts from the WWW.", "labels": [], "entities": [{"text": "Updating NER", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.4684116989374161}, {"text": "WWW", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.8815639615058899}]}, {"text": "One week's worth of blog texts was crawled on the WWW to generate the additional data.", "labels": [], "entities": [{"text": "WWW", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.5097182393074036}]}, {"text": "shows the statistics of the data used in our experiments.", "labels": [], "entities": []}, {"text": "The test data contained only the blog texts generated in December 2006, and the base data is about a half year older than the test data.", "labels": [], "entities": []}, {"text": "Therefore, it is difficult for the base model to recognize new NEs in the test data.", "labels": [], "entities": []}, {"text": "One week's worth of December 2006 blog texts were prepared for bootstrapping.", "labels": [], "entities": []}, {"text": "The overlap between the test data and the additional data was removed in advance.", "labels": [], "entities": []}, {"text": "We set the rejecter's threshold at 0.5 and selected the data with tag graphs from the additional data.", "labels": [], "entities": []}, {"text": "Japanese Wikipedia entries were used as the seed NE list.", "labels": [], "entities": [{"text": "Japanese Wikipedia entries", "start_pos": 0, "end_pos": 26, "type": "DATASET", "confidence": 0.8917882045110067}, {"text": "seed NE list", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.7047479351361593}]}, {"text": "The titles of Wikipedia articles were regarded as surface forms.", "labels": [], "entities": []}, {"text": "NE types were estimated from the category sections of each article, based on heuristic rules prepared in advance.", "labels": [], "entities": []}, {"text": "We collected 104,296 entries as a seed NE list.", "labels": [], "entities": [{"text": "seed NE list", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.6633365352948507}]}, {"text": "Using this seed list, Updating NER extracted the seed NE and its context from the selected data automatically.", "labels": [], "entities": []}, {"text": "If the system found a match, it extracted the sentence with its tag sequence from the selected data.", "labels": [], "entities": []}, {"text": "The automatically corrected data was then merged with the base data in order to re-train the base model.", "labels": [], "entities": []}, {"text": "For comparison, we evaluated the effect of the seed NE list itself.", "labels": [], "entities": [{"text": "seed NE list", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8129626115163168}]}, {"text": "If there is a sequence of words that can be found in the seed list, then that sequence is always recognized as a NE.", "labels": [], "entities": []}, {"text": "Note that the other words are simply decoded using the base model.", "labels": [], "entities": []}, {"text": "We call this method 'user dictionary'.", "labels": [], "entities": []}, {"text": "Here, we use recall and precision to evaluate the accuracy of the model.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9995230436325073}, {"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9993833303451538}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992445707321167}]}, {"text": "shows the details of accuracy results regarding the following four NE types: PERSON, LOCATION, ORGANIZATION, and ARTI-FACT, which are referred to hereafter as PSN, LOC, ORG and ART, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9994136095046997}, {"text": "PERSON", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.989452600479126}, {"text": "LOCATION", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9137113690376282}, {"text": "ORGANIZATION", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9830859899520874}, {"text": "ARTI-FACT", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9820374250411987}, {"text": "ORG", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.9609954357147217}, {"text": "ART", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.7824064493179321}]}, {"text": "Although we added Wikipedia as a user dictionary to the base model, it only slightly improved the recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9992048144340515}]}, {"text": "In fact, it has no positive and sometimes a negative effect on precision (e.g., ART decreased from 0.666 to 0.619).", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9994840621948242}, {"text": "ART", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9991468191146851}]}, {"text": "This indicates that adding an NE list as a dictionary is not enough to improve the accuracy of a NER system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9990506768226624}]}, {"text": "This is because the NER system cannot discriminate an NE from surrounding unrelated words.", "labels": [], "entities": []}, {"text": "It simply extracts matched sequences of words, so it overestimates the number of NEs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. The Distribution of Replacement Types.", "labels": [], "entities": []}, {"text": " Table 6. Details of Accuracy.", "labels": [], "entities": [{"text": "Details", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.6146310567855835}, {"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9699843525886536}]}]}