{"title": [{"text": "Reordering Model Using Syntactic Information of a Source Tree for Statistical Machine Translation", "labels": [], "entities": [{"text": "Reordering", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9432203769683838}, {"text": "Statistical Machine Translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.8454131484031677}]}], "abstractContent": [{"text": "This paper presents a reordering model using syntactic information of a source tree for phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 88, "end_pos": 132, "type": "TASK", "confidence": 0.6457282304763794}]}, {"text": "The proposed model is an extension of IST-ITG (imposing source tree on inversion trans-duction grammar) constraints.", "labels": [], "entities": []}, {"text": "In the proposed method, the target-side word order is obtained by rotating nodes of the source-side parse-tree.", "labels": [], "entities": []}, {"text": "We modeled the node rotation, monotone or swap, using word alignments based on a training parallel corpus and source-side parse-trees.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.6941439062356949}]}, {"text": "The model efficiently suppresses erroneous target word orderings, especially global orderings.", "labels": [], "entities": []}, {"text": "Furthermore, the proposed method conducts a probabilistic evaluation of target word reorderings.", "labels": [], "entities": []}, {"text": "In English-to-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 199, "end_pos": 205, "type": "METRIC", "confidence": 0.9522354602813721}]}, {"text": "This indicates the validity of the proposed reordering model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation has been wiedely applied in many state-of-the-art translation systems.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.662968377272288}]}, {"text": "A popular statistical machine translation paradigms is the phrase-based model ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6267246107260386}]}, {"text": "In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 3, "end_pos": 47, "type": "TASK", "confidence": 0.5957358404994011}, {"text": "word reordering", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7088690549135208}]}, {"text": "To resolve this problem, many word-reordering constraint techniques have been proposed.", "labels": [], "entities": []}, {"text": "These techniques are categorized into two types.", "labels": [], "entities": []}, {"text": "The first type is linguistically syntaxbased.", "labels": [], "entities": []}, {"text": "In this approach, tree structures for the source), target), or both) are used for model training.", "labels": [], "entities": []}, {"text": "The second type is formal constraints on word permutations.", "labels": [], "entities": []}, {"text": "IBM constraints, the lexical word reordering model, and inversion transduction grammar (ITG) constraints belong to this type of approach.", "labels": [], "entities": []}, {"text": "For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree.", "labels": [], "entities": []}, {"text": "In these node rotations, the source binary tree instance is not considered.", "labels": [], "entities": []}, {"text": "Imposing a source tree on ITG (IST-ITG) constraints) is an extension of ITG constraints and a hybrid of the first and second type of approach.", "labels": [], "entities": []}, {"text": "IST-ITG constraints directly introduce a source sentence tree structure.", "labels": [], "entities": []}, {"text": "Therefore, IST-ITG can obtain stronger constraints for word reordering than the original ITG constraints.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7266921997070312}]}, {"text": "For example, IST-ITG constraints allows only eight word orderings fora four-word sentence, even though twenty-two word orderings are possible with respect to the original ITG constraints.", "labels": [], "entities": []}, {"text": "Although IST-ITG constraints efficiently suppress erroneous target word orderings, the method cannot assign the probability to the target word orderings.", "labels": [], "entities": [{"text": "IST-ITG", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.8241408467292786}]}, {"text": "This paper presents a reordering model using syntactic information of a source tree for phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 88, "end_pos": 132, "type": "TASK", "confidence": 0.6457282304763794}]}, {"text": "The proposed reordering model is an extension of IST-ITG con-straints.", "labels": [], "entities": []}, {"text": "In the proposed method, the target-side word order is obtained by rotating nodes of a sourceside parse-tree in a similar fashion to IST-ITG constraints.", "labels": [], "entities": []}, {"text": "We modeled the rotating positions, monotone or swap, from word alignments of a training parallel corpus and source-side parse-trees.", "labels": [], "entities": []}, {"text": "The proposed method conducts a probabilistic evaluation of target word orderings using syntactic information of the source tree.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the previous approach to resolving erroneous word reordering.", "labels": [], "entities": [{"text": "resolving erroneous word reordering", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.8686779141426086}]}, {"text": "In Section 3, the reordering model using syntactic information of a source tree is presented.", "labels": [], "entities": []}, {"text": "Section 4 shows experimental results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 presnts the summary and some concluding remarks and future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed model, we conducted two experiments: English-to-Japanese and English-toChinese translation.: Statistics of training, development and test corpus for E-J translation.", "labels": [], "entities": [{"text": "English-toChinese translation.", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.6367982923984528}, {"text": "E-J translation", "start_pos": 174, "end_pos": 189, "type": "TASK", "confidence": 0.7283649444580078}]}, {"text": "The first experiment was the English-to-Japanese (E-J) translation.", "labels": [], "entities": [{"text": "English-to-Japanese (E-J) translation", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.6060840487480164}]}, {"text": "shows the training, development and test corpus statistics.", "labels": [], "entities": []}, {"text": "JST JapaneseEnglish paper abstract corpus consists of 1.0M parallel sentences were used for model training.", "labels": [], "entities": [{"text": "JST JapaneseEnglish paper abstract corpus", "start_pos": 0, "end_pos": 41, "type": "DATASET", "confidence": 0.8909773588180542}]}, {"text": "This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of.", "labels": [], "entities": [{"text": "JapaneseEnglish paper abstract corpus", "start_pos": 38, "end_pos": 75, "type": "DATASET", "confidence": 0.9576283842325211}, {"text": "JST", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.6994965076446533}, {"text": "NICT", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.7931719422340393}]}, {"text": "For phrase-based translation model training, we used the GIZA++ toolkit, and 1.0M bilingual sentences.", "labels": [], "entities": [{"text": "phrase-based translation model training", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.8313222825527191}, {"text": "GIZA++ toolkit", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.8744205236434937}]}, {"text": "For language model training, we used the SRI language model toolkit, and 1.0M sentences for the translation model training.", "labels": [], "entities": [{"text": "language model training", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8342523574829102}, {"text": "SRI language model toolkit", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.8082201927900314}]}, {"text": "The language model type was word 5-gram smoothed by Kneser-Ney discounting.", "labels": [], "entities": []}, {"text": "To tune the decoder parameters, we conducted minimum error rate training) with respect to the word BLEU score () using 2.0K development sentence pairs.", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 45, "end_pos": 72, "type": "METRIC", "confidence": 0.8018222898244858}, {"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9572722613811493}]}, {"text": "The test set with 2.0K sentences is used.", "labels": [], "entities": []}, {"text": "In the evaluation and development sets, a single reference was used.", "labels": [], "entities": []}, {"text": "For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser).", "labels": [], "entities": [{"text": "English sentence parse trees", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6684904620051384}]}, {"text": "We used Chasen for segmentation of the Japanese sentences.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.967564582824707}]}, {"text": "For decoding, we used an in-house decoder that is a close relative of the Moses decoder.", "labels": [], "entities": []}, {"text": "The performance of this decoder was configured to be the same as Moses.", "labels": [], "entities": []}, {"text": "Other conditions were the same as the default conditions of the Moses decoder.", "labels": [], "entities": []}, {"text": "In this experiment, the following three methods were compared.", "labels": [], "entities": []}, {"text": "\u2022 IST-ITG : The IST-ITG constraints, the IBM constraints, and the lexical reordering model were used for target word reordering.", "labels": [], "entities": [{"text": "IST-ITG", "start_pos": 2, "end_pos": 9, "type": "DATASET", "confidence": 0.825924813747406}, {"text": "IBM constraints", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.7566262483596802}, {"text": "target word reordering", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.6071364084879557}]}, {"text": "\u2022 Proposed : The proposed reordering model, the IBM constraints, and the lexical reordering model were used for target word reordering.", "labels": [], "entities": [{"text": "IBM constraints", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8216732740402222}, {"text": "target word reordering", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.6061147749423981}]}, {"text": "During minimum error training, each method used each reordering model and reordering constraint.", "labels": [], "entities": []}, {"text": "The proposed reordering model are trained from 1.0M bilingual sentences for the translation model training.", "labels": [], "entities": []}, {"text": "The amount of available training samples represented by subtrees was 9.8M.", "labels": [], "entities": []}, {"text": "In the available training samples, there were 54K subtree types.", "labels": [], "entities": []}, {"text": "The heuristic threshold was 10, and subtrees with training samples of less than 10 were clustered.", "labels": [], "entities": []}, {"text": "The proposed reordering model consisted of 5,960 subtrees types and one clustered model \"other\".", "labels": [], "entities": []}, {"text": "The models not including \"other\" covered 99.29% of all training samples.", "labels": [], "entities": []}, {"text": "The BLEU scores are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.99654620885849}]}, {"text": "In comparing \"Baseline\" method with \"IST-ITG\" method, the improvement in BLEU was a 1.44-point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9993731379508972}]}, {"text": "Furthermore, in comparing \"IST-ITG\" method with \"Proposed\" method, the improvement in BLEU was a 0.49-point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9994062185287476}]}, {"text": "Both the IST-ITG constraints and the proposed reordering model fixed the phrase position for the global reorderings.", "labels": [], "entities": [{"text": "IST-ITG", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.7296831011772156}]}, {"text": "However, the proposed method can conduct a probabilistic evaluation of target word reorderings which the IST-ITG constraints cannot.", "labels": [], "entities": []}, {"text": "Therefore, \"Proposed\" method resulted in a better BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9968714118003845}]}, {"text": "Next, we conducted English-to-Chinese (E-C) newspaper translation experiments for different language pairs.", "labels": [], "entities": [{"text": "English-to-Chinese (E-C) newspaper translation", "start_pos": 19, "end_pos": 65, "type": "TASK", "confidence": 0.6148669918378195}]}, {"text": "The NIST MT08 evaluation campaign English-to-Chinese translation track was used for the training and evaluation corpora.", "labels": [], "entities": [{"text": "NIST MT08 evaluation campaign English-to-Chinese translation track", "start_pos": 4, "end_pos": 70, "type": "DATASET", "confidence": 0.8683628695351737}]}, {"text": "the training, development and test corpus statistics.", "labels": [], "entities": []}, {"text": "For the translation model training, we used 4.6M bilingual sentences.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9802846908569336}]}, {"text": "For the language model training, we used 4.6M sentences which are used for the translation model training.", "labels": [], "entities": []}, {"text": "The language model type was word 3-gram smoothed by Kneser-Ney discounting.", "labels": [], "entities": []}, {"text": "A development set with 1.6K sentences was used as evaluation data in the Chinese-toEnglish translation track for the NIST MT07 evaluation campaign.", "labels": [], "entities": [{"text": "NIST MT07 evaluation campaign", "start_pos": 117, "end_pos": 146, "type": "DATASET", "confidence": 0.756553128361702}]}, {"text": "A single reference was used in the development set.", "labels": [], "entities": []}, {"text": "The evaluation set with 1.9K sentences is the same as the MT08 evaluation data, with 4 references.", "labels": [], "entities": [{"text": "MT08 evaluation data", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9075258572896322}]}, {"text": "In this experiment, the compared methods were the same as in the E-J experiment.", "labels": [], "entities": []}, {"text": "The proposed reordering model are trained from 4.6M bilingual sentences for the translation model training.", "labels": [], "entities": []}, {"text": "The amount of available training samples represented by subtrees was 39.6M.", "labels": [], "entities": []}, {"text": "In the available training samples, there were 193K subtree types.", "labels": [], "entities": []}, {"text": "As in the E-J experiments, the heuristic threshold was 10.", "labels": [], "entities": [{"text": "heuristic threshold", "start_pos": 31, "end_pos": 50, "type": "METRIC", "confidence": 0.9587783217430115}]}, {"text": "The proposed reordering model consisted of 18,955 subtree types and one clustered model \"other.\"", "labels": [], "entities": []}, {"text": "The models not including \"other\" covered 99.45% of all training samples.", "labels": [], "entities": []}, {"text": "The BLEU scores are presented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.99654620885849}]}, {"text": "In comparing \"Baseline\" method with \"IST-ITG\" method, the improvement in BLEU was a 1.06-point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9994308352470398}]}, {"text": "In comparing \"IST-ITG\" method with \"Proposed\" method, the improvement in BLEU was a 0.33-point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.999430239200592}]}, {"text": "As in the E-J experiments, \"Proposed\" method performed the highest BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9992926120758057}]}, {"text": "We demon-strated that the proposed method is effective for multiple language pairs.", "labels": [], "entities": []}, {"text": "However, the improvement of BLEU score in E-C translation is smaller than the improvement in E-J translation, because English and Chinese are similar sentence structures, such as SVO-languages (Japanese is SOV-language).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9831646084785461}]}, {"text": "When the sentence structures are different, the proposed reordering model is effective.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of proposed reordering models.", "labels": [], "entities": []}, {"text": " Table 3: BLEU score results for E-J translation. (1- reference)", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9690364003181458}, {"text": "E-J translation", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.6728415191173553}]}, {"text": " Table 4: Statistics of training, development and test cor- pus for E-C translation.", "labels": [], "entities": [{"text": "E-C translation", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7796862721443176}]}, {"text": " Table 5: BLEU score results for E-C translation. (4- reference)", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9692281782627106}, {"text": "E-C translation", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.6460120528936386}]}]}