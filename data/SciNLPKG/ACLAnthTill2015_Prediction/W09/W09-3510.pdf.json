{"title": [], "abstractContent": [{"text": "The system presented in this paper uses phrase-based statistical machine translation (SMT) techniques to directly transliterate between all language pairs in this shared task.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 40, "end_pos": 90, "type": "TASK", "confidence": 0.7254427926880973}]}, {"text": "The technique makes no language specific assumptions , uses no dictionaries or explicit phonetic information.", "labels": [], "entities": []}, {"text": "The translation process transforms sequences of tokens in the source language directly into to sequences of tokens in the target.", "labels": [], "entities": []}, {"text": "All language pairs were transli-terated by applying this technique in a single unified manner.", "labels": [], "entities": []}, {"text": "The machine translation system used was a system comprised of two phrase-based SMT decoders.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7442647218704224}, {"text": "SMT decoders", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.8515450358390808}]}, {"text": "The first generated from the first token of the target to the last.", "labels": [], "entities": []}, {"text": "The second system generated the target from last to first.", "labels": [], "entities": []}, {"text": "Our results show that if only one of these decoding strategies is to be chosen , the optimal choice depends on the languages involved, and that in general a combination of the two approaches is able to outper-form either approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is possible to couch the task of machine transliteration as a task of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7153932452201843}]}, {"text": "Both processes involve the transformation of sequences of tokens in one language into sequences of tokens in another language.", "labels": [], "entities": []}, {"text": "The principle differences between the machine translation and language translation are: \u2022 Transliteration does not normally require the re-ordering of tokens that are generated in the target", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.719819501042366}, {"text": "language translation", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7028584033250809}]}], "datasetContent": [{"text": "In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa.", "labels": [], "entities": [{"text": "statistical machine translation decoder", "start_pos": 51, "end_pos": 90, "type": "TASK", "confidence": 0.7063236758112907}]}, {"text": "This decoder operates on exactly the same principles as the publicly available MOSES decoder (.", "labels": [], "entities": [{"text": "MOSES decoder", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.909878671169281}]}, {"text": "Like MOSES we utilize a future cost in our calculations.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 5, "end_pos": 10, "type": "DATASET", "confidence": 0.7167234420776367}]}, {"text": "Our decoder was modified to be able to run two instances of the decoder at the same One instance decoding from left-to-right the other decoding from right-to-left.", "labels": [], "entities": []}, {"text": "The hypotheses being combined by linearly interpolating the scores from both decoders at the end of the decoding process.", "labels": [], "entities": []}, {"text": "In addition, the decoders were constrained decode in a monotone manner.", "labels": [], "entities": []}, {"text": "That is, they were not allowed to re-order the phrases during decoding.", "labels": [], "entities": []}, {"text": "The decoders were also configured to produce a list of unique sequences of tokens in their n-best lists.", "labels": [], "entities": []}, {"text": "During SMT decoding it is possible to derive the same sequence of tokens in multiple ways.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.9558097124099731}]}, {"text": "Multiply occurring sequences of this form were combined into a single hypothesis in the n-best list by summing their scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The effect on 1-best accuracy by tuning with respect to BLEU score", "labels": [], "entities": [{"text": "1-best", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9403027296066284}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.894923746585846}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9990915060043335}]}, {"text": " Table 2: Results showing the peformance of three decoding strategies with respect to the evaluation  metrics used for the shared task. Here \ud97b\udf59 denotes left-to-right decoding, \ud97b\udf59 denotes right-to-left de- coding and \ud97b\udf59 denotes bidirectional decoding.", "labels": [], "entities": []}]}