{"title": [{"text": "Is sentence compression an NLG task?", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7969111502170563}]}], "abstractContent": [{"text": "Data-driven approaches to sentence compression define the task as dropping any subset of words from the input sentence while retaining important information and grammaticality.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7515323758125305}]}, {"text": "We show that only 16% of the observed compressed sentences in the domain of subtitling can be accounted for in this way.", "labels": [], "entities": []}, {"text": "We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatible with approximately 55% of the observed data.", "labels": [], "entities": []}, {"text": "We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7231254130601883}]}], "introductionContent": [{"text": "The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.723478689789772}, {"text": "sentence reduction)", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7667742073535919}, {"text": "summarizing a single sentence", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.8424490243196487}]}, {"text": "The compressed sentence should retain the most important information and remain grammatical.", "labels": [], "entities": []}, {"text": "One of the applications is in automatic summarization in order to compress sentences extracted for the summary).", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8807042837142944}]}, {"text": "Other applications include automatic subtitling) and displaying text on devices with very small screens.", "labels": [], "entities": []}, {"text": "A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7376395910978317}]}, {"text": "This formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in, and for virtually all follow-up work on data-driven sentence compression ( It makes two important assumptions: (1) only word deletions are allowed -no substitutions or insertions -and therefore no paraphrases; (2) the word order is fixed.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 169, "end_pos": 189, "type": "TASK", "confidence": 0.7453786432743073}]}, {"text": "In other words, the compressed sentence must be a subsequence of the source sentence.", "labels": [], "entities": []}, {"text": "We will call this the subsequence constraint, and refer to the corresponding compression models as word deletion models.", "labels": [], "entities": []}, {"text": "Another implicit assumption inmost work is that the scope of sentence compression is limited to isolated sentences and that the textual context is irrelevant.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7222602814435959}]}, {"text": "Under this definition, sentence compression is reduced to a word deletion task.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7698264718055725}]}, {"text": "Although one may argue that even this counts as a form of text-to-text generation, and consequently an NLG task, the generation component is virtually nonexistent.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7092646658420563}]}, {"text": "One can thus seriously doubt whether it really is an NLG task.", "labels": [], "entities": []}, {"text": "Things would become more interesting from an NLG perspective if we could show that sentence compression necessarily involves transformations beyond mere deletion of words, and that this requires linguistic knowledge and resources typical to NLG.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7256723642349243}]}, {"text": "The aim of this paper is therefore to challenge the deletion model and the underlying subsequence constraint.", "labels": [], "entities": []}, {"text": "To use an analogy, our aim is to show that sentence compression is less like carving something out of wood -where material can only be removed -and more like molding something out of clay -where the material can be thor-oughly reshaped.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7404920309782028}]}, {"text": "In support of this claim we provide evidence that the coverage of deletion models is in fact rather limited and that word reordering and paraphrasing play an important role.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7620178759098053}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce our text material which comes from the domain of subtitling.", "labels": [], "entities": []}, {"text": "We explain why not all material is equally well suited for studying sentence compression and motivate why we disregard certain parts of the data.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7159796357154846}]}, {"text": "We also describe the manual alignment procedure and the derivation of edit operations from it.", "labels": [], "entities": [{"text": "manual alignment", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.5993409901857376}]}, {"text": "In Section 3, an analysis of the number of deletions, insertions, substitutions, and reorderings in our data is presented.", "labels": [], "entities": []}, {"text": "We determine how many of the compressed sentences actually satisfy the subsequence constraint, and how many of them could in principle be accounted for.", "labels": [], "entities": []}, {"text": "That is, we consider alternatives with the same compression ratio which do not violate the subsequence constraint.", "labels": [], "entities": []}, {"text": "Next is an analysis of the remaining problematic cases in which violation of the subsequence constraint is crucial to accomplish the observed compression ratio.", "labels": [], "entities": []}, {"text": "We single out (1) reordering after deletion and (2) paraphrasing as important factors.", "labels": [], "entities": []}, {"text": "Given the importance of paraphrases, Section 3.4 discusses the perspectives for automatic extraction of paraphrase pairs from large text corpora, and tries to estimate how much text is required to obtain a reasonable coverage.", "labels": [], "entities": [{"text": "automatic extraction of paraphrase pairs from large text corpora", "start_pos": 80, "end_pos": 144, "type": "TASK", "confidence": 0.8394917845726013}]}, {"text": "We finish with a summary and discussion in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Degree of sentence alignment", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6678615659475327}]}, {"text": " Table 2: Properties of the final data set of  5233 pairs of autocue-subtitle sentences: mini- mum value, maximal value, total sum, mean and  standard deviation for number of tokens per au- tocue/subtitle sentence and Compression Ratio", "labels": [], "entities": [{"text": "Compression Ratio", "start_pos": 218, "end_pos": 235, "type": "METRIC", "confidence": 0.954003244638443}]}, {"text": " Table 3: Observed word deletions, insertions, sub- stitutions, and edit distances", "labels": [], "entities": [{"text": "Observed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9406611323356628}]}, {"text": " Table 4: Distribution of difference in tokens  between original non-subsequence subtitle and  equivalent subsequence subtitle", "labels": [], "entities": []}]}