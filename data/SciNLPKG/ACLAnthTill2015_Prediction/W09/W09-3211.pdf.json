{"title": [{"text": "A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions", "labels": [], "entities": [{"text": "Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions", "start_pos": 36, "end_pos": 116, "type": "TASK", "confidence": 0.8399425566196441}]}], "abstractContent": [{"text": "We present a graph-based model for representing the lexical cohesion of a discourse.", "labels": [], "entities": []}, {"text": "In the graph structure, vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically.", "labels": [], "entities": []}, {"text": "We show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiword expressions (MWEs) are defined as \"idiosyncratic interpretations that crossword boundaries or spaces\" ().", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7196852326393127}]}, {"text": "Such expressions are pervasive in natural language; they are estimated to be equivalent in number to simplex words in mental lexicon.", "labels": [], "entities": []}, {"text": "These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly.", "labels": [], "entities": []}, {"text": "Recognizing MWEs has been shown to be useful fora number of applications such as information retrieval ( and POS tagging (.", "labels": [], "entities": [{"text": "Recognizing MWEs", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.83393794298172}, {"text": "information retrieval", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.8697196841239929}, {"text": "POS tagging", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.8135952651500702}]}, {"text": "It has also been shown that MWEs account for 8% of parsing errors with precision grammars ().", "labels": [], "entities": []}, {"text": "Furthermore, MWE detection is used in information extraction and an integral component of symbolic MT systems (.", "labels": [], "entities": [{"text": "MWE detection", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9842158257961273}, {"text": "information extraction", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.8428791165351868}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.8731488585472107}]}, {"text": "However, the special properties of MWEs can also be exploited to recognize MWEs automatically.", "labels": [], "entities": []}, {"text": "There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on the table), extraction (recognizing MWEs as word units at the type level), detecting or measuring compositionality of MWEs, semantic interpretation (interpreting the semantic association among components in MWEs).", "labels": [], "entities": [{"text": "MWEs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.948331356048584}, {"text": "semantic interpretation", "start_pos": 305, "end_pos": 328, "type": "TASK", "confidence": 0.7326191514730453}]}, {"text": "To extract MWEs, various methods have been proposed that exploit the syntactic and lexical fixedness exhibited by MWEs, or apply various statistical measures across all co-occurrence vectors between the whole expression and its component parts (see Section 2).", "labels": [], "entities": []}, {"text": "These methods can be used to automatically identify potentially idiomatic expressions at a type level, but they do not say anything about the idiomaticity of an expression in a particular context.", "labels": [], "entities": []}, {"text": "While some idioms (e.g., ad hoc) are always used idiomatically, there are numerous others that can be used both idiomatically (see Example 1) and non-idiomatically (see Example 2).", "labels": [], "entities": []}, {"text": "(1) When the members of De la Guarda aren't hanging around, they're yelling and bouncing off the wall.", "labels": [], "entities": [{"text": "De la Guarda", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.7821049292882284}]}, {"text": "(2) Blinded by the sun, Erstad leaped at the wall, but the ball bounced off the wall well below his glove.", "labels": [], "entities": []}, {"text": "Our work aims to distinguish the literal and non-literal usages of idiomatic expressions in a discourse context (so-called token based classification).", "labels": [], "entities": [{"text": "token based classification", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.7117641965548197}]}, {"text": "It is therefore different from type-based approaches which aim to detect the general idiomaticity of an expression rather than its actual usage in a particular context.", "labels": [], "entities": []}, {"text": "We utilize the cohesive structure of a discourse to distinguish literal or non-literal usage of MWEs.", "labels": [], "entities": []}, {"text": "The basic idea is that the component words of an MWE contribute to the cohesion of the discourse in the literal case, while in the non-literal case they do not.", "labels": [], "entities": []}, {"text": "For instance, in the literal use of break the ice in Example 3, the content word ice contributes to the overall semantic connectivity of the whole sentence by the fact that ice is semantically related to water.", "labels": [], "entities": []}, {"text": "In contrast, in the non-literal example in 4, the word ice does not contribute to the overall cohesion as it is poorly connected to all the other (content) words in this specific context (play, party, games).", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our graph-based classifiers on a manually annotated data set, which we describe in Sec-tion 5.1.", "labels": [], "entities": []}, {"text": "We report on our experiments and results in Section 5.2.", "labels": [], "entities": [{"text": "Section 5.2", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.8657637238502502}]}], "tableCaptions": [{"text": " Table 2: Idiom statistics (* indicates expressions  for which the literal usage is more common than  the non-literal one)", "labels": [], "entities": []}, {"text": " Table 3: Accuracy (Acc.), literal precision  (LPrec.), recall (LRec.), and F-Score (LF \u03b2=1 ) for  the classifier", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986340403556824}, {"text": "Acc.", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8491670489311218}, {"text": "literal precision  (LPrec.)", "start_pos": 27, "end_pos": 54, "type": "METRIC", "confidence": 0.8483467936515808}, {"text": "recall (LRec.)", "start_pos": 56, "end_pos": 70, "type": "METRIC", "confidence": 0.9493183046579361}, {"text": "F-Score (LF \u03b2=1 )", "start_pos": 76, "end_pos": 93, "type": "METRIC", "confidence": 0.9025018981524876}]}]}