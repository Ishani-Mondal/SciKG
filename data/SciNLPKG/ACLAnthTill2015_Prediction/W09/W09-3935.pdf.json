{"title": [{"text": "Learning to Predict Engagement with a Spoken Dialog System in Open-World Settings", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a machine learning approach that allows an open-world spoken dialog system to learn to predict engagement intentions in situ, from interaction.", "labels": [], "entities": []}, {"text": "The proposed approach does not require any developer supervision, and leverages spatiotemporal and attentional features automatically extracted from a visual analysis of people coming into the proximity of the system to produce models that are attuned to the characteristics of the environment the system is placed in.", "labels": [], "entities": []}, {"text": "Experimental results indicate that a system using the proposed approach can learn to recognize engagement intentions at low false positive rates (e.g. 2-4%) up to 3-4 seconds prior to the actual moment of engagement.", "labels": [], "entities": []}], "introductionContent": [{"text": "We address the challenge of predicting the forthcoming engagement of people with open-world conversational systems, i.e. systems that operate in relatively unconstrained environments, where multiple participants might come and go, establish, maintain and break the communication frame, and simultaneously interact with a system and with others.", "labels": [], "entities": []}, {"text": "Examples of such systems include interactive billboards in a mall, robots in a home environment, intelligent home control systems, interactive systems that provide assistance and support during procedural tasks, etc.", "labels": [], "entities": []}, {"text": "In traditional closed-world dialog systems the engagement problem is generally resolved via simple, unambiguous signals.", "labels": [], "entities": []}, {"text": "For example, engagement is generally assumed once a phone call is answered by a telephony dialog system.", "labels": [], "entities": [{"text": "engagement", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9475703239440918}]}, {"text": "Similarly, a push-to-talk button can provide a clear engagement signal fora speech enabled mobile application.", "labels": [], "entities": []}, {"text": "These solutions are however inappropriate for systems that must operate continuously in open, dynamic environments, and engage with multiple people and groups overtime.", "labels": [], "entities": []}, {"text": "Such systems should ideally be ready to initiate dialog in a fluid, natural manner.", "labels": [], "entities": []}, {"text": "They should manage engagement with participants who are close by, and with those who are at a distance, with participants who have a standing plan to interact with a system, and with those whom opportunistically decide to engage, in-stream with their other ongoing activities.", "labels": [], "entities": []}, {"text": "In recognizing engagement intentions, such systems need to minimize false positives, while also minimizing the unnatural delays and discontinuities that come with false negatives about engagement intentions.", "labels": [], "entities": []}, {"text": "The work described in this paper is set in the larger context of a computational model for supporting fluid engagement in open-world dialog systems that we have previously described in (.", "labels": [], "entities": []}, {"text": "The above mentioned model harnesses components for sensing the engagement state, actions, and intentions of multiple participants in the scene, for making engagement control decisions, and for rendering these decisions into coordinated low-level behaviors, such as the changing pose and expressions of the face of an embodied agent.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the sensing subcomponent of this larger model and describe an approach for automatically learning to detect engagement intentions from interaction.", "labels": [], "entities": []}], "datasetContent": [{"text": "To provide an ecologically valid basis for data collection and for evaluating the proposed approach, we developed a situated conversational agent and deployed it in the real-world.", "labels": [], "entities": [{"text": "data collection", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7732861042022705}]}, {"text": "The system, illustrated in, is an interactive multimodal kiosk that displays a realistically rendered avatar head.", "labels": [], "entities": []}, {"text": "The avatar can engage and interact via natural language with one or more participants, and plays a simple game in which the users have to respond to multiple-choice trivia questions.", "labels": [], "entities": []}, {"text": "The system, and sample interactions are described in more detail in.)", "labels": [], "entities": []}, {"text": "The hardware and software architecture is also illustrated in.", "labels": [], "entities": []}, {"text": "Data gathered from a wide-angle camera, a 4-element linear microphone array, and a 19\" touch-screen is forwarded to a scene analysis module that fuses the incoming streams and constructs in realtime a coherent picture of the dynamics in the surrounding environment.", "labels": [], "entities": []}, {"text": "The system detects and tracks the location of multiple agents in the scene, tracks the head pose for engaged agents, and infers the focus of attention, activities, goals and (group) relationships among different agents in the scene.", "labels": [], "entities": []}, {"text": "An in-depth description of these scene analysis components falls beyond the scope of this paper, but more details are available in).", "labels": [], "entities": []}, {"text": "The scene analysis results are forwarded to the control level, which is structured in a twolayer reactive-deliberative architecture.", "labels": [], "entities": []}, {"text": "The reactive layer implements and coordinates low-level behaviors, including engagement, conversational floor management and turn-taking, and coordinating spoken and gestural outputs.", "labels": [], "entities": [{"text": "conversational floor management", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.6819828748703003}]}, {"text": "The deliberative layer plans the system's dialog moves and high-level engagement actions.", "labels": [], "entities": []}, {"text": "We deployed the system described above in an openspace near the kitchenette area in our building.", "labels": [], "entities": []}, {"text": "As we were interested in exploring the influence of the spatial setup on the engagement models, we deployed the system in two different spatial orientations, illustrated together with the resulting visual fields of view in.", "labels": [], "entities": []}, {"text": "Even though the location is similar, the two orientations create considerable differences in the relative trajectories of people that go by (dashed lines) and people that engage with the system (continuous lines).", "labels": [], "entities": []}, {"text": "In the side orientation, people typically enter the system's field of view and approach it from the sides.", "labels": [], "entities": []}, {"text": "In the front orientation, people enter the field of view and approach either frontally, or from the immediate right side.", "labels": [], "entities": []}, {"text": "We trained and evaluated (using a 10-fold crossvalidation process) a set of models for each of the two system orientations shown in and for each of the 5 feature subsets shown in.", "labels": [], "entities": []}, {"text": "The results on the per-frame classification task, including the ROC curves for the different models are presented and discussed in more detail in Appendix A. At runtime, the system uses these frame-based models to predict across time the likelihood that a given agent intends to engage (see).", "labels": [], "entities": [{"text": "per-frame classification task", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.786499043305715}, {"text": "ROC", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9807208776473999}]}, {"text": "In this context, an evaluation that counts the errors per person (i.e., per trace), rather than errors per frame is more informative.", "labels": [], "entities": []}, {"text": "Furthermore, since early detection is important for supporting a natural engagement process, an informative evaluation should also capture how soon a model can detect a positive engagement intention (see.", "labels": [], "entities": []}, {"text": "Making decisions about an agent's engagement intentions typically involves comparing the probability of engagement against a preset threshold.", "labels": [], "entities": []}, {"text": "Given a threshold, we can compute for each model the number of false-positives at the trace level: if the prediction exceeds the threshold at any point in the trace, we consider that a positive detection.", "labels": [], "entities": []}, {"text": "We note that, if we aim to detect people who will actually engage, there are no false negatives at the trace level.", "labels": [], "entities": []}, {"text": "The system can use the machine learned models in conjunction with the previous heuristic (a user is detected standing in front of the system), to eventually detect when people engage.", "labels": [], "entities": []}, {"text": "Also, given a threshold, we can identify how early a model can correctly detect the intention to engage (compared to the existing F-formation heuristic that defined the moment of engagement in the training data).", "labels": [], "entities": []}, {"text": "These durations are illustrated fora threshold of 0.5 in, and are referred to in the sequel as early detection time.", "labels": [], "entities": [{"text": "durations", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9660607576370239}, {"text": "early detection time", "start_pos": 95, "end_pos": 115, "type": "METRIC", "confidence": 0.8635966976483663}]}, {"text": "By varying the threshold between 0 and 1, we can obtain a profile that links the false-positive rate at the trace level to how early the system can detect engagement, i.e. to the mean early detection time.", "labels": [], "entities": []}, {"text": "shows the false-positive rate as a function of the mean early detection time for models trained using each of the five feature subsets shown in, in the side orientation.", "labels": [], "entities": []}, {"text": "The model that uses only location information (including the size of the face and proximity) performs worst.", "labels": [], "entities": []}, {"text": "Adding automatically extracted information about attention leads only to a marginal improvement.", "labels": [], "entities": []}, {"text": "However, adding information about the tra-    jectory of location and of attention, leads to larger cumulative gains.", "labels": [], "entities": []}, {"text": "Adding the more accurate (manually tagged) information about attention yields the best model.", "labels": [], "entities": []}, {"text": "The relative performance of these models (which can be observed at the frame-level in Appendix A) confirms our expectations and the importance of trajectory features (both spatial and attentional) in detecting engagement intentions.", "labels": [], "entities": []}, {"text": "The results also indicate that the differences, and hence the importance of these features, are larger when trying to detect engagement early on, i.e. at larger early detection times.", "labels": [], "entities": []}, {"text": "For instance, when detecting engagement intentions at a mean early detection above 3 seconds, the model that uses trajectory information, traj(loc+ff), decreases the false positive rate by a factor of 3 compared to the location-only model. and show the results for the front orientation.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 166, "end_pos": 185, "type": "METRIC", "confidence": 0.8238583008448283}]}, {"text": "The relative trends are similar to those observed in the side orientation, highlighting again the importance of trajectory features.", "labels": [], "entities": []}, {"text": "At the same time, the models are performing slightly worse in absolute terms, which is consistent with the increased difficulty of the task.", "labels": [], "entities": []}, {"text": "Several contributing factors can be identified in: people may simply pass by in closer proximity to the system; people who come from the corridor are generally frontally oriented towards the system, making frontal face cues less informative; and finally, people who will engage need to deviate less from the regular trajectory of people who are just passing by.", "labels": [], "entities": []}, {"text": "Next, we review how well the models trained generalize across the two different setups, by evaluating the trajectory models traj(loc+ff) across the two datasets.", "labels": [], "entities": []}, {"text": "The results indicate that the models are attuned to the dataset they are trained on (see.", "labels": [], "entities": []}, {"text": "As we discussed earlier, we expect this result given the different geometry of the relative trajectories of engagement in the two orientations.", "labels": [], "entities": []}, {"text": "These results highlight the importance of learning in situ, and show that the proposed approach can be used to learn the specific patterns of engagement in a given environment automatically, without explicit developer supervision.", "labels": [], "entities": []}, {"text": "Finally, we performed an error analysis.", "labels": [], "entities": [{"text": "error", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9461549520492554}]}, {"text": "We focused on the side orientation and visually inspected the 79 (4%) false-positive errors committed by the traj(loc+ff) show statistically significant improvements in performance (p<0.05) over the corresponding model that uses the immediately previous feature set (e.g. the cell right above).", "labels": [], "entities": []}, {"text": "The traj(loc), traj(loc+ff), traj(loc+attn) always statistically significantly (p<0.05) improve upon the loc models model when using a threshold corresponding to a mean early detection time of 3 seconds.", "labels": [], "entities": []}, {"text": "This analysis indicates that in 22 out of these 79 errors (28%) the person did actually exhibit behaviors consistent with an intention to engage the system, such as stopping by or turning around after passing the system, and approaching and maintaining sustained attention fora significant amount of time.", "labels": [], "entities": []}, {"text": "These cases represent false-negatives committed by our conservative F-formation heuristic with respect to engagement intention; the user did not approach close enough for the system to trigger engagement.", "labels": [], "entities": []}, {"text": "The actual false-positive rate of the trained model is therefore 2.9% rather than 4%.", "labels": [], "entities": []}, {"text": "The system was able to correctly identify these cases because the behavioral patterns are similar to the ones exhibited by people who did approach close enough for the heuristic detector to fire.", "labels": [], "entities": []}, {"text": "We plan to assess the false-negative rate of the current heuristic more closely and explore how many false negatives are actually recovered by the trained model.", "labels": [], "entities": []}, {"text": "This analysis will require that multiple judges assess engagement intentions on all 3274 traces.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. The results on  the per-frame classification task, including the ROC", "labels": [], "entities": [{"text": "per-frame classification task", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.7352415124575297}, {"text": "ROC", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.7530690431594849}]}, {"text": " Table 2. Feature sets for detecting engagement intention.", "labels": [], "entities": [{"text": "detecting engagement intention", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.8583229184150696}]}, {"text": " Table 3. *False-positive rate at different EDT (side)  Table 5. *False-positive rate at different EDT (front)", "labels": [], "entities": [{"text": "False-positive rate", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.9862501919269562}, {"text": "EDT", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.6512864828109741}, {"text": "False-positive rate", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.9849788844585419}, {"text": "EDT", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.8733090758323669}]}, {"text": " Table 4.*Early detection times at different FP rates (side).  Table 6 * Early detection times at different FP rates (front).", "labels": [], "entities": [{"text": "FP", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.8740294575691223}]}, {"text": " Table 1. Baseline, training-set and cross-validation  performance (data average log-likelihood and classifi- cation error) for side orientation models", "labels": [], "entities": []}, {"text": " Table 2. Baseline, training-set and cross-validation  performance (data average log-likelihood and classifi- cation error) for front orientation models", "labels": [], "entities": []}]}