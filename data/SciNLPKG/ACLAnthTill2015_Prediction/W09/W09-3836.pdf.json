{"title": [{"text": "Using Treebanking Discriminants as Parse Disambiguation Features", "labels": [], "entities": [{"text": "Parse Disambiguation", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7532081305980682}]}], "abstractContent": [{"text": "This paper presents a novel approach of incorporating fine-grained treebanking decisions made by human annotators as dis-criminative features for automatic parse disambiguation.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.7747954726219177}]}, {"text": "To our best knowledge, this is the first work that exploits treebank-ing decisions for this task.", "labels": [], "entities": []}, {"text": "The advantage of this approach is that use of human judgements is made.", "labels": [], "entities": []}, {"text": "The paper presents comparative analyses of the performance of discriminative models built using tree-banking decisions and state-of-the-art features.", "labels": [], "entities": []}, {"text": "We also highlight how differently these features scale when these models are tested on out-of-domain data.", "labels": [], "entities": []}, {"text": "We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features .", "labels": [], "entities": []}], "introductionContent": [{"text": "State-of-the-art parse disambiguation models are trained on treebanks, which are either fully handannotated or manually disambiguated from the parse forest produced by the parser.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.9379474222660065}]}, {"text": "While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses.", "labels": [], "entities": []}, {"text": "Some treebanking environments (such as the SRI Cambridge TreeBanker or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation.", "labels": [], "entities": [{"text": "SRI Cambridge TreeBanker", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.8496963381767273}]}, {"text": "These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available).", "labels": [], "entities": [{"text": "dynamic propagation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.775825023651123}]}, {"text": "But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.9150491654872894}]}, {"text": "Previous research has adopted two approaches to use treebanks for disambiguation models.", "labels": [], "entities": []}, {"text": "One approach, known as generative, uses only the gold parse trees).", "labels": [], "entities": [{"text": "generative", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.9820070862770081}]}, {"text": "The other approach, known as discriminative, uses both preferred trees and non-preferred trees).", "labels": [], "entities": []}, {"text": "In this latter approach, features such as local configurations (i.e., local sub-trees), grandparents, ngrams, etc., are extracted from all the trees and are utilized to build the model.", "labels": [], "entities": []}, {"text": "Neither of the approaches considers cognitive aspects of treebanking, i.e. the fine-grained decision-making process of the human annotators.", "labels": [], "entities": []}, {"text": "In this paper, we present our ongoing study of using treebanking decisions for building a parse disambiguation model.", "labels": [], "entities": [{"text": "parse disambiguation", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8558491766452789}]}, {"text": "We present comparative analyses among the features extracted using treebanking decisions and the state-of-the-art feature types.", "labels": [], "entities": []}, {"text": "We highlight how differently these features scale when they are tested on out-of-domain data.", "labels": [], "entities": []}, {"text": "Our results demonstrate that features extracted using treebanking decisions are more efficient, informative and robust, despite the total number of these features being much less than that of the traditional feature types.", "labels": [], "entities": []}, {"text": "The rest of this paper is organised as follows -section 2 presents some motivation along with definition of treebanking decisions.", "labels": [], "entities": []}, {"text": "Section 3 describes the feature extraction templates that have been used for treebanking decisions.", "labels": [], "entities": []}, {"text": "Section 4 explains the experimental data, results and analyses.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper with an outline of our future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiments is to compare various types of features (with TDF) in terms of efficiency, informativeness, and robustness.", "labels": [], "entities": []}, {"text": "To compare among the feature types, we build loglinear training models) for parse selection (which is standard for unificationbased grammars) for TDFC, local configurations, n-grams and active edges . For each model, we calculate the following evaluation metrics - \u2022 Exact (match) accuracy: it is simply the percentage of times that the top-ranked analysis for each test sentences is identical with the gold analysis of the same sentence.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.9007245004177094}, {"text": "Exact (match) accuracy", "start_pos": 267, "end_pos": 289, "type": "METRIC", "confidence": 0.8113820075988769}]}, {"text": "\u2022 5-best (match) accuracy: it is the percentage of times that the five top-ranked analyses for each of the sentences contain the gold analysis.", "labels": [], "entities": [{"text": "5-best (match)", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.6783025339245796}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.6199173927307129}]}, {"text": "\u2022 Feature Hit Count (FHC): it is the total number of occurrences of the features (of a particular feature type) inside all the syntactic analyses for all the test sentences.", "labels": [], "entities": [{"text": "Feature Hit Count (FHC)", "start_pos": 2, "end_pos": 25, "type": "METRIC", "confidence": 0.7861084391673406}]}, {"text": "So, for example, if a feature (of a particular feature type) is observed 100 times, then these 100 occurrences are added to the total FHC.", "labels": [], "entities": [{"text": "FHC", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.7172732949256897}]}, {"text": "\u2022 Feature Type Hit Count (FTHC): it is the total number of distinct features (of the corresponding feature type) observed inside the syntactic analyses of all the test sentences.", "labels": [], "entities": [{"text": "Feature Type Hit Count (FTHC)", "start_pos": 2, "end_pos": 31, "type": "METRIC", "confidence": 0.7521199073110308}]}, {"text": "While exact and 5-best match measures show relative informativeness and robustness of the feature types, FHC and FTHC provide a more comprehensive picture of relative efficiencies.", "labels": [], "entities": [{"text": "FHC", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.6084292531013489}, {"text": "FTHC", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.6618173122406006}]}], "tableCaptions": [{"text": " Table 1: Accuracies obtained on both in-domain and out-of-domain data using n-grams (n=4), local  configurations (with grandparenting level 3), active edges and TDFC.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9760812520980835}, {"text": "TDFC", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.820527195930481}]}, {"text": " Table 2: FHC and FTHC calculated for in-domain  data.", "labels": [], "entities": [{"text": "FHC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.5763292908668518}, {"text": "FTHC", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8077213764190674}]}]}