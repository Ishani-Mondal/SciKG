{"title": [{"text": "Discriminative Models for Semi-Supervised Natural Language Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "1 Discriminative vs. Generative Models An interesting question surrounding semi-supervised learning for NLP is: should we use discriminative models or generative models?", "labels": [], "entities": []}, {"text": "Despite the fact that generative models have been frequently employed in a semi-supervised setting since the early days of the statistical revolution in NLP, we advocate the use of discriminative models.", "labels": [], "entities": []}, {"text": "The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their gen-erative counterparts.", "labels": [], "entities": []}, {"text": "Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006).", "labels": [], "entities": [{"text": "letter-to-phoneme conversion", "start_pos": 224, "end_pos": 252, "type": "TASK", "confidence": 0.7558218538761139}, {"text": "semantic role labeling", "start_pos": 282, "end_pos": 304, "type": "TASK", "confidence": 0.6974207758903503}, {"text": "syntactic parsing", "start_pos": 331, "end_pos": 348, "type": "TASK", "confidence": 0.8110056817531586}, {"text": "language modeling", "start_pos": 372, "end_pos": 389, "type": "TASK", "confidence": 0.7936363816261292}, {"text": "machine translation", "start_pos": 416, "end_pos": 435, "type": "TASK", "confidence": 0.8016146719455719}]}, {"text": "While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a \"no prior\", agnostic learning setting.", "labels": [], "entities": []}, {"text": "See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models.", "labels": [], "entities": [{"text": "generative", "start_pos": 76, "end_pos": 86, "type": "TASK", "confidence": 0.9670274257659912}]}, {"text": "A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, co-training (Blum and Mitchell, 1998), and transduc-tive SVM (Joachims, 1999).", "labels": [], "entities": []}, {"text": "However, none of them seems to outperform the others across different domains , and each has its pros and cons.", "labels": [], "entities": []}, {"text": "Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with the label of each data point, for instance, by placing more weight on the (perfectly labeled) seeds than on the (presumably noisily labeled) bootstrapped data during the learning process.", "labels": [], "entities": []}, {"text": "Co-training is a natural choice if the data possesses two independent, redundant feature splits.", "labels": [], "entities": []}, {"text": "However, this conditional independence assumption is a fairly strict assumption and can rarely be satisfied in practice; worse still, it is typically not easy to determine the extent to which a dataset satisfies this assumption.", "labels": [], "entities": []}, {"text": "Transductive SVM tends to learn better max-margin hyperplanes with the use of unlabeled data, but its optimization procedure is non-trivial and its performance tends to deteriorate if a sufficiently large amount of unlabeled data is used.", "labels": [], "entities": [{"text": "Transductive SVM", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5963934063911438}]}, {"text": "Recently, Brefeld and Scheffer (2004) have proposed anew semi-supervised learning technique, EM-SVM, which is interesting in that it incorporates a discriminative model in an EM setting.", "labels": [], "entities": []}, {"text": "Unlike self-training, EM-SVM takes into account the confidence of the new labels, ensuring that the instances that are labeled with less confidence by the SVM have less impact on the training process than the confidently-labeled instances.", "labels": [], "entities": []}, {"text": "So far, EM-SVM has been tested on text classification problems, out-performing transductive SVM.", "labels": [], "entities": [{"text": "text classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.840716689825058}]}, {"text": "It would be interesting to see whether EM-SVM can beat existing semi-supervised learners for other NLP tasks.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}