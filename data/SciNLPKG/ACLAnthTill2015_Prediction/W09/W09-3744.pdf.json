{"title": [{"text": "Semantic Normalisation : a Framework and an Experiment", "labels": [], "entities": [{"text": "Semantic Normalisation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8971304893493652}]}], "abstractContent": [{"text": "We present a normalisation framework for linguistic representations and illustrate its use by normalising the Stanford Dependency graphs (SDs) produced by the Stanford parser into Labelled Stanford Dependency graphs (LSDs).", "labels": [], "entities": []}, {"text": "The normalised representations are evaluated both on a testsuite of constructed examples and on free text.", "labels": [], "entities": []}, {"text": "The resulting representations improve on standard Predicate/Argument structures produced by SRL by combining role labelling with the semantically oriented features of SDs.", "labels": [], "entities": []}, {"text": "Furthermore, the proposed normalisa-tion framework opens the way to stronger normalisation processes which should be useful in reducing the burden on inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "In automated text understanding, there is a tradeoff between the degree of abstraction provided by the semantic representations used and the complexity of the logical or probabilistic reasoning involved.", "labels": [], "entities": [{"text": "automated text understanding", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6032204528649648}]}, {"text": "Thus, a system that normalises syntactic passives as actives avoids having to reason about equivalences between grammatical dependencies.", "labels": [], "entities": []}, {"text": "Similarly, normalising phrasal synonyms into their one word equivalent (e.g., take a turn for the worse/worsen) or converting the semantic representation of deverbal nominals into their equivalent verbal representations (Caesar's destruction of the city/Caesar destroyed the city) avoids having to reason with the corresponding lexical axioms.", "labels": [], "entities": []}, {"text": "In short, the better, semantic representations abstract away from semantically irrelevant distinctions, the less reasoning needs to be performed.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a normalisation approach and present a framework for normalising linguistic representations which we apply to converting the dependency structures output by the Stanford parser (henceforth, Stanford Dependencies or SDs) into labelled SD graphs (LSD) that is, dependency graphs where grammatical relations have been converted to roles.", "labels": [], "entities": []}, {"text": "The LSD graphs we produce and the normalisation framework we present, provide an interesting alternative both for the shallow Predicate/Argument structures produced by semantic role labelling (SRL) systems and for the complex logical formulae produced by deep parsers.", "labels": [], "entities": [{"text": "semantic role labelling (SRL)", "start_pos": 168, "end_pos": 197, "type": "TASK", "confidence": 0.6830643912156423}]}, {"text": "Thus as we shall see in Section 2, labelled SDs are richer than the standard Predicate/Argument structures produced by SRL in that (i) they indicate dependencies between all parts of a sentence, 1 359 not just the verb and its arguments 1 and (ii) they inherit the semantically oriented features of SDs namely, a detailed set of dependencies, a precise account of noun phrases and a semantically oriented treatment of role marking prepositions, of heads and of conjunctions.", "labels": [], "entities": []}, {"text": "Furthermore, the normalisation framework (formal system and methodology) we present, can be extended to model and implement more advanced normalisation steps (e.g., deverbal/verbal and phrasal/lexical synonym normalisation) thereby potentially supporting a stronger normalisation process than the semantic role labelling already supported by SRL systems and by deep parsers.", "labels": [], "entities": [{"text": "phrasal/lexical synonym normalisation", "start_pos": 185, "end_pos": 222, "type": "TASK", "confidence": 0.6343340218067169}]}, {"text": "In sum, although the normalised SDs presented in this paper, do not exhibit a stronger normalisation than that available in the Predicate/Argument structures already produced by deep parsers and by SRL systems, we believe that they are interesting in their own right in that they combine semantic role labelling with the semantic features of SDs.", "labels": [], "entities": []}, {"text": "Moreover, the proposed normalisation framework opens the way fora stronger normalisation process.", "labels": [], "entities": [{"text": "normalisation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9786070585250854}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the representations handled by the system namely, the SD graphs and their labelled versions, the LSDs.", "labels": [], "entities": []}, {"text": "Section 3 presents the rewriting system used and explains how SDs are converted to LSDs.", "labels": [], "entities": []}, {"text": "Section 4 reports on evaluation.", "labels": [], "entities": [{"text": "evaluation", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9362536668777466}]}, {"text": "Section 5 discusses related work and concludes with pointers for further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our normalisation method both on a testsuite of constructed examples and on real world data namely, the Propbank corpus.", "labels": [], "entities": [{"text": "normalisation", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.9674215316772461}, {"text": "Propbank corpus", "start_pos": 117, "end_pos": 132, "type": "DATASET", "confidence": 0.985069066286087}]}, {"text": "This first evaluation aims to provide a systematic, fine grained assessment of how well the system normalises each of the several syntactic configurations assigned by XTAG to distinct verb types.", "labels": [], "entities": []}, {"text": "The emphasis is herein covering the most exhaustive set of possible syntactic configurations possible.", "labels": [], "entities": []}, {"text": "Because constructing the examples was intricate and time consuming, we did not coverall the possibilities described by XTAG however.", "labels": [], "entities": [{"text": "XTAG", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.868630051612854}]}, {"text": "Instead we concentrated on listing all the configurations specified by XTAG for 4 very distinct families namely, Tnx0Vnx1, Tnx0Vnx2nx1,Tnx0Vplnx1 and Tnx0Vnx1pnx2.", "labels": [], "entities": [{"text": "XTAG", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.9043362140655518}]}, {"text": "The first class is the class for transitive verbs.", "labels": [], "entities": []}, {"text": "Because of passive, this class permits many distinct variations.", "labels": [], "entities": []}, {"text": "The second class is the class of verbs with 3 nominal arguments.", "labels": [], "entities": []}, {"text": "This class is difficult for role labelling as the distinction between the complements often relies on semantic rather than syntactic grounds.", "labels": [], "entities": [{"text": "role labelling", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8287308216094971}]}, {"text": "The third class is the class of verbs with a particle and 2 nominal arguments (ring up) and the fourth, the class of ditransitive.", "labels": [], "entities": []}, {"text": "For these constructed sentences, we had no gold standard i.e., no role annotation.", "labels": [], "entities": []}, {"text": "Hence we used logical inference to check normalisation.", "labels": [], "entities": []}, {"text": "We proceeded by grouping the test items in (non) entailment pairs and then checked whether the associated LSDs supported the detection of the correct entailment relation (i.e., true or false).", "labels": [], "entities": []}, {"text": "Using a restricted lexicon, a set of clauses covering the possible syntactic patterns of the four verb classes and regular expressions describing sentence-semantics pairs, we develop a script generating (sentence,semantics) pairs where sentences contain one or more clauses.", "labels": [], "entities": []}, {"text": "After having manually verified the correctness of the generated pairs, we used them to construct textual entailment testsuite items that is, pairs of sentences annotated with TRUE or FALSE dependending on whether the two sentences are related by entailment (TRUE) or not (FALSE).", "labels": [], "entities": [{"text": "TRUE", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9558854699134827}, {"text": "FALSE", "start_pos": 183, "end_pos": 188, "type": "METRIC", "confidence": 0.9669411182403564}, {"text": "FALSE", "start_pos": 272, "end_pos": 277, "type": "METRIC", "confidence": 0.9730889797210693}]}, {"text": "The resulting testsuite 6 contains 4 976 items of which 2 335 are entailments between a sentence and a clause (1V+TE, example 2), 1 019 between two complex sentences (2V+TE, example 3) and 1 622 are non-entailments (V-TE, example 4).", "labels": [], "entities": []}, {"text": "(2) T 1 : John likes the book that Mary put on the Checking for entailment.", "labels": [], "entities": [{"text": "Checking for entailment", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.5949398080507914}]}, {"text": "For each testsuite item, we then checked for entailment by translating LSDs into FOL formulae and checking entailment between the first five LSDs derived from the parser output for the sentences contained in the testsuite item.", "labels": [], "entities": [{"text": "FOL", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.838100790977478}]}, {"text": "The translation of a LSD into a FOL formula is done as follows.", "labels": [], "entities": [{"text": "FOL", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.45991507172584534}]}, {"text": "Each node is associated with an existentially quantified variable and a predication over that variable where the predicate used is the word labelling the node.", "labels": [], "entities": []}, {"text": "Each edge translates to a binary relation between the source and the target node variables.", "labels": [], "entities": []}, {"text": "The overall formula associated with an LSD is then the conjunction of the predications introduced by each node.", "labels": [], "entities": []}, {"text": "For instance, for the LSD given in, the resulting formula is \u2203x, y, z : This translation procedure is of course very basic.", "labels": [], "entities": [{"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9732032418251038}]}, {"text": "Nonetheless, because the testsuite builds on a restricted syntax and vocabulary 7 , it suffices to check how well the normalisation process succeeds in assigning syntactic variants the same semantic representation.", "labels": [], "entities": []}, {"text": "The test procedure just described is applied to the LSD graphs produced by the normalisation module on the testsuite items.", "labels": [], "entities": []}, {"text": "For each class of testsuite items (1V+TE, 2V+TE, V-TE), we list the percentage of cases recognised by the system as entailment (+TE) and non entailment (-TE).", "labels": [], "entities": []}, {"text": "Because FOL is only semi-decidable, the reasoners do not always return an answer.", "labels": [], "entities": [{"text": "FOL", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.815220296382904}]}, {"text": "The Failure line gives the number of cases for which the reasoners fail.", "labels": [], "entities": [{"text": "Failure line", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9676788151264191}]}, {"text": "The results on positive entailments show that the proposed normalisation method is generally successful in recognising syntax based entailments with an overall average precision of 86.3% (and a breakdown of 94.9% for 1V+TE and 66.6% for 2V+TE cases).", "labels": [], "entities": [{"text": "recognising syntax based entailments", "start_pos": 107, "end_pos": 143, "type": "TASK", "confidence": 0.6636873483657837}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9968792200088501}]}, {"text": "Importantly, the results on negative entailments (99.2% overall precision) show that the method is not overly permissive and does not conflate semantically distinct structures.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9969788789749146}]}, {"text": "Finally, it can be seen that the results degrade for the Tnx0Vnx2nx1 class (John gave Mary a book).", "labels": [], "entities": [{"text": "Tnx0Vnx2nx1 class", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.623910591006279}]}, {"text": "This is due mainly to genuine syntactic ambiguities which cannot be resolved without further semantic (usually ontological) knowledge.", "labels": [], "entities": []}, {"text": "For instance, both The book which John gave the woman and The woman whom John gave the book are assigned the same dependency structures by the Stanford parser.", "labels": [], "entities": []}, {"text": "Hence the same rewrite rule applies to both structures and necessarily assigns one of them the wrong labelling.", "labels": [], "entities": []}, {"text": "Other sources of errors are cases where the DIFF patch used to derive anew rule fail to adequately generalise to the target structure.", "labels": [], "entities": []}, {"text": "In such cases, the erroneous rewrite rule can be modified manually.", "labels": [], "entities": []}, {"text": "Figure 5: Precision on constructed examples.", "labels": [], "entities": []}, {"text": "Each cell gives the proportion of cases recognised as entailment by the system.", "labels": [], "entities": []}, {"text": "Bold face figures give the precision i.e., the proportion of answers given by the system that are correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996021389961243}]}, {"text": "The PropBank (Proposition Bank) was created by semantic annotation of the Wall Street Journal section of Treebank-2.", "labels": [], "entities": [{"text": "Wall Street Journal section of Treebank-2", "start_pos": 74, "end_pos": 115, "type": "DATASET", "confidence": 0.9709348380565643}]}, {"text": "Each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate as illustrated in.", "labels": [], "entities": []}, {"text": "The labels used for the core and adjunct-like arguments are the following 8 . The labels A0 ..", "labels": [], "entities": []}, {"text": "A5 designate arguments associated with a verb predicate as defined in the PropBank Frames scheme.", "labels": [], "entities": [{"text": "PropBank Frames scheme", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.9151190916697184}]}, {"text": "A0 is the agent, A1 the patient or the theme.", "labels": [], "entities": [{"text": "A0", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9059986472129822}]}, {"text": "For A2 to A5 no consistent generalisation can be made and the annotation reflects the decisions made when defining the PropBank Frames scheme.", "labels": [], "entities": [{"text": "PropBank Frames scheme", "start_pos": 119, "end_pos": 141, "type": "DATASET", "confidence": 0.9228412906328837}]}, {"text": "Further, the AM-T label describes adjunct like arguments of various sorts, where T is the type of the adjunct.", "labels": [], "entities": []}, {"text": "Types include locative, temporal, manner, etc.", "labels": [], "entities": []}, {"text": "We used the PropBank to evaluate our normalisation procedure on free text.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.9610761404037476}, {"text": "normalisation", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9622223377227783}]}, {"text": "As in the CoNLL (Conference on Natural Language Learning) shared task for SRL, the evaluation metrics used are precision, recall and F measure.", "labels": [], "entities": [{"text": "CoNLL (Conference on Natural Language Learning) shared task", "start_pos": 10, "end_pos": 69, "type": "TASK", "confidence": 0.5151410818099975}, {"text": "SRL", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9419280886650085}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9995630383491516}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9979372024536133}, {"text": "F measure", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9921359717845917}]}, {"text": "An argument is said to be correctly recognised if the words spanning the argument as well as its semantic role match the PropBank annotation.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.9266952872276306}]}, {"text": "Precision is the proportion of arguments predicted by a system which are correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9888407588005066}]}, {"text": "Recall is the proportion of correct arguments which are predicted by a system.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.983241081237793}]}, {"text": "F-measure is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9710832834243774}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9993749260902405}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9960999488830566}]}, {"text": "The results are given below.", "labels": [], "entities": []}, {"text": "Precision (80.6%) is comparable to the results obtained in the ConLL 2005 SRL shared task where the top 8 systems have an average precision ranging from 76.55% to 82.28%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.996880054473877}, {"text": "ConLL 2005 SRL shared task", "start_pos": 63, "end_pos": 89, "type": "DATASET", "confidence": 0.7721111416816712}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9981821775436401}]}, {"text": "Recall is generally a little low (the ConLL05 recall ranged from 64.99% to 76.78%) for mainly two reasons: either the Stanford parser, did not deliver the correct analysis or the required rewrite rule was not present.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9954865574836731}, {"text": "ConLL05", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.42585989832878113}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.8083537220954895}]}], "tableCaptions": []}