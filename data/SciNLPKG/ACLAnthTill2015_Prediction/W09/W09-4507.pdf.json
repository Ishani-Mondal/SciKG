{"title": [{"text": "Cascading Classifiers for Named Entity Recognition in Clinical Notes", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.6848465204238892}]}], "abstractContent": [{"text": "Clinical named entities convey great deal of knowledge in clinical notes.", "labels": [], "entities": []}, {"text": "This paper investigates named entity recognition from clinical notes using machine learning approaches.", "labels": [], "entities": [{"text": "named entity recognition from clinical notes", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.7939126143852869}]}, {"text": "We present a cascading system that uses a Conditional Random Fields model, a Support Vector Machine and a Maximum Entropy to reclassify the identified entities in order to reduce misclas-sification.", "labels": [], "entities": []}, {"text": "Voting strategy was employed to determine the class of the recognised entities between the three classifiers.", "labels": [], "entities": []}, {"text": "The experiments were conducted on a corpus of 311 manually annotated admission summaries form an Intensive Care Unit.", "labels": [], "entities": []}, {"text": "The recognition of 10 types of clinical named entities using 10 fold cross-validation achieved an overall results of 83.3 F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9922435879707336}]}, {"text": "The reclassifier effectively increased the performance over stand-alone CRF models by 3.35 F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9981391429901123}]}], "introductionContent": [{"text": "With the rapid growth of clinical data produced by health organisations, efficient information extraction from these free text clinical notes will be valuable for improving the work of clinical wards and gaining greater understanding of patient care as well as progression of disease.", "labels": [], "entities": []}, {"text": "Recognising named entities is a key to unlocking the information stored in unstructured clinical text.", "labels": [], "entities": [{"text": "Recognising named entities", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8547254999478658}]}, {"text": "Named entity recognition is an important subtask of Information Extraction.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6693439781665802}, {"text": "Information Extraction", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7897359132766724}]}, {"text": "It involves the recognition of named entity (NE) phrases, and usually the classification of these NEs into particular categories.", "labels": [], "entities": [{"text": "recognition of named entity (NE) phrases", "start_pos": 16, "end_pos": 56, "type": "TASK", "confidence": 0.8402130603790283}]}, {"text": "In the clinical domain, important entity categories are clinical findings, procedures and drugs.", "labels": [], "entities": []}, {"text": "In recent years, the recognition of named entities in the biomedical scientific literature has become the focus of much research.", "labels": [], "entities": []}, {"text": "A large number of systems have been built to recognise, classify and map biomedical entities to ontologies.", "labels": [], "entities": []}, {"text": "On the other side, only a little work have been reported in clinical named entity recognition.", "labels": [], "entities": [{"text": "clinical named entity recognition", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.568323940038681}]}, {"text": "NER has achieved high performance in scientific articles and newswire text, whereas the clinical notes written by clinicians are in a less structured and often minimal grammatical form with idosyncratic and cryptic shorthand, which poses challenges in NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8518204092979431}]}, {"text": "Principally, the clinical named entity recognition systems are rule or pattern based.", "labels": [], "entities": [{"text": "clinical named entity recognition", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.6610973030328751}]}, {"text": "The rules or patterns may not be generalisable due to the specific writing style of individual clinicians.", "labels": [], "entities": []}, {"text": "However, a machine learning approach is not fully advanced in clinical named entity recognition due to alack of available training data.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.5958200792471567}]}, {"text": "We have investigated the issues of clinical named entity recognition, by constructing a set of annotation guidelines and manually annotating 311 clinical notes from an Intensive Care Unit (ICU), with inter-annotator agreement of 88%.", "labels": [], "entities": [{"text": "clinical named entity recognition", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.6039615869522095}]}, {"text": "In this paper we present a named entity recogniser using a cascade of classifiers to find entities.", "labels": [], "entities": []}, {"text": "The named entities will serve as a prerequisite for clinical relation extraction, clinical notes indexing and question answering from the ICU database.", "labels": [], "entities": [{"text": "clinical relation extraction", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.6838529606660207}, {"text": "clinical notes indexing", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.5929072697957357}, {"text": "question answering", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8359328210353851}, {"text": "ICU database", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9352134466171265}]}, {"text": "There have been many approaches to NER in biomedical literature.", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.994835376739502}]}, {"text": "They roughly fall into three approaches: rule-based approaches, dictionary-based approaches and machine learning based approaches.", "labels": [], "entities": []}, {"text": "The state-of-art machine learning based systems focus on selecting effective features for building classifiers.", "labels": [], "entities": []}, {"text": "Many machine learners have been used for experimentation, for example, Support Vector Machines (SVMs), Hidden Markov Model (HMM), Maximum Entropy Model (ME) and Conditional Random Fields (CRFs).", "labels": [], "entities": []}, {"text": "Conditional Random Fields have been proven to be the best performing learner for the NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.9259698390960693}]}, {"text": "The benefit of using a machine learner is that it can utilise both the information form of the entity themselves and the contextual information surrounding the entity.", "labels": [], "entities": []}, {"text": "It has better generalisability over pattern based approach as it is able to perform prediction without seeing the entire length of the entity.", "labels": [], "entities": []}, {"text": "Nevertheless the performance of biomedical NER systems still trails behind newswire NER systems.", "labels": [], "entities": [{"text": "NER", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.7437083721160889}]}, {"text": "It suggests that individual NER system may not cover entity representations with sufficiently rich features due to the great variety and ambiguity in biomedical named entities.", "labels": [], "entities": [{"text": "NER", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.950337290763855}]}, {"text": "This problem also exists in clinical text as it has characteristic of both formal and informal linguistic styles, with many unseen named entities, spelling variations and abbreviations.", "labels": [], "entities": []}, {"text": "To overcome these difficulties, we propose a classifier cascade approach to clinical NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.784778356552124}]}, {"text": "We firstly build a CRF based classifier to identify the boundary and class of the named entities, then we trained a SVM and an ME model to reclassify the class of the named entities using the output of the CRF models and different features.", "labels": [], "entities": []}, {"text": "The final class of the entity was determined by a majority voting among the output of the CRF, SVM and ME models.", "labels": [], "entities": [{"text": "CRF", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.8655975461006165}, {"text": "ME", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9089949131011963}]}, {"text": "The overall system achieved best performance of 83.26 F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9349284172058105}]}, {"text": "The cascading classifiers improved 3.35 F-sore over the stand-alone CRF system.", "labels": [], "entities": [{"text": "F-sore", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.961789608001709}]}, {"text": "This paper is organised as follows: Section 2 gives an overview of related work in biomedical named entity recognition.", "labels": [], "entities": [{"text": "biomedical named entity recognition", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.665972039103508}]}, {"text": "Section 3 introduces the data used in our experiments.", "labels": [], "entities": []}, {"text": "Section 4 to Section 6 describes the cascading named entity recogniser in detail.", "labels": [], "entities": [{"text": "cascading named entity recogniser", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.7197272479534149}]}, {"text": "Section 7 presents the evaluation of the proposed system as well as discussion of the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data consists of a total of 45953 tokens, 17544 tokens are annotated with entity tags.", "labels": [], "entities": []}, {"text": "The tag density is 38.18%.", "labels": [], "entities": [{"text": "tag density", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9722506999969482}]}, {"text": "There are in total 12882 named entities results with an average of 1.36 tokens per named entity.", "labels": [], "entities": []}, {"text": "The results were evaluated by 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "Each fold was stratified on a sentence level, so that for the rare classes such as organism had some instances in each fold.", "labels": [], "entities": []}, {"text": "We adapted the evaluation scripts provided by the jnlpba 2004 shared task to evaluate the system performance 3 . The standard Recall/Precision/F-score are used as evaluation metrics.", "labels": [], "entities": [{"text": "jnlpba 2004 shared task", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.903608575463295}, {"text": "Recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9282485246658325}, {"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.5370177030563354}]}, {"text": "We use CRF++ 4 package for CRF learning.", "labels": [], "entities": [{"text": "CRF learning", "start_pos": 27, "end_pos": 39, "type": "TASK", "confidence": 0.8594772517681122}]}, {"text": "CRF++ takes the standard CoNLL NER shared task input.", "labels": [], "entities": [{"text": "CRF++", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9197988212108612}, {"text": "CoNLL NER shared task input", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.8614337325096131}]}, {"text": "We converted the data and features into the accepted format and trained the model using the package's default parameter configuration.", "labels": [], "entities": []}, {"text": "We did no feature selection and all folds use the same parameter setting.", "labels": [], "entities": []}, {"text": "CRF++ can produce output tags along with the tag's probability, these probabilities are used for reclassification.", "labels": [], "entities": [{"text": "CRF++", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8920497000217438}]}, {"text": "We use LibSVM 5 and Maxent 6 for reclassification.", "labels": [], "entities": []}, {"text": "We use the polynomial kernel with degree 2 in SVM learning, and set the C values to 8, i.e. approximately the ratio of the number of negative instances to the number of positive instances in the training data.", "labels": [], "entities": [{"text": "SVM learning", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.9445151686668396}]}, {"text": "The other parameters are obtained by a 10-fold cross-validation on the training data.", "labels": [], "entities": []}, {"text": "The probability of SVM tags are obtained by setting appropriate software options to enable probability output during training and prediction.", "labels": [], "entities": [{"text": "SVM tags", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.8941174447536469}]}, {"text": "To train the Maxent model, we use Maxent package's default parameters and terminate the learning process when the training model converges.", "labels": [], "entities": []}, {"text": "shows the performance of the CRF classifier.", "labels": [], "entities": []}, {"text": "Features were added to the model progressively to understand the contribution of each feature.", "labels": [], "entities": []}, {"text": "The overall performance is very promising, with a score F-score of 79.91.", "labels": [], "entities": [{"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9991859793663025}]}, {"text": "All experiments used window size of 5 and previously predicted labels.", "labels": [], "entities": []}, {"text": "The baseline model was built using only word features.", "labels": [], "entities": []}, {"text": "The dictionary features are very useful, the use of a dictionary allows for the identification of unseen words in the test set.", "labels": [], "entities": []}, {"text": "The dictionary entries also act as trigger words described in some biomedical NER systems, and can help identify the boundary of entity.", "labels": [], "entities": [{"text": "NER", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.710979700088501}]}, {"text": "POS tag is not as effective as expected, this maybe due to the inaccurate POS tagging by the GENIA tagger and that the sentences are poorly structured.", "labels": [], "entities": [{"text": "POS tag", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.653031975030899}, {"text": "POS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7216659784317017}, {"text": "GENIA", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.8855313062667847}]}, {"text": "Other features all make moderate contribution to the performance.", "labels": [], "entities": []}, {"text": "Different context window sizes were investigated and a window size 5 produced the best performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The inter-annotator agreement measured by  F-score for 10 Entity Classes.", "labels": [], "entities": [{"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9961007833480835}]}, {"text": " Table 3: Contribution of features by adding features  progressively (using window size of 5). Different win- dow sizes were investigated.", "labels": [], "entities": []}, {"text": " Table 4: Results of reclassification for correctly iden- tified named entities.", "labels": [], "entities": []}, {"text": " Table 5: Performance of combined systems using re- classification.", "labels": [], "entities": [{"text": "re- classification", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.49756157398223877}]}, {"text": " Table 6: The performance of the best cascading sys- tem and baseline CRF systems with detailed informa- tion for each class.", "labels": [], "entities": []}, {"text": " Table 8: Results of different partial matching criteria.", "labels": [], "entities": []}]}