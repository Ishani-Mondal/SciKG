{"title": [{"text": "Transducing Logical Relations from Automatic and Manual GLARF", "labels": [], "entities": [{"text": "Transducing Logical Relations from Automatic and Manual GLARF", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.7066591084003448}]}], "abstractContent": [{"text": "GLARF relations are generated from tree-bank and parses for English, Chinese and Japanese.", "labels": [], "entities": []}, {"text": "Our evaluation of system output for these input types requires consideration of multiple correct answers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Systems, such as treebank-based parsers) and semantic role labelers (), are trained and tested on hand-annotated data.", "labels": [], "entities": []}, {"text": "Evaluation is based on differences between system output and test data.", "labels": [], "entities": []}, {"text": "Other systems use these programs to perform tasks unrelated to the original annotation.", "labels": [], "entities": []}, {"text": "For example, participating systems in CONLL (), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand.", "labels": [], "entities": []}, {"text": "This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences.", "labels": [], "entities": [{"text": "generating Grammatical and Logical Representation Framework (GLARF) for English, Chinese and Japanese sentences", "start_pos": 135, "end_pos": 246, "type": "TASK", "confidence": 0.7555384561419487}]}, {"text": "The paper describes GLARF () and GLARFers and compares GLARF produced from treebank and parses.", "labels": [], "entities": []}, {"text": "includes simplified GLARF analyses for English, Chinese and Japanese sentences.", "labels": [], "entities": [{"text": "GLARF", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.7419638633728027}]}, {"text": "For each sentence, a GLARFer constructs both a Feature Structure (FS) representing a constituency analysis and a set of 31-tuples, each representing up to three dependency relations between pairs of words.", "labels": [], "entities": []}, {"text": "Due to space limitations, we will focus on the 6 fields of the 31-tuple represented in.", "labels": [], "entities": []}, {"text": "These include: (1) a functor (func); (2) the depending argument (Arg); (3) a surface (Surf) label based on the position in the parse tree with no regularizations; (4) a logic1 label (L \u00af 1) fora relation that reflects grammar-based regularizations of the surface level.", "labels": [], "entities": [{"text": "Arg", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9426570534706116}]}, {"text": "This marks relations for filling gaps in relative clauses or missing infinitival subjects, represents passives as paraphrases as actives, etc.", "labels": [], "entities": []}, {"text": "The interpretation of the *CONJ relations in the Japanese example, include not only that the nouns [zaisan] (assets) and [seimei] (lives) are conjoined, but also that these two nouns, together form the object of the Japanese verb [mamoru] (protect).", "labels": [], "entities": [{"text": "CONJ", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.8664429783821106}]}, {"text": "Thus, for example, semantic selection patterns should treat these nouns as possible objects for this verb.", "labels": [], "entities": []}, {"text": "Transparent relations may serve to neutralize some of the problematic cases of attachment ambiguity.", "labels": [], "entities": []}, {"text": "For example, in the English sentence A number of phrases with modifiers are not ambiguous, there is a transparent *COMP relation between numbers and of and a transparent *OBJ relation between of and phrases.", "labels": [], "entities": [{"text": "OBJ", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.9406710267066956}]}, {"text": "Thus, high attachment of the PP with modifiers, would have the same interpretation as low attachment since phrases is the underlying head of number of In this same example, the adverb not can be attached to either the copula are or the predicative adjective, with no discernible difference in meaning-this factor is indicated by the transparent designation of the relations where the copula is a functor.", "labels": [], "entities": []}, {"text": "Transparent features also provide us with a simple way of handling certain function words, such as the Chinese word De which inherits the function of its underlying head, connecting a variety of such modifiers to head nouns (an adjective in the Chinese example.).", "labels": [], "entities": []}, {"text": "For conjunction cases, the number of underlying relations would multiply, e.g., Mary and John bought and sold stock would (underlyingly) have four subject relations derived by pairing each of the underlying subject nouns Mary and John with each of the underlying main predicate verbs bought and sold.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran GLARFers on both manually created treebanks and automatically produced parses for English, Chinese and Japanese.", "labels": [], "entities": []}, {"text": "For each corpus, we created one or more answer keys by correcting system output.", "labels": [], "entities": []}, {"text": "For this paper, we evaluate solely on the logic1 relations (the second column in figure 1.) lists our results for all three languages, based on treebank and parser input.", "labels": [], "entities": []}, {"text": "As in ( , we generated 4-tuples consisting of the following for each dependency: (A) the logic1 label (SBJ, OBJ, etc.), (B) its transparency (True or False), (C) The functor (a single word or a named entity); and (D) the argument (a single word or a named entity).", "labels": [], "entities": []}, {"text": "In the case of conjunction where there was no lexical conjunction word, we used either punctuation (commas or semi-colons) or the placeholder *NULL*.", "labels": [], "entities": []}, {"text": "We then corrected these results by hand to produce the answer key-an answer was correct if all four members of the tuple were correct and incorrect otherwise.", "labels": [], "entities": []}, {"text": "provides the Precision, Recall and F-scores for our output.", "labels": [], "entities": [{"text": "Precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9991075396537781}, {"text": "Recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9920448064804077}, {"text": "F-scores", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.991778552532196}]}, {"text": "The F-T columns indicates a modified F-score derived by ignoring the +/-Transparent distinction (resulting changes in precision, recall and F-score are the same).", "labels": [], "entities": [{"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9930566549301147}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9991835951805115}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9974457025527954}, {"text": "F-score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.9948979020118713}]}, {"text": "For English and Japanese, an expert native speaking linguist corrected the output.", "labels": [], "entities": []}, {"text": "For Chinese, several native speaking computational linguists shared the task.", "labels": [], "entities": []}, {"text": "By checking compatibility of the answer keys with outputs derived from different sources (parser, treebank), we could detect errors and inconsistencies.", "labels": [], "entities": []}, {"text": "We processed the following corpora.", "labels": [], "entities": []}, {"text": "English: 86 sentence article (wsj 2300) from the Wall Street Journal PTB test corpus (WSJ); 46 sentence letter from Good Will (LET), the first 100 sentences of a switchboard telephone transcript (TEL) and the first 100 sentences of a narrative from the Charlotte Narrative and Conversation (NAR).", "labels": [], "entities": [{"text": "Wall Street Journal PTB test corpus (WSJ)", "start_pos": 49, "end_pos": 90, "type": "DATASET", "confidence": 0.9492572281095717}, {"text": "Good Will (LET)", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.8839486956596374}, {"text": "Charlotte Narrative and Conversation (NAR)", "start_pos": 253, "end_pos": 295, "type": "DATASET", "confidence": 0.9640829988888332}]}, {"text": "These samples are taken from the PTB WSJ Corpus and the SIGANN shared subcorpus of the OANC.", "labels": [], "entities": [{"text": "PTB WSJ Corpus", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9166636268297831}, {"text": "SIGANN shared subcorpus of the OANC", "start_pos": 56, "end_pos": 91, "type": "DATASET", "confidence": 0.8215386072794596}]}, {"text": "The filenames are: 110CYL067, NapierDianne and sw2014.", "labels": [], "entities": [{"text": "110CYL067", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.9104844927787781}, {"text": "NapierDianne", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8461557030677795}]}, {"text": "Chinese: a 20 sentence sample of text from the Penn Chinese Treebank (CTB) ().", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 47, "end_pos": 74, "type": "DATASET", "confidence": 0.952134519815445}]}, {"text": "Japanese: 20 sentences from the Kyoto Corpus (KYO) (", "labels": [], "entities": [{"text": "Kyoto Corpus (KYO)", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9626866102218627}]}], "tableCaptions": []}