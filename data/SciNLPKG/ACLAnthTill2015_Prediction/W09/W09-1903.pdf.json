{"title": [{"text": "Estimating Annotation Cost for Active Learning in a Multi-Annotator Environment", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an empirical investigation of the annotation cost estimation task for active learning in a multi-annotator environment.", "labels": [], "entities": []}, {"text": "We present our analysis from two perspectives: selecting examples to be presented to the user for annotation; and evaluating selective sampling strategies when actual annotation cost is not available.", "labels": [], "entities": []}, {"text": "We present our results on a movie review classification task with rationale annotations.", "labels": [], "entities": [{"text": "movie review classification task", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.728543795645237}]}, {"text": "We demonstrate that a combination of instance, annotator and annotation task characteristics are important for developing an accurate estimator, and argue that both correlation coefficient and root mean square error should be used for evaluating annotation cost estimators.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 165, "end_pos": 188, "type": "METRIC", "confidence": 0.9690232872962952}, {"text": "root mean square error", "start_pos": 193, "end_pos": 215, "type": "METRIC", "confidence": 0.6681714579463005}]}], "introductionContent": [{"text": "Active Learning is the process of selectively querying the user to annotate examples with the goal of minimizing the total annotation cost.", "labels": [], "entities": []}, {"text": "Annotation cost has been traditionally measured in terms of the number of examples annotated, but it has been widely acknowledged that different examples may require different annotation effort (.", "labels": [], "entities": []}, {"text": "Ideally, we would use actual human annotation cost for evaluating selective sampling strategies, but this will require conducting several user studies, one per strategy on the same dataset.", "labels": [], "entities": []}, {"text": "Alternatively, we maybe able to simulate the real user by an annotation cost estimator that can then be used to evaluate several selective sampling strategies without having to run anew user study each time.", "labels": [], "entities": []}, {"text": "An annotation cost estimator models the characteristics that can differentiate the examples in terms of their annotation time.", "labels": [], "entities": []}, {"text": "The characteristics that strongly correlate with the annotation time can be used as a criterion in selective sampling strategies to minimize the total annotation cost.", "labels": [], "entities": []}, {"text": "In some domains, the annotation cost of an example is known or can be calculated exactly before querying the user.", "labels": [], "entities": []}, {"text": "For example, in biological experiments it might be calculable from the cost of the equipment and the material used ().", "labels": [], "entities": []}, {"text": "In NLP, sometimes a simplifying assumption is made that the annotation cost for an example can be measured in terms of its length (e.g. seconds of voicemail annotated; number of tokens annotated).", "labels": [], "entities": []}, {"text": "Another assumption is that the number of user annotation actions can be used as a proxy for annotation cost of an example (e.g. number of brackets added for parsing a sentence); number of clicks for correcting named entities).", "labels": [], "entities": []}, {"text": "While these are important factors in determining the annotation cost, none of them alone can fully substitute for the actual annotation cost.", "labels": [], "entities": []}, {"text": "For example, a short sentence with a lot of embedded clauses maybe more costly to annotate than a longer sentence with simpler grammatical structure.", "labels": [], "entities": []}, {"text": "Similarly, a short sentence with multiple verbs and discontinuous arguments may take more time to annotate with semantic roles than a longer sentence with a single verb and simple subject-verb-object structure (.", "labels": [], "entities": []}, {"text": "What further complicates the estimation of annotation cost is that even for the same example, annotation cost may vary across annotators (.", "labels": [], "entities": []}, {"text": "For example, non-native speakers of English were found to take longer time to annotate part of speech tags ( . Often multiple annotators are used for creating an annotated corpus to avoid annotator bias, and we may not know all our annotators beforehand.", "labels": [], "entities": []}, {"text": "Annotation cost also depends on the user interface used for annotation (), and the user interface may change during an annotation task.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9622736573219299}]}, {"text": "Thus, we need a general annotation cost estimator that can predict annotation cost fora given annotator and user interface.", "labels": [], "entities": []}, {"text": "A general estimator can be built by using annotator and user interface characteristics in addition to the instance characteristics for learning an annotation cost model, and training on data from multiple annotators and multiple user interfaces.", "labels": [], "entities": []}, {"text": "Such a general estimator is important for active learning research where the goal is to compare selective sampling strategies independent of the annotator and the user interface.", "labels": [], "entities": []}, {"text": "In this work, we investigate the annotation cost estimation problem fora movie review classification task in a multi-annotator environment with a fixed user interface.", "labels": [], "entities": [{"text": "annotation cost estimation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6926291386286417}, {"text": "movie review classification task", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.7528517097234726}]}, {"text": "We demonstrate that a combination of instance, annotation task and annotator characteristics is important for accurately estimating the annotation cost.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we first present a survey of related work and an analysis of the data collected.", "labels": [], "entities": []}, {"text": "We then describe the features used for our supervised learning approach to annotation cost estimation, followed by the experimental setup and results.", "labels": [], "entities": [{"text": "annotation cost estimation", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.7257685859998068}]}, {"text": "Finally, we conclude with some future directions we would like to explore.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our annotation methodology and analysis of the data we collected, followed by a description of the features we used.We then present our experimental setup followed by a discussion of our results.", "labels": [], "entities": []}, {"text": "We use both Root Mean Square (RMS) error and Correlation Coefficient (CRCoef) to evaluate our model, since the two metrics evaluate different aspects of an estimate.", "labels": [], "entities": [{"text": "Root Mean Square (RMS) error and Correlation Coefficient (CRCoef)", "start_pos": 12, "end_pos": 77, "type": "METRIC", "confidence": 0.8481060633292565}]}, {"text": "RMS is away to quantify the amount by which an estimator differs from the true value of the quantity being estimated.", "labels": [], "entities": [{"text": "RMS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.955493688583374}]}, {"text": "It tells us how 'off' our estimate is from the truth.", "labels": [], "entities": []}, {"text": "CRCoef on the other hand measures the strength and direction of a linear relationship between two random variables.", "labels": [], "entities": [{"text": "CRCoef", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9328229427337646}]}, {"text": "It tells us how well correlated our estimate is with the actual annotation time.", "labels": [], "entities": []}, {"text": "Thus, for evaluating how accurate our model is in predicting annotation times, RMS is a more appropriate metric.", "labels": [], "entities": [{"text": "predicting annotation", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8382954597473145}, {"text": "RMS", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9933136105537415}]}, {"text": "For evaluating the utility of the estimated annotation cost as a criterion for ranking and selecting examples for user's annotation, CRCoef is a better metric.", "labels": [], "entities": [{"text": "CRCoef", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.662284255027771}]}, {"text": "We learn an annotation cost estimator using the Linear Regression and SMO Regression () learners from the Weka machine learning toolkit).", "labels": [], "entities": [{"text": "SMO Regression", "start_pos": 70, "end_pos": 84, "type": "METRIC", "confidence": 0.8150003254413605}, {"text": "Weka machine learning toolkit", "start_pos": 106, "end_pos": 135, "type": "DATASET", "confidence": 0.9117035865783691}]}, {"text": "As mentioned earlier, we have 5 sets of 25 documents each, and each set was annotated by four annotators.", "labels": [], "entities": []}, {"text": "The results reported are averaged over five folds, where each set is one fold, and two algorithms (Linear Regression and SMO Regression).", "labels": [], "entities": [{"text": "SMO Regression", "start_pos": 121, "end_pos": 135, "type": "METRIC", "confidence": 0.5595515221357346}]}, {"text": "Varying the algorithm helps us find the most predictive feature combinations across different algorithms.", "labels": [], "entities": []}, {"text": "Since each set was annotated by different annotators, we never train and test on the data from same annotators.", "labels": [], "entities": []}, {"text": "We used the JMP 2 and Minitab 3 statistical tools for our analysis.", "labels": [], "entities": [{"text": "JMP", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.7687188386917114}]}, {"text": "We used an ANOVA model with Standard Least Squares fitting to compare the different experimental conditions.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.7347204089164734}, {"text": "Standard Least Squares fitting", "start_pos": 28, "end_pos": 58, "type": "METRIC", "confidence": 0.7591195032000542}]}, {"text": "We make all comparisons in terms of both the CRCoef and the RMS metrics.", "labels": [], "entities": [{"text": "CRCoef", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8893758058547974}, {"text": "RMS metrics", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.6593556106090546}]}, {"text": "For significance results reported, we used 2-tailed paired T-test, considering (p < 0.05) as significant.", "labels": [], "entities": [{"text": "2-tailed paired T-test", "start_pos": 43, "end_pos": 65, "type": "METRIC", "confidence": 0.5467495024204254}]}, {"text": "We present our results and analysis in three parts.", "labels": [], "entities": []}, {"text": "We first compare the four instance characteristics, annotator and annotation task characteristics; and their combination.", "labels": [], "entities": []}, {"text": "We then present an analysis of the interaction between features and annotation time.", "labels": [], "entities": []}, {"text": "Finally, we compare the ranking of features based on the two evaluation metrics we used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The Table shows the average nativeness and average", "labels": [], "entities": []}, {"text": " Table 3: Mean and the standard deviation for the feature oc-", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994588494300842}]}, {"text": " Table 4: CR-Coef and RMS results for Character Length", "labels": [], "entities": [{"text": "CR-Coef", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8233768939971924}, {"text": "RMS", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.7243247628211975}, {"text": "Character Length", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8691562414169312}]}, {"text": " Table 5: CR-Coef and RMS results for seven feature com-", "labels": [], "entities": [{"text": "CR-Coef", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8744387626647949}, {"text": "RMS", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6388568878173828}]}, {"text": " Table 6: Each block of 3 rows in this table compares the", "labels": [], "entities": []}, {"text": " Table 7: Correlation between Character Length (CL), Num-", "labels": [], "entities": [{"text": "Character Length (CL)", "start_pos": 30, "end_pos": 51, "type": "METRIC", "confidence": 0.7830669283866882}]}]}