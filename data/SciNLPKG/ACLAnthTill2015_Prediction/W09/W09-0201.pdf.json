{"title": [{"text": "One distributional memory, many semantic spaces", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose an approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus.", "labels": [], "entities": []}, {"text": "Task-specific semantic spaces are then built on demand from the repository.", "labels": [], "entities": []}, {"text": "A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks, ranging from concept categorization to relation extraction to many others.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.8465395271778107}]}, {"text": "The typical approach in the field has been a \"local\" one, in which each semantic task (or set of closely related tasks) is treated as a separate problem, that requires its own corpus-derived model and algorithms.", "labels": [], "entities": []}, {"text": "Its successes notwithstanding, the \"one task -one model\" approach has also some drawbacks.", "labels": [], "entities": []}, {"text": "From a cognitive angle, corpus-based models hold promise as simulations of how humans acquire and use conceptual and linguistic information from their environment).", "labels": [], "entities": []}, {"text": "However, the common view in cognitive (neuro)science is that humans resort to a multipurpose semantic memory, i.e., a database of interconnected concepts and properties), adapting the information stored thereto the task at hand.", "labels": [], "entities": []}, {"text": "From an engineering perspective, going back to the corpus to train a different model for each application is inefficient and it runs the risk of overfitting the model to a specific task, while losing sight of its adaptivity -a highly desirable feature for any intelligent system.", "labels": [], "entities": []}, {"text": "Think, by contrast, of WordNet, a single network of semantic information that has been adapted to all sorts of tasks, many of them certainly not envisaged by the resource creators.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9484595656394958}]}, {"text": "In this paper, we explore a different approach to corpus-based semantics.", "labels": [], "entities": []}, {"text": "Our model consists of a distributional semantic memory -a graph of weighted links between concepts -built once and for all from our source corpus.", "labels": [], "entities": []}, {"text": "Starting from the tuples that can be extracted from this graph, we derive multiple semantic spaces to solve a wide range of tasks that exemplify various strands of corpus-based semantic research: measuring semantic similarity between concepts, concept categorization, selectional preferences, analogy of relations between concept pairs, finding pairs that instantiate a target relation and spotting an alternation in verb argument structure.", "labels": [], "entities": [{"text": "analogy of relations between concept pairs", "start_pos": 293, "end_pos": 335, "type": "TASK", "confidence": 0.8602676391601562}]}, {"text": "Given a graph like the one in below, adaptation to all these tasks (and many others) can be reduced to two basic operations: 1) building semantic spaces, as cooccurrence matrices defined by choosing different units of the graph as row and column elements; 2) measuring similarity in the resulting matrix either between specific rows or between a row and an average of rows whose elements share a certain property.", "labels": [], "entities": []}, {"text": "After reviewing some of the most closely related work (Section 2), we introduce our approach (Section 3) and, in Section 4, we proceed to test it in various tasks, showing that its performance is always comparable to that of task-specific methods.", "labels": [], "entities": []}, {"text": "Section 5 draws the current conclusions and discusses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now look at three views of the DM graph: concept-by-link+concept (CxLC), concept+concept-by-link (CCxL), and concept+link-by-concept.", "labels": [], "entities": []}, {"text": "Each view will be tested on one or more semantic tasks and compared with alternative models.", "labels": [], "entities": []}, {"text": "There is a fourth possible view, links-by-concept+concept (LxCC), that is not explored here, but would lead to meaningful semantic tasks (finding links that express similar semantic relations).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A fragment of the CxLC space", "labels": [], "entities": []}, {"text": " Table 2: Correlation with similarity ratings", "labels": [], "entities": [{"text": "similarity", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9427102208137512}]}, {"text": " Table 3: Concrete noun categorization", "labels": [], "entities": [{"text": "Concrete noun categorization", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7550401488939921}]}, {"text": " Table 4: Correlation with verb-argument plausibil- ity judgments", "labels": [], "entities": []}, {"text": " Table 5: A fragment of the CCxL space", "labels": [], "entities": []}, {"text": " Table 6: Accuracy with SAT analogies", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980417490005493}]}, {"text": " Table 7. The DM+ scores  ignore the 32% pairs not in our CCxL space; the  DM\u2212 scores assume random performance on such  pairs. These scores give the range within which  our performance will lie once we introduce tech- niques to deal with unseen pairs. We also report  results of the SEMEVAL systems that did not use  the organizer-provided WordNet sense labels nor  information about the query used to retrieve the  examples, as well as performance of several trivial  classifiers, also from the SEMEVAL task descrip- tion.", "labels": [], "entities": []}, {"text": " Table 7: SEMEVAL relation classification", "labels": [], "entities": [{"text": "SEMEVAL relation classification", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8999860882759094}]}, {"text": " Table 8: A fragment of the CLxC space", "labels": [], "entities": []}]}