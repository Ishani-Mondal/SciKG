{"title": [{"text": "Statistical Phrase Alignment Model Using Dependency Relation Probability", "labels": [], "entities": [{"text": "Statistical Phrase Alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6421152849992117}, {"text": "Dependency Relation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7015289068222046}]}], "abstractContent": [{"text": "When aligning very different language pairs, the most important needs are the use of structural information and the capability of generating one-to-many or many-to-many correspondences.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel phrase alignment method which models word or phrase dependency relations in dependency tree structures of source and target languages.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7851867079734802}]}, {"text": "The dependency relation model is a kind of tree-based reordering model, and can handle non-local reorderings which sequential word-based models often cannot handle properly.", "labels": [], "entities": []}, {"text": "The model is also capable of estimating phrase correspondences automatically without any heuristic rules.", "labels": [], "entities": [{"text": "estimating phrase correspondences", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.8087352315584818}]}, {"text": "Experimental results of alignment show that our model could achieve F-measure 1.7 points higher than the conventional word alignment model with sym-metrization algorithms.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9939696788787842}, {"text": "word alignment", "start_pos": 118, "end_pos": 132, "type": "TASK", "confidence": 0.6969605088233948}]}], "introductionContent": [{"text": "We consider that there are two important needs in aligning parallel sentences written in very different languages such as Japanese and English.", "labels": [], "entities": []}, {"text": "One is to adopt structural or dependency analysis into the alignment process to overcome the difference in word order.", "labels": [], "entities": []}, {"text": "The other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences.", "labels": [], "entities": []}, {"text": "Most existing alignment methods simply consider a sentence as a sequence of words (, and generate phrase correspondences using heuristic rules (.", "labels": [], "entities": []}, {"text": "Some studies incorporate structural information into the alignment process after this simple word alignment ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.7187963128089905}]}, {"text": "However, this is not sufficient because the basic word alignment itself is not good.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.673390194773674}]}, {"text": "On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. and proposed a structural alignment methods.", "labels": [], "entities": [{"text": "structural alignment", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.726610392332077}]}, {"text": "These methods use heuristic rules when resolving correspondence ambiguities. and proposed a tree-based probabilistic alignment methods.", "labels": [], "entities": []}, {"text": "These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid.", "labels": [], "entities": []}, {"text": "Yamada and Knight flattened the trees by collapsing nodes.", "labels": [], "entities": []}, {"text": "Gildea cloned sub-trees to deal with the problem.", "labels": [], "entities": []}, {"text": "proposed a model which uses a source side dependency tree structure and constructs a discriminative model.", "labels": [], "entities": []}, {"text": "However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments.", "labels": [], "entities": []}, {"text": "also proposed a model focusing on the dependency relations.", "labels": [], "entities": []}, {"text": "Their model has the constraint that content words can only correspond to content words on the other side, and the same applies for function words.", "labels": [], "entities": []}, {"text": "This sometimes leads to an incorrect alignment.", "labels": [], "entities": []}, {"text": "We have removed this constraint to make more flexible alignments possible.", "labels": [], "entities": []}, {"text": "Moreover, in their model, some function words are brought together, and thus they cannot handle the situation where each function word corresponds to a different part.", "labels": [], "entities": []}, {"text": "The smallest unit of our model is a single word, which should solve this problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel phrase alignment method which models word or phrase dependency relations in dependency tree structures of source and target languages.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7851867079734802}]}, {"text": "For a pair of correspondences which has a parent-child relation on one side, the dependency relation on the other side is defined as the relation between the two correspondences.", "labels": [], "entities": []}, {"text": "It is a kind of tree-based reordering model, and can capture non-local reorderings which sequential word-based models often cannot handle properly.", "labels": [], "entities": []}, {"text": "The model is also capable of estimating phrase correspondences automatically without heuristic rules.", "labels": [], "entities": [{"text": "estimating phrase correspondences", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.8038173119227091}]}, {"text": "The model is trained in two steps: Step 1 estimates word translation probabilities, and Step 2 estimates phrase translation probabilities and dependency relation probabilities.", "labels": [], "entities": [{"text": "word translation probabilities", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7802633742491404}, {"text": "phrase translation probabilities", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.7590480248133341}]}, {"text": "Both Step 1 and Step 2 are performed iteratively by the EM algorithm.", "labels": [], "entities": []}, {"text": "During the Step 2 iterations, word correspondences are grown into phrase correspondences.", "labels": [], "entities": []}], "datasetContent": [{"text": "A JST 1 Japanese-English paper abstract corpus consisting of 1M parallel sentences was used for the model training.", "labels": [], "entities": [{"text": "JST 1 Japanese-English paper abstract corpus", "start_pos": 2, "end_pos": 46, "type": "DATASET", "confidence": 0.7428524792194366}]}, {"text": "This corpus was constructed from a 2M Japanese-English paper abstract corpus by NICT 2 using the method of.", "labels": [], "entities": [{"text": "NICT 2", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.8914101719856262}]}, {"text": "As gold-standard data, we used 475 sentence pairs which were annotated by hand.", "labels": [], "entities": []}, {"text": "The annotations were only sure (S) alignments (there were no possible (P ) alignments) (.", "labels": [], "entities": []}, {"text": "The unit of evaluation was word-base for both Japanese and English.", "labels": [], "entities": []}, {"text": "We used precision, recall, and F-measure as evaluation criteria.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.999605119228363}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9991108775138855}, {"text": "F-measure", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.999064028263092}]}, {"text": "We conducted two experiments to reveal 1) the contribution of our proposed model compared to the existing models, and 2) the effectiveness of using dependency tree structure and phrases, which are larger alignment units than words.", "labels": [], "entities": []}, {"text": "Trainings were run on the original forms of words for both the proposed model and the models used for comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of alignment experiment.  Precision Recall  F  Step 1  77.55  33.92 47.20  Step 2-1  83.46  40.03 54.11  Step 2-2  87.74  45.37 59.81  Step 2-3  87.62  48.92 62.79  Step 2-4  86.87  50.42 63.81  Step 2-5  85.90  50.75 63.80  Step 2-6  85.54  51.00 63.90  Step 2-7  85.18  50.87 63.70  Step 2-8  84.66  50.75 63.46  intersection  90.34  34.28 49.71  grow-final-and  81.32  48.85 61.04  grow-diag-final-and  79.39  51.15 62.22", "labels": [], "entities": [{"text": "Precision Recall  F", "start_pos": 44, "end_pos": 63, "type": "METRIC", "confidence": 0.6416654686133066}]}, {"text": " Table 3: Effectiveness of dependency trees and phrases  (results after 5 iterations in Step 2.)  Precision Recall  F  proposed  85.54  51.00 63.90  dependency tree only  89.77  39.47 54.83  phrase only  84.41  47.33 60.65  none  85.07  38.06 52.59", "labels": [], "entities": [{"text": "Precision Recall  F", "start_pos": 98, "end_pos": 117, "type": "METRIC", "confidence": 0.7924995422363281}]}, {"text": " Table 3. All the  results are the alignment accuracy after 5 iterations  of Step 2.", "labels": [], "entities": [{"text": "alignment", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.6736355423927307}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8876006603240967}]}]}