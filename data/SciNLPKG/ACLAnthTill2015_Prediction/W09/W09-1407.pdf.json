{"title": [{"text": "High-precision biological event extraction with a concept recognizer", "labels": [], "entities": [{"text": "biological event extraction", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6366929312547048}]}], "abstractContent": [{"text": "We approached the problems of event detection , argument identification, and negation and speculation detection as one of concept recognition and analysis.", "labels": [], "entities": [{"text": "event detection", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.7829834520816803}, {"text": "argument identification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.831026017665863}, {"text": "negation and speculation detection", "start_pos": 77, "end_pos": 111, "type": "TASK", "confidence": 0.9143966883420944}, {"text": "concept recognition", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7588174939155579}]}, {"text": "Our methodology involved using the OpenDMAP semantic parser with manually-written rules.", "labels": [], "entities": [{"text": "OpenDMAP semantic parser", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6594350934028625}]}, {"text": "We achieved state-of-the-art precision for two of the three tasks, scoring the highest of 24 teams at precision of 71.81 on Task 1 and the highest of 6 teams at precision of 70.97 on Task 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9993021488189697}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9987455606460571}, {"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9972355961799622}]}, {"text": "The OpenDMAP system and the rule set are available at bionlp.sourceforge.net.", "labels": [], "entities": []}], "introductionContent": [{"text": "We approached the problem of biomedical event recognition as one of concept recognition and analysis.", "labels": [], "entities": [{"text": "biomedical event recognition", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.6869100332260132}, {"text": "concept recognition and analysis", "start_pos": 68, "end_pos": 100, "type": "TASK", "confidence": 0.7247104346752167}]}, {"text": "Concept analysis is the process of taking a textual input and building from it an abstract representation of the concepts that are reflected in it.", "labels": [], "entities": [{"text": "Concept analysis", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8597956001758575}]}, {"text": "Concept recognition can be equivalent to the named entity recognition task when it is limited to locating mentions of particular semantic types in text, or it can be more abstract when it is focused on recognizing predicative relationships, e.g. events and their participants.", "labels": [], "entities": [{"text": "Concept recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7869052588939667}, {"text": "named entity recognition task", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.7847603261470795}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate span  matching/approximate recursive matching table. GS = gold standard (true positives) (given for Tasks 1/3 only), answer  = all responses (true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure. All results are as  calculated by the official scoring application.", "labels": [], "entities": [{"text": "GS = gold standard (true positives)", "start_pos": 160, "end_pos": 195, "type": "METRIC", "confidence": 0.7986048310995102}, {"text": "recall", "start_pos": 297, "end_pos": 303, "type": "METRIC", "confidence": 0.9240908026695251}, {"text": "precision", "start_pos": 309, "end_pos": 318, "type": "METRIC", "confidence": 0.9901836514472961}, {"text": "F-measure", "start_pos": 324, "end_pos": 333, "type": "METRIC", "confidence": 0.9884762763977051}]}, {"text": " Table 3: Updated results on test data for Tasks 1-3, with important bug fixes in the code base. See key above.", "labels": [], "entities": []}]}