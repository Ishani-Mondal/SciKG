{"title": [{"text": "On the Robustness of Syntactic and Semantic Features for Automatic MT Evaluation", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9015485942363739}]}], "abstractContent": [{"text": "Linguistic metrics based on syntactic and semantic information have proven very effective for Automatic MT Evaluation.", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.8621420562267303}]}, {"text": "However, no results have been presented so far on their performance when applied to heavily ill-formed low quality translations.", "labels": [], "entities": []}, {"text": "In order to glean some light into this issue, in this work we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the paradigmatic case of speech translation between non-related languages.", "labels": [], "entities": [{"text": "speech translation between non-related languages", "start_pos": 196, "end_pos": 244, "type": "TASK", "confidence": 0.8477026224136353}]}, {"text": "Corroborating previous findings, we have verified that metrics based on deep linguistic analysis exhibit a very robust and stable behavior at the system level.", "labels": [], "entities": []}, {"text": "However , these metrics suffer a significant decrease at the sentence level.", "labels": [], "entities": []}, {"text": "This is in many cases attributable to a loss of recall, due to parsing errors or to alack of parsing at all, which maybe partially ameliorated by backing off to lexical similarity.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.99870765209198}]}], "introductionContent": [{"text": "Recently, there is a growing interest in the development of automatic evaluation metrics which exploit linguistic knowledge at the syntactic and semantic levels.", "labels": [], "entities": []}, {"text": "For instance, we may find metrics which compute similarities over shallow syntactic structures/sequences (, constituency trees (Liu and) and dependency trees (;.", "labels": [], "entities": []}, {"text": "We may also find metrics operating over shallow semantic structures, such as named entities and semantic roles (.", "labels": [], "entities": []}, {"text": "Linguistic metrics have been proven to produce more reliable system rankings than metrics limiting their scope to the lexical dimension, in particular when applied to test beds with a rich system typology, i.e., test beds in which there are automatic outputs produced by systems based on different paradigms, e.g., statistical, rule-based and human-aided ().", "labels": [], "entities": []}, {"text": "The reason is that they are able to capture deep MT quality distinctions which occur beyond the shallow level of lexical similarities.", "labels": [], "entities": [{"text": "MT quality distinctions", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.8962365587552389}]}, {"text": "However, these metrics have the limitation of relying on automatic linguistic processors, tools which are not equally available for all languages and whose performance may vary depending on the type of analysis conducted and the application domain.", "labels": [], "entities": []}, {"text": "Thus, it could be argued that linguistic metrics should suffer a significant quality drop when applied to a different translation domain, or to ill-formed sentences.", "labels": [], "entities": []}, {"text": "Clearly, metric scores computed on partial or wrong syntactic/semantic structures will be less informed.", "labels": [], "entities": []}, {"text": "But, should this necessarily lead to less reliable evaluations?", "labels": [], "entities": []}, {"text": "In this work, we have analyzed this issue by conducting a contrastive empirical study on the behavior of a heterogeneous set of metrics over several evaluation scenarios of decreasing translation quality.", "labels": [], "entities": []}, {"text": "In particular, we have studied the case of Chinese-to-English speech translation, which is a paradigmatic example of low quality and heavily ill-formed output.", "labels": [], "entities": [{"text": "Chinese-to-English speech translation", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.6185555557409922}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, prior to presenting experimental work, we describe the set of metrics employed in our experiments.", "labels": [], "entities": []}, {"text": "We also introduce a novel family of metrics which operate at the properly semantic level by analyzing similarities over discourse representations.", "labels": [], "entities": []}, {"text": "Experimental work is then presented in Section 3.", "labels": [], "entities": []}, {"text": "Metrics are evaluated both in terms of human likeness and human acceptability).", "labels": [], "entities": []}, {"text": "Finally, in Section 4, main conclusions are summarized and future work is outlined.", "labels": [], "entities": []}], "datasetContent": [{"text": "'DR' metrics analyze similarities between automatic and reference translations by comparing their respective DRSs.", "labels": [], "entities": []}, {"text": "These are automatically obtained using the C&C Tools (Clark and Curran, 2004) 2 . Sentences are first parsed on the basis of a combinatory categorial grammar ().", "labels": [], "entities": []}, {"text": "Then, the BOXER component ( extracts DRSs.", "labels": [], "entities": [{"text": "BOXER", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9925038814544678}]}, {"text": "As an illustration, shows the DRS representation for the sentence \"Every man loves Mary.\".", "labels": [], "entities": [{"text": "DRS", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.7050549387931824}]}, {"text": "The reader may find the output of the BOXER component (top) together with the equivalent first-order formula (bottom).", "labels": [], "entities": [{"text": "BOXER", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9841912984848022}]}, {"text": "DRS maybe viewed as semantic trees, which are built through the application of two types of DRS conditions:, in this case applied to DRSs instead of constituency trees.", "labels": [], "entities": []}, {"text": "All semantic subpaths in the candidate and the reference trees are retrieved.", "labels": [], "entities": []}, {"text": "The fraction of matching subpaths of a given length, l \u2208 [1..9], is computed.", "labels": [], "entities": []}, {"text": "Then, average accumulated scores up to a given length are retrieved.", "labels": [], "entities": []}, {"text": "For instance, 'DR-STM-4' corresponds to the average accumulated proportion of matching subpaths up to length-4.", "labels": [], "entities": []}, {"text": "DR-O r -t These metrics compute lexical overlapping 3 between discourse representation structures (i.e., discourse referents and discourse conditions) according to their type 't'.", "labels": [], "entities": []}, {"text": "For instance, 'DR-Or -pred' roughly reflects lexical overlapping between the referents associated to predicates (i.e., one-place properties), whereas 'DR-Or -imp' reflects lexical overlapping between referents associated to implication conditions.", "labels": [], "entities": []}, {"text": "We also introduce the 'DROr -\u22c6' metric, which computes average lexical overlapping overall DRS types.", "labels": [], "entities": []}, {"text": "DR-O rp -t These metrics compute morphosyntactic overlapping (i.e., between parts of speech associated to lexical items) between discourse representation structures of the same type t.", "labels": [], "entities": []}, {"text": "We also define the 'DR-Orp-\u22c6' metric, which computes average morphosyntactic overlapping overall DRS types.", "labels": [], "entities": []}, {"text": "Note that in the case of some complex conditions, such as implication or question, the respective order of the associated referents in the tree is important.", "labels": [], "entities": []}, {"text": "We take this aspect into account by making order information explicit in the construction of the semantic tree.", "labels": [], "entities": []}, {"text": "We also make explicit the type, symbol, value and date of conditions when these are applicable (e.g., predicates, relations, named entities, time expressions, cardinal expressions, or anaphoric conditions).", "labels": [], "entities": []}, {"text": "Finally, the extension to the evaluation setting based on multiple references is computed by assigning the maximum score attained against each individual reference.", "labels": [], "entities": []}, {"text": "Formally:  In this section, we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the case of speech translation between non-related languages.", "labels": [], "entities": [{"text": "speech translation between non-related languages", "start_pos": 152, "end_pos": 200, "type": "TASK", "confidence": 0.8536510109901428}]}, {"text": "We have used the test bed from the Chineseto-English translation task at the \"2006 Evaluation Campaign on Spoken Language Translation\" . The test set comprises 500 translation test cases corresponding to simple conversations (question/answer scenario) in the travel domain.", "labels": [], "entities": [{"text": "Chineseto-English translation task", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.69480828444163}, {"text": "Spoken Language Translation", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.5613445242245992}]}, {"text": "In addition, there are 3 different evaluation subscenarios of increasing translation difficulty, according to the translation source:  Our experiment requires a mechanism for evaluating the quality of evaluation metrics, i.e., a metaevaluation criterion.", "labels": [], "entities": []}, {"text": "The two most prominent are: \u2022 Human Acceptability: Metrics are evaluated in terms of their ability to capture the degree of acceptability to humans of automatic translations, i.e., their ability to emulate human assessors.", "labels": [], "entities": []}, {"text": "The underlying assumption is that good translations should be acceptable to human evaluators.", "labels": [], "entities": []}, {"text": "Human acceptability is usually measured on the basis of correlation between automatic metric scores and human assessments of translation quality.).", "labels": [], "entities": []}, {"text": "In this work, metrics are evaluated both in terms of human acceptability and human likeness.", "labels": [], "entities": []}, {"text": "In the case of human acceptability, metric quality is measured on the basis of correlation with human assessments both at the sentence and document (i.e., system) levels.", "labels": [], "entities": []}, {"text": "We compute Pearson correlation coefficients.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.661830335855484}]}, {"text": "The sum of adequacy and fluency is used as a global measure of quality.", "labels": [], "entities": []}, {"text": "Assessments from different judges have been averaged.", "labels": [], "entities": []}, {"text": "In the case of human likeness, we use the probabilistic KING measure defined inside the QARLA Framework ().", "labels": [], "entities": [{"text": "KING measure", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.9611452519893646}, {"text": "QARLA Framework", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9221993684768677}]}, {"text": "KING represents the probability, estimated over the set of test cases, that the score attained by a human reference is equal or greater than the score attained by any automatic translation.", "labels": [], "entities": [{"text": "KING", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9731625914573669}]}, {"text": "Although KING computations do not require human assessments, for the sake of comparison, we have limited to the set of test cases counting on human assessments.", "labels": [], "entities": [{"text": "KING computations", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8299436271190643}]}, {"text": "presents meta-evaluation results fora set of metric representatives from different linguistic levels over the three subscenarios defined ('CRR', 'ASR read' and 'ASR spont').", "labels": [], "entities": []}, {"text": "Highest scores in each column have been highlighted.", "labels": [], "entities": []}, {"text": "Lowest scores appear in italics.", "labels": [], "entities": [{"text": "Lowest", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9604511260986328}]}], "tableCaptions": [{"text": " Table 1: IWSLT 2006 MT Evaluation Campaign. Chinese-to-English test bed description", "labels": [], "entities": [{"text": "IWSLT 2006 MT Evaluation Campaign. Chinese-to-English test bed", "start_pos": 10, "end_pos": 72, "type": "DATASET", "confidence": 0.7648614711231656}]}, {"text": " Table 2: Meta-evaluation results for a set of metric representatives from different linguistic levels", "labels": [], "entities": []}, {"text": " Table 3: Meta-evaluation results. Improved sentence-level evaluation of SR and DR metrics", "labels": [], "entities": [{"text": "SR and DR metrics", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.5232159867882729}]}]}