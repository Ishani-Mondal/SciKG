{"title": [{"text": "Random Walks for Text Semantic Similarity", "labels": [], "entities": [{"text": "Text Semantic Similarity", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.5930519004662832}]}], "abstractContent": [{"text": "Many tasks in NLP stand to benefit from robust measures of semantic similarity for units above the level of individual words.", "labels": [], "entities": []}, {"text": "Rich semantic resources such as WordNet provide local semantic information at the lexical level.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.959433376789093}]}, {"text": "However, effectively combining this information to compute scores for phrases or sentences is an open problem.", "labels": [], "entities": []}, {"text": "Our algorithm aggregates local re-latedness information via a random walkover a graph constructed from an underlying lexical resource.", "labels": [], "entities": []}, {"text": "The stationary distribution of the graph walk forms a \"se-mantic signature\" that can be compared to another such distribution to get a relat-edness score for texts.", "labels": [], "entities": []}, {"text": "On a paraphrase recognition task, the algorithm achieves an 18.5% relative reduction in error rate over a vector-space baseline.", "labels": [], "entities": [{"text": "paraphrase recognition task", "start_pos": 5, "end_pos": 32, "type": "TASK", "confidence": 0.9001655379931132}, {"text": "relative reduction in error rate", "start_pos": 66, "end_pos": 98, "type": "METRIC", "confidence": 0.7236405730247497}]}, {"text": "We also show that the graph walk similarity between texts has complementary value as a feature for recognizing textual entailment, improving on a competitive baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language processing applications must directly or indirectly assess the semantic similarity of text passages.", "labels": [], "entities": []}, {"text": "Modern approaches to information retrieval, summarization, and textual entailment, among others, require robust numeric relevance judgments when a pair of texts is provided as input.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7791099846363068}, {"text": "summarization", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9851807355880737}, {"text": "textual entailment", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7286938279867172}]}, {"text": "Although each task demands its own scoring criteria, a simple lexical overlap measure such as cosine similarity of document vectors can often serve as a surprisingly powerful baseline.", "labels": [], "entities": []}, {"text": "We argue that there is room to improve these general-purpose similarity measures, particularly for short text passages.", "labels": [], "entities": []}, {"text": "Most approaches fall under one of two categories.", "labels": [], "entities": []}, {"text": "One set of approaches attempts to explicitly account for fine-grained structure of the two passages, e.g. by aligning trees or constructing logical forms for theorem proving.", "labels": [], "entities": [{"text": "theorem proving", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.8250550031661987}]}, {"text": "While these approaches have the potential for high precision on many examples, errors in alignment judgments or formula construction are often insurmountable.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9963566064834595}, {"text": "alignment judgments", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.933321088552475}, {"text": "formula construction", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.8916880488395691}]}, {"text": "More broadly, it's not always clear that there is a correct alignment or logical form that is most appropriate fora particular sentence pair.", "labels": [], "entities": []}, {"text": "The other approach tends to ignore structure, as canonically represented by the vector space model, where any lexical item in common between the two passages contributes to their similarity score.", "labels": [], "entities": []}, {"text": "While these approaches often fail to capture distinctions imposed by, e.g. negation, they do correctly capture abroad notion of similarity or aboutness.", "labels": [], "entities": []}, {"text": "This paper presents a novel variant of the vector space model of text similarity based on a random walk algorithm.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7088577002286911}]}, {"text": "Instead of comparing two bags-of-words directly, we compare the distribution each text induces when used as the seed of a random walkover a graph derived from WordNet and corpus statistics.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 159, "end_pos": 166, "type": "DATASET", "confidence": 0.9556309580802917}]}, {"text": "The walk posits the existence of a distributional particle that roams the graph, biased toward the neighborhood surrounding an input bag of words.", "labels": [], "entities": []}, {"text": "Eventually, the walk reaches a stationary distribution overall nodes in the graph, smoothing the peaked input distribution over a much larger semantic space.", "labels": [], "entities": []}, {"text": "Two such stationary distributions can be compared using conventional measures of vector similarity, producing a final relatedness score.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions.", "labels": [], "entities": []}, {"text": "We present a novel random graph walk algorithm", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the system on two tasks that might benefit from semantic similarity judgments: paraphrase recognition and recognizing textual entailment.", "labels": [], "entities": [{"text": "paraphrase recognition", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.9127081036567688}, {"text": "recognizing textual entailment", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.8247584700584412}]}, {"text": "A complete solution to either task will certainly require tools more tuned to linguistic structure; the paraphrase detection evaluation argues that the walk captures a useful notion of semantics at the sentence level.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.8688929378986359}]}, {"text": "The entailment system evaluation demonstrates that the walk score can improve a larger system that does make use of more fine-grained linguistic knowledge.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ranks of sample words in the distribu- tion for I ate a salad and spaghetti after a given  number of steps and at convergence. Words in the  vector are ordered by probability at time step t; the  word with the highest probability in the vector has  rank 1. \"-\" indicates that node had not yet been  reached.", "labels": [], "entities": []}, {"text": " Table 2: Three measures of distributional similar- ity between vectors x and y used to compare the  stationary distributions from passage-specific ran- dom walks. D(pq) is KL-divergence, defined as", "labels": [], "entities": [{"text": "D(pq)", "start_pos": 164, "end_pos": 169, "type": "METRIC", "confidence": 0.9288144111633301}]}, {"text": " Table 3: System performance on 1725 examples of  the MSR paraphrase detection test set. Accuracy  (micro-averaged F 1 ), F 1 for c 1 \"paraphrase\" and  c 0 \"non-paraphrase\" classes, and macro-averaged", "labels": [], "entities": [{"text": "MSR paraphrase detection test set", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.790071040391922}, {"text": "Accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9983758926391602}, {"text": "micro-averaged F 1 )", "start_pos": 100, "end_pos": 120, "type": "METRIC", "confidence": 0.7249834686517715}, {"text": "F 1", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9838175177574158}]}, {"text": " Table 5: Accuracy when the random walk is  added as a feature of an existing RTE system  (left column) under various distance metrics (right  columns).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983727931976318}]}]}