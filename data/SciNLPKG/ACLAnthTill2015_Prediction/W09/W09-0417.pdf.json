{"title": [{"text": "LIMSI's statistical translation systems for WMT'09", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.7490020394325256}, {"text": "WMT'09", "start_pos": 44, "end_pos": 50, "type": "TASK", "confidence": 0.652759313583374}]}], "abstractContent": [{"text": "This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.7438617547353109}, {"text": "WMT09 (en:fr) shared task", "start_pos": 73, "end_pos": 98, "type": "DATASET", "confidence": 0.7285981178283691}]}, {"text": "For this evaluation, we have developed four systems, using two different MT Toolkits: our primary submission , in both directions, is based on Moses, boosted with contextual information on phrases, and is contrasted with a conventional Moses-based system.", "labels": [], "entities": []}, {"text": "Additional contrasts are based on the Ncode toolkit, one of which uses (part of) the En-glish/French GigaWord parallel corpus.", "labels": [], "entities": [{"text": "En-glish/French GigaWord parallel corpus", "start_pos": 85, "end_pos": 125, "type": "DATASET", "confidence": 0.7226748913526535}]}], "introductionContent": [{"text": "This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.7438617547353109}, {"text": "WMT09 (en:fr) shared task", "start_pos": 73, "end_pos": 98, "type": "DATASET", "confidence": 0.7285981178283691}]}, {"text": "For this evaluation, we have developed four systems, using two different MT toolkits: our primary submission, in both direction, is based on Moses, boosted with contextual information on phrases; we also provided a contrast with a vanilla Moses-based system.", "labels": [], "entities": []}, {"text": "Additional contrasts are based on the N-code decoder, one of which takes advantage of (part of) the English/French GigaWord parallel corpus.", "labels": [], "entities": [{"text": "English/French GigaWord parallel corpus", "start_pos": 100, "end_pos": 139, "type": "DATASET", "confidence": 0.7011023312807083}]}], "datasetContent": [{"text": "One exciting novelty of this year's campaign was the availability of a very large parallel corpus for the en:fr pair, containing about 20M aligned sentences.", "labels": [], "entities": []}, {"text": "Our preliminary work consisted in selecting the most useful pairs of sentences, based on their average perplexity, as computed on our development language models.", "labels": [], "entities": []}, {"text": "The top ranking sentences (about 8M sentences) were then fed into the usual system development procedure: alignment, reordering (for the N-code system), phrase pair extraction, model estimation.", "labels": [], "entities": [{"text": "alignment", "start_pos": 106, "end_pos": 115, "type": "TASK", "confidence": 0.9578863382339478}, {"text": "phrase pair extraction", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.7715265154838562}, {"text": "model estimation", "start_pos": 177, "end_pos": 193, "type": "TASK", "confidence": 0.675356388092041}]}, {"text": "Given the unusual size of this corpus, each of these steps proved extremely resource intensive, and, for some systems, actually failed to complete.", "labels": [], "entities": []}, {"text": "Contrarily, the N-code systems, conceptually simpler, proved to scale nicely.", "labels": [], "entities": []}, {"text": "Given the very late availability of this corpus, our experiments were very limited and we eventually failed to deliver the test submissions of our \"GigaWord\" system.", "labels": [], "entities": []}, {"text": "Preliminary experiments using the N-code systems (see), however, showed a clear improvement of performance.", "labels": [], "entities": []}, {"text": "There is no reason to doubt that similar gains would be observed with the Moses systems.", "labels": [], "entities": []}, {"text": "The various systems presented above were all developed according to the same procedure: training used all the available parallel text; tuning was  Our primary submission corresponds to the +context entry, our first contrast to Moses+LargeLM, and our second contrast to Ncode+largeLM.", "labels": [], "entities": []}, {"text": "Due to lack of time, no official submission was submitted for the +giga variant.", "labels": [], "entities": []}, {"text": "For the record, the score we eventually obtained on the test corpus was 26.81, slightly better than our primary submission which obtained a score of 25.74 (all these numbers were computed on the complete test set).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora used to train the target language  models in English and French.", "labels": [], "entities": []}, {"text": " Table 2: Results on the devtest set", "labels": [], "entities": []}]}