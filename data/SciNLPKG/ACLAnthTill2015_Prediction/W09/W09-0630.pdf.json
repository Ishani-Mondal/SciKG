{"title": [{"text": "Realizing the Costs: Template-Based Surface Realisation in the GRAPH Approach to Referring Expression Generation", "labels": [], "entities": [{"text": "Template-Based Surface Realisation", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.6132651269435883}, {"text": "GRAPH Approach", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.7286020219326019}, {"text": "Referring Expression Generation", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.8699414531389872}]}], "abstractContent": [{"text": "We describe anew realiser developed for the TUNA 2009 Challenge, and present its evaluation scores on the development set, showing a clear increase in performance compared to last year's simple realiser.", "labels": [], "entities": [{"text": "TUNA 2009 Challenge", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.8953306873639425}]}], "introductionContent": [{"text": "The TUNA Challenge 2009 is the last in a series of challenges using the TUNA corpus of referring expressions () for comparative evaluation of referring expression generation.", "labels": [], "entities": [{"text": "TUNA Challenge 2009", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8408916195233663}, {"text": "TUNA corpus of referring expressions", "start_pos": 72, "end_pos": 108, "type": "DATASET", "confidence": 0.938728928565979}, {"text": "referring expression generation", "start_pos": 142, "end_pos": 173, "type": "TASK", "confidence": 0.6417098343372345}]}, {"text": "Challenge is aimed at end-to-end referring expression generation, which encompasses two subtasks: (1) attribute selection, choosing a number of attributes that uniquely characterize a target object, distinguishing it from other objects in a visual scene, and (2) realisation, converting the selected set of attributes into a word string.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6247126857439677}]}, {"text": "Our contributions to the previous Challenges focused on subtask (1), but this year we focus on subtask (2).", "labels": [], "entities": []}, {"text": "Below, we briefly sketch how attribute selection is performed in our system, describe our newly developed realiser, and present our evaluation results on the TUNA 2009 development set.", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.687247171998024}, {"text": "TUNA 2009 development set", "start_pos": 158, "end_pos": 183, "type": "DATASET", "confidence": 0.9657775014638901}]}], "datasetContent": [{"text": "System performance is measured by comparing the generated word strings to the human descrip-: Results on the 2009 development set (between brackets are those using last year's realiser).", "labels": [], "entities": [{"text": "2009 development set", "start_pos": 109, "end_pos": 129, "type": "DATASET", "confidence": 0.6408363779385885}]}, {"text": "tions in the TUNA development set, comprising 80 furniture and 68 people descriptions.", "labels": [], "entities": [{"text": "TUNA development set", "start_pos": 13, "end_pos": 33, "type": "DATASET", "confidence": 0.9436911543210348}]}, {"text": "The evaluation measures reported here are mean edit distance (MED), the mean of the token-based Levenshtein edit distance between the reference word strings and the system word strings, mean normalised edit distance (MNED), where the edit distance is normalised by the number of tokens, and cumulative BLEU 3 score.", "labels": [], "entities": [{"text": "mean edit distance (MED)", "start_pos": 42, "end_pos": 66, "type": "METRIC", "confidence": 0.9397290349006653}, {"text": "mean normalised edit distance (MNED)", "start_pos": 186, "end_pos": 222, "type": "METRIC", "confidence": 0.9237991486276899}, {"text": "BLEU 3 score", "start_pos": 302, "end_pos": 314, "type": "METRIC", "confidence": 0.9813981850941976}]}, {"text": "For comparison, we also provide the results obtained when using last year's simple realiser, which we reimplemented in Java.", "labels": [], "entities": []}, {"text": "We see a clear improvement when we compare the performance of the new and the old realiser, in particular in the people domain.", "labels": [], "entities": []}, {"text": "However, further evaluation experiments are required to determine whether the improvements are mostly due to our use of templates derived from human descriptions, or to the simple improvements in lexical choice incorporated in the rules used as fall-back in case no matching templates are found.", "labels": [], "entities": []}, {"text": "To further improve the realiser, we need to add templates for all remaining attribute combinations found in the corpus.", "labels": [], "entities": []}, {"text": "This should not be difficult, as the set-up of the realiser allows easy creation of templates.", "labels": [], "entities": []}, {"text": "It should also be easily portable to other languages; in fact we intend to explore its use for the realisation of referring expressions in Dutch.", "labels": [], "entities": []}], "tableCaptions": []}