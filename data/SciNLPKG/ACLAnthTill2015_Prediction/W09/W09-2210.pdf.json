{"title": [{"text": "On Semi-Supervised Learning of Gaussian Mixture Models for Phonetic Classification *", "labels": [], "entities": [{"text": "Phonetic Classification", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7848283350467682}]}], "abstractContent": [{"text": "This paper investigates semi-supervised learning of Gaussian mixture models using an unified objective function taking both labeled and unlabeled data into account.", "labels": [], "entities": []}, {"text": "Two methods are compared in this work-the hybrid dis-criminative/generative method and the purely generative method.", "labels": [], "entities": []}, {"text": "They differ in the criterion type on labeled data; the hybrid method uses the class posterior probabilities and the purely generative method uses the data likelihood.", "labels": [], "entities": []}, {"text": "We conducted experiments on the TIMIT database and a standard synthetic data set from UCI Machine Learning repository.", "labels": [], "entities": [{"text": "TIMIT database", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9289427101612091}, {"text": "UCI Machine Learning repository", "start_pos": 86, "end_pos": 117, "type": "DATASET", "confidence": 0.9406303018331528}]}, {"text": "The results show that the two methods behave similarly in various conditions.", "labels": [], "entities": []}, {"text": "For both methods, unlabeled data improve training on models of higher complexity in which the supervised method performs poorly.", "labels": [], "entities": []}, {"text": "In addition, there is a trend that more unlabeled data results in more improvement in classification accuracy over the supervised model.", "labels": [], "entities": [{"text": "classification", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.9452835917472839}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9227668642997742}]}, {"text": "We also provided experimental observations on the relative weights of labeled and unlabeled parts of the training objective and suggested a critical value which could be useful for selecting a good weighing factor.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech recognition acoustic models can be trained using untranscribed speech data; L..", "labels": [], "entities": [{"text": "Speech recognition acoustic", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8273535370826721}]}, {"text": "Most such experiments begin by boostraping * This research is funded by NSF grants an initial acoustic model using a limited amount of manually transcribed data (normally in a scale from 30 minutes to several hours), and then the initial model is used to transcribe a relatively large amount of untranscribed data.", "labels": [], "entities": [{"text": "NSF", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8828417062759399}]}, {"text": "Only the transcriptions with high confidence measures (; L. or high agreement with closed captions () are selected to augment the manually transcribed data, and new acoustic models are trained on the augmented data set.", "labels": [], "entities": []}, {"text": "The general procedure described above exactly lies in the context of semi-supervised learning problems and can be categorized as a self-training algorithm.", "labels": [], "entities": []}, {"text": "Self-training is probably the simplest semisupervised learning method, but it is also flexible to be applied to complex classifiers such as speech recognition systems.", "labels": [], "entities": []}, {"text": "This maybe the reason why little work has been done on exploiting other semisupervised learning methods in speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8737798035144806}]}, {"text": "Though not incorporated to speech recognizers yet, there has been some work on semi-supervised learning of Hidden Markov Models (HMM) for sequential classification.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.6882421523332596}, {"text": "sequential classification", "start_pos": 138, "end_pos": 163, "type": "TASK", "confidence": 0.8411564826965332}]}, {"text": "Inoue and Ueda (2003) treated the unknown class labels of the unlabeled data as hidden variables and used the expectation-maximization (EM) algorithm to optimize the joint likelihood of labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "Recently applied a homotopy method to select the optimal weight to balance between the log likelihood of labeled and unlabeled data when training HMMs.", "labels": [], "entities": []}, {"text": "Besides generative training of acoustic models, discriminative training is another popular paradigm in the area of speech recognition, but only when the transcriptions are available.", "labels": [], "entities": [{"text": "generative training", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.9345672726631165}, {"text": "speech recognition", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7837658226490021}]}, {"text": "used the self-training method to augment the training set for discriminative training.", "labels": [], "entities": []}, {"text": "investigated another use of discriminative information from labeled data by replacing the likelihood of labeled data with the class posterior probability of labeled data in the semi-supervised training objective for Gaussian Mixture Models (GMM), resulting in a hybrid discriminative/generative objective function.", "labels": [], "entities": []}, {"text": "Their experimental results in binary phonetic classification showed significant improvement in classification accuracy when labeled data are scarce.", "labels": [], "entities": [{"text": "binary phonetic classification", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.7171717683474222}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.966202974319458}]}, {"text": "A similar strategy called \"'multi-conditional learning\"' was presented in) applied to Markov Random Field models for text classification tasks, with the difference that the likelihood of labeled data is also included in the objective.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.8429357012112936}]}, {"text": "The hybrid discriminative/generative objective function can be interpreted as having an extra regularization term, the likelihood of unlabeled data, in the discriminative training criterion for labeled data.", "labels": [], "entities": []}, {"text": "However, both methods in and) encountered the same issue about determining the weights for labeled and unlabeled part in the objective function and chose to use a development set to select the optimal weight.", "labels": [], "entities": []}, {"text": "This paper provides an experimental analysis on the effect of the weight.", "labels": [], "entities": []}, {"text": "With the ultimate goal of applying semisupervised learning in speech recognition, this paper investigates the learning capability of algorithms within Gaussian Mixture Models because GMM is the basic model inside a HMM, therefore 1) the update equations derived for the parameters of GMM can be conveniently extended to HMM for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7780208885669708}, {"text": "speech recognition", "start_pos": 328, "end_pos": 346, "type": "TASK", "confidence": 0.7947332561016083}]}, {"text": "2) GMM can serve as an initial point to help us understand more details about the semisupervised learning process of spectral features.", "labels": [], "entities": [{"text": "GMM", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.7984127998352051}]}, {"text": "This paper makes the following contribution: \u2022 it provides an experimental comparison of hybrid and purely generative training objectives.", "labels": [], "entities": []}, {"text": "\u2022 it studies the impact of model complexity on learning capability of algorithms.", "labels": [], "entities": []}, {"text": "\u2022 it studies the impact of the amount of unlabeled data on learning capability of algorithms.", "labels": [], "entities": []}, {"text": "\u2022 it analyzes the role of the relative weights of labeled and unlabeled parts of the training objective.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The accuracies(%) of the initial MLE model, the supervised model (\u03b1 = 0), the best accuracies with unlabeled  data and the absolute improvements (\u2206) over \u03b1 = 0 for different model complexities for waveform. The bolded  number is the highest value along the same column.", "labels": [], "entities": []}, {"text": " Table 2: The accuracies(%) of the initial MLE model, the supervised model (\u03b1 = 0), the best accuracies with unlabeled  data and the absolute improvements (\u2206) over \u03b1 = 0 for different model complexities for TIMIT. The bolded number  is the highest value along the same column.", "labels": [], "entities": []}, {"text": " Table 3: The critical values for waveform and TIMIT  for different sizes of labeled data (percentage of training  data) with a fixed set of unlabeled data (80 %.)", "labels": [], "entities": [{"text": "TIMIT", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9988229870796204}]}]}