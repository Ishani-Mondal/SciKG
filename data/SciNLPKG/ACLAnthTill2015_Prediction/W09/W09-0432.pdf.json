{"title": [{"text": "Domain Adaptation for Statistical Machine Translation with Monolingual Resources", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6841841638088226}, {"text": "Statistical Machine Translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.843150814374288}]}], "abstractContent": [{"text": "Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8140096664428711}, {"text": "statistical machine translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.6767783264319102}]}, {"text": "The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.", "labels": [], "entities": []}, {"text": "Previous work showed small performance gains by adapting from limited in-domain bilingual data.", "labels": [], "entities": []}, {"text": "Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language.", "labels": [], "entities": []}, {"text": "We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language.", "labels": [], "entities": []}, {"text": "Investigations were conducted on a state-of-the-art phrase-based system trained on the Spanish-English part of the UN corpus , and adapted on the corresponding Europarl data.", "labels": [], "entities": [{"text": "UN corpus", "start_pos": 115, "end_pos": 124, "type": "DATASET", "confidence": 0.756531685590744}, {"text": "Europarl data", "start_pos": 160, "end_pos": 173, "type": "DATASET", "confidence": 0.9933184385299683}]}, {"text": "Translation, reordering , and language models were estimated after translating in-domain texts with the base-line.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.949138343334198}]}, {"text": "By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9821158349514008}]}], "introductionContent": [{"text": "A well-known problem of Statistical Machine Translation (SMT) is that performance quickly degrades as soon as testing conditions deviate from training conditions.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8973430196444193}]}, {"text": "The very simple reason is that the underlying statistical models always tend to closely approximate the empirical distributions of the training data, which typically consist of bilingual texts and monolingual target-language texts.", "labels": [], "entities": []}, {"text": "The former provide a means to learn likely translations pairs, the latter to form correct sentences with translated words.", "labels": [], "entities": []}, {"text": "Besides the general difficulties of language translation, which we do not consider here, there are two aspects that make machine learning of this task particularly hard.", "labels": [], "entities": [{"text": "language translation", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7567123472690582}]}, {"text": "First, human language has intrinsically very sparse statistics at the surface level, hence gaining complete knowledge on translation phrase pairs or target language n-grams is almost impractical.", "labels": [], "entities": []}, {"text": "Second, language is highly variable with respect to several dimensions, style, genre, domain, topics, etc.", "labels": [], "entities": []}, {"text": "Even apparently small differences in domain might result in significant deviations in the underlying statistical models.", "labels": [], "entities": []}, {"text": "While data sparseness corroborates the need of large language samples in SMT, linguistic variability would indeed suggest to consider many alternative data sources as well.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.98826664686203}]}, {"text": "By rephrasing a famous saying we could say that \"no data is better than more and assorted data\".", "labels": [], "entities": []}, {"text": "The availability of language resources for SMT has dramatically increased over the last decade, at least fora subset of relevant languages and especially for what concerns monolingual corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9953085780143738}]}, {"text": "Unfortunately, the increase in quantity has not gone in parallel with an increase in assortment, especially for what concerns the most valuable resource, that is bilingual corpora.", "labels": [], "entities": [{"text": "quantity", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9550817608833313}]}, {"text": "Large parallel data available to the research community are for the moment limited to texts produced by international organizations (European Parliament, United Nations, Canadian Hansard), press agencies, and technical manuals.", "labels": [], "entities": [{"text": "Canadian Hansard)", "start_pos": 170, "end_pos": 187, "type": "DATASET", "confidence": 0.8009049296379089}]}, {"text": "The limited availability of parallel data poses challenging questions regarding the portability of SMT across different application domains and language pairs, and its adaptability with respect to language variability within the same application domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9816675186157227}]}, {"text": "This work focused on the second issue, namely the adaptation of a Spanish-to-English phrasebased SMT system across two apparently close domains: the United Nation corpus and the Euro-pean Parliament corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.821290135383606}, {"text": "United Nation corpus", "start_pos": 149, "end_pos": 169, "type": "DATASET", "confidence": 0.9072493513425192}, {"text": "Euro-pean Parliament corpus", "start_pos": 178, "end_pos": 205, "type": "DATASET", "confidence": 0.9628878831863403}]}, {"text": "Cross-domain adaptation is faced under the assumption that only monolingual texts are available, either in the source language or in the target language.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents previous work on the problem of adaptation in SMT; Section 3 introduces the exemplar task and research questions we addressed; Section 4 describes the SMT system and the adaptation techniques that were investigated; Section 5 presents and discusses experimental results; and Section 6 provides conclusions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9118421077728271}, {"text": "SMT", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.9876835942268372}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of bilingual training corpora,  development and test data (after tokenization).", "labels": [], "entities": [{"text": "tokenization", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.9602502584457397}]}, {"text": " Table 3: Global time, not including decoding, of  the tuning process and BLEU score achieved on  the test set by the uniform interpolation weights  (first row), and by the optimal weights with differ- ent configurations of the tuning parameters.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9757087230682373}]}, {"text": " Table 2: Description and performance on the test set of compared systems in terms of perplexity, out-of- vocabulary percentage of their language model, and four translation scores: BLEU, NIST, word-error- rate, and position-independent error rate. Systems were optimized on the dev2006 development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9992738366127014}, {"text": "NIST", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.7749282717704773}, {"text": "word-error- rate", "start_pos": 194, "end_pos": 210, "type": "METRIC", "confidence": 0.8415706157684326}, {"text": "position-independent error rate", "start_pos": 216, "end_pos": 247, "type": "METRIC", "confidence": 0.673550546169281}, {"text": "dev2006 development set", "start_pos": 279, "end_pos": 302, "type": "DATASET", "confidence": 0.845866858959198}]}]}