{"title": [], "abstractContent": [{"text": "In this paper we describe word alignment experiments using an approach based on a dis-junctive combination of alignment evidence.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7694766819477081}]}, {"text": "A wide range of statistical, orthographic and posi-tional clues can be combined in this way.", "labels": [], "entities": []}, {"text": "Their weights can easily be learned from small amounts of hand-aligned training data.", "labels": [], "entities": []}, {"text": "We can show that this \"evidence-based\" approach can be used to improve the baseline of statistical alignment and also outperforms a discriminative approach based on a maximum entropy classifier.", "labels": [], "entities": [{"text": "statistical alignment", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.811178058385849}]}], "introductionContent": [{"text": "Automatic word alignment has received a lot of attention mainly due to the intensive research on statistical machine translation.", "labels": [], "entities": [{"text": "Automatic word alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6379531919956207}, {"text": "statistical machine translation", "start_pos": 97, "end_pos": 128, "type": "TASK", "confidence": 0.6775747239589691}]}, {"text": "However, parallel corpora and word alignment are not only useful in that field but maybe applied to various tasks such as computer aided language learning (see for example) and bilingual terminology extraction (for example).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7927034795284271}, {"text": "bilingual terminology extraction", "start_pos": 177, "end_pos": 209, "type": "TASK", "confidence": 0.7035683393478394}]}, {"text": "The automatic alignment of corresponding words in translated sentences is a challenging task even for small translation units as the following Dutch-English example tries to illustrate.", "labels": [], "entities": [{"text": "automatic alignment of corresponding words in translated sentences", "start_pos": 4, "end_pos": 70, "type": "TASK", "confidence": 0.7830851636826992}]}], "datasetContent": [{"text": "For our experiments we will use well-known data sets that have been used before for word alignment experiments.", "labels": [], "entities": [{"text": "word alignment experiments", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.8656819462776184}]}, {"text": "Most related work on supervised alignment models reports results on the French-English data set from the shared task at WPT03 derived from the parallel Canadian Hansards corpus.", "labels": [], "entities": [{"text": "French-English data set", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.8039136032263438}, {"text": "WPT03", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.9462659358978271}, {"text": "Canadian Hansards corpus", "start_pos": 152, "end_pos": 176, "type": "DATASET", "confidence": 0.8641479015350342}]}, {"text": "This data set caused a lot of discussion especially because of the flaws in evaluation measures used for word alignment experiments.", "labels": [], "entities": [{"text": "word alignment experiments", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.8607184290885925}]}, {"text": "Therefore, we will apply this set for training purposes only (447 aligned sentences with 4,038 sure (S) links and 13,400 (P ) possible links) and stick to another set for evaluation.", "labels": [], "entities": []}, {"text": "This set includes English-French word alignment data for 100 sentences from the Europarl corpus with a much smaller number of possible links (437 compared to 1,009 sure links) which hopefully leads to more reliable results.", "labels": [], "entities": [{"text": "English-French word alignment", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.6033673485120138}, {"text": "Europarl corpus", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.9931070506572723}]}, {"text": "Some of the alignment clues require large parallel corpora for estimating reliable feature values (for example co-occurrence measures).", "labels": [], "entities": []}, {"text": "For training we use the Canadian Hansards as provided for the WPT03 workshop and for evaluation these values are taken from the Europarl corpus.", "labels": [], "entities": [{"text": "Canadian Hansards", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.7726675271987915}, {"text": "WPT03 workshop", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.8788901567459106}, {"text": "Europarl corpus", "start_pos": 128, "end_pos": 143, "type": "DATASET", "confidence": 0.9964834749698639}]}, {"text": "For evaluation we use the standard measures used in related research: For the F-measure we give balanced values and also unbalanced F-values with \u03b1 = 0.4.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.7494510412216187}, {"text": "F-values", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9102868437767029}]}, {"text": "The latter is supposed to show a better correlation with BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9991921782493591}]}, {"text": "However, we did not perform any tests with statistical MT using our alignment techniques to verify this for the data we have used.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9692050218582153}]}, {"text": "For comparison we use the IBM model 4 alignments and the intersection and grow-diag-final-and symmetrizaton heuristics as implemented in the Moses toolkit.", "labels": [], "entities": []}, {"text": "We also compare our results with a discriminative alignment approach using the same alignment search algorithm, the same features and a global maximum entropy classifier trained on the same training data (using default settings of the megam toolkit).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Overview of results:  Statistical  word alignment derived from GIZA++/Moses  (intersection/grow-diag-final), discriminative word  alignment using a maximum entropy classifier  (MaxEnt), and the evidence-based alignment (Clues).", "labels": [], "entities": [{"text": "Statistical  word alignment", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6400730212529501}, {"text": "GIZA++/Moses", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.7630959947903951}, {"text": "discriminative word  alignment", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.692379355430603}]}, {"text": " Table 2: Examples of weights learned from prediction  precision of individual clues.", "labels": [], "entities": []}]}