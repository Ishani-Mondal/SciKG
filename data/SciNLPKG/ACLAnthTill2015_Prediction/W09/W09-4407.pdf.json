{"title": [], "abstractContent": [{"text": "Example sentences provide an intuitive means of grasping the meaning of a word, and are frequently used to complement conventional word definitions.", "labels": [], "entities": [{"text": "grasping the meaning of a word", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.8637794554233551}]}, {"text": "When a word has multiple meanings, it is useful to have example sentences for specific senses (and hence definitions) of that word rather than indiscriminately lumping all of them together.", "labels": [], "entities": []}, {"text": "In this paper, we investigate to what extent such sense-specific example sentences can be extracted from parallel corpora using lexical knowledge bases for multiple languages as a sense index.", "labels": [], "entities": []}, {"text": "We use word sense disambiguation heuris-tics and a cross-lingual measure of semantic similarity to link example sentences to specific word senses.", "labels": [], "entities": [{"text": "word sense disambiguation heuris-tics", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.666257955133915}]}, {"text": "From the sentences found fora given sense, an algorithm then selects a smaller subset that can be presented to end users, taking into account both representativeness and diversity.", "labels": [], "entities": []}, {"text": "Preliminary results show that a precision of around 80% can be obtained fora reasonable number of word senses, and that the subset selection yields convincing results.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9988508224487305}]}], "introductionContent": [{"text": "Many dictionaries provide not only definitions but also short sentences that demonstrate how a given word is used in context.", "labels": [], "entities": []}, {"text": "Linguists and average dictionary users alike appreciate genuine examples of a word being employed in a sentence.", "labels": [], "entities": []}, {"text": "Goal An example sentence fora word sense is any genuine sentence that contains that word being used in the respective sense.", "labels": [], "entities": []}, {"text": "A set of example sentences may (1) allow the user to grasp a word's meaning, and see in what circumstances a word would typically be used in practice.", "labels": [], "entities": []}, {"text": "The first aspect is relevant because traditional intensional word definitions maybe too abstract or even confusing to users of a dictionary.", "labels": [], "entities": []}, {"text": "Often, the meaning of a word can be determined from its context.", "labels": [], "entities": []}, {"text": "In conjunction with conventional definitions, example sentences may allow users to verify whether they have correctly interpreted a definition.", "labels": [], "entities": []}, {"text": "The second aspect is relevant since example sentences may reveal possible contexts a word can be used in.", "labels": [], "entities": []}, {"text": "For instance, synonymous words such as 'child' and 'youngster' can have the same meaning, yet be used in somewhat different contexts.", "labels": [], "entities": []}, {"text": "Examples provide evidence of typical collocations and expressions, e.g. the word 'birth' often occurs as in 'to give birth' or 'birth rate' (but not *'to give nascence' or *'nascence rate').", "labels": [], "entities": []}, {"text": "For this reason, dictionaries typically include not only conventional definitions, but also example sentences that convey additional information about the meaning of a word.", "labels": [], "entities": []}, {"text": "These are often short, limited in number, and in some dictionaries elicited rather than genuine.", "labels": [], "entities": []}, {"text": "Hence, retrieving further example sentences can be helpful for lexicographical purposes, or to make the meanings and use more clear to language learners and other laypeople.", "labels": [], "entities": []}, {"text": "In modern digital dictionaries, the tight space constraints of print media no longer apply, and thus a larger number of example sentences can be presented to the user on demand.", "labels": [], "entities": []}, {"text": "Our aim is to automatically obtain a set of sensedisambiguated example sentences that are known to mention a specific sense of a word.", "labels": [], "entities": []}, {"text": "For instance, fora polysemous word such as 'bat', we would like to obtain a set of example sentences that refer to the animal sense (e.g. 'There were many bats flying out of the cave.'), and, separately, a list of example sentences that mention the word in its sports sense (e.g. 'In professional baseball, only wooden bats are permitted.').", "labels": [], "entities": []}, {"text": "When a user browses a digital dictionary or lexical database, the example sentences could then be provided together with the relevant definitions of the word.", "labels": [], "entities": []}, {"text": "Even in digital media, however, more examples maybe available than can initially be displayed.", "labels": [], "entities": []}, {"text": "For this reason, a means of choosing a restricted set of particularly representative example sentences is an additional requirement.", "labels": [], "entities": []}, {"text": "Contribution Our approach consists of two major building blocks that address the two issues just described.", "labels": [], "entities": []}, {"text": "The first step (Section 2) involves extracting the sense-disambiguated example sentences from a parallel corpus by harnessing cross-lingual information to aid in assigning sentences to word senses.", "labels": [], "entities": []}, {"text": "The second step (Section 3) selects a limited set of particularly representative example sentences for each word sense, using an algorithm that assesses the contributions made by individual sentences.", "labels": [], "entities": []}, {"text": "We provide preliminary experimental results in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We generated sense-disambiguated example sentences for several setups, and evaluated random samples by assessing whether or not the word was indeed used in the sense determined by our method.", "labels": [], "entities": []}, {"text": "The results were generalized using Wilson score intervals, and are presented in.", "labels": [], "entities": []}, {"text": "The smoothing parameter \u03b1 from Section 2 was set to 0.3.", "labels": [], "entities": []}, {"text": "In, we provide a few anecdotic examples of the output.", "labels": [], "entities": []}, {"text": "In general, this approach yields high-quality example sentences compared to current systems for monolingual text.", "labels": [], "entities": []}, {"text": "Automatic word alignment is known to be error-prone, and many heuristics have been proposed to mitigate the effects of this, e.g. aligning in both directions and then intersecting the alignment.", "labels": [], "entities": [{"text": "Automatic word alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5960927605628967}]}, {"text": "In our setting, incorrect alignments are unlikely to lead to incorrect example sentences.", "labels": [], "entities": []}, {"text": "This is because two erroneously aligned words inmost cases have very different meanings and hence are unlikely to share a semantically similar word sense.", "labels": [], "entities": []}, {"text": "The main cause of the inaccuracies we encountered instead turned out to be the sense inventory's incompleteness.", "labels": [], "entities": []}, {"text": "For instance, when an English word has multiple senses shared by the aligned Spanish word, but the sense inventory only lists one of those senses for the Spanish word, our method would lead us to believe that that sense is the right one with high certainty.", "labels": [], "entities": []}, {"text": "On a few occasions, incorrect output by the morphological analyser induced errors.", "labels": [], "entities": []}, {"text": "For example, when the word 'shed' was labelled a verb although it was used as a noun, the wrong sense was selected.", "labels": [], "entities": []}, {"text": "A drawback of our approach is that the number of word senses covered is limited.", "labels": [], "entities": []}, {"text": "To some degree, this can be addressed by using larger corpora and more language combinations.", "labels": [], "entities": []}, {"text": "A reasonably full level of coverage of the senses listed in WordNet would however likely also require relaxing the scoring functions to take into account also less obvious (and hence less reliable) input sentences.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9720585942268372}]}, {"text": "We also applied the sentence selection approach described in Section 3.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.8435574471950531}]}, {"text": "provides ranked lists of example sentences created using Algorithm 3.1.", "labels": [], "entities": []}, {"text": "It is clear that frequent collocations such as 'right side', 'electrical current', and 'when nightfall comes' are given a high weight.", "labels": [], "entities": []}, {"text": "We also see at least one example sentence wrongly associated with a sense ('convey').", "labels": [], "entities": []}, {"text": "Since the algorithm does not depend on sense-disambiguated example sentences, we additionally show sentences from the monolingual RCV1 corpus in.", "labels": [], "entities": [{"text": "RCV1 corpus", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.8859703540802002}]}, {"text": "A larger number of example sentences is typically available here, so the algorithm suceeds even better at choosing sentences that highlight typical collocations, e.g. 'long term', 'a long time' for the word 'long', or 'colonial rule' and 'colonial power ' for 'colonial'.", "labels": [], "entities": []}, {"text": "The RCV1 corpus is strongly biased towards the financial domain, which is reflected in the example sentences chosen by the algorithm.", "labels": [], "entities": [{"text": "RCV1 corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9539714157581329}]}, {"text": "line (something, as a cord or rope, that is long and thin and flexible) I got some fishing line if you want me to stitch that.", "labels": [], "entities": []}, {"text": "Von Sefelt, get the stern line.", "labels": [], "entities": []}, {"text": "line (the descendants of one individual) What line of kings do you descend from?", "labels": [], "entities": []}, {"text": "catch (catch up with and possibly overtake) He's got 100 laps to catch Beau Brandenburg if he wants to become world champion.", "labels": [], "entities": [{"text": "catch", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9680478572845459}]}, {"text": "They won't catch up. catch (grasp with the mind or develop I didn't catch your name. an understanding of) Sorry, I didn't catch it. talk (exchange thoughts, talk with) Why don't we have a seat and talk it over.", "labels": [], "entities": []}, {"text": "Okay I'll talk to you but one condition...", "labels": [], "entities": []}, {"text": "talk (use language) But we'll be listening from the kitchen so talk loud.", "labels": [], "entities": []}, {"text": "You spit when you talk.", "labels": [], "entities": []}, {"text": "opening (a ceremony accompanying the start of some enterprise) We don't have much time until the opening day of Exhibition.", "labels": [], "entities": [{"text": "Exhibition", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.935633659362793}]}, {"text": "What a disaster tomorrow is the opening ceremony!", "labels": [], "entities": []}, {"text": "opening (the first performance, as of a theatrical production) It will be rehearsed in the morning ready for the opening tomorrow night.", "labels": [], "entities": []}, {"text": "You ready for our big opening night?", "labels": [], "entities": []}, {"text": "The approach could also be extended to simultaneously consider aligned sentences from more than two languages to harness example sentences when individual alignments of two languages do not provide enough information fora reliable disambiguation.", "labels": [], "entities": []}, {"text": "For sentence selection, one could consider investigating additional input information for the algorithm, e.g. sentence lengths.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8054038286209106}]}], "tableCaptions": [{"text": " Table 1: Number and Accuracy of sense-disambiguated example sentences", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9348815679550171}, {"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8769380450248718}]}]}