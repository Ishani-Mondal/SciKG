{"title": [{"text": "Improving Statistical Machine Translation Using Domain Bilingual Multiword Expressions", "labels": [], "entities": [{"text": "Improving Statistical Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8745937347412109}]}], "abstractContent": [{"text": "Multiword expressions (MWEs) have been proved useful for many natural language processing tasks.", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7464107990264892}]}, {"text": "However, how to use them to improve performance of statistical machine translation (SMT) is not well studied.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 51, "end_pos": 88, "type": "TASK", "confidence": 0.8192080358664194}]}, {"text": "This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions.", "labels": [], "entities": [{"text": "extract domain bilingual multiword expressions", "start_pos": 55, "end_pos": 101, "type": "TASK", "confidence": 0.7748258829116821}]}, {"text": "In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-of-the-art phrase-based machine translation system.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.6431247194608053}]}, {"text": "Experiments show that bilingual MWEs could improve translation performance significantly.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.7787488698959351}, {"text": "translation", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9650678634643555}]}], "introductionContent": [{"text": "Phrase-based machine translation model has been proved a great improvement over the initial wordbased approaches.", "labels": [], "entities": [{"text": "Phrase-based machine translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.70168004433314}]}, {"text": "Recent syntax-based models perform even better than phrase-based models.", "labels": [], "entities": []}, {"text": "However, when syntaxbased models are applied to new domain with few syntax-annotated corpus, the translation performance would decrease.", "labels": [], "entities": []}, {"text": "To utilize the robustness of phrases and makeup the lack of syntax or semantic information in phrase-based model for domain translation, we study domain bilingual multiword expressions and integrate them to the existing phrase-based model.", "labels": [], "entities": [{"text": "domain translation", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7344604581594467}]}, {"text": "A multiword expression (MWE) can be considered as word sequence with relatively fixed structure representing special meanings.", "labels": [], "entities": [{"text": "multiword expression (MWE)", "start_pos": 2, "end_pos": 28, "type": "TASK", "confidence": 0.6982348084449768}]}, {"text": "There is no uniform definition of MWE, and many researchers give different properties of MWE.", "labels": [], "entities": []}, {"text": "roughly defined MWE as \"idiosyncratic interpretations that crossword boundaries (or spaces)\".", "labels": [], "entities": []}, {"text": "focused on the noncompositional property of MWE, i.e. the property that whole expression cannot be derived from their component words.", "labels": [], "entities": []}, {"text": "Stanford university launched a MWE project 1 , in which different qualities of MWE were presented.", "labels": [], "entities": []}, {"text": "For bilingual multiword expression (BiMWE), we define a bilingual phrase as a bilingual MWE if (1) the source phrase is a MWE in source language; (2) the source phrase and the target phrase must be translated to each other exactly, i.e. there is no additional (boundary) word in target phrase which cannot find the corresponding word in source phrase, and vice versa.", "labels": [], "entities": [{"text": "bilingual multiword expression (BiMWE)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.6175004641215006}]}, {"text": "In recent years, many useful methods have been proposed to extract MWEs or BiMWEs automatically ().", "labels": [], "entities": []}, {"text": "Since MWE usually constrains possible senses of a polysemous word in context, they can be used in many NLP applications such as information retrieval, question answering, word sense disambiguation and soon.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.8223924338817596}, {"text": "question answering", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.9182944893836975}, {"text": "word sense disambiguation", "start_pos": 171, "end_pos": 196, "type": "TASK", "confidence": 0.7276071707407633}]}, {"text": "For machine translation, have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8167188167572021}, {"text": "MWE identification", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9868814945220947}, {"text": "MT", "start_pos": 174, "end_pos": 176, "type": "TASK", "confidence": 0.9800803661346436}]}, {"text": "This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general fixed expressions and idioms.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9804045557975769}]}, {"text": "Although some MT systems may employ a machine-readable bilingual dictionary of MWE, it is time-consuming and inefficient to obtain this resource manually.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9812248349189758}]}, {"text": "Therefore, some researchers have tried to use automatically extracted bilingual MWEs in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9811958074569702}]}, {"text": "described an approach of noun-noun compound machine translation, but no significant comparison was presented.", "labels": [], "entities": [{"text": "noun-noun compound machine translation", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.7590521574020386}]}, {"text": "presented a method in which bilingual MWEs were used to modify the word alignment so as to improve the SMT quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9944871068000793}]}, {"text": "In their work, a bilingual MWE in training corpus was grouped as one unique token before training alignment models.", "labels": [], "entities": []}, {"text": "They reported that both alignment quality and translation accuracy were improved on a small corpus.", "labels": [], "entities": [{"text": "alignment", "start_pos": 24, "end_pos": 33, "type": "TASK", "confidence": 0.929602861404419}, {"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9477205276489258}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9139699339866638}]}, {"text": "However, in their further study, they reported even lower BLEU scores after grouping MWEs according to part-of-speech on a large corpus).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9994224309921265}]}, {"text": "Nonetheless, since MWE represents liguistic knowledge, the role and usefulness of MWE in full-scale SMT is intuitively positive.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9065358638763428}]}, {"text": "The difficulty lies in how to integrate bilingual MWEs into existing SMT system to improve SMT performance, especially when translating domain texts.", "labels": [], "entities": [{"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9709562063217163}, {"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9926557540893555}]}, {"text": "In this paper, we implement three methods that integrate domain bilingual MWEs into a phrasebased SMT system, and show that these approaches improve translation quality significantly.", "labels": [], "entities": [{"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.7935071587562561}]}, {"text": "The main difference between our methods and Lambert and Banchs' work is that we directly aim at improving the SMT performance rather than improving the word alignment quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.994154155254364}, {"text": "word alignment", "start_pos": 152, "end_pos": 166, "type": "TASK", "confidence": 0.7620850503444672}]}, {"text": "In detail, differences are listed as follows: \u2022 Instead of using the bilingual n-gram translation model, we choose the phrase-based SMT system, Moses 2 , which achieves significantly better translation performance than many other SMT systems and is a state-ofthe-art SMT system.", "labels": [], "entities": [{"text": "bilingual n-gram translation", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.706700066725413}, {"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.8388714790344238}, {"text": "SMT", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.9766305685043335}, {"text": "SMT", "start_pos": 267, "end_pos": 270, "type": "TASK", "confidence": 0.9926706552505493}]}, {"text": "\u2022 Instead of improving translation indirectly by improving the word alignment quality, we directly target at the quality of translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.7139984965324402}]}, {"text": "Some researchers have argued that large gains of alignment performance under many metrics only led to small gains in translation performance (.", "labels": [], "entities": []}, {"text": "Besides the above differences, there are some advantages of our approaches: \u2022 In our method, automatically extracted MWEs are used as additional resources rather than as phrase-table filter.", "labels": [], "entities": []}, {"text": "Since bilingual MWEs are extracted according to noisy automatic word alignment, errors in word alignment would further propagate to the SMT and hurt SMT performance.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.6959223449230194}, {"text": "SMT", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.9021250009536743}, {"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9655908346176147}]}, {"text": "\u2022 We conduct experiments on domain-specific corpus.", "labels": [], "entities": []}, {"text": "For one thing, domain-specific 2 http://www.statmt.org/moses/ corpus potentially includes a large number of technical terminologies as well as more general fixed expressions and idioms, i.e. domain-specific corpus has high MWE coverage.", "labels": [], "entities": []}, {"text": "For another, after the investigation, current SMT system could not effectively deal with these domain-specific MWEs especially for Chinese, since these MWEs are more flexible and concise.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9838823080062866}]}, {"text": "Take the Chinese term \" \" for example.", "labels": [], "entities": []}, {"text": "The meaning of this term is \"soften hard mass and dispel pathogenic accumulation\".", "labels": [], "entities": []}, {"text": "Every word of this term represents a special meaning and cannot be understood literally or without this context.", "labels": [], "entities": []}, {"text": "These terms are difficult to be translated even for humans, let alone machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7470843195915222}]}, {"text": "So, treating these terms as MWEs and applying them in SMT system have practical significance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9827733039855957}]}, {"text": "\u2022 In our approach, no additional corpus is introduced.", "labels": [], "entities": []}, {"text": "We attempt to extract useful MWEs from the training corpus and adopt suitable methods to apply them.", "labels": [], "entities": []}, {"text": "Thus, it benefits for the full exploitation of available resources without increasing great time and space complexities of SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.9854801297187805}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the bilingual MWE extraction technique.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9466354846954346}]}, {"text": "Section 3 proposes three methods to apply bilingual MWEs in SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9820972084999084}]}, {"text": "Section 4 presents the experimental results.", "labels": [], "entities": []}, {"text": "Section 5 draws conclusions and describes the future work.", "labels": [], "entities": []}, {"text": "Since this paper mainly focuses on the application of BiMWE in SMT, we only give a brief introduction on monolingual and bilingual MWE extraction.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.990174412727356}, {"text": "MWE extraction", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.8800124526023865}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Traditional medicine corpus", "labels": [], "entities": []}, {"text": " Table 2: Chemical industry corpus", "labels": [], "entities": []}, {"text": " Table 4: Translation results of using bilingual  MWEs in traditional medicine domain", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9443603754043579}]}, {"text": " Table 5: Translation results of using bilingual  MWEs in chemical industry domain", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9337670207023621}]}]}