{"title": [{"text": "Automatic Adaptation of Annotation Standards for Dependency Parsing -Using Projected Treebank as Source Corpus", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7138689160346985}]}], "abstractContent": [{"text": "We describe for dependency parsing an annotation adaptation strategy, which can automatically transfer the knowledge from a source corpus with a different annotation standard to the desired target parser, with the supervision by a target corpus annotated in the desired standard.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7941232323646545}]}, {"text": "Furthermore , instead of a hand-annotated one, a projected treebank derived from a bilingual corpus is used as the source corpus.", "labels": [], "entities": []}, {"text": "This benefits the resource-scarce languages which haven't different hand-annotated treebanks.", "labels": [], "entities": []}, {"text": "Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic annotation adaptation for sequence labeling () aims to enhance a tagger with one annotation standard by transferring knowledge from a source corpus annotated in another standard.", "labels": [], "entities": [{"text": "Automatic annotation adaptation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6194332242012024}, {"text": "sequence labeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.6433939039707184}]}, {"text": "It would be valuable to adapt this strategy to parsing, since for some languages there are also several treebanks with different annotation standards, such as Chomskian-style Penn Treebank ( and HPSG LinGo Redwoods Treebank () for English.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 175, "end_pos": 188, "type": "DATASET", "confidence": 0.9659351110458374}, {"text": "HPSG LinGo Redwoods Treebank", "start_pos": 195, "end_pos": 223, "type": "DATASET", "confidence": 0.9477283209562302}]}, {"text": "However, we are not content with conducting annotation adaptation between existing different treebanks, because it would be more valuable to boost the parsers also for the resource-scarce languages, rather than only for the resource-rich ones that already have several treebanks.", "labels": [], "entities": []}, {"text": "Although hand-annotated treebanks are costly and scarce, it is not difficult for many languages to collect large numbers of bilingual sentence-pairs aligned to English.", "labels": [], "entities": []}, {"text": "According to the word alignment, the English parses can be projected across to their translations, and the projected trees can be leveraged to boost parsing.", "labels": [], "entities": []}, {"text": "Many efforts are devoted to the research on projected treebanks, such as (), () and (, etc.", "labels": [], "entities": []}, {"text": "Considering the fact that a projected treebank partially inherits the English annotation standard, some hand-written rules are designed to deal with the divergence between languages such as in ().", "labels": [], "entities": []}, {"text": "However, it will be more valuable and interesting to adapt this divergence automatically and boost the existing parsers with this projected treebank.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the automatic annotation adaptation strategy for Chinese dependency parsing, where the source corpus for adaptation is a projected treebank derived from a bilingual corpus aligned to English with word alignment and English trees.", "labels": [], "entities": [{"text": "Chinese dependency parsing", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.6387159923712412}]}, {"text": "We also propose a novel, errortolerant tree-projecting algorithm, which dynamically searches the project Chinese tree that has the largest consistency with the corresponding English tree, according to an alignment matrix rather than a single alignment.", "labels": [], "entities": []}, {"text": "Experiments show that when the target corpus is smaller, the projected Chinese treebank, although with inevitable noise caused by non-literal translation and word alignment error, can be successfully utilized and result in significant improvement over the baseline model trained on the target corpus only.", "labels": [], "entities": [{"text": "Chinese treebank", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.8338533639907837}, {"text": "word alignment", "start_pos": 158, "end_pos": 172, "type": "TASK", "confidence": 0.7163307666778564}]}, {"text": "In the rest of the paper, we first present the treeprojecting algorithm (section 2), and then the annotation adaptation strategy (section 3).", "labels": [], "entities": []}, {"text": "After discussing the related work (section 4) we show the experiments (section 5).", "labels": [], "entities": []}, {"text": "some filtering is needed to eliminate the inaccurate or conflicting labels or dependency edges.", "labels": [], "entities": []}, {"text": "Here we propose a more robust algorithm for dependency tree projection.", "labels": [], "entities": [{"text": "dependency tree projection", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.8013416330019633}]}, {"text": "According to the alignment matrix, this algorithm dynamically searches the projected Chinese dependency tree which has the largest consistency with the corresponding English tree.", "labels": [], "entities": [{"text": "consistency", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9788945913314819}]}, {"text": "We briefly introduce the alignment matrix before describing our projecting algorithm.", "labels": [], "entities": []}, {"text": "Given a Chinese sentence C 1:M and its English translation E 1:N , the alignment matrix A is an M \u00d7 N matrix with each element A i,j denoting the probability of Chinese word Ci aligned to English word E j . Such structure potentially encodes many more possible alignments.", "labels": [], "entities": []}, {"text": "Using C(T C |T E , A) to denote the degree of Chinese tree T C being consistent with English tree T E according to alignment matrix A, the projecting algorithm aims to find\u02c6T We can obtain Ce by simple accumulation across all possible alignments The searching procedure, argmax operation in equation 1, can be effectively solved by a simple, bottom-up dynamic algorithm with cube-pruning speed-up).", "labels": [], "entities": []}, {"text": "We omit the detailed algorithm here due to space restrictions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The source corpus for annotation adaptation, that is, the projected Chinese treebank, is derived from 5.6 millions LDC Chinese-English sentence pairs.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7857182919979095}, {"text": "Chinese treebank", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.8724420070648193}]}, {"text": "The Chinese side of the bilingual corpus is wordsegmented and POS-tagged by an implementation of (, and the English sentences are parsed by an implementation of) which is instead trained on WSJ section of Penn English Treebank ().", "labels": [], "entities": [{"text": "WSJ section of Penn English Treebank", "start_pos": 190, "end_pos": 226, "type": "DATASET", "confidence": 0.8924443225065867}]}, {"text": "The alignment matrixes for sentence pairs are obtained according to ( ).", "labels": [], "entities": []}, {"text": "The English trees are then projected across to Chinese using the algorithm in section 2.", "labels": [], "entities": []}, {"text": "Out of these projected trees, we only select 500 thousands with word count l s.t.", "labels": [], "entities": []}, {"text": "6 \u2264 l \u2264 100 and with projecting confidence c = C(T C |T E , A) 1/l s.t. c \u2265 0.35.", "labels": [], "entities": []}, {"text": "While for the target corpus, we take Penn Chinese Treebank (CTB) 1.0 and CTB 5.0 () respectively, and follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training.", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB) 1.0", "start_pos": 37, "end_pos": 68, "type": "DATASET", "confidence": 0.9720827000481742}, {"text": "CTB 5.0", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.8263009786605835}]}, {"text": "We adopt the 2nd-order MST model) as the target parser for better performance, and the 1st-order MST model as the source parser for fast training.", "labels": [], "entities": []}, {"text": "Both the two parsers are trained with averaged perceptron algo-: Performances of annotation adaptation with CTB 1.0 and CTB 5.0 as the target corpus respectively, as well as of the baseline parsers (2nd-order MST parsers trained on the target corpora).", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.6861343532800674}]}, {"text": "The development set of CTB is also used to determine the best model for the source parser, conditioned on the hypothesis of larger isomorphisme between Chinese and English.", "labels": [], "entities": []}, {"text": "shows that the experimental results of annotation adaptation, with CTB 1.0 and CTB 5.0 as the target corpus respectively.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.8101244568824768}]}, {"text": "We can see that the source parsers, directly trained on the source corpora of projected trees, performs poorly on both CTB test sets (which are in fact the same).", "labels": [], "entities": [{"text": "CTB test sets", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.8828756411870321}]}, {"text": "This is partly due to the noise in the projected treebank, and partly due to the heterogeneous between the CTB trees and the projected trees.", "labels": [], "entities": [{"text": "CTB trees", "start_pos": 107, "end_pos": 116, "type": "DATASET", "confidence": 0.8946587443351746}]}, {"text": "On the contrary, automatic annotation adaptation effectively transfers the knowledge to the target parsers, achieving improvement on both target corpora.", "labels": [], "entities": [{"text": "automatic annotation adaptation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6029665768146515}]}, {"text": "Especially on CTB 1.0, an accuracy increment of 1.3 points is obtained over the baseline parser.", "labels": [], "entities": [{"text": "CTB 1.0", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9273565709590912}, {"text": "accuracy increment", "start_pos": 26, "end_pos": 44, "type": "METRIC", "confidence": 0.9743325710296631}]}, {"text": "We observe that for the much larger CTB 5.0, the performance of annotation adaptation is much lower.", "labels": [], "entities": [{"text": "CTB 5.0", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.8832435011863708}, {"text": "annotation adaptation", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7941482961177826}]}, {"text": "To further investigate the adaptation performances with target corpora of different scales, we conduct annotation adaptation on a series of target corpora which consist of different amount of dependency trees from CTB 5.0.", "labels": [], "entities": [{"text": "CTB 5.0", "start_pos": 214, "end_pos": 221, "type": "DATASET", "confidence": 0.9270259737968445}]}, {"text": "Curves in shows the experimental results.", "labels": [], "entities": []}, {"text": "We see that the smaller the training corpus is, the more significant improvement can be obtained.", "labels": [], "entities": []}, {"text": "For example, with a target corpus composed of 2K trees, nearly 2 points of accuracy increment is achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9990493655204773}]}, {"text": "This is a good news to the resource-scarce languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performances of annotation adaptation  with CTB 1.0 and CTB 5.0 as the target corpus re- spectively, as well as of the baseline parsers (2nd- order MST parsers trained on the target corpora).", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.808886706829071}]}]}