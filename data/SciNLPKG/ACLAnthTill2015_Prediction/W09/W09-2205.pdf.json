{"title": [{"text": "A Comparison of Structural Correspondence Learning and Self-training for Discriminative Parse Selection", "labels": [], "entities": [{"text": "Discriminative Parse Selection", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.6316012342770895}]}], "abstractContent": [{"text": "This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7940952479839325}]}, {"text": "The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Self-training (Abney, 2007; McClosky et al., 2006).", "labels": [], "entities": [{"text": "Structural Correspondence Learning (SCL)", "start_pos": 28, "end_pos": 68, "type": "TASK", "confidence": 0.766662577788035}]}, {"text": "A preliminary evaluation favors the use of SCL over the simpler self-training techniques .", "labels": [], "entities": [{"text": "SCL", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9194134473800659}]}], "introductionContent": [], "datasetContent": [{"text": "The system used in this study is Alpino, a two-stage dependency parser for Dutch).", "labels": [], "entities": []}, {"text": "The first stage consists of a HPSG-like grammar that constitutes the parse generation component.", "labels": [], "entities": [{"text": "parse generation", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.9064507186412811}]}, {"text": "The second stage is a Maximum Entropy (MaxEnt) parse selection model.", "labels": [], "entities": []}, {"text": "To train the MaxEnt model, parameters are estimated based on informative samples).", "labels": [], "entities": []}, {"text": "A parse is added to the training data with a score indicating its \"goodness\").", "labels": [], "entities": []}, {"text": "The score is obtained by comparing it with the gold standard (if available; otherwise the score is approximated through parse probability).", "labels": [], "entities": []}, {"text": "The source domain is the Alpino Treebank (van Noord and) (newspaper text; approx. 7,000 sentences; 145k tokens).", "labels": [], "entities": [{"text": "Alpino Treebank (van Noord and)", "start_pos": 25, "end_pos": 56, "type": "DATASET", "confidence": 0.9670806612287249}]}, {"text": "We use Wikipedia both as testset and as unlabeled target data source.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.9573203921318054}]}, {"text": "We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about the New Power Generation, the Purple rain movie, or other American singers and artists, should be of help.", "labels": [], "entities": []}, {"text": "Thus, we exploit Wikipedia's category system to gather domain-specific target data.", "labels": [], "entities": []}, {"text": "In our empirical setup, we follow and balance the size of source and target data.", "labels": [], "entities": []}, {"text": "Thus, depending on the size of the resulting target domain dataset, and the \"broadness\" of the categories involved in creating it, we might wish to filter out certain pages.", "labels": [], "entities": []}, {"text": "We implemented a filter mechanism that excludes pages of a certain category (e.g. a supercategory that is hypothesized to be \"too broad\").", "labels": [], "entities": []}, {"text": "Further details about the dataset construction are given in The size of the target domain testsets is given in.", "labels": [], "entities": []}, {"text": "As evaluation measure concept accuracy (CA)) is used (similar to labeled dependency accuracy).", "labels": [], "entities": [{"text": "concept accuracy (CA))", "start_pos": 22, "end_pos": 44, "type": "METRIC", "confidence": 0.7979416608810425}, {"text": "labeled dependency accuracy", "start_pos": 65, "end_pos": 92, "type": "METRIC", "confidence": 0.5327181418736776}]}, {"text": "The training data for the pivot predictors are the 1-best parses of source and target domain data as selected by the original Alpino model.", "labels": [], "entities": []}, {"text": "We report on results of SCL with dimensionality parameter set to h = 25, and remaining settings identical to Plank (2009) (i.e., no feature-specific regularization and no feature normalization and rescaling).", "labels": [], "entities": [{"text": "SCL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9304827451705933}]}], "tableCaptions": [{"text": " Table 1 provides information on the target domain  datasets constructed from Wikipedia.", "labels": [], "entities": []}, {"text": " Table 2. As evaluation measure concept accuracy  (CA)", "labels": [], "entities": [{"text": "accuracy  (CA)", "start_pos": 40, "end_pos": 54, "type": "METRIC", "confidence": 0.9121712744235992}]}, {"text": " Table 2: Supervised Baseline results.", "labels": [], "entities": []}, {"text": " Table 3: Results of SCL and self-training (single itera- tion, no selection). Entries marked with \u22c6 are statistically  significant at p < 0.05. The \u03c6 score incorporates upper- and lower-bounds.", "labels": [], "entities": [{"text": "SCL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9870849251747131}, {"text": "\u03c6 score", "start_pos": 149, "end_pos": 156, "type": "METRIC", "confidence": 0.9615695774555206}]}]}