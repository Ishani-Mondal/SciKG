{"title": [], "abstractContent": [], "introductionContent": [{"text": "In order to understand what is going on in a meeting, it is important to know who is talking, what is being said, and who is being addressed (talked to).", "labels": [], "entities": []}, {"text": "Here, we focus on the question of whom the speech is addressed to.", "labels": [], "entities": []}, {"text": "We present results obtained in developing a classifier for real-time addressee prediction to be used in an assistant fora remote participant in a hybrid meeting, a meeting where a number of participants share a common meeting room and one or more others take part via teleconferencing software.", "labels": [], "entities": [{"text": "addressee prediction", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7036122381687164}]}, {"text": "It is obvious that in order to effectively participate in a meeting, participants need to know who is being addressed at all times.", "labels": [], "entities": []}, {"text": "For remote participants in hybrid meetings, understanding the course of the conversation can be difficult due to the fact that it is hard to figure out who is being addressed.", "labels": [], "entities": [{"text": "understanding the course of the conversation", "start_pos": 44, "end_pos": 88, "type": "TASK", "confidence": 0.8680294255415598}]}, {"text": "But it is not only meeting participants who are interested in addressees.", "labels": [], "entities": []}, {"text": "The question who is being addressed has long been of interest for science: group therapists, small group research, or outside observers who analyse recorded meetings.", "labels": [], "entities": []}, {"text": "How speakers address listeners, what kind of procedures speakers use to designate their audience and to make clear whom they address has been the focus of conversational analysis, sociolinguistics and ethnomethodology for quite sometime.", "labels": [], "entities": [{"text": "conversational analysis", "start_pos": 155, "end_pos": 178, "type": "TASK", "confidence": 0.7533897161483765}]}, {"text": "An analysis of addressee selection is presented in.", "labels": [], "entities": [{"text": "addressee selection", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.755480170249939}]}, {"text": "Addressing as a special type of multi-modal interactional referring expression generation behavior is considered in (op den).", "labels": [], "entities": [{"text": "multi-modal interactional referring expression generation", "start_pos": 32, "end_pos": 89, "type": "TASK", "confidence": 0.6868436574935913}]}, {"text": "The problem of automatic addressee detection is one of the problems that come up when technology makes the move from two-party man-machine natural dialogue systems to systems for multiparty conversations.", "labels": [], "entities": [{"text": "automatic addressee detection", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6415444413820902}]}, {"text": "In this context the addressing problem was raised by.", "labels": [], "entities": [{"text": "addressing", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.969036877155304}]}, {"text": "Since Jovanovi\u00b4cJovanovi\u00b4c (2004), presented her research on addressee prediction in meetings at SigDial, quite a few publications on the topic appeared.", "labels": [], "entities": [{"text": "addressee prediction", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7166977971792221}, {"text": "SigDial", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.6255619525909424}]}, {"text": "Jovanovi\u00b4cvanovi\u00b4c used a number of multi-modal meeting corpora developed in the European projects M4 and AMI.", "labels": [], "entities": []}, {"text": "In) the first multi-modal multi-party corpus containing hand labeled addressee annotations was presented.", "labels": [], "entities": []}, {"text": "The public release of the multi-modal AMI meeting corpus), a hour annotated corpus of small group meetings has already shown to bean important achievement for research; not only for conversational speech recognition and tracking of visual elements but also for automatic multi-modal conversational scene analysis.", "labels": [], "entities": [{"text": "AMI meeting corpus", "start_pos": 38, "end_pos": 56, "type": "DATASET", "confidence": 0.5977598329385122}, {"text": "conversational speech recognition", "start_pos": 182, "end_pos": 215, "type": "TASK", "confidence": 0.6725971500078837}, {"text": "multi-modal conversational scene analysis", "start_pos": 271, "end_pos": 312, "type": "TASK", "confidence": 0.6213084012269974}]}, {"text": "The M4 and AMI corpora are the only multi-modal meeting corpora (partly) annotated with addressee labels.", "labels": [], "entities": [{"text": "AMI corpora", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8933786153793335}]}, {"text": "Addressee detection in robot-human interaction is studied in) and in multi-party dialogue systems in).", "labels": [], "entities": [{"text": "Addressee detection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9201728701591492}]}, {"text": "Addressing in face-to-face conversations is achieved by multi-modal behavior and addressee detection is thus a multi-modal recognition task.", "labels": [], "entities": [{"text": "addressee detection", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7329361885786057}]}, {"text": "This task requires not only speech recognition but also gaze and gesture recognition, the recognition of deictic references, and, ideally, the understanding of the \"what's going on\" in the meeting.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7821907997131348}, {"text": "gaze and gesture recognition", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6204294711351395}, {"text": "understanding of the \"what's going on\" in the meeting", "start_pos": 143, "end_pos": 196, "type": "TASK", "confidence": 0.7003099794189135}]}, {"text": "It requires the detection of who is involved in current (parallel) activities.", "labels": [], "entities": []}, {"text": "Speakers show explicit addressing behavior when they are not confident that the participants they want to address are paying attention to their words.", "labels": [], "entities": []}, {"text": "Analysis of the remote meetings recorded in the EC project AMIDA reinforces our experiences that this happens more in remote meetings than in small group face-toface meetings.", "labels": [], "entities": [{"text": "EC project AMIDA", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.859937051932017}]}, {"text": "In AMIDA, the European follow-up project of AMI, the two new research goals are: (1) real-time processing (real-time speech recognition (, focus of attention recognition, real-time dialogue act labeling ( and addressee detection); and (2) technology for (remote) meeting support.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7068663984537125}, {"text": "focus of attention recognition", "start_pos": 139, "end_pos": 169, "type": "TASK", "confidence": 0.5923276394605637}, {"text": "real-time dialogue act labeling", "start_pos": 171, "end_pos": 202, "type": "TASK", "confidence": 0.5862689688801765}, {"text": "addressee detection", "start_pos": 209, "end_pos": 228, "type": "TASK", "confidence": 0.7301407158374786}]}, {"text": "Technology based on the analysis of how people behave and converse in meetings is now going to re-shape the meetings, and hopefully make them more effective and more engaging.", "labels": [], "entities": []}, {"text": "Social interaction graphs that show who is talking to whom and how frequently in a meeting may help the group by mirroring its interpersonal relations, dominance, and group dynamics, and understand social mechanisms as possible causes of ineffectiveness.", "labels": [], "entities": []}, {"text": "Although, feedback about the social interactions may also be useful during meetings, it doesn't require the prediction of the speaker's addressees in real-time.", "labels": [], "entities": []}, {"text": "A participant in a meeting, however, needs to know who is being addressed by the speaker at \"the time of speaking\".", "labels": [], "entities": []}, {"text": "This holds for humans as well as for an artificial partner, a robot or a virtual Embodied Conversational Agent in a multi-party conversation.", "labels": [], "entities": []}, {"text": "The problem of addressee prediction comes in different flavors, depending on the relations that the subject who is in need of an answer, has with the event itself.", "labels": [], "entities": [{"text": "addressee prediction", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7419088780879974}]}, {"text": "Time is one of the aspects that play a role here: whether the subject needs to know the addressee of an utterance in real-time or offline.", "labels": [], "entities": [{"text": "Time", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9410594701766968}]}, {"text": "But it is not only time that plays a role.", "labels": [], "entities": []}, {"text": "The addressing problem is an interactional problem, meaning that it is determined by the role that the subject has in the interaction itself; if and how the speaker and others communicate with each other and with the subject.", "labels": [], "entities": []}, {"text": "Is he himself a possible addressee of the speaker or is he an outside observer?", "labels": [], "entities": []}, {"text": "What type of communication channels are available to the subject and which channels of communication are available to the conversational partners in the meeting?", "labels": [], "entities": []}, {"text": "It is often harder to follow a face-to-face discussion on the radio than to follow a radio broadcasted multi-party discussion that was held via a point-to-point telephone connection.", "labels": [], "entities": []}, {"text": "What speakers do to make clear whom they are addressing depends on the status and capacities of the communication lines with their interlocutors.", "labels": [], "entities": []}, {"text": "Discussion leaders in TV shows are aware of their TV audience.", "labels": [], "entities": []}, {"text": "Every now and then, they explicitly address their virtual audience at home.", "labels": [], "entities": []}, {"text": "They also design their questions so as to make clear to the TV viewer whom their questions are addressed to.", "labels": [], "entities": []}, {"text": "Outside observers in the form of a video camera will, however, not affect the way speakers make clear whom they address as long as the camera is not considered as a participant interested in the speaker's intention.", "labels": [], "entities": []}, {"text": "Because remote participants are often out of sight, speakers in the meeting room do not take them into account when they converse to others in the meeting room.", "labels": [], "entities": []}, {"text": "Remote participants become a kind of outside observers and share the same problems that annotators have when they watch video recordings of meetings to see what is happening in the meeting and who is being addressed by the speaker.", "labels": [], "entities": []}, {"text": "In section 2 we will specify the particular type of addressing problem that we are trying to tackle here.", "labels": [], "entities": []}, {"text": "We make clear how our problem and approach differ from those of other researchers and what this means for the applicability of previous results and available data.", "labels": [], "entities": []}, {"text": "In section 3 we present the data we used for testing and training.", "labels": [], "entities": []}, {"text": "We set a baseline for the performance of our classifiers as well as a hypothesized maximum value, or ceiling, based on the complexity of the task at hand.", "labels": [], "entities": []}, {"text": "In section 4 we discuss the experiments, for selecting the optimal features, classifiers, and parameters.", "labels": [], "entities": []}, {"text": "In section 5 we present the experimental results.", "labels": [], "entities": []}, {"text": "In section 6 we discuss how the currently implemented addressing module works in the meeting assistant and what is required to use all the features of the addressee predictor in a hybrid meeting.", "labels": [], "entities": []}, {"text": "The classification problem is to assign an addressee label to a dialogue act, a hand-labeled and hand-segmented sequence of words, which is obtained by manual transcription of a speaker's utterance.", "labels": [], "entities": []}, {"text": "The output of the classifier is one of a set of possible addressee labels: Group, or P0,P1,P2,P3, which are the four fixed positions around the table of the four participants in the meeting.", "labels": [], "entities": []}, {"text": "Since the AMI data contains several meetings of different groups of four people, the class value cannot be the name of a participant, as that is not an invariant of the meeting setting.", "labels": [], "entities": [{"text": "AMI data", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.8592379689216614}]}, {"text": "Positions at the rectangular table are invariant.", "labels": [], "entities": []}, {"text": "This implies that the classifiers can only be used for meetings with this setting and four participants.", "labels": [], "entities": []}, {"text": "A comparison of the statistical classifier of Jovanovi\u00b4cJovanovi\u00b4c with a rule-based method using the same part of the AMI corpus is presented in (op den.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 119, "end_pos": 129, "type": "DATASET", "confidence": 0.8815959095954895}]}, {"text": "The same data is also used by in their study of a related problem: finding the person the speaker refers to when he uses a second person pronoun (e.g. 'you' or 'your') as a deictic referring expression.", "labels": [], "entities": [{"text": "finding the person the speaker refers to when he uses a second person pronoun (e.g. 'you' or 'your')", "start_pos": 67, "end_pos": 167, "type": "TASK", "confidence": 0.6062432022198386}]}, {"text": "Their class values are not positions at the table but \"virtual positions\" in the speaking order (e.g. next speaker, previous speaker), a solution that generalises to a broader class of conversations than four participants in a face-to-face meeting.", "labels": [], "entities": []}, {"text": "Ina more recent study, use positions at the table relative to the position of the speaker as class values: L1, L2, L3.", "labels": [], "entities": []}, {"text": "The reason for this is to alleviate the problem of class imbalance in the corpus.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Confusion matrix for one pair of annota- tors ( ).", "labels": [], "entities": []}, {"text": " Table 2: Confusion matrix for one pair of anno- tators, considering addressed to A or not (derived  from the matrix in", "labels": [], "entities": []}, {"text": " Table 3: Recall, Precision, F-measure and Accu- racy values for the 6 pairs of annotators.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9961429238319397}, {"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9983870983123779}, {"text": "F-measure", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9939815402030945}, {"text": "Accu- racy", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9651282032330831}]}, {"text": " Table 4: Performance values of the Methods dis- cussed in this paper: Accuracy, Recall, Precision,  F-measure and Percentage of Hypothezised Maxi- mum Score (PoM).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9992006421089172}, {"text": "Recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9979130625724792}, {"text": "Precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9952046871185303}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9959852695465088}, {"text": "Percentage", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.974838376045227}, {"text": "Hypothezised Maxi- mum Score (PoM)", "start_pos": 129, "end_pos": 163, "type": "METRIC", "confidence": 0.7971815690398216}]}]}