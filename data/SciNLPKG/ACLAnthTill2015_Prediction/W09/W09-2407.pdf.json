{"title": [{"text": "Making Semantic Topicality Robust Through Term Abstraction *", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite early intuitions, semantic similarity has not proven to be robust for splitting multi-party interactions into separate conversations.", "labels": [], "entities": []}, {"text": "We discuss some initial successes with using thesaural headwords to abstract the semantics of an utterance.", "labels": [], "entities": []}, {"text": "This simple profiling technique showed improvements over base-line conversation threading models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic segmentation is the problem of dividing a document into smaller coherent units.", "labels": [], "entities": [{"text": "Topic segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8036430776119232}]}, {"text": "The segments can be hierarchical or linear; the topics can be localized or distributed; the documents can be newswire or chat logs.", "labels": [], "entities": []}, {"text": "Of course, each of these variables is best analyzed as continuous rather than discrete.", "labels": [], "entities": []}, {"text": "Newswire, for instance, is a more formal, monologue-style genre while a chat log tends towards the informal register with different conversations interwoven.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.907712996006012}]}, {"text": "We present a topic segmenter which uses semantics to define coherent conversations within a larger, multi-party document.", "labels": [], "entities": [{"text": "topic segmenter", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.7578859329223633}]}, {"text": "Using a word's thesaurus entry as a proxy for its underlying semantics provides a domain-neutral metric for distinguishing conversations.", "labels": [], "entities": []}, {"text": "Also, our classifier does not rely on metalinguistic properties that may not be robust across genres.", "labels": [], "entities": []}, {"text": "* The first author was partially funded through a fellowship from the SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc.", "labels": [], "entities": []}, {"text": "(http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004)", "labels": [], "entities": [{"text": "Air Force Grant No.s FA8750-07-C-0077", "start_pos": 26, "end_pos": 63, "type": "DATASET", "confidence": 0.8347760438919067}, {"text": "FA8750-07-D-0019", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.8148323893547058}]}], "datasetContent": [{"text": "Our primary dataset was distributed by.", "labels": [], "entities": []}, {"text": "They collected conversations from the IRC (Internet Relay Chat) channel ##LINUX, a very popular room on freenode.net with widely ranging topics.", "labels": [], "entities": [{"text": "LINUX", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.6442657709121704}]}, {"text": "University students then annotated these chat logs into conversations.", "labels": [], "entities": []}, {"text": "We take the collection of these annotations to be our gold standard for topic segmentation with respect to the chat logs.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7590899169445038}]}], "tableCaptions": [{"text": " Table 1: Spreading Activation Weights.", "labels": [], "entities": [{"text": "Spreading Activation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.89231276512146}]}, {"text": " Table 2: Similarity Score Calculations.", "labels": [], "entities": [{"text": "Similarity Score", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7553056180477142}]}, {"text": " Table 3: A Example Similarity Scoring for Two Conver- sations. 'High' and 'low' refers to headword activation.", "labels": [], "entities": []}, {"text": " Table 4: General statistics for our model as compared  with Elsner and Charniak's human annotators. Some  numbers are taken from Table 1 (Elsner and Charniak,  2008).", "labels": [], "entities": []}, {"text": " Table 5: Comparative many-to-1 measures for evaluating  differences in annotation granularity. Some numbers are  taken from Table 1 (Elsner and Charniak, 2008).", "labels": [], "entities": []}, {"text": " Table 6. Our 1-to-1 measures are just  barely above the baseline, on average. On the other", "labels": [], "entities": []}, {"text": " Table 7: Source of misclassified utterances as a percent- age of misclassified utterances and all utterances.", "labels": [], "entities": []}, {"text": " Table 6: Metric values for our model as compared with Elsner and Charniak's human annotators and classifier. Some  numbers are taken from", "labels": [], "entities": []}]}