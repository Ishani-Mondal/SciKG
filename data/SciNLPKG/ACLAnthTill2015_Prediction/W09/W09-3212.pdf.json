{"title": [{"text": "Quantitative analysis of treebanks using frequent subtree mining methods", "labels": [], "entities": []}], "abstractContent": [{"text": "The first task of statistical computational linguistics, or any other type of data-driven processing of language, is the extraction of counts and distributions of phenomena.", "labels": [], "entities": [{"text": "statistical computational linguistics", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.7245007554690043}, {"text": "extraction of counts and distributions of phenomena", "start_pos": 121, "end_pos": 172, "type": "TASK", "confidence": 0.7370679548808506}]}, {"text": "This is much more difficult for the type of complex structured data found in treebanks and in corpora with sophisticated annotation than for tokenized texts.", "labels": [], "entities": []}, {"text": "Recent developments in data mining, particularly in the extraction of frequent sub-trees from treebanks, offer some solutions.", "labels": [], "entities": [{"text": "data mining", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.7490819096565247}]}, {"text": "We have applied a modified version of the TreeMiner algorithm to a small treebank and present some promising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical corpus linguistics and many natural language processing applications rely on extracting the frequencies and distributions of phenomena from natural language data sources.", "labels": [], "entities": [{"text": "Statistical corpus linguistics", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7334074974060059}]}, {"text": "This is relatively simple when language data is treated as bags of tokens or as n-grams, but much more complicated for corpora annotated with complex feature schemes and for treebanks where syntactic dependencies are marked.", "labels": [], "entities": []}, {"text": "A great deal of useful information is encoded in these more complex structured corpora, but access to it is very limited using the traditional algorithms and analytical tools of computational linguistics.", "labels": [], "entities": []}, {"text": "Many of the most powerful techniques available to natural language processing have been built on the basis of n-gram and bag of words models, but we already know that these methods are inadequate to fully model the information in texts or we would have little use for treebanks or annotation schemes.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6783179839452108}]}, {"text": "Suffix trees provide some improvement over n-grams and bag-of-words schemes by identifying all frequently occurring sequences regardless of length.", "labels": [], "entities": []}, {"text": "While this has value in identifying some multi-word phenomena, any algorithm that models languages on the basis of frequent contiguous string discovery will have trouble modeling a number of pervasive phenomena in natural language.", "labels": [], "entities": []}, {"text": "In particular: \u2022 Long distance dependencies -i.e., dependencies between words that are too far apart to be accessible to n-gram models.", "labels": [], "entities": []}, {"text": "\u2022 Flexible word orders -languages usually have contexts where word order can vary.", "labels": [], "entities": []}, {"text": "\u2022 Languages with very rich morphologies that must betaken into account or where too much important information is lost through lemmatization.", "labels": [], "entities": []}, {"text": "\u2022 Correlations between different levels of abstraction in annotation, such as between the lemma of a verb and the semantic or syntactic class of its arguments.", "labels": [], "entities": []}, {"text": "\u2022 Extra-syntactic correlations that may involve any nearby word, such as semantic priming effects.", "labels": [], "entities": []}, {"text": "In treebanks and other annotated corpora that can be converted into rooted, directed graphs, many of these phenomena are accessible as frequently recurring subtrees.", "labels": [], "entities": []}, {"text": "For example, consider the Dutch idiom \"naar huis gaan\", (to go home).", "labels": [], "entities": []}, {"text": "The components of this phrase can appear in a variety of orders and with words inserted between the constituents: 1.", "labels": [], "entities": []}, {"text": "Ik zou naar huis kunnen gaan. home.)", "labels": [], "entities": []}, {"text": "2. We gaan naar huis.", "labels": [], "entities": []}, {"text": "(We're going home.)", "labels": [], "entities": []}, {"text": "Ina treebank, these two sentences would share a common subtree that encompasses the phrase \"naar huis gaan\", as in.", "labels": [], "entities": []}, {"text": "Note that for this purpose, two subtrees are treated as identical if the only difference between them is the order of the children of some or all the nodes.", "labels": [], "entities": []}, {"text": "Most theories of syntax use trees to represent interlexical dependencies, and generally theories of morphology and phonology use either hierarchical tree structures to represent their formalisms, or use unstructured bags that can be trivially represented as trees.", "labels": [], "entities": []}, {"text": "Most types of linguistic feature systems are at least in part hierarchical and representable in tree form.", "labels": [], "entities": []}, {"text": "Because so many linguistic phenomena are manifest as frequent subtrees within hierarchical representations that are motivated by linguistic theories, efficient methods for extracting frequent subtrees from treebanks are therefore potentially very valuable to corpus and computational linguistics.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Runtime and closed trees extracted at dif- ferent minimum frequency thresholds, using the  7137 sentence sample of the Alpino Treebank.", "labels": [], "entities": [{"text": "7137 sentence sample of the Alpino Treebank", "start_pos": 101, "end_pos": 144, "type": "DATASET", "confidence": 0.8181198069027492}]}, {"text": " Table 3: Runtime and closed trees extracted from  automatically parsed samples of the Europarl  Dutch corpus, with minimum frequency thresh- olds kept roughly constant as a proportion of the  sample size.", "labels": [], "entities": [{"text": "Europarl  Dutch corpus", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.9840881625811259}, {"text": "minimum frequency thresh- olds", "start_pos": 116, "end_pos": 146, "type": "METRIC", "confidence": 0.7836728096008301}]}, {"text": " Table 4: Non-closed trees from the 7137 sentence  sample of the Alpino Treebank, produced erro- neously as closed trees because of repeated labels.  There were no non-closed trees extracted at fre- quencies over 30.", "labels": [], "entities": [{"text": "7137 sentence  sample of the Alpino Treebank", "start_pos": 36, "end_pos": 80, "type": "DATASET", "confidence": 0.8322132996150425}]}]}