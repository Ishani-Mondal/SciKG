{"title": [{"text": "Another look at indirect negative evidence", "labels": [], "entities": []}], "abstractContent": [{"text": "Indirect negative evidence is clearly an important way for learners to constrain over-generalisation, and yet a good learning theoretic analysis has yet to be provided for this, whether in a PAC or a proba-bilistic identification in the limit framework.", "labels": [], "entities": []}, {"text": "In this paper we suggest a theoretical analysis of indirect negative evidence that allows the presence of ungrammati-cal strings in the input and also accounts for the relationship between grammatical-ity/acceptability and probability.", "labels": [], "entities": []}, {"text": "Given independently justified assumptions about lower bounds on the probabilities of grammatical strings, we establish that a limited number of membership queries of some strings can be probabilistically simulated.", "labels": [], "entities": []}], "introductionContent": [{"text": "First language acquisition has been studied fora longtime from a theoretical point of view,), but a consensus has not emerged as to the most appropriate model for learnability.", "labels": [], "entities": [{"text": "First language acquisition", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7703587412834167}]}, {"text": "The two main competing candidates, Gold-style identification in the limit and PAC-learning both have significant flaws.", "labels": [], "entities": []}, {"text": "For most NLP researchers, these issues are simply not problems: for all empirical purposes, one is interested in modelling the distribution of examples or the conditional distribution of labels given examples and the obvious solution -an \u2212 \u03b4 bound on some suitable loss function such as the Kullback-Leibler Divergence -is sufficient).", "labels": [], "entities": []}, {"text": "There maybe some complexity issues involved with computing these approximations, but there is no debate about the appropriateness of the learning paradigm.", "labels": [], "entities": []}, {"text": "However, such an approach is unappealing to linguists fora number of reasons: it fails to draw a distinction between grammatical and ungrammatical sentences, and for many linguists the key data are not the \"performance\" data but rather the \"voice of competence\" as expressed in grammaticality and acceptability judgments.", "labels": [], "entities": []}, {"text": "Many of the most interesting sentences for syntacticians are comparatively rare and unusual and may occur with negligible frequency in the data.", "labels": [], "entities": []}, {"text": "We do not want to get into this debate here: in this paper, we will assume that there is a categorical distinction between grammatical and ungrammatical sentences.", "labels": [], "entities": []}, {"text": "Within this view learnability is technically quite difficult to formalise in a realistic way.", "labels": [], "entities": []}, {"text": "Children clearly are provided with examples of the language -so-called positive data -but the status of examples not in the language -negative data -is one of the endless and rather circular debates in the language acquisition literature).", "labels": [], "entities": []}, {"text": "Here we do not look at the role of corrections and other forms of negative data but we focus on what has been called indirect negative evidence.", "labels": [], "entities": []}, {"text": "INE is the non-occurrence of data in the primary linguistic data; informally, if the child does not hear certain ungrammatical sentences, then by their absence the child can infer that those strings are ungrammatical.", "labels": [], "entities": [{"text": "INE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9764449000358582}]}, {"text": "Indirect negative evidence has long been recognised as an important source of information.", "labels": [], "entities": []}, {"text": "However it has been surprisingly difficult to find an explicit learning theoretic account of INE.", "labels": [], "entities": [{"text": "INE", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.4257301986217499}]}, {"text": "Indeed, in both the PAC and IIL paradigms it can be shown, that under the standard assumptions, INE cannot help the learner.", "labels": [], "entities": [{"text": "INE", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9892929792404175}]}, {"text": "Thus in many of these models, there is a sharp and implausible distinction between learning paradigms where the learner is provided systematically with every negative example, and those where the learner is denied any negative evidence at all.", "labels": [], "entities": []}, {"text": "Neither of these is very realistic.", "labels": [], "entities": []}, {"text": "In this paper, we suggest a resolution for this conflict, by re-examining the standard learnability assumptions.", "labels": [], "entities": []}, {"text": "We make three uncontroversial ob-servations: first that the examples the child is provided with are unlabelled, secondly that there area small proportion of ungrammatical sentences in the input to the child, and thirdly that in spite of this, the child does in fact learn.", "labels": [], "entities": []}, {"text": "We then draw a careful distinction between probability and grammaticality and propose a restriction on the class of distributions allowed to take account of the fact that children are exposed to some ungrammatical utterances.", "labels": [], "entities": []}, {"text": "We call this the Disjoint Distribution Assumption: the assumption that the classes of distributions for different languages must be disjoint.", "labels": [], "entities": []}, {"text": "Based on this assumption, we argue that the learner can infer lower bounds on the probabilities of grammatical strings, and that using these lower bounds allow a probabilistic approximation to membership queries of some strings.", "labels": [], "entities": []}, {"text": "On this basis we conclude that the learner does have some limited access to indirect negative evidence, and we discuss some of the limitations on this data and the implications for learnability.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}