{"title": [], "abstractContent": [{"text": "crater is Educational Testing Service's technology for the content scoring of short student responses.", "labels": [], "entities": [{"text": "Educational Testing Service", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.8904897570610046}, {"text": "content scoring of short student responses", "start_pos": 59, "end_pos": 101, "type": "TASK", "confidence": 0.7299330731232961}]}, {"text": "A major step in the scoring process is Model Building where variants of model answers are generated that correspond to the rubric for each item or test question.", "labels": [], "entities": [{"text": "Model Building", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7347367256879807}]}, {"text": "Until recently, Model Building was knowledge-engineered (KE) and hence labor and time intensive.", "labels": [], "entities": [{"text": "Model Building", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7506133019924164}]}, {"text": "In this paper, we describe our approach to automating Model Building in crater.", "labels": [], "entities": [{"text": "automating Model Building in crater", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.6640282213687897}]}, {"text": "We show that crater achieves comparable accuracy on automatically built and KE models.", "labels": [], "entities": [{"text": "crater", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.6260223388671875}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994199275970459}]}], "introductionContent": [{"text": "c-rater () is Educational Testing Service's (ETS) technology for the automatic content scoring of short freetext student answers, ranging in length from a few words to approximately 100 words.", "labels": [], "entities": [{"text": "automatic content scoring of short freetext student answers", "start_pos": 69, "end_pos": 128, "type": "TASK", "confidence": 0.7519819736480713}]}, {"text": "While other content scoring systems [e.g., Intelligent.", "labels": [], "entities": [{"text": "Intelligent.", "start_pos": 43, "end_pos": 55, "type": "DATASET", "confidence": 0.8585190176963806}]}, {"text": "Essay Assessor, SEAR), IntelliMetric (Vantage Learning] take a holistic 1 approach, c-rater takes an analytical approach to scoring content.", "labels": [], "entities": [{"text": "SEAR", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8072049617767334}]}, {"text": "The item rubrics specify content in terms of main points or concepts required to appear in a student's correct answer.", "labels": [], "entities": []}, {"text": "An example of a test question or item follows:", "labels": [], "entities": []}], "datasetContent": [{"text": "The study involves 12 test items developed at ETS for grades 7 and 8.", "labels": [], "entities": [{"text": "ETS", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9662097692489624}]}, {"text": "There are seven Reading Comprehension items, denoted R1-R7 and five Mathematics items, denoted M1-M5.", "labels": [], "entities": []}, {"text": "Score points for the items range from 0 to 3 and the number of concepts ranges from 2 to 7.", "labels": [], "entities": [{"text": "Score", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9631351828575134}]}, {"text": "The answers for these items were collected in schools in Maine, USA.", "labels": [], "entities": []}, {"text": "The number of answers collected for each item ranges from 190-264.", "labels": [], "entities": []}, {"text": "Answers were concept-based scored by two human raters (H1, H2).", "labels": [], "entities": []}, {"text": "We split the double-scored students' answers available into DEV (90-100 answers), XVAL (40-50) and BLIND (60-114).", "labels": [], "entities": [{"text": "DEV", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.8876703977584839}, {"text": "XVAL", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.8599177002906799}, {"text": "BLIND", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.9937157034873962}]}, {"text": "Training data refer to DEV together with XVAL datasets.", "labels": [], "entities": [{"text": "Training data", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.7950032651424408}, {"text": "DEV", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.9387387633323669}, {"text": "XVAL datasets", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8550501465797424}]}, {"text": "Results are reported in terms of un-weighted kappa, representing scoring agreement with humans on the BLIND dataset.", "labels": [], "entities": [{"text": "BLIND dataset", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9551606476306915}]}, {"text": "H1/2 refers to the agreement between the two humans, c-H1/2 denotes the average of kappa values between c-rater and each human (c-H1 and c-H2).", "labels": [], "entities": []}, {"text": "reports the best kappa over the 40 experiments on BLIND (Auto I or U).", "labels": [], "entities": [{"text": "BLIND", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.8609955906867981}]}, {"text": "The baseline (Auto C) uses concepts as model sentences.", "labels": [], "entities": []}, {"text": "The accuracy using the automated approach with Evidence as model sentences is comparable to that of the KE approach (noted in the column labeled, \"Manual\") with a 0.1 maximum difference in un-weighted kappa statistics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995813965797424}]}, {"text": "The first methods (in terms of running order) yielding the best results for the items (in order of appearance in This approach was only evaluated on a small number of items.", "labels": [], "entities": []}, {"text": "We expect that some methods will outperform others through additional evaluation.", "labels": [], "entities": []}, {"text": "In an operational setting (i.e., not a research environment), we must choose a model before we score the BLIND data.", "labels": [], "entities": [{"text": "BLIND data", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.7377352714538574}]}, {"text": "Hence, a voting strategy overall the experiments has to be devised based on the results on DEV and XVAL.", "labels": [], "entities": [{"text": "DEV", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9664318561553955}, {"text": "XVAL", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.6943737268447876}]}, {"text": "Following our original logic, i.e., using XVAL to avoid over-fitting and predicting the results of BLIND, we implemented a simple voting strategy.", "labels": [], "entities": [{"text": "XVAL", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.7975338697433472}, {"text": "BLIND", "start_pos": 99, "end_pos": 104, "type": "METRIC", "confidence": 0.4397987127304077}]}, {"text": "We considered c-H1/2 on XVAL for each experiment.", "labels": [], "entities": [{"text": "XVAL", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.46009427309036255}]}, {"text": "We found the maximum overall the c-H1/2 for all experiments.", "labels": [], "entities": []}, {"text": "The model corresponding to the maximum was considered the model for the item and used to score the BLIND data.", "labels": [], "entities": [{"text": "BLIND data", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.8272929489612579}]}, {"text": "When there was a tie, the first method to yield the maximum W chosen.", "labels": [], "entities": []}, {"text": "shows the results on BLIND using the voting strategy.", "labels": [], "entities": [{"text": "BLIND", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.522458553314209}]}, {"text": "The results are comparable to those of the manual approach except for R7 which has 7 concepts, the highest number of concepts among all items.", "labels": [], "entities": []}, {"text": "The results also show that the voting strategy did not select the \"best\" model or experiment.", "labels": [], "entities": []}, {"text": "We notice that some methods were better in detecting whether an answer entailed a concept C than detecting whether it entailed another concept D, specified for the same item.", "labels": [], "entities": [{"text": "detecting whether an answer entailed a concept C", "start_pos": 43, "end_pos": 91, "type": "TASK", "confidence": 0.7179738283157349}]}, {"text": "This implies that the voting strategy will have to be a function that not only considers the overall kappa agreement (i.e., holistic scores), but concept-based agreement (i.e., using conceptbased scores).", "labels": [], "entities": []}, {"text": "Next, we noticed that for R7, XVAL did not predict the results on BLIND.", "labels": [], "entities": [{"text": "XVAL", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9322842359542847}, {"text": "BLIND", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.48528116941452026}]}, {"text": "This was mainly due to the inability to apply stratified sampling with such a small sample size when there are 7 concepts involved.", "labels": [], "entities": []}, {"text": "Further, we may need to take advantage of the training data differently, e.g. an n-fold crossvalidation approach.", "labels": [], "entities": []}, {"text": "Finally, when there is a tie, factors other than running order should be considered.", "labels": [], "entities": []}, {"text": "In all of the above experiments, the Evidence was corrected using the c-rater's automatic spelling corrector using the stimulus (in case of Reading), the concepts, and the prompts to guide the selection of the correctly-spelled words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Best on BLIND over all experiments", "labels": [], "entities": [{"text": "BLIND", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9441221356391907}]}, {"text": " Table 4. Voting Strategy results on BLIND", "labels": [], "entities": [{"text": "BLIND", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.48894521594047546}]}]}