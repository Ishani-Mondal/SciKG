{"title": [{"text": "Efficient Linearization of Tree Kernel Functions", "labels": [], "entities": []}], "abstractContent": [{"text": "The combination of Support Vector Machines with very high dimensional kernels, such as string or tree kernels, suffers from two major drawbacks: first, the implicit representation of feature spaces does not allow us to understand which features actually triggered the generalization; second, the resulting computational burden may in some cases render un-feasible to use large data sets for training.", "labels": [], "entities": []}, {"text": "We propose an approach based on feature space reverse engineering to tackle both problems.", "labels": [], "entities": [{"text": "feature space reverse engineering", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.7475104331970215}]}, {"text": "Our experiments with Tree Kernels on a Semantic Role Labeling data set show that the proposed approach can drastically reduce the computational footprint while yielding almost unaffected accuracy.", "labels": [], "entities": [{"text": "Semantic Role Labeling data set", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7047441363334656}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9951307773590088}]}], "introductionContent": [{"text": "The use of Support Vector Machines (SVMs) in supervised learning frameworks is spreading across different communities, including Computational Linguistics and Natural Language Processing, thanks to their solid mathematical foundations, efficiency and accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9958540201187134}]}, {"text": "Another important reason for their success is the possibility of using kernel functions to implicitly represent examples in some high dimensional kernel space, where their similarity is evaluated.", "labels": [], "entities": []}, {"text": "Kernel functions can generate a very large number of features, which are then weighted by the SVM optimization algorithm obtaining a feature selection side-effect.", "labels": [], "entities": []}, {"text": "Indeed, the weights encoded by the gradient of the separating hyperplane learnt by the SVM implicitly establish a ranking between features in the kernel space.", "labels": [], "entities": []}, {"text": "This property has been exploited in feature selection models based on approximations or transformations of the gradient, e.g., or (.", "labels": [], "entities": []}, {"text": "However, kernel based systems have two major drawbacks: first, new features maybe discovered in the implicit space but they cannot be directly observed.", "labels": [], "entities": []}, {"text": "Second, since learning is carried out in the dual space, it is not possible to use the faster SVM or perceptron algorithms optimized for linear spaces.", "labels": [], "entities": []}, {"text": "Consequently, the processing of large data sets can be computationally very expensive, limiting the use of large amounts of data for our research or applications.", "labels": [], "entities": []}, {"text": "We propose an approach that tries to fill in the gap between explicit and implicit feature representations by 1) selecting the most relevant features in accordance with the weights estimated by the SVM and 2) using these features to build an explicit representation of the kernel space.", "labels": [], "entities": []}, {"text": "The most innovative aspect of our work is the attempt to model and implement a solution in the context of structural kernels.", "labels": [], "entities": []}, {"text": "In particular we focus on Tree Kernel (TK) functions, which are especially interesting for the Computational Linguistics community as they can effectively encode rich syntactic data into a kernelbased learning algorithm.", "labels": [], "entities": []}, {"text": "The high dimensionality of a TK feature space poses interesting challenges in terms of computational complexity that we need to address in order to come up with a viable solution.", "labels": [], "entities": []}, {"text": "We will present a number of experiments carried out in the context of Semantic Role Labeling, showing that our approach can noticeably reduce training time while yielding almost unaffected classification accuracy, thus allowing us to handle larger data sets at a reasonable computational cost.", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.8567952911059061}, {"text": "accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.5666458606719971}]}, {"text": "The rest of the paper is structured as follows: Sec- Figure 1: Esemplification of a fragment space and the kernel product between two trees.", "labels": [], "entities": []}, {"text": "tion 2 will briefly review SVMs and Tree Kernel functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous work on related subjects; Section 5 will describe our experiments and comment on their results; finally, in Section 6 we will draw our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations () and automatic Charniak parse trees) as provided for the CoNLL 2005 evaluation campaign).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.7800159454345703}, {"text": "CoNLL 2005 evaluation campaign", "start_pos": 151, "end_pos": 181, "type": "DATASET", "confidence": 0.918133869767189}]}, {"text": "SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word ware identified, and role classification, where each argument is assigned the proper role.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9432716369628906}, {"text": "boundary detection", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.717383474111557}, {"text": "role classification", "start_pos": 143, "end_pos": 162, "type": "TASK", "confidence": 0.8355761170387268}]}, {"text": "The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role Multi-class Classifier (RM).", "labels": [], "entities": []}, {"text": "If the constituency parse tree t of a sentence sis available, we can look at all the pairs p, n i , where n i is any node in the tree and p is the node dominating w, and decide whether n i is an argument node or not, i.e. whether it exactly dominates all and only the words encoding any of w's arguments.", "labels": [], "entities": []}, {"text": "The objects that we classify are subsets of the input parse tree that encompass both p and n i . Namely, we use the AST m structure defined in (, which is the minimal tree that covers all and only the words of p and n i . In the AST m , p and n i are marked so that they can be distinguished from the other nodes.", "labels": [], "entities": [{"text": "AST m", "start_pos": 229, "end_pos": 234, "type": "TASK", "confidence": 0.8516236543655396}]}, {"text": "An AST m is regarded as a positive example for BC if n i is an argument node, otherwise it is considered a negative example.", "labels": [], "entities": [{"text": "AST m", "start_pos": 3, "end_pos": 8, "type": "TASK", "confidence": 0.878990650177002}, {"text": "BC", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9257044792175293}]}, {"text": "Positive BC examples can be used to train an efficient RM: for each role r we can train a classifier whose positive examples are argument nodes whose label is exactly r, whereas negative examples are argument nodes labeled r = r.", "labels": [], "entities": [{"text": "RM", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9737752079963684}]}, {"text": "Two AST m s extracted from an example parse tree are shown in: the first structure is a negative example for BC and is not part of the data set of RM, whereas the second is a positive instance for BC and A1.", "labels": [], "entities": [{"text": "AST m", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.8782235682010651}]}, {"text": "To train BC we used PropBank sections 1 through 6, extracting AST m structures out of the first 1 million p, n i pairs from the corresponding parse trees.", "labels": [], "entities": [{"text": "BC", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9382420182228088}]}, {"text": "As a test set we used the 149,140 instance collected from the annotations in Section 24.", "labels": [], "entities": []}, {"text": "There are 61,062 positive examples in the training set (i.e. 6.1%) and 8,515 in the test set (i.e. 5.7%).", "labels": [], "entities": []}, {"text": "For RM we considered all the argument nodes of any of the six PropBank core roles (i.e. A0, . .", "labels": [], "entities": [{"text": "RM", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9479276537895203}]}, {"text": ", A5) from all the available training sections, i.e. 2 through 21, fora total of 179,091 training instances.", "labels": [], "entities": []}, {"text": "Similarly, we collected 5,928 test instances from the annotations of Section 24.", "labels": [], "entities": []}, {"text": "In the remainder, we will mark with an the linearized classifiers, i.e. BC and RM will refer to the linearized boundary and role classifiers, respectively.", "labels": [], "entities": [{"text": "BC", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9904327392578125}]}, {"text": "Their traditional, vanilla SST counterparts will be simply referred to as BC and RM.", "labels": [], "entities": [{"text": "SST", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9142813086509705}]}, {"text": "We used 10 splits for the FMI stage and we set maxdepth = 4 and maxexp = 5 during FMI and TFX.", "labels": [], "entities": [{"text": "maxdepth", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.96402907371521}]}, {"text": "We didn't carryout an extensive validation of these parameters.", "labels": [], "entities": []}, {"text": "These values were selected during the development of the software because, on a very small development set, they resulted in a very responsive system.", "labels": [], "entities": []}, {"text": "Since the main topic of this paper is the assessment of the efficiency and accuracy of our linearization technique, we did not carryout an evaluation on the whole SRL task using the official CoNLL'05 evaluator.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9990230798721313}, {"text": "SRL task", "start_pos": 163, "end_pos": 171, "type": "TASK", "confidence": 0.8354924917221069}, {"text": "CoNLL'05 evaluator", "start_pos": 191, "end_pos": 209, "type": "DATASET", "confidence": 0.9326132833957672}]}, {"text": "Indeed, producing complete annotations requires several steps (e.g. overlap resolution, OvA or Pairwise combination of individual role classifiers) that would shade off the actual impact of the methodology on classification.", "labels": [], "entities": [{"text": "overlap resolution", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7112345099449158}, {"text": "OvA", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9694530963897705}]}, {"text": "All the experiments were run on a machine equipped with 4 Intel R Xeon R CPUs clocked at 1.6 GHz and 4 GB of RAM running on a Linux 2.6.9 kernel.", "labels": [], "entities": []}, {"text": "As a supervised learning framework we used SVM-Light-TK 1 , which extends the SVMLight optimizer) with tree kernel support.", "labels": [], "entities": []}, {"text": "During FSL, we learn the models using a normalized SST kernel and the default decay factor \u03bb = 0.4.", "labels": [], "entities": [{"text": "FSL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8171212077140808}]}, {"text": "The same parameters are used to train the models of the non linearized classifiers.", "labels": [], "entities": []}, {"text": "During ESL, the classifier is trained using a linear kernel.", "labels": [], "entities": []}, {"text": "We did not carryout further parametrization of the learning algorithm.", "labels": [], "entities": []}, {"text": "The left side of shows the distribution of positive (Column Pos) and negative (Neg) data points in each classifier's training set.", "labels": [], "entities": []}, {"text": "The central group of columns lists training and test efficiency and accuracy of BC and RM, i.e. the nonlinearized classifiers, along with figures for the individual role classifiers that makeup RM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9992497563362122}]}, {"text": "Training BC took more than two days of CPU time and testing about 4 hours.", "labels": [], "entities": []}, {"text": "The classifier achieves an F 1 measure of 81.76, with a good balance between precision and recall.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9896072944005331}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9995858073234558}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9976425766944885}]}, {"text": "Concerning RM, sequential training of the 6 models took 2,596 minutes, while classification took 27 minutes.", "labels": [], "entities": [{"text": "RM", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9266879558563232}, {"text": "classification", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.9577909708023071}]}, {"text": "The slowest of the individual role classifiers happens to be A1, which has an almost 1:1 ratio between positive and negative examples, i.e. they are 90,636 and 88,455 respectively.", "labels": [], "entities": [{"text": "A1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9973168969154358}]}, {"text": "We varied the threshold value (i.e. the number of fragments that we mine from each model, see Section 3) to measure its effect on the resulting classifier accuracy and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9922741055488586}]}, {"text": "In this context, we call training time all the time necessary to obtain a linearized model, i.e. the sum of FSL, FMI and TFX time for every split, plus the time for ESL.", "labels": [], "entities": [{"text": "FSL", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9647433161735535}, {"text": "TFX time", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9092408716678619}]}, {"text": "Similarly, we call test time the time necessary to classify a linearized test set, i.e. the sum of TFX and ESC on test data.", "labels": [], "entities": [{"text": "TFX", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8996888399124146}, {"text": "ESC", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9365076422691345}]}, {"text": "In we plot the efficiency of BC learn-ing with respect to different threshold values.", "labels": [], "entities": [{"text": "BC learn-ing", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.6495348215103149}]}, {"text": "The Overall training time is shown alongside with partial times coming from FSL (which is the same for every threshold value and amounts to 433 minutes), FMI, training data TFX and ESL.", "labels": [], "entities": [{"text": "FSL", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9981828331947327}, {"text": "FMI", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.7426868081092834}, {"text": "TFX", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.5023269653320312}, {"text": "ESL", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9061416387557983}]}, {"text": "The plot shows that TFX has a logarithmic behaviour, and that quite soon becomes the main player in total training time after FSL.", "labels": [], "entities": [{"text": "TFX", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.7369962334632874}, {"text": "FSL", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.8189955949783325}]}, {"text": "For threshold values lower than 10k, ESL time decreases as the threshold increases: too few fragments are available and adding new ones increases the probability of including relevant fragments in the dictionary.", "labels": [], "entities": [{"text": "ESL time", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8115904927253723}]}, {"text": "After 10k, all the relevant fragments are already there and adding more only makes computation harder.", "labels": [], "entities": []}, {"text": "We can see that fora threshold value of 100k total training time amounts to 1,104 minutes, i.e. 36% of BC.", "labels": [], "entities": [{"text": "BC", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9993460774421692}]}, {"text": "For a threshold value of 10k, learning time further decreases to 916 minutes, i.e. less than 30%.", "labels": [], "entities": [{"text": "learning time", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9144743978977203}]}, {"text": "This threshold value was used to train the individual linearized role classifiers that makeup RM . These considerations are backed by the trend of classification accuracy shown in, where the Precision, Recall and F 1 measure of BC , evaluated on the test set, are shown in comparison with BC.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9521768093109131}, {"text": "Precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9922699928283691}, {"text": "Recall", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.878046989440918}, {"text": "F 1 measure", "start_pos": 213, "end_pos": 224, "type": "METRIC", "confidence": 0.9543382128079733}, {"text": "BC", "start_pos": 228, "end_pos": 230, "type": "METRIC", "confidence": 0.6827223896980286}, {"text": "BC", "start_pos": 289, "end_pos": 291, "type": "METRIC", "confidence": 0.9702761769294739}]}, {"text": "We can see that BC precision is almost constant, while its recall increases as we increase the threshold, reaches a maximum of 78.95% fora threshold of 10k and then settles around 78.8%.", "labels": [], "entities": [{"text": "BC", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9863967299461365}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8145669102668762}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9997984766960144}]}, {"text": "The F 1 score is maximized fora threshold of 10k, where it measures 81.10, i.e. just 0.66 points less than BC.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9866334001223246}, {"text": "BC", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9947425127029419}]}, {"text": "We can also see that BC is constantly more conservative than BC, i.e. it always has higher precision and lower recall.  are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes).", "labels": [], "entities": [{"text": "BC", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9750216007232666}, {"text": "BC", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9201812744140625}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.99892657995224}, {"text": "recall.", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9993016719818115}]}, {"text": "Concerning RM , we can see that the accuracy loss is even less than with BC , i.e. it reaches an F 1 measure of 87.13 which is just 0.52 less than RM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.999718964099884}, {"text": "BC", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9866885542869568}, {"text": "F 1 measure", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9857000708580017}]}, {"text": "It is also interesting to note how the individual linearized role classifiers manage to perform accurately regardless of the distribution of examples in the data set: for all the six classifiers the final accuracy is inline with that of the corresponding non-linearized classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9653530120849609}]}, {"text": "In two cases, i.e. A2 and A4, the accuracy of the linearized classifier is even higher, i.e. 74.20 vs. 73.13 and 69.72 vs. 69.10, respectively.", "labels": [], "entities": [{"text": "A2", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.931423008441925}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995958209037781}]}, {"text": "As for the efficiency, total training time for RM is 37% of RM, i.e. 1,190 vs. 2,596 minutes, while test time is reduced to 60%, i.e. 16 vs 27 minutes.", "labels": [], "entities": [{"text": "RM", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.7798486948013306}, {"text": "RM", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.984459638595581}]}, {"text": "These improvements are less evident than those measured for boundary detection.", "labels": [], "entities": [{"text": "boundary detection", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.827315628528595}]}, {"text": "The main reason is that the training set for boundary classification is much larger, i.e. 1 million vs. 179k instances: therefore, splitting training data during FSL has a reduced impact on the overall efficiency of RM . Parallelization.", "labels": [], "entities": [{"text": "boundary classification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.6457395702600479}, {"text": "RM", "start_pos": 216, "end_pos": 218, "type": "TASK", "confidence": 0.9587233662605286}]}, {"text": "All the efficiency improvements that have been discussed so far considered a completely sequential process.", "labels": [], "entities": []}, {"text": "But one of the advantages of our approach is that it allows us to parallelize some aspect of SVM training.", "labels": [], "entities": [{"text": "SVM training", "start_pos": 93, "end_pos": 105, "type": "TASK", "confidence": 0.9024271667003632}]}, {"text": "Indeed, every activity (but ESL) can exploit some degree of parallelism: during FSL, all the models can be learnt at the same time (for this activity, the maximum degree of parallelization is conditioned by the number of training data splits); during FMI, models can be mined concurrently; during TFX, the data-set to be linearized can be split arbitrarily and individual segments can be processed in parallel.", "labels": [], "entities": []}, {"text": "Exploiting this possibility we can drastically improve learning efficiency.", "labels": [], "entities": []}, {"text": "As an example, in we show how the total learning of the BC can be cut to as low as 215 seconds when exploiting ten CPUs and using a threshold of 10k.", "labels": [], "entities": []}, {"text": "Even running on just 5 CPUs, the overall computational cost of BC is less than 10% of BC (Column Non Lin.).", "labels": [], "entities": [{"text": "BC", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.7383223176002502}, {"text": "BC", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9960898160934448}]}, {"text": "Similar considerations can be drawn concerning the role multi-classifier.", "labels": [], "entities": []}, {"text": "In this section we take a look at the fragments included in the dictionary of the BC classifier.", "labels": [], "entities": []}, {"text": "During FMI, we incrementally merge the fragments mined from each of the models learnt during FSL.", "labels": [], "entities": [{"text": "FMI", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.5056754946708679}, {"text": "FSL", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.8092628121376038}]}, {"text": "plots, for different threshold values, the percentage of new fragments (on they axis) that the i-th model (on the x axis) contributes with respect to the number of fragments mined from each model (i.e. the threshold value).", "labels": [], "entities": []}, {"text": "If we consider the curve fora threshold equal to 100k, we can see that each model after the first approximately contributes with the same number of fragments.", "labels": [], "entities": []}, {"text": "On the other hand, if the threshold is set to 1k than the contribution of subsequent models is increasingly more marginal.", "labels": [], "entities": []}, {"text": "Eventually, less than 10% of the fragments mined from the last model are new ones.", "labels": [], "entities": []}, {"text": "This behaviour suggests that there is a core set of very relevant fragments which is common across models learnt on different data, i.e. they are relevant for the task and do not strictly depend on the training data that we use.", "labels": [], "entities": []}, {"text": "When we increase the threshold value, the new fragments that we index are more and more data specific.", "labels": [], "entities": []}, {"text": "The dictionary compiled with a threshold of 10k lists 62,760 distinct fragments.", "labels": [], "entities": []}, {"text": "15% of the fragments contain the predicate node (which generally is the node encoding the predicate word's POS tag), more than one third contain the candidate argument node and, of these, about one third are rooted in it.", "labels": [], "entities": []}, {"text": "This last figure strongly suggests that the internal structure of an argument is indeed a very powerful feature not only for role classification, as we would expect, but also for boundary detection.", "labels": [], "entities": [{"text": "role classification", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8510646522045135}, {"text": "boundary detection", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.8015986680984497}]}, {"text": "About 10% of the fragments contain both the predicate and the argument node, while about 1% encode the Path feature traditionally used in explicit semantic role labeling models ().", "labels": [], "entities": []}, {"text": "About 5% encode a sort of extended Path feature, where the argument node is represented together with its descendants.", "labels": [], "entities": []}, {"text": "Overall, about 2/3 of the fragments contain at least some terminal symbol (i.e. words), generally a preposition or an adverb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy (P, R, F 1 ), training (Train) and test (Test) time of non-linearized (center) and linearized (right)  classifiers. Times are in minutes. For each task, columns Pos and Neg list the number of positive and negative training  examples, respectively. The accuracy of the role multiclassifiers is the micro-average of the individual classifiers  trained to recognize core PropBank roles.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986166954040527}, {"text": "accuracy", "start_pos": 271, "end_pos": 279, "type": "METRIC", "confidence": 0.9995073080062866}]}]}