{"title": [{"text": "Automatic Extraction of Citation Contexts for Research Paper Summarization: A Coreference-chain based Approach", "labels": [], "entities": [{"text": "Automatic Extraction of Citation Contexts", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7291357934474945}, {"text": "Research Paper Summarization", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6134726603825887}]}], "abstractContent": [{"text": "This paper proposes anew method based on coreference-chains for extracting citations from research papers.", "labels": [], "entities": [{"text": "extracting citations from research papers", "start_pos": 64, "end_pos": 105, "type": "TASK", "confidence": 0.8517866730690002}]}, {"text": "To evaluate our method we created a corpus of citations comprised of citing papers for 4 cited papers.", "labels": [], "entities": []}, {"text": "We analyze some phenomena of citations that are present in our corpus, and then evaluate our method against a cue-phrase-based technique.", "labels": [], "entities": []}, {"text": "Our method demonstrates higher precision by 7-10%.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.999663233757019}]}], "introductionContent": [{"text": "Review and comprehension of existing research is fundamental to the ongoing process of conducting research; however, the ever increasing volume of research papers makes accomplishing this task increasingly more difficult.", "labels": [], "entities": []}, {"text": "To mitigate this problem of information overload, a form of knowledge reduction maybe necessary.", "labels": [], "entities": [{"text": "knowledge reduction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7770970165729523}]}, {"text": "Past research ( has shown that citations contain a plethora of latent information available and that much can be gained by exploiting it.", "labels": [], "entities": []}, {"text": "Indeed, there is a wealth of literature on topic-clustering, e.g. bibliographic coupling, or cocitation analysis.", "labels": [], "entities": [{"text": "bibliographic coupling", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7198030948638916}, {"text": "cocitation analysis", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7658766508102417}]}, {"text": "Subsequent research demonstrated that citations could be clustered on their quality, using keywords that appeared in the running-text of the citation).", "labels": [], "entities": []}, {"text": "Similarly, other work has shown the utility in the IR domain of ranking the relevance of cited papers by using supplementary index terms extracted from the content of citations in citing papers, including methods that search through a fixed character-length window, or that focus solely on the sentence containing the citation () for acquiring these terms.", "labels": [], "entities": [{"text": "IR domain", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.858709990978241}]}, {"text": "A prior case study () pointed out the challenges in proper identification of the full span of a citation in running text and acknowledged that fixed-width windows have their limits.", "labels": [], "entities": [{"text": "identification of the full span of a citation", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.7792582511901855}]}, {"text": "In contrast to this, endeavors have been made to extract the entire span of a citation by using cue-phrases collected and deemed salient by statistical merit ().", "labels": [], "entities": []}, {"text": "This has met in evaluations with some success.", "labels": [], "entities": []}, {"text": "The Cite-Sum system) also aims at knowledge reduction through use of citations.", "labels": [], "entities": [{"text": "knowledge reduction", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7861550450325012}]}, {"text": "It receives a paper title as a query and attempts to generate a summary of the paper by finding citing papers 1 and extracting citations in the running-text that refer to the paper.", "labels": [], "entities": []}, {"text": "Before outputting a summary, it also classifies extracted citation text, and removes citations with redundant content.", "labels": [], "entities": []}, {"text": "Another similar study aims at using the content of citations within citing papers to generate summaries of fields of research.", "labels": [], "entities": []}, {"text": "It is clear that merit exists behind extraction of citations in running text.", "labels": [], "entities": []}, {"text": "This paper proposes anew method for performing this task based on coreference-chains.", "labels": [], "entities": []}, {"text": "To evaluate our method we created a corpus of citations comprised of citing papers for 4 cited papers.", "labels": [], "entities": []}, {"text": "We also analyze some phenomena of citations that are present in our corpus.", "labels": [], "entities": []}, {"text": "The paper organization is as follows.", "labels": [], "entities": []}, {"text": "We first define terminology, discuss the construction of our corpus and the results found through its analysis, and then move onto our proposed method using coreference-chains.", "labels": [], "entities": []}, {"text": "We evaluate the proposed method by using the constructed corpus, and then conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our coreference-chain extraction method we compare it with a cue-phrases technique () and two baselines.", "labels": [], "entities": [{"text": "coreference-chain extraction", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.8986541926860809}]}, {"text": "Baseline 1 extracts only the c-site anchor sentence as the c-site; baseline 2 includes sentences before/after the c-site anchor sentence as part of the c-site with a 50/50 probability -it tosses a coin for each consecutive sentence to decide its inclusion.", "labels": [], "entities": []}, {"text": "We also created two hybrid methods that combine the results of the cue-phrases and coreference-chain techniques, one the union of their results (includes the extracted sentences of both methods), and the other the intersection (includes sentences only for which both methods agree), to measure their mutual compatibility.", "labels": [], "entities": []}, {"text": "The annotated corpus provided the locations of c-site anchors for the cited paper within the citing paper's running-text.", "labels": [], "entities": []}, {"text": "We then compared the extracted c-sites of each method to the c-sites of the annotated corpus.", "labels": [], "entities": []}, {"text": "The results of our experiments are presented in Table 4.", "labels": [], "entities": []}, {"text": "We evaluated each method as follows.", "labels": [], "entities": []}, {"text": "Recall and precision were measured fora c-site based on the number of extracted sentences; if an extracted sentence was annotated as part of the c-site, it counted as correct, and if an extracted sentence was not part of a c-site, incorrect; sentences annotated as being part of the c-site not extracted by the method counted as part of the total sentences for that c-site.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9874879121780396}, {"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9995371103286743}]}, {"text": "As an example, if an annotated csite has 3 sentences (including the c-site anchor sentence), and the evaluated method extracted 2 of these and 1 incorrect sentence, then the recall for this c-site using this method would be 2/3, and the precision 2/(2 + 1).", "labels": [], "entities": [{"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9953480362892151}, {"text": "precision", "start_pos": 237, "end_pos": 246, "type": "METRIC", "confidence": 0.997153639793396}]}, {"text": "Since the evaluation is inherently sentencebased, we provide two averages in.", "labels": [], "entities": []}, {"text": "The micro-average is for sentences across all c-sites; in other words, we tallied the correct and incorrect sentence count for the whole corpus and then divided by the total number of sentences (94).", "labels": [], "entities": []}, {"text": "This average provides a clearer picture on the efficacy of each method than does the macro-average.", "labels": [], "entities": []}, {"text": "The macro-average was computed per c-site (as explained above) and then averaged over the total number of c-sites in the corpus (50).", "labels": [], "entities": []}, {"text": "With the exception of a 3% lead in macroaverage recall, coreference-chains outperform cue-phrases in every way.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9712077975273132}]}, {"text": "We can see a substan-tial difference in micro-average precision, which results in nearly a 5% higher F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.8729321360588074}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9979189038276672}]}, {"text": "The macro-average precision is also higher by more than 6%.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9921703934669495}]}, {"text": "It matches more and misses far less.", "labels": [], "entities": []}, {"text": "The loss in the macro-average recall can be attributed to the coreference-chain method missing one of two sentences for several c-sites, which would lower its overall recall score; keep in mind that since in the macro-average all csites are treated equally, even large c-sites in which the coreference-chain method performs well, such an advantage will be reduced with averaging and is therefore misleading.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9156601428985596}, {"text": "recall score", "start_pos": 167, "end_pos": 179, "type": "METRIC", "confidence": 0.9886597692966461}]}, {"text": "Baseline 2 performed as expected, i.e. higher than baseline 1 for recall.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8776119947433472}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9953472018241882}]}, {"text": "Looking only at Fmeasures for evaluating performance in this case is misleading.", "labels": [], "entities": [{"text": "Fmeasures", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.978888988494873}]}, {"text": "This is particularly the case because precision is more important than recall -we want accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9992213249206543}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9988982677459717}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9984857439994812}]}, {"text": "Coreference-chains achieved a precision of over 87.2 compared to the 71.2 of baseline 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9991174340248108}]}, {"text": "The combined methods also showed promise.", "labels": [], "entities": []}, {"text": "In particular, the intersection method had very high precision (91.2 and 95.7), and marginally managed to extract more sentences than baseline 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.999261200428009}]}, {"text": "The union method has more conservative scores.", "labels": [], "entities": []}, {"text": "We also understood from our corpus that only about half of c-sites were represented by c-site anchor sentences.", "labels": [], "entities": []}, {"text": "The largest c-site in the corpus was 6 sentences, and the average 1.8.", "labels": [], "entities": []}, {"text": "This means using the c-site anchor sentence alone excludes on average about half of the valuable data.", "labels": [], "entities": []}, {"text": "These results are promising, but a larger corpus is necessary to validate the results presented here.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results for coreference resolution against the MUC-7 formal corpus.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.9882753491401672}, {"text": "MUC-7 formal corpus", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.9572083155314127}]}, {"text": " Table 4: Evaluation results for c-site extraction w/o background information", "labels": [], "entities": [{"text": "c-site extraction", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.6961578875780106}]}]}