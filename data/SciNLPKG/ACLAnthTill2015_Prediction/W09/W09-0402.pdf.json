{"title": [{"text": "Syntax-oriented evaluation measures for machine translation output", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8016113638877869}]}], "abstractContent": [{"text": "We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the BLEU score on the detailed Part-of-Speech (POS) tags as well as the precision, recall and F-measure obtained on POS n-grams.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.804616908232371}, {"text": "BLEU score", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9754650890827179}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9995494484901428}, {"text": "recall", "start_pos": 215, "end_pos": 221, "type": "METRIC", "confidence": 0.9976454377174377}, {"text": "F-measure", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.9981624484062195}]}, {"text": "We also introduced F-measure based on both word and POS n-grams.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9932500123977661}]}, {"text": "Correlations between the new met-rics and human judgments were calculated on the data of the first, second and third shared task of the Statistical Machine Translation Workshop.", "labels": [], "entities": [{"text": "Statistical Machine Translation Workshop", "start_pos": 136, "end_pos": 176, "type": "TASK", "confidence": 0.8336449712514877}]}, {"text": "Machine translation outputs in four different European languages were taken into account: En-glish, Spanish, French and German.", "labels": [], "entities": []}, {"text": "The results show that the new measures correlate very well with the human judgements and that they are competitive with the widely used BLEU, METEOR and TER metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9982267022132874}, {"text": "METEOR", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.921875}, {"text": "TER", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.9816768169403076}]}], "introductionContent": [{"text": "We proposed several syntax-oriented automatic evaluation measures based on sequences of POS tags and investigated how they correlate with human judgments.", "labels": [], "entities": []}, {"text": "The new measures are the POS-BLEU score, i.e. the BLEU score calculated on POS tags instead of words, as well as the POSP, the POSR and the POSF score: precision, recall and Fmeasure calculated on POS n-grams.", "labels": [], "entities": [{"text": "POS-BLEU score", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.7404057383537292}, {"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.97970250248909}, {"text": "POSP", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.8714588284492493}, {"text": "POSR", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.8331589102745056}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9989820122718811}, {"text": "recall", "start_pos": 163, "end_pos": 169, "type": "METRIC", "confidence": 0.9991857409477234}, {"text": "Fmeasure", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9990966320037842}]}, {"text": "In addition to the metrics based only on POS tags, we investigated a WPF score, i.e. an F-measure which takes into account both word and POS n-grams.", "labels": [], "entities": [{"text": "WPF score", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.8860288560390472}, {"text": "F-measure", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.98589688539505}]}, {"text": "The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (), second and third shared translation task.", "labels": [], "entities": []}, {"text": "Preliminary experiments were carried out on the data from the first (2006) and the second task (2007) -Spearman's rank correlation coefficients between the adequacy and fluency scores and the POSBLEU, POSP, POSR and POSF scores were calculated.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 103, "end_pos": 130, "type": "METRIC", "confidence": 0.7395614087581635}, {"text": "POSBLEU", "start_pos": 192, "end_pos": 199, "type": "METRIC", "confidence": 0.5983290076255798}, {"text": "POSF", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.8087033629417419}]}, {"text": "The POSBLEU and the POSF score were shown to be the most promising, so that these metrics were submitted to the official shared evaluation task 2008.", "labels": [], "entities": [{"text": "POSBLEU", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9767613410949707}, {"text": "POSF score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9568313658237457}]}, {"text": "The results of this evaluation showed that these metrics also correlate well on the document level with another human score, i.e. the sentence ranking.", "labels": [], "entities": []}, {"text": "However, on the sentence level the results were less promising.", "labels": [], "entities": []}, {"text": "The possible reason for this is the main drawback of the metrics based on pure POS tags, i.e. neglecting the lexical aspect.", "labels": [], "entities": []}, {"text": "Therefore we also introduced a WPF score which takes into account both word n-grams and POS n-grams.", "labels": [], "entities": [{"text": "WPF score", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.7946048080921173}]}], "datasetContent": [{"text": "\u2022 WPF F-measure based both on word and POS ngrams: takes into account all word n-grams and all POS n-grams which have a counterpart both in the corresponding reference and hypothesis.", "labels": [], "entities": [{"text": "WPF", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.7723715305328369}, {"text": "F-measure", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.68378084897995}]}, {"text": "The prerequisite for all metrics is availability of an appropriate POS tagger for the target language.", "labels": [], "entities": []}, {"text": "It should be noted that the POS tags cannot be only basic but must have all details (e.g. verb tenses, cases, number, gender, etc.).", "labels": [], "entities": []}, {"text": "The n-gram scores as well as the POSBLEU score are based on fourgrams (i.e. the value of maximal n is 4).", "labels": [], "entities": [{"text": "POSBLEU score", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.8680152595043182}]}, {"text": "For the n-gram-based measures, two types of n-gram averaging were investigated: geometric mean and aritmetic mean.", "labels": [], "entities": []}, {"text": "Geometric mean is already widely used in the BLEU score, but is also argued not to be optimal because the score becomes equal to zero even if only one of the ngram counts is equal to zero.", "labels": [], "entities": [{"text": "Geometric mean", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9843002557754517}, {"text": "BLEU score", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9635234773159027}]}, {"text": "However, this problem is probably less critical for POS-based metrics because the tag set sizes are much smaller than vocabulary sizes.", "labels": [], "entities": []}, {"text": "The preliminary experiments with the new evaluation metrics were performed on the data from the first two shared tasks in order to investigate Spearman correlation coefficients \u03c1 between POSbased evaluation measures and the human scores adequacy and fluency.", "labels": [], "entities": [{"text": "Spearman correlation coefficients \u03c1", "start_pos": 143, "end_pos": 178, "type": "METRIC", "confidence": 0.7473316639661789}]}, {"text": "The metrics described in Section 2 (except the WPF score) were calculated for all translation outputs.", "labels": [], "entities": [{"text": "WPF score)", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.7206909656524658}]}, {"text": "For each new metric, the \u03c1 coefficient with the adequacy and with the fluency score on the document level were calculated.", "labels": [], "entities": [{"text": "\u03c1 coefficient", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9667362868785858}]}, {"text": "Then the results were summarised by averaging obtained coefficients overall translation outputs, and the average correlations are presented in shows that the new measures have high \u03c1 coefficients both with respect to the adequacy and to the fluency score.", "labels": [], "entities": []}, {"text": "The POSBLEU score has the highest correlations, followed by the POSF score.", "labels": [], "entities": [{"text": "POSBLEU", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.7697639465332031}, {"text": "correlations", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9781428575515747}, {"text": "POSF score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9124751091003418}]}, {"text": "Furthermore, the POSBLEU score has higher correlations than each of the three widely used metrics, and all the new metrics except the POSP have higher correlations than the TER.", "labels": [], "entities": [{"text": "POSBLEU score", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.8997537195682526}, {"text": "POSP", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.6161514520645142}, {"text": "TER", "start_pos": 173, "end_pos": 176, "type": "METRIC", "confidence": 0.9834533333778381}]}, {"text": "The POSF correlations with the fluency are higher than those for the standard metrics, and with the adequacy are comparable to those for the METEOR and the BLEU score.", "labels": [], "entities": [{"text": "POSF correlations", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9718441963195801}, {"text": "METEOR", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9570415616035461}, {"text": "BLEU score", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.96741983294487}]}, {"text": "presents the percentage of the documents for which the particular new metric has higher correlation than BLEU, METEOR or TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9987050294876099}, {"text": "METEOR", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9786230325698853}, {"text": "TER", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.975023090839386}]}, {"text": "It can be seen that on the majority of the documents the POSBLEU metric outperforms all three standard measures, especially the correlation with the fluency score.", "labels": [], "entities": [{"text": "POSBLEU metric", "start_pos": 57, "end_pos": 71, "type": "METRIC", "confidence": 0.9196279048919678}]}, {"text": "The geometric mean POSF shows similar behaviour, having higher correlation than the standard measures in majority of the cases but slightly less often than the POSBLEU.", "labels": [], "entities": [{"text": "POSF", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.6119092106819153}, {"text": "correlation", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.968864917755127}]}, {"text": "The POSR has higher correlation than the standard measures in 50-70% of cases, and the POSP score has the lowest percentage, 30-60%.", "labels": [], "entities": [{"text": "POSR", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9950146079063416}, {"text": "correlation", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9885618686676025}, {"text": "POSP score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9745981395244598}]}, {"text": "It can be also seen that the geometric mean averaging of the n-grams correlates better with the human judgments more often than the artimetic mean.", "labels": [], "entities": []}, {"text": "For the official shared evaluation task in 2008, the human evaluation scores were different -the adequacy and fluency scores were abandoned being rather time consuming and often inconsistent, and the sentence ranking was proposed as one of the human evaluation scores: the manual evaluators were asked to rank translated sentences relative to each other.", "labels": [], "entities": []}, {"text": "RWTH participated in this shared task with the two most promising metrics according to the previous experiments, i.e. POSBLEU and POSF, and the detailed results can be found in.", "labels": [], "entities": [{"text": "RWTH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9331930875778198}, {"text": "POSBLEU", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.7075408101081848}, {"text": "POSF", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.6745297908782959}]}, {"text": "It was shown that these metrics also correlate very well with the sentence ranking on the document level.", "labels": [], "entities": []}, {"text": "However, on the sentence level the performance was much weaker: a percentage of sentence pairs for which the human comparison yields the same result as the comparison using particular automatic metric was not very high.", "labels": [], "entities": []}, {"text": "We believe that the main reason for this is the fact that the metrics based only on the POS tags can assign high scores to translations without correct semantic meaning, because they are taking into account only syntactic structure without taking into account the actual words.", "labels": [], "entities": []}, {"text": "For example, if the reference translation is \"This sentence is correct\", a translation output \"This tree is high\" would have a POS-based matching score of 100%.", "labels": [], "entities": [{"text": "POS-based matching score", "start_pos": 127, "end_pos": 151, "type": "METRIC", "confidence": 0.8504577279090881}]}, {"text": "Therefore we introduced the WPF score -an F-measure metrics which counts both matching POS n-grams and matching word n-grams.", "labels": [], "entities": [{"text": "WPF score", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.6446400582790375}]}, {"text": "The \u03c1 coefficients for the POSBLEU, POSF and WPF with the sentence ranking averaged overall translation outputs are shown in.", "labels": [], "entities": [{"text": "POSF", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8615953922271729}]}, {"text": "The correlations for several known metrics are shown as well, i.e. for the BLEU, METEOR and TER along with their variants: METEOR-r denotes the variant optimised for ranking, whereas MBLEU and MTER are BLEU and TER computed using the flexible matching as used in METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9980865716934204}, {"text": "METEOR", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.8804998993873596}, {"text": "TER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9515597224235535}, {"text": "METEOR-r", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.7726686000823975}, {"text": "MBLEU", "start_pos": 183, "end_pos": 188, "type": "METRIC", "confidence": 0.9919390082359314}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9932699799537659}, {"text": "METEOR", "start_pos": 263, "end_pos": 269, "type": "DATASET", "confidence": 0.9249933958053589}]}, {"text": "It can be seen that the correlation coefficients for all three syntactic metrics are high.", "labels": [], "entities": [{"text": "correlation", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9732898473739624}]}, {"text": "The POSBLEU score has the highest correlation with the sentence ranking, followed by presents the percentage of the documents where the particular syntactic metric has higher correlation with the sentence ranking than the particular standard metric.", "labels": [], "entities": [{"text": "POSBLEU score", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.6651301085948944}]}, {"text": "All syntactic metrics have higher correlation than the MTER on almost all documents, and on a large number of documents than the MBLEU score.", "labels": [], "entities": [{"text": "correlation", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9595242142677307}, {"text": "MTER", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.7807861566543579}, {"text": "MBLEU", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.9772726893424988}]}, {"text": "The correlations for syntactic measures are better than those for the BLEU score for more than 60% of documents.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9695558845996857}]}, {"text": "As for the METEOR scores, the syntactic metrics are comparable (about 50%).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.903441309928894}]}], "tableCaptions": [{"text": " Table 1: Average system-level correlations be- tween automatic evaluation measures and ade- quacy/fluency scores for 2006 and 2007 test data  (gm = geometric mean for n-gram averaging, am  = arithmetic mean).", "labels": [], "entities": [{"text": "arithmetic mean", "start_pos": 192, "end_pos": 207, "type": "METRIC", "confidence": 0.9221681654453278}]}, {"text": " Table 3: Average system-level correlations be- tween automatic evaluation measures and human  ranking for 2008 test data.", "labels": [], "entities": []}, {"text": " Table 2: Percentage of documents from the 2006 and 2007 shared tasks where the particular new metric  has better correlation with adequacy/fluency than the particular standard metric.", "labels": [], "entities": []}, {"text": " Table 4: Percentage of documents from the 2008 shared task where the new metric has better correlation  with the human sentence ranking than the standard metric.", "labels": [], "entities": []}]}