{"title": [{"text": "Multilingual Resources, Technologies and Evaluation for Central and Eastern European Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "One of the LT 1-applications that ensures the access to the information, in the user's mother tongue, is machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.7980459809303284}]}, {"text": "Unfortunately less spoken languages-a category in which the Balkan and Slavic languages can be included-have to overcome a major gap in language resources, reference-systems and tools.", "labels": [], "entities": []}, {"text": "In its simplest form, statistical machine translation (SMT) is based only on the existence of a big parallel corpus and therefore it seems to be a solution for these languages.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.8343855986992518}]}, {"text": "In this paper the performance of a Moses-based SMT system, for Roma-nian and German, is investigated using test data from two different domains-legislation (JRC-ACQUIS) and a manual of an electronic device.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9427986145019531}]}, {"text": "The obtained results are compared with the ones given by the Google on-line translation tool.", "labels": [], "entities": [{"text": "Google on-line translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.5660976469516754}]}, {"text": "An analysis of the obtained translation results gives an overview of the main challenges and sources of errors in translation, in these experimental settings .", "labels": [], "entities": [{"text": "translation", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.9694066047668457}]}], "introductionContent": [{"text": "\"Less interesting languages\" 2 have to overcome a major gap in language resources, reference-systems and tools which ensure the development of an MT-system of higher quality.", "labels": [], "entities": [{"text": "MT-system", "start_pos": 146, "end_pos": 155, "type": "TASK", "confidence": 0.9269595742225647}]}, {"text": "In its simplest form, statistical machine translation (SMT) is based only on the existence of a big parallel corpus, thereby it seems to be a solution for this kind of languages.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.8317689547936121}]}, {"text": "From the currently available corpora for the languages considered in the description of the workshop, JRC-ACQUIS is used for the experiments described in this paper.", "labels": [], "entities": [{"text": "JRC-ACQUIS", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.7651009559631348}]}, {"text": "The languages addressed are Romanian and German.", "labels": [], "entities": []}, {"text": "The size of bilingual subsets of JRC-ACQUIS differs strongly from language pair to language pair, e.g. for English-German the size of the corpus is over 1 million sentences, for GermanRomanian is less than 350000 sentences.", "labels": [], "entities": [{"text": "GermanRomanian", "start_pos": 178, "end_pos": 192, "type": "DATASET", "confidence": 0.9317688941955566}]}, {"text": "Compared to EUROPARL or to the \"News Corpus\" used in recent investigations in the EUROMATRIX project, bilingual subsets in JRC-ACQUIS have approximately six times less aligned sentences, for the language-pair considered.", "labels": [], "entities": [{"text": "EUROPARL", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.9388306736946106}, {"text": "News Corpus\"", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9058247407277426}, {"text": "JRC-ACQUIS", "start_pos": 123, "end_pos": 133, "type": "DATASET", "confidence": 0.8992039561271667}]}, {"text": "In this paper the performance of a simplistic Mosesbased SMT-system, when trained and tested on JRC-ACQUIS (version 2.2), is investigated.", "labels": [], "entities": [{"text": "SMT-system", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.8899909853935242}, {"text": "JRC-ACQUIS", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9156504273414612}]}, {"text": "For one of the test-set, data from a small technical corpus is used.", "labels": [], "entities": []}, {"text": "The obtained results are compared with the ones given by the Google SMT on-line translation tool.", "labels": [], "entities": [{"text": "SMT on-line translation", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8506794571876526}]}, {"text": "The outcome shows that for less resourced languages -in this case Romanian -the development of further parallel corpora on broader domains and the improvement of the existing resources seem to be unavoidable.", "labels": [], "entities": []}, {"text": "In the case of JRC-ACQUIS, for Romanian, such a step has already been done with JRC-ACQUIS Version 3 3 . The paper is organized as follows: in section 2 the used corpora are presented; sections 3 and 4 describe the experiments performed and their results.", "labels": [], "entities": [{"text": "JRC-ACQUIS", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.8677809238433838}, {"text": "JRC-ACQUIS Version 3", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.9082462390263876}]}, {"text": "The last section concludes the presented results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The SMT system used follows the description of the baseline system given for the EACL 2009 4th Workshop on SMT 9 and it is based on Moses 10 -see.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9898256659507751}, {"text": "EACL 2009 4th Workshop on SMT 9", "start_pos": 81, "end_pos": 112, "type": "TASK", "confidence": 0.7112187317439488}, {"text": "Moses 10", "start_pos": 132, "end_pos": 140, "type": "DATASET", "confidence": 0.9127896130084991}]}, {"text": "Wanting to see what results can be obtained by a very simple SMT, two parameters were changed: the tuning step is left out and the LM order is 3.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9939948916435242}]}, {"text": "All test data-sets were translated with the Mosesbased system and with the Google on-line translation tool 11 . In both cases, the same metrics were used for evaluation: BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9992730021476746}, {"text": "TER", "start_pos": 179, "end_pos": 182, "type": "METRIC", "confidence": 0.9977701902389526}]}, {"text": "For these experiments the use of other linguistic resources was avoided deliberately, in order to be able to evaluate the robustness of a pure SMT-System at domain change.", "labels": [], "entities": [{"text": "SMT-System", "start_pos": 143, "end_pos": 153, "type": "TASK", "confidence": 0.9806937575340271}]}, {"text": "When changing the domain it is expected that out-oftraining-vocabulary words (OOV-Words) -especially in domain specific vocabulary -play a major role.", "labels": [], "entities": []}, {"text": "In the following subsection this aspect is presented.", "labels": [], "entities": []}, {"text": "In the experiments, due to the lack of multiple references, the comparison with only one reference translation is considered.", "labels": [], "entities": []}, {"text": "The following metrics are used: \u2022 BLEU (bilingual evaluation understudy) -The NIST/BLEU implementation, version 12 12 is used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9983079433441162}, {"text": "NIST", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.8769164085388184}, {"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.7657690048217773}]}, {"text": "Although criticized, BLEU is mostly used in the last years for MT evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9981860518455505}, {"text": "MT evaluation", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9496355354785919}]}, {"text": "It measures the number of n-grams, of different lengths, of the system output that appear in a set of references.", "labels": [], "entities": []}, {"text": "More details about BLEU can be found in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9896406531333923}]}, {"text": "As for previous developed systems BLEU is one of the evaluation metrics, for comparison reasons, it is still important to calculate it.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9970241189002991}]}, {"text": "\u2022 TER (translation error rate)  which moves sequences of words.", "labels": [], "entities": [{"text": "TER (translation error rate", "start_pos": 2, "end_pos": 29, "type": "METRIC", "confidence": 0.7634101390838623}]}, {"text": "More information about TER one can find in.", "labels": [], "entities": [{"text": "TER", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.974913477897644}]}, {"text": "The obtained results are shown in, and    The BLEU scores from, 4 and 5 are graphically represented in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.99922776222229}]}, {"text": "It is seen from, 4 and 5 that the BLEU and the TER scores are in all cases correlated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9995457530021667}, {"text": "TER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9991466999053955}]}, {"text": "The interpretation of the results is focused on three directions 1.", "labels": [], "entities": []}, {"text": "variations of the evaluation metrics across sets of test data; 2.", "labels": [], "entities": []}, {"text": "the comparison with the Google MT on-line tool; 3.", "labels": [], "entities": []}, {"text": "A Moses-based system, that considers also Romanian and German, is described in.", "labels": [], "entities": []}, {"text": "Although not comparable, as the experimental settings are not the same, the BLEU scores reported in this paper are 0.2789 for Romanian-German and 0.2695 for GermanRomanian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9995537400245667}, {"text": "GermanRomanian", "start_pos": 157, "end_pos": 171, "type": "DATASET", "confidence": 0.9647436141967773}]}, {"text": "In order to extract the sources of errors, the translations of 100 paragraphs from Test 4 data-set, obtained by the Moses-based SMT system, were manually analyzed.", "labels": [], "entities": [{"text": "Test 4 data-set", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.7862563530604044}, {"text": "SMT", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9009279608726501}]}, {"text": "In order to have different paragraph-types, 50 were chosen from the beginning and 50 from the end.", "labels": [], "entities": []}, {"text": "As the human evaluator has as mother tongue Romanian, the translation direction considered was German-Romanian.", "labels": [], "entities": []}, {"text": "If some paragraphs consist of only one word, it was observed that the last 50 paragraphs are longer: e.g. paragraph 863 has 82 words and consists of one phrase and two sentences.", "labels": [], "entities": []}, {"text": "There are 49 paragraphs shorter than 6 words.", "labels": [], "entities": []}, {"text": "The eight sources of translation errors are presented in.", "labels": [], "entities": [{"text": "translation errors", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8705024421215057}]}, {"text": "Some errors (e.g. OOV-words) presented in are due to the limited training data.", "labels": [], "entities": [{"text": "OOV-words", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9067287445068359}]}, {"text": "Due to the German compounds and syntax, an important source of errors is the word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.7298788279294968}]}, {"text": "These errors can be solved by adding more data or a bilingual dictionary.", "labels": [], "entities": []}, {"text": "In around 10% of the paragraphs, the translation was adequate and fluent, but it was the reference translation rephrased -e.g. passive voice translated", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Evaluation Results for the SMT System for  the JRC-ACQUIS Test Data", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9951419830322266}, {"text": "JRC-ACQUIS Test Data", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.9329377810160319}]}, {"text": " Table 4: Evaluation Results for the Google On-line  Translation System for the JRC-ACQUIS Test Data", "labels": [], "entities": [{"text": "Google On-line  Translation", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.5341173112392426}, {"text": "JRC-ACQUIS Test Data", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.9501648743947347}]}, {"text": " Table 5: Evaluation Results for the for the Manual  Test Data -Test 5", "labels": [], "entities": [{"text": "Manual  Test Data -Test 5", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.8377740979194641}]}]}