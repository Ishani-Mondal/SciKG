{"title": [{"text": "Explorations in Automatic Image Annotation using Textual Features", "labels": [], "entities": [{"text": "Automatic Image Annotation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.7185186545054117}]}], "abstractContent": [{"text": "In this paper, we report our work on automatic image annotation by combining several textual features drawn from the text surrounding the image.", "labels": [], "entities": [{"text": "automatic image annotation", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.6290290753046671}]}, {"text": "Evaluation of our system is performed on a dataset of images and texts collected from the web.", "labels": [], "entities": []}, {"text": "We report our findings through comparative evaluation with two gold standard collections of manual annotations on the same dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite the usefulness of images in expressing ideas, machine understanding of the meaning of an image remains a daunting task for computers, as the interplay between the different visual components of an image does not conform to any fixed pattern that allows for formal reasoning of its semantics.", "labels": [], "entities": [{"text": "machine understanding of the meaning of an image", "start_pos": 54, "end_pos": 102, "type": "TASK", "confidence": 0.8598049134016037}]}, {"text": "Often, the machine interpretation of the concepts present in an image, known as automatic image annotation, can only be inferred by its accompanying text or co-occurrence information drawn from a large corpus of texts and images ().", "labels": [], "entities": []}, {"text": "Not surprisingly, humans have the innate ability to perform this task reliably, but given a large database of images, manual annotation is both labor-intensive and time-consuming.", "labels": [], "entities": []}, {"text": "Our work centers around the question : Provided an image with its associated text, can we use the text to reliably extract keywords that relevantly describe the image ? Note that we are not concerned with the generation of keywords for an image, but rather their extraction from the related text.", "labels": [], "entities": []}, {"text": "Our goal eventually is to automate this task by leveraging on texts which are naturally occurring with images.", "labels": [], "entities": []}, {"text": "In all our experiments, we only consider the use of nouns as annotation keywords.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate the performance of each of the four annotation methods individually, followed by a combined approach using all of them.", "labels": [], "entities": []}, {"text": "In the individual setting, we simply obtain the set of candidates proposed by each method as possible annotation keywords for the image.", "labels": [], "entities": []}, {"text": "In the unsupervised combined setting, only the labels proposed by all individual methods are selected, and listed in reverse order of their combined rankings.", "labels": [], "entities": []}, {"text": "We allow each system to produce a re-ranked list of top k words to be the final annotations fora given image.", "labels": [], "entities": []}, {"text": "A system can discretionary generate less (but not more) thank words that is appropriate to its confidence level.", "labels": [], "entities": []}, {"text": "Similar to, we evaluate our systems using precision, recall and F-measure for k=10, k=15 and k=20 words.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.999542236328125}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9993360638618469}, {"text": "F-measure", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9991679191589355}]}, {"text": "For comparison, we also implemented two baselines systems: tf * idf and Doc Title, which simply takes all the words in the title of the web page and uses them as annotation labels for the image.", "labels": [], "entities": []}, {"text": "In the absence of a document title, we use the first sentence in the document.", "labels": [], "entities": []}, {"text": "The results for GS intuition and GS context are tabulated in respectively.", "labels": [], "entities": []}, {"text": "We further illustrate our results with an annotation example (an image taken from a webpage discussing strep throat among teens) in.", "labels": [], "entities": []}, {"text": "Words in bold matches GS context while those underlined matches GS intuition .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for Automatic Image Annotation for GS intuition . In both Tables 2 and 3, statistically  significant results are marked with  *  (measured against Doc Title, p<0.05, paired t-test), \u00d7 (measured  against tf*idf, p<0.1, paired t-test),  \u2020 (measured against tf*idf, p<0.05, paired t-test).", "labels": [], "entities": [{"text": "Automatic Image Annotation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.5903138518333435}, {"text": "GS intuition", "start_pos": 53, "end_pos": 65, "type": "TASK", "confidence": 0.8559597432613373}, {"text": "Doc Title", "start_pos": 165, "end_pos": 174, "type": "DATASET", "confidence": 0.926545262336731}]}, {"text": " Table 3: Results for Automatic Image Annotation for GS context", "labels": [], "entities": [{"text": "Automatic Image Annotation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.5693440536657969}]}]}