{"title": [], "abstractContent": [{"text": "We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines.", "labels": [], "entities": []}, {"text": "As the structures underlying these translation engines are not known, an evaluation-based strategy is applied to select systems for combination.", "labels": [], "entities": []}, {"text": "The experiments show promising improvements in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9987383484840393}]}], "introductionContent": [{"text": "With the wealth of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7075707763433456}]}, {"text": "Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 118, "end_pos": 155, "type": "TASK", "confidence": 0.8201325933138529}, {"text": "rulebased machine translation (RBMT)", "start_pos": 160, "end_pos": 196, "type": "TASK", "confidence": 0.774137278397878}]}, {"text": "Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (, and confusion networks (.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.8382770121097565}]}, {"text": "The approach by) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems.", "labels": [], "entities": [{"text": "SMT system", "start_pos": 66, "end_pos": 76, "type": "TASK", "confidence": 0.9127960801124573}]}, {"text": "The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus.", "labels": [], "entities": []}, {"text": "This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton.", "labels": [], "entities": []}, {"text": "On the other hand, it emphasizes that the additional translations should be produced by RBMT systems with lexicons that cannot be learned from the data.", "labels": [], "entities": []}, {"text": "The present work continues on the same track as the paper mentioned above but implements a number of important changes, most prominently a relaxation of the restrictions on the number and type of input systems.", "labels": [], "entities": []}, {"text": "These differences are described in more detail in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 explains the implementation of our system and Section 4 its application in a number of experiments.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes this paper with a summary and some thoughts on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since both the system translations and the reference translations are available for the tuning set, we first compare each output to the reference translation using BLEU () and METEOR (Banerjee and) and a combined scoring scheme provided by the ULC toolkit (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9987655878067017}, {"text": "METEOR", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9962224960327148}, {"text": "ULC toolkit", "start_pos": 244, "end_pos": 255, "type": "DATASET", "confidence": 0.9261357188224792}]}, {"text": "In our experiments, we selected a subset of 5 systems for the combination, inmost cases, based on BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9967001080513}]}, {"text": "On the other hand, some systems maybe designed in away that they deliver interesting unique translation segments.", "labels": [], "entities": []}, {"text": "Therefore, we also measure the similarity among system outputs as shown in in a given collection by calculating average similarity scores across every pair of outputs.", "labels": [], "entities": []}, {"text": "The range of BLEU scores cannot indicate the similarity of the systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.998615026473999}]}, {"text": "The direction with the most systems submitted is Spanish-English but their respective performances are very close to each other.", "labels": [], "entities": []}, {"text": "As for the selected subset, the EnglishFrench systems have the most similar performance in terms of BLEU scores.", "labels": [], "entities": [{"text": "EnglishFrench", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9234142303466797}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.999187171459198}]}, {"text": "The French-English translations have the largest range in BLEU but the similarity in this group is not the lowest.: Similarity of the system outputs Ideally, we should select systems with highest quality scores and lowest similarity scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9902562499046326}, {"text": "similarity", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9793775677680969}, {"text": "similarity", "start_pos": 222, "end_pos": 232, "type": "METRIC", "confidence": 0.9587999582290649}]}, {"text": "For German-English, we selected the three with the highest METEOR scores and another two with high METEOR scores but low similarity scores to the first three.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9976223111152649}, {"text": "METEOR scores", "start_pos": 99, "end_pos": 112, "type": "METRIC", "confidence": 0.97393798828125}]}, {"text": "For the other language directions, we chose five systems from different institutions with the highest scores.", "labels": [], "entities": []}, {"text": "The purpose of this exercise is to understand the nature of the system combination task in practice.", "labels": [], "entities": []}, {"text": "Therefore, we restrict ourselves to the training data and system translations provided by the shared task.", "labels": [], "entities": []}, {"text": "The types of the systems that produced the translations are assumed to be unknown.", "labels": [], "entities": []}, {"text": "We report results for six translation directions between four languages.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of system outputs' BLEU scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9974470138549805}]}, {"text": " Table 2: Similarity of the system outputs", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores of direct system combina- tion", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990893602371216}]}, {"text": " Table 4: BLEU scores of integrated SMT systems  (Bas: Baseline, Med: Median)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998330295085907}, {"text": "SMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9576888084411621}]}]}