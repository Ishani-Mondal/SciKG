{"title": [{"text": "SVD Feature Selection for Probabilistic Taxonomy Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models.", "labels": [], "entities": []}, {"text": "We leverage on the computation of logistic regression to exploit unsu-pervised feature selection of singular value decomposition (SVD).", "labels": [], "entities": [{"text": "singular value decomposition (SVD)", "start_pos": 100, "end_pos": 134, "type": "TASK", "confidence": 0.6942342718442281}]}, {"text": "Experiments show that this way of using SVD for feature selection positively affects performances.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.6400929987430573}]}], "introductionContent": [{"text": "Taxonomies are extremely important knowledge repositories in a variety of applications for natural language processing and knowledge representation.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6741135716438293}, {"text": "knowledge representation", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.7074578702449799}]}, {"text": "Yet, manually built taxonomies such as WordNet often lack in coverage when used in specific knowledge domains.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.967474639415741}, {"text": "coverage", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9681596755981445}]}, {"text": "Automatically creating or extending taxonomies for specific domains is then a very interesting area of research).", "labels": [], "entities": []}, {"text": "Automatic methods for learning taxonomies from corpora often use distributional hypothesis and exploit some induced lexical-syntactic patterns).", "labels": [], "entities": []}, {"text": "In these models, within a very large set, candidate word pairs are selected as new word pairs in hyperonymy and added to an existing taxonomy.", "labels": [], "entities": []}, {"text": "Candidate pairs are represented in some feature space.", "labels": [], "entities": []}, {"text": "Often, these feature spaces are huge and, then, models may take into consideration noisy features.", "labels": [], "entities": []}, {"text": "In machine learning, feature selection has been often used to reduce the dimensions in huge feature spaces.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7191120386123657}]}, {"text": "This has many advantages, e.g., reducing the computational cost and improving performances by removing noisy features.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models.", "labels": [], "entities": []}, {"text": "Given the probabilistic taxonomy learning model introduced by), we leverage on the computation of logistic regression to exploit singular value decomposition (SVD) as unsupervised feature selection.", "labels": [], "entities": []}, {"text": "SVD is used to compute the pseudo-inverse matrix needed in logistic regression.", "labels": [], "entities": [{"text": "SVD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6722578406333923}]}, {"text": "To describe our idea, we firstly review how SVD can be used as unsupervised feature selection (Sec. 2).", "labels": [], "entities": []}, {"text": "In Section 3 we then describe the probabilistic taxonomy learning model introduced by).", "labels": [], "entities": []}, {"text": "We will then shortly review the logistic regression used to compute the taxonomy learning model to describe where SVD can be naturally used.", "labels": [], "entities": []}, {"text": "We will describe our experiments in Sec.", "labels": [], "entities": []}, {"text": "4. Finally, we will draw some conclusions and describe our future work (Sec. 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we want to empirically explore whether our use of SVD feature selection positively affects performances of the probabilistic taxonomy learner.", "labels": [], "entities": []}, {"text": "The best way of determining how a taxonomy learner is performing is to see if it can replicate an existing \"taxonomy\".", "labels": [], "entities": []}, {"text": "We will experiment with the attempt of replicating a portion of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9747159481048584}]}, {"text": "In the experiments, we will address two issues: 1) determining to what extent SVD feature selection affect performances of the taxonomy learner; 2) determining if SVD as unsupervised feature selection is better for the task than some simpler model for taxonomy learning.", "labels": [], "entities": []}, {"text": "We will explore the effects on both the flat and the inductive probabilistic taxonomy learner.", "labels": [], "entities": []}, {"text": "The rest of the section is organized as follows.", "labels": [], "entities": []}, {"text": "4.1 we will describe the experimental setup in terms of: how we selected the portion of WordNet, the description of the corpus used to extract evidences, a description of the feature space we used, and, finally, the description of a baseline models for taxonomy learning we have used.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.9711563587188721}]}, {"text": "4.2 we will present the results of the experiments in term of performance.", "labels": [], "entities": []}, {"text": "To completely define the experiments we need to describe some issues: how we defined the taxonomy to replicate, which corpus we have used to extract evidences for pairs of words, which feature space we used, and, finally, the baseline model we compared our feature selection model against.", "labels": [], "entities": []}, {"text": "As target taxonomy we selected a portion of WordNet 1.", "labels": [], "entities": [{"text": "WordNet 1", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.9459947645664215}]}, {"text": "Namely, we started from the 44 concrete nouns listed in) and divided in 3 classes: animal, artifact, and vegetable.", "labels": [], "entities": []}, {"text": "For sake of comprehension, this set is described in Tab.", "labels": [], "entities": []}, {"text": "2. For each word w, we selected the synset s w that is compliant with the class it belongs to.", "labels": [], "entities": []}, {"text": "We then obtained a set S of synsets (see 2).", "labels": [], "entities": []}, {"text": "We then expanded the set to S adding the siblings (i.e., the coordinate terms) for each synset in S.", "labels": [], "entities": []}, {"text": "The set S contains 265 coordinate terms plus the 44 original concrete nouns.", "labels": [], "entities": []}, {"text": "For each element in S we collected its hyperonym, obtaining the set H.", "labels": [], "entities": []}, {"text": "We then removed from the set H the 4 topmosts: entity, unit, object, and whole.", "labels": [], "entities": []}, {"text": "The set H contains 77 hyperonyms.", "labels": [], "entities": []}, {"text": "For the purpose of the experiments we both derived from the previous sets a taxonomy T and produced a set of negative examples T . The two sets have been obtained as follows.", "labels": [], "entities": []}, {"text": "The taxonomy T is the portion of WordNet implied by O = H \u222a S , i.e., T contains all the (s, h) \u2208 O \u00d7 O that are in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9373202919960022}]}, {"text": "On the contrary, T contains all the (s, h) \u2208 O \u00d7 O that are not in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9669201970100403}]}, {"text": "We then have 5108 positive pairs in T and 52892 negative pairs in T . We then split the set T \u222a T in two parts, training and testing.", "labels": [], "entities": []}, {"text": "As we want to see if it is possible to attach the set S to the right hyperonym, the split has been done as follows.", "labels": [], "entities": []}, {"text": "We randomly divided the set S in two parts S tr and S ts , respectively, of 70% and 30% of the original S . We then selected as training T tr all the pairs in T containing a synset in S tr and as testing set T ts those pairs of T containing a synset of S ts . For the probabilistic model, T tr is the initial taxonomy whereas T ts \u222a T is the unknown set.", "labels": [], "entities": []}, {"text": "As corpus we used the English Web as Corpus (ukWaC).", "labels": [], "entities": [{"text": "English Web as Corpus (ukWaC)", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.8259468674659729}]}, {"text": "This is a web extracted corpus of about 2700000 web pages containing more than 2 billion words.).", "labels": [], "entities": []}, {"text": "As the focus of the paper is the analysis of the effect of the SVD feature selection, we used as feature spaces both n-grams and bag-of-words.", "labels": [], "entities": [{"text": "SVD feature selection", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.6948161125183105}]}, {"text": "Out of the T \u222a T , we selected only those pairs that appeared at a distance of at most 3 tokens.", "labels": [], "entities": []}, {"text": "Using these 3 tokens, we generated three spaces: (1) 1-gram that contains monograms, (2) 2-gram that contains monograms and bigrams, and (3) the 3-gram space that contains monograms, bigrams, and trigrams.", "labels": [], "entities": []}, {"text": "For the purpose of this experiment, we used a reduced stop list as classical stop words as punctuation, parenthesis, the verb to be are very relevant in the context of features for learning a taxonomy.", "labels": [], "entities": []}, {"text": "Finally, we want to describe our baseline model for taxonomy learning.", "labels": [], "entities": [{"text": "taxonomy learning", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.8753361999988556}]}, {"text": "This model only contains Heart's patterns as features.", "labels": [], "entities": []}, {"text": "The feature value is the point-wise mutual information.", "labels": [], "entities": []}, {"text": "These features are in some sense the best features for the task as these have been manually selected after a process of corpus analysis.", "labels": [], "entities": []}, {"text": "These baseline features are included in our 3-gram model.", "labels": [], "entities": []}, {"text": "We can then compare our best models with this baseline features in order to see if our SVD feature selection model outperforms manual feature selection.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Concrete nouns, Classes and senses selected in WordNet", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8479353785514832}]}]}