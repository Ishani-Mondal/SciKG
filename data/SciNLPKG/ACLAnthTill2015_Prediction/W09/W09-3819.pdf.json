{"title": [{"text": "Hebrew Dependency Parsing: Initial Results", "labels": [], "entities": [{"text": "Hebrew Dependency Parsing", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5474490225315094}]}], "abstractContent": [{"text": "We describe a newly available Hebrew Dependency Treebank, which is extracted from the Hebrew (constituency) Tree-bank.", "labels": [], "entities": [{"text": "Hebrew Dependency Treebank", "start_pos": 30, "end_pos": 56, "type": "DATASET", "confidence": 0.8491785923639933}, {"text": "Hebrew (constituency) Tree-bank", "start_pos": 86, "end_pos": 117, "type": "DATASET", "confidence": 0.6024939358234406}]}, {"text": "We establish some baseline un-labeled dependency parsing performance on Hebrew, based on two state-of-the-art parsers, MST-parser and MaltParser.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.6524966359138489}]}, {"text": "The evaluation is performed both in an artificial setting, in which the data is assumed to be properly morphologically segmented and POS-tagged, and in a real-world setting , in which the parsing is performed on automatically segmented and POS-tagged text.", "labels": [], "entities": []}, {"text": "We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data.", "labels": [], "entities": []}, {"text": "Results indicate that (a) MST-parser performs better on Hebrew data than Malt-Parser, and (b) both parsers do not make good use of morphological information when parsing Hebrew.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hebrew is a Semitic language with rich morphological structure and free constituent order.", "labels": [], "entities": []}, {"text": "Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution, Hebrew NP-chunking (), and Hebrew constituency parsing).", "labels": [], "entities": [{"text": "Hebrew POS tagging", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.5330575207869211}, {"text": "unknown word resolution", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.6569344103336334}, {"text": "Hebrew constituency parsing", "start_pos": 126, "end_pos": 153, "type": "TASK", "confidence": 0.6133227447668711}]}, {"text": "Here, we focus on Hebrew dependency parsing.", "labels": [], "entities": [{"text": "Hebrew dependency parsing", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6481526295344034}]}, {"text": "Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (.", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.6199785073598226}]}, {"text": "These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew.", "labels": [], "entities": []}, {"text": "However, parsing accuracies for Arabic usually lag behind non-semitic languages.", "labels": [], "entities": []}, {"text": "Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific properties of Arabic making it easy or hard to parse in comparison to other languages.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 72, "end_pos": 86, "type": "METRIC", "confidence": 0.9304929375648499}, {"text": "Arabic dependency parsing", "start_pos": 124, "end_pos": 149, "type": "TASK", "confidence": 0.6429383854071299}]}, {"text": "Our aim is to evaluate current state-of-the-art dependency parsers and approaches on Hebrew dependency parsing, to understand some of the difficulties in parsing a Semitic language, and to establish a strong baseline for future work.", "labels": [], "entities": [{"text": "dependency parsers", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.700678676366806}, {"text": "Hebrew dependency parsing", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6342528065045675}, {"text": "parsing a Semitic language", "start_pos": 154, "end_pos": 180, "type": "TASK", "confidence": 0.8718201220035553}]}, {"text": "We present the first published results on Dependency Parsing of Hebrew.", "labels": [], "entities": [{"text": "Dependency Parsing of Hebrew", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.9250720888376236}]}, {"text": "Some aspects that make Hebrew challenging from a parsing perspective are: Affixation Common prepositions, conjunctions and articles are prefixed to the following word, and pronominal elements often appear as suffixes.", "labels": [], "entities": []}, {"text": "The segmentation of prefixes and suffixes is often ambiguous and must be determined in a specific context only.", "labels": [], "entities": [{"text": "segmentation of prefixes and suffixes", "start_pos": 4, "end_pos": 41, "type": "TASK", "confidence": 0.8504528880119324}]}, {"text": "In term of dependency parsing, this means that the dependency relations occur not between space-delimited tokens, but instead between sub-token elements which we'll refer to as segments.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7769175469875336}]}, {"text": "Furthermore, any mistakes in the underlying token segmentations are sure to be reflected in the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9584800601005554}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.8677123188972473}]}, {"text": "Relatively free constituent order The ordering of constituents inside a phrase is relatively free.", "labels": [], "entities": []}, {"text": "This is most notably apparent in the verbal phrases and sentential levels.", "labels": [], "entities": []}, {"text": "In particular, while most sentences follow an SVO order, OVS and VSO configurations are also possible.", "labels": [], "entities": []}, {"text": "Verbal arguments can appear before or after the verb, and in many ordering.", "labels": [], "entities": []}, {"text": "For example, the message \"went from Israel to Thailand\" can be expressed as \"went to Thailand from Israel\", \"to Thailand went from Israel\", \"from Israel went to Thailand\", \"from Israel to Thailand went\" and \"to Thailand from Israel went\".", "labels": [], "entities": []}, {"text": "This results in long and flat VP and S structures and a fair amount of sparsity, which suggests that a dependency representations might be more suitable to Hebrew than a constituency one.", "labels": [], "entities": []}, {"text": "Rich templatic morphology Hebrew has a very productive morphological structure, which is based on a root+template system.", "labels": [], "entities": []}, {"text": "The productive morphology results in many distinct word forms and a high out-of-vocabulary rate, which makes it hard to reliably estimate lexical parameters from annotated corpora.", "labels": [], "entities": []}, {"text": "The root+template system (combined with the unvocalized writing system) makes it hard to guess the morphological analyses of an unknown word based on its prefix and suffix, as usually done in other languages.", "labels": [], "entities": []}, {"text": "Unvocalized writing system Most vowels are not marked in everyday Hebrew text, which results in a very high level of lexical and morphological ambiguity.", "labels": [], "entities": []}, {"text": "Some tokens can admit as many as 15 distinct readings, and the average number of possible morphological analyses per token in Hebrew text is 2.7, compared to 1.4 in English.", "labels": [], "entities": []}, {"text": "This means that on average, every token is ambiguous with respect to its POS and morphological features.", "labels": [], "entities": []}, {"text": "Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree in Gender and Number and definiteness), and between Subjects and Verbs (which should agree in Gender and Number).", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We follow the train-test-dev split established in.", "labels": [], "entities": []}, {"text": "Specifically, we use Sections 2-12 (sentences 484-5724) of the Hebrew Dependency Treebank as our training set, and report results on parsing the development set, Section 1 (sentences 0-483).", "labels": [], "entities": [{"text": "Hebrew Dependency Treebank", "start_pos": 63, "end_pos": 89, "type": "DATASET", "confidence": 0.879208505153656}]}, {"text": "We do not evaluate on the test set in this work.", "labels": [], "entities": []}, {"text": "The data in the Treebank is segmented and POS-tagged.", "labels": [], "entities": []}, {"text": "All of the models were trained on the gold-standard segmented and tagged data.", "labels": [], "entities": []}, {"text": "When evaluating the parsing models, we perform two sets of evaluations.", "labels": [], "entities": []}, {"text": "The first one is an oracle experiment, assuming gold segmentation and tagging is available.", "labels": [], "entities": [{"text": "gold segmentation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.6370687484741211}]}, {"text": "The second one is a real-world experiment, in which we segment and POS-tag the test-set sentences using the morphological disambiguator described in prior to parsing.", "labels": [], "entities": [{"text": "POS-tag", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9643139839172363}]}, {"text": "Parsers and parsing models We use the freely available implementation of MaltParser 2 and MSTParser 3 , with default settings for each of the parsers.", "labels": [], "entities": []}, {"text": "For MaltParser, we experiment both with the default feature representation (MALT) and the feature representation used for parsing Arabic in CoNLL 2006 and 2007 multilingual dependency parsing shared tasks (MALT-ARA).", "labels": [], "entities": [{"text": "parsing Arabic in CoNLL 2006 and 2007 multilingual dependency parsing shared tasks", "start_pos": 122, "end_pos": 204, "type": "TASK", "confidence": 0.7453213334083557}]}, {"text": "For MST parser, we experimented with firstorder (MST1) and second-order (MST2) models.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8390613198280334}]}, {"text": "We varied the amount of lexical information available to the parser.", "labels": [], "entities": []}, {"text": "Each of the parsers was trained on 3 datasets: LEXFULL, in which all the lexical items are available, LEX20, in which lexical items appearing less than 20 times in the training data were replaced by an OOV token, and LEX100 in which we kept only lexical items appearing more than 100 times in training.", "labels": [], "entities": [{"text": "LEXFULL", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.9483478665351868}, {"text": "LEX100", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.8405600190162659}]}, {"text": "We also wanted to control the effect of the rich morphological information available in Hebrew (gender and number marking, person, and so on).", "labels": [], "entities": []}, {"text": "To this end, we trained and tested each model either with all the available morphological information (+MORPH) or without any morphological information (-MORPH).", "labels": [], "entities": [{"text": "MORPH", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9864895343780518}, {"text": "MORPH", "start_pos": 154, "end_pos": 159, "type": "METRIC", "confidence": 0.980999231338501}]}, {"text": "Evaluation Measure We evaluate the resulting parses in terms of unlabeled accuracy -the percent of correctly identified (child,parent) pairs . To be precise, we calculate: number of correctly identified pairs number of pairs in gold parse For the oracle casein which the gold-standard token segmentation is available for the parser, this is the same as the traditional unlabeled-accuracy evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.885607123374939}]}, {"text": "However, in the real-word setting in which the token segmentation is done automatically, the yields of the gold-standard and the: Unlabeled dependency accuracy with automatic token segmentation and POS-tagging.", "labels": [], "entities": [{"text": "token segmentation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.706009641289711}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9191944599151611}, {"text": "token segmentation", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.7137525081634521}]}, {"text": "automatic parse may differ, and one needs to decide how to handle the cases in which one or more elements in the identified (child,parent) pair are not present in the gold-standard parse.", "labels": [], "entities": []}, {"text": "Our evaluation metric penalizes these cases by regarding any such case as a mistake.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Unlabeled dependency accuracy with  oracle token segmentation and POS-tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9803426861763}, {"text": "oracle token segmentation", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.681554913520813}]}, {"text": " Table 2: Unlabeled dependency accuracy with  automatic token segmentation and POS-tagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9829105734825134}, {"text": "token segmentation", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7915500402450562}]}]}