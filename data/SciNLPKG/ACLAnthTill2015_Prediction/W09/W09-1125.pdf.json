{"title": [{"text": "Fine-Grained Classification of Named Entities Exploiting Latent Semantic Kernels", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a kernel-based approach for fine-grained classification of named entities.", "labels": [], "entities": []}, {"text": "The only training data for our algorithm is a few manually annotated entities for each class.", "labels": [], "entities": []}, {"text": "We defined kernel functions that implicitly map entities, represented by aggregating all contexts in which they occur, into a latent semantic space derived from Wikipedia.", "labels": [], "entities": []}, {"text": "Our method achieves a significant improvement over the state of the art for the task of populating an ontology of people, although requiring considerably less training instances than previous approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Populating an ontology with relevant entities extracted from unstructured textual documents is a crucial step in Semantic Web and knowledge management systems.", "labels": [], "entities": []}, {"text": "As the concepts in an ontology are generally arranged in deep class/subclass hierarchies, the problem of populating ontologies is typically solved top-down, firstly identifying and classifying entities in the most general concepts, and then refining the classification process.", "labels": [], "entities": []}, {"text": "Recent advances have made supervised approaches very successful in entity identification and classification.", "labels": [], "entities": [{"text": "entity identification and classification", "start_pos": 67, "end_pos": 107, "type": "TASK", "confidence": 0.7818250209093094}]}, {"text": "However, to achieve satisfactory performance, supervised systems must be supplied with a sufficiently large amount of training data, usually consisting of hand tagged texts.", "labels": [], "entities": []}, {"text": "As domain specific ontologies generally contains hundreds of subcategories, such approaches are not directly applicable fora more fine-grained categorization because the number of documents required to find sufficient positive examples for all subclasses becomes too large, making the manual annotation very expensive.", "labels": [], "entities": []}, {"text": "Consequently, in the literature, supervised approaches are confined to classify entities into broad categories, such as persons, locations, and organizations, while the fine-grained classification has been approached with minimally supervised (e.g., and) and unsupervised learning algorithms (e.g., and).", "labels": [], "entities": []}, {"text": "Following this trend, we present a minimally supervised approach to fine-grained categorization of named entities previously recognized into coarsegrained categories, e.g., by a named-entity recognizer.", "labels": [], "entities": []}, {"text": "The only training data for our algorithm is a few manually annotated entities for each class.", "labels": [], "entities": []}, {"text": "For example, Niels Bohr, Albert Einstein, and Enrico Fermi might be used as examples for the class physicists.", "labels": [], "entities": []}, {"text": "In some cases, training entities can be acquired (semi-) automatically from existing ontologies allowing us to automatically derive training entities for use with our machine learning algorithm.", "labels": [], "entities": []}, {"text": "For instance, we may easily obtain tens of training entities for very specific classes, such as astronomers, materials scientists, nuclear physicists, by querying the Yago ontology (.", "labels": [], "entities": []}, {"text": "We represent the entities using features extracted from the textual contexts in which they occur.", "labels": [], "entities": []}, {"text": "Specifically, we use a search engine to collect such contexts from the Web.", "labels": [], "entities": []}, {"text": "Throughout this paper, we will refer to such a representation as multi-context representation, in contrast to the single-context rep-resentation in which an entity is categorized using solely features extracted from the local context surrounding it, usually a window of a few words around the entity occurrence.", "labels": [], "entities": []}, {"text": "Single-context features are commonly used in named-entity recognition, however to assign very specific categories the local context might not provide sufficient information.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7377946972846985}]}, {"text": "For example, in the sentence \"Prof. Enrico Fermi discovered away to induce artificial radiation in heavy elements by shooting neutrons into their atomic nuclei,\" single-context features such as, the prefix Prof. and the capital letters, provides enough evidence that Enrico Fermi is a person and a professor.", "labels": [], "entities": []}, {"text": "However, to discover that he is a physicist we need to analyze a wider context, or alternatively multiple ones.", "labels": [], "entities": []}, {"text": "Recently, has shown that exploiting multi-context information can greatly improve the fine-grained classification of named entities, when compared to methods using single context only.", "labels": [], "entities": []}, {"text": "In order to effectively represent entities' multicontexts, we extend the traditional vector space model (VSM), offering away to integrate external semantic information in the classification process by means of latent semantic kernels.", "labels": [], "entities": []}, {"text": "As a result, we obtain a generalized similarity function between multi-contexts that incorporates semantic relations between terms, automatically learned from unlabeled data.", "labels": [], "entities": []}, {"text": "In particular, we use Wikipedia to build the latent semantic space.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.9343241453170776}]}, {"text": "The underlying idea is that similar named entities tend to have a similar description in Wikipedia.", "labels": [], "entities": []}, {"text": "As Wikipedia provides reliable information and it exceeds all other encyclopedias in coverage, it should be a valuable resource for the task of populating an ontology.", "labels": [], "entities": []}, {"text": "To validate this hypothesis, we compare this model with one built from a news corpus.", "labels": [], "entities": []}, {"text": "Our approach achieves a significant improvement over the state of the art for the task of populating the People Ontology (, although requiring considerably less training instances than previous approaches.", "labels": [], "entities": []}, {"text": "The task consists in classifying person names into a multi-level taxonomy composed of 21 categories derived from WordNet, making very fine-grained distinctions (e.g., physicists vs. mathematicians).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9380412101745605}]}, {"text": "It provides a more realistic and challenging benchmark than the ones previously available (e.g., and), that consider a smaller number of categories arranged in a one-level taxonomy.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare performance of different kernel setups and previous approaches on an ontology population task.", "labels": [], "entities": []}, {"text": "We built two proximity matrices \u03a0 W and \u03a0 NY T . The former is derived from the 200,000 most visited Wikipedia articles, while the latter from 200,000 articles published by the New York Times between June 1, 1998 and January.", "labels": [], "entities": []}, {"text": "After removing terms that occur less than 5 times, the resulting dictionaries contain about 300,000 and 150,000 terms respectively.", "labels": [], "entities": []}, {"text": "We used the SVDLIBC package 2 to compute the SVD, truncated to 400 dimensions.", "labels": [], "entities": []}, {"text": "To derive the multi-context representation, we collected 100 english snippets for each person instance by querying Google TM . To classify each person instance into one of the fine-grained categories, we used a KNN classifier (K = 1).", "labels": [], "entities": []}, {"text": "No parameter optimization was performed.", "labels": [], "entities": []}, {"text": "shows micro-and macro-averaged results for, the random baseline, and most frequent baseline.", "labels": [], "entities": []}, {"text": "Where K W and K NY T are instances of the latent semantic kernel, K LS , using the proximity matrices \u03a0 W and \u03a0 NY T , derived from Wikipedia and the New York Times corpus, respectively.", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 150, "end_pos": 171, "type": "DATASET", "confidence": 0.6483367085456848}]}, {"text": "Table 2 shows detailed results for each sub-and supercategory for K BOW + K W . shows the confusion matrix of K BOW + K W , in which the rows are ground truth classes and the columns are predictions.", "labels": [], "entities": [{"text": "BOW", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8004575967788696}]}, {"text": "The matrix has been calculated for the finergrained categories and, then, grouped according to their super-class.", "labels": [], "entities": []}, {"text": "To be compared with the IBOP method, all experiments were conducted using only 20 training examples per category.", "labels": [], "entities": []}, {"text": "Finally, shows the learning curves for K BOW + K W obtained varying the number of snippets (12, 25, 50, and 100) used to derive the multi-contexts.", "labels": [], "entities": [{"text": "BOW", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9387269020080566}]}], "tableCaptions": [{"text": " Table 1: Comparison among the kernel-based ap- proaches, the IBOP method (", "labels": [], "entities": []}, {"text": " Table 3: Confusion matrix of K BOW + K W for the more fine-grained categories grouped according to their top-level  concepts of the People Ontology.", "labels": [], "entities": [{"text": "BOW", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.8157221078872681}]}, {"text": " Table 2: Results for each category using K BOW + K W .", "labels": [], "entities": [{"text": "BOW", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.8003358244895935}]}]}