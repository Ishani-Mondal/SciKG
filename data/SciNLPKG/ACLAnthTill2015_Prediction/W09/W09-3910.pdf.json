{"title": [{"text": "Detecting the Noteworthiness of Utterances in Human Meetings", "labels": [], "entities": [{"text": "Detecting the Noteworthiness of Utterances in Human Meetings", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7500546723604202}]}], "abstractContent": [{"text": "Our goal is to make note-taking easier in meetings by automatically detecting noteworthy utterances in verbal exchanges and suggesting them to meeting participants for inclusion in their notes.", "labels": [], "entities": []}, {"text": "To show feasibility of such a process we conducted a Wizard of Oz study where the Wizard picked automatically transcribed utterances that he judged as noteworthy, and suggested their contents to the participants as notes.", "labels": [], "entities": []}, {"text": "Over 9 meetings , participants accepted 35% of these suggestions.", "labels": [], "entities": []}, {"text": "Further, 41.5% of their notes at the end of the meeting contained Wizard suggested text.", "labels": [], "entities": []}, {"text": "Next, in order to perform noteworthiness detection automatically , we annotated a set of 6 meetings with a 3-level noteworthiness annotation scheme, which is a break from the binary \"in summary\"/ \"not in summary\" labeling typically used in speech summariza-tion.", "labels": [], "entities": [{"text": "noteworthiness detection", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8055424988269806}]}, {"text": "We report Kappa of 0.44 for the 3-way classification, and 0.58 when two of the 3 labels are merged into one.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9940832257270813}]}, {"text": "Finally, we trained an SVM classifier on this annotated data; this classifier's performance lies between that of trivial baselines and inter-annotator agreement.", "labels": [], "entities": []}], "introductionContent": [{"text": "We regularly exchange information verbally with others over the course of meetings.", "labels": [], "entities": []}, {"text": "Often we need to access this information afterwards.", "labels": [], "entities": []}, {"text": "Typically we record the information we consider important by taking notes.", "labels": [], "entities": []}, {"text": "Note taking at meetings is a difficult task, however, because the participant must summarize and write down the information in away such that it is comprehensible afterwards, while paying attention to and participating in the ongoing discussion.", "labels": [], "entities": [{"text": "Note taking at meetings", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.834326907992363}]}, {"text": "Our goal is to make note-taking easier by automatically extracting noteworthy items from spoken interactions in real time, and proposing them to the humans for inclusion in their notes.", "labels": [], "entities": []}, {"text": "Judging which pieces of information in a meeting are noteworthy is a very subjective task.", "labels": [], "entities": [{"text": "Judging which pieces of information in a meeting", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8833591118454933}]}, {"text": "The subjectivity of this task is likely to be more acute than even that of meeting summarization, where low inter-annotator agreement is typical e.g. (), ,, etc -whether apiece of information should be included in a participant's notes depends not only on its importance, but also on factors such as the participant's need to remember, his perceived likelihood of forgetting, etc.", "labels": [], "entities": []}, {"text": "To investigate whether it is feasible even fora human to predict what someone else might find noteworthy in a meeting, we conducted a Wizard of Oz-based user study where a human suggested notes (with restriction) to meeting participants during the meeting.", "labels": [], "entities": []}, {"text": "We concluded from this study (presented in section 2) that this task appears to be feasible for humans.", "labels": [], "entities": []}, {"text": "Assuming feasibility, we then annotated 6 meetings with a 3-level noteworthiness scheme.", "labels": [], "entities": []}, {"text": "Having 3 levels instead of the typical 2 allows us to explicitly separate utterances of middling noteworthiness from those that are definitely noteworthy or not noteworthy, and allows us to encode more human knowledge than a 2-level scheme.", "labels": [], "entities": []}, {"text": "We describe this annotation scheme in more detail in section 3, and show high interannotator agreement compared to that typically reported in the summarization literature.", "labels": [], "entities": [{"text": "agreement", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.6649084687232971}]}, {"text": "Finally in sections 4 and 5 we use this annotated data to train and test a simple Support Vector Machinebased predictor of utterance noteworthiness.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper we use a Support Vector Machinesbased classifier, which is a popular choice for extractive meeting summarization, e.g. (Xie,; we use a linear kernel in this paper.", "labels": [], "entities": [{"text": "extractive meeting summarization", "start_pos": 94, "end_pos": 126, "type": "TASK", "confidence": 0.6417376001675924}]}, {"text": "In the results reported here we use the output of the Sphinx speech recognizer, using speakerindependent acoustic models, and language models trained on publicly available meeting data.", "labels": [], "entities": [{"text": "Sphinx speech recognizer", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.5894593596458435}]}, {"text": "The word error rate was around 44% -more details of the speech recognition process are in.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.6633912225564321}, {"text": "speech recognition process", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7874270975589752}]}, {"text": "For training purposes, we merged the annotations from the two annotators by choosing a \"middle or lower ground\" for all disagreements.", "labels": [], "entities": []}, {"text": "Thus, if for an utterance the two labels are \"definitely show\" and \"don't show\", we set the merged label as the middle ground of \"maybe show\".", "labels": [], "entities": []}, {"text": "On the other hand if the two labels were on adjacent levels, we chose the lower one -\"maybe show\" when the labels were \"definitely show\" and \"maybe show\", and \"don't show\" when the labels were \"maybe show\" and \"don't show\".", "labels": [], "entities": []}, {"text": "Thus only utterances that both annotators labeled as \"definitely show\" were also labeled as \"definitely show\" in the merged annotation.", "labels": [], "entities": []}, {"text": "We plan to try other merging strategies in the future.", "labels": [], "entities": []}, {"text": "For testing, we evaluated against each annotator's labels separately, and averaged the results.", "labels": [], "entities": []}, {"text": "presents the accuracy, precision, recall and f-measure results of the 3-way classification task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996114373207092}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9990853071212769}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9994731545448303}, {"text": "f-measure", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9781514406204224}, {"text": "3-way classification task", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.658574124177297}]}, {"text": "(We use the Weka implementation of SVM that internally devolves the 3-way classification task into a sequence of pair-wise classifications. We use the final per-utterance classification here.)", "labels": [], "entities": []}, {"text": "Observe that the overall accuracy of 61.4% is only 11% lower relative to the accuracy obtained by comparing the two annotators' annotations (69%,).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995844960212708}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9991205334663391}]}, {"text": "However, the precision, recall and f-measure values for the \"definitely show\" class are substantially lower for the predicted labels than the agreement between the two annotators.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996154308319092}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9974597096443176}, {"text": "f-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.924913763999939}]}, {"text": "The numbers are closer for the \"maybe show\" and the \"don't show\" classes.", "labels": [], "entities": []}, {"text": "This implies that it is more difficult to accurately detect utterances labeled \"definitely show\" than it is to detect the other classes.", "labels": [], "entities": []}, {"text": "One reason for this difference is the size of each utterance class.", "labels": [], "entities": []}, {"text": "Utterances labeled \"definitely show\" are only around 14% of all utterances, thus there is less data for this class than the others.", "labels": [], "entities": []}, {"text": "We also ran the algorithm using manually transcribed data, and found improvement in only the \"Definitely show\" class with an f-measure of 0.21.", "labels": [], "entities": []}, {"text": "This improvement is perhaps because the speech recognizer is particularly prone to getting names and other technical terms wrong, which maybe important clues of noteworthiness.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6842698156833649}]}, {"text": "presents the ROUGE-1 F-measure scores averaged over the 6 meetings.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9959887862205505}, {"text": "F-measure", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.8403350710868835}]}, {"text": "(ROUGE is described briefly in section 4.3 and in detail in).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9752505421638489}]}, {"text": "Similar to the inter-annotator agreement computations, we computed ROUGE between the text of the utterances labeled \"definitely show\" by the system against that of utterances labeled \"definitely show\" by the two annotators.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9985674619674683}]}, {"text": "(We computed the scores separately against each of the annotators in turn and then averaged the two values.)", "labels": [], "entities": []}, {"text": "We did the same thing for the set of utterances labeled either \"definitely show\" or \"maybe show\".", "labels": [], "entities": []}, {"text": "Observe that the R1-F score for the \"definitely show\" comparison is nearly 50% relative higher than the trivial baseline of labeling every utterance as \"definitely show\".", "labels": [], "entities": [{"text": "R1-F score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9824331402778625}]}, {"text": "However the score is 30% lower than the corresponding inter-annotator agreement.", "labels": [], "entities": []}, {"text": "The corresponding R1-Fmeasure score using manual transcriptions is only marginally better -0.47.", "labels": [], "entities": [{"text": "R1-Fmeasure score", "start_pos": 18, "end_pos": 35, "type": "METRIC", "confidence": 0.978051632642746}]}, {"text": "The set of utterances labeled either definitely or maybe shows (second row of table 4) does not outperform the all-utterances baseline when using automatic transcriptions, but does so with manual transcriptions, whose R1-F value is 0.74.", "labels": [], "entities": [{"text": "R1-F", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9730629324913025}]}], "tableCaptions": [{"text": " Table 1: Distribution of Labels for Each Annotator", "labels": [], "entities": [{"text": "Distribution of Labels", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8530779878298441}]}, {"text": " Table 2 Inter-Annotator Agreement using Accuracy Etc.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9613940119743347}]}, {"text": " Table 3 Results of the 3-Way Classification", "labels": [], "entities": []}]}