{"title": [], "abstractContent": [{"text": "This paper describes the CMU entry for the system combination shared task at WMT'09.", "labels": [], "entities": [{"text": "WMT'09", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.8453148007392883}]}, {"text": "Our combination method is hypothesis selection, which uses information from n-best lists from several MT systems.", "labels": [], "entities": [{"text": "hypothesis selection", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8688545823097229}, {"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9424469470977783}]}, {"text": "The sentence level features are independent from the MT systems involved.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9520957469940186}]}, {"text": "To compensate for various n-best list sizes in the workshop shared task including first-best-only entries, we normalize one of our high-impact features for varying sub-list size.", "labels": [], "entities": []}, {"text": "We combined restricted data track entries in French-English, German-En-glish and Hungarian-English using provided data only.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the combination of machine translation systems there have been two main approaches described in recent publications.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.741036593914032}]}, {"text": "One uses confusion network decoding to combine translation systems as described in and.", "labels": [], "entities": []}, {"text": "The other approach selects whole hypotheses from a combined n-best list.", "labels": [], "entities": []}, {"text": "Our setup follows the approach described in.", "labels": [], "entities": []}, {"text": "We combine the output from the available translation systems into one joint n-best list, then calculate a set of features consistently for all hypotheses.", "labels": [], "entities": []}, {"text": "We use MER training on a development set to determine feature weights and re-rank the joint n-best list.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the development of the modification on the nbest list n-gram agreement feature we used n-best lists from three large scale Arabic to English translation systems.", "labels": [], "entities": []}, {"text": "We evaluate using the case insensitive BLEU score for the MT08 test set with four references, which was unseen data for the individual systems as well as the system combination.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9845641851425171}, {"text": "MT08 test set", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.9377489884694418}]}, {"text": "shows the initial scores of the three input systems.", "labels": [], "entities": []}, {"text": "To compare the behavior of the combination result for different n-best list sizes we combined the 100-best lists from systems A and C and then added three n-best list sizes from the middle system B into the combination: 1-best, 10-best and full 100-best.", "labels": [], "entities": []}, {"text": "For each of these four combination options we ran the hypothesis selection using the plain version of the n-gram agreement feature a as well as the normalized version without a norm and with smoothing a smooth .  The modified feature has as expected no impact on the combination of n-best lists of the same size (see), however it shows an improvement of BLEU +0.5 for the combination with the 1st-best from system B.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 354, "end_pos": 358, "type": "METRIC", "confidence": 0.9992384910583496}]}, {"text": "The smoothing seems to have no significant impact for this dataset, but different smoothing factors will be investigated in the future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Arabic-English Baselines: BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9969080090522766}]}, {"text": " Table 2: Combination results: BLEU on MT08", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9986708164215088}, {"text": "MT08", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.6908425688743591}]}, {"text": " Table 3: French-English Results: BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9975044131278992}]}, {"text": " Table 4: German-English Results: BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9967072606086731}]}, {"text": " Table 5: Hungarian-English Results: BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9974420070648193}]}]}