{"title": [{"text": "Active Dual Supervision: Reducing the Cost of Annotating Examples and Features", "labels": [], "entities": []}], "abstractContent": [{"text": "When faced with the task of building machine learning or NLP models, it is often worthwhile to turn to active learning to obtain human annotations at minimal costs.", "labels": [], "entities": []}, {"text": "Traditional active learning schemes query a human for labels of intelligently chosen examples.", "labels": [], "entities": []}, {"text": "However, human effort can also be expended in collecting alternative forms of annotations.", "labels": [], "entities": []}, {"text": "For example, one may attempt to learn a text classifier by labeling class-indicating words, instead of, or in addition to, documents.", "labels": [], "entities": []}, {"text": "Learning from two different kinds of supervision brings anew, unexplored dimension to the problem of active learning.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate the value of such active dual supervision in the context of sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.9627009332180023}]}, {"text": "We show how interleaving queries for both documents and words significantly reduces human effort-more than what is possible through traditional one-dimensional active learning, or by passive combinations of supervisory inputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "As a canonical running example for the theme of this paper, consider the problem of sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9644801616668701}]}, {"text": "Given apiece of text as input, the desired output is a polarity score that indicates whether this text expresses a positive or negative opinion towards a topic of interest.", "labels": [], "entities": []}, {"text": "From a machine learning viewpoint, this problem maybe posed as atypical binary text classification task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7148527055978775}]}, {"text": "Sentiment, however, is often conveyed with subtle linguistic mechanisms such as sarcasm, negation and the use of highly domain-specific and contextual cues.", "labels": [], "entities": []}, {"text": "This brings a multi-disciplinary flavor to the problem, drawing interest from both Natural Language Processing and Machine Learning communities.", "labels": [], "entities": []}, {"text": "Many methodologies proposed in these disciplines share a common limitation that their performance is bounded by the amount and quality of labeled data.", "labels": [], "entities": []}, {"text": "However, they differ conceptually in the type of human effort they require.", "labels": [], "entities": []}, {"text": "On one hand, supervised machine learning techniques require human effort in acquiring labeled examples, which requires reading documents and annotating them with their aggregate sentiment.", "labels": [], "entities": []}, {"text": "On the other hand, dictionary-based NLP systems require human effort in collecting labeled features: for example, in the domain of movie reviews, words that evoke positive sentiment (e.g., \"mesmerizing\", \"thrilling\" etc) maybe labeled positive, while words that evoke negative sentiment (e.g., \"boring\",\"disappointing\") maybe labeled negative.", "labels": [], "entities": []}, {"text": "This kind of annotation requires a human to condense prior linguistic experience with a word into a sentiment label that reflects the net emotion that the word evokes.", "labels": [], "entities": []}, {"text": "We refer to the general setting of learning from both labels on examples and features as dual supervision.", "labels": [], "entities": []}, {"text": "This setting arises more broadly in tasks wherein addition to labeled documents, it is frequently possible to provide domain knowledge in the form of words, or phrases ( or even more sophisticated linguistic features, that associate strongly with a class.", "labels": [], "entities": []}, {"text": "Recent work has demonstrated that the presence of word supervision can greatly reduce the number of labeled documents required to build high quality text classifiers.", "labels": [], "entities": []}, {"text": "In general, these two sources of supervision are not mutually redundant, and have different annotation costs, human response quality, and degrees of utility towards learning a dual supervision model.", "labels": [], "entities": []}, {"text": "This leads naturally to the problem of active dual supervision, or, how to optimally query a human oracle to simultaneously collect document and feature annotations, with the objective of building the highest quality model with the lowest cost.", "labels": [], "entities": []}, {"text": "Much of the machine learning literature on active learning has focused on one-sided example-only annotation for classification problems.", "labels": [], "entities": []}, {"text": "Less attention has been devoted to simultaneously acquiring alternative forms of supervisory domain knowledge, such as the kind routinely encountered in NLP.", "labels": [], "entities": []}, {"text": "Our contribution maybe viewed as a step in this direction.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}