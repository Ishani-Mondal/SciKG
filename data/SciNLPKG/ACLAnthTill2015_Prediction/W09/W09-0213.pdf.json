{"title": [{"text": "Handling Sparsity for Verb Noun MWE Token Classification", "labels": [], "entities": [{"text": "Verb Noun MWE Token Classification", "start_pos": 22, "end_pos": 56, "type": "DATASET", "confidence": 0.6102298974990845}]}], "abstractContent": [{"text": "We address the problem of classifying multiword expression tokens in running text.", "labels": [], "entities": []}, {"text": "We focus our study on Verb-Noun Constructions (VNC) that vary in their id-iomaticity depending on context.", "labels": [], "entities": [{"text": "Verb-Noun Constructions (VNC)", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6954556047916413}]}, {"text": "VNC tokens are classified as either idiomatic or literal.", "labels": [], "entities": []}, {"text": "Our approach hinges upon the assumption that a literal VNC will have more in common with its component words than an idiomatic one.", "labels": [], "entities": []}, {"text": "Commonal-ity is measured by contextual overlap.", "labels": [], "entities": []}, {"text": "To this end, we set out to explore different contextual variations and different similarity measures handling the sparsity in the possible contexts via four different parameter variations.", "labels": [], "entities": []}, {"text": "Our approach yields state of the art performance with an overall accuracy of 75.54% on a TEST data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.999279797077179}, {"text": "TEST data set", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.8796759843826294}]}], "introductionContent": [{"text": "A Multi-Word Expression (MWE), for our purposes, can be defined as a multi-word unit that refers to a single concept, for example -kick the bucket, spill the beans, make a decision, etc.", "labels": [], "entities": [{"text": "Multi-Word Expression (MWE)", "start_pos": 2, "end_pos": 29, "type": "TASK", "confidence": 0.7312372088432312}]}, {"text": "An MWE typically has an idiosyncratic meaning that is more or different than the meaning of its component words.", "labels": [], "entities": []}, {"text": "An MWE meaning is transparent, i.e. predictable, in as much as the component words in the expression relay the meaning portended by the speaker compositionally.", "labels": [], "entities": [{"text": "MWE meaning", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.8693969547748566}]}, {"text": "Accordingly, MWEs vary in their degree of meaning compositionality; compositionality is correlated with the level of idiomaticity.", "labels": [], "entities": []}, {"text": "An MWE is compositional if the meaning of an MWE as a unit can be predicted from the meaning of its component words such as in make a decision meaning to decide.", "labels": [], "entities": []}, {"text": "If we conceive of idiomaticity as being a continuum, the more idiomatic an expression, the less transparent and the more non-compositional it is.", "labels": [], "entities": []}, {"text": "MWEs are pervasive in natural language, especially in web based texts and speech genres.", "labels": [], "entities": []}, {"text": "Identifying MWEs and understanding their meaning is essential to language understanding, hence they are of crucial importance for any Natural Language Processing (NLP) applications that aim at handling robust language meaning and use.", "labels": [], "entities": [{"text": "Identifying MWEs", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7363211810588837}, {"text": "language understanding", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.739631712436676}]}, {"text": "To date, most research has addressed the problem of MWE type classification for VNC expressions in English; Van de Cruys and Villada, not token classification.", "labels": [], "entities": [{"text": "MWE type classification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.9210277001063029}, {"text": "token classification", "start_pos": 138, "end_pos": 158, "type": "TASK", "confidence": 0.7237967997789383}]}, {"text": "For example: he spilt the beans over the kitchen counter is most likely a literal usage.", "labels": [], "entities": []}, {"text": "This is given away by the use of the prepositional phrase over the kitchen counter, since it is plausable that beans could have literally been spilt on a location such as a kitchen counter.", "labels": [], "entities": []}, {"text": "Most previous research would classify spilt the beans as idiomatic irrespective of usage.", "labels": [], "entities": []}, {"text": "A recent study by) of 60 idiom MWE types concluded that almost half of them had clear literal meaning and over 40% of their usages in text were actually literal.", "labels": [], "entities": []}, {"text": "Thus, it would be important for an NLP application such as machine translation, for example, when given anew MWE token, to be able to determine whether it is used idiomatically or not.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7522044479846954}]}, {"text": "In this paper, we address the problem of MWE classification for verb-noun (VNC) token constructions in running text.", "labels": [], "entities": [{"text": "MWE classification", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.987138956785202}]}, {"text": "We investigate the binary classification of an unseen VNC token expression as being either Idiomatic (IDM) or Literal (LIT).", "labels": [], "entities": []}, {"text": "An IDM expression is certainly an MWE, however, the converse is not necessarily true.", "labels": [], "entities": []}, {"text": "We handle the problem of sparsity for MWE classification by exploring different vector space features: various vector similarity metrics, and more linguistically oriented feature sets.", "labels": [], "entities": [{"text": "MWE classification", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9878295660018921}]}, {"text": "We evaluate our results against a standard data set from the study by.", "labels": [], "entities": []}, {"text": "We achieve state of the art performance in classifying VNC tokens as either literal (F-measure: F \u03b2 1 =0.64) or idiomatic (F \u03b2 1 =0.82), corresponding to an overall accuracy of 75.54%.", "labels": [], "entities": [{"text": "F-measure: F \u03b2 1", "start_pos": 85, "end_pos": 101, "type": "METRIC", "confidence": 0.81822829246521}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9991492033004761}]}, {"text": "This paper is organized as follows: In Section 2 we describe our understanding of the various classes of MWEs in general.", "labels": [], "entities": []}, {"text": "Section 3 is a summary of previous related research.", "labels": [], "entities": []}, {"text": "Section 4 describes our approach.", "labels": [], "entities": []}, {"text": "In Section 5 we present the details of our experiments.", "labels": [], "entities": []}, {"text": "We discuss the results in Section 6.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We vary four of the experimental parameters: Context-Extent {sentence only narrow (N), sentence + paragraph broad(B)}, Context-Content {Words (W), Words+NER (NE)}, Dimension {no threshold (nT), frequency (F), ratio (R)}, and V-N compositionality {Additive (A), Multiplicative (M)}.", "labels": [], "entities": []}, {"text": "We present the results for all similarity measures.", "labels": [], "entities": [{"text": "similarity", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9783489108085632}]}, {"text": "The thresholds (for Dimension F req and Dimension Ratio ) are tuned on all the similarity measures collectively.", "labels": [], "entities": [{"text": "Dimension F req", "start_pos": 20, "end_pos": 35, "type": "METRIC", "confidence": 0.6027093629042307}]}, {"text": "It is observed that the performance of all the measures improved/worsened together, illustrating the same trends in performance, over the various settings of the thresholds evaluated on the DEV data set.", "labels": [], "entities": [{"text": "DEV data set", "start_pos": 190, "end_pos": 202, "type": "DATASET", "confidence": 0.9845608472824097}]}, {"text": "Based on tuning on the DEV set, we empirically set the value of the threshold on F to be 188 and for R to be 175 across all experimental conditions.", "labels": [], "entities": [{"text": "DEV set", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9157734513282776}, {"text": "F", "start_pos": 81, "end_pos": 82, "type": "METRIC", "confidence": 0.985663890838623}, {"text": "R", "start_pos": 101, "end_pos": 102, "type": "METRIC", "confidence": 0.9861752390861511}]}, {"text": "We present results here for 10 experimental conditions based on the four experimental parameters: {nT-A-W-N, nT-M-W-N, F-A-W-N, F-M-W-N, R-A-W-N, R-M-W-N, R-A-W-B, R-M-W-B, R-A-NE-B, R-M-NE-B}.", "labels": [], "entities": []}, {"text": "For instance, R-A-W-N, the Dimension parameter is set to the Ratio Dimension Ratio (R), the V-N compositionality mode is addition (A), and the Context-Content is set to Context \u2212 Content W ords (W), and, Context-Extent is set to Context N arrow (N).", "labels": [], "entities": []}, {"text": "For both DEV and TEST, we note that the R parameter settings coupled with the A parameter setting.", "labels": [], "entities": [{"text": "DEV", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.839003324508667}, {"text": "A parameter setting", "start_pos": 78, "end_pos": 97, "type": "METRIC", "confidence": 0.9656837582588196}]}, {"text": "For DEV, we observe that the results yielded from the Broad context extent, contextual sentence and surrounding paragraph, yield higher results than those obtained from the narrow N, context sentence only, across M and A conditions.", "labels": [], "entities": [{"text": "DEV", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7084038257598877}]}, {"text": "This trend is not consistent with the results on the TEST data set.", "labels": [], "entities": [{"text": "TEST data set", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9526671965916952}]}, {"text": "R-A-W-N, outperforms R-A-W-B, however, R-M-W-B outperforms R-M-W-N.", "labels": [], "entities": []}, {"text": "We would like to point out that R-M-W-N has very low values for the LIT F-measure, this is attributed to the use of a unified R threshold value of 175.", "labels": [], "entities": [{"text": "LIT F-measure", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.8527588248252869}]}, {"text": "We experimented with different optimal thresholds for R depending on the parameter setting combination and we discovered that for R-M-W-N, the fine-tuned optimal threshold should have been 27 as tuned on the DEV set, yielding LIT F-measures of 0.68 and 0.63, for DEV and TEST, respectively.", "labels": [], "entities": [{"text": "DEV set", "start_pos": 208, "end_pos": 215, "type": "DATASET", "confidence": 0.9414405822753906}, {"text": "LIT F-measures", "start_pos": 226, "end_pos": 240, "type": "METRIC", "confidence": 0.8675124049186707}, {"text": "DEV", "start_pos": 263, "end_pos": 266, "type": "DATASET", "confidence": 0.7742981910705566}, {"text": "TEST", "start_pos": 271, "end_pos": 275, "type": "METRIC", "confidence": 0.8343049883842468}]}, {"text": "Hence when using the unified value of 175, more of the compositional vectors components of V+N are pruned away leading to similarity values between the V+N vector and the VNC vector of 0 (across all similarity measures).", "labels": [], "entities": []}, {"text": "Accordingly, most of the expressions are mis-classified as IDM.", "labels": [], "entities": []}, {"text": "The best results overall are yielded from the NE conditions.", "labels": [], "entities": []}, {"text": "This result strongly suggests that using class based linguistic information and novel ways to keep the relevant tokens in the vectors such as R yields better MWE classification.", "labels": [], "entities": [{"text": "MWE classification", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9620372653007507}]}, {"text": "Qualitatively, we note the best results are obtained on the following VNCs from the TEST set in the Overlap similarity measure for the R-A-W-B experimental setting (percentage of tokens classified correctly): make hay(94%), make mark(88%), pull punch (86%), have word(81%), blow whistle (80%), hit wall (79%), hold fire (73%).", "labels": [], "entities": [{"text": "TEST set", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.8381825387477875}, {"text": "Overlap similarity measure", "start_pos": 100, "end_pos": 126, "type": "METRIC", "confidence": 0.65172478556633}]}, {"text": "While we note the highest performance on the following VNCs in the corresponding R-A-NE-B experimental setting: make hay(88%), make mark(87%), pull punch (91%), have word(85%), blow whistle (84%), hold fire (82%).", "labels": [], "entities": [{"text": "make mark", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.889231413602829}]}, {"text": "We observe that both conditions performed the worse on tokens from the following VNCs lose thread, make hit.", "labels": [], "entities": []}, {"text": "Especially, make hit is problematic since it mostly a literal expression, yet in the gold standard set we see it marked inconsistently.", "labels": [], "entities": [{"text": "make hit", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.8567046821117401}, {"text": "gold standard set", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.7767195403575897}]}, {"text": "For instance, the literal sentence He bowled it himself and Wilfred Rhodes made the winning hit while the following annotates make hit as idiomatic: It was the TV show Saturday Night Live which originally made Martina huge hit in the States.", "labels": [], "entities": []}, {"text": "We also note the difference in performance in the hard cases of VNCs that are relatively transparent, only the R-A-W-B and R-A-NE-B experimental conditions were able to classify them correctly with high F-measures as either IDM or LIT, namely: have word, hit wall, make mark.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 203, "end_pos": 213, "type": "METRIC", "confidence": 0.9510193467140198}, {"text": "LIT", "start_pos": 231, "end_pos": 234, "type": "METRIC", "confidence": 0.9407023787498474}]}, {"text": "For R-A-W-B, the yielded accuracies are 81%, 79% and 88% respectively, and for R-A-NE-B, the accuracies are 85%, 65%, and 87%, respectively.", "labels": [], "entities": []}, {"text": "However, in the nT-A-W-N condition have word is classified incorrectly 82% of the time and in F-A-W-N it is classified incorrectly 85% of the time.", "labels": [], "entities": []}, {"text": "Make mark is classified incorrectly 77% of the time, make hay (77%) and hit wall (57%) in the F-A-W-N experimental setting.", "labels": [], "entities": [{"text": "Make mark", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9119177758693695}, {"text": "F-A-W-N", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9338489174842834}]}, {"text": "This maybe attributed to the use of the Broader context, or the use of R in the other more accurate experimental settings.", "labels": [], "entities": [{"text": "Broader context", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8976709544658661}, {"text": "R", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.9502935409545898}]}], "tableCaptions": [{"text": " Table 1: Evaluation on of different experimental conditions on DEV", "labels": [], "entities": [{"text": "DEV", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8094491958618164}]}, {"text": " Table 2: Evaluation of different experimental conditions on TEST", "labels": [], "entities": [{"text": "TEST", "start_pos": 61, "end_pos": 65, "type": "TASK", "confidence": 0.49483707547187805}]}]}