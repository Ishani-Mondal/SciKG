{"title": [{"text": "Relation detection between named entities: report of a shared task", "labels": [], "entities": [{"text": "Relation detection between named entities", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.9347334861755371}]}], "abstractContent": [{"text": "In this paper we describe the first evaluation contest (track) for Portuguese whose goal was to detect and classify relations between named entities in running text, called ReRelEM.", "labels": [], "entities": []}, {"text": "Given a collection annotated with named entities belonging to ten different semantic categories, we marked all relationships between them within each document.", "labels": [], "entities": []}, {"text": "We used the following fourfold relationship classification: identity, included-in, located-in, and other (which was later on explicitly detailed into twenty different relations).", "labels": [], "entities": []}, {"text": "We provide a quantitative description of this evaluation resource , as well as describe the evaluation architecture and summarize the results of the participating systems in the track.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Our first concern in this pilot track was to make a clear separation between the evaluation of relations and the evaluation of NE detection, which was the goal of HAREM.", "labels": [], "entities": [{"text": "NE detection", "start_pos": 127, "end_pos": 139, "type": "TASK", "confidence": 0.9902532994747162}, {"text": "HAREM", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.809347927570343}]}, {"text": "So, ReRelEM 's evaluation uses as a starting point the set of alignments that correspond to a mapping of the NE in the golden collection (GC) to a (candidate) NE in the participation.", "labels": [], "entities": []}, {"text": "Evaluation has the following stages: \u2022 Maximization: the sets of relations annotated in both the GC and in the participation are maximized, applying the rules in; \u2022 Selection: the alignments where the NE in the GC is different from the corresponding one in the participation are removed, and so are all relations held between removed NEs; \u2022 Normalization: The identifiers of the NE in the participation are normalized in order to make it possible to compare the relations in both sides, given that each system uses its own identifiers.", "labels": [], "entities": []}, {"text": "\u2022 Translation: The alignments are translated to triples: arg1 relation arg2, where the arguments consist of the identifiers of the NE together with the facet, for example x67 LOCAL sede-de ty45 ORGANIZACAO.", "labels": [], "entities": [{"text": "Translation", "start_pos": 2, "end_pos": 13, "type": "TASK", "confidence": 0.9711624383926392}, {"text": "LOCAL sede-de ty45 ORGANIZACAO", "start_pos": 175, "end_pos": 205, "type": "METRIC", "confidence": 0.6164662539958954}]}, {"text": "\u2022 Filtering: removing relations of types not being evaluated (because HAREM, and therefore ReRelEM, allows for partial participation -and evaluation -scenarios 12 ).", "labels": [], "entities": []}, {"text": "\u2022 Individual evaluation: the triples in the GC are compared to the triples in the participation. of the European Union (...)", "labels": [], "entities": []}, {"text": "We are mainly a community of values and these common values constitute the foundation of the European Union.", "labels": [], "entities": []}, {"text": "In other words, it is possible to select a subset of the classification hierarchy.", "labels": [], "entities": []}, {"text": "\u2022 Global evaluation: measures (precision, recall and F-measure) are calculated based on the score of each triple.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9994719624519348}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9983294606208801}, {"text": "F-measure", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9967344403266907}]}, {"text": "Each triple is scored as correct, missing or incorrect.", "labels": [], "entities": []}, {"text": "We only considered as correct triples (and correct relations) those which linked the correct NEs and whose relation was well classified.", "labels": [], "entities": []}, {"text": "So, a system doesn't score if it correctly matches the NEs to be related, but fails to recognize the kind of relation.", "labels": [], "entities": []}, {"text": "We assign one point to each correct relation and none to incorrect or missing relations, and then we compute precision, recall and F-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.999574601650238}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9995384216308594}, {"text": "F-measure", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.997080385684967}]}, {"text": "ReRelEM's golden collection includes 12 texts with 4,417 words and 573 NEs (corresponding to 642 different facets).", "labels": [], "entities": [{"text": "ReRelEM's golden collection", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.872658982872963}]}, {"text": "In all we annotated 6,790 relations (1436 identity; 1612 inclusion; 1232 placement; 2510 other).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Category distribution in the golden collection", "labels": [], "entities": [{"text": "golden collection", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.860970139503479}]}, {"text": " Table 3: Frequency of other relations.", "labels": [], "entities": []}]}