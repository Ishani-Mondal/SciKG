{"title": [{"text": "Data-Driven Dependency Parsing of New Languages Using Incomplete and Noisy Training Data", "labels": [], "entities": [{"text": "Data-Driven Dependency Parsing of New Languages", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.685550257563591}]}], "abstractContent": [{"text": "We present a simple but very effective approach to identifying high-quality data in noisy data sets for structured problems like parsing, by greedily exploiting partial structures.", "labels": [], "entities": [{"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9744473695755005}]}, {"text": "We analyze our approach in an annotation projection framework for dependency trees, and show how dependency parsers from two different paradigms (graph-based and transition-based) can be trained on the resulting tree fragments.", "labels": [], "entities": []}, {"text": "We train parsers for Dutch to evaluate our method and to investigate to which degree graph-based and transition-based parsers can benefit from incomplete training data.", "labels": [], "entities": []}, {"text": "We find that partial correspondence projection gives rise to parsers that out-perform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006).", "labels": [], "entities": [{"text": "correspondence projection", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.7349225878715515}, {"text": "UAS", "start_pos": 214, "end_pos": 217, "type": "METRIC", "confidence": 0.9771317839622498}, {"text": "CoNLL-X Shared Task on supervised parsing", "start_pos": 235, "end_pos": 276, "type": "TASK", "confidence": 0.46864298979441327}]}], "introductionContent": [{"text": "Many weakly supervised approaches to NLP rely on heuristics or filtering techniques to deal with noise in unlabeled or automatically labeled training data, e.g., in the exploitation of parallel corpora for crosslingual projection of morphological, syntactic or semantic information.", "labels": [], "entities": [{"text": "crosslingual projection of morphological, syntactic or semantic information", "start_pos": 206, "end_pos": 281, "type": "TASK", "confidence": 0.8513056337833405}]}, {"text": "While heuristic approaches can implement (linguistic) knowledge that helps to detect noisy data (e.g.,), they are typically task-and language-specific and thus introduce a component of indirect supervision.", "labels": [], "entities": []}, {"text": "Non-heuristic filtering techniques, on the other hand, employ reliability measures (often unrelated to the task) to predict high-precision data points (e.g.,).", "labels": [], "entities": []}, {"text": "In order to reach a sufficient level of precision, filtering typically has to be aggressive, especially for highly structured tasks like parsing.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9978660941123962}]}, {"text": "Such aggressive filtering techniques incur massive data loss and enforce trade-offs between the quality and the amount of usable data.", "labels": [], "entities": []}, {"text": "Ideally, a general filtering strategy for weakly supervised training of structured analysis tools should eliminate noisy subparts in the automatic annotation without discarding its high-precision aspects; thereby data loss would be kept to a minimum.", "labels": [], "entities": []}, {"text": "In this paper, we propose an extremely simple approach to noise reduction which greedily exploits partial correspondences in a parallel corpus, i.e., correspondences potentially covering only substructures of translated sentences.", "labels": [], "entities": [{"text": "noise reduction", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.783990740776062}]}, {"text": "We implemented this method in an annotation projection framework to create training data for two dependency parsers representing different parsing paradigms: The MSTParser () as an instance of graph-based dependency parsing, and the MaltParser () to represent transitionbased dependency parsing.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 162, "end_pos": 171, "type": "DATASET", "confidence": 0.9180889129638672}, {"text": "transitionbased dependency parsing", "start_pos": 260, "end_pos": 294, "type": "TASK", "confidence": 0.6380388935407003}]}, {"text": "In an empirical evaluation, we investigate how they react differently to incomplete and noisy training data.", "labels": [], "entities": []}, {"text": "Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task ().", "labels": [], "entities": [{"text": "UAS", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.9696642160415649}]}, {"text": "After a summary of related work in Sec.", "labels": [], "entities": []}, {"text": "2, we discuss dependency tree projection (Sec. 3) and partial correspondence.", "labels": [], "entities": [{"text": "dependency tree projection", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.7865231235822042}]}, {"text": "5, we give an overview of graph-and transition-based dependency parsing and describe how each can be adapted for training on partial training data in Sec.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.6359017292658488}]}, {"text": "6. Experimental results are presented in Sec.", "labels": [], "entities": []}, {"text": "7, followed by an analysis in Sec.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Data reduction effect of noise filters.", "labels": [], "entities": [{"text": "Data reduction", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6336667984724045}]}, {"text": " Table 2: Fragmented parses projected with the alignment  filter. The sentences included in the data set 'bi+frags \u22643 '  are in boldface.", "labels": [], "entities": []}, {"text": " Table 3: Upper and lower bounds (UAS).", "labels": [], "entities": [{"text": "Upper and lower bounds (UAS)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.6970390251704625}]}, {"text": " Table 4: UAS of parsers trained on projected dependency  structures for (a) a sample of 100,000 sentences, subject  to filtering, (b) 10 random samples, each with 100,000  words after filtering (average scores given), and (c) the  entire Europarl corpus, subject to filtering.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6505199074745178}, {"text": "Europarl corpus", "start_pos": 239, "end_pos": 254, "type": "DATASET", "confidence": 0.992281973361969}]}, {"text": " Table 5: Performance relative to dependency length. (a) Projected MaltParsers and (b) projected MSTParsers.", "labels": [], "entities": []}, {"text": " Table 6: UAS relative to sentence length. (a) Projected  MaltParsers and (b) projected MSTParsers.", "labels": [], "entities": []}]}