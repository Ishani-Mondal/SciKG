{"title": [{"text": "A Novel Approach to Automatic Gazetteer Generation using Wikipedia", "labels": [], "entities": [{"text": "Automatic Gazetteer Generation", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7336777448654175}]}], "abstractContent": [{"text": "Gazetteers or entity dictionaries are important knowledge resources for solving a wide range of NLP problems, such as entity extraction.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7833432555198669}]}, {"text": "We introduce a novel method to automatically generate gazetteers from seed lists using an external knowledge resource, the Wikipedia.", "labels": [], "entities": []}, {"text": "Unlike previous methods, our method exploits the rich content and various structural elements of Wikipe-dia, and does not rely on language-or domain-specific knowledge.", "labels": [], "entities": []}, {"text": "Furthermore, applying the extended gazetteers to an entity extraction task in a scientific domain, we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers.", "labels": [], "entities": [{"text": "entity extraction task", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7955062488714854}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9945059418678284}]}], "introductionContent": [{"text": "Entity extraction is the task of identifying and classifying atomic text elements into predefined categories such as person names, place names, and organization names.", "labels": [], "entities": [{"text": "Entity extraction is the task of identifying and classifying atomic text elements into predefined categories such as person names, place names, and organization names", "start_pos": 0, "end_pos": 166, "type": "Description", "confidence": 0.7274421774424039}]}, {"text": "Entity extraction often serves as a fundamental step for complex Natural Language Processing (NLP) applications such as information retrieval, question answering, and machine translation.", "labels": [], "entities": [{"text": "Entity extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8446906507015228}, {"text": "information retrieval", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.7908815145492554}, {"text": "question answering", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.9091110229492188}, {"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.8044213652610779}]}, {"text": "It has been recognized that in this task, gazetteers, or entity dictionaries, play a crucial role ().", "labels": [], "entities": []}, {"text": "In addition, they serve as important resources for other studies, such as assessing level of ambiguities of a language, and disambiguation ().", "labels": [], "entities": []}, {"text": "Because building and maintaining high quality gazetteers by hand is very time consuming), many solutions have proposed generating gazetteers automatically from existing resources.", "labels": [], "entities": []}, {"text": "In particular, the success that solutions which exploit Wikipedia have been enjoying in many other NLP applications has encouraged a number of research works on automatic gazetteer generation to use Wikipedia, http://en.wikipedia.org such as works by, and.", "labels": [], "entities": [{"text": "gazetteer generation", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.6829276382923126}]}, {"text": "Unfortunately, current systems still present several limitations.", "labels": [], "entities": []}, {"text": "First, none have exploited the full content and structure of Wikipedia articles, but instead, only make use of the article's first sentence.", "labels": [], "entities": []}, {"text": "However, the full content and structure of Wikipedia carry rich information that has been proven useful in many other NLP problems, such as document classification (), entity disambiguation (Bunescu and), and semantic relatedness).", "labels": [], "entities": [{"text": "document classification", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.7807624340057373}, {"text": "entity disambiguation", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.7391850352287292}]}, {"text": "Second, no other works have evaluated their methods in the context of entity extraction tasks.", "labels": [], "entities": [{"text": "entity extraction tasks", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.82586141427358}]}, {"text": "Evaluating these generated gazetteers in real NLP applications is important, because the quality of these gazetteers has a major impact on the performance of NLP applications that make use of them.", "labels": [], "entities": []}, {"text": "Third, the majority of approaches focus on newswire domain and the four classic entity types location (LOC), person (PER), organization (ORG) and miscellaneous (MISC), which have been studied extensively.", "labels": [], "entities": []}, {"text": "However, it has been argued that entity extraction is often much harder in scientific domains due to complexity of domain languages, density of information and specificity of classes ().", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8381780087947845}]}, {"text": "In this paper we propose a novel approach to automatically generating gazetteers using external knowledge resources.", "labels": [], "entities": []}, {"text": "Our method is language-and domain-independent, and scalable.", "labels": [], "entities": []}, {"text": "We show that the content and various structural elements of Wikipedia can be successfully exploited to generate high quality gazetteers.", "labels": [], "entities": []}, {"text": "To assess gazetteer quality, we evaluate it in the context of entity extraction in the scientific domain of Archaeology, and demonstrate that the generated gazetteers improve the performance of an SVM-based entity tagger across all entity types on an archaeological corpus.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7808758616447449}, {"text": "SVM-based entity tagger", "start_pos": 197, "end_pos": 220, "type": "TASK", "confidence": 0.6527727742989858}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section, we review related work.", "labels": [], "entities": []}, {"text": "In section 3 we explain our methodology for auto-1 matic gazetteer generation.", "labels": [], "entities": [{"text": "auto-1 matic gazetteer generation", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.5858761444687843}]}, {"text": "Section 4 introduces the problem domain and describes the experiments conducted.", "labels": [], "entities": []}, {"text": "Section 5 presents and discusses the results.", "labels": [], "entities": []}, {"text": "Finally we conclude with an outline of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experiments.", "labels": [], "entities": []}, {"text": "Our goal is to build extended gazetteers using the methods proposed in section 3, and test them in an entity extraction task to improve a baseline system.", "labels": [], "entities": [{"text": "entity extraction task", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7867074906826019}]}, {"text": "First we introduce the setting, an entity extraction task in the archaeological domain; next we describe data preparation including training data annotation and gazetteer generation; then, we introduce our baseline; and finally present the results.", "labels": [], "entities": [{"text": "entity extraction task", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7820409635702769}, {"text": "gazetteer generation", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.7100277692079544}]}], "tableCaptions": [{"text": " Table 3. Top 5 most frequently extracted (counted by number of seed entities sharing that label) fine- grained type labels for each entity type. Numbers in brackets are the number of unique labels extracted", "labels": [], "entities": []}]}