{"title": [{"text": "Evaluating automatic extraction of rules for sentence plan construction", "labels": [], "entities": [{"text": "Evaluating automatic extraction of rules", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8197465419769288}, {"text": "sentence plan construction", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.6853506863117218}]}], "abstractContent": [{"text": "The freely available SPaRKy sentence planner uses handwritten weighted rules for sentence plan construction, and a user-or domain-specific second-stage ranker for sentence plan selection.", "labels": [], "entities": [{"text": "SPaRKy sentence planner", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6529366970062256}, {"text": "sentence plan construction", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.651754210392634}, {"text": "sentence plan selection", "start_pos": 163, "end_pos": 186, "type": "TASK", "confidence": 0.6268242200215658}]}, {"text": "However, coming up with sentence plan construction rules fora new domain can be difficult.", "labels": [], "entities": [{"text": "sentence plan construction", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6444596449534098}]}, {"text": "In this paper, we automatically extract sentence plan construction rules from the RST-DT corpus.", "labels": [], "entities": [{"text": "sentence plan construction", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6728931268056234}, {"text": "RST-DT corpus", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.7736583948135376}]}, {"text": "In our rules, we use only domain-independent features that are available to a sentence planner at runtime.", "labels": [], "entities": []}, {"text": "We evaluate these rules, and outline ways in which they can be used for sentence planning.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7801291048526764}]}, {"text": "We have integrated them into a revised version of SPaRKy.", "labels": [], "entities": [{"text": "SPaRKy", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8700219392776489}]}], "introductionContent": [{"text": "Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.8244921465714773}, {"text": "content selection", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.6995997577905655}, {"text": "discourse planning", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.6702951043844223}, {"text": "sentence planning", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7108073085546494}, {"text": "surface realization", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.7906523048877716}]}, {"text": "A sentence planner maps from an input discourse plan to an output sentence plan.", "labels": [], "entities": []}, {"text": "As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7507406771183014}, {"text": "discourse cue insertion", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.6451747318108877}, {"text": "referring expression generation", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.7335522572199503}]}, {"text": "The developer of a sentence planner must typically write rules by hand (e.g. () or learn a domainspecific model from a corpus of training data (e.g. ().", "labels": [], "entities": []}, {"text": "Unfortunately, there are very few corpora annotated with discourse plans, and it is hard to automatically label a corpus for discourse structure.", "labels": [], "entities": []}, {"text": "It is also hard to hand-write sentence planning rules starting from a \"blank slate\", as it were.", "labels": [], "entities": [{"text": "hand-write sentence planning", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.6143113970756531}]}, {"text": "In this paper, we outline a method for extracting sentence plan construction rules from the only publicly available corpus of discourse trees, the RST Discourse Treebank (RST-DT)).", "labels": [], "entities": [{"text": "extracting sentence plan construction", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.6823956817388535}, {"text": "RST Discourse Treebank (RST-DT))", "start_pos": 147, "end_pos": 179, "type": "DATASET", "confidence": 0.8079750041166941}]}, {"text": "These rules use only domainindependent information available to a sentence planner at run-time.", "labels": [], "entities": []}, {"text": "They have been integrated into the freely-available SPaRKy sentence planner.", "labels": [], "entities": [{"text": "SPaRKy sentence planner", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.5800638298193613}]}, {"text": "They serve as a starting point fora user of SPaRKy, who can add, remove or modify rules to fit a particular domain.", "labels": [], "entities": []}, {"text": "We also describe a set of experiments in which we look at each sentence plan construction task in order, evaluating our rules for that task in terms of coverage and discriminative power.", "labels": [], "entities": [{"text": "sentence plan construction task", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.7525771409273148}]}, {"text": "We discuss the implications of these experiments for sentence planning.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.887020081281662}]}, {"text": "The rest of this paper is structured as follows: In Section 2 we describe the sentence planning process using SPaRKy as an example.", "labels": [], "entities": [{"text": "sentence planning process", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.7756494383017222}]}, {"text": "In Sections 3 through 5 we describe how we obtain sentence plan construction rules.", "labels": [], "entities": [{"text": "sentence plan construction", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6633285482724508}]}, {"text": "In Section 6, we evaluate alternative rule sets.", "labels": [], "entities": []}, {"text": "In Section 7, we describe our modifications to the SPaRKy sentence planner to use these rules.", "labels": [], "entities": [{"text": "SPaRKy sentence planner", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.565894365310669}]}, {"text": "In Section 8 we conclude and present future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are two basic approaches to NLG, text-totext generation (in which a model learned from a text corpus is applied to produce new texts from text input) and data-to-text generation (in which non-text input is converted into text output).", "labels": [], "entities": [{"text": "text-totext generation", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7111077159643173}, {"text": "data-to-text generation", "start_pos": 160, "end_pos": 183, "type": "TASK", "confidence": 0.7360003441572189}]}, {"text": "In text-to-text generation, there has been considerable work on sentence fusion and information ordering, which are partly sentence planning tasks.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7384258359670639}, {"text": "sentence fusion", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7623947858810425}, {"text": "information ordering", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7766546607017517}]}, {"text": "For evaluation, researchers typically compare automatically produced text to the original humanproduced text, which is assumed to be \"correct\" (e.g.).", "labels": [], "entities": []}, {"text": "However, an evaluation that considers the only \"correct\" answer fora sentence planning task to be the answer in the original text is overly harsh.", "labels": [], "entities": []}, {"text": "First, although we assume that all the possibilities in the human-produced text are \"reasonable\", some maybe awkward or incorrect for particular domains, while other less frequent ones in the newspaper domain maybe more \"correct\" in another domain.", "labels": [], "entities": []}, {"text": "Our purpose is to layout sentence plan construction possibilities, not to reproduce the WSJ authorial voice.", "labels": [], "entities": [{"text": "layout sentence plan construction", "start_pos": 18, "end_pos": 51, "type": "TASK", "confidence": 0.6910149082541466}, {"text": "WSJ authorial voice", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.8307327429453532}]}, {"text": "Second, because SPaRKy is a twostage sentence planner and we are focusing hereon sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy's output.", "labels": [], "entities": [{"text": "sentence plan construction", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.7207299470901489}]}, {"text": "Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. ().", "labels": [], "entities": [{"text": "sentence planning tasks", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7881625493367513}, {"text": "datato-text generation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7645819187164307}]}, {"text": "By contrast, we want good coverage for all core sentence planning tasks.", "labels": [], "entities": [{"text": "core sentence planning tasks", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.7014468610286713}]}, {"text": "Although Walker et al. performed an evaluation of SPaRKy (, they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately.", "labels": [], "entities": []}, {"text": "Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.7906142175197601}, {"text": "span ordering", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.7387466728687286}, {"text": "punctuation selection", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.7109134346246719}, {"text": "discourse cue selection and placement", "start_pos": 175, "end_pos": 212, "type": "TASK", "confidence": 0.6848930656909943}]}, {"text": "However, they assumed that surface realization was already complete, so they used lexical features.", "labels": [], "entities": []}, {"text": "Their sentence planner is not publicly available.", "labels": [], "entities": [{"text": "sentence planner", "start_pos": 6, "end_pos": 22, "type": "TASK", "confidence": 0.720982015132904}]}, {"text": "In the following sections, we evaluate the information in our sentence plan construction rules in terms of coverage and discriminative power.", "labels": [], "entities": [{"text": "coverage", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9659217000007629}]}, {"text": "The first type of evaluation allows us to assess the degree to which our rules are general and provide system developers with an adequate number of choices for sentence planning.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 160, "end_pos": 177, "type": "TASK", "confidence": 0.7798995971679688}]}, {"text": "The second type of evaluation allows us to evaluate whether our reduced feature set helps us choose from the available possibilities better than a feature set consisting simply of the relation (i.e. is the complicated feature extraction necessary).", "labels": [], "entities": []}, {"text": "Because we include the full feature set in this evaluation, it can also be seen as a text-to-text generation type of evaluation for readers who would like to use the sentence planning rules for news-style text generation.", "labels": [], "entities": [{"text": "news-style text generation", "start_pos": 194, "end_pos": 220, "type": "TASK", "confidence": 0.6193386813004812}]}], "tableCaptions": [{"text": " Table 2: Span ordering classification accuracy.  For first-level data, n = 3147. For all data, n =  10170. Labels = {0 1, 1 0}.", "labels": [], "entities": [{"text": "Span ordering classification", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8248383402824402}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9752992391586304}]}, {"text": " Table 3: Between-span punctuation classification  accuracy. For first-level data, n = 3147. For all  data, n = 10170. Labels = {semicolon, comma,  full, N/A}.", "labels": [], "entities": [{"text": "Between-span punctuation classification", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.5587500929832458}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9775731563568115}]}, {"text": " Table 4: Discourse cue classification accuracy.  For first-level data, n = 3147 and no. labels = 92.  For all data, n = 10170 and no. labels = 203.", "labels": [], "entities": [{"text": "Discourse cue classification", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7550534605979919}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9897320866584778}]}]}