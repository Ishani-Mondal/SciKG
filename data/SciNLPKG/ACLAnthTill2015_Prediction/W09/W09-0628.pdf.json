{"title": [{"text": "Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE)", "labels": [], "entities": [{"text": "First NLG Challenge on Generating Instructions in Virtual Environments (GIVE)", "start_pos": 14, "end_pos": 91, "type": "TASK", "confidence": 0.7166795060038567}]}], "abstractContent": [{"text": "We describe the first installment of the Challenge on Generating Instructions in Virtual Environments (GIVE), anew shared task for the NLG community.", "labels": [], "entities": [{"text": "Challenge on Generating Instructions in Virtual Environments (GIVE)", "start_pos": 41, "end_pos": 108, "type": "TASK", "confidence": 0.6749519407749176}]}, {"text": "We motivate the design of the challenge, describe how we carried it out, and discuss the results of the system evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper reports on the methodology and results of the First Challenge on Generating Instructions in Virtual Environments (GIVE-1), which we ran from.", "labels": [], "entities": [{"text": "First Challenge on Generating Instructions in Virtual Environments (GIVE-1)", "start_pos": 57, "end_pos": 132, "type": "TASK", "confidence": 0.5950137837366625}]}, {"text": "GIVE is anew shared task for the NLG community.", "labels": [], "entities": [{"text": "GIVE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.657400906085968}]}, {"text": "It provides an end-to-end evaluation methodology for NLG systems that generate instructions which are meant to help a user solve a treasure-hunt task in a virtual 3D world.", "labels": [], "entities": []}, {"text": "The most innovative aspect from an NLG evaluation perspective is that the NLG system and the user are connected over the Internet.", "labels": [], "entities": []}, {"text": "This makes it possible to cheaply collect large amounts of evaluation data.", "labels": [], "entities": []}, {"text": "Five NLG systems were evaluated in GIVE-1 over a period of three months from November 2008 to February 2009.", "labels": [], "entities": [{"text": "GIVE-1", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.8965610265731812}]}, {"text": "During this time, we collected 1143 games that were played by users from 48 countries.", "labels": [], "entities": []}, {"text": "As far as we know, this makes GIVE-1 the largest evaluation effort in terms of experimental subjects ever.", "labels": [], "entities": [{"text": "GIVE-1", "start_pos": 30, "end_pos": 36, "type": "TASK", "confidence": 0.5444974899291992}]}, {"text": "We have evaluated the five systems both on objective measures (success rate, completion time, etc.) and subjective measures which were collected by asking the users to fill in a questionnaire.", "labels": [], "entities": [{"text": "completion time", "start_pos": 77, "end_pos": 92, "type": "METRIC", "confidence": 0.9177087545394897}]}, {"text": "GIVE-1 was intended as a pilot experiment in order to establish the validity of the evaluation methodology and understand the challenges involved in the instruction-giving task.", "labels": [], "entities": [{"text": "GIVE-1", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7077137231826782}]}, {"text": "We believe that we have achieved these purposes.", "labels": [], "entities": []}, {"text": "At the same time, we provide evaluation results for the five NLG systems which will help their developers improve them for participation in a future challenge, GIVE-2.", "labels": [], "entities": [{"text": "GIVE-2", "start_pos": 160, "end_pos": 166, "type": "DATASET", "confidence": 0.8852846026420593}]}, {"text": "GIVE-2 will retain the successful aspects of GIVE-1, while refining the task to emphasize aspects that we found to be challenging.", "labels": [], "entities": [{"text": "GIVE-2", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9671633243560791}, {"text": "GIVE-1", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.8567384481430054}]}, {"text": "We invite the ENLG community to participate in designing GIVE-2.", "labels": [], "entities": [{"text": "ENLG", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.9212510585784912}, {"text": "GIVE-2", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.723046600818634}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we will describe and motivate the GIVE Challenge.", "labels": [], "entities": [{"text": "GIVE Challenge", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.5496252626180649}]}, {"text": "In Section 3, we will then describe the evaluation method and infrastructure for the challenge.", "labels": [], "entities": []}, {"text": "Section 4 reports on the evaluation results.", "labels": [], "entities": []}, {"text": "Finally, we conclude and discuss future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The GIVE Challenge addresses a need fora new evaluation paradigm for natural language generation (NLG).", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.8103504677613577}]}, {"text": "NLG systems are notoriously hard to evaluate.", "labels": [], "entities": []}, {"text": "On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good.", "labels": [], "entities": []}, {"text": "Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult.", "labels": [], "entities": []}, {"text": "Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system's functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies.", "labels": [], "entities": []}, {"text": "Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it difficult to compare different approaches to the same problem, which in turn leads to an overall difficulty in measuring progress in the field.", "labels": [], "entities": []}, {"text": "The GIVE-1 evaluation was conducted via a client/server architecture which allows any user with an Internet connection to provide system evaluation data.", "labels": [], "entities": []}, {"text": "Internet-based studies have been shown to provide generous amounts of data in other areas of AI (von.", "labels": [], "entities": []}, {"text": "Our implementation allows smaller teams to develop a system that will participate in the challenge, without taking on the burden of running the human evaluation experiment, and it provides a direct comparison of all participating systems on the same evaluation data.", "labels": [], "entities": []}, {"text": "Now we describe the method we applied to obtain experimental data, and sketch the software infrastructure we developed for this purpose.", "labels": [], "entities": []}], "tableCaptions": []}