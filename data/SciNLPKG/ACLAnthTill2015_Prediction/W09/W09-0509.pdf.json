{"title": [{"text": "RUBISC -a Robust Unification-Based Incremental Semantic Chunker", "labels": [], "entities": []}], "abstractContent": [{"text": "We present RUBISC, anew incremen-tal chunker that can perform incremental slot filling and revising as it receives a stream of words.", "labels": [], "entities": [{"text": "RUBISC", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.8969393968582153}, {"text": "slot filling", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.7371102869510651}]}, {"text": "Slot values can influence each other via a unification mechanism.", "labels": [], "entities": []}, {"text": "Chunks correspond to sense units, and end-of-sentence detection is done in-crementally based on a notion of seman-tic/pragmatic completeness.", "labels": [], "entities": [{"text": "end-of-sentence detection", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7236931174993515}]}, {"text": "One of RU-BISC's main fields of application is in dialogue systems where it can contribute to responsiveness and hence naturalness, because it can provide a partial or complete semantics of an utterance while the speaker is still speaking.", "labels": [], "entities": []}, {"text": "The chunker is evaluated on a German transcribed speech corpus and achieves a concept error rate of 43.3% and an F-Score of 81.5.", "labels": [], "entities": [{"text": "German transcribed speech corpus", "start_pos": 30, "end_pos": 62, "type": "DATASET", "confidence": 0.7048285156488419}, {"text": "concept error rate", "start_pos": 78, "end_pos": 96, "type": "METRIC", "confidence": 0.6916960875193278}, {"text": "F-Score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.9996373653411865}]}], "introductionContent": [{"text": "Real-time NLP applications such as dialogue systems can profit considerably from incremental processing of language.", "labels": [], "entities": []}, {"text": "When syntactic and semantic structure is built on-line while the speech recognition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.8387500166893005}]}, {"text": "As ( and show, such incremental systems are typically preferred by users over nonincremental systems.", "labels": [], "entities": []}, {"text": "To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. ( etc.).", "labels": [], "entities": []}, {"text": "However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. (,,).", "labels": [], "entities": []}, {"text": "The complexity of the semantic representations needed is thus limited.", "labels": [], "entities": []}, {"text": "Moreover, user behaviour (ungrammatical sentences, hesitations, false starts) and error-prone ASR require the parsing process to be robust.", "labels": [], "entities": [{"text": "ASR", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9445129632949829}, {"text": "parsing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.970978319644928}]}, {"text": "We argue that obtaining relatively flat semantics in a limited domain while needing exigent robustness calls for investigating shallower incremental chunking approaches as alternatives to CFG or dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.6818746477365494}]}, {"text": "Previous work that uses a combination of shallow and deep parsing in dialogue systems also indicates that shallow methods can be superior to deep parsing ().", "labels": [], "entities": []}, {"text": "The question addressed in this paper is how to construct a chunker that works incrementally and robustly and builds the semantics required in a dialogue system.", "labels": [], "entities": []}, {"text": "In our framework chunks are built according to the semantic information they contain while syntactic structure itself is less important.", "labels": [], "entities": []}, {"text": "This approach is inspired by Selkirk's sense units.", "labels": [], "entities": []}, {"text": "She claims such units to be relevant for prosodic structure and different to syntactic structure.", "labels": [], "entities": []}, {"text": "Similarly, describes some characteristics of chunks as follows-properties which also make them seem to be useful units to be considered in spoken dialogue systems: \"when I read a sentence, I read it a chunk at a time..]", "labels": [], "entities": []}, {"text": "These chunks correspond in someway to prosodic patterns.", "labels": [], "entities": []}, {"text": "Chunks also represent a grammatical watershed of sorts.", "labels": [], "entities": []}, {"text": "The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template.", "labels": [], "entities": []}, {"text": "By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. and the order in which chunks occur is much more flexible than the order of words within chunks.\"", "labels": [], "entities": []}, {"text": "In our approach chunks are built incrementally (one at a time) and are defined semantically (a sense unit is complete when a slot in our template or frame semantics can be filled).", "labels": [], "entities": []}, {"text": "Ideally, in a full system, the definition of their boundaries will also be aided by prosodic information.", "labels": [], "entities": []}, {"text": "The current implementation builds the chunks or sense units by identifying a more or less fixed sequence of content and function words, similar to what Abney describes as a fixed template.", "labels": [], "entities": []}, {"text": "The relationships between the units are mediated by a unification mechanism which prevents selectional restrictions from being violated.", "labels": [], "entities": []}, {"text": "This allows the order of the sense units to be flexible, even as flexible as they appear in ungrammatical utterances.", "labels": [], "entities": []}, {"text": "This unification mechanism and the incremental method of operation are also the main difference to Abney's work and other chunkers.", "labels": [], "entities": []}, {"text": "In this paper, we first present our approach of chunking, show our grammar formalism, the main features of the chunker (unification mechanism, incrementality, robustness), and explain how the chunker can cope with certain tasks that are an issue in dialogue systems, such as online utterance endpointing and revising hypotheses.", "labels": [], "entities": []}, {"text": "In Section 3, we evaluate the chunker on a German corpus (of transcribed spontaneous speech) in terms of concept error rate and slot filling accuracy.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8175272047519684}, {"text": "concept error rate", "start_pos": 105, "end_pos": 123, "type": "METRIC", "confidence": 0.6685155928134918}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.7799668908119202}]}, {"text": "Then we discuss related work, followed by a general discussion and the conclusion.", "labels": [], "entities": []}, {"text": "shows a simple example where the chunker segments the input stream incrementally into semantically relevant chunks.", "labels": [], "entities": []}, {"text": "The figure also displays how the frame is being filled incrementally.", "labels": [], "entities": []}, {"text": "The chunk grammar developed for this work and the dialogue corpus used were German, but we give some examples in English for better readability.", "labels": [], "entities": []}], "datasetContent": [{"text": "The sense unit chunker was evaluated in terms of how well it performed in slot filling on an unseen part of our corpus.", "labels": [], "entities": [{"text": "sense unit chunker", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6183682878812155}, {"text": "slot filling", "start_pos": 74, "end_pos": 86, "type": "TASK", "confidence": 0.8342541754245758}]}, {"text": "This corpus comes annotated with utterance boundaries.", "labels": [], "entities": []}, {"text": "500 of these utterances were manually labelled in terms of the semantic slots defined in the grammar.", "labels": [], "entities": []}, {"text": "The annotators were not involved in the construction of the chunker or grammar.", "labels": [], "entities": []}, {"text": "The annotation guidelines detailed the possible values for each slot.", "labels": [], "entities": []}, {"text": "The entity names had to be filled in with the letter names of the pieces, the end slot with body parts or right, left, horizontal etc., and the position slots with positive and negative numbers.", "labels": [], "entities": []}, {"text": "The chunker was then run on 400 of these utterances and the slot values were compared with the annotated frames.", "labels": [], "entities": []}, {"text": "100 of the labelled utterances and 50 additional utter-ances were used by the author for developing the grammar.", "labels": [], "entities": []}, {"text": "We examined the following evaluation measures: \u2022 the concept error (concept err) rate (percentage of wrong frames) \u2022 the percentage of complete frames that were correct (frames corr) \u2022 the percentage of slots that were correct \u2022 the percentage of action slots correct \u2022 the percentage of end slots correct \u2022 the percentage of object:name slots correct \u2022 the percentage of object:xpos slots correct \u2022 the percentage of object:ypos slots correct The results are shown in.", "labels": [], "entities": [{"text": "concept error (concept err) rate", "start_pos": 53, "end_pos": 85, "type": "METRIC", "confidence": 0.701733478478023}]}, {"text": "We used a very simple baseline: a system that does not fill any slots.", "labels": [], "entities": []}, {"text": "This strategy still gets 17% of the frames right, because some utterances do not contain any real content.", "labels": [], "entities": []}, {"text": "For the sentence Also das ist recht schwer (Trans: That's quite difficult.), for instance, the gold standard semantic representation would be: {action:None, end:None, object:{xpos:None, name:None, ypos:None}}.", "labels": [], "entities": []}, {"text": "As the baseline 'system' always returns the empty frame, it scores perfectly for this example sentence.", "labels": [], "entities": []}, {"text": "We are aware that this appears to be a very easy baseline.", "labels": [], "entities": []}, {"text": "However, for some slots, such as the xpos and ypos slots it still turned out to be quite hard to beat this baseline, as wrong entries were common for those slots.", "labels": [], "entities": []}, {"text": "The chunker achieves a frame accuracy of 54.5% and an overall slot filling accuracy of 86.80% (compared to 17% and 64.3% baseline).", "labels": [], "entities": [{"text": "frame", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9518172144889832}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.7898083329200745}, {"text": "slot filling", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.6631285399198532}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8939327597618103}]}, {"text": "Of the individual slots the action slot was the one that improved the most.", "labels": [], "entities": []}, {"text": "The position slots were the only ones to deteriorate.", "labels": [], "entities": []}, {"text": "As 17% of our utterances did not contain any relevant content, i.e. the frame was completely empty, we repeated the evaluation without these irrelevant data.", "labels": [], "entities": []}, {"text": "The results are shown in brackets in the table.", "labels": [], "entities": []}, {"text": "To check the impact of the unification mechanism, we performed another evaluation with this mechanism turned off, i.e. slots are always filled when they are empty without regarding other slots.", "labels": [], "entities": []}, {"text": "In the second step in, the end slot would hence be filled.", "labels": [], "entities": []}, {"text": "This resulted in a decline in performance as can also be seen in.", "labels": [], "entities": []}, {"text": "We also turned off robustness features to test for their impact.", "labels": [], "entities": []}, {"text": "Surprisingly, turning off the skipping of one word within a string specified by a grammar rule (as in to erm clockwise), did not have an effect on the results on our corpus.", "labels": [], "entities": []}, {"text": "When we also turnoff allowing initial material (erm the piece), however, performance drops considerably.", "labels": [], "entities": []}, {"text": "We also tested a variant of the system RUBISCo (for RUBISC-overlap) which considers overlapping chunks: Take the third piece will result in xpos:3 for the original chunker, even if the utterance is continued with from the right.", "labels": [], "entities": []}, {"text": "RUBISC-o also considers the previous chunk the third piece for the search of a surface representation.", "labels": [], "entities": [{"text": "RUBISC-o", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.6598387360572815}]}, {"text": "In this case, it overwrites 3 with -3.", "labels": [], "entities": []}, {"text": "In general, this behaviour improves the results.", "labels": [], "entities": []}, {"text": "To allow a comparison with other work that reports recall and precision as measures, we also computed those values for RUBISC: for our test corpus recall was 83.47% and precision was 79.69% (F-score 81.54).", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9968215227127075}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9969546794891357}, {"text": "RUBISC", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.8844475150108337}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9893442392349243}, {"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9995463490486145}, {"text": "F-score", "start_pos": 191, "end_pos": 198, "type": "METRIC", "confidence": 0.9944992065429688}]}, {"text": "A direct comparison with other systems is of course not possible, because the tasks and data are different.", "labels": [], "entities": []}, {"text": "Nevertheless, the numbers allow an approximate feel of how well the system performs.", "labels": [], "entities": []}, {"text": "To get an even better idea of the performance, we let a second annotator label the data we tested on; inter-annotator agreement is given in.", "labels": [], "entities": []}, {"text": "The accuracy for most slots is around 90% agreement beween annotators.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996235370635986}]}, {"text": "The concept error rate is 32.25%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9288579821586609}]}, {"text": "We also examined 50 utterances of the test corpus for an error analysis.", "labels": [], "entities": []}, {"text": "The largest part of the errors was due to vocabulary restrictions or restrictions in the regular expressions: subjects used names for pieces or body parts or even verbs which had not been seen or considered during grammar development.", "labels": [], "entities": []}, {"text": "As our rules for end positions contained pronouns like (into the back), they were too restricted for some description variants (such that it touches the back).", "labels": [], "entities": []}, {"text": "Another problem that appears is that descriptions of starting positions can be confounded with descriptions of end positions.", "labels": [], "entities": []}, {"text": "Sometimes subjects refer to end positions not with body parts but with at the right side etc.", "labels": [], "entities": []}, {"text": "In some cases this leads to wrong entries in the object-position slots.", "labels": [], "entities": []}, {"text": "In some cases a full parser might be helpful, but not always, because some expressions are syntactically ambiguous: f\u00fcge das Teil ganz rechts in das Rechteck ein.", "labels": [], "entities": []}, {"text": "(put the piece on the right into the square/put the piece into the square on the right.)", "labels": [], "entities": []}, {"text": "A minority of errors was also: Evaluation results (in %) for RUBISC in comparison with the baseline, RUBISC without unification mechanism (w/o unif), without robustness (w/o rob), RUBISC with overlap (RUBISC-o), and inter-annotator aggreement (i-annotator).", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9686594009399414}]}, {"text": "See the text for more information.", "labels": [], "entities": []}, {"text": "due to complex descriptions (the damaged t where the right part has dropped downwards -referring to the f), transcription errors (recht statt rechts) etc.", "labels": [], "entities": []}], "tableCaptions": []}