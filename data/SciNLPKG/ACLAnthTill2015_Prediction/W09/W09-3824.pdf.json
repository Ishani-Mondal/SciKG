{"title": [{"text": "Cross Parser Evaluation and Tagset Variation : a French Treebank Study", "labels": [], "entities": [{"text": "Cross Parser Evaluation and Tagset Variation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7219776411851248}, {"text": "French Treebank Study", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.973228394985199}]}], "abstractContent": [{"text": "This paper presents preliminary investigations on the statistical parsing of French by bringing a complete evaluation on French data of the main probabilistic lexicalized and unlexicalized parsers first designed on the Penn Treebank.", "labels": [], "entities": [{"text": "statistical parsing of French", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.7574978172779083}, {"text": "Penn Treebank", "start_pos": 219, "end_pos": 232, "type": "DATASET", "confidence": 0.9945587813854218}]}, {"text": "We adapted the parsers on the two existing treebanks of French (Abeill\u00e9 et al., 2003; Schluter and van Genabith, 2007).", "labels": [], "entities": []}, {"text": "To our knowledge, mostly all of the results reported here are state-of-the-art for the constituent parsing of French on every available treebank.", "labels": [], "entities": [{"text": "constituent parsing of French", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.6655657440423965}]}, {"text": "Regarding the algorithms, the comparisons show that lexicalized parsing models are outperformed by the unlexicalized Berke-ley parser.", "labels": [], "entities": []}, {"text": "Regarding the treebanks, we observe that, depending on the parsing model, a tag set with specific features has direct influence over evaluation results.", "labels": [], "entities": []}, {"text": "We show that the adapted lexical-ized parsers do not share the same sensitivity towards the amount of lexical material used for training, thus questioning the relevance of using only one lexicalized model to study the usefulness of lexical-ization for the parsing of French.", "labels": [], "entities": [{"text": "parsing of French", "start_pos": 256, "end_pos": 273, "type": "TASK", "confidence": 0.9103842775026957}]}], "introductionContent": [{"text": "The development of large scale symbolic grammars has long been a lively topic in the French NLP community.", "labels": [], "entities": [{"text": "French NLP community", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.7937807639439901}]}, {"text": "Surprisingly, the acquisition of probabilistic grammars aiming at stochastic parsing, using either supervised or unsupervised methods, has not attracted much attention despite the availability of large manually syntactic annotated data for French.", "labels": [], "entities": []}, {"text": "Nevertheless, the availability of the Paris 7 French Treebank (, allowed) to carryout the extraction of a Tree Adjoining Grammar and led to induce the first effective lexicalized parser for French.", "labels": [], "entities": [{"text": "Paris 7 French Treebank", "start_pos": 38, "end_pos": 61, "type": "DATASET", "confidence": 0.9417540282011032}]}, {"text": "Yet, as noted by, the use of the treebank was \"challenging\".", "labels": [], "entities": []}, {"text": "Indeed, before carrying out successfully any experiment, the authors had to perform a deep restructuring of the data to remove errors and inconsistencies.", "labels": [], "entities": []}, {"text": "For the purpose of building a statistical LFG parser, ( have reannotated a significant subset of the treebank with two underlying goals: (1) designing an annotation scheme that matches as closely as possible the LFG theory and (2) ensuring a more consistent annotation.", "labels": [], "entities": []}, {"text": "On the other hand, showed that with anew released and corrected version of the treebank 1 it was possible to train statistical parsers from the original set of trees.", "labels": [], "entities": []}, {"text": "This path has the advantage of an easier reproducibility and eases verification of reported results.", "labels": [], "entities": []}, {"text": "With the problem of the usability of the data source being solved, the question of finding one or many accurate language models for parsing French raises.", "labels": [], "entities": [{"text": "parsing French", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.889213502407074}]}, {"text": "Thus, to answer this question, this paper reports a set of experiments where five algorithms, first designed for the purpose of parsing English, have been adapted to French: a PCFG parser with latent annotation (), a Stochastic Tree Adjoining Grammar parser, the Charniak's lexicalized parser) and the Bikel's implementation of Collins' Model 1 and 2 (Collins, 1999) described in ).", "labels": [], "entities": [{"text": "Collins' Model 1 and 2 (Collins, 1999)", "start_pos": 328, "end_pos": 366, "type": "DATASET", "confidence": 0.844877177476883}]}, {"text": "To ease further comparisons, we report results on two versions of the treebank: (1) the last version made available in December 2007, hereafter FTB , and described in () and the (2) LFG inspired version of.", "labels": [], "entities": [{"text": "FTB", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.7229874730110168}]}, {"text": "The paper is structured as follows : After a brief presentation of the treebanks, we discuss the use-fulness of testing different parsing frameworks over two parsing paradigms before introducing our experimental protocol and presenting our results.", "labels": [], "entities": []}, {"text": "Finally, we discuss and compare with related works on cross-language parser adaptation, then we conclude.", "labels": [], "entities": [{"text": "cross-language parser adaptation", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.8460975885391235}]}], "datasetContent": [{"text": "In this section, we specify the settings of the parsers for French, the evaluation protocol and the different instantiations of the treebanks we used for conducting the experiments.", "labels": [], "entities": []}, {"text": "For the BKY parser, we use the Berkeley implementation, with an initial horizontal markovization h=0, and 5 split/merge cycles.", "labels": [], "entities": [{"text": "BKY", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8130918741226196}]}, {"text": "For the COLLINS' MODEL, we use the standard parameters set for the model 2, without any argu-ment adjunct distinction table, as a rough emulation of the COLLINS MODEL 1.", "labels": [], "entities": [{"text": "COLLINS' MODEL", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.7484443187713623}]}, {"text": "The same set of parameters used for COLLINS' MODEL 2 is used for the MODEL X except for the parameters \"Mod{Nonterminal,Word}ModelStructureNumber\" set to 1 instead of 2.", "labels": [], "entities": []}, {"text": "Constituency Evaluation: we use the standard labeled bracketed PARSEVAL metric for evaluation (, along with unlabeled dependency evaluation, which is described as a more annotation-neutral metric in.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.7989531755447388}]}, {"text": "In the remainder of this paper, we use PARSEVAL as a shortcut for Labeled Brackets results on sentence of length 40 or less.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9569999575614929}]}, {"text": "Dependency Evaluation: unlabeled dependencies are computed using the algorithm, and the Dybro Johansens's head propagation rules cited above . The unlabeled dependency accuracy gives the percentage of input words (excluding punctuation) that receive the correct head.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.691847562789917}]}, {"text": "All reported evaluations in this paper are calculated on sentences of length less than 40 words.", "labels": [], "entities": []}, {"text": "In (, the authors showed that it was possible to accurately train the Petrov's parser () on the FTB using a more fine grained tag set.", "labels": [], "entities": [{"text": "FTB", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9549338817596436}]}, {"text": "This tagset, named CC 12 annotates the basic non-terminal labels with verbal mood information, and wh-features.", "labels": [], "entities": []}, {"text": "Results were shown to be state of the art with a F 1 parseval score of 86.42% on less than 40 words sentences.", "labels": [], "entities": [{"text": "F 1 parseval score", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.9545763283967972}]}, {"text": "To summarize, the authors tested the impact of tagset variations over the FTB using constituency measures as performance indicators.", "labels": [], "entities": [{"text": "FTB", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9597821235656738}]}, {"text": "Knowing that the MFT has been built with PCFGbased LFG parsing performance in mind (Schluter TREEBANKS+ in. and van Genabith, 2008) but suffers from a small training size and yet allows surprisingly high parsing results (PARSEVAL F-score (<=40) of 79.95 % on the MFT gold standard), one would have wished to verify its performance with more annotated data.", "labels": [], "entities": [{"text": "PCFGbased LFG parsing", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7340636054674784}, {"text": "TREEBANKS", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9758591055870056}, {"text": "PARSEVAL", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.9724122881889343}, {"text": "F-score", "start_pos": 230, "end_pos": 237, "type": "METRIC", "confidence": 0.5168813467025757}, {"text": "MFT gold standard", "start_pos": 263, "end_pos": 280, "type": "DATASET", "confidence": 0.8657699823379517}]}, {"text": "However, some semi-automatic modifications brought to the global structure of this treebank cannot be applied, in an automatic and reversible way, to the FTB.", "labels": [], "entities": [{"text": "FTB", "start_pos": 154, "end_pos": 157, "type": "DATASET", "confidence": 0.9613334536552429}]}, {"text": "Anyway, even if we cannot evaluate the influence of a treebank structure to another, we can evaluate the influence of one tagset to another treebank using handwritten conversion tools.", "labels": [], "entities": []}, {"text": "In order to evaluate the relations between tagsets and parsing accuracy on a given treebank, we extract the optimal tagsets 13 from the FTB, the CC tagset and we convert the MFT POS tags to this tagset.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9649732708930969}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9736506342887878}, {"text": "FTB", "start_pos": 136, "end_pos": 139, "type": "DATASET", "confidence": 0.9413142800331116}]}, {"text": "We then do the same for the FTB on which we apply the MFT's optimal tagset (ie. SCHLU).", "labels": [], "entities": [{"text": "FTB", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.929140567779541}]}, {"text": "Before introducing the results of our experiments, we briefly describe these tagsets.", "labels": [], "entities": []}, {"text": "Results of these experiments, presented in, show that BKY displays higher performances 13 W.r.t constituent parsing accuracy in every aspects (constituency and dependency, except for the MFT-SCHLU).", "labels": [], "entities": [{"text": "BKY", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.6248879432678223}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.915825366973877}, {"text": "MFT-SCHLU", "start_pos": 187, "end_pos": 196, "type": "DATASET", "confidence": 0.7261531352996826}]}, {"text": "Regardless of the parser type, we note that unlabeled dependency scores are higher with the SCHLU tagset than with the CC tagset.", "labels": [], "entities": [{"text": "SCHLU tagset", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.8496645092964172}]}, {"text": "That can be explained by the finest granularity of the SCHLU based rule set compared to the other tagset's rules.", "labels": [], "entities": [{"text": "SCHLU based rule set", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.8208475410938263}]}, {"text": "As these rules have all been generated from meta description (a general COORD label rewrites into COORD_vfinite, COORD_Sint, etc..) their coverage and global accuracy is higher.", "labels": [], "entities": [{"text": "coverage", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.993080198764801}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9692019820213318}]}, {"text": "For example the FTB-CC contains 18 head rules whereas the FTB-SCHLU contains 43 rules.", "labels": [], "entities": [{"text": "FTB-CC", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.968447744846344}, {"text": "FTB-SCHLU", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9809485673904419}]}, {"text": "Interestingly, the ranking of lexicalized parsers w.r.t PARSEVAL metrics shows that CHARNIAK has the highest performance over both treebank tagsets variation even though the MFT's table (table 5) exhibits anon statistically significant variation between CHARNIAK and STIG-spinal on PARSEVAL evaluation of the MFT-CC.", "labels": [], "entities": [{"text": "MFT-CC", "start_pos": 309, "end_pos": 315, "type": "DATASET", "confidence": 0.8624597787857056}]}, {"text": "One the other hand, unlabeled dependency evaluations over lexicalized parsers are different among treebanks.", "labels": [], "entities": []}, {"text": "In the case of the FTB, CHARNIAK exhibits the highest F-score ( FTB-CC: 89.7, FTB-SCHLU: 89.67) whereas SPINAL STIG performs slightly better on the MFT-SCHLU (MFT-CC: 86,7, MFT-SCHLU: 87.16).", "labels": [], "entities": [{"text": "FTB", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.7899748682975769}, {"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9987012147903442}, {"text": "FTB-CC", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.5605587959289551}, {"text": "FTB-SCHLU", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.5369077920913696}, {"text": "MFT-SCHLU", "start_pos": 148, "end_pos": 157, "type": "DATASET", "confidence": 0.8599162697792053}, {"text": "MFT-SCHLU", "start_pos": 173, "end_pos": 182, "type": "DATASET", "confidence": 0.8161009550094604}]}, {"text": "Note that both tested variations of the Collins' model 2 display very high unlabeled dependency scores with the SCHLU tagset.", "labels": [], "entities": [{"text": "Collins' model 2", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.9513273040453593}, {"text": "SCHLU tagset", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.8936916589736938}]}], "tableCaptions": [{"text": " Table 3: Labeled F 1 scores for unlexicalised  and lexicalised parsers on treebanks with minimal  tagsets", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.906357983748118}]}, {"text": " Table 5: Evaluation Results: MFT-CC vs MFT-SCHLU and FTB-CC vs FTB-SCHLU", "labels": [], "entities": [{"text": "MFT-CC", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.7167055010795593}, {"text": "MFT-SCHLU", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.7720558047294617}, {"text": "FTB-CC", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.8620847463607788}, {"text": "FTB-SCHLU", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.796977698802948}]}, {"text": " Table 6: Labeled bracket scores on Arun's FTB  version and on the MFT", "labels": [], "entities": [{"text": "Arun's FTB  version", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.8427232056856155}, {"text": "MFT", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8639866709709167}]}]}