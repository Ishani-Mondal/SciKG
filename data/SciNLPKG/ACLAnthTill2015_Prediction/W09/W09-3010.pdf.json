{"title": [{"text": "Human Evaluation of Article and Noun Number Usage: Influences of Context and Construction Variability", "labels": [], "entities": [{"text": "Article and Noun Number Usage", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.5601794958114624}]}], "abstractContent": [{"text": "Evaluating systems that correct errors in non-native writing is difficult because of the possibility of multiple correct answers and the variability inhuman agreement.", "labels": [], "entities": []}, {"text": "This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number.", "labels": [], "entities": []}, {"text": "These systems should, ideally, be evaluated on a corpus of learners' writing, annotated with acceptable corrections.", "labels": [], "entities": []}, {"text": "However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9990149736404419}, {"text": "predicting what a native writer originally wrote in well-formed text", "start_pos": 122, "end_pos": 190, "type": "TASK", "confidence": 0.8252642452716827}]}, {"text": "This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case.", "labels": [], "entities": []}, {"text": "Two studies have already challenged this \"single correct construction\" assumption by comparing the output of a system to the original text.", "labels": [], "entities": []}, {"text": "In), two human judges were presented with 200 sentences and, for each sentence, they were asked to select which preposition (either the writer's preposition, or the system's) better fits the context.", "labels": [], "entities": []}, {"text": "In 28% of the cases where the writer and the system differed, the human raters found the system's prediction to be equal to or better than the writer's original preposition.", "labels": [], "entities": []}, {"text": "() found similar results on the sentence level in a task that evaluated many different parts of speech.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: For each noun, two sentences were se- lected from each configuration of number, article  and anaphor.", "labels": [], "entities": []}, {"text": " Table 4: Breakdown of the annotations by rater  and by stage. See  \u00a74 for a discussion.", "labels": [], "entities": [{"text": "Breakdown", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9354467988014221}]}, {"text": " Table 5: The confusion tables of the two raters for  the two stages.", "labels": [], "entities": []}]}