{"title": [{"text": "Incremental Hypothesis Alignment with Flexible Matching for Building Confusion Networks: BBN System Description for WMT09 System Combination Task", "labels": [], "entities": [{"text": "Incremental Hypothesis Alignment", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.721945563952128}, {"text": "WMT09", "start_pos": 116, "end_pos": 121, "type": "DATASET", "confidence": 0.6419926285743713}]}], "abstractContent": [{"text": "This paper describes the incremental hypothesis alignment algorithm used in the BBN submissions to the WMT09 system combination task.", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.73554727435112}, {"text": "BBN", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.7390319108963013}, {"text": "WMT09 system combination task", "start_pos": 103, "end_pos": 132, "type": "TASK", "confidence": 0.6549343019723892}]}, {"text": "The alignment algorithm used a sentence specific alignment order, flexible matching, and new shift heuristics.", "labels": [], "entities": []}, {"text": "These refinements yield more compact confusion networks compared to using the pair-wise or incremental TER alignment algorithms.", "labels": [], "entities": [{"text": "TER alignment", "start_pos": 103, "end_pos": 116, "type": "TASK", "confidence": 0.7085873484611511}]}, {"text": "This should reduce the number of spurious insertions in the system combination output and the system combination weight tuning converges faster.", "labels": [], "entities": []}, {"text": "System combination experiments on the WMT09 test sets from five source languages to English are presented.", "labels": [], "entities": [{"text": "WMT09 test sets", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9611903031667074}]}, {"text": "The best BLEU scores were achieved by combing the English outputs of three systems from all five source languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9946287274360657}]}], "introductionContent": [{"text": "Machine translation (MT) systems have different strengths and weaknesses which can be exploited by system combination methods resulting in an output with a better performance than any individual MT system output as measured by automatic evaluation metrics.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.872668182849884}]}, {"text": "Confusion network decoding has become the most popular approach to MT system combination.", "labels": [], "entities": [{"text": "MT system combination", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.9236153960227966}]}, {"text": "The first confusion network decoding method () was based on multiple string alignment (MSA) ( borrowed from biological sequence analysis.", "labels": [], "entities": [{"text": "multiple string alignment", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6584602197011312}]}, {"text": "However, MSA does not allow re-ordering.", "labels": [], "entities": [{"text": "MSA", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8605713844299316}]}, {"text": "The translation edit rate (TER)) produces an alignment between two strings and allows shifts of blocks of words.", "labels": [], "entities": [{"text": "translation edit rate (TER))", "start_pos": 4, "end_pos": 32, "type": "METRIC", "confidence": 0.8433933953444163}]}, {"text": "The availability of the TER software has made it easy to build a high performance system combination baseline ().", "labels": [], "entities": []}, {"text": "The pair-wise TER alignment originally described by has various limitations.", "labels": [], "entities": [{"text": "TER alignment", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.6729065030813217}]}, {"text": "First, the hypotheses are aligned independently against the skeleton which determines the word order of the output.", "labels": [], "entities": []}, {"text": "The same word from two different hypotheses maybe inserted in different positions w.r.t. the skeleton and multiple insertions require special handling.", "labels": [], "entities": []}, {"text": "described an incremental TER alignment to mitigate these problems.", "labels": [], "entities": [{"text": "TER alignment", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.8670681118965149}]}, {"text": "The incremental TER alignment used a global order in which the hypotheses were aligned.", "labels": [], "entities": [{"text": "TER alignment", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.6983427107334137}]}, {"text": "Second, the TER software matches words with identical surface strings.", "labels": [], "entities": []}, {"text": "The pairwise alignment methods proposed by,, and are able to match also synonyms and words with identical stems.", "labels": [], "entities": []}, {"text": "Third, the TER software uses a set of heuristics which is not always optimal in determining the block shifts.", "labels": [], "entities": []}, {"text": "proposed using inversion transduction grammars to produce different pair-wise alignments.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "A refined incremental alignment algorithm is described in Section 2.", "labels": [], "entities": []}, {"text": "Experimental evaluation comparing the pair-wise and incremental TER alignment algorithms with the refined alignment algorithm on WMT09 system combination task is presented in Section 3.", "labels": [], "entities": [{"text": "TER alignment", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.8027804791927338}, {"text": "WMT09 system combination task", "start_pos": 129, "end_pos": 158, "type": "DATASET", "confidence": 0.9201071560382843}]}, {"text": "Conclusions and future work are presented in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "System combination experiments combining the English WMT09 translation task outputs were performed.", "labels": [], "entities": [{"text": "WMT09 translation task", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.622730036576589}]}, {"text": "A total of 96 English outputs were provided including primary, contrastive, and \u00a2 -best outputs.", "labels": [], "entities": []}, {"text": "Only the primary -best outputs were combined due to time constraints.", "labels": [], "entities": []}, {"text": "The numbers of primary systems per source language were: 3 for Czech, 15 for German, 9 for Spanish, 15 for French, and 3 for Hungarian.", "labels": [], "entities": []}, {"text": "The English bigram and 5-gram language models were interpolated from four LM components trained on the English monolingual Europarl (45M tokens) and News (510M tokens) corpora, and the English sides of the News Commentary (2M tokens) and GigaFrEn (683M tokens) parallel corpora.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.8463677167892456}]}, {"text": "The interpolation weights were tuned to minimize perplexity on news-dev2009 set.", "labels": [], "entities": [{"text": "news-dev2009 set", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9215967655181885}]}, {"text": "The system combination weights -one for each system, LM weight, and word and NULL insertion penalties -were tuned to maximize the BLEU () score on the tuning set (newssyscomb2009).", "labels": [], "entities": [{"text": "NULL insertion penalties", "start_pos": 77, "end_pos": 101, "type": "METRIC", "confidence": 0.7241581082344055}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9992796778678894}]}, {"text": "Since the system combination was performed on tokenized and lower cased outputs, a trigrambased true caser was trained on all News training data.", "labels": [], "entities": [{"text": "News training data", "start_pos": 126, "end_pos": 144, "type": "DATASET", "confidence": 0.8571459452311198}]}, {"text": "The tuning maybe summarized as follows: 1.", "labels": [], "entities": []}, {"text": "Tokenize and lowercase the outputs; 2.", "labels": [], "entities": []}, {"text": "Align hypotheses incrementally using each output as a skeleton; 3.", "labels": [], "entities": []}, {"text": "Join the confusion networks into a lattice with skeleton specific prior estimates; 4.", "labels": [], "entities": []}, {"text": "Extract a \u00a3 \u00a3 -best list from the lattice given the current weights; -best list from the lattice given the best decoding weights and re-score hypotheses with a 5-gram; 9.", "labels": [], "entities": []}, {"text": "Tune re-scoring weights given the final \u00a3 \u00a3 -best list; 10.", "labels": [], "entities": [{"text": "final \u00a3 \u00a3 -best list", "start_pos": 34, "end_pos": 54, "type": "METRIC", "confidence": 0.537486066420873}]}, {"text": "Extract -best hypotheses from the \u00a3 \u00a3 -best list given the best re-scoring weights, re-case, and detokenize.", "labels": [], "entities": [{"text": "Extract", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9426945447921753}, {"text": "re-case", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9417876601219177}, {"text": "detokenize", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9905421137809753}]}, {"text": "After tuning the system combination weights, the outputs on a test set maybe combined using the same steps excluding 4-7 and 9.", "labels": [], "entities": []}, {"text": "The hypothesis scores and tuning are identical to the setup used in (.", "labels": [], "entities": [{"text": "tuning", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9578156471252441}]}, {"text": "Case insensitive TER and BLEU scores for the combination outputs using the pair-wise and incremental TER alignment as well as the flexible alignment on the tuning (dev) and test sets are shown in.", "labels": [], "entities": [{"text": "TER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9968971014022827}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9981439113616943}, {"text": "TER alignment", "start_pos": 101, "end_pos": 114, "type": "METRIC", "confidence": 0.9653966724872589}]}, {"text": "Only case insensitive scores are reported since the re-casers used by different systems are very different and some are trained using larger resources than provided for WMT09.", "labels": [], "entities": [{"text": "WMT09", "start_pos": 169, "end_pos": 174, "type": "DATASET", "confidence": 0.9024703502655029}]}, {"text": "The scores of the worst and best individual system outputs are also shown.", "labels": [], "entities": []}, {"text": "The best and worst TER and BLEU scores are not necessarily from the same system output.", "labels": [], "entities": [{"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.997374415397644}, {"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9840397238731384}]}, {"text": "Both incremental and flexible alignments used sentence specific alignment order.", "labels": [], "entities": []}, {"text": "Combinations using the incremental and flexible hypothesis alignment algorithms consistently outperform the ones using the pair-wise TER alignment.", "labels": [], "entities": [{"text": "TER", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.8751474022865295}]}, {"text": "The flexible alignment is slightly better than the incremental alignment on Czech, Spanish, and Hungarian, and significantly better on French to English test set scores.", "labels": [], "entities": []}, {"text": "Since the test sets for each language pair consist of translations of the same documents, it is possible to combine outputs from many source languages to English.", "labels": [], "entities": []}, {"text": "There were a total of 46 English primary -best system outputs.", "labels": [], "entities": [{"text": "English primary -best system outputs", "start_pos": 25, "end_pos": 61, "type": "DATASET", "confidence": 0.8100025355815887}]}, {"text": "Using all 46 outputs would have required too much memory in tuning, so a subset of 11 outputs was chosen.", "labels": [], "entities": []}, {"text": "The 11 outputs consist of google, uedin, and uka outputs on all languages.", "labels": [], "entities": []}, {"text": "Case insensitive TER and BLEU scores for the xx-en combination are shown in.", "labels": [], "entities": [{"text": "TER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9901266098022461}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9982909560203552}]}, {"text": "In addition to incremental and flexible alignment methods which used sentence specific alignment order, scores for incremental TER alignment with a fixed alignment order used in the BBN submissions to WMT08 dev cz-en de-en es-en fr-en hu-: Case insensitive TER and BLEU scores on newssyscomb2009 (dev) and newstest2009 (test) for five source languages.", "labels": [], "entities": [{"text": "BBN submissions to WMT08 dev cz-en de-en es-en fr-en hu", "start_pos": 182, "end_pos": 237, "type": "DATASET", "confidence": 0.8889432191848755}, {"text": "BLEU", "start_pos": 265, "end_pos": 269, "type": "METRIC", "confidence": 0.9980891346931458}]}, {"text": "() are marked as incr-wmt08.", "labels": [], "entities": []}, {"text": "The sentence specific alignment order yields about a half BLEU point gain on the tuning set and a one BLEU point gain on the test set.", "labels": [], "entities": [{"text": "sentence specific alignment", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.5557024379571279}, {"text": "BLEU point gain", "start_pos": 58, "end_pos": 73, "type": "METRIC", "confidence": 0.9673619071642557}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.998216450214386}]}, {"text": "All system combination experiments yield very good BLEU gains on both sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9996819496154785}]}, {"text": "The scores are also significantly higher than any combination from a single source language.", "labels": [], "entities": []}, {"text": "This shows that the outputs from different source languages are likely to be more diverse than outputs from different MT systems on a single language pair.", "labels": [], "entities": []}, {"text": "The combination is not guaranteed to be the best possible as the set of outputs was chosen arbitrarily.", "labels": [], "entities": []}, {"text": "The compactness of the confusion networks maybe measured by the average number of nodes and arcs per segment.", "labels": [], "entities": []}, {"text": "All xx-en confusion networks for newssyscomb2009 and newstest2009 after the incremental TER alignment had on average 44.5 nodes and 112.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Case insensitive TER and BLEU scores on newssyscomb2009 (dev) and newstest2009  (test) for five source languages.", "labels": [], "entities": [{"text": "TER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.71544349193573}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9904404878616333}]}, {"text": " Table 2: Case insensitive TER and BLEU  scores on newssyscomb2009 (dev) and  newstest2009 (test) for xx-en combination.", "labels": [], "entities": [{"text": "TER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.6748005151748657}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9874380230903625}]}]}