{"title": [{"text": "Multilingual semantic parsing with a pipeline of linear classifiers", "labels": [], "entities": [{"text": "Multilingual semantic parsing", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7121869524319967}]}], "abstractContent": [{"text": "I describe a fast multilingual parser for semantic dependencies.", "labels": [], "entities": []}, {"text": "The parser is implemented as a pipeline of linear classifiers trained with support vector machines.", "labels": [], "entities": []}, {"text": "I use only first order features, and no pair-wise feature combinations in order to reduce training and prediction times.", "labels": [], "entities": []}, {"text": "Hyper-parameters are carefully tuned for each language and sub-problem.", "labels": [], "entities": []}, {"text": "The system is evaluated on seven different languages: Catalan, Chinese, Czech, English, German, Japanese and Spanish.", "labels": [], "entities": []}, {"text": "An analysis of learning rates and of the reliance on syntactic parsing quality shows that only modest improvements could be expected for most languages given more training data; Better syntactic parsing quality, on the other hand, could greatly improve the results.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7380376160144806}]}, {"text": "Individual tuning of hyper-parameters is crucial for obtaining good semantic parsing quality.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7662600874900818}]}], "introductionContent": [{"text": "This paper presents my submission for the semantic parsing track of the CoNLL 2009 shared task on syntactic and semantic dependencies in multiple languages).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7327077835798264}, {"text": "CoNLL 2009 shared task", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7288186550140381}]}, {"text": "The submitted parser is simpler than the submission in which I participated at the CoNLL 2008 shared task on joint learning of syntactic and semantic dependencies (, in which we used a more complex committee based approach to both syntax and semantics.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.7811251580715179}]}, {"text": "Results are on par with our previous system, while the parser is orders of magnitude faster both at training and prediction time and is able to process natural language text in Catalan, Chinese, Czech, English, German, Japanese and Spanish.", "labels": [], "entities": []}, {"text": "The parser depends on the input to be annotated with part-of-speech tags and syntactic dependencies.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Semantic labelled and unlabelled F 1 -scores for  each language and domain. Left column: official labelled  F 1 -score. Middle column: post submission labelled F 1 - score. Right column: post submission unlabelled F 1 - score.  \u2020 indicates out-of-domain test data.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9017047733068466}, {"text": "F 1 -score", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9331784099340439}, {"text": "F 1 - score", "start_pos": 170, "end_pos": 181, "type": "METRIC", "confidence": 0.8165823370218277}, {"text": "F 1 - score", "start_pos": 224, "end_pos": 235, "type": "METRIC", "confidence": 0.9513641744852066}]}, {"text": " Table 2: Semantic labelled F 1 -scores w.r.t. training set  size.  \u2020 indicates out-of-domain test data.", "labels": [], "entities": [{"text": "Semantic labelled F 1 -scores w.r.t. training set  size", "start_pos": 10, "end_pos": 65, "type": "METRIC", "confidence": 0.7270812839269638}]}, {"text": " Table 3: Semantic unlabelled F 1 -scores w.r.t. training set  size.  \u2020 indicates out-of-domain test data.", "labels": [], "entities": [{"text": "F 1 -scores w.r.t. training set  size", "start_pos": 30, "end_pos": 67, "type": "METRIC", "confidence": 0.8658469542860985}]}, {"text": " Table 4: Predicate sense disambiguation F 1 -scores w.r.t.  training set size.  \u2020 indicates out-of-domain test data.", "labels": [], "entities": [{"text": "Predicate sense disambiguation F 1 -scores w.r.t.  training set size", "start_pos": 10, "end_pos": 78, "type": "METRIC", "confidence": 0.8535549586469476}]}, {"text": " Table 5: Percentage of exactly matched predicate- argument frames w.r.t. training set size.  \u2020 indicates out- of-domain test data.", "labels": [], "entities": []}, {"text": " Table 6: Semantic labelled F 1 -scores w.r.t. training set  size, using gold standard syntactic and part-of-speech tag  annotation.  \u2020 indicates out-of-domain test data.", "labels": [], "entities": [{"text": "F 1 -scores w.r.t. training set  size", "start_pos": 28, "end_pos": 65, "type": "METRIC", "confidence": 0.7742666974663734}]}, {"text": " Table 7: Semantic unlabelled F 1 -scores w.r.t. training set  size, using gold standard syntactic and part-of-speech tag  annotation.  \u2020 indicates out-of-domain test data.", "labels": [], "entities": [{"text": "F 1 -scores w.r.t. training set  size", "start_pos": 30, "end_pos": 67, "type": "METRIC", "confidence": 0.8437581211328506}]}]}