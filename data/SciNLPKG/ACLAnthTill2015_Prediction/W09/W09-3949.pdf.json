{"title": [{"text": "Simultaneous dialogue act segmentation and labelling using lexical and syntactic features", "labels": [], "entities": []}], "abstractContent": [{"text": "Segmentation of utterances and annotation as dialogue acts can be helpful for several modules of dialogue systems.", "labels": [], "entities": []}, {"text": "In this work, we study a statistical machine learning model to perform these tasks simultaneously using lexical features and incorporating deterministic syntactic restrictions.", "labels": [], "entities": []}, {"text": "There is a slight improvement in both seg-mentation and labelling due to these restrictions .", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialogue acts (DA) are linguistic abstractions that are commonly accepted and employed by the the dialogue community.", "labels": [], "entities": [{"text": "Dialogue acts (DA)", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6227999687194824}]}, {"text": "In the framework of dialogue systems, they can be helpful to identify and model user intentions and system answers by the dialogue manager.", "labels": [], "entities": []}, {"text": "Furthermore, in other dialogue modules such as the automatic speech recognizer or speech synthesiser, DA information maybe also used to increase their performance.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7011196613311768}]}, {"text": "Many researchers have studied automatic DA labelling using different techniques.", "labels": [], "entities": [{"text": "DA labelling", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9352777898311615}]}, {"text": "However, inmost of this work it is common to assume that the dialogue turns are already segmented into separate utterances, where each utterance corresponds to just one DA label, as in;;).", "labels": [], "entities": []}, {"text": "This is not a realistic situation because the segmentation of turns into utterances is not a trivial problem.", "labels": [], "entities": [{"text": "segmentation of turns into utterances", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.8913438439369201}]}, {"text": "There have been many previous approaches to segmentation of turns prior to DA labelling, beginning with).", "labels": [], "entities": [{"text": "segmentation of turns", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.8842638532320658}, {"text": "DA labelling", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.8557245135307312}]}, {"text": "Typically some combination of words and part of speech (POS) tags is used to predict segmentation boundaries.", "labels": [], "entities": []}, {"text": "In this work we make use of a statistical model to solve both the DA labelling task and the segmentation task simultaneously, following (;).", "labels": [], "entities": [{"text": "DA labelling task", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8818671107292175}, {"text": "segmentation task", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.87689608335495}]}, {"text": "Our aim is to see whether going beyond the word n-gram models can improve accuracy, using syntactic information (constituent structure) obtained from the dialogue transcriptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9984309077262878}]}, {"text": "We examine whether this information can improve the segmentation of the dialogue turns into DA segments.", "labels": [], "entities": []}, {"text": "Intuitively, it seems logical to believe that most of these segments must coincide with particular syntactic structures, and that segment boundaries would respect constituent boundaries.", "labels": [], "entities": []}], "datasetContent": [{"text": "Ten cross-validation experiments were performed for each model using, in each experiment a training partition composed of 1136 dialogues and a test set of 19 dialogues, as in).", "labels": [], "entities": []}, {"text": "The N-grams were obtained using the SLM toolkit (Rosenfeld (1998)) with GoodTuring discounting and the HMMs were trained using the Baum-Welch algorithm.", "labels": [], "entities": []}, {"text": "We use the following evaluation measures: \u2022 To evaluate the labelling, we use the DA Error Rate (equivalent to Word Error Rate) and the percentage of error labelling of whole turns.", "labels": [], "entities": [{"text": "DA Error Rate", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.9744288325309753}, {"text": "Word Error Rate)", "start_pos": 111, "end_pos": 127, "type": "METRIC", "confidence": 0.8542229384183884}]}, {"text": "\u2022 For the segment evaluation, we only check where the segments bounds are produced (word position in the segment), making use of F-score obtained from precision and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9984766840934753}, {"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9992247819900513}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9961796998977661}]}, {"text": "The results from using different sizes for the set X are shown for labelling performance in, and F-score of the segmentation in", "labels": [], "entities": [{"text": "F-score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9958147406578064}]}], "tableCaptions": [{"text": " Table 1: Occurrences and percentage of the syn- tactic categories that correspond with the most fre- quent MGSE of the last segment word (except last  segment) and MGSS of the first segment word (ex- cept first segment).", "labels": [], "entities": [{"text": "Occurrences", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9912014007568359}, {"text": "fre- quent MGSE", "start_pos": 97, "end_pos": 112, "type": "METRIC", "confidence": 0.7922711670398712}, {"text": "MGSS", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.5876814126968384}]}, {"text": " Table 2: DAER for models using MGSE, MGSS  and both features. SizeX indicates the size of the  set of most frequent categories accepted. Without  syntactic categories (baseline) we obtain a DAER  of 54.41.", "labels": [], "entities": [{"text": "DAER", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9847437739372253}, {"text": "MGSE", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.6985145807266235}, {"text": "MGSS", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8132338523864746}, {"text": "DAER", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.9979398846626282}]}, {"text": " Table 3: Percentage of error of labelling of com- plete turns for all the possible models. The base- line value is 55.41.", "labels": [], "entities": [{"text": "Percentage of error", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7839112679163615}]}, {"text": " Table 4: F-score of segmentation. The baseline  value is 71.17.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9983246922492981}, {"text": "segmentation", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9833420515060425}]}]}