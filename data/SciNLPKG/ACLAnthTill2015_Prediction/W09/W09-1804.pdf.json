{"title": [{"text": "A New Objective Function for Word Alignment", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7402687668800354}]}], "abstractContent": [{"text": "We develop anew objective function for word alignment that measures the size of the bilingual dictionary induced by an alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7791839838027954}]}, {"text": "A word alignment that results in a small dictionary is preferred over one that results in a large dictionary.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 2, "end_pos": 16, "type": "TASK", "confidence": 0.6904264837503433}]}, {"text": "In order to search for the alignment that minimizes this objective, we cast the problem as an integer linear program.", "labels": [], "entities": []}, {"text": "We then extend our objective function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.703393965959549}]}, {"text": "shows sample input fora word aligner.", "labels": [], "entities": []}, {"text": "After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair.", "labels": [], "entities": []}, {"text": "Word alignment has several downstream consumers.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6792614012956619}]}, {"text": "One is machine translation, where programs extract translation rules from word-aligned corpora ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8336601555347443}]}, {"text": "Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR () or re-score candidate translation outputs ( ).", "labels": [], "entities": []}, {"text": "Many methods of automatic alignment have been proposed.", "labels": [], "entities": [{"text": "automatic alignment", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6934455931186676}]}, {"text": "EM algorithms estimate dictionary and other probabilities in order to maximize those quantities.", "labels": [], "entities": []}, {"text": "One can then ask for Viterbi alignments that maximize P(alignment | e, f).", "labels": [], "entities": []}, {"text": "Discriminative models, e.g. (), instead set parameters to maximize alignment accuracy against a hand-aligned development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.93445885181427}]}, {"text": "EMD training) combines generative and discriminative elements.", "labels": [], "entities": []}, {"text": "Low accuracy is a weakness for all systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994304776191711}]}, {"text": "Most practitioners still use 1990s algorithms to align their data.", "labels": [], "entities": []}, {"text": "It stands to reason that we have not yet seen the last word in alignment models.", "labels": [], "entities": [{"text": "alignment", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.9639232754707336}]}, {"text": "In this paper, we develop anew objective function for alignment, inspired by watching people manually solve the alignment exercise of.", "labels": [], "entities": [{"text": "alignment", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.9867089986801147}]}, {"text": "When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible.", "labels": [], "entities": []}, {"text": "Previous machine aligners emulate this to some degree, but they are not explicitly programmed to do so.", "labels": [], "entities": []}, {"text": "We also address another weakness of current aligners: they only align full words.", "labels": [], "entities": []}, {"text": "With few exceptions, e.g. (), aligners do not operate at the sub-word level, making them much less useful for agglutinative languages such as Turkish.", "labels": [], "entities": []}, {"text": "Our present contributions are as follows: \u2022 We offer a simple new objective function that scores a corpus alignment based on how many distinct bilingual word pairs it contains.", "labels": [], "entities": []}, {"text": "\u2022 We use an integer programming solver to carryout optimization and corpus alignment.", "labels": [], "entities": [{"text": "corpus alignment", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7652802169322968}]}, {"text": "\u2022 We extend the system to perform subword alignment, which we demonstrate on a Turkish-English corpus.", "labels": [], "entities": [{"text": "subword alignment", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8317198157310486}]}, {"text": "The results in this paper constitute a proof of concept of these ideas, executed on small corpora.", "labels": [], "entities": []}, {"text": "We conclude by listing future directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}