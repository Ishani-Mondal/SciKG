{"title": [{"text": "Tightly Packed Tries: How to Fit Large Models into Memory, and Make them Load Fast, Too", "labels": [], "entities": []}], "abstractContent": [{"text": "We present Tightly Packed Tries (TPTs), a compact implementation of read-only, compressed trie structures with fast on-demand paging and short load times.", "labels": [], "entities": []}, {"text": "We demonstrate the benefits of TPTs for storing n-gram back-off language models and phrase tables for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 102, "end_pos": 133, "type": "TASK", "confidence": 0.7464727958043417}]}, {"text": "Encoded as TPTs, these databases require less space than flat text file representations of the same data compressed with the gzip utility.", "labels": [], "entities": []}, {"text": "At the same time, they can be mapped into memory quickly and be searched directly in time linear in the length of the key, without the need to decompress the entire file.", "labels": [], "entities": []}, {"text": "The overhead for local decompression during search is marginal.", "labels": [], "entities": []}], "introductionContent": [{"text": "The amount of data available for data-driven Natural Language Processing (NLP) continues to grow.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.7371010780334473}]}, {"text": "For some languages, language models (LM) are now being trained on many billions of words, and parallel corpora available for building statistical machine translation (SMT) systems can run into tens of millions of sentence pairs.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 134, "end_pos": 171, "type": "TASK", "confidence": 0.7980754474798838}]}, {"text": "This wealth of data allows the construction of bigger, more comprehensive models, often without changes to the fundamental model design, for example by simply increasing the n-gram size in language modeling or the phrase length in phrase tables for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 249, "end_pos": 252, "type": "TASK", "confidence": 0.9900814890861511}]}, {"text": "The large sizes of the resulting models pose an engineering challenge.", "labels": [], "entities": []}, {"text": "They are often too large to fit entirely in main memory.", "labels": [], "entities": []}, {"text": "What is the best way to organize these models so that we can swap information in and out of memory as needed, and as quickly as possible?", "labels": [], "entities": []}, {"text": "This paper presents Tightly Packed Tries (TPTs), a compact and fast-loading implementation of readonly trie structures for NLP databases that store information associated with token sequences, such as language models, n-gram count databases, and phrase tables for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 264, "end_pos": 267, "type": "TASK", "confidence": 0.9896911978721619}]}, {"text": "In the following section, we first recapitulate some basic data structures and encoding techniques that are the foundations of TPTs.", "labels": [], "entities": []}, {"text": "We then layout the organization of TPTs.", "labels": [], "entities": []}, {"text": "Section 3 discusses compression of node values (i.e., the information associated with each key).", "labels": [], "entities": []}, {"text": "Related work is discussed in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we report empirical results from run-time tests of TPTs in comparison to other implementations.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present here the results of empirical evaluations of the effectiveness of TPTs for encoding ngram language models and phrase tables for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9906182885169983}]}, {"text": "We have also used TPTs to encode n-gram count databases such as the Google 1T web n-gram database (), but are notable to provide detailed results within the space limitations of this paper.", "labels": [], "entities": [{"text": "Google 1T web n-gram database", "start_pos": 68, "end_pos": 97, "type": "DATASET", "confidence": 0.8660846471786499}]}], "tableCaptions": [{"text": " Table 1: Memory use and runtimes of different LM implementations on a perplexity computation task.", "labels": [], "entities": []}, {"text": " Table 2: Language model statistics.", "labels": [], "entities": []}, {"text": " Table 3: Model load times and translation speed for batch translation with the Portage SMT system.", "labels": [], "entities": [{"text": "batch translation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.6864131987094879}, {"text": "Portage SMT", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.6121418476104736}]}]}