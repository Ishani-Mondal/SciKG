{"title": [{"text": "A Hybrid Approach for Building Arabic Diacritizer", "labels": [], "entities": []}], "abstractContent": [{"text": "Modern standard Arabic is usually written without diacritics.", "labels": [], "entities": []}, {"text": "This makes it difficult for performing Arabic text processing.", "labels": [], "entities": [{"text": "Arabic text processing", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.5649079581101736}]}, {"text": "Diacritiza-tion helps clarify the meaning of words and disambiguate any vague spellings or pronunciations , as some Arabic words are spelled the same but differ in meaning.", "labels": [], "entities": []}, {"text": "In this paper, we address the issue of adding diacritics to undia-critized Arabic text using a hybrid approach.", "labels": [], "entities": []}, {"text": "The approach requires an Arabic lexicon and large corpus of fully diacritized text for training purposes in order to detect diacritics.", "labels": [], "entities": []}, {"text": "Case-Ending is treated as a separate post processing task using syntactic information.", "labels": [], "entities": []}, {"text": "The hybrid approach relies on lexicon retrieval, bigram, and SVM-statistical prioritized techniques.", "labels": [], "entities": []}, {"text": "We present results of an evaluation of the proposed diacritization approach and discuss various modifications for improving the performance of this approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern Arabic written texts usually include Arabic scripts without short vowels and other diacritic marks.", "labels": [], "entities": []}, {"text": "This often leads to considerable ambiguity since several words that have different diacritic patterns may appear identical in a diacritic-less setting.", "labels": [], "entities": []}, {"text": "Educated modern Arabic speakers are able to accurately derive/restore diacritics in a document.", "labels": [], "entities": []}, {"text": "This is based on the context and their linguistic knowledge of Arabic.", "labels": [], "entities": []}, {"text": "However, a text without diacritics brings difficulties for Arabic readers.", "labels": [], "entities": []}, {"text": "It is also problematic for Arabic processing applications, such as textto-speech, speech-to-text, and text analysis, where the lack of diacritics adds another layer of ambiguity when processing the input data.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.7755131423473358}]}, {"text": "As an example, full vocalization of Arabic text is required for text-to-speech applications, where the mapping from graphemes to phonemes is complicated compared to languages such as English and French; where there is, inmost cases, simple one-to-one relationship.", "labels": [], "entities": []}, {"text": "Nevertheless, using Arabic text with diacritics has proven an improvement in the accuracy of speech-recognition applications ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9972209930419922}]}, {"text": "The problem of automatic restoration (i.e., derivation) of the diacritic signs of Arabic text can be solved by two approaches.", "labels": [], "entities": []}, {"text": "The first is a rulebased approach that involves a complex integration of the Arabic morphological, syntactic, and semantic tools with significant efforts to acquire respective linguistic rules.", "labels": [], "entities": []}, {"text": "A morphological analyzer gets the breakdowns of the undiacritized word according to known patterns or templates and recognizes its prefixes and suffixes.", "labels": [], "entities": []}, {"text": "A syntax analyzer applies specific syntactic rules to determine the case-ending diacritics, usually, by techniques such as finite-state automata.", "labels": [], "entities": []}, {"text": "Semantics handling helps to resolve ambiguous cases and to filter out hypothesis.", "labels": [], "entities": [{"text": "Semantics handling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9101552963256836}]}, {"text": "Hence, rule-based diacritization approach is a complicated process and takes longer time to process an Arabic sentence which is naturally long.", "labels": [], "entities": []}, {"text": "The second approach is the statistical approach that requires linguistic resources such as a large tagged corpus (in particular a TreeBank) to extract language statistics for estimating the missing diacritical marks.", "labels": [], "entities": []}, {"text": "The approach is fully automated and does not require efforts to acquire respective linguistic knowledge.", "labels": [], "entities": []}, {"text": "Results are usually improved by increasing the size of the corpus.", "labels": [], "entities": []}, {"text": "It is worth noting that identifying some of the diacritic marks can be seen as a morphological problem and the relevant letters are called internal characters in this paper.", "labels": [], "entities": []}, {"text": "Moreover, diacritic mark of the last character of the Arabic is called case ending (\u202b\u0627\ufefb\ufecb\ufeae\u0627\u0628\u202c \u202b.)\ufecb\ufefc\ufee3\ufe94\u202c The identification of case-ending diacritics is determined at the syn-tactic processing level (case ending depends on the position of the word within the sentence) whereas detecting the internal diacritics is determined at the morphological processing level.", "labels": [], "entities": []}, {"text": "In widespread cases, the case-ending come internally rather than with the last character such as \"\u202b\u0650\ufeec\ufe8e\u202c \u202b\u064e\ufee0\ufee4\u202c \u202b\u0650\ufed8\u202c \u202b\"\ufe91\u202c (by-her-pen).", "labels": [], "entities": []}, {"text": "In this paper, an Arabic diacritizer is proposed.", "labels": [], "entities": []}, {"text": "Internal diacritization was restored by a model based on the synergy of three different techniques: retrieval of unambiguous lexicon entries, retrieval of two-word expression from a preprocessed diacritized bigram database, and a prediction using statistical approach based on SVM-learning technique,) and.", "labels": [], "entities": []}, {"text": "The later technique tokenizes a text and provides a Reduced Tag Set (RTS) of Part of Speech (POS) 1 for each token.", "labels": [], "entities": [{"text": "Reduced Tag Set (RTS) of Part of Speech (POS) 1", "start_pos": 52, "end_pos": 99, "type": "METRIC", "confidence": 0.7595752711806979}]}, {"text": "The tags are used to restore the diacritics.", "labels": [], "entities": []}, {"text": "From the obtained diacritization results of these techniques, the most consistent one is selected.", "labels": [], "entities": []}, {"text": "The Case-Ending diacritization is treated as a post-process of the internal diacritization task using the same machine learning approach that was trained on Base phrase (BP)-Chunk as well as POS features of individual tokens with correct case-ending tags.", "labels": [], "entities": []}, {"text": "A utility has been designed to extract correct case-ending tags from the LDC's Arabic Tree Bank (ATB).", "labels": [], "entities": [{"text": "LDC's Arabic Tree Bank (ATB)", "start_pos": 73, "end_pos": 101, "type": "DATASET", "confidence": 0.9408636093139648}]}, {"text": "This paper presents anew simple but efficient approach that gets results comparable with the best performing systems, to our knowledge,.", "labels": [], "entities": []}, {"text": "The achieved results are: 11.795% Word Error Rate (WER) and about 3.245% Diacritics Error Rate (DER).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.9452237586180369}, {"text": "Diacritics Error Rate (DER)", "start_pos": 73, "end_pos": 100, "type": "METRIC", "confidence": 0.8913306693236033}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews closely related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the proposed diacritization approach.", "labels": [], "entities": []}, {"text": "Section 4 describes the training process.", "labels": [], "entities": []}, {"text": "Section 5 presents the evaluation experiment.", "labels": [], "entities": []}, {"text": "Section 6 concludes the article and gives direction for future research.", "labels": [], "entities": []}, {"text": "based approach that uses morphological analyzer for vowelization was proposed.", "labels": [], "entities": []}, {"text": "Another, rulebased grapheme to sound conversion approach appeared in 2003 by Y..", "labels": [], "entities": [{"text": "rulebased grapheme to sound conversion", "start_pos": 9, "end_pos": 47, "type": "TASK", "confidence": 0.6837806940078736}]}, {"text": "There are many related works dealing with the problem of Arabic diacritization in general),,,). and (); all trying to handle this problem using statistical approaches but they tend to handle the case ending diacritic mark in the same way they used to handle the internal (any letter but the last) diacritics.", "labels": [], "entities": []}, {"text": "In our proposed approach we differentiate between them as the detection of case-ending diacritics is a syntacticbased problem whereas detecting the internal diacritics is a morphological-based problem.", "labels": [], "entities": []}, {"text": "introduced a system called MADA-D that uses Buckwalter's Arabic morphological analyzer where they used 14 taggers and a lexeme-based language model.", "labels": [], "entities": []}, {"text": "MADA is so far the best performing system to date.", "labels": [], "entities": [{"text": "MADA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7130074501037598}]}, {"text": "It has been reported that it achieved a WER of 14.9% and a DER of 4.8%.", "labels": [], "entities": [{"text": "WER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9951790571212769}, {"text": "DER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9963569045066833}]}], "datasetContent": [{"text": "For Arabic tokenizer, POS tagger, BP-chunk, and statistical Case-Ending, we used a standard SVM with a polynomial kernel of degree 2 and C=1.0.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 22, "end_pos": 32, "type": "TASK", "confidence": 0.7240143865346909}]}, {"text": "Evaluation of the system was done by calculating the performance using the standard evaluation measures: accuracy, precision, recall, and the f-measure .We used YamCha () implementation of SVMs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9996080994606018}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9985665678977966}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9990584254264832}]}, {"text": "Diacritization evaluation of our experiments is reported in terms of word error rate (WER), and diacritization error rate (DER) . We conducted experiments to: 1.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 69, "end_pos": 90, "type": "METRIC", "confidence": 0.9429884453614553}, {"text": "diacritization error rate (DER)", "start_pos": 96, "end_pos": 127, "type": "METRIC", "confidence": 0.9430420498053232}]}, {"text": "Evaluate the impact of tokenization, part-ofspeech, chunking, and case-ending parameters on the training models, see Section 5.1. 2. Evaluate the impact of including and excluding the case-ending on the performance of the Arabic diacritizer, see Section 5.2. 3. Compare our approach of Tokenization and POS tagger with the ArabicSVMTools tagger using different parameters and feature(s), see Section 5.2.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 303, "end_pos": 313, "type": "TASK", "confidence": 0.696874812245369}]}, {"text": "Two tokenization tasks was performed on window sizes of -2 /+2 and -4/+4, for illustration see TOK1 and TOK2 tasks in.", "labels": [], "entities": [{"text": "TOK1", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.9057763814926147}]}, {"text": "For each window size there are two columns.", "labels": [], "entities": []}, {"text": "The first one contains a sequence of Buckwalter's transliterated Arabic letters shown from top to bottom that resembles the left-to-right Arabic writing system (e.g., \u2026.wyblg Eddhm \u2026.. are the transliteration of the Arabic words , respectively).", "labels": [], "entities": [{"text": "Buckwalter's transliterated Arabic letters", "start_pos": 37, "end_pos": 79, "type": "DATASET", "confidence": 0.9186869263648987}]}, {"text": "The second column contains the corresponding tokenization tags presented by Inside-Outside-Beginning (I-O-B) of a chunk, i.e., prefix (PRE), word (WRD), and suffix (SUFF), respectively, (.", "labels": [], "entities": []}, {"text": "The tokenization tags are: B-PRE1, I-PRE1, B-PRE2, I-PRE2, B-PRE3, I-PRE3, B-WORD-1, I-WORD-1, B-SUFF1, I-SUFF1 and O for outside word boundary.", "labels": [], "entities": [{"text": "O", "start_pos": 116, "end_pos": 117, "type": "METRIC", "confidence": 0.9902188181877136}]}, {"text": "We made segmentation for the determiner \"Al\" -\"\u202b.\"\u0627\u0644\u202c This segmentation is important for the case-ending detection for: the adjective and the noun it modifies \"\u202b\u0648\u0627\ufedf\ufee4\ufeee\ufebb\ufeee\u0641\u202c \u202b,\"\u0627\ufedf\ufebc\ufed4\ufe94\u202c 1 stand 2 nd Particle of the construction Annexed and Annexed noun \"\u202b\u0625\ufedf\ufef4\ufeea\u202c \u202b\u0627\ufedf\ufee4\ufec0\ufe8e\u0641\u202c \u202b\u0648\u202c \u202b,\"\u0627\ufedf\ufee4\ufec0\ufe8e\u0641\u202c and Nunation \u202b\u0627\ufedf\ufe98\ufee8\ufeee\ufef3\ufee6\u202c \" \".", "labels": [], "entities": [{"text": "case-ending detection", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7454604506492615}]}, {"text": "The result of the evaluation of the two tokenization tasks is shown in.: Tokenization results with window sizes of -2/+2 and -4/+4 Evaluation of the impact of the part-of-speech parameter on the training process A POS tagging (POS1) task was performed on a sequence of tokens produced from the tokenization task.", "labels": [], "entities": [{"text": "POS tagging (POS1) task", "start_pos": 214, "end_pos": 237, "type": "TASK", "confidence": 0.8192976117134094}]}, {"text": "A window size of +2/ -2 tokens centered at the focus token.", "labels": [], "entities": []}, {"text": "We made another POS tagging (POS2) task by adding the last two characters as an extra feature for enhancing the accuracy of some tags such as plural or dual noun (NNS) and singular noun (NN).", "labels": [], "entities": [{"text": "POS tagging (POS2)", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8165688991546631}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9989762306213379}]}, {"text": "For illustration see POS1 and POS2 tasks in.", "labels": [], "entities": []}, {"text": "The result of the evaluation of the two POS tagging tasks is shown in.", "labels": [], "entities": [{"text": "POS tagging tasks", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8556907971700033}]}, {"text": "The chunking task was performed on tokens produced from the tokenization and POS tasks.", "labels": [], "entities": [{"text": "chunking task", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8971185386180878}]}, {"text": "The evaluation included 16 tag-set (features) of a window size of -2/+2 for both tokens and POS, and only the previous two chunk tags.", "labels": [], "entities": []}, {"text": "The result of the evaluation of is shown in.: Results for BP-chunk  Two case-ending tasks were performed.", "labels": [], "entities": []}, {"text": "The first case-ending (CE1) task was discussed in a previous work.", "labels": [], "entities": [{"text": "case-ending (CE1) task", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.736969393491745}]}, {"text": "It was performed on window size of -3/+3 and 8 tag sets.", "labels": [], "entities": []}, {"text": "The evaluation has achieved 95.35% inaccuracy.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9426642656326294}]}, {"text": "We noticed that in some cases the system can produce unacceptable case ending (e.g., Tanween on the sound plural masculine \" \u202b\ufe9f\ufee4\ufeca\u202c \u202b\u0627\ufedf\ufeb4\ufe8e\ufedf\ufee2\u202c \u202b)\"\u0627\ufedf\ufee4\ufeac\u0622\ufeae\u202c that we could improved by: 1-Enhancing the POS tagging (POS2) task by adding last two characters (L2Ch) as a feature.", "labels": [], "entities": [{"text": "POS tagging (POS2) task", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.8375065128008524}]}, {"text": "2-Enhancing the case ending (CE2) task by adding the last character (LCh) and the last two characters (L2Ch) as features.", "labels": [], "entities": [{"text": "case ending (CE2) task", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.8054606914520264}]}, {"text": "The following modifications were done to conduct the second case-ending (CE2) task, for illustration see: \u2022 Adding the last two characters (L2Ch) and the last character (LCh) as features.", "labels": [], "entities": []}, {"text": "\u2022 Enhancing the case ending representation by adding an extra tagset for \"indeclension of the fatha\" -\"\u202b\u0627\ufedf\ufed4\ufe98\ufea2\u202c \u202b\ufecb\ufee0\ufef0\u202c \u202b\"\ufee3\ufe92\ufee8\ufef2\u202c that is presented in Treebank as \"PVSUFF_SUNJ:3MS\".", "labels": [], "entities": [{"text": "case ending representation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.8499712546666464}]}, {"text": "presents the results obtained for the two case ending (CE1 and CE2) tasks.", "labels": [], "entities": []}, {"text": "As shown, the performance is improved.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Tokenization results with window sizes of  -2/+2 and -4/+4", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9314501881599426}]}, {"text": " Table 5: POS results for different window sizes", "labels": [], "entities": [{"text": "POS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.68364018201828}]}, {"text": " Table 6: Results for BP-chunk", "labels": [], "entities": [{"text": "BP-chunk", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.7939391136169434}]}, {"text": " Table 8 through Table 10.", "labels": [], "entities": []}, {"text": " Table 8: WER and DER for Lexicon Retrieval and  Statistical SVM techniques for including and exclud- ing case ending", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9968340992927551}, {"text": "DER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9972689747810364}, {"text": "Lexicon Retrieval", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7799802124500275}]}, {"text": " Table 9: WER and DER for different combination of  diacritization techniques", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9972537159919739}, {"text": "DER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9966392517089844}]}, {"text": " Table 10.  ArabicSVMTools gave better results than our  proposed tagger. However, our proposed tagger  is about 4 times faster than ArabicSVMTools  because we use less features.", "labels": [], "entities": [{"text": "ArabicSVMTools", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.8782619833946228}]}, {"text": " Table 11: WER and DER for different techniques", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9976487755775452}, {"text": "DER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9935804605484009}]}]}