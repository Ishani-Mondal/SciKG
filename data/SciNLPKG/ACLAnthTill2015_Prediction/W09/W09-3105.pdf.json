{"title": [{"text": "Active Learning of Extractive Reference Summaries for Lecture Speech Summarization", "labels": [], "entities": [{"text": "Active Learning of Extractive Reference Summaries", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.6107293466726939}, {"text": "Lecture Speech Summarization", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.6793236633141836}]}], "abstractContent": [{"text": "We propose using active learning for tagging extractive reference summary of lecture speech.", "labels": [], "entities": [{"text": "tagging extractive reference summary of lecture speech", "start_pos": 37, "end_pos": 91, "type": "TASK", "confidence": 0.8819796953882489}]}, {"text": "The training process of feature-based summarization model usually requires a large amount of training data with high-quality reference summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8623359799385071}]}, {"text": "Human production of such summaries is tedious, and since inter-labeler agreement is low, very unreliable.", "labels": [], "entities": []}, {"text": "Active learning helps assuage this problem by automatically selecting a small amount of unlabeled documents for humans to hand correct.", "labels": [], "entities": []}, {"text": "Our method chooses the unla-beled documents according to the similarity score between the document and the comparable resource-PowerPoint slides.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 61, "end_pos": 77, "type": "METRIC", "confidence": 0.957438200712204}]}, {"text": "After manual correction, the selected documents are returned to the training pool.", "labels": [], "entities": []}, {"text": "Summarization results show an increasing learning curve of ROUGE-L F-measure, from 0.44 to 0.514, consistently higher than that of using randomly chosen training samples.", "labels": [], "entities": [{"text": "ROUGE-L F-measure", "start_pos": 59, "end_pos": 76, "type": "METRIC", "confidence": 0.7818966209888458}]}], "introductionContent": [{"text": "The need for the summarization of classroom lectures, conference speeches, political speeches is ever increasing with the advent of remote learning, distributed collaboration and electronic archiving.", "labels": [], "entities": [{"text": "summarization of classroom lectures", "start_pos": 17, "end_pos": 52, "type": "TASK", "confidence": 0.8765405863523483}]}, {"text": "These user needs cannot be sufficiently met by short abstracts.", "labels": [], "entities": []}, {"text": "In recent years, virtually all summarization systems are extractive -compiling bullet points from the document using some saliency criteria.", "labels": [], "entities": [{"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9767816066741943}]}, {"text": "Reference summaries are often manually compiled by one or multiple human annotators (.", "labels": [], "entities": [{"text": "Reference summaries", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7924050092697144}]}, {"text": "Unlike for speech recognition where the reference sentence is clear and unambiguous, and unlike for machine translation where there are guidelines for manual translating reference sentences, there is no clear guideline for compiling a good reference summary.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7670900821685791}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7634941637516022}]}, {"text": "As a result, one of the most important challenges in speech summarization remains the difficulty to compile, evaluate and thus to learn what a good summary is.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.6487614065408707}]}, {"text": "Human judges tend to agree on obviously good and very bad summaries but cannot agree on borderline cases.", "labels": [], "entities": []}, {"text": "Consequently, annotator agreement is low.", "labels": [], "entities": [{"text": "annotator agreement", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.7788474559783936}]}, {"text": "Reference summary generation is a tedious and low efficiency task.", "labels": [], "entities": [{"text": "Reference summary generation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8987408479054769}]}, {"text": "On the other hand, supervised learning of extractive summarization requires a large amount of training data of reference summaries.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.6175127923488617}]}, {"text": "To reduce the amount of human annotation effort and improve annotator agreement on the reference summaries, we propose that active learning (selective sampling) is one possible solution.", "labels": [], "entities": []}, {"text": "Active learning has been applied to NLP tasks such as spoken language understanding (), information extraction (), and text classification ().", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.63860884308815}, {"text": "information extraction", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.8078804314136505}, {"text": "text classification", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.8119538128376007}]}, {"text": "Different from supervised learning which needs the entire corpus with manual labeling result, active learning selects the most useful examples for labeling and requires manual labeling of training dataset to re-train model.", "labels": [], "entities": []}, {"text": "In this paper, we suggest a framework of reference summary annotation with relatively high inter labeler agreement based on the rhetorical structure in presentation slides.", "labels": [], "entities": []}, {"text": "Based on this framework, we further propose a certainty-based active learning method to alleviate the burden of human annotation of training data.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 depicts the corpus for our experiments, the extractive summarizer, and outlines the acoustic/prosodic, and linguistic feature sets for representing each sentence.", "labels": [], "entities": []}, {"text": "Section 3 depicts how to compile reference summaries with high inter labeler agreement by using the RDTW algorithm and our active learning algorithm for tagging extractive reference summary.", "labels": [], "entities": [{"text": "tagging extractive reference summary", "start_pos": 153, "end_pos": 189, "type": "TASK", "confidence": 0.8790691047906876}]}, {"text": "We describe our experiments and evaluate the results in Section 4.", "labels": [], "entities": []}, {"text": "Our conclusion follows in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our lecture speech corpus ( contains 111 presentations recorded from the NCMMSC2005 and NCMMSC2007 conferences for evaluating our approach.", "labels": [], "entities": [{"text": "NCMMSC2005", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.9772751331329346}, {"text": "NCMMSC2007", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.691099226474762}]}, {"text": "The manual transcriptions and the comparable corpusPowerPoint slides are also collected.", "labels": [], "entities": [{"text": "corpusPowerPoint slides", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.9534308612346649}]}, {"text": "Each presentation lasts for 15 minutes on average.", "labels": [], "entities": []}, {"text": "We select 71 of the 111 presentations with well organized PowerPoint slides that always have clear sketches and evidently aligned with the transcriptions.", "labels": [], "entities": []}, {"text": "We use about 90% of the lecture corpus from the 65 presentations as original unlabeled data U and the remaining 6 presentations as held-out test set.", "labels": [], "entities": []}, {"text": "We randomly select 5 presentations from U as our seed presentations.", "labels": [], "entities": []}, {"text": "Reference summaries of the seed presentations and the presentations of test set are generated from the PowerPoint slides and presentation transcriptions using RDTW followed by manual correction, as described in Section 3.", "labels": [], "entities": [{"text": "RDTW", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.7324041128158569}]}, {"text": "We gradually increase the training data pool by choosing five more documents each time for manual correction.", "labels": [], "entities": [{"text": "manual correction", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.6757684648036957}]}, {"text": "We carryout two sets of experiments for comparing our algorithm and random selection.", "labels": [], "entities": []}, {"text": "We evaluate the summarizer by ROUGE-L (summary-level Longest Common Subsequence) F-measure.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9972483515739441}, {"text": "summary-level Longest Common Subsequence) F-measure", "start_pos": 39, "end_pos": 90, "type": "METRIC", "confidence": 0.5751559535662333}]}, {"text": "The performance of our algorithm is illustrated by the increasing ROUGE-L F-measure curve in.", "labels": [], "entities": [{"text": "ROUGE-L F-measure curve", "start_pos": 66, "end_pos": 89, "type": "METRIC", "confidence": 0.8897211949030558}]}, {"text": "It is shown to be consistently higher than: Active learning vs. random selection using randomly chosen samples.", "labels": [], "entities": []}, {"text": "We also find that by using only 51 documents for training, the performance of the summarization model achieved by our approach is better than that of the model trained by random selection using all 65 presentations (0.514 vs. 0.512 ROUGE-L F-measure).", "labels": [], "entities": [{"text": "ROUGE-L F-measure", "start_pos": 232, "end_pos": 249, "type": "METRIC", "confidence": 0.8445491790771484}]}, {"text": "This shows that our active learning approach requires 22% less training data.", "labels": [], "entities": []}, {"text": "Besides, acoustic features can improve the performance of active learning of speech summarization.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.6079244464635849}]}, {"text": "Without acoustic features, our summarizer only performs 0.47 ROUGE-L F-measure.", "labels": [], "entities": [{"text": "ROUGE-L F-measure", "start_pos": 61, "end_pos": 78, "type": "METRIC", "confidence": 0.8822672665119171}]}], "tableCaptions": []}