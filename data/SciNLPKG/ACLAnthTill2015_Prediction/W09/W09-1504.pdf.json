{"title": [{"text": "Integrated NLP Evaluation System for Pluggable Evaluation Metrics with Extensive Interoperable Toolkit", "labels": [], "entities": []}], "abstractContent": [{"text": "To understand the key characteristics of NLP tools, evaluation and comparison against different tools is important.", "labels": [], "entities": []}, {"text": "And as NLP applications tend to consist of multiple semi-independent sub-components, it is not always enough to just evaluate complete systems, a fine grained evaluation of underlying components is also often worthwhile.", "labels": [], "entities": []}, {"text": "Standardization of NLP components and resources is not only significant for reusability, but also in that it allows the comparison of individual components in terms of reliability and robustness in a wider range of target domains.", "labels": [], "entities": []}, {"text": "But as many evaluation metrics exist in even a single domain, any system seeking to aid inter-domain evaluation needs not just predefined metrics, but must also support pluggable user-defined me-trics.", "labels": [], "entities": []}, {"text": "Such a system would of course need to be based on an open standard to allow a large number of components to be compared, and would ideally include visualization of the differences between components.", "labels": [], "entities": []}, {"text": "We have developed a pluggable evaluation system based on the UIMA framework, which provides vi-sualization useful in error analysis.", "labels": [], "entities": [{"text": "UIMA framework", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.9179385602474213}, {"text": "error analysis", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.6787762343883514}]}, {"text": "It is a single integrated system which includes a large ready-to-use, fully interoperable library of NLP tools.", "labels": [], "entities": []}], "introductionContent": [{"text": "When building NLP applications, the same subtasks tend to appear frequently while constructing different systems.", "labels": [], "entities": []}, {"text": "Due to this, the reusability of tools designed for such subtasks is a common design consideration; fine grained interoperability between sub components, not just between complete systems.", "labels": [], "entities": []}, {"text": "In addition to the benefits of reusability, interoperability is also important in evaluation of components.", "labels": [], "entities": []}, {"text": "Evaluations are normally done by comparing two sets of data, a gold standard data and test data showing the components performance.", "labels": [], "entities": []}, {"text": "Naturally this comparison requires the two data sets to be in the same data format with the same semantics.", "labels": [], "entities": []}, {"text": "Comparing of \"Apples to Apples\" provides another reason why standardization of NLP tools is beneficial.", "labels": [], "entities": []}, {"text": "Another advantage of standardization is that the number of gold standard data sets that can be compared against is also increased, allowing tools to be tested in a wider range of domains.", "labels": [], "entities": [{"text": "standardization", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.9662185311317444}]}, {"text": "The ideal is that all components are standardized to conform to an open, widely used interoperability framework.", "labels": [], "entities": []}, {"text": "One possible such framework is UIMA; Unstructured Information Management Architecture), which is an open project of OASIS and Apache.", "labels": [], "entities": []}, {"text": "We have been developing U-Compare ( , an integrated testing an evaluation platform based on this framework.", "labels": [], "entities": []}, {"text": "Although U-Compare already provided a wide range of tools and NLP resources, its inbuilt evaluation mechanisms were hard coded into the system and were not customizable by end users.", "labels": [], "entities": []}, {"text": "Furthermore the evaluation metrics used were based only on simple strict matchings which severely limited its domains of application.", "labels": [], "entities": []}, {"text": "We have extended the evaluation mechanism to allow users to define their own metrics which can be integrated into the range of existing evaluation tools.", "labels": [], "entities": []}, {"text": "The U-Compare library of interoperable tools has also been extended; especially with regard to resources related to biomedical named entity extraction.", "labels": [], "entities": [{"text": "biomedical named entity extraction", "start_pos": 116, "end_pos": 150, "type": "TASK", "confidence": 0.7189596742391586}]}, {"text": "U-Compare is currently providing the world largest library of type system compatible UIMA components.", "labels": [], "entities": [{"text": "U-Compare", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8755341172218323}]}, {"text": "In section 2 of this paper we first look at the underlying technologies, UIMA and U-Compare.", "labels": [], "entities": [{"text": "UIMA", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.7223449349403381}]}, {"text": "Then we describe the new pluggable evaluation mechanism in section 3 and our interoperable toolkit with our type system in section 4 and 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "While U-Compare already has a mechanism to automatically create possible combinations of components for comparison from a specified workflow, the comparison (evaluation) metric itself was hard coded into the system.", "labels": [], "entities": []}, {"text": "Only comparison based on simple strict matching was possible.", "labels": [], "entities": []}, {"text": "However, many different evaluation metrics exist, even for the same type of annotations.", "labels": [], "entities": []}, {"text": "For example, named entity recognition results are often evaluated based on several different annotation intersection criteria: exact match, left/right only match, overlap, etc.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6174428363641103}, {"text": "exact match", "start_pos": 127, "end_pos": 138, "type": "METRIC", "confidence": 0.9591242671012878}, {"text": "overlap", "start_pos": 163, "end_pos": 170, "type": "METRIC", "confidence": 0.9783929586410522}]}, {"text": "Evaluation metrics for nested components can be even more complex (e.g. biomedical relations, deep syntactic structures).", "labels": [], "entities": []}, {"text": "Sometimes new metrics are also required for specific tasks.", "labels": [], "entities": []}, {"text": "Thus, a mechanism for pluggable evaluation metrics in a standardized way is seen as desirable.", "labels": [], "entities": []}, {"text": "Our design goal for the evaluation systems is to do as much of the required work as possible and to provide utilities to reduce developer's labor.", "labels": [], "entities": []}, {"text": "We also want our design to be generic and fix within existing UIMA standards.", "labels": [], "entities": [{"text": "UIMA standards", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9109900295734406}]}, {"text": "The essential process of evaluation can be generalized and decomposed as follows: (a) prepare a pair of annotation sets which will be used for comparison, For example, in the case of the Penn Treebank style syntactic bracket matching, these steps correspond to (a) prepare two sets of constituents and tokens, (b) select only the constituents (removing null elements if required), (c) compare constituents between the sets and return any matches.", "labels": [], "entities": [{"text": "Penn Treebank style", "start_pos": 187, "end_pos": 206, "type": "DATASET", "confidence": 0.9505951404571533}, {"text": "syntactic bracket matching", "start_pos": 207, "end_pos": 233, "type": "TASK", "confidence": 0.7891497611999512}]}, {"text": "In our new design, step (a) is performed by the system, (b) and (c) are performed by an evaluation component.", "labels": [], "entities": []}, {"text": "The evaluation component is just a normal UIMA component, pluggable based on the UIMA standard.", "labels": [], "entities": [{"text": "UIMA standard", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9313921630382538}]}, {"text": "This component is run on a CAS which was constructed by the system during step (a).", "labels": [], "entities": []}, {"text": "This CAS includes an instance of ComparisonSet type and its features GoldAnnotationGroup and TestAnnotationGroup.", "labels": [], "entities": []}, {"text": "Corresponding to step (b), based on this input the comparison component should make a selection of annotations and store them as FSArray for both GoldAnnotations and TestAnnotations.", "labels": [], "entities": [{"text": "FSArray", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.6689478158950806}, {"text": "GoldAnnotations", "start_pos": 146, "end_pos": 161, "type": "DATASET", "confidence": 0.8647403120994568}]}, {"text": "Finally for step (c), the component should perform a matching and store the results as MatchedPair instances in the MatchedAnnotations feature of the ComparisonSet.", "labels": [], "entities": []}, {"text": "Precision, recall, and F1 scores are calculated by U-Compare based on the outputted ComparisonSet.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9948805570602417}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.99592125415802}, {"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9997567534446716}]}, {"text": "These calculation can be overridden and customized if the developer so desires.", "labels": [], "entities": []}, {"text": "Implementation of the compare() method of the evaluation component is recommended.", "labels": [], "entities": []}, {"text": "It is used by the system when showing instance based evaluations of what feature values are used in matching, which features are matched, and which are not.", "labels": [], "entities": []}, {"text": "By default, evaluation statistics are calculated by simply counting the numbers of gold, test, matched annotations in the returned ComparisonSet instance.", "labels": [], "entities": []}, {"text": "Then precision, recall, and F1 scores for each CAS and for the complete set of CASes are calculated.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9997121691703796}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9988131523132324}, {"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9998579025268555}]}, {"text": "Users can specify which evaluation metrics are used for each type of annotations based on the input specifications they set for supplied evaluation components.", "labels": [], "entities": []}, {"text": "Normally, precision, recall, and F1 scores are the only evaluation statistics used in the NLP community.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9996795654296875}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9988973140716553}, {"text": "F1 scores", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9791294932365417}]}, {"text": "It is often the casein many research reports that anew tool A performs better than another tool B, increasing the F1 score by 1%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.991432785987854}]}, {"text": "In such cases it is important to analysis what proportion of annotations are shared between A, B, and the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.8821694254875183}]}, {"text": "Is A a strict 1% increase over B?", "labels": [], "entities": [{"text": "A", "start_pos": 3, "end_pos": 4, "type": "METRIC", "confidence": 0.9854684472084045}]}, {"text": "Or does it cover 2% of instances B doesn't but miss a different 1%?", "labels": [], "entities": []}, {"text": "Our system provides these statistics as well.", "labels": [], "entities": []}, {"text": "Further, our standardized evaluation system makes more advanced evaluation available.", "labels": [], "entities": []}, {"text": "Since the evaluation metrics themselves are more or less arbitrary, we should carefully observe the results of evaluations.", "labels": [], "entities": []}, {"text": "When two or more metrics are available for the same type of annotations, we can compare the results of each to analyze and validate the individual evaluations.", "labels": [], "entities": []}, {"text": "An immediate application of such comparison would be in a voting system, which takes the results of several tools as input and selects common overlapping annotations as output.", "labels": [], "entities": []}, {"text": "U-Compare also provides visualizations of evaluation results allowing instance-based error analysis.", "labels": [], "entities": [{"text": "instance-based error analysis", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.6458194156487783}]}], "tableCaptions": []}