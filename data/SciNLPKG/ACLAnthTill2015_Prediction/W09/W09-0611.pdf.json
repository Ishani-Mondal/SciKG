{"title": [{"text": "Learning Lexical Alignment Policies for Generating Referring Expressions in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Learning Lexical Alignment Policies", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.690094344317913}]}], "abstractContent": [{"text": "We address the problem that different users have different lexical knowledge about problem domains, so that automated dialogue systems need to adapt their generation choices online to the users' domain knowledge as it encounters them.", "labels": [], "entities": []}, {"text": "We approach this problem using policy learning in Markov Decision Processes (MDP).", "labels": [], "entities": []}, {"text": "In contrast to related work we propose anew statistical user model which incorporates the lexical knowledge of different users.", "labels": [], "entities": []}, {"text": "We evaluate this user model by showing that it allows us to learn dialogue policies that automatically adapt their choice of referring expressions online to different users, and that these policies are significantly better than adaptive hand-coded policies for this problem.", "labels": [], "entities": []}, {"text": "The learned policies are consistently between 2 and 8 turns shorter than a range of different hand-coded but adaptive baseline lexical alignment policies.", "labels": [], "entities": []}], "introductionContent": [{"text": "In current \"troubleshooting\" spoken dialogue systems (SDS), the major part of the conversation is directed by the system, while the user follows the system's instructions.", "labels": [], "entities": [{"text": "troubleshooting\" spoken dialogue systems (SDS)", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.7407436296343803}]}, {"text": "Once the system decides what instruction to give the user (at the dialogue management level), it faces several decisions to be made at the natural language generation (NLG) level.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 139, "end_pos": 172, "type": "TASK", "confidence": 0.7838774025440216}]}, {"text": "These include, deciding which concepts to include in the utterance, deciding the referring expressions (RE) to use in the utterance and soon.", "labels": [], "entities": []}, {"text": "A little-studied problem is to what extent a system could automatically align to the user's lexical knowledge by adapting its RE choices, in particular based on his domain expertise, and how this can be modelled and optimised computationally.", "labels": [], "entities": []}, {"text": "( show how two interlocutors adapt their language in a conversation by assessing each other's domain expertise during dialogue, by observing how they react to each other's RE choices.", "labels": [], "entities": []}, {"text": "This is called alignment through Audience Design.", "labels": [], "entities": [{"text": "alignment", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9848604798316956}, {"text": "Audience Design", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.6685639917850494}]}, {"text": "Using inappropriate REs in instructions has been identified as a serious problem affecting a system's usability.", "labels": [], "entities": []}, {"text": "In this paper, we treat NLG within a computational learning paradigm.", "labels": [], "entities": []}, {"text": "We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user's lexical knowledge dynamically during the course of the dialogue.", "labels": [], "entities": [{"text": "audience design", "start_pos": 80, "end_pos": 95, "type": "TASK", "confidence": 0.7404919564723969}]}, {"text": "This can avoid clarification requests from the users and keep the dialogues short.", "labels": [], "entities": []}, {"text": "The example given below describes the kind of lexical alignment behaviour that we want the system to learn.", "labels": [], "entities": []}, {"text": "The system chooses \"small white box\" instead of \"ADSL filter\" and \"monitor symbol\" instead of \"network icon\", because it learnt that the user is a novice based on their clarification requests.", "labels": [], "entities": []}, {"text": "However, it switches to using technical terms like \"browser\", when it learns that the user is not a complete novice (as he verifies the description for the network icon in Usr 4 We study lexical alignment in the context of troubleshooting dialogues -where users wish to mend their broken internet connection.", "labels": [], "entities": [{"text": "lexical alignment", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.7351080179214478}]}, {"text": "This task demands anew type of User Simulation.", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.8104902505874634}]}, {"text": "In this paper, after a review of related work (section 2), we first present (section 3) a statistical User Simulation which supports different domain knowledge profiles and reacts accordingly to questions or instructions from an SDS.", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.6951613128185272}]}, {"text": "We then present a reinforcement learning model of lexical alignment due to audience design (in sections.", "labels": [], "entities": []}, {"text": "We then evaluate the User Simulation (section 6), testing whether a simulation that is sensitive to a system's RE choices can be used to learn good lexical alignment policies.", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7612750232219696}]}, {"text": "Finally, we compare policies learned in interaction with the User Simulation with hand-coded policies, and present the results in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated both the learned policies using a testing simulation and compared the results to other baseline hand-coded policies.", "labels": [], "entities": []}, {"text": "Unlike the training simulation, the testing simulation used the Bayesian knowledge model to produce all different kinds of user knowledge profiles.", "labels": [], "entities": []}, {"text": "It produced around 90 different profiles in varying distribution, resembling a realistic user population.", "labels": [], "entities": []}, {"text": "The tests were run over 250 simulated dialogues each.", "labels": [], "entities": []}, {"text": "Several rule-based baseline policies were manually created for the sake of comparison: 1.", "labels": [], "entities": []}, {"text": "Random -Choose REs at random.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 7: Rewards and Dialogue Length.", "labels": [], "entities": [{"text": "Rewards", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.615693986415863}]}]}