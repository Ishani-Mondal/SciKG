{"title": [{"text": "Automatic Selection of High Quality Parses Created By a Fully Unsupervised Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "The average results obtained by unsupervised statistical parsers have greatly improved in the last few years, but on many specific sentences they are of rather low quality.", "labels": [], "entities": []}, {"text": "The output of such parsers is becoming valuable for various applications, and it is radically less expensive to create than manually annotated training data.", "labels": [], "entities": []}, {"text": "Hence, automatic selection of high quality parses created by unsupervised parsers is an important problem.", "labels": [], "entities": []}, {"text": "In this paper we present PUPA, a POS-based Unsupervised Parse Assessment algorithm.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.5558568239212036}, {"text": "POS-based Unsupervised Parse Assessment", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.5052556619048119}]}, {"text": "The algorithm assesses the quality of a parse tree using POS sequence statistics collected from a batch of parsed sentences.", "labels": [], "entities": []}, {"text": "We evaluate the algorithm by using an unsupervised POS tagger and an unsupervised parser, selecting high quality parsed sentences from En-glish (WSJ) and German (NEGRA) corpora.", "labels": [], "entities": []}, {"text": "We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsuper-vised baseline.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.5555406808853149}]}, {"text": "Consequently, PUPA allows obtaining high quality parses without any human involvement.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.47143033146858215}]}], "introductionContent": [{"text": "In unsupervised parsing an algorithm should uncover the syntactic structure of an input sentence without using any manually created structural training data.", "labels": [], "entities": []}, {"text": "The last decade has seen significant progress in this field of research ().", "labels": [], "entities": []}, {"text": "Many NLP systems use the output of supervised parsers (e.g.,) for QA,) for IE,) for SRL, for Textual Inference and () for MT).", "labels": [], "entities": [{"text": "SRL", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9067780375480652}, {"text": "Textual Inference", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7473850250244141}, {"text": "MT", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.936156153678894}]}, {"text": "To achieve good performance, these parsers should be trained on large amounts of manually created training data from a domain similar to that of the sentences they parse ().", "labels": [], "entities": []}, {"text": "In the highly variable Web, where many of these systems are used, it is very difficult to create a representative corpus for manual annotation.", "labels": [], "entities": []}, {"text": "The high cost of manual annotation of training data for supervised parsers imposes a significant burden on their usage.", "labels": [], "entities": []}, {"text": "A possible answer to this problem can be provided by high quality parses produced by unsupervised parsers that require little to no manual efforts for their training.", "labels": [], "entities": []}, {"text": "These parses can be used either as input for applications, or as training material for modern supervised parsers whose output will in turn be used by applications.", "labels": [], "entities": []}, {"text": "Although unsupervised parser results improve, the quality of many of the parses they produce is still too low for such goals.", "labels": [], "entities": []}, {"text": "For example, the Seginer (2007) parser achieves an F-score of 75.9% on the WSJ10 corpus and 59% on the NEGRA10 corpus, but the percentage of individual sentences with an F-score of 100% is 21.5% for WSJ10 and 11% for NEGRA10.", "labels": [], "entities": [{"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9988446235656738}, {"text": "WSJ10 corpus", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.981464296579361}, {"text": "NEGRA10 corpus", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.9727667272090912}, {"text": "F-score", "start_pos": 170, "end_pos": 177, "type": "METRIC", "confidence": 0.9843918085098267}, {"text": "WSJ10", "start_pos": 199, "end_pos": 204, "type": "DATASET", "confidence": 0.96355801820755}, {"text": "NEGRA10", "start_pos": 217, "end_pos": 224, "type": "DATASET", "confidence": 0.9746125340461731}]}, {"text": "When requirements are relaxed, only asking for an F-score higher than 85%, percentage is still low, 42% for WSJ10 and 15% for In this paper we address the task of a fully unsupervised assessment of high quality parses cre-ated by an unsupervised parser.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9968773126602173}, {"text": "percentage", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9838085770606995}, {"text": "WSJ10", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.6960105299949646}]}, {"text": "The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers.", "labels": [], "entities": []}, {"text": "Assessing the quality of a learning algorithm's output and selecting high quality instances has been addressed for supervised algorithms) and specifically for supervised parsers.", "labels": [], "entities": []}, {"text": "Moreover, it has been shown to be valuable for supervised parser adaptation between domains.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.7949804961681366}]}, {"text": "However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers.", "labels": [], "entities": []}, {"text": "Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences 1 . The constituents in the batch are represented using the POS sequences of their yield and of the yields of neighboring constituents.", "labels": [], "entities": [{"text": "POS-based Unsupervised Parse Assessment (PUPA)", "start_pos": 4, "end_pos": 50, "type": "TASK", "confidence": 0.7396911808422634}]}, {"text": "Constituents whose representation is frequent in the output of the parser are considered to be of a high quality.", "labels": [], "entities": []}, {"text": "A score for each range of constituent length is calculated, reflecting the robustness of statistics used for the creation of the constituents of that length.", "labels": [], "entities": []}, {"text": "The final sentence score is a weighted average of the scores calculated for each constituent length.", "labels": [], "entities": []}, {"text": "The score thus integrates the quality of short and long constituents into one score reflecting the quality of the whole parse tree.", "labels": [], "entities": []}, {"text": "PUPA provides a quality score for every sentence in a parsed sentences set.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9261059761047363}]}, {"text": "An NLP application can then decide if to use a parse or not, according to its own definition of a high quality parse.", "labels": [], "entities": []}, {"text": "For example, it can select every sentence whose score is above some threshold, or the k top scored sentences.", "labels": [], "entities": []}, {"text": "The selection strategy is application dependent and is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "The unsupervised parser we use is the Seginer (2007) incremental parser 2 , which achieves state-of-the-art results without using manually created POS tags.", "labels": [], "entities": []}, {"text": "The POS tags we use are induced by the unsupervised tagger of . Since both tagger and parser do not require any manual annotation, PUPA identifies high quality parses without any human involvement.", "labels": [], "entities": []}, {"text": "The incremental parser of does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward.", "labels": [], "entities": []}, {"text": "Such a prediction can be given by supervised parsers in terms of the parse likelihood, but this was shown to be of medium quality.", "labels": [], "entities": []}, {"text": "While the algorithms of, and are supervised (Section 3), the ensemble based SEPA algorithm can be applied to unsupervised parsers in away that preserves the unsupervised nature of the selection task.", "labels": [], "entities": []}, {"text": "To compare between two algorithms, we use each of them to assess the quality of the sentences in English and German corpora (WSJ and NEGRA) . We show that for every sentence length (up to 20) the quality of the top scored k sentences according to PUPA is higher than the quality of SEPA's list (for every k).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.7817057371139526}, {"text": "PUPA", "start_pos": 247, "end_pos": 251, "type": "DATASET", "confidence": 0.7682416439056396}, {"text": "SEPA's list", "start_pos": 282, "end_pos": 293, "type": "DATASET", "confidence": 0.8443625966707865}]}, {"text": "As in, the quality of a set selected from the parser's output is evaluated using two measures: constituent F-score and average sentence F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.5557634830474854}, {"text": "average sentence F-score", "start_pos": 119, "end_pos": 143, "type": "METRIC", "confidence": 0.5599323709805807}]}, {"text": "Section 2 describes the PUPA algorithm, Section 3 discusses previous work, and Sections 4 and 5 present the evaluation setup and results.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.4832031726837158}]}], "datasetContent": [{"text": "We experiment with sentences of up to 20 words from the English WSJ Penn Treebank (WSJ20, 25236 sentences, 225126 constituents) and the German NEGRA corpus (Brants, 1997) (NEGRA20, 15610 sentences, 108540 constiteunts), both containing newspaper texts.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.8715536793073019}, {"text": "WSJ20", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.7966938614845276}, {"text": "German NEGRA corpus (Brants, 1997)", "start_pos": 136, "end_pos": 170, "type": "DATASET", "confidence": 0.8741710558533669}]}, {"text": "The unsupervised parsers of the kind addressed in this paper output unlabeled parse trees.", "labels": [], "entities": []}, {"text": "To evaluate the quality of a single parse tree with respect to another, we use the unlabeled F-score (U F = 2\u00b7U R\u00b7U P U R+U P ), where U Rand U P are unlabeled recall and unlabeled precision respectively.", "labels": [], "entities": [{"text": "F-score (U F = 2\u00b7U R\u00b7U P U R+U P )", "start_pos": 93, "end_pos": 127, "type": "METRIC", "confidence": 0.7180983970562617}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9739946722984314}, {"text": "precision", "start_pos": 181, "end_pos": 190, "type": "METRIC", "confidence": 0.8662598133087158}]}, {"text": "Following the unsupervised parsing literature, multiple brackets and brackets covering a single word are not counted, but the sentence level bracket is.", "labels": [], "entities": []}, {"text": "We exclude punctuation and null elements according to the scheme of (.", "labels": [], "entities": []}, {"text": "The performance of unsupervised parsers markedly degrades as sentence length increases.", "labels": [], "entities": []}, {"text": "For example, the Average sentence F-score for WSJ sentences of length 10 is 71.4% compared to 58.5 for sentences of length 20 (the numbers for NEGRA are 48.2% and 36.9%).", "labels": [], "entities": [{"text": "Average sentence F-score", "start_pos": 17, "end_pos": 41, "type": "METRIC", "confidence": 0.8592825730641683}, {"text": "NEGRA", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.8567752242088318}]}, {"text": "We therefore evaluate PUPA (and the baseline) for sentences of a given length.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.7287517189979553}]}, {"text": "We do this for every sentence of length 2-20 in WSJ20 and NEGRA20.", "labels": [], "entities": [{"text": "WSJ20", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.969158411026001}, {"text": "NEGRA20", "start_pos": 58, "end_pos": 65, "type": "DATASET", "confidence": 0.9159731864929199}]}, {"text": "For every sentence length L, we use PUPA and the baseline algorithm (SEPA) to give a quality score to each of the sentences of that length in the experimental corpus.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.6665852665901184}, {"text": "SEPA", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.7518747448921204}]}, {"text": "We then compare the quality of the top k parsed sentences according to each algorithm.", "labels": [], "entities": []}, {"text": "We do this for every k from 1 to the number of sentences of length L.", "labels": [], "entities": []}, {"text": "Following, we use two measures to evaluate the quality of a set of parses: the constituent F-score (the traditional Fscore used in the parsing literature), and the average F-score of the parses in the set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9800455570220947}, {"text": "Fscore", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9251258969306946}, {"text": "F-score", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.9910910129547119}]}, {"text": "In the first measure we treat the whole set as a bag of constituents.", "labels": [], "entities": []}, {"text": "Each constituent is marked as correct (if it appears in the gold standard parses of the set) or erroneous (if it does not).", "labels": [], "entities": []}, {"text": "Then, recall, precision and F-score are calculated over these constituents.", "labels": [], "entities": [{"text": "recall", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9996324777603149}, {"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996761083602905}, {"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9994915723800659}]}, {"text": "In the second measure, the constituent F-score of each of the parses in the set is computed, and then results are averaged.", "labels": [], "entities": [{"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9862798452377319}]}, {"text": "There are applications that use individual constituents from the output of a parser while others need the whole parse tree.", "labels": [], "entities": []}, {"text": "For example, if the selected set is used for training supervised parsers such as the Collins parser, which collects constituent statistics, the constituent F-score of the selected set is the important measure.", "labels": [], "entities": [{"text": "Collins", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9330152869224548}, {"text": "F-score", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9191234111785889}]}, {"text": "In applications such as the syntax based machine translation model of), a low quality tree might lead to errorenous translation of the sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7510101199150085}]}, {"text": "For such applications the average F-score is more indicative.", "labels": [], "entities": [{"text": "F-score", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9982613921165466}]}, {"text": "These measures thus represent complementary aspects of a set quality and we consider both of them.", "labels": [], "entities": []}, {"text": "The parser we use is the incremental parser of, POS tags are induced using the unsupervised POS tagger of (, neyessenmorph model).", "labels": [], "entities": []}, {"text": "In each experiment, the tagger was trained with the raw sentences of the experiment corpus, and then the corpus words were POS tagged.", "labels": [], "entities": []}, {"text": "The output of the unsupervised POS tagger depends on a random initialization.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.632691815495491}]}, {"text": "We ran the tagger 5 times, each time with a different random initialization, and then ran PUPA with its output.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.722455620765686}]}, {"text": "The results we report for PUPA are the average over these 5 runs.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.910422682762146}]}, {"text": "Random selection results (given for reference) were also averages over 5 samples.", "labels": [], "entities": []}, {"text": "PUPA 's parameter estimation is completely unsupervised (see Section 2).", "labels": [], "entities": [{"text": "PUPA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9568120837211609}]}, {"text": "No development data was used to tune its parameters.", "labels": [], "entities": []}, {"text": "A 200 sentences development set from each corpus was used for calibrating the parameters of the SEPA algorithm.", "labels": [], "entities": [{"text": "SEPA algorithm", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.5796405673027039}]}, {"text": "Based on the analysis of SEPA performance with different assignments of its parameters given by Reichart and Rappoport (2007) (see Section 3), we ran the SEPA algorithm with sample size (SEPA parameter S) of 30% and 80%, and with 2 -10 committee members (N ) . The optimal parameters were N = 10,S = 80 for WSJ20, and Random selection is presented for reference as a dotted line.", "labels": [], "entities": [{"text": "SEPA", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.9673148989677429}, {"text": "SEPA parameter S)", "start_pos": 187, "end_pos": 204, "type": "METRIC", "confidence": 0.6384082809090614}, {"text": "WSJ20", "start_pos": 307, "end_pos": 312, "type": "DATASET", "confidence": 0.9326496124267578}]}, {"text": "Top two rows: Average F-score for PUPA, SEPA and MC for sentences from WSJ (top row) and NEGRA (bottom row).", "labels": [], "entities": [{"text": "Average", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9834890365600586}, {"text": "F-score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.8375735282897949}, {"text": "PUPA", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.7078556418418884}, {"text": "WSJ", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8843039274215698}, {"text": "NEGRA", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.8430929780006409}]}, {"text": "Bottom two rows: Constituents F-score for PUPA, SEPA and MC for sentences from WSJ (top row) and NEGRA (bottom row).", "labels": [], "entities": [{"text": "Constituents F-score", "start_pos": 17, "end_pos": 37, "type": "METRIC", "confidence": 0.7553720474243164}, {"text": "PUPA", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.7728817462921143}, {"text": "SEPA", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.6779540181159973}, {"text": "WSJ", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8841802477836609}, {"text": "NEGRA", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.8278557658195496}]}, {"text": "Results are presented for sentence lengths of 5,10,15 and 20 (patterns for other sentence lengths between 2 and 20 are very similar).", "labels": [], "entities": []}, {"text": "PUPA is superior in all cases.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8045824766159058}]}, {"text": "The graphs for PUPA and SEPA show a downward trend because parsed sentences were sorted according to score, which correlates positively with F-score (unlike MC).", "labels": [], "entities": [{"text": "SEPA", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.6237642168998718}, {"text": "F-score", "start_pos": 141, "end_pos": 148, "type": "METRIC", "confidence": 0.986774206161499}]}, {"text": "The graphs converge because on the extreme right all test sentences were selected.", "labels": [], "entities": []}, {"text": "N = 10, S = 30 for NEGRA20.", "labels": [], "entities": [{"text": "N", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9657579064369202}, {"text": "NEGRA20", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.8775832056999207}]}, {"text": "We also compare PUPA to a baseline selecting the sentences with the lowest number of constituents.", "labels": [], "entities": [{"text": "PUPA", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.6354411244392395}]}, {"text": "Since the number of constituents is an indication of the complexity of the syntactic structure of a sentence, it is reasonable to assume that selecting the sentences with the lowest number of constituents is a good selection strategy.", "labels": [], "entities": []}, {"text": "We denote this baseline by MC (for minimum constituents).", "labels": [], "entities": [{"text": "MC", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9665949940681458}]}, {"text": "The incremental parser does not give any prediction of its output quality as supervised generative parsers do.", "labels": [], "entities": []}, {"text": "We are thus notable to compare to such a score.", "labels": [], "entities": []}, {"text": "shows Average F-score and Constituents Fscore results for PUPA SEPA and MC, for sentences of lengths 5,10,15 and 20 in WSJ20 and NEGRA20.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9554212093353271}, {"text": "Constituents", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9414325952529907}, {"text": "Fscore", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.5767474174499512}, {"text": "PUPA SEPA", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.6575970649719238}, {"text": "WSJ20", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.9480510354042053}, {"text": "NEGRA20", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.8625640869140625}]}, {"text": "The top two rows are for Average F-score (top row: WSJ, bottom row: NEGRA), while the bottom two rows are for Constituents F-score (top row: WSJ, bottom row: NEGRA).", "labels": [], "entities": [{"text": "Average", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9954212307929993}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.5768475532531738}, {"text": "NEGRA", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.7694792151451111}, {"text": "Constituents F-score", "start_pos": 110, "end_pos": 130, "type": "METRIC", "confidence": 0.6862798631191254}, {"text": "WSJ", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.82358318567276}, {"text": "NEGRA", "start_pos": 158, "end_pos": 163, "type": "DATASET", "confidence": 0.877856969833374}]}], "tableCaptions": [{"text": " Table 1: Average F-score for the top k% of constituents  selected from WSJ20 (up) and NEGRA20 (down). No sen- tence length restriction is imposed. Results presented for  PUPA , SEPA and MC. Average F-score of random se- lection is 66.55 (WSJ20) and 47.05 (NEGRA20). PUPA is  superior over all methods.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9702870845794678}, {"text": "WSJ20", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.9432325959205627}, {"text": "NEGRA20", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9271613359451294}, {"text": "sen- tence length restriction", "start_pos": 106, "end_pos": 135, "type": "METRIC", "confidence": 0.6852124214172364}, {"text": "F-score", "start_pos": 199, "end_pos": 206, "type": "METRIC", "confidence": 0.9701859354972839}, {"text": "WSJ20", "start_pos": 239, "end_pos": 244, "type": "DATASET", "confidence": 0.9110487699508667}, {"text": "NEGRA20", "start_pos": 257, "end_pos": 264, "type": "DATASET", "confidence": 0.9085469841957092}]}]}