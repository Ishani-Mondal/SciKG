{"title": [{"text": "A Non-negative Tensor Factorization Model for Selectional Preference Induction", "labels": [], "entities": [{"text": "Selectional Preference Induction", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.9165123105049133}]}], "abstractContent": [{"text": "Distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity.", "labels": [], "entities": [{"text": "induction of semantic similarity", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.7155915200710297}]}, {"text": "Up till now, most algorithms use two-way co-occurrence data to compute the meaning of words.", "labels": [], "entities": []}, {"text": "Co-occurrence frequencies, however, need not be pairwise.", "labels": [], "entities": []}, {"text": "One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond.", "labels": [], "entities": []}, {"text": "This paper will investigate a tensor factorization method called non-negative tensor factor-ization to build a model of three-way co-occurrences.", "labels": [], "entities": []}, {"text": "The approach is applied to the problem of selectional preference induction , and automatically evaluated in a pseudo-disambiguation task.", "labels": [], "entities": [{"text": "selectional preference induction", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8379262685775757}]}, {"text": "The results show that non-negative tensor factoriza-tion is a promising tool for NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity.", "labels": [], "entities": [{"text": "induction of semantic similarity", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.7155915200710297}]}, {"text": "The aggregate of a word's contexts generally provides enough information to compute its meaning, viz.", "labels": [], "entities": []}, {"text": "its semantic similarity or relatedness to other words.", "labels": [], "entities": []}, {"text": "Up till now, most algorithms use two-way cooccurrence data to compute the meaning of words.", "labels": [], "entities": []}, {"text": "A word's meaning might for example be computed by looking at: \u2022 the various documents that the word appears in (words \u00d7 documents); \u2022 a bag of words context window around the word (words \u00d7 context words); \u2022 the dependency relations that the word appears with (words \u00d7 dependency relations).", "labels": [], "entities": []}, {"text": "The extracted data -representing the cooccurrence frequencies of two different entities -is encoded in a matrix.", "labels": [], "entities": []}, {"text": "Co-occurrence frequencies, however, need not be pairwise.", "labels": [], "entities": []}, {"text": "One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond.", "labels": [], "entities": []}, {"text": "In an information retrieval context, one such situation might be the investigation of words \u00d7 documents \u00d7 authors.", "labels": [], "entities": []}, {"text": "In an NLP context, one might want to investigate words \u00d7 dependency relations \u00d7 bag of word context words, or verbs \u00d7 subjects \u00d7 direct objects.", "labels": [], "entities": []}, {"text": "Note that it is not possible to investigate the three-way co-occurrences in a matrix representation form.", "labels": [], "entities": []}, {"text": "It is possible to capture the cooccurrence frequencies of a verb with its subjects and its direct objects, but one cannot capture the co-occurrence frequencies of the verb appearing with the subject and the direct object at the same time.", "labels": [], "entities": []}, {"text": "When the actual three-way cooccurrence data is 'matricized', valuable information is thrown-away.", "labels": [], "entities": []}, {"text": "To be able to capture the mutual dependencies among the three modes, we will make use of a generalized tensor representation.", "labels": [], "entities": []}, {"text": "Two-way co-occurrence models (such as latent semantic analysis) have often been augmented with some form of dimensionality reduction in order to counter noise and overcome data sparseness.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6586925586064657}]}, {"text": "We will also make use of a dimensionality reduction algorithm appropriate for tensor representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of the NTF model have been quantitatively evaluated in a pseudo-disambiguation task, similar to the one used by.", "labels": [], "entities": []}, {"text": "It is used to evaluate the generalization capabilities of the algorithm.", "labels": [], "entities": []}, {"text": "The task is to judge which subject (s or s ) and direct object (o or o ) is more likely fora particular verb v, where (s, v, o) is a combination drawn from the corpus, and sand o area subject and direct object randomly drawn from the corpus.", "labels": [], "entities": []}, {"text": "A triple is considered correct if the algorithm prefers both sand o over their counterparts .20 keur goed 'pass' .03 amendement 'amendment' .09: Top 10 subjects, verbs and direct objects for the 'legislation' dimension sand o (so the (s, v, o) triple -that appears in the test corpus -is preferred over the triples (s , v, o ), (s , v, o) and (s, v, o )).", "labels": [], "entities": [{"text": "pass'", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.9685975909233093}]}, {"text": "The PARAFAC results indicate the fitness of tensor factorization for the induction of three-way selectional preferences.", "labels": [], "entities": [{"text": "PARAFAC", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8680661916732788}]}, {"text": "Even without the constraint of non-negativity, the model outperforms the matrix factorization models, reaching a score of about 85%.", "labels": [], "entities": []}, {"text": "The model deteriorates when more dimensions are used.", "labels": [], "entities": []}, {"text": "Both matrix factorization models perform worse than their tensor factorization counterparts.", "labels": [], "entities": []}, {"text": "The NMF still scores reasonably well, indicating the positive effect of the non-negativity constraint.", "labels": [], "entities": [{"text": "NMF", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.49550503492355347}]}, {"text": "The simple SVD model performs worst, reaching a score of about 70% with 50 dimensions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top 10 subjects, verbs and direct objects for the 'police action' dimension", "labels": [], "entities": []}, {"text": " Table 2: Top 10 subjects, verbs and direct objects for the 'legislation' dimension", "labels": [], "entities": []}, {"text": " Table 3: Top 10 subjects, verbs and direct objects for the 'exhibition' dimension", "labels": [], "entities": []}, {"text": " Table 5: Results of the 10-fold cross-validation for  the NTF, PARAFAC, NMF and SVD model for 50,  100 and 300 dimensions (averages and standard  deviation)", "labels": [], "entities": [{"text": "NTF", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9223905801773071}, {"text": "PARAFAC", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9055390954017639}, {"text": "NMF", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8456897735595703}]}]}