{"title": [{"text": "Supporting inferences in semantic space: representing words as regions", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic space models represent the meaning of a word as a vector in high-dimensional space.", "labels": [], "entities": []}, {"text": "They offer a framework in which the meaning representation of a word can be computed from its context, but the question remains how they support inferences.", "labels": [], "entities": []}, {"text": "While there has been some work on paraphrase-based inferences in semantic space, it is not clear how semantic space models would support inferences involving hyponymy, like horse ran \u2192 animal moved.", "labels": [], "entities": []}, {"text": "In this paper, we first discuss what a point in semantic space stands for, contrasting semantic space with G\u00e4rdenforsian conceptual space.", "labels": [], "entities": []}, {"text": "Building on this, we propose an extension of the semantic space representation from a point to a region.", "labels": [], "entities": []}, {"text": "We present a model for learning a region representation for word meaning in semantic space, based on the fact that points at close distance tend to represent similar meanings.", "labels": [], "entities": []}, {"text": "We show that this model can be used to predict, with high precision, when a hyponymy-based inference rule is applicable.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9974161386489868}]}, {"text": "Moving beyond paraphrase-based and hyponymy-based inference rules, we last discuss in what way semantic space models can support inferences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic space models represent the meaning of a word as a vector in a highdimensional space, where the dimensions stand for contexts in which the word occurs.", "labels": [], "entities": []}, {"text": "They have been used successfully in NLP, as well as in psychology.", "labels": [], "entities": [{"text": "NLP", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9506515264511108}]}, {"text": "Semantic space models, which are induced automatically from corpus data, can be used to characterize the meaning of an occurrence of a word in a specific sentence without recourse to dictionary senses.", "labels": [], "entities": [{"text": "characterize the meaning of an occurrence of a word in a specific sentence", "start_pos": 88, "end_pos": 162, "type": "TASK", "confidence": 0.7898148848460271}]}, {"text": "This is interesting especially in the light of the recent debate about the problems of dictionary senses.", "labels": [], "entities": []}, {"text": "However, it makes sense to characterize the meaning of words through semantic space representations only if these representations allow for inferences.", "labels": [], "entities": []}, {"text": "(1) Google acquired YouTube =\u21d2 Google bought YouTube (2) A horse ran =\u21d2 An animal moved Ex. is an example of an inference involving a paraphrase: acquire can be substituted for buy in some contexts, but not all, for example not in contexts involving acquiring skills.", "labels": [], "entities": []}, {"text": "Ex. is an inference based on hyponymy: run implies move in some contexts, but not all, for example not in the context computer runs.", "labels": [], "entities": []}, {"text": "In this paper, we concentrate on these two important types of inferences, but return to the broader question of how inferences are supported by semantic space models towards the end.", "labels": [], "entities": []}, {"text": "Semantic space models support paraphrase inferences: Lists of potential paraphrases (for example buy and gain for acquire) and the applicability of a paraphrase rule in context can be read off semantic space representations.", "labels": [], "entities": []}, {"text": "The same cannot be said for hyponymy-based inferences.", "labels": [], "entities": []}, {"text": "The most obvious conceptualization of hyponymy in semantic space, illustrated in (left), is to view the vectors as feature structures, and hyponymy as subsumption.", "labels": [], "entities": []}, {"text": "However, it seems unlikely that horse would occur in a subset of the contexts in which animal is found (though see Cimiano et al).", "labels": [], "entities": []}, {"text": "There is another possible conceptualization of hyponymy in semantic space, illustrated in: If the representation of a word's meaning in semantic space were a region rather than a point, hyponymy could be modeled as the sub-region relation.", "labels": [], "entities": []}, {"text": "This is also the model that G\u00e4rdenfors proposes within his framework of conceptual spaces, however it is not clear that the notion of a point in space is the same in conceptual space as in semantic space.", "labels": [], "entities": []}, {"text": "To better contrast the two frameworks, we will refer to semantic space as co-occurrence space in the rest of this paper.", "labels": [], "entities": []}, {"text": "This paper makes two contributions.", "labels": [], "entities": []}, {"text": "First, it discusses the notion of a point in space in both conceptual and co-occurrence space, arguing that they are fundamentally different, with points in co-occurrence space not representing potential entities but mixtures of uses.", "labels": [], "entities": []}, {"text": "Second, it introduces a computational model for extending the representation of word meaning in co-occurrence space from a point to a region.", "labels": [], "entities": []}, {"text": "In doing so, it makes use of the property that points in co-occurrence space that are close together represent similar meanings.", "labels": [], "entities": []}, {"text": "We do not assume that the subregion relation will hold between induced hyponym and hypernym representations, no more than that the subsumption relation would hold between them.", "labels": [], "entities": []}, {"text": "Instead, we will argue that the region representations make it possible to encode hyponymy information collected from another source, for example WordNet or a hyponymy induction scheme.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 146, "end_pos": 153, "type": "DATASET", "confidence": 0.9656531810760498}]}, {"text": "2 gives a short overview of existing geometric models of meaning.", "labels": [], "entities": []}, {"text": "3 we discuss the significance of a point in conceptual space and in co-occurrence space, finding that the two frameworks differ fundamentally in this respect, but that we can still represent word meanings as regions in co-occurrence space.", "labels": [], "entities": []}, {"text": "Building on this, Sec.", "labels": [], "entities": []}, {"text": "4 introduces a region model of word meaning in co-occurrence space that can be learned automatically from corpus data.", "labels": [], "entities": []}, {"text": "5 reports on experiments testing the model on the task of predicting hyponymy relations between occurrences of words.", "labels": [], "entities": [{"text": "predicting hyponymy relations between occurrences of words", "start_pos": 58, "end_pos": 116, "type": "TASK", "confidence": 0.8408550790378025}]}, {"text": "6 looks at both paraphrase-based and hyponymy-based inferences to see how their applicability can be tested in co-occurrence space and how this generalizes to other types of inference rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report on experiments on hyponymy in co-occurrence space.", "labels": [], "entities": []}, {"text": "We test whether different co-occurrence space models can predict, given meaning representations (summation vectors, occurrence vectors, or regions) of two words, whether one of the two words is a hypernym of the other.", "labels": [], "entities": []}, {"text": "In all tests, the models do not seethe words, just the co-occurrence space representations.", "labels": [], "entities": []}, {"text": "We used a Minipar dependency parse of the British National Corpus (BNC) as the source of data for all experiments below.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.9622249801953634}]}, {"text": "The written portion of the BNC was split at random into two halves: a training half and a test half.", "labels": [], "entities": [{"text": "BNC", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.40147557854652405}]}, {"text": "We used WordNet 3.0 as the \"ground truth\" against which to evaluate models.", "labels": [], "entities": []}, {"text": "We work with two main sets of lemmas: first, the set of monosemous verbs according to WordNet (we refer to this set as Mon), and second, the set of hypernyms of the verbs in Mon (we call this set Hyp).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9591513276100159}]}, {"text": "We concentrate on monosemous words in the current paper since they will allow us to evaluate property (P3) most directly.", "labels": [], "entities": []}, {"text": "Since the model from Sec.", "labels": [], "entities": []}, {"text": "4 needs substantive amounts of occurrence vectors for training, we restricted both sets Mon and Hyp to verbs that occur with at least 50 different direct objects in the training half of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 190, "end_pos": 193, "type": "DATASET", "confidence": 0.9510443806648254}]}, {"text": "The direct objects, in turn, were restricted to those that occurred no more than 6,500 and no less than 270 times with verbs in the BNC, to remove both uninformative and sparse objects.", "labels": [], "entities": [{"text": "BNC", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.9461276531219482}]}, {"text": "(The boundaries were determined heuristically by inspection of the direct objects for this pilot study.)", "labels": [], "entities": []}, {"text": "This resulted in a set Mon consisting of 120 verbs, and Hyp consisting of 430 verbs.", "labels": [], "entities": []}, {"text": "Summation vectors for all words were computed with the dv package 2 from the training half of the BNC, using vectors of 500 dimensions with raw co-occurrence counts as dimension values.", "labels": [], "entities": [{"text": "BNC", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.9416146874427795}]}, {"text": "Above, we have hypothesized that cooccurrence space representations of hyponyms and hypernyms, in the form in which they are induced from corpus data, cannot in general be assumed to be in either a subsumption or a subregion relation.", "labels": [], "entities": []}, {"text": "We test this hypothesis, starting with subsumption.", "labels": [], "entities": []}, {"text": "We define subsumption as x y \u21d0\u21d2 \u2200i(y i > 0 \u2192 xi > 0).", "labels": [], "entities": []}, {"text": "Now, any given verb in Mon will be the hyponym of some verbs in Hyp and unrelated to others.", "labels": [], "entities": [{"text": "Hyp", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9531570076942444}]}, {"text": "So we test, for each summation vector v 1 of a verb in Mon and summation vector v 2 of a verb in Hyp, whether v 1 v 2 . The result is that Mon verbs subsume 5% of the Hyp verbs of which they are hyponyms, and 1% of the Hyp verbs that are unrelated.", "labels": [], "entities": [{"text": "Hyp", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.955899178981781}]}, {"text": "We conclude that subsumption between summation vectors in co-occurrence space is not a reliable indicator of the hyponymy relation between words.", "labels": [], "entities": []}, {"text": "Experiment 2: Subregion relation.", "labels": [], "entities": [{"text": "Subregion relation", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.913375586271286}]}, {"text": "Next, we test whether, when we represent a Hyp-verb as a region in co-occurrence space, occurrences of its Mon-hyponyms fall inside that region, and occurrences of non-hypernyms are outside.", "labels": [], "entities": []}, {"text": "First, we compute occurrence vectors for each Hyp or Mon verb v as described in Sec.", "labels": [], "entities": []}, {"text": "2: Given an occurrence of a verb v, we compute its occurrence vector by combining the summation vector of v with the summation vector of the direct object of v in the given sentence 3 . We combine two summation vectors by computing their average.", "labels": [], "entities": []}, {"text": "In this experiment, we use occurrences from both halves of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.9377803802490234}]}, {"text": "With those summation and occurrence vectors in hand, we then learn a region representation for each Hyp verb using the model from Sec.", "labels": [], "entities": []}, {"text": "4. We implemented the region model using the OpenNLP maxent package 4 . Last, we test, for each Mon verb occurrence vector and each Hyp region, whether the occurrence vector is classified as being inside the region.", "labels": [], "entities": []}, {"text": "The result is that the region models classified zero hyponym occurrences as being inside, resulting in precision and recall of 0.0.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9995627999305725}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9991705417633057}]}, {"text": "These results show clearly that our earlier hypothesis was correct: The co-occurrence representations that we have induced from corpus data do not lend themselves to reading off hyponymy relations through either subsumption or the subregion relation.", "labels": [], "entities": []}, {"text": "Experiment 3: Encoding hyponymy.", "labels": [], "entities": []}, {"text": "These findings do not mean that it is impossible to test the applicability of hyponymy-based inferences in co-occurrence space.", "labels": [], "entities": []}, {"text": "If we cannot induce hyponymy relations from existing vector representations, we may still be able to encode hyponymy information from a separate source such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.9844374656677246}]}, {"text": "Note that this would be difficult in a words-as-points representation: The only possibility there would be to modify summation vectors.", "labels": [], "entities": []}, {"text": "With a words-as-regions representation, we can keep the summation vectors constant and modify the regions.", "labels": [], "entities": []}, {"text": "Our aim in this experiment is to produce a region representation fora Hyp verb v such that occurrence vectors of v's hyponyms will fall into the region.", "labels": [], "entities": []}, {"text": "We use only direct hypernyms of Mon verbs in this experiment, a 273-verb subset of Hyp we call DHyp.", "labels": [], "entities": [{"text": "Hyp", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.926002025604248}]}, {"text": "For each DHyp verb v, we learn a region representation centered on v's summation vector, using as positive training data all occurrences of v and v's direct hyponyms in the training half of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 194, "end_pos": 197, "type": "DATASET", "confidence": 0.9408361315727234}]}, {"text": "(Negative training data are occurrences of other DHyp verbs and their children.)", "labels": [], "entities": []}, {"text": "We then test, for each occurrence of a Mon verb in the test half of the BNC that does not occur in the training half with the same direct object, whether it is classified as being inside v's region.", "labels": [], "entities": [{"text": "BNC", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.8857764601707458}]}, {"text": "The result of this experiment is a precision of 95.2, recall of 43.4, and F-score of 59.6 (against a random baseline of prec=11.0, rec=50.2, and F=18.0).", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9996693134307861}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9997982382774353}, {"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9997479319572449}, {"text": "prec", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9719275832176208}, {"text": "F", "start_pos": 145, "end_pos": 146, "type": "METRIC", "confidence": 0.9992122650146484}]}, {"text": "This shows that it is possible to encode hyponymy information in a co-occurrence space representation: The region model identifies hyponym occurrences with very high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9903696179389954}]}, {"text": "If anything, the region is too narrow, classifying many actual hyponyms as negatives.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Some co-occurrence counts for letter, surprise in Austen's Pride  and Prejudice", "labels": [], "entities": [{"text": "letter", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9253721833229065}]}]}