{"title": [{"text": "A Scalable Global Model for Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9810780882835388}]}], "abstractContent": [{"text": "We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.9433990120887756}]}, {"text": "We compare our model, which operates at the sub-sentence or \"concept\"-level, to a sentence-level model, previously solved with an ILP.", "labels": [], "entities": []}, {"text": "Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences.", "labels": [], "entities": []}, {"text": "We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7228081375360489}, {"text": "sentence selection", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.6946515887975693}]}, {"text": "The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.8095453580220541}]}], "introductionContent": [{"text": "Automatic summarization systems are typically extractive or abstractive.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.605120450258255}]}, {"text": "Since abstraction is quite hard, the most successful systems tested at the Text Analysis Conference (TAC) and Document Understanding Conference (DUC) , for example, are extractive.", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.8089501510063807}, {"text": "Document Understanding Conference (DUC)", "start_pos": 110, "end_pos": 149, "type": "TASK", "confidence": 0.6229955156644186}]}, {"text": "In particular, sentence selection represents a reasonable trade-off between linguistic quality, guaranteed by longer textual units, and summary content, often improved with shorter units.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7777470052242279}]}, {"text": "Whereas the majority of approaches employ a greedy search to find a set of sentences that is TAC is a continuation of DUC, which ran from both relevant and non-redundant (), some recent work focuses on improved search).", "labels": [], "entities": [{"text": "TAC", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.7489027976989746}]}, {"text": "Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%.", "labels": [], "entities": []}, {"text": "His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences.", "labels": [], "entities": []}, {"text": "Under a maximum summary length constraint, this problem can be expressed as a quadratic knapsack () and many methods are available to solve it ().", "labels": [], "entities": []}, {"text": "However, McDonald reports that the method is not scalable above 100 input sentences and discusses more practical approximations.", "labels": [], "entities": []}, {"text": "Still, an ILP formulation is appealing because it gives exact solutions and lends itself well to extensions through additional constraints.", "labels": [], "entities": []}, {"text": "Methods like McDonald's, including the wellknown Maximal Marginal Relevance (MMR) algorithm (), are subject to another problem: Summary-level redundancy is not always well modeled by pairwise sentence-level redundancy.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance (MMR) algorithm", "start_pos": 49, "end_pos": 91, "type": "METRIC", "confidence": 0.7944755979946682}, {"text": "Summary-level redundancy", "start_pos": 128, "end_pos": 152, "type": "TASK", "confidence": 0.9461265206336975}]}, {"text": "shows an example where the combination of sentences (1) and (2) overlaps completely with sentence (3), a fact not captured by pairwise redundancy measures.", "labels": [], "entities": []}, {"text": "Redundancy, like content selection, is a global problem.", "labels": [], "entities": [{"text": "content selection", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7236250042915344}]}, {"text": "Here, we discuss a model for sentence selection with a globally optimal solution that also addresses redundancy globally.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8069338798522949}]}, {"text": "We choose to represent infor-  mation at a finer granularity than sentences, with concepts, and assume that the value of a summary is the sum of the values of the unique concepts it contains.", "labels": [], "entities": []}, {"text": "While the concepts we use in experiments are word n-grams, we use the generic term to emphasize that this is just one possible definition.", "labels": [], "entities": []}, {"text": "Only crediting each concept once serves as an implicit global constraint on redundancy.", "labels": [], "entities": []}, {"text": "We show how the resulting optimization problem can be mapped to an ILP that can be solved efficiently with standard software.", "labels": [], "entities": []}, {"text": "We begin by comparing our model to McDonald's (section 2) and detail the differences between the resulting ILP formulations (section 3), showing that ours can give competitive results (section 4) and offer better scalability 2 (section 5).", "labels": [], "entities": []}, {"text": "Next we demonstrate how our ILP formulation can be extended to include efficient parse-tree-based sentence compression (section 6).", "labels": [], "entities": [{"text": "parse-tree-based sentence compression", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.6127926707267761}]}, {"text": "We review related work (section 7) and conclude with a discussion of potential improvements to the model (section 8).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: There is a strong relationship between the docu- ment frequency of input bigrams and the fraction of those  bigrams that appear in the human generated \"gold\" set:  Let d i be document frequency i and p i be the percent of  input bigrams with d i that are actually in the gold set.", "labels": [], "entities": []}, {"text": " Table 2: Scores for both systems and a baseline on TAC  2008 data (Set A) for ROUGE-2 and Pyramid evalua- tions.", "labels": [], "entities": [{"text": "TAC  2008 data", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.9074300130208334}, {"text": "ROUGE-2", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.749226450920105}]}, {"text": " Table 3: Scores of the system with and without sentence  compression included in the ILP (TAC'08 Set A data).", "labels": [], "entities": [{"text": "ILP (TAC'08 Set A data)", "start_pos": 86, "end_pos": 109, "type": "DATASET", "confidence": 0.7698589818818229}]}]}