{"title": [{"text": "Context-Dependent Regression Testing for Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Regression testing of natural language systems is problematic for two main reasons: component input and output is complex, and system behaviour is context-dependent.", "labels": [], "entities": [{"text": "Regression testing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8949608504772186}]}, {"text": "We have developed a generic approach which solves both of these issues.", "labels": [], "entities": []}, {"text": "We describe our regression tool, CONTEST, which supports context-dependent testing of dialogue system components, and discuss the regression test sets we developed, designed to effectively isolate components from changes and problems earlier in the pipeline.", "labels": [], "entities": []}, {"text": "We believe that the same approach can be used in regression testing for other dialogue systems, as well as in testing any complex NLP system containing multiple components.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing systems, and dialogue systems in particular, often consist of large sets of components operating as a pipeline, including parsing, semantic interpretation, dialogue management, planning, and generation.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 159, "end_pos": 182, "type": "TASK", "confidence": 0.6621968299150467}, {"text": "dialogue management", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.7201287895441055}]}, {"text": "Testing such a system can be a difficult task for several reasons.", "labels": [], "entities": []}, {"text": "First, the component output maybe context-dependent.", "labels": [], "entities": []}, {"text": "This is particularly true fora dialogue system -reference resolution, ellipsis, and sometimes generation typically have to query the system state to produce their output, which depends both on the state of the world (propositions defined in a knowledge base) and on the dialogue history (object salience).", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8461224138736725}]}, {"text": "Under these conditions, unit testing using the input and output of a single component in isolation is of limited value -the entire system state needs to be preserved to check that context-dependent components are functioning as expected.", "labels": [], "entities": []}, {"text": "Second, the inputs and outputs of most system components are usually very complex and often changeover time as the system develops.", "labels": [], "entities": []}, {"text": "When two complex representations are compared it maybe difficult to determine what impact any change is likely to have on system performance (far-reaching or relatively trivial).", "labels": [], "entities": []}, {"text": "Further, if we test components in isolation by saving their inputs, and these inputs are reasonably complex, then it will become difficult to maintain the test sets for the components further along the pipeline (such as diagnosis and generation) as the output of the earlier components changes during development.", "labels": [], "entities": [{"text": "diagnosis and generation)", "start_pos": 220, "end_pos": 245, "type": "TASK", "confidence": 0.8563673496246338}]}, {"text": "The simplest way to deal with both of these issues would be to save a set of test dialogues as a gold standard, checking that the final system output is correct given the system input.", "labels": [], "entities": []}, {"text": "However, this presents another problem.", "labels": [], "entities": []}, {"text": "If a single component (generation, for example) malfunctions, it becomes impossible to verify that a component earlier in the pipeline (for example, reference resolution) is working properly.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 149, "end_pos": 169, "type": "TASK", "confidence": 0.7021928578615189}]}, {"text": "In principle we could also save the messages passing between components and compare their content, but then we are faced again with the problems arising from the complexity of component input and output which we described above.", "labels": [], "entities": []}, {"text": "To solve these problems, we developed a regression tool called CONTEST (for CONtext-dependent TESTing).", "labels": [], "entities": []}, {"text": "CONTEST allows the authors of individual system components to control what information to record for regression testing.", "labels": [], "entities": []}, {"text": "Test dialogues are saved and replayed through the system, and individual components are tested by comparing only their specific regression output, ignoring the outputs generated by other components.", "labels": [], "entities": []}, {"text": "The components are isolated by maintaining a minimal set of inputs that are guaranteed to be processed correctly.", "labels": [], "entities": []}, {"text": "To deal with issues of output complexity we extend the approach of de for testing a deep parser.", "labels": [], "entities": []}, {"text": "They created test sets at different levels of granularity, some including detailed representations, but some just saving very simple output of a textual entailment component.", "labels": [], "entities": []}, {"text": "They showed that, given a carefully selected test set, testing on the final system output can be a fast and effective way to discover problems in the interpretation pipeline.", "labels": [], "entities": [{"text": "interpretation pipeline", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.9088696539402008}]}, {"text": "We show how the same idea can be used to test other dialogue system components as well.", "labels": [], "entities": []}, {"text": "We describe the design of three different test sets that effectively isolate the interpretation, tutorial planning and generation components of our system.", "labels": [], "entities": []}, {"text": "Using CONTEST allows us to detect system errors and maintain consistent test sets even as the underlying representations change, and gives us much greater confidence that the results of our testing are relevant to the performance of the system with real users.", "labels": [], "entities": []}, {"text": "The rest of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe our system and its components in more detail.", "labels": [], "entities": []}, {"text": "The design of the CONTEST tool and the test sets are described in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "Finally, in Section 5 we discuss how the interactive nature of the dialogue influences the design of the test sets and the process of verifying the answers; and we discuss features that we would like to implement in the future.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}