{"title": [{"text": "Optimizing Textual Entailment Recognition Using Particle Swarm Optimization", "labels": [], "entities": [{"text": "Optimizing Textual Entailment Recognition", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8114238455891609}, {"text": "Particle Swarm Optimization", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.66737828652064}]}], "abstractContent": [{"text": "This paper introduces anew method to improve tree edit distance approach to tex-tual entailment recognition, using particle swarm optimization.", "labels": [], "entities": [{"text": "tex-tual entailment recognition", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.8225398262341818}, {"text": "particle swarm optimization", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.767220695813497}]}, {"text": "Currently, one of the main constraints of recognizing textual en-tailment using tree edit distance is to tune the cost of edit operations, which is a difficult and challenging task in dealing with the entailment problem and datasets.", "labels": [], "entities": []}, {"text": "We tried to estimate the cost of edit operations in tree edit distance algorithm automatically , in order to improve the results for textual entailment.", "labels": [], "entities": []}, {"text": "Automatically estimating the optimal values of the cost operations overall RTE development datasets, we proved a significant enhancement inaccuracy obtained on the test sets.", "labels": [], "entities": [{"text": "RTE development datasets", "start_pos": 75, "end_pos": 99, "type": "DATASET", "confidence": 0.6974095900853475}]}], "introductionContent": [{"text": "One of the main aspects of natural languages is to express the same meaning in many possible ways, which directly increase the language variability and emerges the complex structure in dealing with human languages.", "labels": [], "entities": []}, {"text": "Almost all computational linguistics tasks such as Information Retrieval (IR), Question Answering (QA), Information Extraction (IE), text summarization and Machine Translation (MT) have to cope with this notion.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.8604653239250183}, {"text": "Question Answering (QA)", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.850642991065979}, {"text": "Information Extraction (IE)", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.8362308144569397}, {"text": "text summarization", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7954423427581787}, {"text": "Machine Translation (MT)", "start_pos": 156, "end_pos": 180, "type": "TASK", "confidence": 0.8509957551956177}]}, {"text": "Textual Entailment Recognition was proposed by), as a generic task in order to conquer the problem of lexical, syntactic and semantic variabilities in languages.", "labels": [], "entities": [{"text": "Textual Entailment Recognition", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.826746920744578}]}, {"text": "Textual Entailment can be explained as an association between a coherent text (T) and a language expression, called hypothesis (H) such that entailment function for the pair T-H returns the true value when the meaning of H can be inferred from the meaning of T and false, otherwise.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7280508279800415}]}, {"text": "Amongst the approaches to the problem of textual entailment, some methods utilize the notion of distance between the pair of T and H as the main feature which separates the entailment classes (positive and negative).", "labels": [], "entities": []}, {"text": "One of the successful algorithms implemented Tree Edit Distance (TED), based on the syntactic features that are represented in the structured parse tree of each string.", "labels": [], "entities": []}, {"text": "In this method the distance is computed as the cost of the edit operations (insertion, deletion and substitution) that transform the text T into the hypothesis H.", "labels": [], "entities": []}, {"text": "Each edit operation has an associated cost and the entailment score is calculated such that the set of operations would lead to the minimum cost.", "labels": [], "entities": []}, {"text": "Generally, the initial cost is assigned to each edit operation empirically, or based on the expert knowledge and experience.", "labels": [], "entities": []}, {"text": "These methods emerge a critical problem when the domain, field or application is new and the level of expertise and empirical knowledge is very limited.", "labels": [], "entities": []}, {"text": "In dealing with textual entailment, () tried to experiment different cost values based on various linguistics knowledge and probabilistics estimations.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7297745645046234}]}, {"text": "For instance, they defined the substitution cost as a function of similarity between two nodes, or, for insertion cost, they employed Inverse Document Frequency (IDF) of the inserted node.", "labels": [], "entities": [{"text": "Inverse Document Frequency (IDF)", "start_pos": 134, "end_pos": 166, "type": "METRIC", "confidence": 0.8437976936499277}]}, {"text": "However, the results could not proven to be optimal.", "labels": [], "entities": []}, {"text": "Other approaches towards estimating the cost of operations in TED tried to learn a generic or discriminative probabilistic model () from the data, without concerning the optimal value of each operation.", "labels": [], "entities": []}, {"text": "One of the drawbacks of those approaches is that the cost values of edit operations are hidden behind the probabilistic model.", "labels": [], "entities": []}, {"text": "Additionally, the cost cannot be weighted or varied according to the tree context and node location.", "labels": [], "entities": []}, {"text": "In order to overcome these drawbacks, we are proposing a stochastic method based on Particle Swarm Optimization (PSO), to estimate the cost of each edit operation for textual entailment problem.", "labels": [], "entities": [{"text": "Particle Swarm Optimization (PSO)", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.7199627061684927}]}, {"text": "Implementing PSO, we try to learn the optimal cost for each operation in order to improve the prior textual entailment model.", "labels": [], "entities": [{"text": "PSO", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.931512713432312}]}, {"text": "In this paper, the goal is to automatically estimate the best possible operation costs on the development set.", "labels": [], "entities": []}, {"text": "A further advantage of such method, besides automatic learning of the operation costs, is being able to investigate the cost values to better understand how TED approaches the data in textual entailment.", "labels": [], "entities": [{"text": "TED", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.9282906651496887}]}, {"text": "The rest of the paper is organized as follows: After describing the TED approach to textual entailment in the next section, PSO optimization algorithm and our method in applying it to the problem are explained in sections 4 and 5.", "labels": [], "entities": [{"text": "PSO optimization", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.9102296531200409}]}, {"text": "Then we present our experimental setup as well as the results, in detail.", "labels": [], "entities": []}, {"text": "Finally, in the conclusion, the main advantages of our approach are reviewed and further developments are proposed accordingly.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we show an increase in the performance of TED based approach to textual entailment, by optimizing the cost of edit operations.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.716843992471695}]}, {"text": "In the following subsections, the framework and dataset of our experiments are elaborated.", "labels": [], "entities": []}, {"text": "Our experiments were conducted on the basis of the Recognizing Textual Entailment (RTE) datasets 2 , which were developed under PASCAL RTE challenge.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE) datasets 2", "start_pos": 51, "end_pos": 98, "type": "DATASET", "confidence": 0.7110843025147915}, {"text": "PASCAL RTE challenge", "start_pos": 128, "end_pos": 148, "type": "DATASET", "confidence": 0.8487250208854675}]}, {"text": "Each RTE dataset includes its own development and test set, however, RTE-4 was released only as a test set and the data from RTE-1 to RTE-3 were used as development set.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 5, "end_pos": 16, "type": "DATASET", "confidence": 0.9280970096588135}, {"text": "RTE-4", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.9064764976501465}, {"text": "RTE-1", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.9042704105377197}, {"text": "RTE-3", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.6099526286125183}]}, {"text": "More details about the RTE datasets are illustrated in: RTE-1 to RTE-4 datasets.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.806395024061203}, {"text": "RTE-1", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.7821437120437622}, {"text": "RTE-4 datasets", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.8655688762664795}]}, {"text": "In our experiments, in order to deal with TED approach to textual entailment, we used EDITS 3 package (Edit Distance Textual Entailment Suite) . This system is an open source software based on edit distance algorithms, and computes the T-H distance as the cost of the edit operations (i.e. insertion, deletion and substitution) that are necessary to transform T into H.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.6696960777044296}]}, {"text": "By defining the edit distance algorithm and a cost scheme (assigning a cost to the edit operations), this package is able to learn a TED threshold, over a set of string pairs, to decide if the entailment exists in a pair.", "labels": [], "entities": [{"text": "TED", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.9026305675506592}]}, {"text": "In addition, we partially exploit the JSwarm-PSO 4) package, with some adaptations, as an implementation of PSO algorithm.", "labels": [], "entities": [{"text": "JSwarm-PSO 4) package", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.809130385518074}]}, {"text": "Each pair in the datasets is converted to two syntactic dependency parse trees using the Stanford statistical parser , developed in the Stanford university NLP group by).", "labels": [], "entities": []}, {"text": "In order to take advantage of PSO optimization approach, we integrated EDITS and JSwarm-PSO to provide a flexible framework for the experiments (.", "labels": [], "entities": [{"text": "PSO optimization", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.9259286522865295}]}, {"text": "In this way, we applied the defined fitness functions in the integrated system.", "labels": [], "entities": []}, {"text": "The Bhattacharyya distance between two classes (YES and NO), in each experiment, could be computed based on the TED score of each pair in the dataset.", "labels": [], "entities": [{"text": "Bhattacharyya distance", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.9817174673080444}, {"text": "YES", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9235454797744751}, {"text": "NO", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.7325651049613953}, {"text": "TED score", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9733098745346069}]}, {"text": "Moreover, the accuracy, by default, is computed by EDITS over the training set based on 10-fold cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996763467788696}, {"text": "EDITS", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.6166174411773682}]}, {"text": "We conducted six different experiments in two sets on each RTE dataset.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9272169172763824}]}, {"text": "The costs were estimated on the training set and the results obtained based on the estimated costs over the test set.", "labels": [], "entities": []}, {"text": "In the first http://jswarm-pso.sourceforge.net/ 5 http://nlp.stanford.edu/software/lex-parser.shtml set of experiments, we set a simple cost scheme based on three operations.", "labels": [], "entities": []}, {"text": "Implementing this cost scheme, we expect to optimize the cost of each edit operation without considering that the operation costs may vary based on different characteristics of anode, such assize, location or content.", "labels": [], "entities": []}, {"text": "The results were obtained considering three different settings: 1) the random cost assignment; 2) assigning the cost based on the human expertise knowledge and intuition (called Intuitive), and 3) automatic estimated and optimized cost for each operation.", "labels": [], "entities": []}, {"text": "In the second case, we used the same scheme which was used in EDITS by its developers ( ).", "labels": [], "entities": []}, {"text": "In the second set of experiments, we tried to compose an advanced cost scheme with more fine-grained operations to assign a weight to the edit operations based on the characteristics of the nodes.", "labels": [], "entities": []}, {"text": "For example if anode is in the list of stopwords, the deletion cost is set to zero.", "labels": [], "entities": []}, {"text": "Otherwise, the cost of deletion would be equal to the number of words in H multiplied by word's length (number of characters).", "labels": [], "entities": []}, {"text": "Similarly, the cost of inserting a word win H is set to 0 if w is a stop word, and to the number of words in T multiplied by words length otherwise.", "labels": [], "entities": []}, {"text": "The cost of substituting two words is the Levenshtein distance (i.e. the edit distance calculated at the level of characters) between their lemmas, multiplied by the number of words in T, plus number of words in H.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.6991606801748276}, {"text": "edit distance calculated", "start_pos": 73, "end_pos": 97, "type": "METRIC", "confidence": 0.9287314414978027}]}, {"text": "By this intuition, we tried to optimize nine specialized costs for edit operations (i.e. each particle is defined by 9 parameters to be optimized).", "labels": [], "entities": []}, {"text": "We conducted the experiments using all three cases mentioned in the simple cost scheme.", "labels": [], "entities": []}, {"text": "In each experiment, we applied both fitness functions in the optimization; however, at the final phase, the costs which led to the maximum results were chosen as the estimated operation costs.", "labels": [], "entities": []}, {"text": "In order to save breath and time, we set the number of iterations to 10, in addition, the weight \u03c9 was set to 0.95 for better global exploration).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: RTE-1 to RTE-4 datasets.", "labels": [], "entities": [{"text": "RTE-1", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.7173296809196472}, {"text": "RTE-4 datasets", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9053221046924591}]}, {"text": " Table 2: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.999126136302948}, {"text": "RTE datasets", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.7484695017337799}]}]}