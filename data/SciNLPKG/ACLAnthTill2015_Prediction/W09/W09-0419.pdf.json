{"title": [{"text": "Statistical Post Editing and Dictionary Extraction: Systran/Edinburgh submissions for ACL-WMT2009", "labels": [], "entities": [{"text": "Statistical Post Editing", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6993639866511027}, {"text": "Dictionary Extraction", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.6773657500743866}, {"text": "ACL-WMT2009", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.5376402735710144}]}], "abstractContent": [{"text": "We describe here the two Sys-tran/University of Edinburgh submissions for WMT2009.", "labels": [], "entities": [{"text": "Sys-tran/University of Edinburgh submissions", "start_pos": 25, "end_pos": 69, "type": "DATASET", "confidence": 0.6344789117574692}, {"text": "WMT2009", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.46643030643463135}]}, {"text": "They involve a statistical post-editing model with a particular handling of named entities (English to French and German to English) and the extraction of phrasal rules (English to French).", "labels": [], "entities": []}], "introductionContent": [{"text": "Previous results had shown a rather satisfying performance for hybrid systems such as the Statistical Phrase-based Post-Editing (SPE)) combination in comparison with purely phrase-based statistical models, reaching similar BLEU scores and often receiving better human judgement (German to English at WMT2007) against the BLEU metric.", "labels": [], "entities": [{"text": "Statistical Phrase-based Post-Editing (SPE))", "start_pos": 90, "end_pos": 134, "type": "TASK", "confidence": 0.5664806713660558}, {"text": "BLEU", "start_pos": 223, "end_pos": 227, "type": "METRIC", "confidence": 0.9977503418922424}, {"text": "WMT2007", "start_pos": 300, "end_pos": 307, "type": "DATASET", "confidence": 0.8226621747016907}, {"text": "BLEU", "start_pos": 321, "end_pos": 325, "type": "METRIC", "confidence": 0.9798855185508728}]}, {"text": "This last result was in accordance with the previous acknowledgment) that systems of too differing structure could not be compared reliably with BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9667624235153198}]}, {"text": "We participated in the recent Workshop on Machine Translation (WMT'09) in the language pairs English to French and German to English.", "labels": [], "entities": [{"text": "Machine Translation (WMT'09)", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.8033721268177032}]}, {"text": "On the one hand we trained a PostEditing system with an additional special treatment to avoid the loss of entities such as dates and numbers.", "labels": [], "entities": []}, {"text": "On the other hand we trained an additional English-to-French system (as a secondary submission) that made use of automatically extracted linguistic entries.", "labels": [], "entities": []}, {"text": "In this paper, we will present both approaches.", "labels": [], "entities": []}, {"text": "The latter is part of ongoing work motivated by the desire to both make use of corpus statistics and keep the advantage of the often (relative to automatic metrics's scores) higher rank inhuman judgement given to rulebased systems on out-of-domain data, as seen on", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Compared results of original RBMT system,post-editing and dictionary extraction: real-cased,  untokenized NIST Bleu scores on the full newstest2009 set(%)", "labels": [], "entities": [{"text": "dictionary extraction", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.6423415839672089}, {"text": "NIST", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.6120768189430237}, {"text": "Bleu", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.530406653881073}, {"text": "newstest2009 set", "start_pos": 145, "end_pos": 161, "type": "DATASET", "confidence": 0.819285124540329}]}, {"text": " Table 4: Results of dictionary extraction for English-French: real-cased, untokenized NIST Bleu scores  (%)", "labels": [], "entities": [{"text": "dictionary extraction", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7259644567966461}, {"text": "untokenized NIST Bleu scores", "start_pos": 75, "end_pos": 103, "type": "METRIC", "confidence": 0.5074802190065384}]}]}