{"title": [{"text": "Dependency Parsing with Energy-based Reinforcement Learning", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8446161448955536}]}], "abstractContent": [{"text": "We present a model which integrates dependency parsing with reinforcement learning based on Markov decision process.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8165708482265472}]}, {"text": "At each time step, a transition is picked up to construct the dependency tree in terms of the long-run reward.", "labels": [], "entities": []}, {"text": "The optimal policy for choosing transitions can be found with the SARSA algorithm.", "labels": [], "entities": []}, {"text": "In SARSA, an approximation of the state-action function can be obtained by calculating the negative free energies for the Restricted Boltzmann Machine.", "labels": [], "entities": [{"text": "SARSA", "start_pos": 3, "end_pos": 8, "type": "TASK", "confidence": 0.7991814613342285}]}, {"text": "The experimental results on CoNLL-X multilingual data show that the proposed model achieves comparable results with the current state-of-the-art methods.", "labels": [], "entities": [{"text": "CoNLL-X multilingual data", "start_pos": 28, "end_pos": 53, "type": "DATASET", "confidence": 0.7951160271962484}]}], "introductionContent": [{"text": "Dependency parsing, an important task, can be used to facilitate some natural language applications.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8385090529918671}]}, {"text": "Given a sentence, dependency parsing is to find an acyclic labeled directed tree, projective or non-projective.The label of each edge gives the syntactic relationship between two words.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7847173810005188}]}, {"text": "Data-driven dependency parsers can be categorized into graph-based and transition-based models.", "labels": [], "entities": [{"text": "Data-driven dependency parsers", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5677893459796906}]}, {"text": "Both of these two models have their advantages as well as drawbacks.", "labels": [], "entities": []}, {"text": "As discussed in, transition-based models use local training and greedy inference algorithms, with a rich feature set, whereas they might lead to error propagation.", "labels": [], "entities": []}, {"text": "In contrast, graph-based models are globally trained coupled with exact inference algorithms, whereas their features are restricted to a limited number of graph arcs.", "labels": [], "entities": []}, {"text": "presented a successful attempt to integrate these two models by exploiting their complementary strengths.", "labels": [], "entities": []}, {"text": "There are other researches on improving the individual model with a novel framework.", "labels": [], "entities": []}, {"text": "For example, applied a greedy search to transition-based model, which was adjusted by the resulting errors.", "labels": [], "entities": []}, {"text": "Motivated by his work, our transition-based model is expected to overcome local dependencies by using a long-term desirability introduced by reinforcement learning (RL).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 141, "end_pos": 168, "type": "TASK", "confidence": 0.702598512172699}]}, {"text": "We rely on a \"global\" policy to guide each action selection fora particular state during parsing.", "labels": [], "entities": []}, {"text": "This policy considers not only the current configuration but also a few of look-ahead steps.", "labels": [], "entities": []}, {"text": "Thus it yields an optimal action from the longterm goal.", "labels": [], "entities": []}, {"text": "For example, an action might return a high value even if it produces a low immediate reward, because its following state-actions might yield high rewards.", "labels": [], "entities": []}, {"text": "The reverse also holds true.", "labels": [], "entities": []}, {"text": "Finally we formulate the parsing problem with the Markov Decision Process (MDP) for the dynamic settings.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9799351096153259}]}, {"text": "The reminder of this paper is organized as follows: Section 2 describes the transition-based dependency parsing.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 76, "end_pos": 111, "type": "TASK", "confidence": 0.5995988051096598}]}, {"text": "Section 3 presents the proposed reinforcement learning model.", "labels": [], "entities": []}, {"text": "Section 4 gives the experimental results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of dependency accuracy with Nivre", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9194245338439941}, {"text": "Nivre", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.7570158839225769}]}]}