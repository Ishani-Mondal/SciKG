{"title": [{"text": "Experiments Using OSTIA fora Language Production Task", "labels": [], "entities": [{"text": "Language Production", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7570167779922485}]}], "abstractContent": [{"text": "The phenomenon of meaning-preserving corrections given by an adult to a child involves several aspects: (1) the child produces an incorrect utterance, which the adult nevertheless understands, (2) the adult produces a correct utterance with the same meaning and (3) the child recognizes the adult utterance as having the same meaning as its previous utterance, and takes that as a signal that its previous utterance is not correct according to the adult grammar.", "labels": [], "entities": []}, {"text": "An adequate model of this phenomenon must incorporate utterances and meanings, account for how the child and adult can understand each other's meanings , and model how meaning-preserving corrections interact with the child's increasing mastery of language production.", "labels": [], "entities": []}, {"text": "In this paper we are concerned with how a learner who has learned to comprehend utterances might go about learning to produce them.", "labels": [], "entities": []}, {"text": "We consider a model of language comprehension and production based on finite sequential and subsequential transducers.", "labels": [], "entities": []}, {"text": "Utterances are modeled as finite sequences of words and meanings as finite sequences of predicates.", "labels": [], "entities": []}, {"text": "Comprehension is interpreted as a mapping of utterances to meanings and production as a mapping of meanings to utterances.", "labels": [], "entities": []}, {"text": "Previous work (Castel-lanos et al., 1993; Pieraccini et al., 1993) has applied subsequential transducers and the OSTIA algorithm to the problem of learning to comprehend language; here we apply them to the problem of learning to produce language.", "labels": [], "entities": []}, {"text": "For ten natural languages and a limited domain of geometric shapes and their properties and relations we define sequential transducers to produce pairs consisting of an utterance in that language and its meaning.", "labels": [], "entities": []}, {"text": "Using this data we empirically explore the properties of the OSTIA and DD-OSTIA algorithms for the tasks of learning comprehension and production in this domain, to assess whether they may provide a basis fora model of meaning-preserving corrections .", "labels": [], "entities": [{"text": "meaning-preserving corrections", "start_pos": 219, "end_pos": 249, "type": "TASK", "confidence": 0.7653839886188507}]}], "introductionContent": [{"text": "The role of corrections in language learning has recently received substantial attention in Grammatical Inference.", "labels": [], "entities": [{"text": "Grammatical Inference", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.9639595448970795}]}, {"text": "The kinds of corrections considered are mainly syntactic corrections based on proximity between strings.", "labels": [], "entities": []}, {"text": "For example, a correction of a string maybe given by using edit distance or based on the shortest extension of the queried string), among others.", "labels": [], "entities": []}, {"text": "In these approaches semantic information is not used.", "labels": [], "entities": []}, {"text": "However, in natural situations, a child's erroneous utterances are corrected by her parents based on the meaning that the child intends to express; typically, the adult's corrections preserve the intended meaning of the child.", "labels": [], "entities": []}, {"text": "Adults use corrections in part as away of making sure they have understood the child's intentions, in order to keep the conversation \"on track\".", "labels": [], "entities": []}, {"text": "Thus the child's utterance and the adult's correction have the same meaning, but the form is different.", "labels": [], "entities": []}, {"text": "As Chouinard and Clark point out, because children attend to contrasts inform, any change inform that does not mark a different meaning will signal to children that they may have produced something that is not acceptable in the target language.", "labels": [], "entities": []}, {"text": "Results in show that adults reformulate erroneous child utterances often enough to help learning.", "labels": [], "entities": []}, {"text": "Moreover, these re-sults show that children cannot only detect differences between their own utterance and the adult reformulation, but that they do make use of that information.", "labels": [], "entities": []}, {"text": "Thus in some natural situations, corrections have a semantic component that has not been taken into account in previous Grammatical Inference studies.", "labels": [], "entities": [{"text": "Grammatical Inference", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.853195458650589}]}, {"text": "Some interesting questions arise: What are the effects of corrections on learning syntax?", "labels": [], "entities": []}, {"text": "Can corrections facilitate the language learning process?", "labels": [], "entities": []}, {"text": "One of our long-term goals is to find a formal model that gives an account of this kind of correction and in which we can address these questions.", "labels": [], "entities": []}, {"text": "Moreover, such a model might allow us to show that semantic information can simplify the problem of learning formal languages.", "labels": [], "entities": []}, {"text": "A simple computational model of semantics and context for language learning incorporating semantics was proposed in).", "labels": [], "entities": []}, {"text": "This model accommodates two different tasks: comprehension and production.", "labels": [], "entities": []}, {"text": "That paper focused only on the comprehension task and formulated the learning problem as follows.", "labels": [], "entities": []}, {"text": "The teacher provides to the learner several example pairs consisting of a situation and an utterance denoting something in the situation; the goal of the learner is to learn the meaning function, allowing the learner to comprehend novel utterances.", "labels": [], "entities": []}, {"text": "The results in that paper show that under certain assumptions, a simple algorithm can learn to comprehend an adult's utterance in the sense of producing the same sequence of predicates, even without mastering the adult's grammar.", "labels": [], "entities": []}, {"text": "For example, receiving the utterance the blue square above the circle, the learner would be able to produce the sequence of predicates (bl, sq, ab, ci).", "labels": [], "entities": []}, {"text": "In this paper we focus on the production task, using sequential and subsequential transducers to model both comprehension and production.", "labels": [], "entities": []}, {"text": "Adult production can be modeled as converting a sequence of predicates into an utterance, which can be done with access to the meaning transducer for the adult's language.", "labels": [], "entities": []}, {"text": "However, we do not assume that the child initially has access to the meaning transducer for the adult's language; instead we assume that the child's production progresses through different stages.", "labels": [], "entities": []}, {"text": "Initially, child production is modeled as consisting of two different tasks: finding a correct sequence of predicates, and inverting the meaning function to produce a kind of \"telegraphic speech\".", "labels": [], "entities": []}, {"text": "For example, from (gr, tr, le, sq) the child may produce green triangle left square.", "labels": [], "entities": []}, {"text": "Our goal is to model how the learner might move from this telegraphic speech to speech that is grammatical in the adult's sense.", "labels": [], "entities": []}, {"text": "Moreover, we would like to find a formal framework in which corrections (in form of expansions, for example, the green triangle to the left of the square) can be given to the child during the intermediate stages (before the learner is able to produce grammatically correct utterances) to study their effect on language learning.", "labels": [], "entities": []}, {"text": "We thus propose to model the problem of child language production as a machine translation problem, that is, as the task of translating a sequence of predicate symbols (representing the meaning of an utterance) into a corresponding utterance in a natural language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7609108090400696}]}, {"text": "In this paper we explore the possibility of applying existing automata-theoretic approaches to machine translation to model language production.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7297634482383728}]}, {"text": "In Section 2, we describe the use of subsequential transducers for machine translation tasks and review the OS-TIA algorithm to learn them.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.8338089982668558}]}, {"text": "In Section 3, we present our model of how the learner can move from telegraphic to adult speech.", "labels": [], "entities": []}, {"text": "In Section 4, we present the results of experiments in the model made using OSTIA.", "labels": [], "entities": [{"text": "OSTIA", "start_pos": 76, "end_pos": 81, "type": "DATASET", "confidence": 0.7927135825157166}]}, {"text": "Discussion of these results is presented in Section 5 and ideas for future work are in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments were made fora limited domain of geometric shapes and their properties and relations.", "labels": [], "entities": []}, {"text": "This domain is a simplification of the Miniature Language Acquisition task proposed by).", "labels": [], "entities": [{"text": "Miniature Language Acquisition task", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.8269163817167282}]}, {"text": "Previous applications of OSTIA to language understanding and machine translation have also used adaptations and extensions of the Feldman task.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7489389181137085}, {"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7500259578227997}]}, {"text": "In our experiments, we have predicates for three different shapes (circle (ci), square (sq) and triangle (tr)), three different colors (blue (bl), green (gr) and red (re)) and three different relations (to the left of (le), to the right of (ler), and above (ab)).", "labels": [], "entities": []}, {"text": "We consider ten different natural languages: Arabic, English, Greek, Hebrew, Hindi, Hungarian, Mandarin, Russian, Spanish and Turkish.", "labels": [], "entities": []}, {"text": "We created a data sequence of input-output pairs, each consisting of a predicate sequence and a natural language utterance.", "labels": [], "entities": []}, {"text": "For example, one pair for Spanish is ((ci, re, ler, tr), el circulo rojo a la derecha del triangulo).", "labels": [], "entities": []}, {"text": "We ran OSTIA on initial segments of the sequence of pairs, of lengths 10, 20, 30, . .", "labels": [], "entities": [{"text": "OSTIA", "start_pos": 7, "end_pos": 12, "type": "TASK", "confidence": 0.7205134034156799}]}, {"text": "., to produce a sequence of subsequential transducers.", "labels": [], "entities": []}, {"text": "The whole data sequence was used to test the correctness of the transducers generated during the process.", "labels": [], "entities": []}, {"text": "An error is counted whenever given a data pair (x, y), the subsequential transducer translates x toy , and y = y.", "labels": [], "entities": []}, {"text": "We say that OSTIA has converged to a correct transducer if all the transducers produced afterwards have the same number of states and edges, and 0 errors on the whole data sequence.", "labels": [], "entities": []}, {"text": "To generate the sequences of input-output pairs, for each language we constructed a meaning transducer capable of producing the 444 different possible meanings involving one or two objects.", "labels": [], "entities": []}, {"text": "We randomly generated 400 unique (non-repeated) input-output pairs for each language.", "labels": [], "entities": []}, {"text": "This process was repeated 10 times.", "labels": [], "entities": []}, {"text": "In addition, to investigate the effect of the order of presentation of the inputoutput pairs, we repeated the data generation process for each language, sorting the pairs according to a length-lex ordering of the utterances.", "labels": [], "entities": []}, {"text": "We give some examples to illustrate the transducers produced.", "labels": [], "entities": []}, {"text": "shows an example of a transducer produced by OSTIA after just ten pairs of input-output examples for Spanish.", "labels": [], "entities": []}, {"text": "This transducer correctly translates the ten predicate sequences used to construct it, but the data is not sufficient for OSTIA to generalize correctly in all cases, and many other correct meanings are still incorrectly translated.", "labels": [], "entities": []}, {"text": "For example, the sequence (ci, bl) is translated as el circulo a la izquierda del circulo verde azul instead of el circulo azul.", "labels": [], "entities": []}, {"text": "The transducers produced after convergence by OSTIA and DD-OSTIA correctly translate all 444 possible correct meanings.", "labels": [], "entities": [{"text": "OSTIA", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.765421450138092}]}, {"text": "Examples for Spanish are shown in (OSTIA) and (DD-OSTIA).", "labels": [], "entities": []}, {"text": "Note that although they correctly translate all 444 correct meanings, the behavior of these two transducers on other (incorrect) predicate sequences is different, for example on (tr, tr).: Production task, DD-OSTIA.", "labels": [], "entities": []}, {"text": "A transducer produced (after convergence) using random unique input-output pairs (predicate-sequence, utterance) for Spanish.", "labels": [], "entities": []}, {"text": "tasks , we also tried using DD-OSTIA for learning to translate a sequence of predicates to an utterance.", "labels": [], "entities": [{"text": "translate a sequence of predicates to an utterance", "start_pos": 53, "end_pos": 103, "type": "TASK", "confidence": 0.7881323024630547}]}, {"text": "We used the same sequences of input-output pairs as in the previous experiment.", "labels": [], "entities": []}, {"text": "The results obtained are shown in Table 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Production task, OSTIA. The entries give  the median number of input-output pairs until con- vergence in 10 runs. For Greek, Hindi and Hun- garian, the median for the unsorted case is calcu- lated using all 444 random unique pairs, instead of  400.", "labels": [], "entities": []}, {"text": " Table 2: Production task, DD-OSTIA. The entries  give the median number of input-output pairs un- til convergence in 10 runs. For Greek, Hindi and  Hungarian, the median for the unsorted case is cal- culated using all 444 random unique pairs, instead  of 400.", "labels": [], "entities": []}, {"text": " Table 3: Production task, OSTIA. Sizes of trans- ducers at convergence.", "labels": [], "entities": [{"text": "OSTIA", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.4061931371688843}]}, {"text": " Table 4: Production task, DD-OSTIA. Sizes of  transducers at convergence.", "labels": [], "entities": []}, {"text": " Table 5: Comprehension task, OSTIA and DD- OSTIA. Median number (in 10 runs) of input- output pairs until convergence using a sequence of  400 random unique pairs of (utterance, predicate  sequence).", "labels": [], "entities": []}, {"text": " Table 6: Comprehension task, OSTIA and DD- OSTIA. Sizes of transducers at convergence using  400 random unique input-output pairs (utterance,  predicate sequence). In cases of disagreement, the  number reported is the mode.", "labels": [], "entities": []}]}