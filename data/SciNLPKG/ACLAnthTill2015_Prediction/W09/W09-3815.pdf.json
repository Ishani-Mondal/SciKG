{"title": [{"text": "Weight pushing and binarization for fixed-grammar parsing", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.8277916312217712}]}], "abstractContent": [{"text": "We apply the idea of weight pushing (Mohri, 1997) to CKY parsing with fixed context-free grammars.", "labels": [], "entities": [{"text": "weight pushing (Mohri, 1997)", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.7145531943866185}, {"text": "CKY parsing", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.6631996631622314}]}, {"text": "Applied after rule binarization, weight pushing takes the weight from the original grammar rule and pushes it down across its binarized pieces, allowing the parser to make better pruning decisions earlier in the parsing process.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 212, "end_pos": 227, "type": "TASK", "confidence": 0.8991813957691193}]}, {"text": "This process can be viewed as generalizing weight pushing from transducers to hypergraphs.", "labels": [], "entities": [{"text": "generalizing weight pushing", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6645863056182861}]}, {"text": "We examine its effect on parsing efficiency with various bi-narization schemes applied to tree substitution grammars from previous work.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.972357988357544}]}, {"text": "We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars.", "labels": [], "entities": [{"text": "weight pushing", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.8021424412727356}]}], "introductionContent": [{"text": "Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time.", "labels": [], "entities": []}, {"text": "This is in contrast to markovized grammars), variants of tree-adjoining grammars), or grammars with wildcard rules), all of which allow the construction and use of rules not seen in the training data.", "labels": [], "entities": []}, {"text": "Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n 3 |G|) (n the sentence length, |G| the grammar size) complexity of algorithms such as the CKY algorithm.", "labels": [], "entities": [{"text": "O", "start_pos": 92, "end_pos": 93, "type": "METRIC", "confidence": 0.99685138463974}]}, {"text": "Recently, explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), showing that binarization has a significant effect on both the number of rules and new nonterminals introduced, and subsequently on parsing time.", "labels": [], "entities": [{"text": "Penn Treebank (the Treebank PCFG)", "start_pos": 86, "end_pos": 119, "type": "DATASET", "confidence": 0.8666898012161255}, {"text": "parsing", "start_pos": 253, "end_pos": 260, "type": "TASK", "confidence": 0.9770509600639343}]}, {"text": "This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar.", "labels": [], "entities": []}, {"text": "Increasing sharing reduces the amount of state that the parser must explore.", "labels": [], "entities": []}, {"text": "Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (.", "labels": [], "entities": [{"text": "Binarization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9294849038124084}, {"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7900934517383575}]}, {"text": "The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.969670832157135}]}, {"text": "In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses.", "labels": [], "entities": []}, {"text": "This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0.", "labels": [], "entities": [{"text": "bottom-up parsing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.5261342525482178}]}, {"text": "Because these rules are reconstructed from the bottom up, pruning procedures do not have a good estimate of the complete cost of a rule until the entire original rule has been reconstructed.", "labels": [], "entities": []}, {"text": "It is preferable to have this information earlier on, especially for larger rules.", "labels": [], "entities": []}, {"text": "In this paper we adapt the technique of weight pushing for finite state transducers to arbitrary binarizations of context-free grammar rules.", "labels": [], "entities": []}, {"text": "Weight pushing takes the probability (or, more generally, the weight) of a rule in the original grammar and pushes it down across the rule's binarized pieces.", "labels": [], "entities": [{"text": "Weight pushing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6912408024072647}]}, {"text": "This helps the parser make bet-ter pruning decisions, and to make them earlier in the bottom-up parsing process.", "labels": [], "entities": [{"text": "parser", "start_pos": 15, "end_pos": 21, "type": "TASK", "confidence": 0.9722295999526978}]}, {"text": "We investigate this algorithm with different binarization schemes and grammars, and find that it improves the time vs. accuracy tradeoff for parsers roughly proportionally to the size of the grammar being binarized.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8891744613647461}]}, {"text": "This paper extends the work of in three ways.", "labels": [], "entities": []}, {"text": "First, weight pushing further reduces the amount of time required for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9862263798713684}]}, {"text": "Second, we apply these techniques to Tree Substitution Grammars (TSGs) learned from the Treebank, which are both larger and more accurate than the context-free grammar read directly from the Treebank.", "labels": [], "entities": []}, {"text": "1 Third, we examine the interaction between binarization schemes and the inexact search heuristic of beam-based and k-best pruning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present results from four different grammars: 1.", "labels": [], "entities": []}, {"text": "The standard Treebank probabilistic contextfree grammar (PCFG).", "labels": [], "entities": []}, {"text": "2. A \"spinal\" tree substitution grammar (TSG), produced by extracting n lexicalized subtrees from each length n sentence in the training data.", "labels": [], "entities": [{"text": "spinal\" tree substitution grammar (TSG)", "start_pos": 6, "end_pos": 45, "type": "TASK", "confidence": 0.8142962530255318}]}, {"text": "Each subtree is defined as the sequence of CFG rules from leaf upward all sharing the same lexical head, according to the Magerman head-selection rules  are summed.", "labels": [], "entities": []}, {"text": "From these counts we remove (a) all unlexicalized subtrees of height greater than six and (b) all lexicalized subtrees containing more than twelve terminals on their frontier, and we add all subtrees of height one (i.e., the Treebank PCFG).", "labels": [], "entities": [{"text": "Treebank PCFG", "start_pos": 225, "end_pos": 238, "type": "DATASET", "confidence": 0.921877920627594}]}], "tableCaptions": [{"text": " Table 1: Grammar statistics. A rule's rank is the  number of symbols on its right-hand side.", "labels": [], "entities": []}]}