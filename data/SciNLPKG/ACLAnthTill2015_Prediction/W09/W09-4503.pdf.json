{"title": [{"text": "A Joint Model for Normalizing Gene and Organism Mentions in Text", "labels": [], "entities": [{"text": "Normalizing Gene and Organism Mentions", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.865804934501648}]}], "abstractContent": [{"text": "The aim of gene mention normalization is to propose an appropriate canonical name, or an iden-tifier from a popular database, fora gene or a gene product mentioned in a given piece of text.", "labels": [], "entities": [{"text": "gene mention normalization", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.648982306321462}]}, {"text": "The task has attracted a lot of research attention for several organisms under the assumption that both the mention boundaries and the target organism are known.", "labels": [], "entities": []}, {"text": "Here we extend the task to also recognizing whether the gene mention is valid and to finding the organism it is from.", "labels": [], "entities": []}, {"text": "We solve this extended task using a joint model for gene and organism name normaliza-tion which allows for instances from different organisms to share features, thus achieving sizable performance gains with different learning methods: Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, Maximum Entropy, Percep-tron and mira, as well as averaged versions of the last two.", "labels": [], "entities": [{"text": "Percep-tron", "start_pos": 272, "end_pos": 283, "type": "METRIC", "confidence": 0.9328513741493225}]}, {"text": "The evaluation results for our joint classifier show F1 score of over 97%, which proves the potential of the approach.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9891036748886108}]}], "introductionContent": [{"text": "Gene mention normalization is one of the emerging tasks in bio-medical text processing along with gene mention tagging, protein-protein interaction, and biomedical event extraction.", "labels": [], "entities": [{"text": "Gene mention normalization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7771704792976379}, {"text": "bio-medical text processing", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6242469946543375}, {"text": "gene mention tagging", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.6214699546496073}, {"text": "biomedical event extraction", "start_pos": 153, "end_pos": 180, "type": "TASK", "confidence": 0.6789080401261648}]}, {"text": "The objective is to propose an appropriate canonical name, or a unique identifier from a predefined list, for each gene or gene product name mentioned in a given piece of text.", "labels": [], "entities": []}, {"text": "Solving this task is important for many practical application, e.g., enriching high precision databases such as the Protein and Interaction Knowledge Base (pikb), part of LinkedLifeData 1 , or compiling gene-related search indexes for large document collections such as LifeSKIM 2 and medie 3 . In this work, we focus on the preparation of good training data and on improving the performance of the normalization classifier rather than on building an integrated solution for gene mention normalization.", "labels": [], "entities": [{"text": "gene mention normalization", "start_pos": 475, "end_pos": 501, "type": "TASK", "confidence": 0.6674361030260721}]}, {"text": "The remainder of the paper is organized as follows: Section 2 gives an overview of the related work, Section 3 present our method, Section 4 describes the experiments and discusses the results, and Section 5 concludes and suggests directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Below we describe our experiments in evaluating the joint model described in Section 3.4 on the standard test sets of BioCreAtIvE.", "labels": [], "entities": [{"text": "BioCreAtIvE", "start_pos": 118, "end_pos": 129, "type": "DATASET", "confidence": 0.8509187698364258}]}, {"text": "We tried Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, Maximum Entropy, Perceptron, mira, as well as averaged versions of the last two.", "labels": [], "entities": []}, {"text": "Since the training data for this task were limited, we also studied the dependence of each classifier on the number of training examples from both manually annotated and noisy data sources.", "labels": [], "entities": []}, {"text": "shows the performance of our six classifiers as a function of the number of manually annotated training examples for mouse.", "labels": [], "entities": []}, {"text": "Maximum Entropy, averaged mira and Perceptron outperformed the rest by 3-4% of F 1 , in the range of 2,000-2,600 training examples.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9989192485809326}, {"text": "F 1", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9768549799919128}]}, {"text": "For the case of limited training data, in the range of 100-200 examples, the best-scoring classifier was Maximum Entropy.", "labels": [], "entities": []}, {"text": "As shows, for the human training data, in the range of 2,000-3,000 manually annotated examples, the best classifiers were again Maximum Entropy and the averaged Perceptron, and for 100-200 training examples, Maximum Entropy and averaged mira were tied for the first place.", "labels": [], "entities": []}, {"text": "The Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier was the worse-performing one for both the 100-200 and 2,000-3,000 ranges.", "labels": [], "entities": []}, {"text": "As a second set of experiments, we combined the mouse and the human training examples, and we used a multi-class version of the learning schemata to train and evaluate the joint mouse-human statistical model that has been described above.", "labels": [], "entities": []}, {"text": "shows the performance for different numbers of training examples for the joint model.", "labels": [], "entities": []}, {"text": "Again, the Maximum Entropy and the averaged Perceptron outperformed the remaining classifiers in the full range of numbers of training examples; Perceptron scored third in this experiment.", "labels": [], "entities": [{"text": "Maximum Entropy", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.7509574890136719}]}, {"text": "shows the data for Maximum Entropy, Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, Perceptron and mira presented already in detail on, e.g., the evaluation is presented separately for mouse and human and in terms of precision, recall and F 1 -measure.", "labels": [], "entities": [{"text": "Maximum Entropy", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.8640382289886475}, {"text": "Perceptron", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9428791403770447}, {"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9996826648712158}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9996211528778076}, {"text": "F 1 -measure", "start_pos": 211, "end_pos": 223, "type": "METRIC", "confidence": 0.9805393815040588}]}, {"text": "Note that all learning methods show well-balanced precision and recall, which is a very desirable property fora gene mention normalization system.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9993708729743958}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9994113445281982}]}, {"text": "The best performing classifier for the joint model when tested on human examples was Maximum Entropy -it outperformed the rest by more than 2% absolute difference in F 1 -measure.", "labels": [], "entities": [{"text": "Maximum Entropy", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.8700326681137085}, {"text": "F 1 -measure", "start_pos": 166, "end_pos": 178, "type": "METRIC", "confidence": 0.9714673012495041}]}, {"text": "For mouse, mira was the best, directly followed by Maximum Entropy.", "labels": [], "entities": []}, {"text": "In order to boost the performance of the classifier even further, we added to the model the additional noisy training examples that were provided by the BioCreAtIvE organizers for both human and mouse.", "labels": [], "entities": []}, {"text": "For this set of experiments, we selected the Maximum Entropy classifier, and we achieved an absolute increase in F 1 score of more than 12% and 7% for mouse and human test examples respectively (see, column A).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9912435015042623}]}, {"text": "In our last experiment, we used the feature function decomposition as described in Section 3.4, which resulted in further improvement to reach the final F 1 score for the Maximum Entropy classifier of 97.09% and 97.64% for mouse and human respectively (see, Column B ).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9864510297775269}]}, {"text": "human-mouse model, with regular ( A) and decomposed ( B) feature functions.", "labels": [], "entities": []}, {"text": "Precision (P), recall (R) and F1-measure are shown in %.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9477571547031403}, {"text": "recall (R)", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.964106485247612}, {"text": "F1-measure", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9987567663192749}]}], "tableCaptions": [{"text": " Table 2: Evaluation of the joint human-mouse model with different classifiers. Precision (P), recall (R) and  F 1 -measure are shown in %.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 80, "end_pos": 93, "type": "METRIC", "confidence": 0.9616015553474426}, {"text": "recall (R)", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.96075139939785}, {"text": "F 1 -measure", "start_pos": 111, "end_pos": 123, "type": "METRIC", "confidence": 0.9855928122997284}]}]}