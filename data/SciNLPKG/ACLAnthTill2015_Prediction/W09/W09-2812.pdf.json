{"title": [{"text": "Reducing redundancy in multi-document summarization using lexical semantic similarity", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6149541139602661}]}], "abstractContent": [{"text": "We present an automatic multi-document summarization system for Dutch based on the MEAD system.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.5078267008066177}, {"text": "MEAD system", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.8640662431716919}]}, {"text": "We focus on redundancy detection, an essential ingredient of multi-document summarization.", "labels": [], "entities": [{"text": "redundancy detection", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7901628017425537}, {"text": "multi-document summarization", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.677930623292923}]}, {"text": "We introduce a semantic overlap detection tool, which goes beyond simple string matching.", "labels": [], "entities": [{"text": "semantic overlap detection", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.8168017665545145}, {"text": "string matching", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.76998570561409}]}, {"text": "Our results so far do not confirm our expectation that this tool would out-perform the other tested methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the main issues in automatic multidocument summarization is avoiding redundancy.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6151977479457855}]}, {"text": "As the source documents are all related to the same topic, at least some of their content is likely to overlap.", "labels": [], "entities": []}, {"text": "In fact, this is in part what makes multi-document summarization feasible.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.5731893479824066}]}, {"text": "For example, news articles that report on a particular event, or that are based on the same source, often contain similar information expressed in different ways.", "labels": [], "entities": []}, {"text": "A multi-document summarizer should include this overlapping information not more than once.", "labels": [], "entities": []}, {"text": "The backbone of most current approaches to automatic summarization is a vector space model in which a sentence is regarded as a bag of words and a weighted cosine similarity measure is used to quantify the amount of shared information between a pair of sentences.", "labels": [], "entities": [{"text": "automatic summarization", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6059821844100952}]}, {"text": "Cosine similarity (in this context) essentially amounts to calculating word overlap, albeit with weighting of the terms and normalization for differences in sentence length.", "labels": [], "entities": [{"text": "calculating word overlap", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7451026539007822}]}, {"text": "It is clear that this approach to detecting redundancy is far from satisfactory, because it only covers redundancy in its most trivial form, i.e., identical words.", "labels": [], "entities": [{"text": "detecting redundancy", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.9160000085830688}]}, {"text": "In contrast, the redundancy that we ultimately want to avoid in summarization is that at the semantic level.", "labels": [], "entities": [{"text": "summarization", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9821141362190247}]}, {"text": "As an extreme casein point, two sentences with no words in common can still carry virtually the same meaning.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured in the following way.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce a tool for detecting semantic overlap.", "labels": [], "entities": [{"text": "detecting semantic overlap", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.8166614373524984}]}, {"text": "In section 3 we present a Dutch multi-document summarization system, based on the MEAD summarization toolkit ().", "labels": [], "entities": [{"text": "Dutch multi-document summarization", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.43326114614804584}, {"text": "MEAD summarization", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.6283082067966461}]}, {"text": "Next, in section 4 we describe the experimental setup and the data set that we used.", "labels": [], "entities": []}, {"text": "Section 5 reports on the results, and we conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To perform proper evaluation of the summarization system we constructed anew data set for evaluating Dutch multi-document summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9799869656562805}, {"text": "Dutch multi-document summarization", "start_pos": 101, "end_pos": 135, "type": "TASK", "confidence": 0.46466659506162006}]}, {"text": "It consists of 30 query-based document clusters.", "labels": [], "entities": []}, {"text": "The document clusters were created manually following the guidelines of DUC 2006).", "labels": [], "entities": [{"text": "DUC 2006", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9416273832321167}]}, {"text": "Each cluster contains a query description and 5 to 25 newspaper articles relevant for that particular question.", "labels": [], "entities": []}, {"text": "For each cluster five annotators wrote an abstract of approximately 250 words.", "labels": [], "entities": []}, {"text": "These summaries serve as a gold standard for comparison with automatically generated extracts.", "labels": [], "entities": []}, {"text": "We split our data set in a test set of 20 clusters and a development set of 10 clusters.", "labels": [], "entities": []}, {"text": "We use the development set for parameter tuning and feature selection for the summarizer.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6876032948493958}]}, {"text": "We tryout each of the characteristics discussed in section 3.", "labels": [], "entities": []}, {"text": "The best combination found on the development set is the feature combination position, centroid, length with cut-off 13, and queryCosine.", "labels": [], "entities": [{"text": "length", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9917071461677551}]}, {"text": "We tested the different rerankers and vary the similarity thresholds to determine their optimal threshold value.", "labels": [], "entities": []}, {"text": "As the novelty-reranker scored lower than the other rerankers on the development set, we did not include it in our experiments on the test set.", "labels": [], "entities": []}, {"text": "For the experiments on the development set, we compare each of the automatically produced extracts with five manually written summaries and report macro-average Rouge-2 and Rouge-SU4 scores (.", "labels": [], "entities": []}, {"text": "For the experiments on the test set, we also perform a manual evaluation.", "labels": [], "entities": []}, {"text": "We follow the DUC 2006 guidelines for manual evaluation of responsiveness and the linguistic quality of the produced summaries.", "labels": [], "entities": [{"text": "DUC 2006", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9418290257453918}]}, {"text": "The responsiveness scores express the information content of the summary with respect to the query.", "labels": [], "entities": []}, {"text": "The linguistic quality is evaluated on five different objectives: grammaticality, non-redundancy, coherence, referential clarity and focus.", "labels": [], "entities": [{"text": "clarity", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.8348535895347595}]}, {"text": "The annotators can choose a value on a five point scale where 1 means 'very poor' and 5 means 'very good'.", "labels": [], "entities": []}, {"text": "We use two independent annotators to evaluate the summaries and we report the average scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Macro-average Rouge scores and manual evaluation on the test set on these aspects:  grammaticality, non-redundancy, referential clarity, focus, structure and responsiveness.", "labels": [], "entities": []}]}