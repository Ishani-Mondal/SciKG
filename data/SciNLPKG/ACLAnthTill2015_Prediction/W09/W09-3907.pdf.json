{"title": [{"text": "Towards the Interpretation of Utterance Sequences in a Dialogue System", "labels": [], "entities": [{"text": "Interpretation of Utterance Sequences", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.7294079512357712}]}], "abstractContent": [{"text": "This paper describes a probabilistic mechanism for the interpretation of sentence sequences developed fora spoken dialogue system mounted on a robotic agent.", "labels": [], "entities": []}, {"text": "The mechanism receives as input a sequence of sentences, and produces an interpretation which integrates the interpretations of individual sentences.", "labels": [], "entities": []}, {"text": "For our evaluation, we collected a corpus of hypothetical requests to a robot.", "labels": [], "entities": []}, {"text": "Our mechanism exhibits good performance for sentence pairs, but requires further improvements for sentence sequences.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We first describe our experimental set-up, followed by our results.", "labels": [], "entities": []}, {"text": "We conducted a web-based survey to collect a corpus comprising multi-sentence requests.", "labels": [], "entities": []}, {"text": "To this effect, we presented participants with a scenario where they are in a meeting room, and they ask a robot to fetch something from their office.", "labels": [], "entities": []}, {"text": "The idea is that if people cannot see a scene, their instructions will be more segmented than if they can view the scene.", "labels": [], "entities": []}, {"text": "The participants were free to decide which object to fetch, and what was in the office.", "labels": [], "entities": []}, {"text": "There were no restrictions on vocabulary or grammatical form for the requests.", "labels": [], "entities": []}, {"text": "We collected 115 sets of instructions mostly from different participants (a few people did the survey more than once).", "labels": [], "entities": []}, {"text": "The sentence sequences in our corpus contain between 1 and 9 sentences, with 74% of the sequences comprising 1 to 3 sentences.", "labels": [], "entities": []}, {"text": "Many of the sentences had grammatical requirements which exceeded the capabilities of our system.", "labels": [], "entities": []}, {"text": "To be able to use these instruction sets in our evaluation, we made systematic manual changes to produce sentences that meet our system's grammatical restrictions (in the future, we We acknowledge the modest size of our corpus compared to that of some publicly available corpora, e.g., ATIS.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 286, "end_pos": 290, "type": "DATASET", "confidence": 0.9392645359039307}]}, {"text": "However, we must generate our own corpus since our task differs in nature from the tasks where these large corpora are used.", "labels": [], "entities": []}, {"text": "will relax these restrictions, as required by a deployable system).", "labels": [], "entities": []}, {"text": "Below are the main types of changes we made.", "labels": [], "entities": []}, {"text": "\u2022 Indirect Speech Acts in the form of questions were changed to imperatives.", "labels": [], "entities": []}, {"text": "For instance, \"Can you get my tea?\" was changed to \"Get my tea\".", "labels": [], "entities": []}, {"text": "\u2022 Conjoined verb phrases or sentences were separated into individual sentences.", "labels": [], "entities": []}, {"text": "\u2022 Composite verbs were simplified, e.g., \"I think I left it on\" was changed to \"it is on\", and outof-vocabulary composite nouns were replaced by simple nouns or adjectives, e.g., \"the diary is A4 size\" to \"the diary is big\".", "labels": [], "entities": []}, {"text": "\u2022 Conditional sentences were removed.", "labels": [], "entities": [{"text": "Conditional", "start_pos": 2, "end_pos": 13, "type": "METRIC", "confidence": 0.9787769913673401}]}, {"text": "shows two original texts compared with the corresponding modified texts (the changed portions in the originals have been italicized).", "labels": [], "entities": []}, {"text": "Our evaluation consists of two experiments: (1) ICGs for sentence pairs, and (2) UCGs for sentence sequences.", "labels": [], "entities": [{"text": "ICGs", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.8558265566825867}]}, {"text": "We extracted 106 sentence pairs from our corpus -each pair containing one declarative and one imperative sentence.", "labels": [], "entities": []}, {"text": "To evaluate the ICGs, we constructed a virtual environment comprising a main office and a small office).", "labels": [], "entities": []}, {"text": "Furniture and objects were placed in a manner compatible with what was mentioned in the requests in our corpus; distractors were also placed in the virtual space.", "labels": [], "entities": []}, {"text": "In total, our environment contains 183 instantiated concepts (109 office and household objects, 43 actions and 31 relations).", "labels": [], "entities": []}, {"text": "The (x, y, z) coordinates, colour and dimensions of these objects were stored in a knowledge base.", "labels": [], "entities": []}, {"text": "Since we have two sentences and their mode is known, no corpus-based information is used for this experiment, and hence no training is required.", "labels": [], "entities": []}, {"text": "Since UCGs contain only syntactic information, no additional setup was required.", "labels": [], "entities": []}, {"text": "However, for this experiment we need to train our mode classifier (Section 2.2), and estimate the probability distribution of referring to a particular sentence in a sequence (Section 2.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Scusi?'s interpretation performance", "labels": [], "entities": []}]}