{"title": [], "abstractContent": [{"text": "Named entity recognition (NER) is used in many domains beyond the newswire text that comprises current gold-standard corpora.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7998457998037338}]}, {"text": "Recent work has used Wikipedia's link structure to automatically generate near gold-standard annotations.", "labels": [], "entities": []}, {"text": "Until now, these resources have only been evaluated on newswire corpora or themselves.", "labels": [], "entities": []}, {"text": "We present the first NER evaluation on a Wikipedia gold standard (WG) corpus.", "labels": [], "entities": [{"text": "NER evaluation", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.8780234158039093}, {"text": "Wikipedia gold standard (WG) corpus", "start_pos": 41, "end_pos": 76, "type": "DATASET", "confidence": 0.8388299686568124}]}, {"text": "Our analysis of cross-corpus performance on WG shows that Wikipedia text maybe a harder NER domain than newswire.", "labels": [], "entities": [{"text": "WG", "start_pos": 44, "end_pos": 46, "type": "DATASET", "confidence": 0.9445636868476868}]}, {"text": "We find that an automatic annotation of Wikipedia has high agreement with WG and, when used as training data, outper-forms newswire models by up to 7.7%.", "labels": [], "entities": [{"text": "WG", "start_pos": 74, "end_pos": 76, "type": "DATASET", "confidence": 0.910997211933136}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is the task of identifying and classifying people, organisations and other named entities (NE) within text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7799089352289835}, {"text": "identifying and classifying people, organisations and other named entities (NE) within text", "start_pos": 46, "end_pos": 137, "type": "TASK", "confidence": 0.5968369086583455}]}, {"text": "NER is central to many NLP systems, especially information extraction and question answering.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7690746784210205}, {"text": "information extraction", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.8345613479614258}, {"text": "question answering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.8946791291236877}]}, {"text": "Machine learning approaches now dominate NER, learning patterns associated with individual entity classes from annotated training data.", "labels": [], "entities": [{"text": "NER", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9452638626098633}]}, {"text": "This training data, including English newswire from the MUC-6, MUC-7, and CONLL-03) competitive evaluation tasks, and the BBN Pronoun Coreference and Entity Type Corpus (, is critical to the success of these approaches.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9524713754653931}, {"text": "MUC-7", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.8575525879859924}, {"text": "CONLL-03", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.7982999086380005}, {"text": "BBN Pronoun Coreference and Entity Type Corpus", "start_pos": 122, "end_pos": 168, "type": "DATASET", "confidence": 0.7966031091553825}]}, {"text": "This data dependence has impeded the adaptation or porting of existing NER systems to new domains, such as scientific or biomedical text, e.g..", "labels": [], "entities": []}, {"text": "Similar domain sensitivity is exhibited by most tasks across NLP, e.g. parsing, and the adaptation penalty is still apparent even when the same set of named entity classes is used in text from similar domains.", "labels": [], "entities": [{"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.9635113477706909}]}, {"text": "Wikipedia is an important corpus for information extraction, e.g. and because of its size, currency, rich semi-structured content, and its closer resemblance to web text than newswire.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.728153333067894}]}, {"text": "Recently, Wikipedia's markup has been exploited to automatically derive NE annotated text for training statistical models (.", "labels": [], "entities": []}, {"text": "However, without a gold standard, existing evaluations of these models were forced to compare against mismatched newswire corpora or the noisy Wikipedia-derived annotations themselves.", "labels": [], "entities": []}, {"text": "Further, it was not possible to directly ascertain the accuracy of these automatic extraction methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.999387264251709}]}, {"text": "We have manually annotated 39,007 tokens of Wikipedia with coarse-grained named entity tags (WG).", "labels": [], "entities": []}, {"text": "We present the first evaluation of Wikipedia-trained models on Wikipedia: the C&C NER tagger) trained on (a) automatically annotated Wikipedia text (WP2) extracted by; and (b) traditional newswire NER corpora (MUC, CONLL and BBN).", "labels": [], "entities": [{"text": "BBN", "start_pos": 225, "end_pos": 228, "type": "DATASET", "confidence": 0.572006344795227}]}, {"text": "The WP2 model, though trained on noisy annotations, outperforms newswire models on WG by 7.7%.", "labels": [], "entities": [{"text": "WP2", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9009856581687927}, {"text": "WG", "start_pos": 83, "end_pos": 85, "type": "DATASET", "confidence": 0.9249643683433533}]}, {"text": "However, every model, including WP2, performs far worse on WG than on the newswire.", "labels": [], "entities": [{"text": "WP2", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9347853660583496}, {"text": "WG", "start_pos": 59, "end_pos": 61, "type": "DATASET", "confidence": 0.9769397974014282}]}, {"text": "We examined the quality of WG, and found that our annotation strategy produced a high-quality, consistent corpus.", "labels": [], "entities": [{"text": "WG", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.759285569190979}]}, {"text": "Our analysis suggests that it is the form and distribution of NEs in Wikipedia that make it a difficult target domain.", "labels": [], "entities": []}, {"text": "Finally, we compared WG with the annotations extracted by, and found agreement comparable to our inter-annotator agreement, demonstrating that NE corpora can be derived very accurately from Wikipedia.", "labels": [], "entities": [{"text": "agreement", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9842603802680969}]}], "datasetContent": [{"text": "Meaningful automatic evaluation of NER is difficult and a number of metrics have been proposed (.", "labels": [], "entities": [{"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9828663468360901}]}, {"text": "Ambiguity leads to entities correctly delimited but misclassified, or boundaries mismatched despite correct classification.", "labels": [], "entities": []}, {"text": "Although the MUC-7 evaluation) defined a metric which was less sensitive to often-meaningless boundary errors, we consider only exact entity matches as correct, following the standard CONLL evaluation).", "labels": [], "entities": [{"text": "MUC-7 evaluation", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.7951689660549164}]}, {"text": "We report precision, recall and F -score for each entity type.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9991245865821838}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9997333884239197}, {"text": "F -score", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.991436779499054}]}, {"text": "We compared the gold-standard annotations in our WG corpus to those sentences that were automatically annotated by.", "labels": [], "entities": [{"text": "WG corpus", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.9525469243526459}]}, {"text": "Their automatic annotation process does not retain all Wikipedia sentences.", "labels": [], "entities": []}, {"text": "Rather, it selects sentences where, on the basis of capitalisation heuristics, it seems all named entities in the sentence have been tagged by the automatic process.", "labels": [], "entities": []}, {"text": "We adopt this confidence criterion to produce automaticallyannotated subsets of the WG corpus.", "labels": [], "entities": [{"text": "WG corpus", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.910189688205719}]}, {"text": "Two variants of their automatic annotation procedure were used: WP2 uses a few rules to infer tags for non-linked NEs in Wikipedia; WP4 has looser criteria for inferring additional links, and its over-generation typically reduced its performance as training data.", "labels": [], "entities": [{"text": "WP2", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9059788584709167}]}, {"text": "A large proportion of sentences in our WG corpus cannot be automatically tagged with confidence.", "labels": [], "entities": [{"text": "WG corpus", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.8601117134094238}]}, {"text": "Sentence selection leaves 571 sentences (33.7%) after the WP2 process and 698 (41.2%) after the WP4 process (see).", "labels": [], "entities": [{"text": "Sentence selection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.960917055606842}, {"text": "WP2", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.814895749092102}, {"text": "WP4 process", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.8666419684886932}]}, {"text": "The use of the more permissive WP4 process may lead to the labelling of more NEs, but many maybe spurious.", "labels": [], "entities": []}, {"text": "We use three approaches to compare automatic and manual annotations of WG text: (a) treat each corpus as test data and evaluate NER performance on each; (b) treat WP2 and WP4-style subsets as NER predictions on the WG corpus to calculate an F -score; and (c) treat the automatic annotations like human annotators and calculate \u03ba values.", "labels": [], "entities": [{"text": "WG corpus", "start_pos": 215, "end_pos": 224, "type": "DATASET", "confidence": 0.8664051592350006}, {"text": "F -score", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9877843459447225}]}, {"text": "We first evaluate the WP2 model on each corpus and find that performance is higher on automatically-annotated subsets of WG.", "labels": [], "entities": []}, {"text": "This is unsurprising given the common automatic annotation process and the effects of the selection criterion.", "labels": [], "entities": []}, {"text": "F -score for the WP2 model when evaluated on 10 folds of automatically-annotated (WP2-style) test data.", "labels": [], "entities": [{"text": "F -score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9823601841926575}, {"text": "WP2 model", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.8321491777896881}, {"text": "WP2-style) test data", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.7722571492195129}]}, {"text": "This F -score is 8\u221210% higher than WP2's performance on the WP2-style subset of WG, suggesting that WG's text is somewhat more difficult to annotate than typical portions of WP2-style text.", "labels": [], "entities": [{"text": "F -score", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9949869513511658}, {"text": "WP2", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8006352186203003}, {"text": "WP2-style subset of WG", "start_pos": 60, "end_pos": 82, "type": "DATASET", "confidence": 0.8948575854301453}]}, {"text": "We compare the annotations of WG text more directly by treating the automatic annotations as if they are the output from a tagger run on the 698 and 571 sentences that were confidently chosen.", "labels": [], "entities": []}, {"text": "A reasonable agreement between the gold standard and automatic annotation is observed, with F -scores of 87.2% and 89.0% achieved by WP2 and WP4.", "labels": [], "entities": [{"text": "F -scores", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9947492480278015}, {"text": "WP2", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.9733077883720398}, {"text": "WP4", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.966399073600769}]}, {"text": "Table 12 also shows inter-annotator agreement calculated between the automatically annotated subsets and the gold-standard annotations in WG, using Cohen's \u03ba in the same way as for human annotators.", "labels": [], "entities": [{"text": "WG", "start_pos": 138, "end_pos": 140, "type": "DATASET", "confidence": 0.9081483483314514}]}, {"text": "The agreement was very high: equal or better than the agreement between human annotators prior to discussion and correction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Tagger performance on various corpora.  Asterisks indicate that MISC tags are ignored.", "labels": [], "entities": []}, {"text": " Table 2. The WP2 tag- ger performed substantially better on WG than tag- gers trained on newswire text, with a 7\u221211% in- crease in F -score compared to BBN and CONLL- 03, and a 16% increase compared to MUC-7, when  miscellaneous NEs in the corpus are not consid- ered in the evaluation. The Wikipedia trained  model thus outperforms newswire models on our  new WG corpus even though the training annota- tions were automatically extracted.", "labels": [], "entities": [{"text": "WG", "start_pos": 61, "end_pos": 63, "type": "DATASET", "confidence": 0.8675411939620972}, {"text": "in- crease", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9579224189122518}, {"text": "F -score", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9894464214642843}, {"text": "BBN", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.891442060470581}, {"text": "WG corpus", "start_pos": 362, "end_pos": 371, "type": "DATASET", "confidence": 0.8372590243816376}]}, {"text": " Table 4: NE class distribution, tag entropy and NE density statistics for gold-standard corpora and WG.", "labels": [], "entities": []}, {"text": " Table 6: Comparing MISC NE lengths (cumulative).", "labels": [], "entities": [{"text": "MISC NE", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.6995976269245148}]}, {"text": " Table 7: Feature-tag gain ratios.", "labels": [], "entities": []}, {"text": " Table 9: Comparing non-MISC NE lengths (cumulative).", "labels": [], "entities": [{"text": "Comparing non-MISC NE lengths", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.5358647629618645}]}, {"text": " Table 10: Size of WG and auto-annotated subsets.", "labels": [], "entities": []}, {"text": " Table 11: NER performance of the WP2-trained  model on auto-annotated subsets of WG.", "labels": [], "entities": [{"text": "NER", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9668982625007629}]}, {"text": " Table 12: Comparing WP2-style WG and WP4- style WG on WG. The automatically annotated data  was treated as predicted annotations on WG.", "labels": [], "entities": [{"text": "WP2-style WG", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.7923475801944733}, {"text": "WG", "start_pos": 55, "end_pos": 57, "type": "DATASET", "confidence": 0.9183182716369629}, {"text": "WG", "start_pos": 133, "end_pos": 135, "type": "DATASET", "confidence": 0.9674448370933533}]}]}