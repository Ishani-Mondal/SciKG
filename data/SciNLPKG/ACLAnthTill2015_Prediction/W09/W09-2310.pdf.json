{"title": [{"text": "Coupling hierarchical word reordering and decoding in phrase-based statistical machine translation", "labels": [], "entities": [{"text": "Coupling hierarchical word reordering", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.5863450691103935}, {"text": "phrase-based statistical machine translation", "start_pos": 54, "end_pos": 98, "type": "TASK", "confidence": 0.6055245250463486}]}], "abstractContent": [{"text": "In this paper, we start with the existing idea of taking reordering rules automatically derived from syntactic representations, and applying them in a preprocessing step before translation to make the source sentence structurally more like the target; and we propose anew approach to hierarchically extracting these rules.", "labels": [], "entities": []}, {"text": "We evaluate this, combined with a lattice-based decoding, and show improvements over state-of-the-art distortion models.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the big challenges for the MT community is the problem of placing translated words in a natural order.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9934093952178955}]}, {"text": "This issue originates from the fact that different languages are characterized by different word order requirements.", "labels": [], "entities": []}, {"text": "The problem is especially important if the distance between words which should be reordered is high (global reordering); in this case the reordering decision is very difficult to take based on statistical information due to dramatic expansion of the search space with the increase in number of words involved in the search process.", "labels": [], "entities": []}, {"text": "Classically, statistical machine translation (SMT) systems do not incorporate any linguistic analysis and work at the surface level of word forms.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.7900624026854833}]}, {"text": "However, more recently MT systems are moving towards including additional linguistic and syntactic informative sources (for example, source-and/or targetside syntax) into word reordering process.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9746536612510681}]}, {"text": "In this paper we propose using a syntactic reordering system operating with fully, partially and non-lexicalized reordering patterns, which are applied on the step prior to translation; the novel idea in this paper is in the derivation of these rules in a hierarchical manner, inspired by.", "labels": [], "entities": []}, {"text": "Furthermore, we propose generating a word lattice from the bilingual corpus with the reordered source side, extending the search space on the decoding step.", "labels": [], "entities": []}, {"text": "A thorough study of the combination of syntactical and word lattice reordering approaches is another novelty of the paper.", "labels": [], "entities": [{"text": "word lattice reordering", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7228237589200338}]}], "datasetContent": [{"text": "We used the Stanford Parser () for both languages, Penn English Treebank ( and Penn Arabic Treebank set ().", "labels": [], "entities": [{"text": "Penn English Treebank", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.9614716172218323}, {"text": "Penn Arabic Treebank set", "start_pos": 79, "end_pos": 103, "type": "DATASET", "confidence": 0.9560329169034958}]}, {"text": "The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories.", "labels": [], "entities": [{"text": "English Treebank", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9453064501285553}, {"text": "Arabic Treebank", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9345003664493561}]}, {"text": "As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds k part and k gener to relatively low values, 1 and 3, respectively.", "labels": [], "entities": []}, {"text": "Evaluation conditions were case-insensitive and with punctuation marks considered.", "labels": [], "entities": []}, {"text": "The targetside 4-gram language model was estimated using the SRILM toolkit) and modified Kneser-Ney discounting with interpolation.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9041144549846649}]}, {"text": "The highest BLEU score () was chosen as the optimization criterion.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9825479090213776}]}, {"text": "Apart from BLEU, a standard automatic measure METEOR () was used for evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9972231388092041}, {"text": "METEOR", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.7369499206542969}]}], "tableCaptions": [{"text": " Table 1: Basic statistics of the BTEC training corpus.", "labels": [], "entities": [{"text": "BTEC training corpus", "start_pos": 34, "end_pos": 54, "type": "DATASET", "confidence": 0.8058993220329285}]}, {"text": " Table 2: Summary of BTEC experimental results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.5883563160896301}, {"text": "BTEC", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.5915566682815552}]}, {"text": " Table 3: Summary of NIST50K experimental results.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7281928062438965}, {"text": "NIST50K", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.7426111698150635}]}, {"text": " Table 4: Basic reordering rules statistics.", "labels": [], "entities": []}]}