{"title": [], "abstractContent": [{"text": "We present an asymmetric approach to a run-time combination of two parsers where one component serves as a predic-tor to the other one.", "labels": [], "entities": []}, {"text": "Predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions.", "labels": [], "entities": []}, {"text": "Previously, the same architecture has been successfully used with pre-dictors providing partial or inferior information about the parsing problem.", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.9165275692939758}]}, {"text": "It has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level.", "labels": [], "entities": []}, {"text": "Results show that the combined system outperforms its individual components , even though their performance in isolation is already fairly high.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning techniques for automatically acquiring processing models from a data collection and traditional methods of eliciting linguistic knowledge from human experts are usually considered as two alternative roadmaps towards natural language processing solutions.", "labels": [], "entities": []}, {"text": "Since the resulting components exhibit quite different performance characteristics with respect to coverage, robustness and output quality, they might be able to provide some kind of complementary information, which could even lead to a notable degree of synergy between them when combined within a single system solution.", "labels": [], "entities": [{"text": "coverage", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9551307559013367}]}, {"text": "For the task of dependency parsing, the high potential for such a synergy has indeed been demonstrated already (e.g. Zeman and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y (2005),).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8787707984447479}]}, {"text": "A popular approach for combining alternative decision procedures is voting (Zeman and\u017dabokrtsk\u00b4y and\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2005).", "labels": [], "entities": []}, {"text": "It makes use of asymmetric architecture, where a meta component chooses from among the available candidate hypotheses by means of a (weighted) voting scheme.", "labels": [], "entities": []}, {"text": "Such an approach not only requires the target structures of all components to be of the same kind, but in case of complex structures like parse trees also requires sophisticated decision procedures which are able to select the optimal hypotheses with respect to additional global constraints (e.g. the tree property).", "labels": [], "entities": []}, {"text": "Since this optimization problem has to be solved by the individual parser anyhow, an asymmetric architecture suggests itself as an alternative.", "labels": [], "entities": []}, {"text": "In asymmetric architectures, a master component, i.e. a full fledged parser, is solely in charge of deciding on the target structure, whilst the others (so called helper or predictor components) provide additional evidence which is integrated into the global decision by suitable means.", "labels": [], "entities": []}, {"text": "Such a scheme has been extensively investigated for the Weighted Constraint Dependency Grammar, WCDG ).", "labels": [], "entities": [{"text": "Weighted Constraint Dependency Grammar, WCDG", "start_pos": 56, "end_pos": 100, "type": "TASK", "confidence": 0.5951784749825796}]}, {"text": "External evidence from the predictor components is integrated by means of constraints, which check for compatibility between a local structure and a prediction, and penalize this hypothesis in case of a conflict.", "labels": [], "entities": []}, {"text": "So far, however, all the additional information sources which have been considered in this research differed considerably from the master component: They either focused on particular aspects of the parsing problem (e.g. POS tagging, chunking, PP attachment), or used a simplified scheme for structural annotation (e.g. projective instead of non-projective trees).", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 198, "end_pos": 213, "type": "TASK", "confidence": 0.9110924303531647}, {"text": "POS tagging", "start_pos": 220, "end_pos": 231, "type": "TASK", "confidence": 0.5590085536241531}, {"text": "PP attachment", "start_pos": 243, "end_pos": 256, "type": "TASK", "confidence": 0.7041597217321396}]}, {"text": "This paper takes one step further by investigating the same architecture under the additional condition that (1) the helper component provides the very same kind of target structure as the master, and (2) the quality levels of each of the components in isolation are considered.", "labels": [], "entities": []}, {"text": "As a helper component MSTParser), a state-of-the-art dependency parser for non-projective structures based on a discriminative learning paradigm, is considered.", "labels": [], "entities": []}, {"text": "The accuracy of MSTParser differs insignificatly from that of WCDG with all the previously used helper components active.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996843338012695}]}, {"text": "Section two introduces WCDG with a special emphasis on the soft integration of external evidence while section three describes MSTParser which is used as anew predictor component.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 127, "end_pos": 136, "type": "DATASET", "confidence": 0.7813491821289062}]}, {"text": "Since parsing results for these systems have been reported in quite different experimental settings we first evaluate them under comparable conditions and provide the results of using MSTParser as a guiding predictor for WCDG in section four and discuss whether the expected synergies have really materialized.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 184, "end_pos": 193, "type": "DATASET", "confidence": 0.8798941373825073}, {"text": "WCDG", "start_pos": 221, "end_pos": 225, "type": "TASK", "confidence": 0.8082728385925293}]}, {"text": "Section five concentrates on a comparative error analysis.", "labels": [], "entities": [{"text": "comparative error analysis", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6914147734642029}]}], "datasetContent": [{"text": "The most common general measures for the quality of dependency trees are structural accuracy that points out the percentage of words correctly attached to their headword, and labeled accuracy which is the ratio of the correctly attached words which also have the correct label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.8767271637916565}, {"text": "labeled accuracy", "start_pos": 175, "end_pos": 191, "type": "METRIC", "confidence": 0.7256899774074554}]}, {"text": "Still, it is difficult to directly compare the results reported for different parsers, as the evaluation results are influenced by the data used during the experiment, the domain of the data, and different annotation guidelines.", "labels": [], "entities": []}, {"text": "Moreover, the particular kind of POS information might be relevant, which either can be obtained from the manual annotations or be provided by areal tagger.", "labels": [], "entities": []}, {"text": "Even such a condition as the treatment of punctuation has not yet become a standard.", "labels": [], "entities": []}, {"text": "Following the evaluation procedure in the CoNLL-X shared task (), we will not include punctuation into the performance measures, as was done in previous WCDG experiments ().", "labels": [], "entities": []}, {"text": "The source of POS tagging information will need to be specified in each individual case.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.8183369934558868}]}, {"text": "All the evaluations were performed on a thousand sentences) from the NEGRA treebank, the same data set that was previously used in the performance evaluations of WCDG, e.g. in ).", "labels": [], "entities": [{"text": "NEGRA treebank", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9635807573795319}]}, {"text": "The NEGRA treebank is a collection of newspaper articles; in the original, it stores phrase structure annotations.", "labels": [], "entities": [{"text": "NEGRA treebank", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9522401094436646}]}, {"text": "These have been automatically translated into dependency trees and then manually corrected to bring them in accord with the annotation guidelines of WCDG.", "labels": [], "entities": [{"text": "WCDG", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.9085758328437805}]}, {"text": "The major difference consists in a different treatment of non-projectivity, where WCDG only allows non-projectivity in the attachment of verbal arguments, relative clauses and coordinations, i.e., the cases where it helps to decrease ambiguity.", "labels": [], "entities": []}, {"text": "Furthermore, corrections were applied when the annotations of NEGRA itself turned out to be inconsistent (usually in connection with co-ordinated or elliptical structures, adverbs and subclauses).", "labels": [], "entities": []}, {"text": "Unfortunately, these manually corrected data were only available fora small part (3, 000 sentences) of the NEGRA corpus, which is not sufficient for training MSTParser on WCDGconforming tree structures.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9341665208339691}]}, {"text": "Previous evaluations of the MSTParser have used much larger training sets.", "labels": [], "entities": []}, {"text": "E.g., during the CoNLL-X shared task 39,216 sentences from the TIGER Treebank () were used.", "labels": [], "entities": [{"text": "TIGER Treebank", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.7176328897476196}]}, {"text": "Therefore, we used 20, 000 sentences from the online archive of www.heise.de as an alternative training set.", "labels": [], "entities": []}, {"text": "They have been manually annotated according to the WCDG guidelines (and are referred to heiseticker in the following) . The texts in this corpus are all from roughly the same domain as in NEGRA, and although very many technical terms and proper nouns are used, the sentences have only a slightly longer mean length compared to the NEGRA corpus.", "labels": [], "entities": [{"text": "WCDG", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8628318309783936}, {"text": "NEGRA corpus", "start_pos": 331, "end_pos": 343, "type": "DATASET", "confidence": 0.8976574540138245}]}, {"text": "Using POS tags from the gold annotations, MSTParser achieves 90.5% structural and 87.5% labeled accuracy on the aforementioned NEGRA test set).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.837825357913971}, {"text": "NEGRA test set", "start_pos": 127, "end_pos": 141, "type": "DATASET", "confidence": 0.9351632793744405}]}, {"text": "Even a model trained on the inconsistent NEGRA data excluding the test set reaches state-of-the-art 90.5 and 87.3% for structural and labeled accuracy respectively, despite the obvious mismatch between training and test data.", "labels": [], "entities": [{"text": "NEGRA data excluding the test set", "start_pos": 41, "end_pos": 74, "type": "DATASET", "confidence": 0.8068316181500753}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9665563106536865}]}, {"text": "This performance is almost the same as the 90.4%/87.3% reported on the TIGER data during the CoNLL-X 2006 shared task.", "labels": [], "entities": [{"text": "TIGER data during the CoNLL-X 2006 shared task", "start_pos": 71, "end_pos": 117, "type": "DATASET", "confidence": 0.8342974297702312}]}, {"text": "The combined experiments in which MSTParser was used as a predictor for WCDG have achieved higher accuracy than each of the combined components in isolation: the structural accuracy rises to 92.0% while the labeled accuracy also gets over the 90%-boundary (WCDG + MST experiment in (C)) . Finally, the MSTParser predictor was evaluated in combination with the other predictors available for WCDG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9975636005401611}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.8735512495040894}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.5480753183364868}]}, {"text": "The results of the experiments are shown in (C).", "labels": [], "entities": []}, {"text": "Every combination of MSTParser with other predictors (first four experiments) improves the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9988600015640259}]}, {"text": "The increase is highest (0.4%) for the combination with the supertagger.", "labels": [], "entities": []}, {"text": "This confirms earlier experiments with WCDG, in which the supertagger also contributed the largest gains.", "labels": [], "entities": []}, {"text": "The experimental results again confirm that WCDG is a reliable platform for information integration.", "labels": [], "entities": [{"text": "information integration", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.8890855610370636}]}, {"text": "Although the use of multiple predictors does not lead to an accumulation of the individual improvements, the performance of predictor combinations is always higher that using them separately.", "labels": [], "entities": []}, {"text": "A maximum performance of 92.9%/91.4% is reached with all the six available predictors active.", "labels": [], "entities": []}, {"text": "For comparison, the same experiment with POS tags from the gold standard has achieved even better results of 93.3%/92.0%.", "labels": [], "entities": []}, {"text": "Unfortunately, the PP attacher brings accuracy reductions when it is working parallel to the shiftreduce predictor (experiment PP + CP + SR in Table 2 (C)).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9991242289543152}, {"text": "SR", "start_pos": 137, "end_pos": 139, "type": "METRIC", "confidence": 0.8355010151863098}]}, {"text": "This effect has already been observed in the experiments that combined the two alone (experiment PP + SR in (A)).", "labels": [], "entities": [{"text": "SR", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.8973327279090881}]}, {"text": "When MST was combined with the PP attacher (experiment PP in (C)), the increase of the performance was also below a tenth of a percent.", "labels": [], "entities": []}, {"text": "The possible reasons why the use of an additional information source does not improve the performance in this case maybe the disadvantages of the PP attacher compared to a full parser.", "labels": [], "entities": []}, {"text": "MSTParser: Structural accuracy, (%), for different parsing runs for non-projective vs. projective sentences.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.852550745010376}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8765587210655212}]}, {"text": "MSTParser generally tends to find many more non-projective edges than the data has, while the precision remains restricted.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8772393465042114}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9993351101875305}]}, {"text": "The number of nonprojective edges was determined by counting how often an edge crosses some other edge.", "labels": [], "entities": []}, {"text": "Thus, if a non-projective edge crossed three other edges the number of non-projective edges equals three.", "labels": [], "entities": []}, {"text": "For MSTParser experiments with areal POS tagger (MSTParser POS-experiment in), the non-projective edge recall, the ratio of the nonprojective edges found in the experiment to the corresponding value in the gold standard, is at 23% and non-projective edge precision, the ratio of the correctly found non-projective edges to all non-projective edges found, is also only 36% (second column in).: Recall (r, %) and precision (p, %) of the non-projective edges and sentences for different parsing runs.", "labels": [], "entities": [{"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9671274423599243}, {"text": "precision", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9129778742790222}, {"text": "precision", "start_pos": 411, "end_pos": 420, "type": "METRIC", "confidence": 0.9996122717857361}]}], "tableCaptions": [{"text": " Table 1: Structural/labeled accuracy results with  POS tagging from the gold standard. WCDG  -no statistical enhancements used. MSTParser- h -MSTParser trained on the heiseticker.  MSTParser-N -MSTParser trained on NEGRA.  5P -with all five statistical predictors of WCDG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9719141125679016}, {"text": "NEGRA", "start_pos": 216, "end_pos": 221, "type": "DATASET", "confidence": 0.9148916602134705}]}, {"text": " Table 2: Structural/labeled accuracy results with  a real POS tagger. (A) WCDG experiments with  different statistical enhancements (B) MSTParser  experiment with a real POS tagger. (C) Com- bined experiments of WCDG and MSTParser with  other statistical enhancements of WCDG. CP - chunker, ST -supertagger, PP -prepositional  attacher, SR -shift-reduce oracle parser, 5P - POS + CP + PP + ST + SR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9556007385253906}]}, {"text": " Table 3: Per label structural precision (p,  %) and label recal (r, %) in comparison for  the experiments with the real POS tagger (1)  WCDG, (2) MSTParser, (3) WCDG combined  with MSTParser", "labels": [], "entities": [{"text": "Per label structural precision", "start_pos": 10, "end_pos": 40, "type": "METRIC", "confidence": 0.6202496737241745}, {"text": "label recal", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9366878867149353}, {"text": "MSTParser", "start_pos": 147, "end_pos": 156, "type": "DATASET", "confidence": 0.9179537296295166}, {"text": "MSTParser", "start_pos": 182, "end_pos": 191, "type": "DATASET", "confidence": 0.8040974736213684}]}, {"text": " Table 4: Structural accuracy, (%), for different  parsing runs for non-projective vs. projective sen- tences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9242064356803894}]}, {"text": " Table 5: Recall (r, %) and precision (p, %) of the  non-projective edges and sentences for different  parsing runs.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9837958812713623}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9995766282081604}]}]}