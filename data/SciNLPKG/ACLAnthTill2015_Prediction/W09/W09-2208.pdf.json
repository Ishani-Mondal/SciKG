{"title": [{"text": "A Simple Semi-supervised Algorithm For Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7700758377710978}]}], "abstractContent": [{"text": "We present a simple semi-supervised learning algorithm for named entity recognition (NER) using conditional random fields (CRFs).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.7910289913415909}]}, {"text": "The algorithm is based on exploiting evidence that is independent from the features used fora classifier, which provides high-precision labels to unlabeled data.", "labels": [], "entities": []}, {"text": "Such independent evidence is used to automatically extract high-accuracy and non-redundant data, leading to a much improved classifier at the next iteration.", "labels": [], "entities": []}, {"text": "We show that our algorithm achieves an average improvement of 12 in recall and 4 in precision compared to the supervised algorithm.", "labels": [], "entities": [{"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9995113611221313}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9994691014289856}]}, {"text": "We also show that our algorithm achieves high accuracy when the training and test sets are from different domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9990628361701965}]}], "introductionContent": [{"text": "Named entity recognition (NER) or tagging is the task of finding names such as organizations, persons, locations, etc.", "labels": [], "entities": [{"text": "Named entity recognition (NER) or tagging is the task of finding names such as organizations, persons, locations, etc.", "start_pos": 0, "end_pos": 118, "type": "Description", "confidence": 0.7414838212231795}]}, {"text": "Since whether or not a word is a name and the entity type of a name are determined mostly by the context of the word as well as by the entity type of its neighbors, NER is often posed as a sequence classification problem and solved by methods such as hidden Markov models (HMM) and conditional random fields (CRF).", "labels": [], "entities": [{"text": "NER", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.9852721691131592}, {"text": "sequence classification", "start_pos": 189, "end_pos": 212, "type": "TASK", "confidence": 0.7345970571041107}]}, {"text": "Automatically tagging named entities (NE) with high precision and recall requires a large amount of hand-annotated data, which is expensive to obtain.", "labels": [], "entities": [{"text": "tagging named entities (NE)", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.8757051527500153}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9985538125038147}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9990687966346741}]}, {"text": "This problem presents itself time and again because tagging the same NEs in different domains usually requires different labeled data.", "labels": [], "entities": [{"text": "tagging the same NEs", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7299132347106934}]}, {"text": "However, inmost domains one often has access to large amounts of unlabeled text.", "labels": [], "entities": []}, {"text": "This fact motivates semi-supervised approaches for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9566879272460938}]}, {"text": "Semi-supervised learning involves the utilization of unlabeled data to mitigate the effect of insufficient labeled data on classifier accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9313088655471802}]}, {"text": "One variety of semi-supervised learning essentially attempts to automatically generate high-quality training data from an unlabeled corpus.", "labels": [], "entities": []}, {"text": "Algorithms such as co-training)) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach.", "labels": [], "entities": []}, {"text": "The main requirement for the automatically generated training data in addition to high accuracy, is that it covers regions in the feature space with low probability density.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9990550875663757}]}, {"text": "Furthermore, it is necessary that all the classes are represented according to their prior probabilities in every region in the feature space.", "labels": [], "entities": []}, {"text": "One approach to achieve these goals is to select unlabeled data that has been classified with low confidence by the classifier trained on the original training data, but whose labels are known with high precision from independent evidence.", "labels": [], "entities": [{"text": "precision", "start_pos": 203, "end_pos": 212, "type": "METRIC", "confidence": 0.9542968273162842}]}, {"text": "Here independence means that the high-precision decision rule that classifies these low confidence instances uses information that is independent of the features used by the classifier.", "labels": [], "entities": []}, {"text": "We propose two ways of obtaining such independent evidence for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9837228655815125}]}, {"text": "The first is based on the fact that multiple mentions of capitalized tokens are likely to have the same label and occur in independently chosen contexts.", "labels": [], "entities": []}, {"text": "We call this the multi-mention property.", "labels": [], "entities": []}, {"text": "The second is based on the fact that entities such as organizations, persons, etc., have context that is highly indicative of the class, yet is independent of the other context (e.g. company suffixes like Inc., Co., etc., person titles like Mr., CEO, etc.).", "labels": [], "entities": []}, {"text": "We call such context high precision independent context.", "labels": [], "entities": []}, {"text": "Let us first look at two examples.", "labels": [], "entities": []}, {"text": "Example 1: 1) said Harry You, CEO of HearingPoint ....", "labels": [], "entities": []}, {"text": "2) For this year's second quarter, You said the company's ...", "labels": [], "entities": []}, {"text": "The classifier tags \"Harry You\" as person (PER) correctly since its context (said, CEO) makes it an obvious name.", "labels": [], "entities": []}, {"text": "However, in the second sentence, the classifier fails to tag \"You\" as a person since \"You\" is usually a stopword.", "labels": [], "entities": []}, {"text": "The second sentence is exactly the type of data needed in the training set.", "labels": [], "entities": []}, {"text": "Example 2: (1) Medtronic Inc 4Q profits rise 10 percent...", "labels": [], "entities": [{"text": "Medtronic Inc 4Q", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.8976739843686422}]}, {"text": "(2) Medtronic 4Q profits rise 10 percent...", "labels": [], "entities": []}, {"text": "The classifier tags \"Medtronic\" correctly in the first sentence because of the company suffix \"Inc\" while it fails to tag \"Medtronic\" in the second sentence since \"4Q profits\" is anew pattern and \"Medtronic\" is unseen in the training data.", "labels": [], "entities": []}, {"text": "Thus the second sentence is what we need in the training set.", "labels": [], "entities": []}, {"text": "The two examples have one thing in common.", "labels": [], "entities": []}, {"text": "In both cases, the second sentence has anew pattern and incorrect labels, which can be fixed by using either multi-mention or high-precision context from the first sentence.", "labels": [], "entities": []}, {"text": "We actually artificially construct the second sentence to be added to the training set in Example 2 although only the first sentence exists in the unlabeled corpus.", "labels": [], "entities": []}, {"text": "By leveraging such independent evidence, our algorithm can automatically extract high-accuracy and non-redundant data for training, and thus obtain an improved model for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 170, "end_pos": 173, "type": "TASK", "confidence": 0.9520126581192017}]}, {"text": "Specifically, our algorithm starts with a model trained with a small amount of gold data (manually tagged data).", "labels": [], "entities": []}, {"text": "This model is then used to extract high-confidence data, which is then used to discover low-confidence data by using other independent features.", "labels": [], "entities": []}, {"text": "These lowconfidence data are then added to the training data to retrain the model.", "labels": [], "entities": []}, {"text": "The whole process repeats until no significant improvement can be achieved.", "labels": [], "entities": []}, {"text": "Our experiments show that the algorithm is not only much better than the initial model, but also better than the supervised learning when a large amount of gold data are available.", "labels": [], "entities": []}, {"text": "Especially, even when the domain from which the original training data is sampled is different from the domain of the testing data, our algorithm still provides significant gains in classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 182, "end_pos": 196, "type": "TASK", "confidence": 0.9436139464378357}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.900582492351532}]}], "datasetContent": [{"text": "The data set used in the experiments is explained in.", "labels": [], "entities": []}, {"text": "Although we have 1000 labeled news documents from the Thomson Financial (TF) News source, only 60 documents are used as the initial training data in our algorithm.", "labels": [], "entities": [{"text": "Thomson Financial (TF) News source", "start_pos": 54, "end_pos": 88, "type": "DATASET", "confidence": 0.9785977176257542}]}, {"text": "For the evaluation, the gold data was split into training and test sets as appropriate.", "labels": [], "entities": []}, {"text": "The toolbox we used for CRF is Mallet).", "labels": [], "entities": [{"text": "CRF", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9405209422111511}, {"text": "Mallet", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9441385865211487}]}, {"text": "We first investigated our assumption that a high confidence score indicates high classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9682583808898926}]}, {"text": "illustrates how accuracy varies as CRF confidence score changes when 60 documents are used as training data and the remaining are used as testing data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9991312623023987}, {"text": "CRF confidence score", "start_pos": 35, "end_pos": 55, "type": "METRIC", "confidence": 0.6783212820688883}]}, {"text": "When the threshold is 0.98, the token accuracy is close to 99%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9913607239723206}]}, {"text": "We believe this accuracy is sufficiently high to justify using the high confidence score to extract tokens with correct labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.999265730381012}]}, {"text": "We wished to study the accuracy of our training data generation strategy from how well it does on the gold data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.999272882938385}, {"text": "training data generation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6311728159586588}]}, {"text": "We treat the remaining gold data (except the data trained for the initial model) as if they were unlabeled, and then applied our data extraction strategy on them.", "labels": [], "entities": [{"text": "data extraction", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.7905560731887817}]}, {"text": "illustrates the precision and recall for the three types of NEs of the extracted data, which only accounts fora small part of the gold data.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.99958735704422}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9995606541633606}]}, {"text": "The average F-score is close to 95%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9996086955070496}]}, {"text": "Although the precision and recall are not perfect, we believe they are good enough for the training purpose, considering that human tagged data is seldom more accurate.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9997506737709045}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9996514320373535}]}, {"text": "We compared the semi-supervised algorithm with a supervised algorithm using the same features.", "labels": [], "entities": []}, {"text": "The semi-supervised algorithm starts with 60 labeled documents (around 20,000 tokens) and ends with around 1.5 million tokens.", "labels": [], "entities": []}, {"text": "We trained the supervised algorithm with two data sets: using only 60 documents (around 20,000 tokens) and using 700 documents (around 220,000 tokens) respectively.", "labels": [], "entities": []}, {"text": "The reason for the choice of the training set size is the fact that 20,000 tokens area reasonably small amount of data for human to tag, and 220,000 tokens are the amount usually used for supervised algorithms) training set has around 220,000 tokens).", "labels": [], "entities": []}, {"text": "illustrates the results when 300 documents are used for testing.", "labels": [], "entities": []}, {"text": "As shown in, starting with only 6% of the gold data, the semisupervised algorithm achieves much better results than the semi-supervised algorithm when the same amount of gold data is used.", "labels": [], "entities": []}, {"text": "For LOC, ORG, and PER, the recall increases 5.5, 16.8, and 8.2 respectively, and the precision increases 2.4, 1.5, and 6.8 respectively.", "labels": [], "entities": [{"text": "ORG", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9927276372909546}, {"text": "PER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9077041745185852}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9998168349266052}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9996868371963501}]}, {"text": "Even compared with the model trained with 220,000 tokens, the semi-supervised learning algorithm is better.", "labels": [], "entities": []}, {"text": "Especially, for PER, the precision and recall increase 2.8 and 4.6 respectively.", "labels": [], "entities": [{"text": "PER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.5293622016906738}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9998517036437988}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9998656511306763}]}, {"text": "illustrates how the classifier is improved at each iteration in the semi-supervised learning algorithm.", "labels": [], "entities": []}, {"text": "compares the results when the multimention property is also used in testing as a highprecision rule.", "labels": [], "entities": []}, {"text": "Comparing, we can see that with the same training data, using multimention property helps improve classification results.", "labels": [], "entities": []}, {"text": "However, this improvement is less than that obtained by using this property to extract training data thus improve the model itself.", "labels": [], "entities": []}, {"text": "(For a fair comparison, the model used in the semi-supervised algorithm in only uses multi-mention property to extract data.)", "labels": [], "entities": []}, {"text": "Our last experiment is to test how this method can be used when the initial gold data and the testing data are from different domains.", "labels": [], "entities": []}, {"text": "We use the) training set as the initial training data, and automatically extract training data from the TF financial news corpus.", "labels": [], "entities": [{"text": "TF financial news corpus", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.9765884727239609}]}, {"text": "The CoNLL data is a collection of news wire documents from the Reuters Corpus, while TF data includes financial-related news only.", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9421351253986359}, {"text": "Reuters Corpus", "start_pos": 63, "end_pos": 77, "type": "DATASET", "confidence": 0.9723680019378662}, {"text": "TF data", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.7019257098436356}]}, {"text": "tokens, the results are not better than the results when only 60 TF docs are used for training.", "labels": [], "entities": []}, {"text": "This indicates that data from different domains can adversely affect NER accuracy for supervised learning.", "labels": [], "entities": [{"text": "NER", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.983572781085968}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9053196907043457}]}, {"text": "However, the semi-supervised algorithm achieves reasonably high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9981763362884521}]}, {"text": "For LOC, ORG, and PER, the recall increases 16, 20.3, and 4.7 respectively, and the precision increases 4.5, 5.5, and 4.7 respectively.", "labels": [], "entities": [{"text": "ORG", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9923742413520813}, {"text": "PER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9039597511291504}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.999789297580719}, {"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9997335076332092}]}, {"text": "Therefore our semi-supervised approach is effective for situation where the test and training data are from different sources.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Data source. Tokens include words, punctuation  and sentence breaks.  Gold Data  1000 docs from TF news  (around 330 tokens per doc)  Unlabeled Corpus 100,000 docs from TF news", "labels": [], "entities": [{"text": "TF news", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.8909183740615845}, {"text": "TF news", "start_pos": 179, "end_pos": 186, "type": "DATASET", "confidence": 0.9160652160644531}]}, {"text": " Table 5: Precision and recall of the automatically ex- tracted training data  NE  Precision% Recall% F-score%  LOC  94.5  96.8  95.6  ORG  96.6  93.4  94.9  PER  95.0  89.6  92.2", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9966520667076111}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9990407824516296}, {"text": "Recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9821208119392395}, {"text": "F-score", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9685954451560974}, {"text": "LOC  94.5  96.8  95.6  ORG  96.6  93.4  94.9", "start_pos": 112, "end_pos": 156, "type": "METRIC", "confidence": 0.8163776025176048}, {"text": "PER", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.6153327226638794}]}, {"text": " Table 6: Classification results. P/R represents Preci- sion/Recall. The numbers inside the parentheses are the  result differences when the model trained from 60 docs is  used as baseline.", "labels": [], "entities": [{"text": "P/R", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.6779852708180746}, {"text": "Preci- sion/Recall", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.8511482119560242}]}, {"text": " Table 7: Classification results when multi-mention prop- erty (M) is used in testing", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9181053042411804}, {"text": "multi-mention prop- erty (M)", "start_pos": 38, "end_pos": 66, "type": "METRIC", "confidence": 0.7695269073758807}]}, {"text": " Table 8: Classification results trained on CoNLL data and  test on TF data. Training data for the semi-supervised  algorithm are automatically extracted using both multi- mention and high-precision context from TF corpus.", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.964607834815979}, {"text": "TF data", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.7638991475105286}, {"text": "TF corpus", "start_pos": 212, "end_pos": 221, "type": "DATASET", "confidence": 0.7003684639930725}]}]}