{"title": [{"text": "Acquiring Applicable Common Sense Knowledge from the Web", "labels": [], "entities": [{"text": "Acquiring Applicable Common Sense Knowledge from the Web", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7602702304720879}]}], "abstractContent": [{"text": "In this paper, a framework for acquiring commonsense knowledge from the Web is presented.", "labels": [], "entities": []}, {"text": "Common sense knowledge includes information about the world that humans use in their everyday lives.", "labels": [], "entities": []}, {"text": "To acquire this knowledge , relationships between nouns are retrieved by using search phrases with automatically filled constituents.", "labels": [], "entities": []}, {"text": "Through empirical analysis of the acquired nouns over Word-Net, probabilities are produced for relationships between a concept and a word rather than between two words.", "labels": [], "entities": []}, {"text": "A specific goal of our acquisition method is to acquire knowledge that can be successfully applied to NLP problems.", "labels": [], "entities": []}, {"text": "We test the validity of the acquired knowledge by means of an application to the problem of word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.7376460532347361}]}, {"text": "Results show that the knowledge can be used to improve the accuracy of a state of the art un-supervised disambiguation system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9986587762832642}]}], "introductionContent": [{"text": "Common sense knowledge (CSK) is the knowledge we use in everyday life without necessarily being aware of it. of the Cyc project, define commonsense as \"the knowledge that every person assumes his neighbors also possess\".", "labels": [], "entities": [{"text": "Common sense knowledge (CSK)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6879462550083796}]}, {"text": "Although the term commonsense maybe understood as a process such as reasoning, we are referring only to knowledge.", "labels": [], "entities": []}, {"text": "It is CSK that tells us keys are kept in one's pocket and keys are used to open a door, but CSK does not hold that keys are kept in a kitchen sink or that keys are used to turn on a microwave, although all are possible.", "labels": [], "entities": [{"text": "CSK", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.9017205238342285}]}, {"text": "To show the need for this information more clearly we provide a couple sentences: She put the batter in the refrigerator.", "labels": [], "entities": []}, {"text": "(1) He ate the apple in the refrigerator.", "labels": [], "entities": []}, {"text": "In (1), we are dealing with lexical ambiguity.", "labels": [], "entities": []}, {"text": "There is little doubt for us to determine just what the \"batter\" is (food/substance used in baking).", "labels": [], "entities": []}, {"text": "However, a computer must determine that it is not someone who swings a bat in baseball that is being put into a refrigerator, although it is entirely possible to do (depending on the size of the refrigerator).", "labels": [], "entities": []}, {"text": "This demonstrates how CSK can be useful in solving word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7184839447339376}]}, {"text": "We know it is common for food to be found in a refrigerator and so we easily resolve batter as a food/substance rather than a person.", "labels": [], "entities": []}, {"text": "CSK can also help to solve syntactic ambiguity.", "labels": [], "entities": []}, {"text": "The problem of prepositional phrase attachment occurs in sentences similar to.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.6356107791264852}]}, {"text": "In this case, it is difficult fora computer to determine if \"he\" is in the refrigerator eating an apple or if the \"apple\" which he ate was in the refrigerator.", "labels": [], "entities": []}, {"text": "Like the previous example, the knowledge that food is commonly found in a refrigerator and people are not, leads us to understand that \"in the refrigerator\" should be attached to the noun phrase \"the apple\" and not as a modifier of the verb phrase \"ate\".", "labels": [], "entities": []}, {"text": "Unfortunately, there are not many sources of CSK readily available for use in computer algorithms.", "labels": [], "entities": []}, {"text": "Those sets of knowledge that are available, such as the CYC projector ConceptNet () rely on manually provided or crafted data.", "labels": [], "entities": [{"text": "CYC projector ConceptNet", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.9122517506281534}]}, {"text": "Our aim is to develop an automatic approach to acquire CSK 1 by turning to the vast amount of unannotated text that is available on the Web.", "labels": [], "entities": []}, {"text": "In turn, we present a method to automatically retrieve and analyze phrases from the Web.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation focuses on the applicability of the acquired CSK.", "labels": [], "entities": []}, {"text": "We acquired relationships for the 30 nouns listed in.", "labels": [], "entities": []}, {"text": "These nouns represent all possible words to fill the nounB constituent of a search phrase.", "labels": [], "entities": []}, {"text": "The corresponding #nounAs indicates the number of nounAs that were acquired from the Web for each nounB.", "labels": [], "entities": []}, {"text": "For example, 4771 nounAs were acquired for 'pocket'.", "labels": [], "entities": []}, {"text": "This means 4771 results from the web matched the parse of a web query for 'pocket' and contained a nounA in WordNet (keeping in mind duplicates Web text were removed).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9424777626991272}]}, {"text": "Delving deeper into our example, below are the top 20 nounAs found for the relationship nounAinpocket.", "labels": [], "entities": []}, {"text": "money, hand, cash, firework, something, dollar, ball, hands, key, coin, pedometer, card, battery, item, phone, penny, music, buck, implant, wallet As described in the concept analysis section, occurrences of each nounA fora given nounB lead top w values, which in turn are used to produce P c values for concepts in WordNet.", "labels": [], "entities": [{"text": "money, hand, cash, firework, something, dollar, ball, hands, key, coin, pedometer, card, battery, item, phone, penny, music, buck, implant, wallet", "start_pos": 0, "end_pos": 146, "type": "Description", "confidence": 0.8144556681315104}, {"text": "WordNet", "start_pos": 316, "end_pos": 323, "type": "DATASET", "confidence": 0.9568508267402649}]}, {"text": "The application of CSK utilizes these probabilities rather than simply lists of words or even lists of concepts.", "labels": [], "entities": []}, {"text": "However, challenges were encountered during the noun acquisition step before the probabilities were produced.", "labels": [], "entities": [{"text": "noun acquisition step", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.8679646849632263}]}, {"text": "Many challenges of the noun acquisition step were overcome through the use of a parser.", "labels": [], "entities": [{"text": "noun acquisition step", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8817746639251709}]}, {"text": "For example, phrases such as \"Palestine is on the road to becoming...\" could be eliminated since the parser marks the prepositional phrase \"to becoming\" as being attached to \"the road\".", "labels": [], "entities": []}, {"text": "Thus, the parse of the web sample does not match the parse of the web query used to acquire it.", "labels": [], "entities": []}, {"text": "Other times, noun-noun relationships were common simply because many web pages seem to copy the text of others.", "labels": [], "entities": []}, {"text": "This problem was handled through the elimination of duplicate text samples from the Web.", "labels": [], "entities": []}, {"text": "In the end, only about one in four results from the Web were actually used.", "labels": [], "entities": []}, {"text": "Numbers in reflect the result of these eliminations.", "labels": [], "entities": []}, {"text": "Some issues of the acquisition step were not directly addressed in this paper.", "labels": [], "entities": []}, {"text": "A domain may tend to be more prevalent on the Internet and skew the CSK, such as fireworkinpocket.", "labels": [], "entities": []}, {"text": "Another example, babyinbasket was very common due to biblical references.", "labels": [], "entities": []}, {"text": "Fictional works and metaphors also provided uncommon relationships dispersed within the results.", "labels": [], "entities": []}, {"text": "Additionally, the parser makes mistakes.", "labels": [], "entities": []}, {"text": "It was the hope that the concept analysis step would help to mitigate some noise from these problems.", "labels": [], "entities": [{"text": "concept analysis", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7097178846597672}]}, {"text": "A final issue was the bottleneck of limited queries per day by the search engines, which restricted us to testing on only the 30 nouns listed.", "labels": [], "entities": []}, {"text": "A goal of our work was to acquire data which could be applied to NLP problems.", "labels": [], "entities": []}, {"text": "We focus particularly on the difficult problem of word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7660526037216187}]}, {"text": "Due to the lack of sense tagged data, we were unable to find an annotated corpus with instances of all the nouns in as prepositional complements.", "labels": [], "entities": []}, {"text": "This was not surprising considering one of the reasons that minimally supervised approaches have become more popular is that they do not require hand-tagged training data).", "labels": [], "entities": []}, {"text": "We created a corpus from sentences in Wikipedia which contained the phrase \"in|on det lemma\", where det is a determiner or possessive pronoun, lemma is a noun from, and in|on is a preposition for either relationship described earlier.", "labels": [], "entities": []}, {"text": "Below we have provided an example from our corpus where the knowledge from 'pocket' can be applied to disambiguate 'key'.", "labels": [], "entities": []}, {"text": "Now Tony's key to the flat is in the pocket of his raincoat, soon returning to his flat sometime later he realizes that he cannot get inside.", "labels": [], "entities": []}, {"text": "The corpus 5 contained a total of 342 sentences, with one target noun annotated per sentence.", "labels": [], "entities": []}, {"text": "The target nouns were selected to potentially fill the nounA constituent in the relationship nounARnounB, and they were assigned all appropriate WordNet 3.0 senses.", "labels": [], "entities": []}, {"text": "Considering the finegrained nature of WordNet (), 26.3% of the instances were annotated with multiple senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9640606641769409}]}, {"text": "We also restricted the corpus to only include polysemous nouns, or nouns which had an additional sense beyond the senses assigned to it.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was used to validate the corpus.", "labels": [], "entities": []}, {"text": "Because the corpus was built by an author of the work, we asked a non-author to reannotate the corpus without knowledge of the original annotations.", "labels": [], "entities": []}, {"text": "This second annotator was told to choose all appropriate senses just as did the original annotator.", "labels": [], "entities": []}, {"text": "Agreement was calculated as: where S1 and S2 are the two sets of sense annotations, and i is an instance of the corpus, C. The agreement and other data concerning corpus annotation can be found in.", "labels": [], "entities": []}, {"text": "As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (   from a disambiguation system in order to get a human upper-bound of performance.", "labels": [], "entities": [{"text": "Senseval 3 all-words task", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7178185135126114}]}, {"text": "Just as the automatic system handled tie votes, when one word had multiple sense annotations, the annotation with the lowest sense number was used.", "labels": [], "entities": []}, {"text": "This performance upper-bound is shown as F1 h in.", "labels": [], "entities": [{"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9990641474723816}]}], "tableCaptions": [{"text": " Table 3: List of nouns which fill the nounB constituent  in a search phrase, and the corresponding occurrences of  nounAs acquired for each.", "labels": [], "entities": []}, {"text": " Table 4: Experimental corpus data for each relation- ship (on, in). insts: number of annotated instances;  agree: inter-annotator agreement %; F1 values (precision  = recall): h: human annotation, rnd: random baseline,  M F S: most frequent sense baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 144, "end_pos": 146, "type": "METRIC", "confidence": 0.9972499012947083}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.8957063555717468}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.8021849989891052}]}, {"text": " Table 5: F1 values (precision = recall) on our experimen- tal corpus with and without CSK. F1 all : using all 4 graph  metrics; F1 indeg : using only the indegree metric; ties:  number of instances where tie votes occurred.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.996177077293396}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9978572726249695}, {"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9898903369903564}, {"text": "F1 all", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9398589134216309}, {"text": "F1", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.9756105542182922}]}]}