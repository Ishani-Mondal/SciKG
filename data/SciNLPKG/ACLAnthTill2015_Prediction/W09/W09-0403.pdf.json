{"title": [{"text": "A Simple Automatic MT Evaluation Metric", "labels": [], "entities": [{"text": "MT Evaluation Metric", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.9517189860343933}]}], "abstractContent": [{"text": "This paper describes a simple evaluation metric for MT which attempts to overcome the well-known deficits of the standard BLEU metric from a slightly different angle.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9938048124313354}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9950879216194153}]}, {"text": "It employes Levenshtein's edit distance for establishing alignment between the MT output and the reference translation in order to reflect the morphological properties of highly inflected languages.", "labels": [], "entities": []}, {"text": "It also incorporates a very simple measure expressing the differences in the word order.", "labels": [], "entities": []}, {"text": "The paper also includes evaluation on the data from the previous SMT workshop for several language pairs.", "labels": [], "entities": [{"text": "SMT workshop", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.9250396192073822}]}], "introductionContent": [{"text": "The problem of finding a reliable machine translation metrics corresponding with a human judgment has recently returned to the centre of attention.", "labels": [], "entities": []}, {"text": "After a brief period following the introduction of generally accepted and widely used metrics, BLEU () and NIST), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results fora particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9960379600524902}, {"text": "NIST", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.9259442687034607}, {"text": "machine translation (MT)", "start_pos": 222, "end_pos": 246, "type": "TASK", "confidence": 0.8596752762794495}]}, {"text": "The results of the NIST evaluation in 2005 () have also strengthened the suspicion that the correlation between human judgment and the BLEU and NIST measures is not as strong as it was widely believed.", "labels": [], "entities": [{"text": "NIST evaluation in 2005", "start_pos": 19, "end_pos": 42, "type": "DATASET", "confidence": 0.9430290758609772}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.994371771812439}, {"text": "NIST", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.8560417890548706}]}, {"text": "Both measures seem to favor the MT output created by systems based on n-gram architecture, they are unable to take into account certain factors which are very important for the human judges of translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9859387278556824}]}, {"text": "The article) thoroughly discusses the deficits of the BLEU and similar metrics.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9870109558105469}]}, {"text": "The authors claim that the existing automatic metrics, including some of the new and seemingly more reliable ones as e.g.) \". .", "labels": [], "entities": []}, {"text": "they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation.\"", "labels": [], "entities": []}, {"text": "This claim is supported by a construction of translation variations which have identical BLEU score, but which are very different fora human judge.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9867479205131531}]}, {"text": "The authors identify three prominent factors which contribute to the inadequacy of BLEUthe failure to deal with synonyms and paraphrases, no penalties for missing content, and the crudeness of the brevity penalty.", "labels": [], "entities": [{"text": "BLEUthe", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9913234710693359}, {"text": "crudeness", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.926550567150116}]}, {"text": "Let us add some more factors based on our experiments with languages typologically different than English, Arabic or Chinese, which are probably the languages most frequently used in recent shared-task MT evaluations.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 202, "end_pos": 216, "type": "TASK", "confidence": 0.8495037853717804}]}, {"text": "The highly inflected languages and languages with a higher degree of word-order freedom may provide additional examples of sentences in which relatively small alterations of correct word forms may have a dire effect on the BLEU score while the sentence still remains understandable and acceptable for human evaluators.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 223, "end_pos": 233, "type": "METRIC", "confidence": 0.9833077788352966}]}, {"text": "The effect of rich inflection has been observed for example in, where the author mentions the fact that the BLEU score used for measuring the improvements in his experimental Czech-German EBMT system penalized heavily all subtle errors in Czech morphology arising from an out-of-context combined partial translations taken from different examples.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9717928469181061}]}, {"text": "The problem of the insensitivity of BLEU to the variations of the order of n-grams identified in reference translations has already been mentioned in the paper).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9977705478668213}]}, {"text": "The authors showed examples where changing a good word order into an unacceptable one did not affect the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9816573262214661}]}, {"text": "We may add a different example documenting the phenomenon that a pair of syntactically correct Czech sentences with the same word forms, differing only in the word order whose n-gram score for n = 2, 3, and 4 differs greatly.", "labels": [], "entities": []}, {"text": "Let us take one of the sentences from the 2008 SMT workshop and its reference translation: When Caligula appointed his horse to the Senate, the horse at least did not have blood on its hoofs.", "labels": [], "entities": [{"text": "SMT workshop", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.9113178253173828}]}, {"text": "-Kdy\u017e Caligula zvolil do sen\u00e1tu sv\u00e9ho kon\u011b, nem\u011bl jeho k\u016f\u0148 aspo\u0148 na kopytech krev.", "labels": [], "entities": []}, {"text": "If we modify the Czech reference sentence into Kdy\u017e sv\u00e9ho kon\u011b do sen\u00e1tu zvolil Caligula, jeho k\u016f\u0148 aspo\u0148 nem\u011bl na kopytech krev., we destroy 8 out of 15 bigrams, 11 out of 14 trigrams and 12 out of 13 quadrigrams while we still have sentence with almost identical meaning and probably very similar human evaluation.", "labels": [], "entities": []}, {"text": "The BLEU score of the modified sentence is, however, lower than it would be for the identical copy of the reference translation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9868697822093964}]}], "datasetContent": [{"text": "We have performed a test of the proposed metric using the data from the last year's SMT workshop.", "labels": [], "entities": [{"text": "SMT workshop", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.9226206243038177}]}, {"text": "The parameters a, b, and shave been set to the same value for all evaluated language pairs, no language dependent alterations were tested in this experiment: The values for the parameters have been setup empirically with special attention being paid to Czech, the only language with really rich inflection among the languages being tested.", "labels": [], "entities": []}, {"text": "We have performed sentence-level and systemlevel evaluation using the Spearman's rank correlation coefficient which is defined as follows: where d i = xi \u2212y i is the difference between the ranks of corresponding values X i and Y i and n is the number of values in each data set.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 70, "end_pos": 109, "type": "METRIC", "confidence": 0.5423767656087876}]}, {"text": "The following scores express the correlation of our automatic metric and the human judgements for the language pairs English-Czech and EnglishGerman.", "labels": [], "entities": []}, {"text": "The sentence-level correlation \u03c1 sent is the average of Spearman's \u03c1 across all sentences.", "labels": [], "entities": [{"text": "sentence-level correlation \u03c1 sent", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.7586876079440117}, {"text": "Spearman's \u03c1", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.660309632619222}]}], "tableCaptions": []}