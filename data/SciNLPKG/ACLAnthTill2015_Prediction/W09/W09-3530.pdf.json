{"title": [{"text": "Name Transliteration with Bidirectional Perceptron Edit Models", "labels": [], "entities": [{"text": "Name Transliteration", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8216012418270111}]}], "abstractContent": [{"text": "We report on our efforts as part of the shared task on the NEWS 2009 Machine Transliteration Shared Task.", "labels": [], "entities": [{"text": "NEWS 2009 Machine Transliteration Shared Task", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.7461928129196167}]}, {"text": "We applied an orthographic perceptron character edit model that we have used previously for name transliteration, enhancing it in two ways: by ranking possible transliterations according to the sum of their scores according to two models, one trained to generate left-to-right, and one right-to-left; and by constraining generated strings to be consistent with character bigrams observed in the respective language's training data.", "labels": [], "entities": [{"text": "name transliteration", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.7744024395942688}]}, {"text": "Our poor showing in the official evaluation was due to a bug in the script used to produce competition-compliant output.", "labels": [], "entities": []}, {"text": "Subsequent evaluation shows that our approach yielded comparatively strong performance on all alphabetic language pairs we attempted.", "labels": [], "entities": []}], "introductionContent": [{"text": "While transliteration is a much simpler problem than another linguistic transduction problem, language translation, it is rarely trivial.", "labels": [], "entities": [{"text": "language translation", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.7769094109535217}]}, {"text": "At least three phenomena complicate the automatic transliteration between two languages using different scripts-differing phoneme sets, lossy orthography, and non-alphabetic orthographies (e.g., syllabaries).", "labels": [], "entities": []}, {"text": "For most language pairs, these difficulties stand in the way of a rule-based treatment of the problem.", "labels": [], "entities": []}, {"text": "For this reason, many machine learning approaches to the problem have been proposed.", "labels": [], "entities": []}, {"text": "We can draw a rough distinction between learning approaches that attempt to model the phonetics of a transliteration problem explicitly, and those that treat the problem as simply one of orthographic transduction, leaving it to the learning algorithm to acquire phonetic distinctions directly from orthographic features of the training data.", "labels": [], "entities": []}, {"text": "For example, address the problem through cascaded finite state transducers, with explicit representations of the phonetics.", "labels": [], "entities": []}, {"text": "Subsequently, Al-Onaizan and realize improvements by adding a \"spelling\" (i.e., orthographic) model.", "labels": [], "entities": []}, {"text": "There has been an increasing emphasis on purely orthographic models, probably because they require less detailed domain knowledge (e.g.,).", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with models of order 2 and 3 (2-gram and 3-gram features) on shared task data for English to.", "labels": [], "entities": []}, {"text": "Based on development accuracy scores, we found models of order 3 to be consistently better than order 2, and our submitted results use only order-3 models, with one exception.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9418834447860718}]}, {"text": "English-to-native-Chinese () was treated as a special case.", "labels": [], "entities": []}, {"text": "Using trigram features in the target language results in an explosion in the feature space, and a model that is slow to train and performs poorly.", "labels": [], "entities": []}, {"text": "Thus, only for this language pair, we devised a mixed-order model, one using trigram features over English strings, and unigram features over Chinese.", "labels": [], "entities": []}, {"text": "Because of the large target-language branching factor, the mixed-order native Chinese model remain-: Post-contest accuracy on evaluation set, including delta from highest-scoring contest participant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9658105969429016}]}, {"text": "ing one of the slowest to train.", "labels": [], "entities": []}, {"text": "We trained all models for 20 epochs, evaluating the 1-best accuracy of intervening models on the development data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9334113597869873}]}, {"text": "In all cases, we observed that accuracy increased steadily for some number of iterations, after which it plateaued.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995905756950378}]}, {"text": "Consequently, for all language pairs, we submitted the predictions of the latest model.", "labels": [], "entities": []}, {"text": "lists accuracy reported by the official evaluation script on the contest evaluation data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9996790885925293}, {"text": "contest evaluation data", "start_pos": 65, "end_pos": 88, "type": "DATASET", "confidence": 0.6237786014874777}]}, {"text": "All non-Chinese runs in the table are \"standard,\" and are trained exclusively on shared task training data.", "labels": [], "entities": []}, {"text": "Those labeled \"baseline\" are left-toright models with no character n-gram constraints.", "labels": [], "entities": []}, {"text": "These results were obtained after release of the evaluation data, but differ from our official submission in only two ways: First, and most importantly, the bug described previously was corrected.", "labels": [], "entities": []}, {"text": "Second, in some cases training runs that had not completed at evaluation time were allowed to run to the full 20 epochs, and the resulting models were used.", "labels": [], "entities": []}, {"text": "The exceptions are Hindi and native Chinese, each of which reflect performance at approximately 10 epochs.", "labels": [], "entities": []}, {"text": "Without exception, abeam of size 100 was used to generate these results.", "labels": [], "entities": []}, {"text": "With the exception of \"EnCh standard,\" all results in the table employed the bidirectional scheme described above.", "labels": [], "entities": [{"text": "EnCh standard", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.5841357111930847}]}, {"text": "The Chinese nonstandard runs differ from standard only in that models were trained to perform English-to-pinyin transliteration, followed by a conversion from pinyin to native Chinese using tables provided by the Unicode consortium.", "labels": [], "entities": []}, {"text": "Non-standard Run 1 retains pinyin tonal diacritics, while Run 2 omits them.", "labels": [], "entities": []}, {"text": "The mapping from pinyin to native Chinese characters introduces indeterminacy, which we accounted for in a simple fashion: First, in constructing a pinyin-to-Chinese conversion table, we discarded any Chinese characters that were used in the training data fewer than some small fraction of cases.", "labels": [], "entities": []}, {"text": "Then, given a ranked list of pinyin transliterations, we generated all possible native Chinese sequences, ranked by the product of observed pinyin-to-Chinese probabilities, according to training frequencies.", "labels": [], "entities": []}, {"text": "It will be observed that our non-standard English-to-Chinese results lag considerably behind the best results.", "labels": [], "entities": []}, {"text": "We suspect this is due in part to the fact that no additional training data was used in these experiments-only a change in representation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Post-contest accuracy on evaluation set,  including delta from highest-scoring contest par- ticipant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9741321802139282}, {"text": "delta", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9788573980331421}]}]}