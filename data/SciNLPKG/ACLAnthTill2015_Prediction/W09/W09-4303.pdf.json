{"title": [{"text": "A Pairwise Event Coreference Model, Feature Impact and Evaluation for Event Coreference Resolution", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9080113172531128}]}], "abstractContent": [{"text": "In past years, there has been substantial work on the problem of entity coreference resolution whereas much less attention has been paid to event coreference resolution.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 65, "end_pos": 94, "type": "TASK", "confidence": 0.7635189096132914}, {"text": "event coreference resolution", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.7423732082049052}]}, {"text": "Starting with some motivating examples, we formally state the problem of event coreference resolution in the ACE 1", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.8713556130727133}, {"text": "ACE 1", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.9637890160083771}]}], "introductionContent": [{"text": "In this paper, we address the task of event coreference resolution specified in the Automatic Content Extraction (ACE) program: grouping all the mentions of events in a document into equivalent classes so that all the mentions in a given class refer to a unified event.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.7683175901571909}]}, {"text": "We adopt the following terminologies used in ACE: \uf06c Entity: an objector set of objects in the world, such as person, organization, facility.", "labels": [], "entities": []}, {"text": "\uf06c Event: a specific occurrence involving participants.", "labels": [], "entities": []}, {"text": "\uf06c Event trigger: the word that most clearly expresses an event's occurrence.", "labels": [], "entities": []}, {"text": "\uf06c Event argument: an entity, or a temporal expression or a value that has a certain role (e.g., PLACE) in an event.", "labels": [], "entities": [{"text": "PLACE", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9088366627693176}]}, {"text": "\uf06c Event mention: a sentence or phrase that mentions an event, including a distinguished trigger and involving arguments.", "labels": [], "entities": []}, {"text": "An event is a cluster of event mentions.", "labels": [], "entities": []}, {"text": "\uf06c Event attributes: an event has six event attributes, event type, subtype, polarity, modality, genericity, and tense.", "labels": [], "entities": []}, {"text": "We demonstrate some motivating examples in table 1 (event triggers are surrounded by curly brackets and event arguments are underlined).", "labels": [], "entities": []}, {"text": "In example 1,event mention EM1 corefers with EM2 because they have the same event type and subtype 1 http://www.nist.gov/speech/tests/ace/ (CONFLICT: ATTACK) indicated by two verb triggers \"tore\" and \"exploded\" respectively, and the argument \"a waiting shed\" in EM1 corefers with \"the waiting shed\" in EM2.", "labels": [], "entities": [{"text": "ATTACK", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9575808048248291}]}, {"text": "In example 2, EM1, EM2 and EM3 corefer with each other because they have the same event type and subtype (LIFE:MARRY) indicated by a verb trigger \"wed\" and two noun triggers \"ceremony\" and \"nuptials\" respectively.", "labels": [], "entities": [{"text": "MARRY", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.7042800188064575}]}, {"text": "Furthermore, the two persons \"Rudolph Giuliani\" and \"Judith Nathan\" involving in the \"Marry\" event in EM1 corefer with \"Giuliani\" and \"Nathan\" in EM3 respectively.", "labels": [], "entities": [{"text": "EM1 corefer", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9508596658706665}, {"text": "EM3", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.9698585271835327}]}, {"text": "In example 3, EM1 does not corefer with EM2 although they have the same event type and subtype (LIFE:DIE) because the event attribute \"polarity\" of EM1 is \"POSITIVE\" (occurred) while in EM2, it is \"NEGATIVE\" (not occurred).", "labels": [], "entities": [{"text": "LIFE:DIE)", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.7863364666700363}, {"text": "POSITIVE", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9693793058395386}]}, {"text": "the ex-mayor's old home.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we used the ACE 2005 English corpus which contains 599 documents in six genres: newswire, broadcast news, broadcast conversations, weblogs, newsgroups and conversational telephone speech transcripts.", "labels": [], "entities": [{"text": "ACE 2005 English corpus", "start_pos": 33, "end_pos": 56, "type": "DATASET", "confidence": 0.9676565378904343}]}, {"text": "We first investigated the performance of the four event attribute classification models using the ground truth event mentions and system generated event mentions respectively.", "labels": [], "entities": []}, {"text": "The evaluation metrics we adopted in this set of experiments are Precision (P), Recall (R) and F-Measure (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9529305547475815}, {"text": "Recall (R)", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9560971707105637}, {"text": "F-Measure (F)", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.9567421674728394}]}, {"text": "We then validated our agglomerative clustering algorithm for the event coreference resolution using the ground truth event mentions and system generated event mentions respectively.", "labels": [], "entities": [{"text": "event coreference resolution", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.7570492923259735}]}, {"text": "The evaluation metrics we adopted in this set of experiments are three conventional metrics for entity coreference resolution, namely, MUC F-Measure, B-Cubed F-Measure and ECM F-Measure.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.7767022053400675}, {"text": "MUC", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.43604952096939087}, {"text": "F-Measure", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.41007548570632935}, {"text": "ECM", "start_pos": 172, "end_pos": 175, "type": "DATASET", "confidence": 0.8436421155929565}, {"text": "F-Measure", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.3934467136859894}]}, {"text": "We conducted all the experiments by ten times ten-fold cross validation and measured significance with the Wilcoxon signed rank test.", "labels": [], "entities": [{"text": "significance", "start_pos": 85, "end_pos": 97, "type": "METRIC", "confidence": 0.9883184432983398}, {"text": "Wilcoxon signed rank test", "start_pos": 107, "end_pos": 132, "type": "METRIC", "confidence": 0.5917899459600449}]}, {"text": "shows that the majority of event mentions are POSITIVE (5162/5349=0.965), ASSERTED (4002/5349 =0.748), SPECIFIC (4145/5349=0.775) and PAST (2720/5349=0.509).", "labels": [], "entities": [{"text": "POSITIVE", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9930209517478943}, {"text": "ASSERTED", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.982363224029541}, {"text": "SPECIFIC", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8352366089820862}, {"text": "PAST", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.8998649716377258}]}, {"text": "shows the performance of the four event attribute classification models using the ground truth event mentions (perfect) and the system generated event mentions (system).", "labels": [], "entities": []}, {"text": "For comparison, we also setup a  baseline for each case using the majority value as output (e.g., for Polarity attribute, we always set the value to POSITIVE because POSITIVE is the majority).", "labels": [], "entities": [{"text": "POSITIVE", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9625949859619141}]}, {"text": "shows that the improvements for Polarity, Modality and Genericity over the baselines are quite limited while the improvements for Tense are significant, either using ground truth event mentions or using system generated event mentions.", "labels": [], "entities": [{"text": "Genericity", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9424723386764526}, {"text": "Tense", "start_pos": 130, "end_pos": 135, "type": "TASK", "confidence": 0.8988457918167114}]}], "tableCaptions": [{"text": " Table 4. Performance of the four event attribute classification models", "labels": [], "entities": []}, {"text": " Table 5. Feature impact using ground truth event mentions", "labels": [], "entities": []}, {"text": " Table 6. Feature impact using system generated event mentions", "labels": [], "entities": []}, {"text": " Table 7. Ratio of type I,II error and ratio of type I,II miss", "labels": [], "entities": [{"text": "Ratio", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9853823184967041}, {"text": "type I,II error", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.6380375623703003}]}]}