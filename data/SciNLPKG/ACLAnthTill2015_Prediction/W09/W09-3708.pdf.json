{"title": [{"text": "Understanding Mental States in Natural Language", "labels": [], "entities": []}], "abstractContent": [{"text": "Understanding mental states in narratives is an important aspect of human language comprehension.", "labels": [], "entities": []}, {"text": "By \"mental states\" we refer to beliefs, states of knowledge, points of view, and suppositions, all of which may changeover time.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach for automatically extracting and understanding multiple mental states in stories.", "labels": [], "entities": [{"text": "automatically extracting and understanding multiple mental states in stories", "start_pos": 42, "end_pos": 118, "type": "TASK", "confidence": 0.7422530386182997}]}, {"text": "Our model consists of two parts: (1) a parser that takes an English sentence and translates it to some semantic operations; (2) a mental-state inference engine that reads in the semantic operations and produces a situation model that represents the meaning of the sentence.", "labels": [], "entities": []}, {"text": "We present the performance of the system on a corpus of children stories containing both fictional and non-fictional texts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language involves statements that carry distinct world-views.", "labels": [], "entities": []}, {"text": "By world-views we refer to states of belief, supposition, intention, advice, perceived reality, as well as situations expressed by tenses in natural language such as past, present and future.", "labels": [], "entities": []}, {"text": "In this paper, we call these world-views \"mental states\".", "labels": [], "entities": []}, {"text": "Mental states are common phenomena.", "labels": [], "entities": []}, {"text": "They span various domains of natural language.", "labels": [], "entities": []}, {"text": "Sentence (1a) and sentence (1b) are examples drawn from two different domains: online news articles and fairy tales.", "labels": [], "entities": []}, {"text": "The police believe the thieves were trying to steal a solar panel from Sarah's tin roof.", "labels": [], "entities": [{"text": "Sarah's tin roof", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.7597953081130981}]}, {"text": "She (little red-cap) was surprised to find the cottage-door standing open.", "labels": [], "entities": []}, {"text": "Both of these two sentences involve multiple mental states, which maybe nested in one another.", "labels": [], "entities": []}, {"text": "Sentence (1a) involves the police's belief and the intention of the thieves in the police's belief; sentence (1b) contains little red-cap's old belief and her updated belief, both of which maybe different from the reality.", "labels": [], "entities": []}, {"text": "Since the information in mental states is rich and often important, we need some processing technique to extract that information and understand it.", "labels": [], "entities": []}, {"text": "There are two problems in extracting and understanding mental states.", "labels": [], "entities": [{"text": "extracting and understanding mental states", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.8249743938446045}]}, {"text": "First, extracting mental states from text requires recognizing linguistic patterns of mental states.", "labels": [], "entities": [{"text": "extracting mental states from text", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.8354010462760926}]}, {"text": "Related problems such as subjectivity recognition have been studied intensively in the natural language processing community.", "labels": [], "entities": [{"text": "subjectivity recognition", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.7456676959991455}]}, {"text": "The problem covers various aspects of the \"private state frame\", including recognizing private states, the sources of private states, the intensity and types of attitudes, among many others.", "labels": [], "entities": []}, {"text": "Second, mental states extracted from natural language need to be encoded in some representation form and can be retrieved for further inference.", "labels": [], "entities": []}, {"text": "There exists many systems that implements nested evolving beliefs (e.g.), but they generally lacked the ability to draw inference directly from natural language.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach to extract and represent mental states based on different mental contexts such as one's belief, intention and supposition.", "labels": [], "entities": []}, {"text": "Our approach utilizes the mental spaces theory in cognitive linguistics.", "labels": [], "entities": []}, {"text": "Our goal for the mental state extraction step is to identify spacebuilders that establish new mental contexts (or spaces) or the ones that refer to an existing mental context.", "labels": [], "entities": [{"text": "mental state extraction", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.6534215112527212}]}, {"text": "The main body of our space-builders consists of agent and psych-term pairs such as \"the police believe\" (refers to the police's belief context) and \"little red-cap was surprised\" (refers to multiple belief contexts of little red-cap) . Different objects and propositions are then bundled in these mental contexts.", "labels": [], "entities": []}, {"text": "In the mental state understanding step, the mental contexts are instantiated and maintained in a context network, where inference rules are applied within and across those contexts.", "labels": [], "entities": [{"text": "mental state understanding", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.6389251450697581}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 provides a high-level overview of our implemented mental state understanding system.", "labels": [], "entities": [{"text": "mental state understanding", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.6316178639729818}]}, {"text": "Excerpt from \"The Little Red-Cap\" in Margaret Hunt's translation of the Grimms Fairy Tales.", "labels": [], "entities": [{"text": "Margaret Hunt's translation of the Grimms Fairy Tales", "start_pos": 37, "end_pos": 90, "type": "DATASET", "confidence": 0.6291182935237885}]}, {"text": "In some other translations, the story is also called \"Little Red Riding Hood\".", "labels": [], "entities": [{"text": "Little Red Riding Hood\"", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.6724902749061584}]}, {"text": "3 Fauconnier (1985) covers a much broader set of space-builders including prepositional phrases (\"from her point of view\"), connectives (\"if ...", "labels": [], "entities": [{"text": "Fauconnier (1985)", "start_pos": 2, "end_pos": 19, "type": "DATASET", "confidence": 0.9250539988279343}]}, {"text": "then ...\"), and subject-verb combinations (\"she thinks\").", "labels": [], "entities": []}, {"text": "Our current system only deals with the last category.", "labels": [], "entities": []}, {"text": "Simple examples will be presented to demonstrate the input and output of the system.", "labels": [], "entities": []}, {"text": "Section 3 and section 4 introduces our parser for mental state extraction and the inference engine for mental state representation, respectively.", "labels": [], "entities": [{"text": "mental state extraction", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6847264965375265}, {"text": "mental state representation", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6787219842274984}]}, {"text": "Section 5 discusses evaluation results on fictional and nonfictional children stories.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use 513 children stories from Project LISTEN's 9 reading tutor story database for system evaluation.", "labels": [], "entities": [{"text": "Project LISTEN's 9 reading tutor story database", "start_pos": 33, "end_pos": 80, "type": "DATASET", "confidence": 0.5943900868296623}]}, {"text": "This corpus contains 213 fictional articles and 300 non-fictional (or informational) articles.", "labels": [], "entities": []}, {"text": "From these, our system extracts 1181 mental state expressions in fictions and 413 in non-fictions.", "labels": [], "entities": []}, {"text": "After the parsing stage, 60.80% of the fictional and 62.71% of the non-fictional mental state expressions are fully parsed (i.e. there is no empty arguments for the mental operations) and sent to the inference engine.", "labels": [], "entities": []}, {"text": "After processing of the mental operations, the system automatically generates 1437 yes-no questions and answers for fictional texts, and 518 for non-fictional texts.", "labels": [], "entities": []}, {"text": "The question-answer pairs are generated by traversing newly visited mental contexts and statements/objects bundled in those contexts immediately after each mental operation is evaluated.", "labels": [], "entities": []}, {"text": "These questions and answers are all in similar forms as the examples demonstrated in section 2.2.", "labels": [], "entities": []}, {"text": "We do a careful evaluation on 431 questions for fictional texts and 155 for non-fictional text that are randomly selected from the question-answer pool.", "labels": [], "entities": []}, {"text": "Since both the questions and answers come from the situation model stored in the system, an error occurred in either the question or the answer counts for an incorrect example.", "labels": [], "entities": []}, {"text": "shows the error rates of each error category for both fictional and non-fictional stories.", "labels": [], "entities": [{"text": "error rates", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9776979684829712}]}, {"text": "The second column (\"argument error\") gives the percentage of incorrect question-answer pairs caused by wrong arguments in the mental operations.", "labels": [], "entities": []}, {"text": "This type of error comes from a misinterpretation of the ASSERT output.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 57, "end_pos": 63, "type": "TASK", "confidence": 0.6894994974136353}]}, {"text": "For example, we assume that ARG1 indicates a proposition for psych-terms if it is not a noun phrase.", "labels": [], "entities": []}, {"text": "But \"she [TAR-GET wanted ] [ARG1 so much]\" is an exception of this assumption.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.6467701196670532}]}, {"text": "The third column (\"statement error\") gives the percentage of incorrect examples (usually questions) caused by incomplete or unnecessary statements in the mental contexts.", "labels": [], "entities": []}, {"text": "This type of errors results from inaccurate ASSERT outputs that are not captured by our statement checking rules.", "labels": [], "entities": [{"text": "ASSERT", "start_pos": 44, "end_pos": 50, "type": "TASK", "confidence": 0.9370212554931641}]}, {"text": "Space-builder error (the fourth column) refers to the cases in which the mental contexts are not correctly constructed (this results in incorrect answers).", "labels": [], "entities": []}, {"text": "This usually happens when there is a mental space that has been neglected by our system.", "labels": [], "entities": []}, {"text": "For example, when given sentence \"Dig for it if you want the gold\", our system would only look at \"you want the gold\" and treat it as a valid statement, without noticing that it is embedded in an if-clause which indicates another mental space.", "labels": [], "entities": []}, {"text": "Ambiguity error (the fifth column) refers to errors cause by the lexical ambiguity of psych-terms (e.g. the word \"will\" indicates future tense in \"Pilly will go to school tomorrow\", but not in \"Some sharks will eat just about anything\").", "labels": [], "entities": [{"text": "Ambiguity error", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.967137485742569}]}, {"text": "Negation error (the sixth column) occurs when there is a negation that has been neglected (this results in an incorrect answer).", "labels": [], "entities": []}, {"text": "For example \"It will do him no good, neither will it help anybody else\" means \"It will not help anybody else\".", "labels": [], "entities": []}, {"text": "But this negation is not captured by our system.", "labels": [], "entities": []}, {"text": "Note that since the five types of errors listed in are not mutually exclusive, the error rates in each row of the table do not sum up to the total error rate of question-answer pairs generated by the system.", "labels": [], "entities": []}, {"text": "Anaphora error has been counted separately since this type of error alone has a significant effect on system performance, and the anaphora annotator is relatively independent with our task compared to the other inhouse components.", "labels": [], "entities": [{"text": "Anaphora error", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8650386035442352}]}, {"text": "During evaluation, we only apply anaphora resolution to pronouns (the word \"it\" is not counted).", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.659261479973793}]}, {"text": "In the same question-answer pool, we observe the anaphora error rate of 19.49% on fictions and 17.42% on non-fictions.", "labels": [], "entities": [{"text": "anaphora error rate", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.9315910538037618}]}], "tableCaptions": []}