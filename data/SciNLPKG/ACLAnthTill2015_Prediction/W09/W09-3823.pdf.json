{"title": [{"text": "Guessing the Grammatical Function of a Non-Root F-Structure in LFG", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexical-Functional Grammar (Kaplan and Bresnan, 1982) f-structures are bilexical labelled dependency representations.", "labels": [], "entities": [{"text": "Lexical-Functional Grammar", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7346975207328796}]}, {"text": "We show that the Naive Bayes classifier is able to guess missing grammatical function labels (i.e. bilexical dependency labels) with reasonably high accuracy (82-91%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9991057515144348}]}, {"text": "In the experiments we use f-structure parser output for English and German Europarl data, automatically \"broken\" by replacing grammatical function labels with a generic UNKNOWN label and asking the classifier to restore the label.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.9065653681755066}]}], "introductionContent": [{"text": "The task of labeling unlabelled dependencies, a sub-task of dependency parsing task, can occur in transfer-based machine translation (when only an inexact match can be found in the training data for the given SL fragment) or in parsing where the system produces fragmented output.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.8029115398724874}, {"text": "transfer-based machine translation", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.6360081533590952}, {"text": "parsing", "start_pos": 228, "end_pos": 235, "type": "TASK", "confidence": 0.9770221710205078}]}, {"text": "In such cases it is often reasonably straightforward to guess which fragments are dependent on which other fragments (e.g. in transfer-based MT).", "labels": [], "entities": [{"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.8637104630470276}]}, {"text": "What is harder to guess are the labels of the dependencies connecting the fragments.", "labels": [], "entities": []}, {"text": "In this paper we systematically investigate the labelling task by automatically deleting function labels from Lexical-Functional Grammar-based parser output for German and English Europarl data, and then restoring them using a Naive Bayes classifier trained on attribute names and attribute values of the f-structure fragments.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 180, "end_pos": 193, "type": "DATASET", "confidence": 0.8499537706375122}]}, {"text": "We achieve 82% (German) to 91% (English) accuracy for both single and multiple missing function labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9917694330215454}]}, {"text": "The paper is organized as follows: in Section 2 we define the problem and the proposed solution more formally.", "labels": [], "entities": []}, {"text": "Section 3 details the experimental evaluations, and in Section 4 we present our conclusions.: Example of a \"broken\" f-structure (simplified).", "labels": [], "entities": []}, {"text": "The sentence is 'Parliament adopted the resolution.'", "labels": [], "entities": []}, {"text": "The missing function off 1 is OBJ.", "labels": [], "entities": [{"text": "OBJ", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9926697015762329}]}], "datasetContent": [{"text": "We present two experiments which assess the accuracy of the proposed approach and compare different variants of it in order to select the best, and an additional one which assesses the usefulness of the approach for practical machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9987792372703552}, {"text": "machine translation", "start_pos": 226, "end_pos": 245, "type": "TASK", "confidence": 0.7573896646499634}]}, {"text": "For our experiments we used sentences from the German-English part of the Europarl corpus () parsed into f-structures with the XLE parser ( ) using English () and German () LFGs.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9839330613613129}]}, {"text": "We parsed only sentences of length 5-15 words.", "labels": [], "entities": [{"text": "parsed", "start_pos": 3, "end_pos": 9, "type": "TASK", "confidence": 0.9668461084365845}]}, {"text": "For the first two experiments, we picked 2000 sentences for training and 1000 for testing for both languages.", "labels": [], "entities": []}, {"text": "We ignored robustness features (FIRST, REST), functions related to c-structure constraints (MOTHER, LEFT SISTER, etc.), and TOPIC.", "labels": [], "entities": [{"text": "FIRST", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9347327947616577}, {"text": "REST", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.8772706985473633}, {"text": "MOTHER", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9670530557632446}, {"text": "LEFT", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.958632230758667}, {"text": "TOPIC", "start_pos": 124, "end_pos": 129, "type": "METRIC", "confidence": 0.9124311208724976}]}, {"text": "Of the remaining functions, we considered only those occurring in the PREDsonly part of f-structure.", "labels": [], "entities": []}, {"text": "If a dependent f-structure has multiple functions within the same parent fstructure, only the first function occurring in the description is considered.", "labels": [], "entities": []}, {"text": "This does not unduely influence the results, as the grammatical function of an f-structure, after exclusion of TOPIC, carries multiple labels in only about 2% of the cases in the English data and about 1% in the German data.", "labels": [], "entities": [{"text": "TOPIC", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.7050570845603943}, {"text": "German data", "start_pos": 212, "end_pos": 223, "type": "DATASET", "confidence": 0.7936777174472809}]}, {"text": "In we provide some useful statistics to help the reader interpret the results of the experiments.", "labels": [], "entities": []}, {"text": "The goal of this experiment is to evaluate the accuracy of the Bayesian guesser in the case when the grammatical function is unknown only for one dependent f-structure, and to assess whether the inclusion of attribute values into the feature set improves the results, and whether attributes other than CASE are useful.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9991170763969421}]}, {"text": "As a baseline, we used a pick-mostfrequent algorithm MF which considers only the function's prior probability and the presence of this function in the parent (returning to Equations (1) and (2), MF is in fact Naive Bayes with an empty feature set \u039e).", "labels": [], "entities": []}, {"text": "The guesser was evaluated in three variants: NB-CASE with the feature set formed only from the values of CASE attributes (if the f-structure has no CASE feature, the classifier degenerates to MF), NB-N with the feature set formed only from attribute names, and NB-N&V with the feature set formed from both attribute names and values.", "labels": [], "entities": []}, {"text": "All grammatical functions in the test set were used as test cases.", "labels": [], "entities": []}, {"text": "At each step in the evaluation, one function was removed and then guessed by each algorithm.", "labels": [], "entities": []}, {"text": "For both languages the test set was split into 10 non-intersecting subsets with approximately equal numbers of grammatical functions in each, and the values obtained for the 10 subsets were further used to assess the statistical significance of the differences in the results with the paired Student's t-test.", "labels": [], "entities": []}, {"text": "For both English and German all the three versions of the classifier clearly outperform the baseline, and even the advantage of NB-CASE over the baseline is statistically significant at the 0.5% level for both languages.", "labels": [], "entities": []}, {"text": "However, NB-CASE performs much worse than NB-N and NB-N&V (their advantage over NB-CASE is statistically significant at the 0.5% level for both languages), confirming that Language MF NB-S NB-J English 22.0% 90.4% 91.2% German 17.1% 81.4% 82.1%: Experiment 2: Guessing Multiple Missing Functions.", "labels": [], "entities": [{"text": "Guessing Multiple Missing Functions", "start_pos": 260, "end_pos": 295, "type": "TASK", "confidence": 0.8346683233976364}]}, {"text": "MF is the pick-most-frequent classifier.", "labels": [], "entities": []}, {"text": "NB-S and NB-J are one-by-one and joinprobability-based Naive Bayesian guessers.", "labels": [], "entities": [{"text": "NB-S", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9033269882202148}]}, {"text": "CASE is not the only feature which is useful in our task.", "labels": [], "entities": [{"text": "CASE", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9435421824455261}]}, {"text": "The increase inaccuracy brought about by including the atomic attribute values into the feature space is visible and significant at the same level.", "labels": [], "entities": []}, {"text": "The increase is somewhat more pronounced for German than for English.", "labels": [], "entities": []}, {"text": "For English the inclusion of attribute values into the feature space affects primarily the accuracy of SUBJ vs. OBJ decisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9992566704750061}]}, {"text": "For German, the accuracy notably increases for telling SUBJ, OBJ and ADJ-GEN from one another.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9997935891151428}, {"text": "telling SUBJ", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.6006739288568497}, {"text": "OBJ", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.6913561820983887}]}, {"text": "The goal of this experiment is to assess the accuracy of the Bayesian guesser for multiple missing grammatical functions within one parent fstructure, and to compare the accuracy of oneby-one vs. joint-probability-based guessing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9989770650863647}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9989402890205383}]}, {"text": "Our evaluation procedure models the extreme case when the functions are unknown for all the dependent f-structures of a particular parent.", "labels": [], "entities": []}, {"text": "As a baseline, we use the same algorithm MF as in Experiment 1, applied to the missing grammatical functions one by one.", "labels": [], "entities": []}, {"text": "Two Bayesian guessers are evaluated, NB-S guessing the missing grammatical functions one by one, and NB-J guessing them all at once by maximizing the joint probability of the values.", "labels": [], "entities": []}, {"text": "Both Bayesian guessers use attribute names and values as features.", "labels": [], "entities": []}, {"text": "All grammatical functions in the test set were used as test cases.", "labels": [], "entities": []}, {"text": "At each step of the experiment, the grammatical functions of all the dependent f-structures of a particular parent were removed simultaneously, and then guessed with each of the algorithms considered in this experiment.", "labels": [], "entities": []}, {"text": "Statistical significance was assessed in the same way as in Experiment 1.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.7660177052021027}]}, {"text": "The one-by-one guesser and the joint-probabilitybased guesser perform nearly equally well, resulting inaccuracy levels very close to those obtained in Experiment 1 for f-structures with a single missing function.", "labels": [], "entities": []}, {"text": "Joint-probability-based guessing achieves an advantage which is statistically significant at the 0.5% level for both languages but is not exceeding 1% absolute improvement.", "labels": [], "entities": []}, {"text": "For both languages errors typically occur in distinguishing OBJ vs. SUBJ and ADJUNCT vs. MOD, and additionally in XCOMP vs. OBJ for English.", "labels": [], "entities": []}, {"text": "The goal of this experiment is to see how the method influences the results of an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9953126907348633}]}, {"text": "For this experiment we use the Sulis SMT system (, and a decoder, which selects the transfer rules by maximizing the source-to-target probability of the complete translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8501573801040649}]}, {"text": "Such a decoder, though simple, allows us to create a realistic environment for evaluation.", "labels": [], "entities": []}, {"text": "From the f-structures produced by the decoder, candidate sentences are generated with XLE, and then the one best translation is selected for each sentence using a language model.", "labels": [], "entities": [{"text": "XLE", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.915643572807312}]}, {"text": "The function guesser is used to postprocess the output of the decoder before sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7462297677993774}]}, {"text": "In the experiment, the function guesser uses both attribute names and values to make a guess.", "labels": [], "entities": []}, {"text": "Guessing of multiple missing functions is performed one-byone, as joint guessing complicates the algorithm and leads to a very small improvement inaccuracy.", "labels": [], "entities": [{"text": "Guessing of multiple missing functions", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8579060792922973}]}, {"text": "The function guesser is trained on 3000 sentences, which area subset of the set used for inducing the transfer rules.", "labels": [], "entities": []}, {"text": "The overall MT system is evaluated both with and without function guessing on 500 held-out sentences, and the quality of the translation is measured using the BLEU metric ().", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9922886490821838}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9974024891853333}]}, {"text": "We also calculate the number of sentences for which the generator output is unempty.", "labels": [], "entities": []}, {"text": "The system without function guesser produced results for 364 sentences out of 500, with BLEU score equal to 5.69%; with function guesser the number of successfully generated sentences increases to 433, with BLEU improving to 6.95%.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9769468009471893}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9995272159576416}]}, {"text": "Thus, the absolute increase of BLEU score brought about by the guesser is 1.24%.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9792680144309998}]}, {"text": "This suggests that the algorithm succeeds on real data and is useful in grammar-based machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.7034404426813126}]}], "tableCaptions": [{"text": " Table 1: Data used in the evaluation. N-GF is the number of different grammatical functions occurring  in the dataset. N-DEP is the number of dependent f-structures in the test set. AVG-DEP, MIN-DEP,  MAX-DEP is the average, min. and max. number of dependant structures per parent in the test set.", "labels": [], "entities": [{"text": "AVG-DEP", "start_pos": 183, "end_pos": 190, "type": "METRIC", "confidence": 0.9691480398178101}]}, {"text": " Table 2: Experiment 1: Guessing a Single Miss- ing Grammatical Function. MF is the pick-most- frequent classifier. NB-CASE is Naive Bayes  (NB) with only CASE values used as features. NB- N is NB with only attribute names used as fea- tures. NB-N&V is NB with both attribute names  and atomic attribute values used as features.", "labels": [], "entities": [{"text": "Guessing a Single Miss- ing Grammatical Function", "start_pos": 24, "end_pos": 72, "type": "TASK", "confidence": 0.7560078874230385}]}]}