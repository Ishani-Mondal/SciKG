{"title": [{"text": "A Proposal on Evaluation Measures for RTE", "labels": [], "entities": [{"text": "Evaluation Measures", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.6874807476997375}, {"text": "RTE", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.927668571472168}]}], "abstractContent": [{"text": "We outline problems with the interpretation of accuracy in the presence of bias, arguing that the issue is a particularly pressing concern for RTE evaluation.", "labels": [], "entities": [{"text": "interpretation of", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8863617181777954}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.7823991775512695}, {"text": "RTE evaluation", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.957724004983902}]}, {"text": "Furthermore , we argue that average precision scores are unsuitable for RTE, and should not be reported.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.9569577872753143}, {"text": "RTE", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9339156746864319}]}, {"text": "We advocate mutual information as anew evaluation measure that should be reported in addition to accuracy and confidence-weighted score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9996094107627869}, {"text": "confidence-weighted score", "start_pos": 110, "end_pos": 135, "type": "METRIC", "confidence": 0.9104851186275482}]}], "introductionContent": [{"text": "We assume that the reader is familiar with the evaluation methodology employed in the RTE challenge.", "labels": [], "entities": [{"text": "RTE challenge", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.8856115937232971}]}, {"text": "We address the following three problems we currently see with this methodology.", "labels": [], "entities": []}, {"text": "1. The distribution of three-way gold standard labels is neither balanced nor representative of an application scenario.", "labels": [], "entities": []}, {"text": "Yet, systems are rewarded for learning this artificial bias from training data, while there is no indication of whether they could learn a different bias.", "labels": [], "entities": []}, {"text": "2. The notion of confidence ranking is misleading in the context of evaluating a ranking by average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.901389479637146}]}, {"text": "The criteria implicitly invoked on rankings by the current evaluation measures can, in fact, contradict those invoked on labellings derived by rank-based thresholding.", "labels": [], "entities": []}, {"text": "3. Language allows for the expression of logical negation, thus imposing asymmetry on the judgements ENTAILED vs. CONTRADICTION.", "labels": [], "entities": [{"text": "ENTAILED", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9417445063591003}]}, {"text": "Average precision does not properly reflect this symmetry.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9986475110054016}]}, {"text": "In this paper, we will first summarize relevant aspects of the current methodology, and outline these three problems in greater depth.", "labels": [], "entities": []}, {"text": "The problem of bias is quite general and widely known.", "labels": [], "entities": []}, {"text": "discuss it in the context of Cohen's kappa, which is one way of addressing the problem.", "labels": [], "entities": []}, {"text": "Yet, it has not received sufficient attention in the RTE community, which is why we will show how it applies to RTE, in particular, and why it is an especially pressing concern for RTE.", "labels": [], "entities": [{"text": "RTE", "start_pos": 181, "end_pos": 184, "type": "TASK", "confidence": 0.8055049777030945}]}, {"text": "Average precision has been imported into the RTE evaluation methodology from IR, tacitly assuming a great level of analogy between IR and RTE.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9793077111244202}, {"text": "RTE evaluation", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.8474099636077881}]}, {"text": "However, we will argue that the analogy is flawed, and that average precision is not suitable for RTE evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9094229340553284}, {"text": "RTE evaluation", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.8972748517990112}]}, {"text": "Then, we will then reframe the problem in information theoretic terms, advocating mutual information as anew evaluation measure.", "labels": [], "entities": []}, {"text": "We will show that it addresses all of the issues raised concerning accuracy and average precision and has advantages over Cohen's kappa.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9987983703613281}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.8962664604187012}]}], "datasetContent": [], "tableCaptions": []}