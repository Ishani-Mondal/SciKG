{"title": [{"text": "Evaluating a Statistical CCG Parser on Wikipedia", "labels": [], "entities": [{"text": "Statistical CCG Parser", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.5344005326430002}]}], "abstractContent": [{"text": "The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9404201805591583}, {"text": "1984 Wall Street Journal (WSJ)", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.8876824719565255}]}, {"text": "In-domain evaluation of this kind is important for system development, but gives little indication about how the parser will perform on many practical problems.", "labels": [], "entities": []}, {"text": "Wikipedia is an interesting domain for parsing that has so far been under-explored.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8711551427841187}, {"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9710803031921387}]}, {"text": "We present statistical parsing results that for the first time provide information about what sort of performance a user parsing Wikipedia text can expect.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.49578872323036194}, {"text": "parsing Wikipedia text", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.7790510654449463}]}, {"text": "We find that the C&C parser's standard model is 4.3% less accurate on Wikipedia text, but that a simple self-training exercise reduces the gap to 3.8%.", "labels": [], "entities": [{"text": "C&C parser", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.7870548218488693}]}, {"text": "The self-training also speeds up the parser on newswire text by 20%.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern statistical parsers are able to retrieve accurate syntactic analyses for sentences that closely match the domain of the parser's training data.", "labels": [], "entities": []}, {"text": "Breaking this domain dependence is now one of the main challenges for increasing the industrial viability of statistical parsers.", "labels": [], "entities": []}, {"text": "Substantial progress has been made in adapting parsers from newswire domains to scientific domains, especially for biomedical literature.", "labels": [], "entities": []}, {"text": "However, there is also substantial interest in parsing encyclopedia text, particularly Wikipedia.", "labels": [], "entities": [{"text": "parsing encyclopedia text", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.8877456585566202}]}, {"text": "Wikipedia has become an influential resource for NLP for many reasons.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8986976146697998}]}, {"text": "In addition to its variety of interesting metadata, it is massive, constantly updated, and multilingual.", "labels": [], "entities": []}, {"text": "Wikipedia is now given its own submission keyword in general CL conferences, and there are workshops largely centred around exploiting it and other collaborative semantic resources.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8564211130142212}]}, {"text": "Despite this interest, there have been few investigations into how accurately existing NLP processing tools work on Wikipedia text.", "labels": [], "entities": []}, {"text": "If it is found that Wikipedia text poses new challenges for our processing tools, then our results will constitute a baseline for future development.", "labels": [], "entities": []}, {"text": "On the other hand, if we find that models trained on newswire text perform well, we will have discovered another interesting way Wikipedia text can be exploited.", "labels": [], "entities": []}, {"text": "This paper presents the first evaluation of a statistical parser on Wikipedia text.", "labels": [], "entities": []}, {"text": "The only previous published results we are aware of were described by, who ran the LinGo HPSG parser over Wikipedia, and found that the correct parse was in the top 500 returned parses for 60% of sentences.", "labels": [], "entities": [{"text": "LinGo HPSG parser over Wikipedia", "start_pos": 83, "end_pos": 115, "type": "DATASET", "confidence": 0.8323182940483094}]}, {"text": "This is an interesting result, but one that gives little indication of how well a user could expect a parser to actually annotate Wikipedia text, or how to go about adjusting one if its performance is inadequate.", "labels": [], "entities": []}, {"text": "To investigate this, we randomly selected 200 sentences from Wikipedia, and hand-labelled them with CCG annotation in order to evaluate the C&C parser.", "labels": [], "entities": []}, {"text": "C&C is the fastest deep-grammar parser, making it a likely choice for parsing Wikipedia, given its size.", "labels": [], "entities": [{"text": "parsing Wikipedia", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8418292105197906}]}, {"text": "Even at the parser's WSJ speeds, it would take about 18 days to parse the current English Wikipedia on a single CPU.", "labels": [], "entities": [{"text": "parse the current English Wikipedia", "start_pos": 64, "end_pos": 99, "type": "TASK", "confidence": 0.8231043934822082}]}, {"text": "We find that the parser is 54% slower on Wikipedia text, so parsing a full dump is inconvenient at best.", "labels": [], "entities": []}, {"text": "The parser is only 4.3% less accurate, however.", "labels": [], "entities": []}, {"text": "We then examine how these figures might be improved.", "labels": [], "entities": []}, {"text": "We try a simple domain adaptation experiment, using self-training.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.748060405254364}]}, {"text": "One of our experiments, which involves self-training using the Simple English Wikipedia, improves the accuracy of the parser's standard model on Wikipedia by 0.8%.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.8044989307721456}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9995220899581909}]}, {"text": "The bootstrapping also makes the parser faster.", "labels": [], "entities": []}, {"text": "Parse speeds on newswire text improve 20%, and speeds on Wikipedia improve by 34%.", "labels": [], "entities": [{"text": "Parse", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9663854241371155}, {"text": "Wikipedia", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9437536597251892}]}], "datasetContent": [{"text": "The inclusion of parsed data from Wikipedia articles in the supertagger's training data improves its accuracy on Wikipedia data, with the FEW enhanced model achieving 89.86% accuracy, compared with the original accuracy of 88.77%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9992226362228394}, {"text": "Wikipedia data", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.9158158898353577}, {"text": "FEW", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.7570173144340515}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.999269425868988}, {"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9977431297302246}]}, {"text": "The SEW enhanced supertagger achieved 89.45% accuracy.", "labels": [], "entities": [{"text": "SEW", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.5347933769226074}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9973800778388977}]}, {"text": "The derivs model parser improves inaccuracy by 0.8%, the hybrid model by 0.5%.", "labels": [], "entities": []}, {"text": "The out-of-domain training data had little impact on the models' accuracy on the WSJ, but did improve parse speed by 20%, as it did on Wikipedia.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9989732503890991}, {"text": "WSJ", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.7992690801620483}, {"text": "speed", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.781288743019104}]}, {"text": "The speed increases because the supertagger's beam width is decided by its confidence scores, which are more narrowly distributed after the model has been trained with more data.", "labels": [], "entities": [{"text": "speed", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9930958151817322}]}, {"text": "After self-training, the derivs and hybrid models performed equally accurately.", "labels": [], "entities": []}, {"text": "With no reason to use the hybrid model, the total speed increase is 34%.", "labels": [], "entities": [{"text": "speed increase", "start_pos": 50, "end_pos": 64, "type": "METRIC", "confidence": 0.9331629574298859}]}, {"text": "With our pre-processing, the full Wikipedia dump had close to 1 billion words, so speed is an important factor.", "labels": [], "entities": [{"text": "Wikipedia dump", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8968930542469025}, {"text": "speed", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9987651109695435}]}, {"text": "Overall, our simple self-training experiment was quite successful.", "labels": [], "entities": []}, {"text": "This result may seem surprising given that the CoNLL 2007 participants generally failed to use similar resources to adapt dependency parsers to biomedical text (.", "labels": [], "entities": [{"text": "CoNLL 2007 participants", "start_pos": 47, "end_pos": 70, "type": "DATASET", "confidence": 0.9064450860023499}]}, {"text": "However, our results confirm finding that the C&C parser's division of labour between the supertagger and parser make it easier to adapt to new domains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence lengths before (and after) length filter.", "labels": [], "entities": []}, {"text": " Table 2: Parsing results with automatic POS tags. SEW and FEW models incorporate self-training.", "labels": [], "entities": [{"text": "SEW", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.4974181652069092}, {"text": "FEW", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9577962160110474}]}]}