{"title": [{"text": "An Ordering of Terms Based on Semantic Relatedness", "labels": [], "entities": [{"text": "Semantic Relatedness", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.7257275581359863}]}], "abstractContent": [{"text": "Term selection methods typically employ a statistical measure to filter or weight terms.", "labels": [], "entities": [{"text": "Term selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9632164537906647}]}, {"text": "Term expansion for IR may also depend on statistics, or use some other, non-metric method based on a lexical resource.", "labels": [], "entities": [{"text": "Term expansion", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9098534286022186}, {"text": "IR", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9891777038574219}]}, {"text": "At the same time, a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.709145704905192}]}, {"text": "This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure.", "labels": [], "entities": []}, {"text": "This semantic order can be exploited by term weighting and term expansion methods.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.685756504535675}, {"text": "term expansion", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.6576702147722244}]}], "introductionContent": [{"text": "Since the early days of the vector space model, it has been debated whether it is a proper carrier of meaning of texts, arguing if distributional similarity is an adequate proxy for lexical semantic relatedness.", "labels": [], "entities": []}, {"text": "With the statistical, i.e. devoid of word semantics approaches there is generally noway to improve both precision and recall at the same time, increasing one is done at the expense of the other.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9991449117660522}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9971875548362732}]}, {"text": "For example, casting a wider net of search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.994358479976654}, {"text": "precision", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9987539052963257}]}, {"text": "In the meantime, practical applications in information retrieval and text classification have been proliferating, especially with developments in kernel methods in the last decade.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.817626029253006}, {"text": "text classification", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7983233332633972}]}, {"text": "Ordering of terms based on semantic relatedness seeks an answer to the simple question, can statistical term weighting be eclipsed?", "labels": [], "entities": [{"text": "statistical term weighting", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.6635996103286743}]}, {"text": "Namely, variants of weighting schemes based on term occurrences and co-occurrences dominate the information retrieval and text classification scenes.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.7910480797290802}, {"text": "text classification", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7776106297969818}]}, {"text": "However, they also have a number of limitations.", "labels": [], "entities": []}, {"text": "The connection between statistics and word semantics is in general not understood very well.", "labels": [], "entities": []}, {"text": "In other words, a systematic discussion of mappings between theories of word meaning and modeling them by mathematical objects is missing for the time being.", "labels": [], "entities": []}, {"text": "Further, enriching weighting schemes by importing their sense content from lexical resources such as WordNet lacks a theoretical interpretation in terms of lexical semantics.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9671344757080078}]}, {"text": "Combining co-occurrence and lexical resource-based approaches for term weighting and term expansion may offer further theoretical insights, as well as performance benefits.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7045677602291107}, {"text": "term expansion", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7247218489646912}]}, {"text": "Using vectors in the vector space model as such mathematical objects for the representation of term, document or query meaning necessarily expresses content mapped on form as a set of coordinates.", "labels": [], "entities": []}, {"text": "These coordinates, at least in the case of the tfidf scheme, are corpus-specific, i.e. term weights are neither constant overtime nor database independent.", "labels": [], "entities": []}, {"text": "Introducing a semantic ordering of terms, hence loading a coordinate with semantic content, reduces the dependence on a specific corpus.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses the most important measures for semantic relatedness with regard to the major linguistic theories.", "labels": [], "entities": [{"text": "semantic relatedness", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7273205816745758}]}, {"text": "Section 3 introduces an algorithm that creates a linear semantic order of terms of a corpus, and Section 4 both offers first results in text classification and discusses some implications.", "labels": [], "entities": [{"text": "text classification", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.8085345327854156}]}, {"text": "Finally, Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Micro-Average and Macro F 1 -measure, Reuters-21578", "labels": [], "entities": [{"text": "Macro F 1 -measure", "start_pos": 28, "end_pos": 46, "type": "METRIC", "confidence": 0.7707242608070374}, {"text": "Reuters-21578", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9432410001754761}]}]}