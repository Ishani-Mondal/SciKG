{"title": [{"text": "The TUNA-REG Challenge 2009: Overview and Evaluation Results", "labels": [], "entities": [{"text": "TUNA-REG Challenge 2009", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.6977985699971517}]}], "abstractContent": [{"text": "The TUNA-REG'09 Challenge was one of the shared-task evaluation competitions at Generation Challenges 2009.", "labels": [], "entities": []}, {"text": "TUNA-REG'09 used data from the TUNA Corpus of paired representations of entities and human-authored referring expressions.", "labels": [], "entities": [{"text": "TUNA-REG'09", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9603687524795532}, {"text": "TUNA Corpus", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9476010799407959}]}, {"text": "The shared task was to create systems that generate referring expressions for entities given representations of sets of entities and their properties.", "labels": [], "entities": []}, {"text": "Four teams submitted six systems to TUNA-REG'09.", "labels": [], "entities": [{"text": "TUNA-REG'09", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8868316411972046}]}, {"text": "We evaluated the six systems and two sets of human-authored referring expressions using several automatic intrinsic measures, a human-assessed intrinsic evaluation and a human task performance experiment.", "labels": [], "entities": []}, {"text": "This report describes the TUNA-REG task and the evaluation methods used, and presents the evaluation results.", "labels": [], "entities": [{"text": "TUNA-REG task", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.509533703327179}]}], "introductionContent": [{"text": "This year's run of the TUNA-REG Shared-Task Evaluation Competition (STEC) is the third, and final, competition to involve the TUNA Corpus of referring expressions.", "labels": [], "entities": [{"text": "TUNA-REG Shared-Task Evaluation Competition (STEC)", "start_pos": 23, "end_pos": 73, "type": "TASK", "confidence": 0.7195494387831006}, {"text": "TUNA Corpus of referring expressions", "start_pos": 126, "end_pos": 162, "type": "DATASET", "confidence": 0.8776441812515259}]}, {"text": "The TUNA Corpus was first used in the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge) which took place between May and September 2007; and again for three of the shared tasks in Referring Expression Generation (REG) Challenges 2008, which ran between", "labels": [], "entities": [{"text": "TUNA Corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8907320499420166}, {"text": "Referring Expression Generation (REG) Challenges 2008", "start_pos": 212, "end_pos": 265, "type": "TASK", "confidence": 0.7958462312817574}]}], "datasetContent": [{"text": "We used a range of different evaluation methods, including intrinsic and extrinsic, 5 automatically computed and human-evaluated, as shown in the overview in.", "labels": [], "entities": []}, {"text": "Participants computed automatic intrinsic evaluation scores on the development set (using the teval program provided by us).", "labels": [], "entities": []}, {"text": "We performed all of the evaluations shown in on the test data set.", "labels": [], "entities": [{"text": "test data set", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.8769388198852539}]}, {"text": "For all measures, results were computed both (a) overall, using the entire test data set, and (b) by entity type, that is, computing separate values for outputs in the furniture and in the people domain.", "labels": [], "entities": []}, {"text": "Evaluation methods for each evaluation type and corresponding evaluation results are presented in the following three sections.", "labels": [], "entities": []}, {"text": "Humanlikeness, by which we mean the similarity of system outputs to sets of human-produced reference 'outputs', was assessed using Accuracy,   string-edit distance, BLEU-3 and NIST-5.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9970965385437012}, {"text": "BLEU-3", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9978359341621399}, {"text": "NIST-5", "start_pos": 176, "end_pos": 182, "type": "DATASET", "confidence": 0.961754560470581}]}, {"text": "Accuracy measures the percentage of cases where a system's output word string was identical to the corresponding description in the corpus.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9913033246994019}]}, {"text": "Stringedit distance (SE) is the classic Levenshtein distance measure and computes the minimal number of insertions, deletions and substitutions required to transform one string into another.", "labels": [], "entities": [{"text": "Stringedit distance (SE)", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.7831920444965362}]}, {"text": "We set the cost for insertions and deletions to 1, and that for substitutions to 2.", "labels": [], "entities": []}, {"text": "If two strings are identical, then this metric returns 0 (perfect match).", "labels": [], "entities": []}, {"text": "Otherwise the value depends on the length of the two strings (the maximum value is the sum of the lengths).", "labels": [], "entities": []}, {"text": "As an aggregate measure, we compute the mean of pairwise SE scores.", "labels": [], "entities": [{"text": "SE", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.8933165073394775}]}, {"text": "BLEU-x is an n-gram based string comparison measure, originally proposed by for evaluation of Machine Translation systems.", "labels": [], "entities": [{"text": "BLEU-x", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9802247881889343}, {"text": "Machine Translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.8221368491649628}]}, {"text": "It computes the proportion of word n-grams of length x and less that a system output shares with several reference outputs.", "labels": [], "entities": []}, {"text": "Setting x = 4 (i.e. considering all n-grams of length \u2264 4) is standard, but because many of the TUNA descriptions are shorter than 4 tokens, we compute BLEU-3 instead.", "labels": [], "entities": [{"text": "TUNA", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.8434816598892212}, {"text": "BLEU-3", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9962726831436157}]}, {"text": "BLEU ranges from 0 to 1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.997653067111969}]}, {"text": "NIST is aversion of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent n-grams, which are taken to be more informative.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.896018922328949}, {"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9984416365623474}, {"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9962664246559143}, {"text": "NIST", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7996540069580078}]}, {"text": "The maximum NIST score depends on the size of the test set.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.6141000688076019}]}, {"text": "Unlike string-edit distance, BLEU and NIST are by definition aggregate measures (i.e. a single score is obtained fora peer system based on the entire set of items to be compared, and this is not generally equal to the average of scores for individual items).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9803628325462341}, {"text": "NIST", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8948204517364502}]}, {"text": "Because the test data has two human-authored reference descriptions per domain, the Accuracy and SE scores had to be computed slightly differently to obtain test data scores (whereas BLEU and NIST are designed for multiple reference texts).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9987818598747253}, {"text": "SE", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9851497411727905}, {"text": "BLEU", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9910128712654114}, {"text": "NIST", "start_pos": 192, "end_pos": 196, "type": "DATASET", "confidence": 0.945678174495697}]}, {"text": "For the test data only, therefore, Accuracy expresses the percentage of a system's outputs that match at least one of the reference outputs, and SE is the average of the two pairwise scores against the reference outputs. is an overview of the selfreported scores on the development set included in the participants' reports (not all participants report Accuracy scores).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9989503026008606}, {"text": "SE", "start_pos": 145, "end_pos": 147, "type": "METRIC", "confidence": 0.9988077878952026}, {"text": "Accuracy", "start_pos": 353, "end_pos": 361, "type": "METRIC", "confidence": 0.9951209425926208}]}, {"text": "The corresponding scores for the test data set as well as NIST scores for the test data (all computed by us), are shown in.", "labels": [], "entities": [{"text": "NIST", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.6521459221839905}]}, {"text": "The table also includes the result of comparing the two sets of human descriptions, HUMAN-1 and HUMAN-2, to each other using the same metrics (their scores are distinct only for non-commutative measures, i.e. NIST and BLEU).", "labels": [], "entities": [{"text": "NIST", "start_pos": 209, "end_pos": 213, "type": "DATASET", "confidence": 0.9113683700561523}, {"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9923579096794128}]}, {"text": "The TUNA'09 Challenge was the first TUNA shared-task competition to include an intrinsic evaluation involving human judgments of quality.", "labels": [], "entities": []}, {"text": "As for earlier shared tasks involving the TUNA data, we carried out a task-performance experiment in which subjects have the task of identifying intended referents.", "labels": [], "entities": [{"text": "TUNA data", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.8599029183387756}]}], "tableCaptions": [{"text": " Table 2: TUNA-REG data: subset sizes.", "labels": [], "entities": [{"text": "TUNA-REG data", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.7185106873512268}]}, {"text": " Table 4: Participating teams' self-reported automatic intrinsic scores on development data set with single  human-authored reference description (listed in order of overall mean SE score).", "labels": [], "entities": [{"text": "SE score", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9552404284477234}]}, {"text": " Table 5: Automatic intrinsic scores on test data set with two human-authored reference descriptions  (listed in order of overall mean SE score).", "labels": [], "entities": [{"text": "SE score", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9540377855300903}]}, {"text": " Table 6: Human-assessed intrinsic scores on test data set, including the two sets of human-authored  reference descriptions (listed in order of overall mean Adequacy score).", "labels": [], "entities": [{"text": "Adequacy score", "start_pos": 158, "end_pos": 172, "type": "METRIC", "confidence": 0.9444247186183929}]}, {"text": " Table 8: Identification speed and accuracy per system. Systems are displayed in descending order of  overall identification accuracy.", "labels": [], "entities": [{"text": "Identification speed", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8504126071929932}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9993113279342651}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9818321466445923}]}, {"text": " Table 10: Correlations (Pearson's r) between all evaluation measures. (  *  significant at p \u2264 .05;   *  *  significant at p \u2264 .01)", "labels": [], "entities": [{"text": "Pearson's r)", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.89080810546875}]}]}