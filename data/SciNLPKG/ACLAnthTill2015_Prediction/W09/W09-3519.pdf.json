{"title": [{"text": "A Noisy Channel Model for Grapheme-based Machine Transliteration", "labels": [], "entities": [{"text": "Grapheme-based Machine Transliteration", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.5766236186027527}]}], "abstractContent": [{"text": "Machine transliteration is an important Natural Language Processing task.", "labels": [], "entities": [{"text": "Machine transliteration", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8177463412284851}, {"text": "Natural Language Processing", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6267715593179067}]}, {"text": "This paper proposes a Noisy Channel Model for Graph-eme-based machine transliteration.", "labels": [], "entities": [{"text": "Graph-eme-based machine transliteration", "start_pos": 46, "end_pos": 85, "type": "TASK", "confidence": 0.6743470231691996}]}, {"text": "Moses, a phrase-based Statistical Machine Translation tool, is employed for the implementation of the system.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6831201513608297}]}, {"text": "Experiments are carried out on the NEWS 2009 Machine Transliteration Shared Task English-Chinese track.", "labels": [], "entities": [{"text": "NEWS 2009 Machine Transliteration Shared Task English-Chinese track", "start_pos": 35, "end_pos": 102, "type": "DATASET", "confidence": 0.8686049580574036}]}, {"text": "English-Chinese back transliteration is studied as well.", "labels": [], "entities": [{"text": "English-Chinese back transliteration", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.49683260917663574}]}], "introductionContent": [{"text": "Transliteration is defined as phonetic translation of names across languages.", "labels": [], "entities": [{"text": "phonetic translation of names across languages", "start_pos": 30, "end_pos": 76, "type": "TASK", "confidence": 0.8364028831322988}]}, {"text": "Transliteration of Named Entities is necessary in many applications, such as machine translation, corpus alignment, cross-language information retrieval, information extraction and automatic lexicon acquisition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8157468140125275}, {"text": "corpus alignment", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7787365615367889}, {"text": "cross-language information retrieval", "start_pos": 116, "end_pos": 152, "type": "TASK", "confidence": 0.7106247742970785}, {"text": "information extraction", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.8301085531711578}, {"text": "automatic lexicon acquisition", "start_pos": 181, "end_pos": 210, "type": "TASK", "confidence": 0.6465756495793661}]}, {"text": "The transliteration modeling approaches can be classified into phoneme-based, graphemebased and hybrid approach of phoneme and grapheme.", "labels": [], "entities": []}, {"text": "Many previous studies are devoted to the phoneme-based approach.", "labels": [], "entities": []}, {"text": "Suppose that E is an English name and C is its Chinese transliteration.", "labels": [], "entities": []}, {"text": "The phoneme-based approach first converts E into an intermediate phonemic representation p, and then converts pinto its Chinese counterpart C.", "labels": [], "entities": []}, {"text": "The idea is to transform both source and target names into comparable phonemes so that the phonetic similarity between two names can be measured easily.", "labels": [], "entities": []}, {"text": "The grapheme-based approach has also attracted much attention ().", "labels": [], "entities": []}, {"text": "It treats the transliteration as a statistical machine translation problem under monotonic constraint.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6203868091106415}]}, {"text": "The idea is to obtain the bilingual orthographical correspondence directly to reduce the possible errors introduced in multiple conversions.", "labels": [], "entities": []}, {"text": "The hybrid approach attempts to utilize both phoneme and grapheme information for transliteration.) proposed away to fuse both phoneme and grapheme features into a single learning process.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly describes the noisy channel model for machine transliteration.", "labels": [], "entities": [{"text": "machine transliteration", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7337377369403839}]}, {"text": "Section 3 introduces the model's implementation details.", "labels": [], "entities": []}, {"text": "Experiments and analysis are given in section 4.", "labels": [], "entities": []}, {"text": "Conclusions and future work are discussed in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the data sets, experimental setup, experiment results and analysis.", "labels": [], "entities": []}, {"text": "Both English-Chinese forward transliteration and back transliteration are studied.", "labels": [], "entities": []}, {"text": "The process can be divided into four steps: language model building, transliteration model training, weight tuning, and decoding.", "labels": [], "entities": [{"text": "language model building", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7126540144284567}, {"text": "weight tuning", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.7724508047103882}]}, {"text": "When building language model, data smoothing techniques Kneser-Ney and interpolate are employed.", "labels": [], "entities": [{"text": "data smoothing", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.8022211194038391}]}, {"text": "In transliteration model training step, the alignment heuristic is growdiag-final, while other parameters are default settings.", "labels": [], "entities": []}, {"text": "Tuning parameters are all defaults.", "labels": [], "entities": []}, {"text": "When decoding, the parameter distortion-limit is set to 0, meaning that no reordering operation is c lin ton needed.", "labels": [], "entities": []}, {"text": "The system outputs the 10-best distinct transliterations.", "labels": [], "entities": []}, {"text": "The whole training set is used for language model building and transliteration model training.", "labels": [], "entities": [{"text": "language model building", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7436862389246622}, {"text": "transliteration model training", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.7920587062835693}]}, {"text": "The development set is used for weight tuning and system testing.", "labels": [], "entities": [{"text": "weight tuning", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8465281426906586}]}, {"text": "The following 6 metrics are used to measure the quality of the transliteration results (): Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank (MRR), MAP ref , MAP 10 , and MAP sys . In the data of English-Chinese transliteration track, each source name only has one reference transliteration.", "labels": [], "entities": [{"text": "Word Accuracy in Top-1 (ACC)", "start_pos": 91, "end_pos": 119, "type": "METRIC", "confidence": 0.8172769887106759}, {"text": "Fuzziness in Top-1 (Mean F-score)", "start_pos": 121, "end_pos": 154, "type": "METRIC", "confidence": 0.7541599571704865}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 156, "end_pos": 182, "type": "METRIC", "confidence": 0.9647197822729746}, {"text": "MAP ref", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.8715932071208954}, {"text": "MAP 10", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.8000276684761047}]}, {"text": "Systems are required to output the 10-best unique transliterations for every source name.", "labels": [], "entities": []}, {"text": "Thus, MAP ref equals ACC, and MAP sys is the same or very close to MAP . So we only choose ACC, Mean F-score, MRR, and MAP 10 to show the system performance.", "labels": [], "entities": [{"text": "MAP ref", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.8101253509521484}, {"text": "ACC", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9682508707046509}, {"text": "ACC", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9869968891143799}, {"text": "Mean", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9301576614379883}, {"text": "F-score", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.5194623470306396}, {"text": "MRR", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.955315351486206}]}], "tableCaptions": [{"text": " Table 1. Training data statistics", "labels": [], "entities": [{"text": "Training data", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.7366047203540802}]}, {"text": " Table 2. E2C tuning performance (forward)", "labels": [], "entities": []}, {"text": " Table 3. E2C tuning performance (back)", "labels": [], "entities": []}, {"text": " Table 4. The final official results of E2C forward", "labels": [], "entities": [{"text": "E2C", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9388870000839233}]}]}