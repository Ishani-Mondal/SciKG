{"title": [{"text": "References Extension for the Automatic Evaluation of MT by Syntactic Hybridization", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.8468733429908752}, {"text": "Syntactic Hybridization", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.7451864182949066}]}], "abstractContent": [{"text": "Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.708133801817894}]}, {"text": "We propose a method to extend the reference set of the automatic evaluation only based on multiple manual references and their syntactic structures.", "labels": [], "entities": []}, {"text": "In our approach, the syntactic equivalents in the reference sentences are identified and hybridized to generate new references.", "labels": [], "entities": []}, {"text": "The new method need no external knowledge and can obtain the equivalents of long sub-segments of reference sentences.", "labels": [], "entities": []}, {"text": "The experimental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments.", "labels": [], "entities": []}], "introductionContent": [{"text": "While human evaluation of machine translation output remains the most reliable method to assess translation quality, it is a costly and time consuming process.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.7662942409515381}]}, {"text": "The development of automatic machine translation evaluation metrics enables the rapid assessment of system output.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8389909863471985}]}, {"text": "By providing immediate feedback on the effectiveness of various techniques, these metrics have guided machine translation research and have facilitated rapid advances in the state of the art.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8201791346073151}]}, {"text": "In addition, automatic evaluation metrics are useful in comparing the performance of multiple machine translation systems on a given translation task.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7448319494724274}]}, {"text": "Since automatic evaluation metrics are meant to serve as a surrogate for human judgments, their quality is determined by how well they correlate with assessors' preferences and how accurately they predicts human judgments.", "labels": [], "entities": []}, {"text": "Although current methods for automatically evaluating machine translation output do not require humans to assess individual system output, humans are nevertheless needed to generate a number of reference translations.", "labels": [], "entities": [{"text": "evaluating machine translation output", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.6607836708426476}]}, {"text": "The quality of machine-generated translations is determined by automatically comparing system output with these references.", "labels": [], "entities": []}, {"text": "All current automatic evaluation metrics are based on the various measures of the general similarity between the system translation and manual references.", "labels": [], "entities": []}, {"text": "This kind of method has an obvious drawback: it does not account for combinations of lexical and syntactic differences that might occur between a perfectly fluent and accuratelytranslated machine output and a human reference translation (beyond variations already captured by the different reference translations themselves).", "labels": [], "entities": []}, {"text": "Moreover, the set of human reference translations is unlikely to bean exhaustive inventory of \"good translations\" for any given foreign language sentence.", "labels": [], "entities": []}, {"text": "Therefore, it would be highly desirable to\ud97b\udf59 extend the coverage of the references for the similarity based evaluation methods.", "labels": [], "entities": []}, {"text": "To match the system translation with various presentation of the same meaning, many work haven been proposed to extend the references by generating lexical variations.", "labels": [], "entities": []}, {"text": "The first strategy focuses on the extension based on paraphrase identi-fication (.", "labels": [], "entities": []}, {"text": "In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of substrings they may share.", "labels": [], "entities": []}, {"text": "In short, all paraphrases of human-generated references should be considered \"good\" translations.", "labels": [], "entities": []}, {"text": "The second strategy extends the references with the synonymy ().", "labels": [], "entities": []}, {"text": "This is an alternation to obtain lexical variations with synonymy dictionaries instead of the paraphrase.", "labels": [], "entities": []}, {"text": "In this kind of method, the reference is matched against to the system translation with the pack of the synonymies of the reference words instead of the exact matching.", "labels": [], "entities": []}, {"text": "Both two strategies can successfully capture the lexical variations and greatly extend the coverage of the references.", "labels": [], "entities": []}, {"text": "But they still have two common deficiencies.", "labels": [], "entities": []}, {"text": "The first is the demand of the external knowledge.", "labels": [], "entities": []}, {"text": "Paraphrase based method need amass of external corpus to extract paraphrases and synonymy based method need manually constructed semantic dictionaries.", "labels": [], "entities": []}, {"text": "These demands seriously limit the application on various languages for which the external knowledge is absent.", "labels": [], "entities": []}, {"text": "Another deficiency is that the two strategies cannot capture the equivalents of long subsegments such as a clause.", "labels": [], "entities": []}, {"text": "Synonymy based method can only capture the equivalents of single words.", "labels": [], "entities": []}, {"text": "Paraphrase based method can capture the equivalents of longer units but the length is still very narrow.", "labels": [], "entities": [{"text": "Paraphrase", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8378719687461853}]}, {"text": "In many cases, some long subsegments can be varied with an entirely different presentation which cannot be decomposed into the variations of words or phrases.", "labels": [], "entities": []}, {"text": "To address these problems we propose a novel strategy to generate variations presentation only using existing multiple manual references without any external knowledge.", "labels": [], "entities": []}, {"text": "We identify the syntactic components on different level as the replaceable units and determine the syntactic equivalents of the components in the corresponding references.", "labels": [], "entities": []}, {"text": "Then the equivalents of the syntactic components are hybridized into new references.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the concept and identification of the syntactic equivalents.", "labels": [], "entities": []}, {"text": "Section 3 proposes a process to hybridize the syntactic equivalents efficiently.", "labels": [], "entities": []}, {"text": "Experimental results are illustrated in section 4.", "labels": [], "entities": []}, {"text": "We also include some related discussion in Section 5.", "labels": [], "entities": []}, {"text": "Finally this work is concluded in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will show experimental results in this section to verify the effectiveness of the extended set of hybridized reference sentences.", "labels": [], "entities": []}, {"text": "In the experiments, multiple translations of the source language sentences are evaluated with several popular automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively.", "labels": [], "entities": []}, {"text": "Finally, the Pearson's correlations between the human assessments and evaluation scores using two reference set are calculated and compared.", "labels": [], "entities": [{"text": "Pearson's correlations", "start_pos": 13, "end_pos": 35, "type": "METRIC", "confidence": 0.9282945990562439}]}, {"text": "The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.8662218153476715}, {"text": "ACL05", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8985931873321533}, {"text": "NistMATR08 (LDC2008E43)", "start_pos": 151, "end_pos": 174, "type": "DATASET", "confidence": 0.7946000546216965}]}, {"text": "& 2 describes the detail of the two datasets.", "labels": [], "entities": []}, {"text": "The popular automatic evaluation metrics include BLEU (), GTM (Melamed et al., 2003), Rouge () and METEOR ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9981820583343506}, {"text": "METEOR", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9376247525215149}]}, {"text": "The syntactic trees of the reference sentences are obtained with the Stanford statistical parser) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43.", "labels": [], "entities": [{"text": "LDC2006T04", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.8920295238494873}, {"text": "Collins parser (Collins 1999)", "start_pos": 117, "end_pos": 146, "type": "DATASET", "confidence": 0.8611159125963846}, {"text": "LDC2008E43", "start_pos": 151, "end_pos": 161, "type": "DATASET", "confidence": 0.8618412017822266}]}, {"text": "gives out the correlations using two reference set on both datasets.", "labels": [], "entities": []}, {"text": "The first column is the name of the used metrics.", "labels": [], "entities": []}, {"text": "The second column is the correlations based on the original reference set.", "labels": [], "entities": [{"text": "correlations", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.9327389001846313}]}, {"text": "The third column is the correlations based on the extended reference set.", "labels": [], "entities": [{"text": "correlations", "start_pos": 24, "end_pos": 36, "type": "METRIC", "confidence": 0.9210091829299927}]}, {"text": "In the experiment, the maximum length of N-gram in BLEU is 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9980081915855408}]}, {"text": "The exponent of GTM is 2.", "labels": [], "entities": []}, {"text": "ROUGE uses skip-bigram with a window of nine words.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6896474957466125}]}, {"text": "And METEOR is run in \"exact\" mode.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9034115672111511}, {"text": "exact", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9779732823371887}]}, {"text": "The syntactic structure of the original references:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3 Pearson's correlations with human assess- ments on sentence level on LDC2006T04", "labels": [], "entities": [{"text": "human assess- ments", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.6605942919850349}, {"text": "LDC2006T04", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.7373344302177429}]}, {"text": " Table 5 Counts of the tree nodes and equivalent  nodes in references.", "labels": [], "entities": []}, {"text": " Table 6 Pearson's correlations based on incremental  reference set", "labels": [], "entities": []}]}