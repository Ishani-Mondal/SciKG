{"title": [{"text": "An Intrinsic Stopping Criterion for Committee-Based Active Learning", "labels": [], "entities": [{"text": "Intrinsic Stopping Criterion", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7385371128718058}]}], "abstractContent": [{"text": "As supervised machine learning methods are increasingly used in language technology, the need for high-quality annotated language data becomes imminent.", "labels": [], "entities": []}, {"text": "Active learning (AL) is a means to alleviate the burden of annotation.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7255360543727875}]}, {"text": "This paper addresses the problem of knowing when to stop the AL process without having the human annotator make an explicit decision on the matter.", "labels": [], "entities": []}, {"text": "We propose and evaluate an intrinsic criterion for committee-based AL of named entity recognizers.", "labels": [], "entities": [{"text": "AL of named entity recognizers", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6481094241142273}]}], "introductionContent": [{"text": "With the increasing popularity of supervised machine learning methods in language processing, the need for high-quality labeled text becomes imminent.", "labels": [], "entities": []}, {"text": "On the one hand, the amount of readily available texts is huge, while on the other hand the labeling and creation of corpora based on such texts is tedious, error prone and expensive.", "labels": [], "entities": []}, {"text": "Active learning (AL) is one way of approaching the challenge of classifier creation and data annotation.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7799574792385101}, {"text": "classifier creation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8193925321102142}]}, {"text": "Examples of AL used in language engineering include named entity recognition), text categorization (), part-of-speech tagging (, and parsing).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.6198668380578359}, {"text": "part-of-speech tagging", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7212547659873962}]}, {"text": "AL is a supervised machine learning technique in which the learner is in control of the data used for learning -the control is used to query an oracle, typically a human, for the correct label of the unlabeled training instances for which the classifier learned so far makes unreliable predictions.", "labels": [], "entities": []}, {"text": "The AL process takes as input a set of labeled instances and a larger set of unlabeled instances, and produces a classifier and a relatively small set of newly labeled data.", "labels": [], "entities": []}, {"text": "The overall goal is to obtain as good a classifier as possible, without having to mark-up and supply the learner with more than necessary data.", "labels": [], "entities": []}, {"text": "The learning process aims at keeping the human annotation effort to a minimum, only asking for advice where the training utility of the result of such a query is high.", "labels": [], "entities": []}, {"text": "The approaches taken to AL in this paper are based on committees of classifiers with access to pools of data.", "labels": [], "entities": [{"text": "AL", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.990116536617279}]}, {"text": "outlines a prototypical committee-based AL loop.", "labels": [], "entities": [{"text": "committee-based AL loop", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.562710722287496}]}, {"text": "In this paper we focus on the question when AL-driven annotation should be stopped (Item 7 in.", "labels": [], "entities": []}, {"text": "Usually, the progress of AL is illustrated by means of a learning curve which depicts how the classifier's performance changes as a result of increasingly more labeled training data being available.", "labels": [], "entities": []}, {"text": "A learning curve might be used to address the issue of knowing when to stop the learning process -once the curve has leveled out, that is, when additional training data does not contribute (much) to increase the performance of the classifier, the AL process maybe terminated.", "labels": [], "entities": []}, {"text": "While in a random selection scenario, classifier performance can be estimated by cross-validation on the labeled data, AL requires a held-out annotated reference corpus.", "labels": [], "entities": []}, {"text": "In AL, the performance of the classifier cannot be reliably estimated using the data labeled in the process since sampling strategies for estimating performance assume independently and identically distributed examples).", "labels": [], "entities": []}, {"text": "The whole point in AL is to obtain a distribution of instances that is skewed in favor of the base learner used.", "labels": [], "entities": [{"text": "AL", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9190539717674255}]}, {"text": "1. Initialize the process by applying EnsembleGenerationMethod using base learner B on labeled training data set DL to obtain a committee of classifiers C.", "labels": [], "entities": []}, {"text": "2. Have each classifier in C predict a label for every instance in the unlabeled data set DU , obtain labeled set DU \u2032 . 3. From DU \u2032 , select the most informative n instances to learn from, obtaining DU \u2032\u2032 . 4. Ask the teacher for classifications of the instances I in DU \u2032\u2032 . 5. Move I, with supplied classifications, from DU to DL.", "labels": [], "entities": []}, {"text": "6. Re-train using EnsembleGenerationMethod and base learner B on the newly extended DL to obtain anew committee, C. 7. Repeat steps 2 through 6 until DU is empty or some stopping criterion is met.", "labels": [], "entities": []}, {"text": "8. Output classifier learned using EnsembleGenerationMethod and base learner B on DL.", "labels": [], "entities": []}, {"text": "In practice, however, an annotated reference corpus is rarely available and its creation would be inconsistent with the goal of creating a classifier with as little human effort as possible.", "labels": [], "entities": []}, {"text": "Thus, other ways of deciding when to stop AL are needed.", "labels": [], "entities": []}, {"text": "In this paper, we propose an intrinsic stopping-criterion for committee-based AL of named entity recognizers.", "labels": [], "entities": [{"text": "committee-based AL of named entity recognizers", "start_pos": 62, "end_pos": 108, "type": "TASK", "confidence": 0.675894096493721}]}, {"text": "It is intrinsic in that it relies on the characteristics of the data and the base learner 1 rather than on external parameters, i.e., the stopping criterion does not require any pre-defined thresholds.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 sketches interpretations of ideal stopping points and describes the idea behind our stopping criterion.", "labels": [], "entities": []}, {"text": "Section 3 outlines related work.", "labels": [], "entities": []}, {"text": "Section 4 describes the experiments we have conducted concerning a named entity recognition scenario, while Section 5 presents the results which are then discussed in Section 6.", "labels": [], "entities": [{"text": "named entity recognition scenario", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.7391179203987122}]}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To challenge the definition of the ISC, we conducted two types of experiments concerning named entity recognition.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.6612403293450674}]}, {"text": "The primary focus of the first type of experiment is on creating classifiers (classifiercentric), while the second type is concerned with the creation of annotated documents (data-centric).", "labels": [], "entities": []}, {"text": "In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure: where k is the number of members in the committee, and V (l, e) is the number of members assigning label l to instance e.", "labels": [], "entities": [{"text": "Vote Entropy measure", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.6006473700205485}]}, {"text": "If an instance obtains a low Vote Entropy value, it means that the committee members are in high agreement concerning its classification, and thus also that it is less a informative one.", "labels": [], "entities": []}, {"text": "In common AL scenarios, the main goal of using AL is to create a good classifier with minimal label complexity.", "labels": [], "entities": []}, {"text": "To follow this idea, we select sentences that are assumed to be useful for classifier training.", "labels": [], "entities": [{"text": "classifier training", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8917880654335022}]}, {"text": "We decided to select complete sentences -instead of, e.g., single tokens -as in practice annotators must seethe context of words to decide on their entity labels.", "labels": [], "entities": []}, {"text": "Our experimental setting is based on the AL approach described by: The committee consists of k = 3 Maximum Entropy (ME) classifiers.", "labels": [], "entities": [{"text": "Maximum Entropy (ME)", "start_pos": 99, "end_pos": 119, "type": "METRIC", "confidence": 0.7545370042324067}]}, {"text": "In each AL iteration, each classifier is trained on a randomly drawn (sampling without replacement) subset L \u2032 \u2282 L with |L \u2032 | = 2 3 L, L being the set of all instances labeled so far (cf. EnsembleGenerationMethod in.", "labels": [], "entities": []}, {"text": "Usefulness of a sentence is estimated as the average token Vote Entropy (cf. Equation 1).", "labels": [], "entities": [{"text": "Vote Entropy", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.8490182757377625}]}, {"text": "In each AL iteration, the 20 most useful sentences are selected (n = 20 in Step 3 in).", "labels": [], "entities": []}, {"text": "AL is started from a randomly chosen seed of 20 sentences.", "labels": [], "entities": []}, {"text": "While we made use of ME classifiers during the selection, we employed an NE tagger based on Conditional Random Fields (CRF) () during evaluation time to determine the learning curves.", "labels": [], "entities": []}, {"text": "CRFs have a significantly higher tagging performance, so the final classifier we are aiming at should be a CRF model.", "labels": [], "entities": []}, {"text": "We have shown before () that MEs are well apt as selectors with the advantage of much shorter training times than CRFs.", "labels": [], "entities": [{"text": "MEs", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9143872857093811}]}, {"text": "For both MEs and CRFs the same features were employed which comprised orthographical (based mainly on regular expressions), lexical and morphological (suffixed/prefixed, word itself), syntactic (POS tags), as well as contextual (features of neighboring tokens) ones.", "labels": [], "entities": []}, {"text": "The experiments on classifier-centric AL have been performed on the English data set of corpus used in the CoNLL-2003 shared task.", "labels": [], "entities": [{"text": "classifier-centric AL", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.6633725166320801}, {"text": "English data set of corpus", "start_pos": 68, "end_pos": 94, "type": "DATASET", "confidence": 0.8574074506759644}, {"text": "CoNLL-2003 shared task", "start_pos": 107, "end_pos": 129, "type": "DATASET", "confidence": 0.7291823029518127}]}, {"text": "This corpus consists of newspaper articles annotated with respect to person, location, and organisation entities.", "labels": [], "entities": []}, {"text": "As AL pool we took the training set which consists of about 14,000 sentences (\u2248 200, 000 tokens).", "labels": [], "entities": []}, {"text": "As validation set and as gold standard for plotting the learning curve we used CoNLL's evaluation corpus which sums up to 3,453 sentences.", "labels": [], "entities": [{"text": "CoNLL's evaluation corpus", "start_pos": 79, "end_pos": 104, "type": "DATASET", "confidence": 0.9149468839168549}]}, {"text": "While AL is commonly used to create as good classifiers as possible, with the amount of human effort kept to a minimum, it may result in fragmented and possibly non re-usable annotations (e.g., a collection of documents in which only some of the names are marked up).", "labels": [], "entities": []}, {"text": "This experiment concerns a method of orchestrating AL in away beneficial for the bootstrapping of annotated data).", "labels": [], "entities": [{"text": "orchestrating AL", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7334462404251099}]}, {"text": "The bootstrapping proper is realized by means of AL for selecting documents to annotate, as opposed to sentences.", "labels": [], "entities": [{"text": "AL", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9925406575202942}]}, {"text": "This way the annotated data set is comprised of entire documents thus promoting data creation.", "labels": [], "entities": [{"text": "data creation", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.7261888533830643}]}, {"text": "As in the classifier-centric setting, the task is to recognize names -persons, organizations, locations, times, dates, monetary expressions, and percentages -in news wire texts.", "labels": [], "entities": []}, {"text": "The texts used are part of the MUC-7 corpus) and consists of 100 documents, 3,480 sentences, and 90,790 tokens.", "labels": [], "entities": [{"text": "MUC-7 corpus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9773524701595306}]}, {"text": "The task is approached using the IOB tagging scheme proposed by, e.g.,, turning the original 7-class task into a 15-class task.", "labels": [], "entities": [{"text": "IOB tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.684104323387146}]}, {"text": "Each token is represented using a fairly standard menagerie of features, including such stemming from the surface appearance of the token (e.g., Contains dollar?", "labels": [], "entities": []}, {"text": "Length in characters), calculated based on linguistic pre-processing made with the English Functional Dependency Grammar) (e.g., Case, Part-of-speech), fetched from precompiled lists of information (e.g., Is first name?), and features based on predictions concerning the context of the token (e.g, Class of previous token).", "labels": [], "entities": [{"text": "Length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9102529287338257}]}, {"text": "The decision committee is made up from 10 boosted decision trees using MultiBoostAB) (cf. EnsembleGenerationMethod in).", "labels": [], "entities": []}, {"text": "Each classifier is created by the REPTree decision tree learner described by.", "labels": [], "entities": []}, {"text": "The informativeness of a document is calculated by means of average token Vote Entropy (cf. Equation 1).", "labels": [], "entities": []}, {"text": "The seed set of the AL process consists of five randomly selected documents.", "labels": [], "entities": []}, {"text": "In each AL iteration, one document is selected for annotation from the corpus (n = 1 in Step 3 in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of tokens and sentences required to  reach the ISC for each partition.", "labels": [], "entities": []}]}