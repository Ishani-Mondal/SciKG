{"title": [{"text": "BagPack: A general framework to represent semantic relations", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce away to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently.", "labels": [], "entities": []}, {"text": "The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring.", "labels": [], "entities": []}], "introductionContent": [{"text": "Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts).", "labels": [], "entities": [{"text": "identification of the semantic relation between two words or concepts", "start_pos": 115, "end_pos": 184, "type": "TASK", "confidence": 0.7879768192768097}]}, {"text": "However, the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely ad-hoc solutions, tuned to specific challenges.", "labels": [], "entities": []}, {"text": "For many practical applications, this is a drawback: Given the large number of semantic relations that might be relevant to one or the other task, we need a multi-purpose approach that, given an appropriate representation and training examples instantiating an arbitrary target relation, can automatically mine new pairs characterized by the same relation.", "labels": [], "entities": []}, {"text": "Building on a recent proposal in this direction by, we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking.", "labels": [], "entities": []}, {"text": "There has been much previous work on corpus-based models to extract broad classes of related words.", "labels": [], "entities": []}, {"text": "The literature on word space models) has focused on taxonomic similarity (synonyms, antonyms, co-hyponyms.", "labels": [], "entities": []}, {"text": ") and general association (e.g., finding topically related words), exploiting the idea that taxonomically or associated words will tend to occur in similar contexts, and thus share a vector of cooccurring words.", "labels": [], "entities": []}, {"text": "The literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest;).", "labels": [], "entities": []}, {"text": "Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences -finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten).", "labels": [], "entities": []}, {"text": "Tasks of this sort differ from relation extraction in that we need to capture productive patterns: we want to find out that shabu shabu (a Japanese meat dish) is eaten whereas ink is not, even if in our corpus neither noun is attested in proximity to forms of the verb to eat.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8802973330020905}]}, {"text": "Turney is the first, to the best of our knowledge, to raise the issue of a unified approach.", "labels": [], "entities": [{"text": "Turney", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9003996253013611}]}, {"text": "In particular, he treats synonymy and association as special cases of relational similarity: in the same way in which we might be able to tell that hands and arms are in a part-of relation by comparing the contexts in which they co-occur to the contexts of known part-of pairs, we can guess that cars and automobiles are synonyms by comparing the contexts in which they co-occur to the contexts linking known synonym pairs.", "labels": [], "entities": []}, {"text": "Here, we build on Turney's work, adding two main methodological innovations that allow us further generalization.", "labels": [], "entities": []}, {"text": "First, merging classic approaches to taxonomic and relational similarity, we represent concept pairs by a vector that concatenates information about the contexts in which the two words occur independently, and the contexts in which they co-occur ( also integrate information from the lexical patterns in which two words co-occur and similarity of the contexts in which each word occurs on its own, to improve performance in lexical entailment acquisition).", "labels": [], "entities": [{"text": "lexical entailment acquisition", "start_pos": 424, "end_pos": 454, "type": "TASK", "confidence": 0.698329508304596}]}, {"text": "Second, we represent contexts as bag of words and bigrams, rather than strings of words (\"patterns\") of arbitrary length: we leave it to the machine learning algorithm to zero in on the most interesting words/bigrams.", "labels": [], "entities": []}, {"text": "Thanks to the concatenated vector, we can tackle tasks in which the two words are not expected to co-occur even in very large corpora (such as selectional preference).", "labels": [], "entities": []}, {"text": "Concatenation, together with unigram/bigram representation of context, allows us to scale down the approach to smaller training corpora (Turney used a corpus of more than 50 billion words), since we do not need to seethe words directly cooccurring, and the unigram/bigram dimensions of the vectors are less sparse than dimensions based on longer strings of words.", "labels": [], "entities": []}, {"text": "We show that our method produces reasonable results also on a corpus of 2 billion words, with many unseen pairs.", "labels": [], "entities": []}, {"text": "Moreover, our bigram and unigram representation is general enough that we do not need to extract separate statistics nor perform adhoc feature selection for each task: we build the cooccurrence matrix once, and use the same matrix in all experiments.", "labels": [], "entities": []}, {"text": "The bag-of-words assumption also makes for faster and more compact model building, since the number of features we extract from a context is linear in the number of words in the context, whereas it is exponential for Turney.", "labels": [], "entities": [{"text": "model building", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7901304066181183}, {"text": "Turney", "start_pos": 217, "end_pos": 223, "type": "DATASET", "confidence": 0.9072362780570984}]}, {"text": "On the other hand, our method is currently lagging behind Turney's in terms of performance, suggesting that at least some task-specific tuning will be necessary.", "labels": [], "entities": []}, {"text": "Following Turney, we focus on devising a suitably general featural representation, and we seethe specific machine learning algorithm employed to perform the various tasks as a parameter.", "labels": [], "entities": [{"text": "Turney", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.8579193949699402}]}, {"text": "Here, we use Support Vector Machines since they area particularly effective general-purpose method.", "labels": [], "entities": []}, {"text": "In terms of empirical evaluation of the model, besides experimenting with the \"classic\" SAT and TOEFL datasets, we show how our algorithm can tackle the selectional preference task proposed in Pad\u00f3 (2007) -a regression task -and we introduce to the corpus-based semantics community a challenge from the ConceptNet repository of commonsense knowledge (extending such repository by automated means is the original motivation of our project).", "labels": [], "entities": [{"text": "TOEFL datasets", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.8632591068744659}]}, {"text": "In the next section, we will present our proposed method along with the corpora and model parameter choices used in the implementation.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the tasks that we use to evaluate the model.", "labels": [], "entities": []}, {"text": "Results are reported in Section 4 and we conclude in Section 5, with a brief overview of the contributions of this paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: ConceptNet relations after filtering.", "labels": [], "entities": []}, {"text": " Table 2: Percentage of correctly answered questions in  SAT analogy task, worst-case scenario.", "labels": [], "entities": [{"text": "SAT analogy task", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.756146252155304}]}, {"text": " Table 4: Spearman correlations between the targets and  estimations for selectional preference task.", "labels": [], "entities": [{"text": "Spearman correlations", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.6804095804691315}]}, {"text": " Table 5: AUC scores for 5 relations of ConceptNet,  classifier trained for v 1 v 2 v 1,2 condition.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.925291121006012}]}]}