{"title": [{"text": "Improving generative statistical parsing with semi-supervised word clustering", "labels": [], "entities": [{"text": "Improving generative statistical parsing", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9253159761428833}, {"text": "word clustering", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7377610504627228}]}], "abstractContent": [{"text": "We present a semi-supervised method to improve statistical parsing performance.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7573042809963226}]}, {"text": "We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.7336008846759796}]}, {"text": "We use a combination of lexicon-aided morphological clustering that preserves tagging ambiguity, and unsuper-vised word clustering, trained on a large unannotated corpus.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.7246973514556885}]}, {"text": "We apply these clus-terings to the French Treebank, and we train a parser with the PCFG-LA unlex-icalized algorithm of (Petrov et al., 2006).", "labels": [], "entities": [{"text": "French Treebank", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9911680519580841}]}, {"text": "We find again in French parsing performance: from a baseline of F 1 =86.76% to F 1 =87.37% using morphological clustering , and up to F 1 =88.29% using further unsupervised clustering.", "labels": [], "entities": [{"text": "French parsing", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.5039698928594589}, {"text": "F 1", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9512498378753662}, {"text": "F 1", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.953990250825882}, {"text": "F 1", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9458444118499756}]}, {"text": "This is the best known score for French probabilistic parsing.", "labels": [], "entities": [{"text": "French probabilistic parsing", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.4998333156108856}]}, {"text": "These preliminary results are encouraging for statistically parsing morphologically rich languages, and languages with small amount of annotated data.", "labels": [], "entities": [{"text": "statistically parsing morphologically", "start_pos": 46, "end_pos": 83, "type": "TASK", "confidence": 0.654281477133433}]}], "introductionContent": [{"text": "Lexical information is known crucial in natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6554420391718546}]}, {"text": "For probabilistic parsing, one main drawback of the plain PCFG approach is to lack sensitivity to the lexicon.", "labels": [], "entities": [{"text": "probabilistic parsing", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6278910934925079}]}, {"text": "The symbols accessible to context-free rules are part-of-speech tags, which encode generalizations that are too coarse for many parsing decisions (for instance subcategorization information is generally absent from tagsets).", "labels": [], "entities": []}, {"text": "The lexicalized models first proposed by Collins reintroduced words at every depth of a parse tree, insuring that attachments receive probabilities that take lexical information into account.", "labels": [], "entities": []}, {"text": "On the other hand, () have proposed probabilistic CFG learning with latent annotation (hereafter PCFG-LA), as away to automate symbol splitting in unlexicalized probabilistic parsing (cf. adding latent annotations to a symbol is comparable to splitting this symbol).", "labels": [], "entities": [{"text": "CFG learning", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.7596217095851898}, {"text": "symbol splitting", "start_pos": 127, "end_pos": 143, "type": "TASK", "confidence": 0.7422442734241486}]}, {"text": "() rendered the method usable in practice, with a tractable technique to retain only the beneficial splits.", "labels": [], "entities": []}, {"text": "We know that both lexicalized parsing algorithm and PCFG-LA algorithm suffer from lexical data sparseness.", "labels": [], "entities": []}, {"text": "For lexicalized parsers, shows that bilexical dependencies parameters are almost useless in the probabilistic scoring of parser because they are too scarce.", "labels": [], "entities": []}, {"text": "For PCFG-LA, we have previously studied the lexicon impact on this so-called \"unlexicalized\" algorithm, for French parsing,.", "labels": [], "entities": [{"text": "PCFG-LA", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8828380703926086}, {"text": "French parsing", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.5584034621715546}]}, {"text": "We have tested a totally unlexicalized parser, trained on a treebank where words are replaced by their POS tags.", "labels": [], "entities": []}, {"text": "It obtains a parseval F 1 =86.28 (note that it induces perfect tagging).", "labels": [], "entities": [{"text": "F 1", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.8970192670822144}]}, {"text": "We compared it to a parser trained with word+tag as terminal symbols (to simulate a perfect tagging), achieving F 1 =87.79.", "labels": [], "entities": [{"text": "F 1", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9926464557647705}]}, {"text": "This proves that lexical information is indeed used by the \"unlexicalized\" PCFG-LA algorithm: some lexical information percolates through parse trees via the latent annotations.", "labels": [], "entities": []}, {"text": "We have also reported a slight improvement (F 1 =88.18) when word forms are clustered on a morphological basis, into lemma+tag clusters.", "labels": [], "entities": [{"text": "F 1", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9904696643352509}]}, {"text": "So PCFG-LA uses lexical information, but it is too sparse, hence it benefits from word clustering.", "labels": [], "entities": [{"text": "PCFG-LA", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.8666127920150757}, {"text": "word clustering", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7485575079917908}]}, {"text": "Yet the use of lemma+tag terminals supposes tagging prior to parsing.", "labels": [], "entities": []}, {"text": "We propose hereto apply rather a deterministic supervised morphological clustering that preserves tagging ambiguities, leaving it to the parser to disambiguate POS tags.", "labels": [], "entities": []}, {"text": "We also investigate the use of unsupervised word clustering, obtained from unannotated text.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7286211103200912}]}, {"text": "It has been proved useful for parsing by and their work directly inspired ours.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9910733103752136}]}, {"text": "They have shown that parsing improves when cluster information is used as features in a discriminative training method that learns dependency parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9820477962493896}]}, {"text": "We investigate in this paper the use of such clusters in a generative approach to probabilistic phrase-structure parsing, simply by replacing each token by its cluster.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.6638486385345459}]}, {"text": "We present in section 2 the treebank instantiation we use for our experiments, the morphological clustering in section 3, and the Brown algorithm for unsupervised clustering in section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents our experiments, results and discussion.", "labels": [], "entities": []}, {"text": "Section 6 discusses related work.", "labels": [], "entities": []}, {"text": "Section 7 concludes with some ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the Brown clustering algorithm, we used Percy Liang's code 3 , run on the L'Est R\u00e9publicain corpus, a 125 million word journalistic corpus, freely available at CNRTL 4 . The corpus was tokenised 5 , segmented into sentences and desinflected using the process described in section 3.", "labels": [], "entities": [{"text": "L'Est R\u00e9publicain corpus, a 125 million word journalistic corpus", "start_pos": 78, "end_pos": 142, "type": "DATASET", "confidence": 0.6631094664335251}, {"text": "CNRTL 4", "start_pos": 164, "end_pos": 171, "type": "DATASET", "confidence": 0.9226285815238953}]}, {"text": "We ran the clustering into 1000 clusters for the desinflected forms appearing at least 20 times.", "labels": [], "entities": []}, {"text": "We tested the use of word clusters for parsing with the Berkeley algorithm ().", "labels": [], "entities": []}, {"text": "Clustering words in this case has a double advantage.", "labels": [], "entities": []}, {"text": "First, it augments the known vocabulary, which is made of all the forms of all the clusters appearing in the treebank.", "labels": [], "entities": []}, {"text": "Second, it reduces sparseness for the latent annotations learning on the lexical rules of the PCFG-LA grammar.", "labels": [], "entities": [{"text": "PCFG-LA grammar", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9274864792823792}]}, {"text": "We used Petrov's code, adapted to French by, for the suffixes used to classify unknown words, and we used the same training(80%)/dev(10%)/test(10%) partition.", "labels": [], "entities": []}, {"text": "We used the FTB-UC treebank to train a baseline parser, and three other parsers by changing the terminal symbols used in training data: desinflected forms: as described in section 3 clusters + cap: each desinflected form is replaced by its cluster bit string.", "labels": [], "entities": [{"text": "FTB-UC treebank", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.9940026700496674}]}, {"text": "If the desinflected form has no corresponding cluster (it did not appear 20 times in the unannotated corpus), a special cluster UNKC is used.", "labels": [], "entities": [{"text": "UNKC", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9091264605522156}]}, {"text": "Further, a _C suffix is added if the form starts with a capital.", "labels": [], "entities": []}, {"text": "clusters + cap + suffixes: same as before, except that 9 additional features are used as suffixes to the cluster: if form is all digits, ends with ant, or r, or ez (cf. this is how end desinflected forms of unambiguous finite verbs), ...", "labels": [], "entities": []}, {"text": "We give in table 1 parsing performance in terms of labeled precision/recall/Fscore, and also the more neutral unlabeled attachment score (UAS) . The desinflection process does help: benefits from reducing data sparseness exceed the loss of agreement markers.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.6740078330039978}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.6946220993995667}, {"text": "Fscore", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.7995355725288391}, {"text": "unlabeled attachment score (UAS)", "start_pos": 110, "end_pos": 142, "type": "METRIC", "confidence": 0.846495529015859}]}, {"text": "Yet tagging decreases a little, and this directly impacts the dependency score, because the dependency extraction uses head propagation rules that are sensitive to tagging.", "labels": [], "entities": [{"text": "dependency extraction", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7608013153076172}]}, {"text": "In the same way, the use of bare clusters increases labeled recall/precision, but the tagging accuracy decreases, and thus the UAS.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9789696931838989}, {"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.8688900470733643}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9563286304473877}, {"text": "UAS", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.6835507154464722}]}, {"text": "This can be due to the coarseness of the clustering method, which sometimes groups words that have different POS (for instance among a cluster of infinite verbs, one may find a present participle).", "labels": [], "entities": [{"text": "POS", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9523991346359253}]}, {"text": "The quality of the clusters is more crucial in our case than when clusters are features, whose informativity is discriminatively learnt.", "labels": [], "entities": []}, {"text": "This observation led us to append a restricted set of suffixes to the clusters, which gives us the best results for now.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing performance when training and parsing use clustered terminal symbols", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9416835904121399}]}]}