{"title": [{"text": "Evaluation of automatic summaries: Metrics under varying data conditions", "labels": [], "entities": []}], "abstractContent": [{"text": "In evaluation of automatic summaries, it is necessary to employ multiple topics and human-produced models in order for the assessment to be stable and reliable.", "labels": [], "entities": [{"text": "evaluation of automatic summaries", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.5842121541500092}]}, {"text": "However , providing multiple topics and models is costly and time-consuming.", "labels": [], "entities": []}, {"text": "This paper examines the relation between the number of available models and topics and the correlations with human judgment obtained by automatic metrics ROUGE and BE, as well as the manual Pyramid method.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 154, "end_pos": 159, "type": "METRIC", "confidence": 0.9691210389137268}, {"text": "BE", "start_pos": 164, "end_pos": 166, "type": "METRIC", "confidence": 0.994506299495697}]}, {"text": "Testing all these methods on the same data set, taken from the TAC 2008 Summarization track, allows us to compare and contrast the methods under different conditions.", "labels": [], "entities": [{"text": "TAC 2008 Summarization track", "start_pos": 63, "end_pos": 91, "type": "DATASET", "confidence": 0.934776708483696}]}], "introductionContent": [{"text": "Appropriate evaluation of results is an important aspect of any research.", "labels": [], "entities": []}, {"text": "In areas such as automatic summarization, the problem is especially complex because of the inherent subjectivity in the task itself and its evaluation.", "labels": [], "entities": [{"text": "automatic summarization", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.5846303105354309}]}, {"text": "There is no single objective standard fora good quality summary; rather, its value depends on the summary's purpose, focus, and particular requirements of the reader.", "labels": [], "entities": []}, {"text": "While the purpose and focus can beset as constant fora specific task, the variability of human judgment is more difficult to control.", "labels": [], "entities": []}, {"text": "Therefore, in attempts to produce stable evaluations, it has become standard to use multiple judges, not necessarily for parallel evaluation, but in such away that each judge evaluates a different subset of the many summaries on which the final system assessment is based.", "labels": [], "entities": []}, {"text": "The incorporation of multiple points of view is also reflected in automatic evaluation, where it takes the form of employing multiple model summaries to which a candidate summary is compared.", "labels": [], "entities": []}, {"text": "Since these measures to neutralize judgment variation involve the production of multiple model summaries, as well as multiple topics, evaluation can become quite costly.", "labels": [], "entities": [{"text": "neutralize judgment variation", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.756996770699819}]}, {"text": "Therefore, it is interesting to examine how many models and topics are necessary to obtain a relatively stable evaluation, and whether this number is different for manual and automatic metrics.", "labels": [], "entities": []}, {"text": "In their examination of summary evaluations, van suggest that it is necessary to use at least 30 to 40 model summaries fora stable evaluation; however, argue that a stable evaluation can be conducted even with a single model, as long as there is an adequate number of topics.", "labels": [], "entities": []}, {"text": "This view is supported by, who concludes that \"correlations to human judgments were increased by using multiple references but using single reference summary with enough number of samples was a valid alternative\".", "labels": [], "entities": []}, {"text": "Interestingly, similar conclusions were also reached in the area of Machine Translation evaluation; in their experiments, show that adding an additional reference translation compensates the effects of removing 10-15% of the testing data, and state that, therefore, \"it seems more cost effective to have more test sentences but fewer reference translations\".", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.9206076065699259}]}, {"text": "In this paper, we look at how various metrics behave with respect to a variable number of topics and models used in the evaluation.", "labels": [], "entities": []}, {"text": "This lets us determine the stability of individual metrics, and helps to illuminate the trade-offs inherent in designing a good evaluation.", "labels": [], "entities": []}, {"text": "For our experiments, we used data from the Summarization track at the Text Analysis Conference (TAC) 2008, where participating systems were assessed on their summarization of 48 topics, and the automatic metrics ROUGE and BE, as well as the manual Pyramid evaluation method, had access to 4 human models.", "labels": [], "entities": [{"text": "Summarization track at the Text Analysis Conference (TAC) 2008", "start_pos": 43, "end_pos": 105, "type": "TASK", "confidence": 0.7282086583701047}, {"text": "ROUGE", "start_pos": 212, "end_pos": 217, "type": "METRIC", "confidence": 0.9522554874420166}, {"text": "BE", "start_pos": 222, "end_pos": 224, "type": "METRIC", "confidence": 0.9893844723701477}]}, {"text": "TAC 2008 was the first task of the TAC/DUC (Document Understanding Conference) series in which the Pyramid method was used on all evaluated data, making it possible to conduct a full com-parison among the manual and automatic methods.", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8834732174873352}, {"text": "TAC/DUC (Document Understanding Conference) series", "start_pos": 35, "end_pos": 85, "type": "DATASET", "confidence": 0.6464910904566447}]}, {"text": "Despite the lack of full Pyramid evaluation in DUC 2007, we look at the remaining metrics applied that year (ROUGE, BE, and Content Responsiveness), in order to see whether they confirm the insights gained from the TAC 2008 data.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8940936326980591}, {"text": "ROUGE", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.9982790946960449}, {"text": "BE", "start_pos": 116, "end_pos": 118, "type": "METRIC", "confidence": 0.9979739785194397}, {"text": "TAC 2008 data", "start_pos": 215, "end_pos": 228, "type": "DATASET", "confidence": 0.9220076600710551}]}], "datasetContent": [{"text": "The main evaluation at TAC 2008 was performed manually, assessing the automatic candidate summaries with respect to Overall Responsiveness, Overall Readability, and content coverage according to the Pyramid framework).", "labels": [], "entities": [{"text": "TAC 2008", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8380499482154846}, {"text": "Pyramid framework", "start_pos": 199, "end_pos": 216, "type": "DATASET", "confidence": 0.8553716540336609}]}, {"text": "Task participants were asked to produce two summaries for each of the 48 topics; the first (initial summary) was a straightforward summary of 10 documents in response to a topic statement, which is a request for information about a subject or event; the second was an update summary, generated on the basis of another set of 10 documents, which followed the first set in temporal order and described further developments in the given topic.", "labels": [], "entities": []}, {"text": "The idea behind the update summary was to avoid repeating all the information included in the first set of documents, on the assumption that the reader is familiar with that information already.", "labels": [], "entities": []}, {"text": "The participating teams submitted up to three runs each; however, only the first and second runs were evaluated manually due to limited resources.", "labels": [], "entities": []}, {"text": "For each summary under evaluation, assessors rated the summary from 1 (very poor) to 5 (very good) in terms of Overall Responsiveness, which measures how well the summary responds to the need for information expressed in the topic statement and whether its linguistic quality is adequate.", "labels": [], "entities": []}, {"text": "Linguistic qualities such as grammaticality, coreference, and focus were also evaluated as Overall Readability, also on the scale from 1 to 5.", "labels": [], "entities": []}, {"text": "Content coverage of each summary was evaluated using the Pyramid framework, where assessors create a list of information nuggets (called Summary Content Units, or SCUs) from the set of human-produced summaries on a given topic, then decide whether any of these nuggets are present in the candidate summary.", "labels": [], "entities": []}, {"text": "All submitted runs were evaluated with the automatic metrics: ROUGE (Lin, 2004b), which calculates the proportion of n-grams shared between the candidate summary and the reference summaries, and Basic Elements (), which compares the candidate to the models in terms of head-modifier pairs.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9983788728713989}]}], "tableCaptions": [{"text": " Table 1: Mean correlations of Responsiveness and other met-", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9552439451217651}]}, {"text": " Table 2: Mean correlations of Responsiveness and other met-", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9558335244655609}, {"text": "Responsiveness", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.678996741771698}]}, {"text": " Table 3: Mean correlations of 4-model Pyramid score and", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9491702914237976}]}, {"text": " Table 4: Mean correlations of 4-model Pyramid score and", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.948729008436203}]}, {"text": " Table 5: Mean correlations of Content Responsiveness and", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9400647878646851}, {"text": "Content Responsiveness", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.6517296731472015}]}, {"text": " Table 7: Mean correlations of 48 topic Responsiveness and", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9640580713748932}]}, {"text": " Table 8: Mean correlations of 48 topic Responsiveness and", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9639948904514313}]}, {"text": " Table 9: Mean correlations of 48 topic Pyramid score and", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9453622996807098}, {"text": "topic Pyramid score", "start_pos": 34, "end_pos": 53, "type": "METRIC", "confidence": 0.9586994647979736}]}, {"text": " Table 10: Mean correlations of 48 topic Pyramid score and", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.9422842860221863}, {"text": "topic Pyramid score", "start_pos": 35, "end_pos": 54, "type": "METRIC", "confidence": 0.9541130661964417}]}, {"text": " Table 11: Mean correlations of 45 topic Content Respon-", "labels": [], "entities": [{"text": "Mean correlations", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.9457507729530334}]}]}