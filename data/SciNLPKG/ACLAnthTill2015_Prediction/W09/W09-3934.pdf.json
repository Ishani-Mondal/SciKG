{"title": [{"text": "Extracting decisions from multi-party dialogue using directed graphical models and semantic similarity", "labels": [], "entities": []}], "abstractContent": [{"text": "We use directed graphical models (DGMs) to automatically detect decision discussions in multi-party dialogue.", "labels": [], "entities": []}, {"text": "Our approach distinguishes between different dialogue act (DA) types based on their role in the formulation of a decision.", "labels": [], "entities": []}, {"text": "DGMs enable us to model dependencies, including sequential ones.", "labels": [], "entities": []}, {"text": "We summarize decisions by extracting suitable phrases from DAs that concern the issue under discussion and its resolution.", "labels": [], "entities": []}, {"text": "Here we use a semantic-similarity metric to improve results on both manual and ASR transcripts.", "labels": [], "entities": []}], "introductionContent": [{"text": "In work environments, people share information and make decisions in multi-party conversations known as meetings.", "labels": [], "entities": []}, {"text": "The demand for systems that can automatically process, understand and summarize information contained in audio and video recordings of meetings is growing rapidly.", "labels": [], "entities": [{"text": "summarize information contained in audio and video recordings of meetings", "start_pos": 70, "end_pos": 143, "type": "TASK", "confidence": 0.8401593744754792}]}, {"text": "Our own research, and that of other contemporary projects (), aim at meeting this demand.", "labels": [], "entities": []}, {"text": "At present, we are focusing on the automatic detection and summarization of decision discussions.", "labels": [], "entities": [{"text": "automatic detection", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.660410538315773}, {"text": "summarization of decision discussions", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.8449305146932602}]}, {"text": "Our approach for detecting decision discussions involves distinguishing between different dialogue act (DA) types based on their role in the decision-making process.", "labels": [], "entities": [{"text": "detecting decision discussions", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.8517704606056213}]}, {"text": "Two of these types are DAs which describe the Issue under discussion, and DAs which describe its Resolution.", "labels": [], "entities": []}, {"text": "To summarize a decision discussion, we identify words and phrases in the Issue and Resolution DAs, which can be used to produce a concise, descriptive summary.", "labels": [], "entities": [{"text": "Issue and Resolution DAs", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.5645351186394691}]}, {"text": "This paper describes new experiments in both detecting and summarizing decision discussions.", "labels": [], "entities": [{"text": "detecting and summarizing decision discussions", "start_pos": 45, "end_pos": 91, "type": "TASK", "confidence": 0.7980338096618652}]}, {"text": "In the detection stage, we investigate the use of Directed Graphical Models (DGMs).", "labels": [], "entities": []}, {"text": "DGMs are attractive because they can be used to model sequence and dependencies between predictor variables.", "labels": [], "entities": []}, {"text": "In the summarization stage, we attempt to improve phrase selection with anew feature that measures the level of semantic similarity between candidate Issue phrases and Resolution utterances, and vice-versa.", "labels": [], "entities": [{"text": "summarization", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.9633973240852356}, {"text": "phrase selection", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.8248254656791687}]}, {"text": "The feature is generated by a semantic-similarity metric which uses WordNet as a knowledge source.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9529053568840027}]}, {"text": "The motivation is that ordinarily, the Issue and Resolution components in a decision summary should be semantically similar.", "labels": [], "entities": [{"text": "Issue and Resolution", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.652650902668635}]}, {"text": "The paper proceeds as follows.", "labels": [], "entities": []}, {"text": "Firstly, Section 2 describes related work, and Section 3, our data-set and annotation scheme for decision discussions.", "labels": [], "entities": [{"text": "decision discussions", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.8859989643096924}]}, {"text": "Section 4 then reports our decision detection experiments using DGMs, and Section 5, the summarization experiments.", "labels": [], "entities": [{"text": "decision detection", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8956255614757538}, {"text": "summarization", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.9067234992980957}]}, {"text": "Finally, Section 6 draws conclusions and proposes ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: For the manual transcripts in our subcorpus, the average length in words of I and R utterances is 12.2 and 11.9 respectively, and for the ASR, 22.4 and 18.1.", "labels": [], "entities": [{"text": "ASR", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.6494728922843933}]}, {"text": "To provide a gold-standard, phrases from I and R utterances in the manual transcriptions were annotated as summaryworthy.", "labels": [], "entities": []}, {"text": "The aim was to select those phrases which should appear in an extractive summary, or could be the basis of a generated abstractive summary.", "labels": [], "entities": []}, {"text": "As a general guideline, we tried to select the phrase(s) which describe the issue/resolution as succinctly as possible.", "labels": [], "entities": [{"text": "issue/resolution", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7652533650398254}]}, {"text": "This does not include phrases which express the speaker's attitude towards the issue/resolution.", "labels": [], "entities": [{"text": "issue/resolution", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.8070125579833984}]}, {"text": "Dialogue 2 is an example where square brackets indicate which phrases were selected as summary-worthy.", "labels": [], "entities": []}, {"text": "Regression models: We use SVMlight (Joachims, 1999) to learn separate SVM regression models for Issues and Resolutions.", "labels": [], "entities": [{"text": "Issues and Resolutions", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7075705726941427}]}, {"text": "These rank the Gemini parses for each utterance according to their likelihood of matching the gold-standard summary.", "labels": [], "entities": []}, {"text": "The top-ranked parse is then entered into the automatically-generated decision summary.", "labels": [], "entities": []}, {"text": "Features: We train the regression models with various types of feature (see), including properties of the WCN paths, parse, semantic and lexical features.", "labels": [], "entities": []}, {"text": "As lexical features are likely to be more domain-specific, and they dramatically increase size of the feature space, we prefer to avoid them if possible.", "labels": [], "entities": []}, {"text": "To generate the semantic-similarity feature for an I/R parse, we compute its semantic similarity with the full transcripts of each of the R/I utterances within the same decision discussion.", "labels": [], "entities": []}, {"text": "The feature's value is then equal to the greatest of the resulting semantic-similarity scores.", "labels": [], "entities": []}, {"text": "Since Ted Pedersen's package operates on the noun portion of WordNet, we must first extract all of the nouns in the parse/utterance transcription.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9442211389541626}]}, {"text": "Next, we form all of the possible pairs containing one noun from the parse, and one from the utterance transcription.", "labels": [], "entities": []}, {"text": "Then we compute the semantic similarity for each pair, and take their sum to be the level of semantic similarity between the parse and the utterance transcription.", "labels": [], "entities": []}, {"text": "We experimented with averaging rather than summing these scores, but the resulting semantic-similarity feature was less predictive.", "labels": [], "entities": []}, {"text": "Evaluation: The models are evaluated in 10-fold cross-validations using the same metric as): Recall is the total proportion of the gold-standard extractive summary   covered by the selected parse; precision is the total proportion of the chosen parse which overlaps with the gold-standard summary.", "labels": [], "entities": [{"text": "Recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9967932105064392}, {"text": "precision", "start_pos": 197, "end_pos": 206, "type": "METRIC", "confidence": 0.9994888305664062}]}, {"text": "The baseline is the entire transcription, and we also compare to an \"oracle\" that always chooses a parse with the highest F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9952607750892639}]}, {"text": "Note that we use the extractive summaries from the manual transcriptions as the gold-standard for the evaluation of the results obtained with ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.7430375814437866}]}, {"text": "Results and analysis: Results with manual transcriptions are shown in, and those with ASR, in.", "labels": [], "entities": [{"text": "ASR", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9215896725654602}]}, {"text": "In all cases, when starting with a feature set containing WCN, parse and semantic features, the F1-score is improved by adding the semantic-similarity feature.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9991384744644165}]}, {"text": "For Issues, the F1-score improves from .66 to .68 with manual transcripts, and from .30 to .32 with ASR.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9994403719902039}, {"text": "ASR", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.6557851433753967}]}, {"text": "The improvements for Resolutions are highly significant: with manual transcripts, the F1 score increases from .64 to .67 (p < 0.005), and with ASR, from .33 to .37 (p < 0.005).", "labels": [], "entities": [{"text": "Resolutions", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9794154167175293}, {"text": "F1 score", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9890467822551727}, {"text": "ASR", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9686380624771118}]}, {"text": "Note that the further addition of lexical features only produces a significant improvement in the case of I summarization with ASR.", "labels": [], "entities": [{"text": "I summarization", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.9218849539756775}]}, {"text": "Compared to the full transcript baseline, we achieve higher F1-scores for Issues-.68 vs. .67 with manual transcriptions, and .35 vs. .31 with ASR-but slightly lower for Resolutions.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.999459445476532}, {"text": "Resolutions", "start_pos": 169, "end_pos": 180, "type": "TASK", "confidence": 0.9806292057037354}]}, {"text": "There remains a fairly large gap between our best scores and their corresponding oracles (especially with ASR), and so there may still be potential for substantial improvement.", "labels": [], "entities": [{"text": "ASR", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.6995632648468018}]}], "tableCaptions": [{"text": " Table 1: F1-score (per utterance) of the DGMs us- ing the best combination of non-lexical features.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995452761650085}]}, {"text": " Table 2: F1-score (40 seconds) of the DGMs using  the best combination of non-lexical features.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996334314346313}]}, {"text": " Table 3: Performance of the DGM classifier vs.  the SVM classifier. Both use the best combination  of non-lexical features.", "labels": [], "entities": []}, {"text": " Table 4: F1-scores (40 seconds) computed using  ASR one-best vs. manual transcriptions.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9992407560348511}, {"text": "ASR one-best", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.5446556955575943}]}, {"text": " Table 6: Features for parse fragment ranking", "labels": [], "entities": [{"text": "parse fragment ranking", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9175657431284586}]}, {"text": " Table 7: Parse ranking results for I & R Utterances  using manual transcriptions.", "labels": [], "entities": [{"text": "Parse", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8556475043296814}, {"text": "I & R Utterances", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7288297265768051}]}, {"text": " Table 8: Parse ranking results for I & R Utterances  using ASR.", "labels": [], "entities": [{"text": "Parse", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.7575633525848389}, {"text": "I & R Utterances", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7492245435714722}]}]}