{"title": [{"text": "On NoMatchs, NoInputs and BargeIns: Do Non-Acoustic Features Support Anger Detection?", "labels": [], "entities": [{"text": "Anger Detection", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7679322361946106}]}], "abstractContent": [{"text": "Most studies on speech-based emotion recognition are based on prosodic and acoustic features, only employing artificial acted corpora where the results cannot be generalized to telephone-based speech applications.", "labels": [], "entities": [{"text": "speech-based emotion recognition", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.6663339932759603}]}, {"text": "In contrast, we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application, taking advantage of additional dialogue features, NLU features and ASR features that are incorporated into the emotion recognition process.", "labels": [], "entities": [{"text": "emotion recognition process", "start_pos": 236, "end_pos": 263, "type": "TASK", "confidence": 0.8057044347127279}]}, {"text": "Depending on the task, non-acoustic features add 2.3% in classification accuracy compared to using only acoustic features.", "labels": [], "entities": [{"text": "classification", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.9218971133232117}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.956641435623169}]}], "introductionContent": [{"text": "Certainly, the most relevant employment of speech-based emotion recognition is that of a telephone-based Interactive Voice Response System (IVR).", "labels": [], "entities": [{"text": "speech-based emotion recognition", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6378551721572876}]}, {"text": "Emotion recognition for IVR differs insofar to \"traditional\" emotion recognition, that it can be reduced to a binary classification problem, namely the distinction between angry and nonangry whereas studies on speech-based emotion recognition analyze complete and relatively long sentences covering the full bandwidth of human emotions.", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8983406722545624}, {"text": "IVR", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9535494446754456}, {"text": "emotion recognition", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8700854778289795}, {"text": "speech-based emotion recognition", "start_pos": 210, "end_pos": 242, "type": "TASK", "confidence": 0.8127782344818115}]}, {"text": "Ina way, emotion recognition in the telephone domain is less challenging since a distinction between two different emotion classes, angry and non-angry, is sufficient.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8005942404270172}]}, {"text": "We don't have to expect callers talking to IVRs in a sad, anxious, happy, disgusted or bored manner.", "labels": [], "entities": []}, {"text": "I.e., even if a caller is happy, the effect on the dialogue will be the same as if he is neutral.", "labels": [], "entities": []}, {"text": "However, there still remain challenges for the system developer such as varying speech quality caused by, e.g., varying distance to the receiver during the call leading to loudness variations (which emotion recognizers might mistakenly interpret as anger).", "labels": [], "entities": []}, {"text": "But also bandwidth limitation introduced by the telephone channel and a strongly unbalanced distribution of non-angry and angry utterances with more than 80% non-angry utterances make a reliable distinction of the caller emotion difficult.", "labels": [], "entities": []}, {"text": "While hot anger with studio quality conditions can be determined with over 90% () studies on IVR anger recognition report lower accuracies due to these limitations.", "labels": [], "entities": [{"text": "IVR anger recognition", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.892659326394399}, {"text": "accuracies", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9596818685531616}]}, {"text": "However, there is one advantage of anger recognition in IVR systems that can be exploited: additional information is available from the dialogue context, the speech recognizer and the natural language parser.", "labels": [], "entities": [{"text": "anger recognition", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7314569801092148}]}, {"text": "This contribution is organized as follows: first, we introduce related work and describe our corpus.", "labels": [], "entities": []}, {"text": "In Section 4 we outline our employed features with emphasis on the non-acoustic ones.", "labels": [], "entities": []}, {"text": "Experiments are shown in Section 5 where we analyze the impact of the newly developed features before we summarize our work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to prevent an adaption of the anger model to specific callers we seperated the corpus randomly into 75% training and 25% testing material and ensured that no speaker contained in training was used for testing.", "labels": [], "entities": []}, {"text": "To exclude that we receive a good classification result by chance, we performed 50 iterations in each test and calculated the performance's mean and standard deviation overall iterations.", "labels": [], "entities": []}, {"text": "Note, that our aim in this study is less finding an optimum classifer, than finding additional features that support the distinction between angry and non-angry callers.", "labels": [], "entities": []}, {"text": "Support Vector Machines and Artificial Neural Networks are thus not considered, although the best performances are reported with those learning algorithms.", "labels": [], "entities": []}, {"text": "A similar performance, i.e. only slightly poorer, can be reached with Rule Learners.", "labels": [], "entities": [{"text": "Rule Learners", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.7379404604434967}]}, {"text": "They enable a thorough study of the features, leading to the decision for one or the other class, since they produce a human readable set of if-then-else rules.", "labels": [], "entities": []}, {"text": "Our hypotheses on a perfect feature set can thus easily be confirmed or rejected.", "labels": [], "entities": []}, {"text": "We performed experiments with two different classes: 'angry' vs. 'non-angry' and 'angry+annoyed' vs. 'non-angry'.", "labels": [], "entities": []}, {"text": "Merging angry and annoyed utterances aims on finding all callers, where the customer satisfaction is endangered.", "labels": [], "entities": []}, {"text": "In both tasks, we employ a) only acoustic features b) only ASR/NLU/DM/Context features and c) a combination of both feature sets.", "labels": [], "entities": []}, {"text": "The number of utterances used for training and testing is shown in.", "labels": [], "entities": []}, {"text": "As result we expect acoustic features to perform better than non-acoustic features.", "labels": [], "entities": []}, {"text": "Among the relevant non-acoustic features we assume as an indicator for angry utterances low ASR confidences and high barge-in rates, which we consider as signal for the caller's impatience.", "labels": [], "entities": [{"text": "ASR confidences", "start_pos": 92, "end_pos": 107, "type": "METRIC", "confidence": 0.7280067205429077}, {"text": "barge-in rates", "start_pos": 117, "end_pos": 131, "type": "METRIC", "confidence": 0.8856255412101746}]}, {"text": "All tests have been performed with the machine learning framework RapidMiner () featuring all common supervised and unsupervised learning schemes.", "labels": [], "entities": []}, {"text": "Results are listed in sion and recall values.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9974911212921143}]}, {"text": "As expected, Test B (angry vs. non-angry) has the highest accuracy with 87.23% since the patterns are more clearly separable compared to Test A (annoyed vs. nonangry, 72.57%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9993869066238403}]}, {"text": "Obviously, adding non-acoustic features increases classification accuracy significantly, but only where the acoustic features are not expressive enough.", "labels": [], "entities": [{"text": "classification", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9541351199150085}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9603756666183472}]}, {"text": "While the additional information increases the accuracy of the combined angry+annoyed task by 2.3 % (Test A), it does not advance the distinction between only angry vs. non-angry (Test B).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996298551559448}]}], "tableCaptions": []}