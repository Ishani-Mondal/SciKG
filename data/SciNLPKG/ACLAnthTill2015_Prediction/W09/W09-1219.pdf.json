{"title": [{"text": "A Simple Generative Pipeline Approach to Dependency Parsing and Se- mantic Role Labeling", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6575983166694641}, {"text": "Se- mantic Role Labeling", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.6385751664638519}]}], "abstractContent": [{"text": "We describe our CoNLL 2009 Shared Task system in the present paper.", "labels": [], "entities": [{"text": "CoNLL 2009 Shared Task system", "start_pos": 16, "end_pos": 45, "type": "DATASET", "confidence": 0.8664565086364746}]}, {"text": "The system includes three cascaded components: a genera-tive dependency parser, a classifier for syntactic dependency labels and a semantic classifier.", "labels": [], "entities": []}, {"text": "The experimental results show that the labeled macro F1 scores of our system on the joint task range from 43.50% (Chinese) to 57.95% (Czech), with an average of 51.07%.", "labels": [], "entities": [{"text": "labeled macro F1 scores", "start_pos": 39, "end_pos": 62, "type": "METRIC", "confidence": 0.5937783271074295}]}], "introductionContent": [{"text": "The CoNLL 2009 shared task is an extension of the tasks addressed in previous years: unlike the English-only 2008 task, the present year deals with seven languages; and unlike 2006 and 2007, semantic role labeling is performed atop the surface dependency parsing.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 191, "end_pos": 213, "type": "TASK", "confidence": 0.6633444925149282}, {"text": "surface dependency parsing", "start_pos": 236, "end_pos": 262, "type": "TASK", "confidence": 0.6913834015528361}]}, {"text": "We took part in the closed challenge of the joint task.", "labels": [], "entities": []}, {"text": "The input of our system contained gold standard lemma, part of speech and morphological features for each token.", "labels": [], "entities": []}, {"text": "Tokens which were considered predicates were marked in the input data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Finally, we performed some preliminary experiments focused on the syntactic parser.", "labels": [], "entities": []}, {"text": "As mentioned in Section \ud97b\udf59 2.1, many features of the parser have to be turned off unless the parser understands the part-of-speech and morphological features.", "labels": [], "entities": []}, {"text": "We used DZ Interset to convert Czech and English CoNLL POS+FEAT strings to PDTlike positional tags.", "labels": [], "entities": [{"text": "Czech and English CoNLL POS+", "start_pos": 31, "end_pos": 59, "type": "DATASET", "confidence": 0.6098822156588236}, {"text": "FEAT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.7874875068664551}]}, {"text": "Then we switched back on the parser options that use up the tags and re-ran parsing.", "labels": [], "entities": []}, {"text": "Non-empty APRED accuracy includes only APRED cells that were non-empty both in gold standard and system output.", "labels": [], "entities": [{"text": "APRED accuracy", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.5971518158912659}]}, {"text": "Feature-pair coverage includes all cells filled by the system.", "labels": [], "entities": []}, {"text": "Unlabeled precision and recall count non-empty vs. empty APREDs without respect to their actual labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9971338510513306}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9990155696868896}, {"text": "APREDs", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.6870747208595276}]}, {"text": "Counted on development data with gold-standard surface syntax.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Time and space requirements of the syntactic parser.", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9802917838096619}]}, {"text": " Table 1. The official results of the system. ISO 639-1 language codes are used (cs = Czech, en = English, de = Ger- man, es = Spanish, ca = Catalan, ja = Japanese, zh = Chinese). \"OOD\" means \"out-of-domain test data\".", "labels": [], "entities": []}, {"text": " Table 4. Most homonymous predicates.", "labels": [], "entities": []}, {"text": " Table 3. The learning curve of the principal scores.", "labels": [], "entities": []}, {"text": " Table 5. APRED detailed analysis. Non-empty APRED accuracy includes only APRED cells that were non-empty  both in gold standard and system output. Feature-pair coverage includes all cells filled by the system. Unlabeled preci- sion and recall count non-empty vs. empty APREDs without respect to their actual labels. Counted on development  data with gold-standard surface syntax.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9253188967704773}, {"text": "recall", "start_pos": 237, "end_pos": 243, "type": "METRIC", "confidence": 0.9784059524536133}]}]}