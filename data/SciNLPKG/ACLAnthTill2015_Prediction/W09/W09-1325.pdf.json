{"title": [{"text": "Semantic Annotation of Papers: Interface & Enrichment Tool (SAPIENT)", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we introduce a web application (SAPIENT) for sentence based annotation of full papers with semantic information.", "labels": [], "entities": []}, {"text": "SAPI-ENT enables experts to annotate scientific papers sentence by sentence and also to link related sentences together, thus forming spans of interesting regions, which can facilitate text mining applications.", "labels": [], "entities": [{"text": "text mining", "start_pos": 185, "end_pos": 196, "type": "TASK", "confidence": 0.734809160232544}]}, {"text": "As part of the system, we developed an XML-aware sentence split-ter (SSSplit) which preserves XML markup and identifies sentences through the addition of in-line markup.", "labels": [], "entities": []}, {"text": "SAPIENT has been used in a systematic study for the annotation of scientific papers with concepts representing the Core Information about Scientific Papers (CISP) to create a corpus of 225 annotated papers .", "labels": [], "entities": [{"text": "SAPIENT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.474918007850647}]}], "introductionContent": [{"text": "Given the rapid growth in the quantity of scientific literature, particularly in the Biosciences, there is an increasing need to work with full papers rather than abstracts, both to identify their key contributions and to provide some automated assistance to researchers.", "labels": [], "entities": []}, {"text": "Initiatives like OTMI 1 , which aim to make full papers available to researchers for text mining purposes is further evidence that relying solely on abstracts presents important limitations for such tasks.", "labels": [], "entities": [{"text": "OTMI 1", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.6779088377952576}, {"text": "text mining", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.7956148386001587}]}, {"text": "A recent study on whether information retrieval from full text is more effective than searching abstracts alone showed that 1 http://opentextmining.org/wiki/Main Page the former is indeed the case.", "labels": [], "entities": [{"text": "information retrieval from full text", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.84137464761734}]}, {"text": "Their experimental results suggested that span-level analysis is a promising strategy for taking advantage of the full papers, where spans are defined as paragraphs of text assessed by humans and deemed to be relevant to one of 36 pre-defined topics.", "labels": [], "entities": [{"text": "span-level analysis", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7481536567211151}]}, {"text": "Therefore, when working with full papers, it is important to be able to identify and annotate spans of text.", "labels": [], "entities": []}, {"text": "In previous research, sentence based annotation has been used to identify text regions with scientific content of interest to the user () or zones of different rhetorical status (AZ) ().", "labels": [], "entities": []}, {"text": "Sentences are the structural units of paragraphs and can be more flexible than paragraphs for text mining purposes other than information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 126, "end_pos": 147, "type": "TASK", "confidence": 0.7541625201702118}]}, {"text": "Current general purpose systems for linguistic annotation such as Callisto 2 allow the creation of a simple annotation schema that is a tag set augmented with simple (e.g. string) attributes for each tag.) is a plug-in of the knowledge representation tool Prot\u00e9g\u00e9 3 , which works as a general purpose text annotation tool and has the advantage that it can work with complex ontologyderived schemas.", "labels": [], "entities": []}, {"text": "However, these systems are not particularly suited to sentence by sentence annotation of full papers, as one would need to highlight entire sentences manually.", "labels": [], "entities": []}, {"text": "Also these systems work mainly with plain text, so they do not necessarily interpret the structural information already available in the paper, which can be crucial to annotation decisions for the type of high level annotation mentioned above.", "labels": [], "entities": []}, {"text": "The OSCAR3 () tool for the recognition and annotation of chemical named entities fully displays underlying paper information in XML but is not suited to sentence by sentence annotation.", "labels": [], "entities": [{"text": "recognition and annotation of chemical named entities", "start_pos": 27, "end_pos": 80, "type": "TASK", "confidence": 0.8551489625658307}]}, {"text": "To address the above issues, we present a system (SAPIENT) for sentence by sentence annotation of scientific papers which supports ontologymotivated concepts representing the core information about scientific papers (CISP) ).", "labels": [], "entities": [{"text": "SAPIENT", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.976028323173523}]}, {"text": "An important aspect of the system is that although annotation is sentence based, the system caters for identifiers, which link together sentences pertaining to the same concept.", "labels": [], "entities": []}, {"text": "This way spans of interest or key regions are formed.", "labels": [], "entities": []}, {"text": "SAPI-ENT also incorporates OSCAR3 capability for the automatic recognition of chemical named entities and runs within a browser, which makes it platform independent.", "labels": [], "entities": [{"text": "automatic recognition of chemical named entities", "start_pos": 53, "end_pos": 101, "type": "TASK", "confidence": 0.7667688628037771}]}, {"text": "SAPIENT takes as input full scientific papers in XML, splits them into individual sentences, displays them and allows the user to annotate each sentence with one of 11 CISP concepts as well as link the sentence to other sentences referring to the same instance of the concept selected.", "labels": [], "entities": []}, {"text": "The system is especially suitable for so called multidimensional annotation or ontology-motivated annotation, where a label originates from a class with properties.", "labels": [], "entities": []}, {"text": "SAPIENT is currently being employed by 16 Chemistry experts to develop a corpus of scientific papers (ART Corpus) annotated with Core Information about Scientific Papers (CISP) covering topics in Physical Chemistry and Biochemistry.", "labels": [], "entities": [{"text": "SAPIENT", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6946619153022766}]}], "datasetContent": [{"text": "SAPIENT and SSSplit have been have been employed by more than 20 different users to successfully display 270 full papers.", "labels": [], "entities": [{"text": "SAPIENT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7998269200325012}]}, {"text": "For a more accurate evaluation of the quality of the sentences produced by SSSplit, we used a Perl script which compared the sentence boundaries (start and end) generated by SSSplit, to sentence tags in a set of 41 papers (SciXML files) annotated manually by human experts.", "labels": [], "entities": []}, {"text": "If both the start and end of a sentence matched up in the generated and manual versions, we considered this a true positive result.", "labels": [], "entities": []}, {"text": "In the case where a sentence did not match in the two versions, we first searched fora matching end in our generated set of sentences and then in the hand annotated version.", "labels": [], "entities": []}, {"text": "If the 'true' end of the sentence (as defined by the manual annotation) was found in later sentences in the SSSplit version, this meant that the system had split a sentence too early, or \"oversplit\".", "labels": [], "entities": []}, {"text": "This we considered to be a false positive, since we had detected a sentence boundary wherein reality there was none.", "labels": [], "entities": []}, {"text": "This would result in the following sentence being matched at the end only, which also counts as a false positive.", "labels": [], "entities": []}, {"text": "In the case where the end of the SSSplit sentence was found in a later sentence, within the set of 'true' sentences, it meant that our sentence spanned too wide, or that the system had \"undersplit\".", "labels": [], "entities": []}, {"text": "These cases we considered to be false negatives, as we had failed to detect a sentence boundary where there was one.", "labels": [], "entities": []}, {"text": "Our training consisted of 14 papers in the fields of physical chemistry and biochemistry.", "labels": [], "entities": []}, {"text": "A different set of 41 papers distinct from the training set but from the same thematic domain was used as a test set.", "labels": [], "entities": []}, {"text": "Out of these 41 papers, 36 feature as a test set (with nfold validation) also for the sentence splitters RASP () and the XML-aware sentence splitter developed by.", "labels": [], "entities": [{"text": "sentence splitters RASP", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.800294041633606}, {"text": "XML-aware sentence splitter", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.6343182722727457}]}, {"text": "The results for all three systems, obtained as medians of Precision, Recall and F-measure for the 36 papers are shown in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9991996884346008}, {"text": "Recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.995001494884491}, {"text": "F-measure", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9983978867530823}]}, {"text": "Precision is the proportion of true positives overall end and start tags returned, giving a measure of the number of boundaries identified correctly.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9941275119781494}]}, {"text": "Recall is the proportion of true positives overall the relevant start and end tags in the hand-annotated papers, giving a measure of the number of boundaries actually found.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9908843636512756}]}, {"text": "F-Measure combines Precision and Recall to give a more balanced view on the system performance.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7270804047584534}, {"text": "Precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.996845543384552}, {"text": "Recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9964051246643066}]}, {"text": "In comparison with RASP and the XML-Aware splitter of, SSSplit performed well, though it did not outperform these systems.", "labels": [], "entities": [{"text": "RASP", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.7982884049415588}]}, {"text": "Their highest result for precision was 0.996 (vs 0.964 for SSSplit) and for recall 0.990 (vs 0.994 for SSSplit).", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9994863271713257}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9973306655883789}]}, {"text": "We can explain their higher results somewhat by their use of n-fold cross-validation on 36 out of the same 41 papers that we used, which can allow information from the test set to leak into the training data.", "labels": [], "entities": []}, {"text": "We did not perform n-fold cross-validation, as this would have involved going through each of the papers and removing any potential influence on our regular expression rules of the sentences included within, which is a non-trivial process.", "labels": [], "entities": []}, {"text": "Our test data was completely unseen, which meant that our eval-  uation is stricter, avoiding any influence from the training data.", "labels": [], "entities": [{"text": "eval-  uation", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9292825857798258}]}, {"text": "In addition to the comparison between SSSplit and the other two XML-aware sentence splitters, we also performed a comparison between our training and testing sets, depicted in.", "labels": [], "entities": [{"text": "XML-aware sentence splitters", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6818987329800924}]}, {"text": "As can be seen in, recall was only slightly better on the training set than the test set, but precision was worse on the training set, presumably because of lack of attention being paid to the oversplitting in a particular paper (\"b103844n\").", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9995325803756714}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9995837807655334}]}, {"text": "This shows that we have not overfitted to the training set in developing our splitter.", "labels": [], "entities": [{"text": "splitter", "start_pos": 77, "end_pos": 85, "type": "TASK", "confidence": 0.9538721442222595}]}, {"text": "Our recall is particularly high, indicating that our splitter makes very few false negative errors.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9996606111526489}]}, {"text": "We can attribute many of the false positive errors to our somewhat small set of abbreviations considered, resulting in oversplit sentences.", "labels": [], "entities": []}, {"text": "We would like to incorporate a more sophisticated approach to abbreviations in the future.", "labels": [], "entities": [{"text": "abbreviations", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9524837136268616}]}], "tableCaptions": [{"text": " Table 1: Comparison of sentence splitters in RASP,  Owusu and SSSplit.", "labels": [], "entities": [{"text": "sentence splitters", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7079873830080032}, {"text": "RASP", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8568066358566284}, {"text": "Owusu", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.8930831551551819}]}, {"text": " Table 2: Comparison of SSSplit on the training and test- ing papers. The training set consisted of 14 papers (1979  sentences) and the testing set of 41 papers (5002 sen- tences).", "labels": [], "entities": []}]}