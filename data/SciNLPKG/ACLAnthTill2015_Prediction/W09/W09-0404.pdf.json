{"title": [{"text": "Textual Entailment Features for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7036406546831131}, {"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.8213619291782379}]}], "abstractContent": [{"text": "We present two regression models for the prediction of pairwise preference judgments among MT hypotheses.", "labels": [], "entities": [{"text": "MT hypotheses", "start_pos": 91, "end_pos": 104, "type": "TASK", "confidence": 0.9014153778553009}]}, {"text": "Both models are based on feature sets that are motivated by textual entailment and incorporate lexical similarity as well as local syntactic features and specific semantic phenomena.", "labels": [], "entities": []}, {"text": "One model predicts absolute scores; the other one direct pairwise judgments.", "labels": [], "entities": []}, {"text": "We find that both models are competitive with regression models built over the scores of established MT evaluation metrics.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.8959039449691772}]}, {"text": "Further data analysis clarifies the complementary behavior of the two feature sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics.", "labels": [], "entities": [{"text": "machine translations", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.6873825043439865}, {"text": "MT", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.994650661945343}]}, {"text": "Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (), NIST).", "labels": [], "entities": [{"text": "MT hypotheses", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9264001250267029}, {"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9953900575637817}, {"text": "NIST", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9045209884643555}]}, {"text": "These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation.", "labels": [], "entities": []}, {"text": "With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g.,).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7773605585098267}]}, {"text": "In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (.", "labels": [], "entities": []}, {"text": "However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor () or MaxSim (.", "labels": [], "entities": []}, {"text": "Other proposals use structural information such as dependency edges (.", "labels": [], "entities": []}, {"text": "In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (), which * This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.8907980918884277}]}, {"text": "The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred..", "labels": [], "entities": []}, {"text": "Figure 1: Entailment status between an MT system hypothesis and a reference translation for good translations (above) and bad translations (below).", "labels": [], "entities": [{"text": "MT system", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.8971293866634369}]}, {"text": "suggests that the quality of an MT hypothesis should be predictable by a combination of lexical and structural features that model the matches and mismatches between system output and reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9908400177955627}]}, {"text": "We use supervised regression models to combine these features and analyze feature weights to obtain further insights into the usefulness of different feature types.", "labels": [], "entities": []}], "datasetContent": [{"text": "Textual entailment (TE) was introduced by as a concept that corresponds more closely to \"common sense\" reasoning than classical, categorical entailment.", "labels": [], "entities": [{"text": "Textual entailment (TE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8190628290176392}]}, {"text": "Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true.", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7480019330978394}]}, {"text": "Information about the presence or absence of entailment between two sentences has been found to be beneficial fora range of NLP tasks such as Word Sense Disambiguation or Question Answering (;).", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.6877981026967367}, {"text": "Question Answering", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.8213752210140228}]}, {"text": "Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in.", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9714521765708923}]}, {"text": "Very good MT output should entail the reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9782604575157166}, {"text": "reference translation", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7158731520175934}]}, {"text": "In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions.", "labels": [], "entities": []}, {"text": "Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over surface-based methods, provided that the linguistic analysis was sufficiently robust.", "labels": [], "entities": [{"text": "recognition of textual entailment (RTE)", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.899510417665754}]}, {"text": "Thus, for RTE, \"deep\" matching outperforms surface matching.", "labels": [], "entities": [{"text": "RTE", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9027435183525085}]}, {"text": "The reason is that linguistic representation makes it considerably easier to distinguish admissible variation (i.e., paraphrase) from true, meaning-changing divergence.", "labels": [], "entities": []}, {"text": "Admissible variation maybe lexical (synonymy), structural (word and phrase placement), or both (diathesis alternations).", "labels": [], "entities": [{"text": "word and phrase placement", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6667846739292145}]}, {"text": "The working hypothesis of this paper is that the benefits of deeper analysis carryover to MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9788694083690643}]}, {"text": "More specifically, we test whether the features that allow good performance on the RTE task can also predict human judgments for MT output.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 83, "end_pos": 91, "type": "TASK", "confidence": 0.8780353665351868}, {"text": "MT output", "start_pos": 129, "end_pos": 138, "type": "TASK", "confidence": 0.9294268488883972}]}, {"text": "Analogously to RTE, these features should help us to differentiate meaning preserving translation variants from bad translations.", "labels": [], "entities": [{"text": "RTE", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.65245121717453}]}, {"text": "Nevertheless, there are also substantial differences between TE and MT evaluation.", "labels": [], "entities": [{"text": "TE", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.5697042346000671}, {"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9783393144607544}]}, {"text": "Crucially, TE assumes the premise and hypothesis to be well-formed sentences, which is not true in MT evaluation.", "labels": [], "entities": [{"text": "TE", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.67368483543396}, {"text": "MT evaluation", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9330589473247528}]}, {"text": "Thus, a possible criticism to the use of TE methods is that the features could become unreliable for ill-formed MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 112, "end_pos": 121, "type": "TASK", "confidence": 0.9293506741523743}]}, {"text": "However, there is a second difference between the tasks that works to our advantage.", "labels": [], "entities": []}, {"text": "Due to its strict compositional nature, TE requires an accurate semantic analysis of all sentence parts, since, for example, one misanalysed negation or counterfactual embedding can invert the entailment status (.", "labels": [], "entities": [{"text": "TE", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9556041955947876}]}, {"text": "In contrast, human MT judgments behave more additively: failure of a translation with respect to a single semantic dimension (e.g., polarity or tense) degrades its quality, but usually not crucially so.", "labels": [], "entities": [{"text": "MT judgments", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9278440475463867}]}, {"text": "We therefore expect that even noisy entailment features can be predictive in MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 77, "end_pos": 90, "type": "TASK", "confidence": 0.9394690990447998}]}, {"text": "To assess and compare the performance of our models, we use corpora that were created by past instances of the WMT workshop.", "labels": [], "entities": [{"text": "WMT workshop", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.5855012536048889}]}, {"text": "Finally, we need to set the tie interval \u03b5.", "labels": [], "entities": [{"text": "tie interval \u03b5", "start_pos": 28, "end_pos": 42, "type": "METRIC", "confidence": 0.8535143534342448}]}, {"text": "Since we did not want to optimize \u03b5, we simply assumed that the percentage of ties observed on WMT 2007 generalizes to test sets such as the 2008 dataset.", "labels": [], "entities": [{"text": "WMT 2007", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.892435610294342}, {"text": "2008 dataset", "start_pos": 141, "end_pos": 153, "type": "DATASET", "confidence": 0.7842930853366852}]}, {"text": "We set \u03b5 so that there are ties for first place on 30% of the sentences, with good practical success (see below).", "labels": [], "entities": []}, {"text": "The first results column (Cons) shows consistency, i.e., accuracy in predicting human pairwise preference judgments.", "labels": [], "entities": [{"text": "consistency", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9979100823402405}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9994099140167236}, {"text": "predicting human pairwise preference judgments", "start_pos": 69, "end_pos": 115, "type": "TASK", "confidence": 0.8541690826416015}]}, {"text": "Note that the performance of a random baseline is not at 50%, but substantially lower.", "labels": [], "entities": []}, {"text": "This is due to (a) the presence of contradictions and ties in the human judgments, which cannot be predicted; and (b) WMT's requirement to compute a total ordering of all translations fora given sentence (rather than independent binary judgments), which introduces transitivity constraints.", "labels": [], "entities": []}, {"text": "(2008) for details.", "labels": [], "entities": []}, {"text": "Among our models, PAIR shows a somewhat better consistency than ABS, as can be expected from a model directly optimized on pairwise judgments.", "labels": [], "entities": [{"text": "PAIR", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8003880381584167}, {"text": "consistency", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9959772229194641}, {"text": "ABS", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.987637996673584}]}, {"text": "Across feature sets, COMB works best with a consistency of 0.53, competitive with published WMT 2008 results.", "labels": [], "entities": [{"text": "COMB", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.6426718831062317}, {"text": "consistency", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.9932411909103394}, {"text": "WMT 2008", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.6208914518356323}]}, {"text": "The two final columns (BASIC and WITHTIES) show Spearman's \u03c1 for the correlation between human judgments and the two types of system-level predictions.", "labels": [], "entities": [{"text": "BASIC", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9422491788864136}, {"text": "Spearman's \u03c1", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.8292309045791626}]}, {"text": "For BASIC system-level predictions, we find that PAIR performs considerably worse than ABS, by a margin of up to \u03c1 = 0.1.", "labels": [], "entities": [{"text": "PAIR", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.7022213339805603}, {"text": "ABS", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9865259528160095}]}, {"text": "Recall that the system-level analysis considers only the top-ranked hypotheses; apparently, a model optimized on pairwise judgments has a harder time choosing the best among the top-ranked hypotheses.", "labels": [], "entities": []}, {"text": "This interpretation is supported by the large benefit that PAIR derives from explicit tie modeling.", "labels": [], "entities": []}, {"text": "ABS gains as well, although not as much, so that the correlation of the tie-aware predictions is similar for ABS and PAIR.", "labels": [], "entities": [{"text": "ABS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9515461325645447}, {"text": "ABS", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.6614392399787903}, {"text": "PAIR", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.7917665243148804}]}, {"text": "Comparing different feature sets, BASIC show a similar pattern to the consistency figures.", "labels": [], "entities": []}, {"text": "There is no clear winner between RTE and TRADMT.", "labels": [], "entities": [{"text": "RTE", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7957818508148193}, {"text": "TRADMT", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.712147057056427}]}, {"text": "The performance of TRADMT is considerably better than the performance of BLEU and TER in the WMT 2008 evaluation, where \u03c1 \u2264 0.55.", "labels": [], "entities": [{"text": "TRADMT", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9826558828353882}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9978563189506531}, {"text": "TER", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.993301510810852}, {"text": "WMT 2008 evaluation", "start_pos": 93, "end_pos": 112, "type": "DATASET", "confidence": 0.8770353396733602}]}, {"text": "RTE is able to match the performance of an  ensemble of state-of-the-art metrics, which validates our hope that linguistically motivated entailment features are sufficiently robust to make a positive contribution in MT evaluation.", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7705209255218506}, {"text": "MT evaluation", "start_pos": 216, "end_pos": 229, "type": "TASK", "confidence": 0.9528816938400269}]}, {"text": "Furthermore, the two individual feature sets are outperformed by the combined feature set COMB.", "labels": [], "entities": [{"text": "COMB", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.809573769569397}]}, {"text": "We interpret this as support for our regressionbased combination approach.", "labels": [], "entities": []}, {"text": "Moving to WITHTIES, we seethe best results from the RTE model which improves by \u2206\u03c1 = 0.06 for ABS and \u2206\u03c1 = 0.11 for PAIR.", "labels": [], "entities": [{"text": "WITHTIES", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.5329357385635376}, {"text": "ABS", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.8860164284706116}, {"text": "PAIR", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.5338616967201233}]}, {"text": "There is less improvement for the other feature sets, in particular COMB.", "labels": [], "entities": [{"text": "COMB", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.9485712051391602}]}, {"text": "We submitted the two overall best models, ABS-RTE and PAIR-RTE with tie-aware prediction, to the WMT 2009 challenge.", "labels": [], "entities": [{"text": "ABS-RTE", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.7865575551986694}, {"text": "PAIR-RTE", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9296502470970154}, {"text": "WMT 2009 challenge", "start_pos": 97, "end_pos": 115, "type": "DATASET", "confidence": 0.829807976881663}]}, {"text": "We analyzed at the models' predictions to gain a better understanding of the differences in the behavior of TRADMT-based and RTE-based models.", "labels": [], "entities": []}, {"text": "As a first step, we computed consistency numbers for the set of \"top\" translations (hypotheses that were ranked highest fora given reference) and for the set of \"bottom\" translations (hypotheses that were ranked worst fora given reference).", "labels": [], "entities": [{"text": "consistency", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9493788480758667}]}, {"text": "We found small but consistent differences between the models: RTE performs about 1.5 percent better on the top hypotheses than on the bottom translations.", "labels": [], "entities": [{"text": "RTE", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.4456678330898285}]}, {"text": "We found the inverse effect for the TRADMT model, which performs 2 points worse on the top hypotheses than on the bottom hypotheses.", "labels": [], "entities": [{"text": "TRADMT", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.8688777685165405}]}, {"text": "Revisiting our initial concern that the entailment features are too noisy for very bad translations, this finding indicates some ungrammaticality-induced degradation for the entailment features, but not much.", "labels": [], "entities": []}, {"text": "Conversely, these numbers also provide support for our initial hypothesis that surface-based features are good at detecting very deviant translations, but can have trouble dealing with legitimate linguistic variation.", "labels": [], "entities": []}, {"text": "Next, we analyzed the average size of the score differences between the best and second-best hypotheses for correct and incorrect predictions.", "labels": [], "entities": []}, {"text": "We found that the RTE-based model predicted on average almost twice the difference for correct predictions (\u2206 = 0.30) than for incorrect predictions (\u2206 = 0.16), while the difference was considerably smaller for the TRADMT-based model (\u2206 = 0.17 for correct vs. \u2206 = 0.13 for incorrect).", "labels": [], "entities": []}, {"text": "We believe it is this better discrimination on the top hypothe-  ses that explains the increased benefit the RTE-based model obtains from tie-aware predictions: if the best hypothesis is wrong, chances are much better than for the TRADMT-based model that counting the secondbest hypothesis as \"best\" is correct.", "labels": [], "entities": []}, {"text": "Unfortunately, this property is not shared by COMB to the same degree, and it does not improve as much as RTE.", "labels": [], "entities": [{"text": "COMB", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.92671799659729}, {"text": "RTE", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.44751566648483276}]}, {"text": "illustrates the difference between RTE and TRADMT.", "labels": [], "entities": [{"text": "TRADMT", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.8995445966720581}]}, {"text": "In the first example, RTE makes a more accurate prediction than TRADMT.", "labels": [], "entities": [{"text": "RTE", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.419715017080307}, {"text": "TRADMT", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.8522340655326843}]}, {"text": "The human rater's favorite translation deviates considerably from the reference translation in lexical choice, syntactic structure, and word order, for which it is punished by TRADMT.", "labels": [], "entities": [{"text": "TRADMT", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9746499061584473}]}, {"text": "In contrast, RTE determines correctly that the propositional content of the reference is almost completely preserved.", "labels": [], "entities": [{"text": "RTE", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6503205299377441}]}, {"text": "The prediction of COMB is between the two extremes.", "labels": [], "entities": [{"text": "COMB", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8749879002571106}]}, {"text": "The second example shows a sentence where RTE provides a worse prediction.", "labels": [], "entities": []}, {"text": "This sentence was rated as bad by the judge, presumably due to the inappropriate translation of the main verb.", "labels": [], "entities": []}, {"text": "This problem, together with the reformulation of the subject, leads TRADMT to correctly predict a low score (rank 5/5).", "labels": [], "entities": [{"text": "TRADMT", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.6179431676864624}]}, {"text": "RTE's deeper analysis comes up with a high score (rank 2/5), based on the existing semantic overlap.", "labels": [], "entities": [{"text": "RTE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9464564323425293}]}, {"text": "The combined model is closer to the truth, predicting rank 4.", "labels": [], "entities": []}, {"text": "Finally, we assessed the importance of the different entailment feature groups in the RTE model.", "labels": [], "entities": [{"text": "RTE", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.8867067098617554}]}, {"text": "1 Since the presence of correlated features makes the weights difficult to interpret, we restrict ourselves to two general observations.", "labels": [], "entities": []}, {"text": "First, we find high weights not only for the score of the alignment between hypothesis and reference, but also fora number of syntacto-semantic match and mismatch features.", "labels": [], "entities": []}, {"text": "This means that we do get an additional benefit from the presence of these features.", "labels": [], "entities": []}, {"text": "For example, features with a negative effect include dropping adjuncts, unaligned root nodes, incompatible modality between the main clauses, person and location mismatches (as opposed to general mismatches) and wrongly handled passives.", "labels": [], "entities": []}, {"text": "Conversely, some factors that increase the prediction are good alignment, matching embeddings under factive verbs, and matches between appositions.", "labels": [], "entities": []}, {"text": "The feature weights are similar for the COMB model.", "labels": [], "entities": [{"text": "COMB", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8483765125274658}]}, {"text": "Second, we find clear differences in the usefulness of feature groups between MT evaluation and the RTE task.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.9429284930229187}, {"text": "RTE task", "start_pos": 100, "end_pos": 108, "type": "TASK", "confidence": 0.8650357127189636}]}, {"text": "Some of them, in particular structural features, can be linked to the generally lower grammaticality of MT hypotheses.", "labels": [], "entities": [{"text": "MT hypotheses", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.9123314917087555}]}, {"text": "A casein point is a feature that fires for mismatches between dependents of predicates and which is too unreliable on the SMT data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.9588162899017334}]}, {"text": "Other differences simply reflect that the two tasks have different profiles, as sketched in Section 2.1.", "labels": [], "entities": []}, {"text": "RTE exhibits high feature weights for quantifier and polarity features, both of which have the potential to influence entailment decisions, but are relatively unimportant for MT evaluation, at least at the current state of the art.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 175, "end_pos": 188, "type": "TASK", "confidence": 0.9640618562698364}]}], "tableCaptions": [{"text": " Table 2: Evaluation on the WMT 2008 dataset for our  regression models, compared to results from WMT 2008", "labels": [], "entities": [{"text": "WMT 2008 dataset", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.9504849910736084}, {"text": "WMT 2008", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.946690171957016}]}]}