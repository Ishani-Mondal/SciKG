{"title": [{"text": "Decoding with Syntactic and Non-Syntactic Phrases in a Syntax-Based Machine Translation System", "labels": [], "entities": [{"text": "Syntax-Based Machine Translation", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.6285468141237894}]}], "abstractContent": [{"text": "A key concern in building syntax-based machine translation systems is how to improve coverage by incorporating more traditional phrase-based SMT phrase pairs that do not correspond to syntactic constituents.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7272818833589554}, {"text": "SMT phrase pairs", "start_pos": 141, "end_pos": 157, "type": "TASK", "confidence": 0.8717867136001587}]}, {"text": "At the same time, it is desirable to include as much syntactic information in the system as possible in order to carryout linguistically motivated reordering, for example.", "labels": [], "entities": []}, {"text": "We apply an extended and modified version of the approach of Tinsley et al.", "labels": [], "entities": []}, {"text": "(2007), extracting syntax-based phrase pairs from a large parallel parsed corpus, combining them with PBSMT phrases, and performing joint decoding in a syntax-based MT framework without loss of translation quality.", "labels": [], "entities": []}, {"text": "This effectively addresses the low coverage of purely syntactic MT without discarding syntactic information.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.7675887942314148}]}, {"text": "Further, we show the potential for improved translation results with the inclusion of a syntactic grammar.", "labels": [], "entities": [{"text": "translation", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9545326828956604}]}, {"text": "We also introduce anew syntax-prioritized technique for combining syntactic and non-syntactic phrases that reduces overall phrase table size and decoding time by 61%, with only a minimal drop in automatic translation metric scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "The dominance of traditional phrase-based statistical machine translation (PBSMT) models () has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 29, "end_pos": 81, "type": "TASK", "confidence": 0.7439127351556506}]}, {"text": "One simple approach is to limit the phrases learned by a standard PBSMT translation model to only those contiguous sequences of words that additionally correspond to constituents in a syntactic parse tree.", "labels": [], "entities": [{"text": "PBSMT translation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7510079741477966}]}, {"text": "However, a total reliance on such syntax-based phrases has been shown to be detrimental to translation quality, as the space of phrase segmentation of a parallel sentence is heavily constrained by both the source-side and target-side tree structures.", "labels": [], "entities": [{"text": "translation", "start_pos": 91, "end_pos": 102, "type": "TASK", "confidence": 0.9592396020889282}, {"text": "phrase segmentation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7646206319332123}]}, {"text": "Noting that the number of phrase pairs extracted from a corpus is reduced by around 80% when they are required to correspond to syntactic constituents, observed that many non-constituent phrase pairs that would not be included in a syntax-only model are in fact extremely important to system performance.", "labels": [], "entities": []}, {"text": "Since then, researchers have explored effective ways for combining phrase pairs derived from syntax-aware methods with those extracted from more traditional PBSMT.", "labels": [], "entities": []}, {"text": "Briefly stated, the goal is to retain the high level of coverage provided by non-syntactic PBSMT phrases while simultaneously incorporating and exploiting specific syntactic knowledge.", "labels": [], "entities": []}, {"text": "overcome the restrictiveness of the syntax-only model by starting with a complete set of phrases as produced by traditional PBSMT heuristics, then annotating the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.", "labels": [], "entities": []}, {"text": "They then introduce new constituent labels to handle the cases where the phrasal entries do not exactly correspond to the syntactic constituents.", "labels": [], "entities": []}, {"text": "also add non-syntactic PBSMT phrases into their tree-to-string translation system.", "labels": [], "entities": []}, {"text": "Working from the other direction, extend a hierarchical PBSMT system with a number of features to prefer or disprefer certain types of syntactic phrases in different contexts.", "labels": [], "entities": []}, {"text": "Restructuring the parse trees to ease their restrictiveness is another recent approach: in particular, binarize source-side parse trees in order to provide phrase pair coverage for phrases that are partially syntactic.", "labels": [], "entities": []}, {"text": "showed an improvement over a PBSMT baseline on four tasks in bidirectional German-English and Spanish-English translation by incorporating syntactic phrases derived from parallel trees into the PBSMT translation model.", "labels": [], "entities": [{"text": "PBSMT translation", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.7226028442382812}]}, {"text": "They first word align and extract phrases from a parallel corpus using the open-source Moses PBSMT toolkit ( , which provides a baseline SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9807177782058716}]}, {"text": "Then, both sides of the parallel corpus are parsed with independent automatic parsers, subtrees from the resulting parallel treebank are aligned, and an additional set of phrases (with each phrase corresponding to a syntactic constituent in the parse tree) is extracted.", "labels": [], "entities": []}, {"text": "The authors report statistically significant improvements in translation quality, as measured by a variety of automatic metrics, when the two types of phrases are combined in the Moses decoder.", "labels": [], "entities": []}, {"text": "Our approach in this paper is structurally similar to that of, but we extend or modify it in a number of key ways.", "labels": [], "entities": []}, {"text": "First, we extract both non-syntactic PBSMT and syntax-driven phrases from a parallel corpus that is two orders of magnitude larger, making our system competitive in size to state-of-the-art SMT systems elsewhere.", "labels": [], "entities": [{"text": "SMT", "start_pos": 190, "end_pos": 193, "type": "TASK", "confidence": 0.9740645885467529}]}, {"text": "Second, we apply a different algorithm for subtree alignment, proposed by , which proceeds bottom-up from existing statistical word alignments, rather than inducing them top-down from lexical alignment probabilities.", "labels": [], "entities": [{"text": "subtree alignment", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7678143680095673}]}, {"text": "Third, in addition to straightforwardly combining syntax-derived phrases with traditional PBSMT phrases, we demonstrate anew combination technique that removes PBSMT phrases whose source-language strings are already covered by a syntax-derived phrase.", "labels": [], "entities": []}, {"text": "This new syntax-prioritized technique results in a 61% reduction in the size of the combined phrase table with only a minimal decrease in automatic translation metric scores.", "labels": [], "entities": []}, {"text": "Finally, and crucially, we carryout the joint decoding over both syntactic and nonsyntactic phrase pairs in a syntax-aware MT system, which allows a syntactic grammar to be put in place on top of the phrase pairs to carryout linguistically motivated reordering, hierarchical decoding, and other operations.", "labels": [], "entities": []}, {"text": "After this introduction, we first describe the base MT system we used, its formalism for specifying translation rules, and the method for extracting syntax-derived phrase pairs from a parallel corpus (Section 2).", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9622419476509094}]}, {"text": "Section 3 gives the two methods for combining PBSMT phrases with our syntactic phrases, and introduces our first steps with including a grammar in the syntax-based translation framework.", "labels": [], "entities": []}, {"text": "The results of our experiments are described in Section 4 and are further discussed in Section 5.", "labels": [], "entities": []}, {"text": "Finally, Section 6 offers some conclusions and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted the lexical resources for our MT system from version 3 of the French-English Europarl parallel corpus ( pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9632360339164734}, {"text": "Europarl parallel corpus", "start_pos": 90, "end_pos": 114, "type": "DATASET", "confidence": 0.8648024400075277}]}, {"text": "Statistical word alignments are learned in both directions with GIZA++ (Och and Ney, 2003), then combined with the \"grow-diag-final\" heuristic.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.6950477063655853}, {"text": "GIZA", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9027760028839111}]}, {"text": "For the extraction of syntax-based phrase pairs, we obtain English-side constituency parses using the Stanford parser (, and Frenchside constituency parses using the).", "labels": [], "entities": [{"text": "English-side constituency parses", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.6443564395109812}, {"text": "Frenchside", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.9395185708999634}]}, {"text": "In phrase extraction, we concentrate on the expanded tree-to-tree-string scenario described in Section 2.2, as it results in a nearly 50% increase in the number of extracted phrase pairs over the tree-to-tree method.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.909209668636322}]}, {"text": "For decoding, we construct a suffix-array language model () from a corpus of 430 million words, including the English side of our training data, the English side of the Hansard corpus, and newswire data.", "labels": [], "entities": [{"text": "Hansard corpus", "start_pos": 169, "end_pos": 183, "type": "DATASET", "confidence": 0.9832095503807068}, {"text": "newswire data", "start_pos": 189, "end_pos": 202, "type": "DATASET", "confidence": 0.8954752087593079}]}, {"text": "The \"gra\" variant uses a nine-rule grammar that is meant to address the most common low-level reorderings between French and English, focusing mainly on the reordering between nouns or noun phrases and adjectives or adjective phrases.", "labels": [], "entities": []}, {"text": "Our test set is the 2000-sentence \"test2007\" data set, also released as part of the WMT workshop series.", "labels": [], "entities": [{"text": "test2007\" data set", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.7067599445581436}, {"text": "WMT workshop series", "start_pos": 84, "end_pos": 103, "type": "DATASET", "confidence": 0.8760184446970621}]}, {"text": "We report case-insensitive scores on version 0.6 of METEOR () with all modules enabled, version 1.04 of IBM-style BLEU (), and version 5 of TER ().", "labels": [], "entities": [{"text": "METEOR", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.8678992390632629}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9122557640075684}, {"text": "TER", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.6305962800979614}]}, {"text": "gives an overall summary of our results on the test2007 data.", "labels": [], "entities": [{"text": "test2007 data", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.917178750038147}]}, {"text": "Overall, we train and test 10 different configurations of phrase pairs in the Stat-XFER decoder.", "labels": [], "entities": [{"text": "Stat-XFER decoder", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.8178163766860962}]}, {"text": "We begin by testing each type of phrase separately, producing one set of baseline systems with only phrase pairs that correspond to syntactic constituents (\"Syntactic only\") and one baseline system with only phrase pairs that were extracted from Moses (\"PBSMT only\").", "labels": [], "entities": []}, {"text": "We then test our two combination techniques, and their variants, as described in Section 3.", "labels": [], "entities": []}, {"text": "Statistical significance is tested on the BLEU metric using paired bootstrap resampling) with n = 1000 and p = 0.05.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.5246252417564392}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9939337372779846}]}, {"text": "In the figure, the best baseline system and the configurations statistically equivalent to it are indicated in bold type.", "labels": [], "entities": []}, {"text": "In addition to automatic metric scores, we also list the number of unique phrase pairs extracted for each configuration.", "labels": [], "entities": []}, {"text": "(Because of the large number of phrase pairs, we pre-filter them to only the set whose source sides appear in the test data; these numbers are the ones reported.)", "labels": [], "entities": []}, {"text": "As an additional point of comparison, we build and tune a Moses MT system on the same data as our Stat-XFER experiments.", "labels": [], "entities": []}, {"text": "The Moses system with a 4-gram language model and a distance-6 lexical reordering model (\"lex RO\") scores similarly to state-of-the-art systems of this type on the test2007 French-English data . Without the reordering model (\"mono\"), the Moses system is as comparable as possible in design and resources to the Stat-XFER PBSMT-only configuration.", "labels": [], "entities": [{"text": "distance-6 lexical reordering model (\"lex RO\")", "start_pos": 52, "end_pos": 98, "type": "METRIC", "confidence": 0.7762515172362328}, {"text": "test2007 French-English data", "start_pos": 164, "end_pos": 192, "type": "DATASET", "confidence": 0.6961836814880371}]}, {"text": "We do not propose in this paper a headto-head performance comparison between the Stat-XFER and Moses decoders; rather, we report results on both to gain a better understanding of the impact of the non-syntactic lexical reordering model in Moses compared with the impact of the syntactic grammar in Stat-XFER.", "labels": [], "entities": []}], "tableCaptions": []}