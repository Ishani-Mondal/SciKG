{"title": [], "abstractContent": [{"text": "We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task.", "labels": [], "entities": [{"text": "paraphrase ranking task", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.8095455964406332}]}, {"text": "An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective.", "labels": [], "entities": [{"text": "SemEval 2007 lexical substitution task", "start_pos": 21, "end_pos": 59, "type": "TASK", "confidence": 0.7751373171806335}]}], "introductionContent": [{"text": "Knowledge about paraphrases is of central importance to textual inference modeling.", "labels": [], "entities": [{"text": "textual inference modeling", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7733733852704366}]}, {"text": "Systems which support automatic extraction of large repositories of paraphrase or inference rules like or thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment.", "labels": [], "entities": [{"text": "question answering", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.8304234445095062}, {"text": "information extraction", "start_pos": 203, "end_pos": 225, "type": "TASK", "confidence": 0.7929392755031586}, {"text": "summarization", "start_pos": 230, "end_pos": 243, "type": "TASK", "confidence": 0.9346118569374084}, {"text": "recognizing textual entailment", "start_pos": 266, "end_pos": 296, "type": "TASK", "confidence": 0.7681581179300944}]}, {"text": "Existing knowledge bases still suffer a number of limitations, making their use in applications challenging.", "labels": [], "entities": []}, {"text": "One of the most serious problems is insensitivity to context.", "labels": [], "entities": []}, {"text": "Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage ().", "labels": [], "entities": []}, {"text": "Application of a rule like \"X shed Y \u21d4 X throw Y \" is appropriate in a sentence like \"a mouse study sheds light on the mixed results,\" but not in sentences like \"the economy seems to be shedding fewer jobs\" or \"cats do not shed the virus to other cats.\"", "labels": [], "entities": []}, {"text": "Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems . Some approaches address the problem of context sensitivity by deriving inference rules whose argument slots bear selectional preference information (.", "labels": [], "entities": [{"text": "precision", "start_pos": 254, "end_pos": 263, "type": "METRIC", "confidence": 0.9892255663871765}, {"text": "context sensitivity", "start_pos": 314, "end_pos": 333, "type": "TASK", "confidence": 0.7175951451063156}]}, {"text": "A different line of accounting for contextual variation has been taken by, who propose a compositional approach, \"contextualizing\" the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments.", "labels": [], "entities": []}, {"text": "A related approach has been proposed by, who integrate selectional preferences into the compositional picture.", "labels": [], "entities": []}, {"text": "In this paper, we propose a context-sensitive vector-space approach which draws some important ideas from Erk and Pado's paper (\"E&P\" in the following), but implements them in a different, more effective way: An evaluation on the SemEval 2007 lexical substitution task data shows that our model significantly outperforms E&P in terms of average precision.", "labels": [], "entities": [{"text": "SemEval 2007 lexical substitution task", "start_pos": 230, "end_pos": 268, "type": "TASK", "confidence": 0.80987046957016}, {"text": "precision", "start_pos": 345, "end_pos": 354, "type": "METRIC", "confidence": 0.8912253975868225}]}, {"text": "Section 2 presents our model and briefly relates it to previous work.", "labels": [], "entities": []}, {"text": "Section 3 describes the evaluation of our model on the lexical substitution task data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task) data, and compare it to a random baseline and E&P's state of the art model.", "labels": [], "entities": [{"text": "SemEval 2007 lexical substitution task) data", "start_pos": 70, "end_pos": 114, "type": "DATASET", "confidence": 0.6889749297073909}]}, {"text": "The lexical substitution task dataset contains 10 instances for 44 target verbs in different sentential contexts.", "labels": [], "entities": []}, {"text": "Systems that participated in the task had to generate paraphrases for each of these instances, which are evaluated against a gold standard containing up to 9 possible paraphrases for individual instances.", "labels": [], "entities": []}, {"text": "Following, we use the data in a different fashion: we pool paraphrases for all instances of a verb in all contexts, and use the models to rank these paraphrase candidates in specific contexts.", "labels": [], "entities": []}, {"text": "shows three instances of the target verb shed together with its paraphrases in the gold standard as an expample.", "labels": [], "entities": []}, {"text": "The paraphrases are attached with weights, which correspond to the number of times they have been given by different annotators.", "labels": [], "entities": []}, {"text": "To allow fora comparision with E&P's model, we follow and extract only sentences from the dataset containing target verbs with overtly realized subject and object, and remove instances from the dataset for which the target verb or one of its arguments is not in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 266, "end_pos": 269, "type": "DATASET", "confidence": 0.9336615204811096}]}, {"text": "We obtain a set of 162 instances for 34 different verbs.", "labels": [], "entities": []}, {"text": "We also remove paraphrases that are not in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9619598984718323}]}, {"text": "On average, target verbs have 20.5 paraphrase candidates, 3.9 of which are correct in specific contexts.", "labels": [], "entities": []}, {"text": "We parse the BNC using MiniPar and extract co-occurrence frequencies, considering only dependency relations for the most frequent 2000 verbs.", "labels": [], "entities": [{"text": "BNC", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6827912330627441}, {"text": "MiniPar", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9216355681419373}]}, {"text": "We don't use raw frequency counts directly but reweight the vectors by pointwise mutual information.", "labels": [], "entities": []}, {"text": "To rank paraphrases in context, we compute contextually constrained vectors for the verb in the input sentence and all its paraphrase candidates by taking the corresponding predicate vectors and restricting them to the argument meanings of the argument head nouns in the input sentence.", "labels": [], "entities": []}, {"text": "The restricted vectors for the paraphrase candidates are then ranked by comparing them to the restricted vector of the input verb using cosine similarity.", "labels": [], "entities": []}, {"text": "In order to compare our model with state of the art, we reimplement E&P's structured vector space model.", "labels": [], "entities": []}, {"text": "We filter stop words, and compute lexical vectors in a \"syntactic\" space using the most frequent 2000 words from the BNC as basis.", "labels": [], "entities": []}, {"text": "We also consider a variant in which the basis corresponds to words indexed by their grammatical roles.", "labels": [], "entities": []}, {"text": "We choose parameters that report to perform best, and use the method described in to compute vectors in context.", "labels": [], "entities": []}, {"text": "As scoring methods, we use both \"precision out of ten\" (P oot ), which was originally used in the lexical substitution task and also used by E&P, and generalized average precision (), a variant of average precision which is frequently used in information extraction tasks and has also been used in the PASCAL RTE challenges ().", "labels": [], "entities": [{"text": "precision out of ten\" (P oot )", "start_pos": 33, "end_pos": 63, "type": "METRIC", "confidence": 0.928118560049269}, {"text": "generalized average precision", "start_pos": 150, "end_pos": 179, "type": "METRIC", "confidence": 0.803658107916514}, {"text": "average precision", "start_pos": 197, "end_pos": 214, "type": "METRIC", "confidence": 0.7622745037078857}, {"text": "information extraction tasks", "start_pos": 243, "end_pos": 271, "type": "TASK", "confidence": 0.8347555001576742}, {"text": "PASCAL RTE challenges", "start_pos": 302, "end_pos": 323, "type": "TASK", "confidence": 0.6748060584068298}]}, {"text": "P oot can be defined as follows: where M is the list of 10 paraphrase candidates top-ranked by the model, G is the corresponding annotated gold data, and f (s) is the weight of the individual paraphrases.", "labels": [], "entities": []}, {"text": "Here, P oot is computed for each target instance separately; below, we report the average overall instances.", "labels": [], "entities": []}, {"text": "Generalized average precision (GAP) is a more precise measure than P oot : Applied to a ranking task with about 20 candidates, P oot just gives the percentage of good candidates found in the upper half of the proposed ranking.", "labels": [], "entities": [{"text": "Generalized average precision (GAP)", "start_pos": 0, "end_pos": 35, "type": "METRIC", "confidence": 0.8795108596483866}]}, {"text": "Average precision is sensitive to the relative position of correct and incorrect candidates in the ranking, GAP moreover rewards the correct order of positive cases w.r.t. their gold standard weight.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9991440773010254}, {"text": "GAP", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.7679144144058228}]}, {"text": "We define average precision first: where xi is a binary variable indicating whether the ith item as ranked by the model is in the gold standard or not, R is the size of the gold standard, and n the number of paraphrase candidates to be ranked.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.7264013290405273}]}, {"text": "If we take xi to be the gold standard weight of the ith item or zero if it is not in the gold standard, we can define generalized average precision as follows: where I(x i ) = 1 if xi is larger than zero, zero otherwise, and y i is the average weight of the ideal ranked list y 1 , . .", "labels": [], "entities": [{"text": "average precision", "start_pos": 130, "end_pos": 147, "type": "METRIC", "confidence": 0.7075695693492889}]}, {"text": ", y i of paraphrases in the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9374504387378693}]}, {"text": "shows the results of our experiments for two variants of our model (\"TDP\"), and compares them to a random baseline and three instantiations (in two variants) of E&P's model.", "labels": [], "entities": []}, {"text": "The \"target only\" models don't use context information, i.e., paraphrases are ranked by cosine similarity of predicate meaning only.", "labels": [], "entities": []}, {"text": "The other models take context into account.", "labels": [], "entities": []}, {"text": "The \"min\" E&P model takes the component-wise minimum to combine a lexical vector with context vectors and considers both subject and object as context; it is the best performing model in.", "labels": [], "entities": []}, {"text": "The \"add\" model uses vector addition and considers only objects as context; it is the best-performing model (in terms of P oot ) for our dataset.", "labels": [], "entities": []}, {"text": "The numbers in brackets refer to variants of the E&P models in which the basis corresponds to words indexed by their syntactic roles.", "labels": [], "entities": []}, {"text": "Note that the results for the E&P models are better than the results published in, which might be due to slightly different datasets or lists of stop-words.", "labels": [], "entities": []}, {"text": "As can be seen, our model performs > 10% better than the random baseline.", "labels": [], "entities": []}, {"text": "It performs > 4% better than the \"min\" E&P model and > 6% better then the \"add\" model in terms of GAP if we use a vectors space with words as basis.", "labels": [], "entities": []}, {"text": "For the variants of the E&P models in which the basis corresponds to words indexed by their syntactic role, we obtain different results, but our model is still > 4% better than these variants.", "labels": [], "entities": []}, {"text": "We can also see that our treatment of context is effective, leading to a > 3% increase of GAP.", "labels": [], "entities": [{"text": "GAP", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8254013657569885}]}, {"text": "A stratified shuffling-based randomization test shows that the differences are statistically significant (p < 0.05).", "labels": [], "entities": []}, {"text": "In terms of P oot , the \"add\" E&P model performs better than our model, which might look surprising, given its low GAP score.", "labels": [], "entities": []}, {"text": "gives a more finegrained comparison between the two models.", "labels": [], "entities": []}, {"text": "It displays the \"precision out of n\" of the two models for varying n.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9983084201812744}]}, {"text": "As can be seen, our model performs better for all n < 10, and much better than the baseline and E&P for n \u2264 4.", "labels": [], "entities": []}], "tableCaptions": []}