{"title": [{"text": "Context-theoretic Semantics for Natural Language: an Overview", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the context-theoretic framework , which provides a set of rules for the nature of composition of meaning based on the philosophy of meaning as context.", "labels": [], "entities": []}, {"text": "Principally, in the framework the composition of the meaning of words can be represented as multiplication of their representative vectors, where multiplication is dis-tributive with respect to the vector space.", "labels": [], "entities": []}, {"text": "We discuss the applicability of the framework to a range of techniques in natural language processing, including subse-quence matching, the lexical entailment model of Dagan et al.", "labels": [], "entities": [{"text": "subse-quence matching", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7335622608661652}]}, {"text": "(2005), vector-based representations of taxonomies, statistical parsing and the representation of uncertainty in logical semantics.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8671490252017975}]}], "introductionContent": [{"text": "Techniques such as latent semantic analysis) and its variants have been very successful in representing the meanings of words as vectors, yet there is currently no theory of natural language semantics that explains how we should compose these representations: what should the representation of a phrase be, given the representation of the words in the phrase?", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6605434517065684}]}, {"text": "In this paper we present such a theory, which is based on the philosophy of meaning as context, as epitomised by the famous sayings of, \"Meaning just is use\" and Firth, \"You shall know a word by the company it keeps\".", "labels": [], "entities": [{"text": "Firth", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.8128729462623596}]}, {"text": "For the sake of brevity we shall present only a summary of our research, which is described in full in, and we give a simplified version of the framework, which nevertheless suffices for the examples which follow.", "labels": [], "entities": []}, {"text": "We believe that the development of theories that can take vector representations of meaning beyond the word level, to the phrasal and sentence levels and beyond are essential for vector based semantics to truly compete with logical semantics, both in their academic standing and in application to real problems in natural language processing.", "labels": [], "entities": []}, {"text": "Moreover the time is ripe for such a theory: never has there been such an abundance of immediately available textual data (in the form of the worldwide web) or cheap computing power to enable vector-based representations of meaning to be obtained.", "labels": [], "entities": []}, {"text": "The need to organise and understand the new abundance of data makes these techniques all the more attractive since meanings are determined automatically and are thus more robust in comparison to hand-built representations of meaning.", "labels": [], "entities": []}, {"text": "A guiding theory of vector based semantics would undoubtedly be invaluable in the application of these representations to problems in natural language processing.", "labels": [], "entities": []}, {"text": "The context-theoretic framework does not provide a formula for how to compose meaning; rather it provides mathematical guidelines for theories of meaning.", "labels": [], "entities": []}, {"text": "It describes the nature of the vector space in which meanings live, gives some restrictions on how meanings compose, and provides us with a measure of the degree of entailment between strings for any implementation of the framework.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows: in Section 2 we present the framework; in Section 3 we present applications of the framework: \u2022 We describe subsequence matching (Section 3.1) and the lexical entailment model of (Dagan et al., 2005) (Section 3.2), both of which have been applied to the task of recognising textual entailment.", "labels": [], "entities": [{"text": "subsequence matching", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.7136313766241074}]}, {"text": "\u2022 We show how a vector based representation of a taxonomy incorporating probabilistic information about word meanings can be con- \u2022 We show how syntax can be represented within the framework in Section 3.4.", "labels": [], "entities": []}, {"text": "\u2022 We summarise our approach to representing uncertainty in logical semantics in Section 3.5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results obtained with our Latent Dirichlet  projection model on the data from the first Recog- nising Textual Entailment Challenge for two doc- ument lengths N = 10 6 and N = 10 7 using a cut- off for the degree of entailment of 0.5 at which  entailment was regarded as holding. CWS is the  confidence weighted score -see (", "labels": [], "entities": [{"text": "Recog- nising Textual Entailment Challenge", "start_pos": 98, "end_pos": 140, "type": "TASK", "confidence": 0.6572569807370504}]}]}