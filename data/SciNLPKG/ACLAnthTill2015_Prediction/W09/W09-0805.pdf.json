{"title": [{"text": "Unsupervised Concept Discovery In Hebrew Using Simple Unsupervised Word Prefix Segmentation for Hebrew and Arabic", "labels": [], "entities": [{"text": "Unsupervised Concept Discovery", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6024075647195181}]}], "abstractContent": [{"text": "Fully unsupervised pattern-based methods for discovery of word categories have been proven to be useful in several languages.", "labels": [], "entities": [{"text": "discovery of word categories", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.7559293210506439}]}, {"text": "The majority of these methods rely on the existence of function words as separate text units.", "labels": [], "entities": []}, {"text": "However, in morphology-rich languages, in particular Semitic languages such as Hebrew and Arabic, the equivalents of such function words are usually written as morphemes attached as prefixes to other words.", "labels": [], "entities": []}, {"text": "As a result, they are missed by word-based pattern discovery methods, causing many useful patterns to be unde-tected and a drastic deterioration in performance.", "labels": [], "entities": [{"text": "word-based pattern discovery", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.6207869748274485}]}, {"text": "To enable high quality lexical category acquisition, we propose a simple unsupervised word segmentation algorithm that separates these morphemes.", "labels": [], "entities": [{"text": "category acquisition", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7587739825248718}, {"text": "word segmentation", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7478457093238831}]}, {"text": "We study the performance of the algorithm for Hebrew and Arabic, and show that it indeed improves a state-of-art unsupervised concept acquisition algorithm in Hebrew.", "labels": [], "entities": [{"text": "concept acquisition", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7545519173145294}]}], "introductionContent": [{"text": "In many NLP tasks, we wish to extract information or perform processing on text using minimal knowledge on the input natural language.", "labels": [], "entities": []}, {"text": "Towards this goal, we sometimes find it useful to divide the set of words in natural language to function words and content words, a division that applies in the vast majority of languages.", "labels": [], "entities": []}, {"text": "Function words (or grammatical words, e.g., a, an, the, in, of, etc) are words that have little or highly ambiguous lexical meaning, and serve to express grammatical or semantic relationships with the other words in a sentence.", "labels": [], "entities": []}, {"text": "In some morphologically-rich languages, important function words are not written as spaceseparated units but as morphemes attached as prefixes to other words.", "labels": [], "entities": []}, {"text": "This fact can cause problems when statistically analyzing text in these languages, for two main reasons: (1) the vocabulary of the language grows, as our lexical knowledge comes solely from a corpus (words appear with and without the function morphemes); (2) information derived from the presence of these morphemes in the sentence is usually lost.", "labels": [], "entities": []}, {"text": "In this paper we address the important task of a fully unsupervised acquisition of Hebrew lexical categories (or concepts -words sharing a significant aspect of their meaning).", "labels": [], "entities": []}, {"text": "We are not aware of any previous work on this task for Hebrew.", "labels": [], "entities": []}, {"text": "Due to the problem above, the performance of many acquisition algorithms deteriorates unacceptably.", "labels": [], "entities": []}, {"text": "This happens, for example, in the () algorithm that utilizes automatically detected function words as the main building block for pattern construction.", "labels": [], "entities": [{"text": "pattern construction", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7142094075679779}]}, {"text": "In order to overcome this problem, one should separate such prefixes from the compound words (words consisting of function morphemes attached to content words) in the input corpus.", "labels": [], "entities": []}, {"text": "When we consider some particular word, there are frequently many options to split it to smaller strings.", "labels": [], "entities": []}, {"text": "Fortunately, the set of function words is small and closed, and the set of grammatical sequences of function prefixes is also small.", "labels": [], "entities": []}, {"text": "Hence we assume it does not cost us much to know in advance what are the possible sequences fora specific language.", "labels": [], "entities": []}, {"text": "Even when considering the small number of possible function words, the task of separating them is not simple, as some words maybe ambiguous.", "labels": [], "entities": []}, {"text": "When reading a word that starts with a prefix known to be a function morpheme, the word maybe a compound word, or it maybe a meaningful word by itself.", "labels": [], "entities": []}, {"text": "For example, the word \"hsws\" in Hebrew can be interpreted as \"hsws\" (hesitation), or \"h sws\" (the horse).", "labels": [], "entities": []}, {"text": "The segmentation of the word is context dependent -the same string maybe segmented differently in different contexts.", "labels": [], "entities": []}, {"text": "One way of doing such word prefix segmentation is to perform a complete morphological disambiguation of the sentence.", "labels": [], "entities": [{"text": "word prefix segmentation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7467160224914551}]}, {"text": "The disambiguation algorithm finds for each word its morphological attributes (POS tag, gender, etc.), and decides whether a word is a compound word or a word without prefixes.", "labels": [], "entities": []}, {"text": "A disambiguation algorithm generally relies on a language-specific morphological analyzer.", "labels": [], "entities": []}, {"text": "It may also require a large manually tagged corpus, construction of which for some particular language or domain requires substantial human labor.", "labels": [], "entities": []}, {"text": "We avoid the utilization of such costly and language-specific disambiguation algorithms and manually annotated data.", "labels": [], "entities": []}, {"text": "In this paper we present a novel method to separate function word prefixes, and evaluate it using manually labeled gold standards in Hebrew and Arabic.", "labels": [], "entities": [{"text": "separate function word prefixes", "start_pos": 43, "end_pos": 74, "type": "TASK", "confidence": 0.6382315531373024}]}, {"text": "We incorporate the method into a pattern-based Hebrew concept acquisition framework and show that it greatly improves state-of-art results for unsupervised lexical category acquisition.", "labels": [], "entities": [{"text": "Hebrew concept acquisition", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.671854297320048}]}, {"text": "This improvement allows the pattern-based unsupervised framework to use one-tenth of the Hebrew data in order to reach a similar level of results.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work, and Section 3 reviews the word categories discovery algorithm.", "labels": [], "entities": [{"text": "word categories discovery", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.77930215994517}]}, {"text": "Section 4 presents the word prefix segmentation algorithm.", "labels": [], "entities": [{"text": "word prefix segmentation", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.727606882651647}]}, {"text": "Results are given in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our algorithm in two stages.", "labels": [], "entities": []}, {"text": "First we test the quality of our unsupervised word segmentation framework on Hebrew and Arabic, comparing our segmentation results to a manually anno-   tated gold standard.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7367357313632965}]}, {"text": "Then we incorporate word segmentation into a concept acquisition framework and compare the performance of this framework with and without word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7187179923057556}, {"text": "word segmentation", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7043459713459015}]}, {"text": "In order to estimate the performance of word segmentation as a standalone algorithm we applied our algorithm on the Hebrew and Arabic corpora, http://mila.cs.technion.ac.il. using different parameter settings.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7376306056976318}]}, {"text": "We first calculated the word frequencies, then applied initial segmentation as described in Section 4.", "labels": [], "entities": []}, {"text": "Then we used SRILM to learn the trigram model from the segmented corpus.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.5826029181480408}]}, {"text": "We utilized Good-Turing discounting with Katz backoff, and we gave words that were not in the training set the constant probability 1E \u2212 9.", "labels": [], "entities": []}, {"text": "Finally we utilized the obtained trigram model to select sentence segmentations.", "labels": [], "entities": [{"text": "sentence segmentations", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7240357547998428}]}, {"text": "To test the influence of the f actor(x) component of the Rank value, we repeated our experiment with and without usage of this component.", "labels": [], "entities": []}, {"text": "We also ran our algorithm with a set of different threshold T values in order to study the influence of this parameter.", "labels": [], "entities": []}, {"text": "show the obtained results for Hebrew and Arabic respectively.", "labels": [], "entities": []}, {"text": "Precision is the ratio of correct prefixes to the total number of detected prefixes in the text.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9898319244384766}]}, {"text": "Recall is the ratio of prefixes that were split correctly to the total number of prefixes.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9869437217712402}]}, {"text": "Accuracy is the number of correctly segmented words divided by the total number of words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.993381142616272}]}, {"text": "As can be seen from the results, the best F-score with and without usage of the f actor(x) component are about the same, but usage of this component gives higher precision for the same F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9949191212654114}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9986118078231812}, {"text": "F-score", "start_pos": 185, "end_pos": 192, "type": "METRIC", "confidence": 0.9794290661811829}]}, {"text": "From comparison of Arabic and Hebrew performance we can also see that segmentation decisions for the task in Arabic are likely to be easier, since the accuracy for T=1 is very high.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.9629997611045837}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9994254112243652}, {"text": "T", "start_pos": 164, "end_pos": 165, "type": "METRIC", "confidence": 0.9045392870903015}]}, {"text": "It means that, unlike in Hebrew (where the best results were obtained for T=0.79), a word which starts with a pre-   fix should generally be segmented.", "labels": [], "entities": [{"text": "T", "start_pos": 74, "end_pos": 75, "type": "METRIC", "confidence": 0.9867171049118042}]}, {"text": "We also compared our best results to the baseline and to previous work.", "labels": [], "entities": []}, {"text": "The baseline draws a segmentation uniformly for each word, from the possible segmentations of the word.", "labels": [], "entities": []}, {"text": "In an attempt to partially reproduce () on our data, we also compared our results to the results obtained from Morfessor Categories-MAP, version 0.9.1 (Described in).", "labels": [], "entities": []}, {"text": "The Morfessor Categories-MAP algorithm gets a list of words and their frequencies, and returns the segmentation for every word.", "labels": [], "entities": []}, {"text": "Since Morfessor may segment words with prefixes which do not exist in our predefined list of valid prefixes, we did not segment the words that had illegal prefixes as segmented by Morfessor.", "labels": [], "entities": []}, {"text": "Results for this comparison are shown in.", "labels": [], "entities": []}, {"text": "Our method significantly outperforms both the baseline and Morfessor-based segmentation.", "labels": [], "entities": [{"text": "Morfessor-based segmentation", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6146398782730103}]}, {"text": "We have also tried to improve the language model by a self training scheme on the same corpus but we observed only a slight improvement, giving 0.848 Precision and 0.872 Recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9923122525215149}, {"text": "Recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9886677861213684}]}], "tableCaptions": [{"text": " Table 1: Ranks vs. Threshold T for Hebrew.", "labels": [], "entities": [{"text": "Threshold T", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9779864251613617}]}, {"text": " Table 2: Ranks vs. Threshold T for Arabic.", "labels": [], "entities": [{"text": "Threshold T", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9842880964279175}]}, {"text": " Table 3: Segmentation results comparison.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9619566202163696}]}, {"text": " Table 4: Human evaluation results.", "labels": [], "entities": []}, {"text": " Table 6: Lexical categories discovery results com- parison. N: number of categories. A: average cat- egory size. J: 'junk' words.", "labels": [], "entities": [{"text": "Lexical categories discovery", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7123867472012838}]}]}