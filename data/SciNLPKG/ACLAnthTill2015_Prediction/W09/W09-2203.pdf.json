{"title": [{"text": "Keepin' It Real: Semi-Supervised Learning with Realistic Tuning", "labels": [], "entities": []}], "abstractContent": [{"text": "We address two critical issues involved in applying semi-supervised learning (SSL) to a real-world task: parameter tuning and choosing which (if any) SSL algorithm is best suited for the task at hand.", "labels": [], "entities": [{"text": "semi-supervised learning (SSL)", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.691745913028717}, {"text": "parameter tuning", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.6764874011278152}]}, {"text": "To gain a better understanding of these issues, we carryout a medium-scale empirical study comparing supervised learning (SL) to two popular SSL algorithms on eight natural language processing tasks under three performance metrics.", "labels": [], "entities": [{"text": "supervised learning (SL)", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.733127772808075}]}, {"text": "We simulate how a practitioner would go about tackling anew problem, including parameter tuning using cross validation (CV).", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.7130487263202667}]}, {"text": "We show that, under such realistic conditions, each of the SSL algorithms can be worse than SL on some datasets.", "labels": [], "entities": []}, {"text": "However, we also show that CV can select SL/SSL to achieve \"agnostic SSL,\" whose performance is almost always no worse than SL.", "labels": [], "entities": []}, {"text": "While CV is often dismissed as unreliable for SSL due to the small amount of labeled data, we show that it is in fact effective for accuracy even when the labeled dataset size is as small as 10.", "labels": [], "entities": [{"text": "SSL", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9655917286872864}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9966180920600891}]}], "introductionContent": [{"text": "Imagine you area real-world practitioner working on a machine learning problem in natural language processing.", "labels": [], "entities": []}, {"text": "If you have unlabeled data, should you use semi-supervised learning (SSL)?", "labels": [], "entities": []}, {"text": "Which SSL algorithm should you use?", "labels": [], "entities": []}, {"text": "How should you set its parameters?", "labels": [], "entities": []}, {"text": "Or could it actually hurt performance, in which case you might be better off with supervised learning (SL)?", "labels": [], "entities": [{"text": "supervised learning (SL)", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.7153810739517212}]}, {"text": "A large number of SSL algorithms have been developed in recent years that allow one to improve performance with unlabeled data, in tasks such as text classification, sequence labeling, and parsing ().", "labels": [], "entities": [{"text": "SSL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9647913575172424}, {"text": "text classification", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.79208704829216}, {"text": "sequence labeling", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.6578897833824158}, {"text": "parsing", "start_pos": 189, "end_pos": 196, "type": "TASK", "confidence": 0.9577058553695679}]}, {"text": "However, many of them are tested on \"SSL-friendly\" datasets, such as \"two moons,\" USPS, and MNIST.", "labels": [], "entities": [{"text": "USPS", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.9199739098548889}, {"text": "MNIST", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9207789897918701}]}, {"text": "Furthermore, the algorithms' parameters are often chosen based on test set performance or manually set based on heuristics and researcher experience.", "labels": [], "entities": []}, {"text": "These issues create practical concerns for deploying SSL in the real world.", "labels": [], "entities": []}, {"text": "We note that ()'s benchmark chapter explores these issues to some extent by comparing several SSL methods on several real and artificial datasets.", "labels": [], "entities": []}, {"text": "The authors reach the conclusions that parameter tuning is difficult with little labeled data and that no method is universally superior.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8039454221725464}]}, {"text": "We reexamine these issues in the context of NLP tasks and offer a simple attempt at overcoming these roadblocks to practical application of SSL.", "labels": [], "entities": [{"text": "SSL", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9487050175666809}]}, {"text": "The contributions of this paper include: \u2022 We present a medium-scale empirical study comparing SL to two popular SSL algorithms on eight less-familiar tasks using three performance metrics.", "labels": [], "entities": []}, {"text": "Importantly, we tune parameters realistically based on cross validation (CV), as a practitioner would do in reality.", "labels": [], "entities": []}, {"text": "\u2022 We show that, under such realistic conditions, each of the SSL algorithms can be worse than SL on some datasets.", "labels": [], "entities": []}, {"text": "\u2022 However, this can be prevented.", "labels": [], "entities": []}, {"text": "We show that CV can be used to select SL/SSL to achieve agnostic SSL, whose performance is almost always no worse than SL.", "labels": [], "entities": []}, {"text": "Traditionally, CV is often dismissed as unreliable for SSL because of the small labeled dataset size.", "labels": [], "entities": [{"text": "SSL", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9599600434303284}]}, {"text": "But we show that CV is effective when using accuracy as an optimization criterion, even when the labeled dataset size is as small as 10.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9982175230979919}]}, {"text": "\u2022 We show the power of cloud computing: we were able to complete roughly 3 months worth of experiments in less than a week.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given the RealSSL procedure in Algorithm 1, we designed an experimental setup to simulate different settings that a real-world practitioner might face when given anew task and a set of algorithms to choose from (some of which use unlabeled data).", "labels": [], "entities": []}, {"text": "This will allow us to compare algorithms across datasets in a variety of situations.", "labels": [], "entities": []}, {"text": "Algorithm 2 measures the performance of one algorithm on one dataset for several different land u combinations.", "labels": [], "entities": []}, {"text": "Specifically, we consider l \u2208 {10, 100} and u \u2208 {100, 1000}.", "labels": [], "entities": []}, {"text": "For each combination, we perform multiple trials (T = 10 here) using different random assignments of data to D labeled and D unlabeled , to obtain confidence intervals around our performance measurements.", "labels": [], "entities": []}, {"text": "All random selections of subsets of data are the same across different algorithms' runs, to permit paired t-tests for evaluation.", "labels": [], "entities": []}, {"text": "Note that, when l = max(L) or u = max(U ), a portion of D pool is not used for training.", "labels": [], "entities": [{"text": "max(U )", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9206074178218842}]}, {"text": "Also, the RealSSL procedure ensures that all parameters are tuned by cross-validation without ever seeing the held-out test set D test . Lastly, we stress that the same grid of algorithm-specific parameter values (discussed in Section 5) is considered for all datasets.", "labels": [], "entities": []}, {"text": "summarizes the datasets used for the comparisons.", "labels": [], "entities": []}, {"text": "In this study we consider only binary classification tasks.", "labels": [], "entities": [{"text": "binary classification tasks", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7547605633735657}]}, {"text": "Note that dis the number of dimensions, P (y = 1) is the proportion of instances in the full dataset belonging to classy = 1, and |D test | refers to the size of the test set (the instances remaining after max(L) + max(U ) = 1100 have been set aside for training trials).", "labels": [], "entities": [{"text": "D test", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9577650129795074}]}, {"text": "[MacWin] is the Mac versus Windows text classification data from the 20-newsgroups dataset, preprocessed by the authors of ().", "labels": [], "entities": []}, {"text": "[Interest] is a binary version of the word sense disambiguation data from.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6172822515169779}]}, {"text": "The task is to distinguish the sense of \"interest\" meaning \"money paid for the use of money\" from the other five senses (e.g., \"readiness to give attention,\" \"a share in a company or business\").", "labels": [], "entities": []}, {"text": "The data comes from a corpus of part-of-speech (POS) tagged sentences containing the word \"interest.\"", "labels": [], "entities": []}, {"text": "Each instance is a bag-of-word/POS vector, excluding words containing the root \"interest\" and those that appeared in less than three sentences overall.", "labels": [], "entities": []}, {"text": "Datasets and are the auto/aviation and real/simulated text classification datasets from the SRAA corpus of UseNet articles.", "labels": [], "entities": [{"text": "SRAA corpus of UseNet articles", "start_pos": 92, "end_pos": 122, "type": "DATASET", "confidence": 0.9258926153182984}]}, {"text": "The and datasets involve identifying corporate and government articles, respectively, in the RCV1 corpus.", "labels": [], "entities": [{"text": "RCV1 corpus", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.9770134687423706}]}, {"text": "We use the versions of these datasets prepared by the authors of ( ).", "labels": [], "entities": []}, {"text": "Finally, the two WISH datasets come from) and involve discriminating between sentences that contain wishful expressions and those that do not.", "labels": [], "entities": [{"text": "WISH datasets", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.9064608514308929}]}, {"text": "The instances in correspond to sentences taken from apolitical discussion board, while is based on sentences from Amazon product reviews.", "labels": [], "entities": []}, {"text": "The features area combination of word and template features as described in (.", "labels": [], "entities": []}, {"text": "Output: Optimal model; Average metric value achieved by optimal parameters during tuning.", "labels": [], "entities": [{"text": "Output", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9811845421791077}]}, {"text": "Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on a specific labeled and unlabeled dataset using cross-validation to tune parameters.", "labels": [], "entities": []}, {"text": "We now aggregate the detailed results to better understand the relative performance of the different methods across all datasets.", "labels": [], "entities": []}, {"text": "We perform this summary evaluation in two ways, based on test set performance (transductive performance is similar).", "labels": [], "entities": []}, {"text": "First, we compare the SSL algorithms across all datasets based on the numbers of times each is worse than, the same as, or better than SL.", "labels": [], "entities": []}, {"text": "For each of the 80 trials of a particular l,u,metric combination, we compare the performance of S3VM, MR, and Best Tuning to SVM.", "labels": [], "entities": [{"text": "MR", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9943789839744568}]}, {"text": "Note that each of these comparisons is akin to a real-world scenario where a practitioner would have to choose an algorithm to use.", "labels": [], "entities": []}, {"text": "lists tuples of the form \"(#trials worse than SVM, #trials equal to SVM, #trials better than SVM).\"", "labels": [], "entities": []}, {"text": "Note that the numbers in each tuple sum to 80.", "labels": [], "entities": []}, {"text": "The perfect SSL algorithm would have a tuple of \"(0, 0, 80),\" meaning that it always outperforms SL.", "labels": [], "entities": []}, {"text": "In terms of accuracy, top) and maxF1 (Table 3, middle), the Best Tuning method turns out to do worse than SVM less often than either S3VM or MR does (i.e., the first number in the tuples for Best Tuning is lower than the corresponding numbers for the other algorithms).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995629191398621}, {"text": "maxF1", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9874399900436401}, {"text": "MR", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9177111983299255}]}, {"text": "At the same time, Best Tuning    outperforms SVM in fewer trials than the other algorithms in some settings for these two metrics.", "labels": [], "entities": []}, {"text": "This is because Best Tuning conservatively selects SVM in many trials.", "labels": [], "entities": []}, {"text": "The take home message is that tuning using CV based on accuracy (and to a lesser extent maxF1) appears to mitigate some risk involved in applying SSL.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9986988306045532}]}, {"text": "AUROC, on the other hand, does not appear as effective for this purpose.", "labels": [], "entities": [{"text": "AUROC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.876961350440979}]}, {"text": "shows that, for u = 1000, Best Tuning is worse than SVM fewer times, but for u = 100, MR achieves better performance overall.", "labels": [], "entities": [{"text": "Best", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.986143946647644}, {"text": "MR", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9989104270935059}]}, {"text": "We also compare overall average test performance (across datasets) for each metric and l,u combination.", "labels": [], "entities": []}, {"text": "reports these results for accuracy, maxF1, and AUROC.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9997665286064148}, {"text": "maxF1", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9925985336303711}, {"text": "AUROC", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9953403472900391}]}, {"text": "In terms of accuracy, we see that the Best Tuning approach leads to better performance than SVM, S3VM, or MR in all settings when averaged over datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993886947631836}, {"text": "MR", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.990394651889801}]}, {"text": "We appear to achieve some synergy in dynamically choosing a different algorithm in each trial.", "labels": [], "entities": []}, {"text": "In terms of maxF1, Best Tuning, S3VM, and MR are all at least as good as SL in three of the four l,u settings, and nearly as good in the fourth.", "labels": [], "entities": [{"text": "maxF1", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9705231189727783}, {"text": "MR", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9992701411247253}, {"text": "SL", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9535545110702515}]}, {"text": "Based on AUROC, though, the results are mixed depending on the specific setting.", "labels": [], "entities": [{"text": "AUROC", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.8252461552619934}]}, {"text": "Notably, though, Best Tuning consistently leads to worse performance than SL when using this metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets used in benchmark comparison. See  text for details.", "labels": [], "entities": [{"text": "benchmark comparison", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.8479902744293213}]}, {"text": " Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,  the boldface indicates the maximum value in each row, as well as others in the row that are not statistically significantly  different based on a paired t-test.", "labels": [], "entities": []}, {"text": " Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of  the form \"(#trials worse than SVM, #trials equal to SVM, #trials better than SVM).\"", "labels": [], "entities": []}, {"text": " Table 4: Aggregate test results averaged over the 80 trials (8 datasets, 10 trials each) in a particular setting.", "labels": [], "entities": []}]}