{"title": [{"text": "Predicting Concept Types in User Corrections in Dialog", "labels": [], "entities": [{"text": "Predicting Concept Types", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8946196635564169}]}], "abstractContent": [{"text": "Most dialog systems explicitly confirm user-provided task-relevant concepts.", "labels": [], "entities": []}, {"text": "User responses to these system confirmations (e.g. corrections, topic changes) maybe misrecognized because they contain unrequested task-related concepts.", "labels": [], "entities": []}, {"text": "In this paper, we propose a concept-specific language model adaptation strategy where the language model (LM) is adapted to the concept type(s) actually present in the user's post-confirmation utterance.", "labels": [], "entities": [{"text": "concept-specific language model adaptation", "start_pos": 28, "end_pos": 70, "type": "TASK", "confidence": 0.6704759001731873}]}, {"text": "We evaluate concept type classification and LM adaptation for post-confirmation utterances in the Let's Go!", "labels": [], "entities": [{"text": "concept type classification", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7023762861887614}, {"text": "LM adaptation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.824728935956955}]}, {"text": "We achieve 93% accuracy on concept type classification using acoustic, lexical and dialog history features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994893074035645}, {"text": "concept type classification", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6417270799477895}]}, {"text": "We also show that the use of concept type classification for LM adaptation can lead to improvements in speech recognition performance.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9719952344894409}, {"text": "speech recognition", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7519446313381195}]}], "introductionContent": [{"text": "In most dialog systems, the system explicitly confirms user-provided task-relevant concepts.", "labels": [], "entities": []}, {"text": "The user's response to a confirmation prompt such as \"leaving from Waterfront?\" may consist of a simple confirmation (e.g. \"yes\"), a simple rejection (e.g. \"no\"), a correction (e.g. \"no, Oakland\") or a topic change (e.g. \"no, leave at 7\" or \"yes, and go to Oakland\").", "labels": [], "entities": []}, {"text": "Each type of utterance has implications for further processing.", "labels": [], "entities": []}, {"text": "In particular, corrections and topic changes are likely to contain unrequested task-relevant concepts that are not well represented in the recognizer's post-confirmation language model (LM) . This means that they are In this paper, we look at user responses to system confirmation prompts CMU's deployed Let's Go!", "labels": [], "entities": [{"text": "CMU's deployed Let's Go!", "start_pos": 289, "end_pos": 313, "type": "DATASET", "confidence": 0.7352245194571358}]}, {"text": "We adopt a two-pass recognition architecture.", "labels": [], "entities": []}, {"text": "In the first pass, the input utterance is processed using a generalpurpose LM (e.g. specific to the domain, or specific to the dialog state).", "labels": [], "entities": []}, {"text": "Recognition may fail on concept words such as \"Oakland\" or \"61C\" , but is likely to succeed on closed-class words (e.g. \"yes\", \"no\", \"and\", \"but\", \"leaving\").", "labels": [], "entities": [{"text": "Recognition", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.821494460105896}]}, {"text": "If the utterance follows a system confirmation prompt, we then use acoustic, lexical and dialog history features to determine the task-related concept type(s) likely to be present in the utterance.", "labels": [], "entities": []}, {"text": "In the second recognition pass, any utterance containing a concept type is re-processed using a concept-specific LM.", "labels": [], "entities": []}, {"text": "We show that: (1) it is possible to achieve high accuracy in determining presence or absence of particular concept types in a post-confirmation utterance; and (2) 2-pass speech recognition with concept type classification and language model adaptation can lead to improved speech recognition performance for post-confirmation utterances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.997294008731842}, {"text": "2-pass speech recognition", "start_pos": 163, "end_pos": 188, "type": "TASK", "confidence": 0.6479619145393372}, {"text": "speech recognition", "start_pos": 273, "end_pos": 291, "type": "TASK", "confidence": 0.7097769975662231}]}, {"text": "The rest of this paper is structured as follows: In Section 2 we discuss related work.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our data.", "labels": [], "entities": []}, {"text": "In Section 4 we present our concept type classification experiment.", "labels": [], "entities": [{"text": "concept type classification", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6195353170235952}]}, {"text": "In Section 5 we present our LM adaptation experiment.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9655820727348328}]}, {"text": "In Section 6 we conclude and discuss future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report the results of experiments on concept type classification in which we examine the impact of the feature sets presented in Table 3.", "labels": [], "entities": [{"text": "concept type classification", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7255039215087891}]}, {"text": "We report performance separately for recognized speech, which is available at runtime; and for transcribed speech, which gives us an idea of best possible performance).", "labels": [], "entities": []}, {"text": "In this section we report the impact of concept type prediction on recognition of post-confirmation utterances in Let's Go!", "labels": [], "entities": [{"text": "concept type prediction", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.709133247534434}, {"text": "recognition of post-confirmation utterances in Let's Go!", "start_pos": 67, "end_pos": 123, "type": "TASK", "confidence": 0.7658854789204068}]}, {"text": "We hypothesized that speech recognition performance for utterances containing a concept can be improved with the use of concept-specific LMs.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7159824967384338}]}, {"text": "We (1) compare the existing dialog state-specific LM adaptation approach used in Let's Go! with our proposed concept-specific adaptation; (2) compare two approaches to concept-specific adaptation (using the system's confirmation prompt type and using our concept type classifiers); and (3) evaluate the impact of different concept type classifiers on concept-specific LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.789819985628128}, {"text": "LM adaptation", "start_pos": 368, "end_pos": 381, "type": "TASK", "confidence": 0.8551414310932159}]}], "tableCaptions": [{"text": " Table 1: Statistics on post-confirmation utterances", "labels": [], "entities": []}, {"text": " Table 2: Confirmation state vs. user concept type", "labels": [], "entities": []}, {"text": " Table 4: Concept type classification results: transcribed speech (all models include feature DIA). Best  overall values in each group are highlighted in bold.", "labels": [], "entities": [{"text": "Concept type classification", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8155093789100647}]}, {"text": " Table 5: Concept type classification results: recognized speech (all models include feature DIA). Best  overall values in each group are highlighted in bold.", "labels": [], "entities": [{"text": "Concept type classification", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.829473594824473}]}, {"text": " Table 6: Speech recognition results. \u2660 indicates significant difference (p<.01). \u2663 indicates significant  difference (p<.05). * indicates near-significant trend in difference (p<.07). Significance for WER is  computed as a paired t-test. Significance for concept recall is an inference on proportion.", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7879902720451355}, {"text": "WER", "start_pos": 202, "end_pos": 205, "type": "TASK", "confidence": 0.573964536190033}]}]}