{"title": [{"text": "Using DEDICOM for Completely Unsupervised Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7042130678892136}]}], "abstractContent": [{"text": "A standard and widespread approach to part-of-speech tagging is based on Hidden Markov Models (HMMs).", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7434276938438416}]}, {"text": "An alternative approach, pioneered by Sch\u00fctze (1993), induces parts of speech from scratch using singular value decomposition (SVD).", "labels": [], "entities": []}, {"text": "We introduce DEDICOM as an alternative to SVD for part-of-speech induction.", "labels": [], "entities": [{"text": "DEDICOM", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.6895930767059326}, {"text": "part-of-speech induction", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.7521149218082428}]}, {"text": "DEDICOM retains the advantages of SVD in that it is completely unsupervised: no prior knowledge is required to induce either the tagset or the associations of types with tags.", "labels": [], "entities": [{"text": "DEDICOM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8374322056770325}]}, {"text": "However, unlike SVD, it is also fully compatible with the HMM framework, in that it can be used to estimate emission-and transition-probability matrices which can then be used as the input for an HMM.", "labels": [], "entities": []}, {"text": "We apply the DEDICOM method to the CONLL corpus (CONLL 2000) and compare the output of DEDICOM to the part-of-speech tags given in the corpus, and find that the correlation (almost 0.5) is quite high.", "labels": [], "entities": [{"text": "CONLL corpus (CONLL 2000)", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.9501981337865194}, {"text": "correlation", "start_pos": 161, "end_pos": 172, "type": "METRIC", "confidence": 0.9582344889640808}]}, {"text": "Using DEDICOM, we also estimate part-of-speech ambiguity for each type, and find that these estimates correlate highly with part-of-speech ambiguity as measured in the original corpus (around 0.88).", "labels": [], "entities": [{"text": "DEDICOM", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.703782320022583}]}, {"text": "Finally, we show how the output of DEDICOM can be evaluated and compared against the more familiar output of supervised HMM-based tagging.", "labels": [], "entities": [{"text": "HMM-based tagging", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.7452400922775269}]}], "introductionContent": [{"text": "Traditionally, part-of-speech tagging has been approached either in a rule-based fashion, or stochastically. was among the first to develop algorithms of the former type.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7660360038280487}]}, {"text": "The rulebased approach relies on two elements: a dictionary to assign possible parts of speech to each word, and a list of hand-written rules -which must be painstakingly developed for each new language or domain -to disambiguate tokens in context.", "labels": [], "entities": []}, {"text": "Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models) to disambiguate tokens in context.", "labels": [], "entities": []}, {"text": "The latter approach was pioneered by and, and became widely known through the work of e.g. and.", "labels": [], "entities": []}, {"text": "A third and more recent approach, known as 'distributional tagging' and exemplified by and, aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain.", "labels": [], "entities": []}, {"text": "Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above.", "labels": [], "entities": [{"text": "Distributional tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8453077673912048}]}, {"text": "Sch\u00fctze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions.", "labels": [], "entities": []}, {"text": "He shows that in the reduced-dimensional space implied by SVD, tokens do indeed cluster intuitively by part-of-speech; and that if context is taken into account, something akin to part-of-speech tagging can be achieved.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.7247958779335022}]}, {"text": "Whereas the performance of stochastic taggers is generally sub-optimal when the domain of the training data differs from that of the test data, distributional tagging sidesteps this problem, since each corpus can be considered in its own right.", "labels": [], "entities": [{"text": "distributional tagging", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.7323476076126099}]}, {"text": "notes two general drawbacks of distributional tagging methods: the performance is relatively modest compared to that of supervised methods; and languages with rich morphology may pose a challenge.", "labels": [], "entities": [{"text": "distributional tagging", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.5478707253932953}]}, {"text": "In this paper, we present an alternative unsupervised approach to distributional tagging.", "labels": [], "entities": [{"text": "distributional tagging", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.8163983821868896}]}, {"text": "Instead of SVD, we use a dimensionality reduction technique known as DEDICOM, which has various advantages over the SVD-based approach.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.6896486282348633}, {"text": "DEDICOM", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.907045304775238}]}, {"text": "Principal among these is that, even though no pre-tagged corpus is required, DEDICOM can easily be used as input to a HMM-based approach (and the two share linear-algebraic similarities, as we will make clear in section 4).", "labels": [], "entities": []}, {"text": "Although our empirical results, like those of, are perhaps still relatively modest, the fact that a clearer connection exists between DEDICOM and HMMs than between SVD and HMMs gives us good reason to believe that with further refinements, DEDICOM maybe able to give us 'the best of both worlds' in many respects: the benefits of avoiding the need fora pre-tagged corpus, with empirical results approaching those of HMM-based tagging.", "labels": [], "entities": []}, {"text": "In the following sections, we introduce DEDICOM, describe its applicability to the partof-speech tagging problem, and outline its connections to the standard HMM-based approach to tagging.", "labels": [], "entities": [{"text": "partof-speech tagging problem", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.8443326354026794}]}, {"text": "We evaluate the use of DEDICOM on the CONLL 2000 shared task data, discuss the results and suggest avenues for improvement.", "labels": [], "entities": [{"text": "CONLL 2000 shared task data", "start_pos": 38, "end_pos": 65, "type": "DATASET", "confidence": 0.9662896275520325}]}], "datasetContent": [{"text": "For all evaluation described here, we used the CONLL 2000 shared task data.", "labels": [], "entities": [{"text": "CONLL 2000 shared task data", "start_pos": 47, "end_pos": 74, "type": "DATASET", "confidence": 0.9577173590660095}]}, {"text": "This English-language newswire corpus consists of 19,440 types and 259,104 tokens (including punctuation marks as separate types/tokens).", "labels": [], "entities": []}, {"text": "Each token is associated with a part-of-speech tag and a chunk tag, although we did not use the chunk tags in the work described here.", "labels": [], "entities": []}, {"text": "The tags are from a 44-item tagset.", "labels": [], "entities": []}, {"text": "The CONLL 2000 tags against which we measure our own results are in fact assigned by the Brill tagger, and while these may not correlate perfectly with those that would have been assigned by a human linguist, we believe that the correlation is likely to be good enough to allow for an informative evaluation of our method.", "labels": [], "entities": [{"text": "CONLL 2000 tags", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9280946254730225}, {"text": "Brill tagger", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.8823033273220062}]}, {"text": "Before discussing the evaluation of unsupervised DEDICOM, let us briefly reconsider the similarities of DEDICOM to the supervised HMM model in the light of actual data in the CONLL corpus.", "labels": [], "entities": [{"text": "CONLL corpus", "start_pos": 175, "end_pos": 187, "type": "DATASET", "confidence": 0.9730297029018402}]}, {"text": "We stated in  Having confirmed that there exists an A (=A*D A ) and R (=R*) which both satisfies the DEDICOM model and can be used directly within a HMM-based tagger to achieve satisfactory results, we now consider whether A and R can be estimated if no tagged training set is available.", "labels": [], "entities": []}, {"text": "We start, therefore, from X, the square 19,440 \u00d7 19,440 (sparse) matrix of raw bigram frequencies from the CONLL 2000 data.", "labels": [], "entities": [{"text": "CONLL 2000 data", "start_pos": 107, "end_pos": 122, "type": "DATASET", "confidence": 0.9846608638763428}]}, {"text": "Using Matlab and the, we computed the best rank-44 non-negative DEDICOM 3 decomposition of this matrix using the 2-way version of the ASALSAN algorithm presented in , which is based on iteratively improving random initial guesses for A and R.", "labels": [], "entities": [{"text": "Matlab", "start_pos": 6, "end_pos": 12, "type": "DATASET", "confidence": 0.9291868805885315}, {"text": "DEDICOM 3 decomposition", "start_pos": 64, "end_pos": 87, "type": "METRIC", "confidence": 0.8836005926132202}, {"text": "ASALSAN", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.726334810256958}]}, {"text": "As with SVD, the rank of the decomposition can be selected by the user; we chose 44 since that was known to be the number of items in the CONLL 2000 tagset, but a lower number could be selected fora coarser-grained part-of-speech analysis.", "labels": [], "entities": [{"text": "CONLL 2000 tagset", "start_pos": 138, "end_pos": 155, "type": "DATASET", "confidence": 0.9342294534047445}]}, {"text": "Ultimately, perhaps the best way to determine the optimal rank would be to evaluate different options within a larger end-to-end system, for example an information retrieval system; this, however, was beyond our scope in this study.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 152, "end_pos": 173, "type": "TASK", "confidence": 0.7497309744358063}]}, {"text": "As already mentioned, there are indeterminacies of rotation and scale in DEDICOM.", "labels": [], "entities": [{"text": "rotation", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9960604310035706}, {"text": "scale", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9891459941864014}, {"text": "DEDICOM", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.7487031817436218}]}, {"text": "As point out, 'when the columns of A are standardized\u2026 the R matrix can then be interpreted as expressing relationships among the dimensions in the same units as the original data.", "labels": [], "entities": []}, {"text": "That is, the R matrix can be interpreted as a matrix of the same kind as the original data matrix X, but describing the relations among the latent aspects of the phrases, rather than the phrases themselves'.", "labels": [], "entities": []}, {"text": "Thus, if DEDICOM is constrained so that A is column-stochastic (which is required in any case of the matrix of emission probabilities), then the sum of the elements in R should approximate the sum of the elements in X.", "labels": [], "entities": []}, {"text": "R is therefore comparable to R* (with some provisos which shall be enumerated below), and to obtain the rowstochastic transition-probability matrix, we simply multiply R by a diagonal matrix D R whose elements are the inverses of R's row sums.", "labels": [], "entities": []}, {"text": "With A as an emission-probability matrix and RD R as a transition-probability matrix, we now have all that is needed for an HMM-based tagger to estimate the most likely sequence of 'tags' given the corpus.", "labels": [], "entities": []}, {"text": "However, since the 'tags' here are numerical indices, as mentioned, to evaluate the output we must look at the correlation between these 'tags' and the gold-standard tags given in the CONLL 2000 data.", "labels": [], "entities": [{"text": "CONLL 2000 data", "start_pos": 184, "end_pos": 199, "type": "DATASET", "confidence": 0.9781248172124227}]}, {"text": "One way this can be done is by presenting a 44 \u00d7 44 confusion matrix (of goldstandard tags against induced tags), and then measuring the correlation coefficient between that matrix and the 'idealized' confusion matrix in which each induced tag corresponds to one and only one 'gold standard' tag.", "labels": [], "entities": []}, {"text": "Using A and RD R as the input to a HMM-based tagger, we tagged the CONLL 2000 dataset with induced tags and obtained the confusion matrix shown in (owing to space constraints, only the first 20 columns are shown).", "labels": [], "entities": [{"text": "CONLL 2000 dataset", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.9735885461171468}]}, {"text": "The correlation between this matrix and the equivalent diagonalized 'ideal' matrix is in fact 0.4942, which is significantly higher than could have occurred by chance.", "labels": [], "entities": []}, {"text": "It should be noted that alack of correlation between the induced tags and the gold standard tags can be attributed to at least two independent factors.", "labels": [], "entities": [{"text": "gold standard tags", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.8584994276364645}]}, {"text": "The first, of course, is any inability of the DEDICOM model to fit the particular problem and data.", "labels": [], "entities": [{"text": "DEDICOM model", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.7568166553974152}]}, {"text": "Clearly, this is undesirable.", "labels": [], "entities": []}, {"text": "The other factor to be borne in mind, which works to DEDICOM's favor, is that the DEDICOM model could yield an A and R which factorize the data more optimally than the A*D and R* implied by the gold-standard tags.", "labels": [], "entities": []}, {"text": "There are three methods we can use to try and tease apart these competing explanations of the results, two quantitative and the other subjective.", "labels": [], "entities": []}, {"text": "Quantitatively, we can compare the respective error matrices E.", "labels": [], "entities": []}, {"text": "We have already mentioned that Similarly, using the A and R from DEDICOM we can compute The fact that the error is lower in the second case implies that DEDICOM allows us to find a part-ofspeech 'factorization' of the data which fits better even than the gold standard, although again there are some caveats to this; we will return to these in the discussion.", "labels": [], "entities": [{"text": "DEDICOM", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.927166223526001}, {"text": "DEDICOM", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.7991051077842712}]}, {"text": "Another way to evaluate the output of DEDICOM is by comparing the number of part-ofspeech tags fora type in the gold standard to the number of classes in the A matrix with which the type is strongly associated.", "labels": [], "entities": []}, {"text": "We test this by measuring the Pearson correlation between the two variables.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 30, "end_pos": 49, "type": "METRIC", "confidence": 0.9807325005531311}]}, {"text": "First, we compute the average number of part-of-speech tags per type using the gold standard.", "labels": [], "entities": []}, {"text": "We refer to this value as ambiguity coefficient; for the CONLL dataset, this is 1.05.", "labels": [], "entities": [{"text": "CONLL dataset", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9826593697071075}]}, {"text": "Because A is dense, if we count all non-zero columns fora type in the A matrix as possible classes, we obtain a much higher ambiguity coefficient.", "labels": [], "entities": []}, {"text": "We therefore set a threshold and consider only those columns whose values exceed a certain threshold.", "labels": [], "entities": []}, {"text": "The threshold is selected so that the ambiguity coefficient of the A matrix is the same as that of the gold standard.", "labels": [], "entities": []}, {"text": "For a given type, every column with a value exceeding the threshold is counted as a possible class for that type.", "labels": [], "entities": []}, {"text": "We then compute the Pearson correlation coefficient between the number of classes fora type in the A matrix and the number of part-of speech tags for that type in the CONLL dataset as provided by the Brill tagger.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 20, "end_pos": 51, "type": "METRIC", "confidence": 0.950490673383077}, {"text": "CONLL dataset", "start_pos": 167, "end_pos": 180, "type": "DATASET", "confidence": 0.9706608057022095}, {"text": "Brill tagger", "start_pos": 200, "end_pos": 212, "type": "DATASET", "confidence": 0.9271714389324188}]}, {"text": "We obtained a correlation coefficient of 0.88, which shows that there is indeed a high correlation between the induced tags and the gold standard tags obtained with DEDICOM.", "labels": [], "entities": [{"text": "DEDICOM", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9106853008270264}]}, {"text": "Finally, we can evaluate the output subjectively by looking at the content of the A matrix.", "labels": [], "entities": []}, {"text": "For each 'tag' (column) in A, the 'types' (rows) can be listed in decreasing order of their weighting in A.", "labels": [], "entities": []}, {"text": "This gives us an idea of which types are most characteristic of which tags, and whether the grouping into tags makes any intuitive sense.", "labels": [], "entities": []}, {"text": "These results (for selected tags only, owing to limitations of space) are given in.", "labels": [], "entities": []}, {"text": "Many groupings in do make sense: for example, the fourth tag is clearly associated with verbs, while the two types with significant weightings for tag 2 are both determiners.", "labels": [], "entities": []}, {"text": "By referring back to, we can see that many tokens in the CONLL 2000 dataset tagged as verbs are indeed tagged by the DEDICOM tagger as 'tag 4', while many determiners are tagged as 'tag 3'.", "labels": [], "entities": [{"text": "CONLL 2000 dataset", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9575640757878622}]}, {"text": "To understand where alack of correlation may arise, however, it is informative to look at apparent anomalies in the A matrix.", "labels": [], "entities": []}, {"text": "For example, it can be seen from that 'new', an adjective, is grouped in the third tag with 'a' and 'the' (and ranking above 'an').", "labels": [], "entities": []}, {"text": "Although not in agreement with the CONLL 2000 'gold standard' tagging, the idea that determiners area type of adjective is in fact in accordance with traditional English grammar.", "labels": [], "entities": [{"text": "CONLL 2000 'gold standard' tagging", "start_pos": 35, "end_pos": 69, "type": "DATASET", "confidence": 0.9059468422617231}]}, {"text": "Here, the grouping of 'new', 'a' and 'the' can be explained by the distributional similarities (all precede nouns).", "labels": [], "entities": []}, {"text": "It should also be emphasized that the A matrix is essentially a 'soft clustering' of types (meaning that types can belong to more than one cluster).", "labels": [], "entities": []}, {"text": "Thus, for example, 'u.s.'", "labels": [], "entities": []}, {"text": "(the abbreviation for United States) appears under both tag 2 (which appears to have high loadings for nouns) and tag 8 (with high loadings for adjectives).", "labels": [], "entities": []}, {"text": "We have alluded above in passing to possible methods for improving the results of the DEDICOM analysis.", "labels": [], "entities": [{"text": "DEDICOM analysis", "start_pos": 86, "end_pos": 102, "type": "DATASET", "confidence": 0.7400102913379669}]}, {"text": "One would be to pre-process the data differently.", "labels": [], "entities": []}, {"text": "Here, a variety of options are available which maintain a generally unsupervised approach (one example is to avoid treating punctuation as tokens).", "labels": [], "entities": []}, {"text": "However, variations in preprocessing are beyond the scope of this paper.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.940322756767273}]}, {"text": "Another method would be to constrain DEDICOM so that the output more closely models the characteristics of A* and R*, the emission-and transition-probability matrices obtained from a tagged training set.", "labels": [], "entities": [{"text": "DEDICOM", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.6100944876670837}]}, {"text": "In particular, there is one important constraint on R* which is not replicated in R: the constraint mentioned above that for all j, . We note that this constraint can be satisfied by Sinkhorn balancing , although it remains to be seen how the constraint on R can best be incorporated into the DEDICOM architecture.", "labels": [], "entities": []}, {"text": "Assuming that A is columnstochastic, another desirable constraint is that the rows of A(D R ) -1 should sum to the same as the rows of X (the respective type frequencies).", "labels": [], "entities": []}, {"text": "With the implementation of these (and any other) constraints, one would expect the fit of DEDICOM to the data to worsen (cf. (6) and (7) above), but incurring this cost could be worthwhile if the payoff were somehow linguistically interesting (for example, if it turned out we could achieve a much higher correlation to gold-standard tagging).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Actual versus expected frequencies for 10 most  common bigrams in CONLL 2000 corpus", "labels": [], "entities": [{"text": "CONLL 2000", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.952938586473465}]}]}