{"title": [{"text": "Superior and Efficient Fully Unsupervised Pattern-based Concept Acquisition Using an Unsupervised Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "Sets of lexical items sharing a significant aspect of their meaning (concepts) are fundamental for linguistics and NLP.", "labels": [], "entities": []}, {"text": "Unsuper-vised concept acquisition algorithms have been shown to produce good results, and are preferable over manual preparation of concept resources, which is labor intensive, error prone and somewhat arbitrary.", "labels": [], "entities": [{"text": "concept acquisition", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7999177575111389}]}, {"text": "Some existing concept mining methods utilize supervised language-specific modules such as POS taggers and computationally intensive parsers.", "labels": [], "entities": [{"text": "concept mining", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7372379153966904}, {"text": "POS taggers", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.6542502343654633}]}, {"text": "In this paper we present an efficient fully unsupervised concept acquisition algorithm that uses syntactic information obtained from a fully unsupervised parser.", "labels": [], "entities": [{"text": "concept acquisition", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7719891369342804}]}, {"text": "Our algorithm incorporates the bracketings induced by the parser into the meta-patterns used by asymmetric patterns and graph-based concept discovery algorithm.", "labels": [], "entities": [{"text": "graph-based concept discovery", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.6608228782812754}]}, {"text": "We evaluate our algorithm on very large corpora in English and Russian, using both human judgments and WordNet-based evaluation.", "labels": [], "entities": []}, {"text": "Using similar settings as the leading fully unsupervised previous work, we show a significant improvement in concept quality and in the extraction of multiword expressions.", "labels": [], "entities": []}, {"text": "Our method is the first to use fully unsupervised parsing for unsupervised concept discovery, and requires no language-specific tools or pattern/word seeds.", "labels": [], "entities": [{"text": "concept discovery", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7723433077335358}]}], "introductionContent": [{"text": "Comprehensive lexical resources for many domains and languages are essential for most NLP applications.", "labels": [], "entities": []}, {"text": "One of the most utilized types of such resources is a repository of concepts: sets of lexical items sharing a significant aspect of their meanings (e.g., types of food, tool names, etc).", "labels": [], "entities": []}, {"text": "While handcrafted concept databases (e.g., WordNet) are extensively used in NLP, manual compilation of such databases is labor intensive, error prone, and somewhat arbitrary.", "labels": [], "entities": []}, {"text": "Hence, for many languages and domains great efforts have been made for automated construction of such databases from available corpora.", "labels": [], "entities": []}, {"text": "While language-specific and domainspecific studies show significant success in development of concept discovery frameworks, the majority of domains and languages remain untreated.", "labels": [], "entities": []}, {"text": "Hence there is a need fora framework that performs well for many diverse settings and is as unsupervised and language-independent as possible.", "labels": [], "entities": []}, {"text": "Numerous methods have been proposed for seedbased concept extraction where a set of concept patterns (or rules), or a small set of seed words for each concept, is provided as input to the concept acquisition system.", "labels": [], "entities": [{"text": "seedbased concept extraction", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6269537508487701}]}, {"text": "However, even simple definitions for concepts are not always available.", "labels": [], "entities": []}, {"text": "To avoid requiring this type of input, a number of distributional and pattern-based methods have been proposed for fully unsupervised seed-less acquisition of concepts from text.", "labels": [], "entities": []}, {"text": "Pattern-based algorithms were shown to obtain high quality results while being highly efficient in comparison to distributional methods.", "labels": [], "entities": []}, {"text": "Such fully unsupervised methods do not incorporate any language-specific parsers or taggers, so can be successfully applied to diverse languages.", "labels": [], "entities": []}, {"text": "However, unsupervised pattern-based methods suffer from several weaknesses.", "labels": [], "entities": []}, {"text": "Thus they are frequently restricted to single-word terms and are unable to discover multiword expressions in efficient and precise manner.", "labels": [], "entities": []}, {"text": "They also usually ignore potentially useful part-of-speech and other syntactic information.", "labels": [], "entities": []}, {"text": "In order to address these weaknesses, several studies utilize language-specific parsing or tagging systems in concept acquisition.", "labels": [], "entities": [{"text": "language-specific parsing or tagging", "start_pos": 62, "end_pos": 98, "type": "TASK", "confidence": 0.6813511028885841}, {"text": "concept acquisition", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.750892847776413}]}, {"text": "Unfortunately, while improving results, this heavily affects the language-and domain-independence of such frameworks, and severely impacts efficiency since even shallow parsing is computationally demanding.", "labels": [], "entities": []}, {"text": "In this paper we present a method to utilize the information induced by unsupervised parsers in an unsupervised pattern-based concept discovery framework.", "labels": [], "entities": []}, {"text": "With the recent development of fast fully unsupervised parsers, it is now possible to add parserbased information to lexical patterns while keeping the language-independence of the whole framework and still avoiding heavy computational costs.", "labels": [], "entities": []}, {"text": "Specifically, we incorporate the bracketings induced by the parser into the meta-patterns used by asymmetric patterns and graph-based unsupervised concept discovery algorithm.", "labels": [], "entities": [{"text": "graph-based unsupervised concept discovery", "start_pos": 122, "end_pos": 164, "type": "TASK", "confidence": 0.6728391945362091}]}, {"text": "We performed a thorough evaluation on two English corpora (the BNC and a 68GB web corpus) and on a 33GB Russian corpus.", "labels": [], "entities": [{"text": "BNC", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.9510859847068787}]}, {"text": "Evaluations were done using both human judgments and WordNet, in similar settings as that of the leading unsupervised previous work.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9625683426856995}]}, {"text": "Our results show that utilization of unsupervised parser both improves the assignment of single-word terms to concepts and allows highprecision discovery and assignment of of multiword expressions to concepts.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for concept discovery with (P+) and  without (P) utilization of parsing data. V is the total num- ber (millions) of different words in the corpus. W is the  number (thousands) of words belonging to at least one of  the terms for one of the concepts. C is the number (thou- sands) of concepts (after merging and windowing). AS  is the average(words) category size.", "labels": [], "entities": [{"text": "concept discovery", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7538256347179413}, {"text": "AS", "start_pos": 341, "end_pos": 343, "type": "METRIC", "confidence": 0.9900702834129333}]}, {"text": " Table 2: Results of evaluation by human judgment of  three data sets. P+ single/mwe: single-word/MWE terms  existing only in P+ concept; P: single-word terms existing  only in P concept; Both: terms existing in both concepts;  Rand: random terms. See text for detailed explanations.", "labels": [], "entities": []}, {"text": " Table 3: WordNet evaluation in comparison to P (Davi- dov and Rappoport, 2006) and to Widdows(Widdows and  Dorow, 2002). Columns show average precision, preci- sion* (as defined in text), recall, and % of new words  added to corresponding WN subtree.", "labels": [], "entities": [{"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9877963066101074}, {"text": "recall", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.9995046854019165}]}, {"text": " Table 4: Percentage of noisy concepts as a function of  windowing.", "labels": [], "entities": []}]}