{"title": [{"text": "Multi-word expressions in textual inference: Much ado about nothing?", "labels": [], "entities": []}], "abstractContent": [{"text": "Multi-word expressions (MWE) have seen much attention from the NLP community.", "labels": [], "entities": [{"text": "Multi-word expressions (MWE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7316394925117493}]}, {"text": "In this paper, we investigate their impact on the recognition of tex-tual entailment (RTE).", "labels": [], "entities": [{"text": "recognition of tex-tual entailment (RTE)", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.8831316573279244}]}, {"text": "Using the manual Microsoft Research annotations, we first manually count and classify MWEs in RTE data.", "labels": [], "entities": [{"text": "RTE data", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.7973796427249908}]}, {"text": "We find few, most of which are arguably unlikely to cause processing problems.", "labels": [], "entities": []}, {"text": "We then consider the impact of MWEs on a current RTE system.", "labels": [], "entities": []}, {"text": "We are unable to confirm that entailment recognition suffers from wrongly aligned MWEs.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9244654476642609}]}, {"text": "In addition, MWE alignment is difficult to improve, since MWEs are poorly represented in state-of-the-art paraphrase resources, the only available sources for multi-word similarities.", "labels": [], "entities": [{"text": "MWE alignment", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.9474245011806488}]}, {"text": "We conclude that RTE should concentrate on other phenomena impacting entailment, and that paraphrase knowledge is best understood as capturing general lexico-syntactic variation.", "labels": [], "entities": [{"text": "RTE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9871299862861633}]}], "introductionContent": [{"text": "Multi-word expressions (MWEs) can be defined as \"idiosyncratic interpretations that crossword boundaries\", such as traffic light or kick the bucket.", "labels": [], "entities": [{"text": "Multi-word expressions (MWEs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6390257120132447}]}, {"text": "Called a \"pain in the neck for NLP\", they have received considerable attention in recent years and it has been suggested that proper treatment could make a significant difference in various NLP tasks).", "labels": [], "entities": [{"text": "NLP", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.946216344833374}]}, {"text": "The importance attributed to them is also reflected in a number of workshops (.", "labels": [], "entities": []}, {"text": "However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications.", "labels": [], "entities": [{"text": "MWE handling", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.9495149552822113}]}, {"text": "This paper investigates the impact of MWEs on the \"recognition of textual entailment\" (RTE) task ( ).", "labels": [], "entities": [{"text": "recognition of textual entailment\" (RTE) task", "start_pos": 51, "end_pos": 96, "type": "TASK", "confidence": 0.8592221008406745}]}, {"text": "Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE.", "labels": [], "entities": [{"text": "RTE", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9764253497123718}]}, {"text": "A number of papers have suggested that paraphrase knowledge plays a very important role (.", "labels": [], "entities": [{"text": "paraphrase knowledge", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.8930766880512238}]}, {"text": "For example, conclude: \"Our analysis also shows that paraphrases standout as a dominant contributor to the entailment task.\"", "labels": [], "entities": []}, {"text": "The term \"paraphrase\" is however often construed broadly.", "labels": [], "entities": [{"text": "paraphrase\"", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9641421735286713}]}, {"text": "In, it refers to the ability of relating lexico-syntactic reformulations such as diathesis alternations, passivizations, or symmetrical predicates (X lent his BMW to Y/Y borrowed X's BMW).", "labels": [], "entities": [{"text": "relating lexico-syntactic reformulations", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.7250792781511942}]}, {"text": "If \"paraphrase\" simply refers to the use of a language's lexical and syntactic possibilities to express equivalent meaning in different ways, then paraphrases are certainly important to RTE.", "labels": [], "entities": [{"text": "RTE", "start_pos": 186, "end_pos": 189, "type": "TASK", "confidence": 0.9625358581542969}]}, {"text": "But such a claim means little more than that RTE can profit from good understanding of syntax and semantics.", "labels": [], "entities": [{"text": "RTE", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.7451666593551636}]}, {"text": "However, given the abovementioned interest in MWEs, there is another possibility: does success in RTE involve proper handling of MWEs, such as knowing that take a pass on is equivalent to aren't purchasing, or kicked the bucket to died?", "labels": [], "entities": [{"text": "RTE", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9411181807518005}]}, {"text": "This seems not too far-fetched: Knowledge about MWEs is under-represented in existing semantic resources like WordNet or distributional thesauri, but should be present in paraphrase resources, which provide similarity judgments between phrase pairs, including MWEs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9540835618972778}]}, {"text": "The goal of our study is to investigate the merits of this second, more precise, hypothesis, measuring the impact of MWE processing on RTE.", "labels": [], "entities": [{"text": "MWE processing", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.9426487684249878}]}, {"text": "In the absence of a universally accepted definition of MWEs, we define MWEs in the RTE setting as multi-word alignments, i.e., words that participate in more than one word alignment link between premise and hypothesis: The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE.", "labels": [], "entities": [{"text": "RTE", "start_pos": 416, "end_pos": 419, "type": "TASK", "confidence": 0.8410796523094177}]}, {"text": "In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 and hypothesis and base at least part of their decision on properties of this alignment.", "labels": [], "entities": []}, {"text": "We proceed in three steps.", "labels": [], "entities": []}, {"text": "First, we analyze the Microsoft Research (MSR) manual word alignments for the RTE2 dataset (), shedding light on the relationship between alignments and multi-word expressions.", "labels": [], "entities": [{"text": "Microsoft Research (MSR) manual word alignments", "start_pos": 22, "end_pos": 69, "type": "DATASET", "confidence": 0.8325932919979095}, {"text": "RTE2 dataset", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9483017325401306}]}, {"text": "We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data.", "labels": [], "entities": []}, {"text": "Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs.", "labels": [], "entities": []}, {"text": "Finally, we investigate the impact of MWEs and their handling on practical entailment recognition.", "labels": [], "entities": [{"text": "practical entailment recognition", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.779154360294342}]}], "datasetContent": [{"text": "In the first part of our study, we estimate the extent to which the inability of aligners to model one-to- OTHER (5), (6), (7): MWEs categories and definition criteria (M-to-M: many-to-many; 1-to-M: one-to-many).", "labels": [], "entities": [{"text": "OTHER", "start_pos": 107, "end_pos": 112, "type": "METRIC", "confidence": 0.8881180286407471}]}, {"text": "many and many-to-many correspondences is an issue.", "labels": [], "entities": []}, {"text": "To do so, we use the Microsoft Research manual alignments for the RTE2 data.", "labels": [], "entities": [{"text": "Microsoft Research manual alignments", "start_pos": 21, "end_pos": 57, "type": "DATASET", "confidence": 0.8999415338039398}, {"text": "RTE2 data", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.930428147315979}]}, {"text": "To date, the MSR data constitutes the only gold standard alignment corpus publicly available.", "labels": [], "entities": [{"text": "MSR data", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.774257481098175}]}, {"text": "Since annotators were not constrained to use one-to-one alignments, we assume that the MSR alignments contain multiword alignments where appropriate.", "labels": [], "entities": []}, {"text": "From the MSR data, we extract all multi-word alignments that fall outside the scope of \"functional\" alignments, i.e., alignments of the form \"many-to-many\" or \"one-to-many\" (in the direction hypothesis-premise).", "labels": [], "entities": []}, {"text": "We annotate them according to the categories defined below.", "labels": [], "entities": []}, {"text": "The MSR data distinguishes between SURE and POSSIBLE alignments.", "labels": [], "entities": [{"text": "MSR data", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7889028191566467}]}, {"text": "We only take the SURE alignments into account.", "labels": [], "entities": [{"text": "SURE", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.44419658184051514}]}, {"text": "While this might mean missing some multi-word alignments, we found many \"possible\" links to be motivated by the desire to obtain a highcoverage alignment, as Ex.", "labels": [], "entities": []}, {"text": "Here, the hypothesis words \"Regina Schueller\" are individually \"sure\"-aligned to the premise words \"Regina Schueller\" (solid lines), but are also both \"possible\"-linked to \"ECB spokeswoman\" (dashed lines).", "labels": [], "entities": [{"text": "ECB spokeswoman", "start_pos": 173, "end_pos": 188, "type": "DATASET", "confidence": 0.9274685084819794}]}, {"text": "This \"possible\" alignment can be motivated on syntactic or referential grounds, but does not indicate a correspondence in meaning (as opposed to reference).", "labels": [], "entities": []}, {"text": "shows the seven categories we define to distinguish the different types of multi-word alignments.", "labels": [], "entities": []}, {"text": "We use two main complementary criteria for our annotation.", "labels": [], "entities": []}, {"text": "The first one is the cardinality of the alignment: does it involve phrases proper on both sides (many-to-many), or just on one side (one-to-many)?", "labels": [], "entities": []}, {"text": "The second one is decomposability: is it possible to create one or more one-to-one alignments that capture the main semantic contribution of the multi-word alignment?", "labels": [], "entities": []}, {"text": "Our motivation for introducing this criterion is that even aligners that are unable to recover the complete MWE have a chance to identify the links crucial for entailment if the MWE is decomposable (categories (1) and (3)).", "labels": [], "entities": []}, {"text": "This is not possible for the more difficult non-decomposable categories (2) and (4).", "labels": [], "entities": []}, {"text": "The remaining categories, (5) to (7), involve auxiliaries, multiple mentions, and named entities, which are not MWEs in the narrow sense.", "labels": [], "entities": []}, {"text": "We will henceforth use the term \"true MWEs\" to refer to categories (1)-(4), as opposed to (5)-.", "labels": [], "entities": []}, {"text": "The results for evaluating the MANLI and UNIQ alignments against the manual alignment links in the MSR RTE2 test set are given in.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8769976496696472}, {"text": "UNIQ alignments", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.7869832813739777}, {"text": "MSR RTE2 test set", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.9022612273693085}]}, {"text": "We present micro-averaged numbers, where each alignment link counts equally (i.e., longer problems have a larger impact).", "labels": [], "entities": []}, {"text": "The overall difference is not large, but MANLI produces a slightly better alignment.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.6806462407112122}]}, {"text": "The ability of MANLI to construct many-tomany alignments is reflected in a different position on the precision/recall curve: the MANLI aligner is less precise than UNIQ, but has a higher recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9990647435188293}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9230993390083313}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.998540997505188}]}, {"text": "Examples for UNIQ and MANLI alignments are shown in.", "labels": [], "entities": [{"text": "UNIQ", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.878131628036499}, {"text": "MANLI", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.6413375735282898}]}, {"text": "A comparison of the alignments shows the pattern to be expected from: MANLI has a higher recall, but contains occasional questionable links, such as at President \u2192 President in.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8030281662940979}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9995301961898804}]}, {"text": "However, the many-to-many alignments that MANLI produces do not correspond well to the MWE alignments.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.9154376983642578}]}, {"text": "The overall impact of the paraphrase resources is very small, and their addition actually hurts MANLI's performance slightly.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.7026890516281128}]}, {"text": "A more detailed analysis revealed two contrary trends.", "labels": [], "entities": []}, {"text": "On the one hand, the paraphrase resources provide 6 Aligner w/o para w/ para UNIQ 63.8 -MANLI 60.6 60.6: Entailment recognition accuracy of the Stanford system on RTE2 test (two-way task).", "labels": [], "entities": [{"text": "UNIQ 63.8 -MANLI 60.6 60.6", "start_pos": 77, "end_pos": 103, "type": "DATASET", "confidence": 0.8600743412971497}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.951652467250824}, {"text": "RTE2 test", "start_pos": 163, "end_pos": 172, "type": "DATASET", "confidence": 0.9001263082027435}]}, {"text": "Aligner w/o para w/ para TAC system UNIQ 63.3 -61.4 MANLI 59.0 57.9 57.0: Entailment recognition accuracy of the Stanford system on RTE4 (two-way task).", "labels": [], "entities": [{"text": "TAC", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8005074262619019}, {"text": "UNIQ 63.3 -61.4 MANLI 59.0 57.9", "start_pos": 36, "end_pos": 67, "type": "DATASET", "confidence": 0.9079552633421761}, {"text": "Entailment recognition", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.6116235107183456}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9361259937286377}, {"text": "RTE4", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.8647510409355164}]}, {"text": "beneficial information, maybe surprisingly, in the form of broad distributional similarities for single words that were not available from the standard lexical resources (e.g., the alignment \"the company's letter\" \u2192 \"the company's certificate\").", "labels": [], "entities": []}, {"text": "On the other hand, MANLI captures not one of the true MWEs identified in the MSR data.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.8040193319320679}, {"text": "MSR data", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.7554303705692291}]}, {"text": "It only finds two many-to-many alignments which belong to the CP category: aimed criticism \u2192 has criticised, European currency \u2192 euro currency.", "labels": [], "entities": [{"text": "European currency \u2192 euro currency", "start_pos": 109, "end_pos": 142, "type": "DATASET", "confidence": 0.7357203960418701}]}, {"text": "We see this as the practical consequences of our observation from Section 4: The scores in current paraphrase resources are too noisy to support accurate MWE recognition (cf.).", "labels": [], "entities": [{"text": "MWE recognition", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.9421723186969757}]}, {"text": "We finally evaluated the performance of the Stanford system using UNIQ and MANLI alignments on the entailment task.", "labels": [], "entities": [{"text": "UNIQ", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9092348217964172}]}, {"text": "We consider two datasets: RTE2 test, the alignment evaluation dataset, and the most recent RTE4 dataset, where current numbers for the Stanford system are available from last year's Text Analysis Conference (TAC).", "labels": [], "entities": [{"text": "RTE2 test", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.7141128778457642}, {"text": "alignment evaluation", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.9158967137336731}, {"text": "RTE4 dataset", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9280409216880798}, {"text": "Text Analysis Conference (TAC)", "start_pos": 182, "end_pos": 212, "type": "TASK", "confidence": 0.8287572662035624}]}, {"text": "A reasonable conjecture would be that better alignments translate into better entailment recognition.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7332336902618408}]}, {"text": "However, as the results in show, this is not the case.", "labels": [], "entities": []}, {"text": "Overall, UNIQ outperforms MANLI by several percent accuracy despite MANLI's better alignments.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8895634412765503}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9993755221366882}, {"text": "MANLI", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.8890005946159363}]}, {"text": "This \"baseline\" difference should not be overinterpreted, since it maybe setup-specific: the features computed in the inference stage of the Stanford system were developed mainly with the UNIQ aligner in mind.", "labels": [], "entities": []}, {"text": "A more significant result is that the integration of paraphrase knowledge in MANLI has no effect on RTE2 test, and even decreases performance on RTE4.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.8815635442733765}, {"text": "RTE4", "start_pos": 145, "end_pos": 149, "type": "DATASET", "confidence": 0.8754109740257263}]}, {"text": "The general picture that we observe is that there is only a loose coupling between alignments and the entailment decision: individual alignments seldom matter.", "labels": [], "entities": []}, {"text": "This is shown, for example, by the alignments in.", "labels": [], "entities": []}, {"text": "Even though MANLI provides a better overall alignment, UNIQ's alignment is \"good enough\" for entailment purposes.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9128841161727905}, {"text": "UNIQ", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8376684784889221}]}, {"text": "In, the two words UNIQ leaves unaligned area preposition (at) and alight verb (aimed), both of which are not critical to determine whether or not the premise entails the hypothesis.", "labels": [], "entities": []}, {"text": "This interpretation is supported by another analysis, where we tested whether entailments involving at least one true MWE are more difficult to recognize.", "labels": [], "entities": []}, {"text": "We computed the entailment accuracy for all applicable RTE2 test pairs (7%, 58 sentences).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9561677575111389}, {"text": "RTE2 test pairs", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.8680919210116068}]}, {"text": "The accuracy on this subset is 62% for the MANLI model without paraphrases, 64% for the MANLI model with paraphrases, and 74% for UNIQ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997026324272156}, {"text": "MANLI", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8784932494163513}, {"text": "MANLI", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.8697251677513123}, {"text": "UNIQ", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.9380842447280884}]}, {"text": "The differences from the numbers in are not significant due to the small size of the MWE sample, but we observe that the accuracy on the MWE subset tends to be higher than on the whole set (rather than lower).", "labels": [], "entities": [{"text": "MWE sample", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.7789523005485535}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9993990659713745}]}, {"text": "Futhermore, even though we finally see a small beneficial effect of paraphrases on the MANLI aligner, the UNIQ aligner, which completely ignores MWEs, still performs substantially better.", "labels": [], "entities": [{"text": "MANLI", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.8668243885040283}]}, {"text": "Our conclusion is that wrong entailment decisions rarely hinge on wrongly aligned MWEs, at least with a probabilistic architecture like the Stanford system.", "labels": [], "entities": []}, {"text": "Consequently, it suffices to recover the most crucial alignment links to predict entailment, and the benefits associated with the use of a more restricted alignment formulation, like the one-to-one alignment formulation of UNIQ, outweighs those of more powerful alignment models, like MANLI's phrasal alignments.", "labels": [], "entities": [{"text": "UNIQ", "start_pos": 223, "end_pos": 227, "type": "DATASET", "confidence": 0.8892986178398132}, {"text": "MANLI", "start_pos": 285, "end_pos": 290, "type": "DATASET", "confidence": 0.8649569749832153}]}], "tableCaptions": [{"text": " Table 2: Frequencies of sentences with different  multi-word alignment categories in MSR data.", "labels": [], "entities": [{"text": "MSR data", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.7735656499862671}]}, {"text": " Table 3: Paraphrases of \"poorly represented\" with  scores (semantic similarities).", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of aligners and resources  against the manual MSR RTE2 test annotations.", "labels": [], "entities": [{"text": "MSR RTE2 test annotations", "start_pos": 67, "end_pos": 92, "type": "DATASET", "confidence": 0.8254814147949219}]}]}