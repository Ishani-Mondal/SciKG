{"title": [{"text": "Paraphrase assessment in structured vector space: Exploring parameters and datasets", "labels": [], "entities": [{"text": "Paraphrase assessment", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9512922465801239}]}], "abstractContent": [{"text": "The appropriateness of paraphrases for words depends often on context: \"grab\" can replace \"catch\" in \"catch a ball\", but not in \"catch a cold\".", "labels": [], "entities": []}, {"text": "Struc-tured Vector Space (SVS) (Erk and Pad\u00f3, 2008) is a model that computes word meaning in context in order to assess the appropriateness of such paraphrases.", "labels": [], "entities": []}, {"text": "This paper investigates \"best-practice\" parameter settings for SVS, and it presents a method to obtain large datasets for paraphrase assessment from corpora with WSD annotation.", "labels": [], "entities": [{"text": "SVS", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9732841849327087}, {"text": "paraphrase assessment", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.9062592685222626}]}], "introductionContent": [{"text": "The meaning of individual occurrences or tokens of a word can change vastly according to its context.", "labels": [], "entities": []}, {"text": "A central challenge for computational lexical semantics is describe these token meanings and how they can be computed for new occurrences.", "labels": [], "entities": []}, {"text": "One prominent approach to this question is the dictionary-based model of token meaning: The different meanings of a word area set of distinct, disjoint senses enumerated in a lexicon or ontology, such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 204, "end_pos": 211, "type": "DATASET", "confidence": 0.9318105578422546}]}, {"text": "For each new occurrence, determining token meaning means choosing one of the senses, a classification task known as Word Sense Disambiguation (WSD).", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.7430403629938761}]}, {"text": "Unfortunately, this task has turned out to be very hard both for human annotators and for machines), not at least due to granularity problems with available resources).", "labels": [], "entities": []}, {"text": "Some researchers have gone so far as to suggest fundamental problems with the concept of categorical word senses).", "labels": [], "entities": []}, {"text": "An interesting alternative is offered by vector space models of word meaning) which characterize the meaning of a word entirely without reference to word senses.", "labels": [], "entities": []}, {"text": "Word meaning is described in terms of a vector in a highdimensional vector space that is constructed with distributional methods.", "labels": [], "entities": [{"text": "Word meaning", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6759753227233887}]}, {"text": "Semantic similarity is then simply distance to vectors of other words.", "labels": [], "entities": []}, {"text": "Vector space models have been most successful in modeling the meaning of word types (i.e. in constructing type vectors).", "labels": [], "entities": []}, {"text": "The characterization of token meaning by corresponding token vectors would represent a very interesting alternative to dictionary-based methods by providing a direct, graded, unsupervised measure of (dis-)similarity between words in context that completely avoids reference to dictionary senses.", "labels": [], "entities": []}, {"text": "However, there are still considerable theoretical and practical problems, even though there is a substantial body of work.", "labels": [], "entities": []}, {"text": "Ina recent paper, we have introduced the structured vector space ( SVS) model which addresses this challenge.", "labels": [], "entities": []}, {"text": "It yields one token vector per input word.", "labels": [], "entities": []}, {"text": "Token vectors are not computed by combining the lexical meaning of the surrounding wordswhich risks resulting in a \"topicality\" vector -but by modifying the type meaning of a word with the semantic expectations of syntactically related words, which can bethought of as selectional preferences.", "labels": [], "entities": []}, {"text": "For example, in catch a ball, the token vector for ball is computed by combining the type vector of ball with a vector for the selectional preferences of catch for its object.", "labels": [], "entities": []}, {"text": "The token vector for catch, conversely, is constructed from the type vector of catch and the inverse object preference vector of ball.", "labels": [], "entities": []}, {"text": "The resulting token vectors describe the meaning of a word in a particular sentence not through a sense label, but through the distance of the token vector to other vectors.", "labels": [], "entities": []}, {"text": "A natural question that arises is how vector-based models of token meaning can be evaluated.", "labels": [], "entities": []}, {"text": "It is of course possible to apply them to a traditional WSD task.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 56, "end_pos": 64, "type": "TASK", "confidence": 0.9350593388080597}]}, {"text": "However, this strategy remains vulnerable to all criticism concerning the annotation of categorical word senses, and also does not take advantage of the vector models' central asset, namely gradedness.", "labels": [], "entities": []}, {"text": "Thus, paraphrase-based assessment for models of token meaning was proposed as a representation-neutral disambiguation task that can replace WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.6405897736549377}]}, {"text": "Given a word token in context and a set of potential paraphrases, the task consists of identifying the subset of valid paraphrases.", "labels": [], "entities": []}, {"text": "In an ideal vector-based model, valid paraphrases such as (1) should possess similar vectors, and invalid ones such as (2) dissimilar ones.", "labels": [], "entities": []}, {"text": "In, we evaluated SVS on two variants of the paraphrase assessment test: first, the prediction of human judgments on a seven-point scale for paraphrases for verb-subject pairs; and second, the original Lexical Substitution task by.", "labels": [], "entities": [{"text": "SVS", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9589567184448242}]}, {"text": "To avoid overfitting, we optimized our parameters on the first dataset and evaluated only the best model on the second dataset.", "labels": [], "entities": []}, {"text": "However, given evidence for substantial inter-task differences, it is unclear to what extent these parameters are optimal beyond the Mitchell and Lapata dataset.", "labels": [], "entities": [{"text": "Mitchell and Lapata dataset", "start_pos": 133, "end_pos": 160, "type": "DATASET", "confidence": 0.6701873764395714}]}, {"text": "This paper addresses this question with two experiments: Impact of parameters.", "labels": [], "entities": []}, {"text": "We re-examine three central parameters of SVS.", "labels": [], "entities": [{"text": "SVS", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.49721160531044006}]}, {"text": "The first one is the choice of vector combination function.", "labels": [], "entities": []}, {"text": "Following, we previously used componentwise multiplication, whose interpretation in vector space is not straightforward.", "labels": [], "entities": []}, {"text": "The second one is reweighting.", "labels": [], "entities": []}, {"text": "We obtained the best performance when the context expectations were reweighted by taking each component to a (high) n-th power, which is counterintuitive.", "labels": [], "entities": []}, {"text": "Finally, we found subjects to be more informative in judging the appropriateness of paraphrases than objects.", "labels": [], "entities": []}, {"text": "This appears to contradict work in theoretical syntax (.", "labels": [], "entities": []}, {"text": "To reassess the role of these parameters, we construct a controlled dataset of transitive instances from the Lexical Substitution corpus to reexamine and investigate these issues, with the aim of providing \"best practice\" settings for SVS.", "labels": [], "entities": [{"text": "Lexical Substitution corpus", "start_pos": 109, "end_pos": 136, "type": "DATASET", "confidence": 0.6478188236554464}, {"text": "SVS", "start_pos": 235, "end_pos": 238, "type": "TASK", "confidence": 0.9326297044754028}]}, {"text": "This turns out to be more difficult than expected, leading us to suspect that a globally optimal parameter setting across tasks may simply not exist.", "labels": [], "entities": []}, {"text": "We also test a simple extension of SVS that uses a richer context (both subject and object) to construct the token vector, with first positive results.", "labels": [], "entities": []}, {"text": "The Lexical Substitution dataset used in was very small, which limits the conclusions that can be drawn from it.", "labels": [], "entities": [{"text": "Lexical Substitution dataset", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.6906270086765289}]}, {"text": "This points towards a more general problem of paraphrase-based assessment for models of token meaning: Until now, all datasets for this task were specifically created by hand.", "labels": [], "entities": []}, {"text": "It would provide a strong boost for paraphrase assessment if the large annotated corpora that are available for WSD could be reused.", "labels": [], "entities": [{"text": "paraphrase assessment", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.9718039333820343}, {"text": "WSD", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9182132482528687}]}, {"text": "We present an experiment on converting the WordNetannotated SemCor corpus into a set of \"pseudoparaphrases\" for paraphrase-based assessment.", "labels": [], "entities": [{"text": "WordNetannotated SemCor corpus", "start_pos": 43, "end_pos": 73, "type": "DATASET", "confidence": 0.9488924543062845}]}, {"text": "We use the synonyms and direct hypernyms of an annotated synset as these \"pseudo-paraphrases\".", "labels": [], "entities": []}, {"text": "While the synonyms and hypernyms are not guaranteed to work as direct replacements of the target word in the given context, they are semantically similar to the target word.", "labels": [], "entities": []}, {"text": "The result is a dataset ten times larger than the LexSub dataset.", "labels": [], "entities": [{"text": "LexSub dataset", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9957356750965118}]}, {"text": "As we describe in this paper, we find that this method is nevertheless problematic: The resulting dataset is considerably more difficult to model than the existing hand-built paraphrase corpora, and its properties differ considerably from the manually constructed Lexical Substitution dataset.", "labels": [], "entities": [{"text": "Lexical Substitution dataset", "start_pos": 264, "end_pos": 292, "type": "DATASET", "confidence": 0.6000477274258932}]}], "datasetContent": [{"text": "The task that we are considering is paraphrase assessment in context.", "labels": [], "entities": [{"text": "paraphrase assessment", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.9596506953239441}]}, {"text": "Given a predicate-argument pair and a paraphrase candidate, the models have to decide how appropriate the paraphrase is for the predicate-argument combination.", "labels": [], "entities": []}, {"text": "This is the main task against which token vector models have been evaluated in the past (.", "labels": [], "entities": []}, {"text": "In Experiment 1, we use manually created paraphrases.", "labels": [], "entities": []}, {"text": "In Experiment 2, we replaces human-generated paraphrases with \"pseudo-paraphrases\", contextually similar words that may not be completely appropriate as paraphrases in the given context, but can be collected automatically.", "labels": [], "entities": []}, {"text": "Our parameter choices for SVS are as similar as possible to the second experiment of our earlier paper.", "labels": [], "entities": [{"text": "SVS", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8984419703483582}]}, {"text": "We use a dependency-based vector space that counts a target word and a context word as co-occurring in a sentence if they are connected by an \"informative\" path in the dependency graph for the sentence.", "labels": [], "entities": []}, {"text": "We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.8402798771858215}, {"text": "Minipar", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9590545892715454}]}, {"text": "It uses raw co-occurrence counts and 2000 dimensions.", "labels": [], "entities": []}, {"text": "We use a prototype-based selectional preference model.", "labels": [], "entities": []}, {"text": "It models the selectional preferences of a predicate for an argument position as the weighted centroid of the vectors for all head words seen for this position in a large corpus.", "labels": [], "entities": []}, {"text": "Let f (a, r, b) denote the frequency of a occurring in relation r to bin the parsed BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.7739630937576294}]}, {"text": "Then, we compute the selectional preferences as: where N is the number of fillers a with f (a, r, b) > 0.", "labels": [], "entities": []}, {"text": "In Erk and Pad\u00f3, we found that applying a reweighting step to the selectional preference vector by taking each component of the centroid vector Rb (r) to the n-th power lead to substantial improvements.", "labels": [], "entities": []}, {"text": "The motivation for this technique is to alleviate noise arising from the use of unfiltered head words for the construction.", "labels": [], "entities": []}, {"text": "The reweighted selectional preference vector Rb (r) is defined as: where we write v 1 , . .", "labels": [], "entities": [{"text": "reweighted selectional preference vector Rb (r)", "start_pos": 4, "end_pos": 51, "type": "METRIC", "confidence": 0.6652913652360439}]}, {"text": ", v m for the sequence of values that makeup a vector Rb (r).", "labels": [], "entities": []}, {"text": "Inverse selectional preferences R \u22121 b (r) of nouns are defined analogously, by computing the centroid of the verbs seen as governors of the noun in relation r.", "labels": [], "entities": []}, {"text": "In this paper, we test reweighting parameters of n between 0.5 and 30.", "labels": [], "entities": []}, {"text": "Generally, small ns will decrease the influence of the selectional preference vector.", "labels": [], "entities": []}, {"text": "The result can bethought of as a \"word type vector modified by context expectations\", while large ns increase the role of context, until we arrive at a \"contextual expectation vector modified by the word type vector\".", "labels": [], "entities": []}, {"text": "We test three vector combination functions , which have different interpretations in vector space.", "labels": [], "entities": []}, {"text": "The simplest one is componentwise addition, abbreviated as add, i.e., simple vector addition.", "labels": [], "entities": []}, {"text": "With addition, context dimensions receive a high count whenever either of the two vectors has a high co-occurrence count for the context.", "labels": [], "entities": []}, {"text": "Next, we test component-wise multiplication (mult).", "labels": [], "entities": []}, {"text": "This operation is more difficult to interpret in terms of vector space, since it does not correspond to the standard inner or outer vector products.", "labels": [], "entities": []}, {"text": "The most straightforward interpretation is to reinterpret the second vector as a diagonal matrix, i.e., as a linear transformation of the first vector.", "labels": [], "entities": []}, {"text": "Large entries in the second vector increase the weight of the corresponding contexts; small entries decrease it. found this method to yield the best results.", "labels": [], "entities": []}, {"text": "The third vector combination function we consider is component-wise minimum (min).", "labels": [], "entities": []}, {"text": "This combination function results in a vector with high counts only for contexts which co-occur frequently with both input vectors and can thus be understood as an intersection between the two context sets.", "labels": [], "entities": []}, {"text": "Since the entries of two vectors need to be on the same order to magnitude for this method to yield meaningful results, we normalize vectors before the combination for min.", "labels": [], "entities": []}, {"text": "Assessing models of token meaning.", "labels": [], "entities": []}, {"text": "Given a transitive verb v with subject a and direct object b, we test three variants of computing a token vector for v.", "labels": [], "entities": []}, {"text": "The first two involve only one combination step.", "labels": [], "entities": []}, {"text": "In the subj condition, v's type vector is combined with the inverse subject preference vector of a.", "labels": [], "entities": []}, {"text": "In the obj condition, v's type vector is combined with the inverse object preference vector of b.", "labels": [], "entities": []}, {"text": "The third variant is the recursive application of the SVS combination procedure described in Section 2 (condition both).", "labels": [], "entities": [{"text": "SVS combination", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.8556261956691742}]}, {"text": "Specifically, we combine v's type vector with both a's inverse subject preference and with b's inverse object preference to obtain a \"richer\" token vector.", "labels": [], "entities": []}, {"text": "In all three cases, the resulting token vector is compared to the type vector of the paraphrase (in Experiment 1) or the semantically related word (in Experiment 2).", "labels": [], "entities": []}, {"text": "We use Cosine Similarity, a standard choice as vector space similarity measure.", "labels": [], "entities": []}, {"text": "In our 2008 paper, we tested the LexSub data only with the parameters that showed best results on the Mitchell and Lapata data: vector combination using componentwise multiplication (mult), and the computation of (inverse) selectional preference vectors with high powers of n = 20 or n = 30.", "labels": [], "entities": [{"text": "LexSub data", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9871220290660858}, {"text": "Mitchell and Lapata data", "start_pos": 102, "end_pos": 126, "type": "DATASET", "confidence": 0.6788061037659645}]}, {"text": "However, there were indications that the two datasets showed fundamental differences.", "labels": [], "entities": []}, {"text": "In particular, the Mitchell and Lapata data could only be modeled using a PMI-transformed vector space, while the LexSub data could only be modeled using raw cooccurrence count vectors.", "labels": [], "entities": [{"text": "LexSub data", "start_pos": 114, "end_pos": 125, "type": "DATASET", "confidence": 0.9865963160991669}]}, {"text": "Another one of our findings that warrants further inquiry stems from our comparison of different context choices (verb plus subject, verb plus object, noun plus embedding verb).", "labels": [], "entities": []}, {"text": "We found that subjects are better disambiguators than objects.", "labels": [], "entities": []}, {"text": "This seems counterintuitive both on theoretical and empirical grounds.", "labels": [], "entities": []}, {"text": "Theoretically,  With a size of 2,000 sentences, even the complete LexSub dataset is tiny in comparison to many other resources in NLP.", "labels": [], "entities": [{"text": "LexSub dataset", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9845153391361237}]}, {"text": "Limiting attention to successfully parsed transitive instances results in an even smaller dataset on which it is difficult to distinguish noise from genuine differences between models.", "labels": [], "entities": []}, {"text": "This is a large problem for the use of paraphrase appropriateness as evaluation task for models of word meaning in context.", "labels": [], "entities": []}, {"text": "In consequence, the automatic creation of larger datasets is an important task.", "labels": [], "entities": []}, {"text": "While unsupervised methods for paraphrase induction are becoming available (e.g., Callison-Burch (2008)), they are still so noisy that the created datasets cannot serve as gold standards.", "labels": [], "entities": [{"text": "paraphrase induction", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.9584290087223053}]}, {"text": "However, there is an alternative strategy: there is a considerable amount of data in different languages annotated with categorical word sense, created (e.g.) for Word Sense Disambiguation exercises such as Senseval.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 163, "end_pos": 188, "type": "TASK", "confidence": 0.6311120887597402}]}, {"text": "We suggest to convert these data for use in a task similar to paraphrase assessment, interpreting available information about the word sense as pseudo-paraphrases.", "labels": [], "entities": [{"text": "paraphrase assessment", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.9621100723743439}]}, {"text": "Of course, the caveat is that these pseudo-paraphrases may behave differently than genuine paraphrases.", "labels": [], "entities": []}, {"text": "To investigate this issue, we repeat Experiment 1 on this dataset.", "labels": [], "entities": []}, {"text": "Construction of the SEMCOR-PARA dataset The SemCor corpus is a subset of the Brown corpus that contains 23,346 lemmas annotated with senses according to WordNet 1.6.", "labels": [], "entities": [{"text": "SEMCOR-PARA dataset", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.8765016496181488}, {"text": "SemCor corpus", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.6675543934106827}, {"text": "Brown corpus", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.8099774420261383}, {"text": "WordNet 1.6", "start_pos": 153, "end_pos": 164, "type": "DATASET", "confidence": 0.9259332120418549}]}, {"text": "Fortunately, WordNet provides a rich characterization of word senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9381017088890076}]}, {"text": "This allows us to use the WordNet synonyms of a given word sense as pseudo-paraphrases.", "labels": [], "entities": []}, {"text": "Since it can be the case that the target word is the only word in a synset, we also, an indicator that they are usually close enough in meaning to function as pseudo-paraphrases.", "labels": [], "entities": []}, {"text": "Again, we parsed the corpus with Minipar and identified all sense-tagged instances of the verbs from LEXSUB-PARA, to keep the two corpora as comparable as possible.", "labels": [], "entities": [{"text": "LEXSUB-PARA", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.934798002243042}]}, {"text": "For each instance w i of word w, we collected all synonyms and direct hypernyms of the synset as the set of appropriate paraphrases.", "labels": [], "entities": []}, {"text": "The list of synonyms and direct hypernyms of all other senses of w, whether they occur in SemCor or not, were considered inappropriate paraphrases for the instance w i . This method does not provide us with frequencies for the pseudo-paraphrases; we thus assumed a uniform frequency of 1.", "labels": [], "entities": []}, {"text": "This does not do away with the gradedness of the meaning representation, though, since each token is still associated with a set of appropriate paraphrases.", "labels": [], "entities": []}, {"text": "Out of 2242 transitive verb instances, we further removed 153 since we could not compute selectional preferences for at least one of the fillers.", "labels": [], "entities": []}, {"text": "484 instances were removed because WordNet did not list any verbal paraphrases for the annotated synset or its direct hypernym.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9526861310005188}]}, {"text": "This resulted in 1605 instances for 40 verbs, a dataset an order of magnitude larger than LEXSUB-PARA.", "labels": [], "entities": [{"text": "LEXSUB-PARA", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.935903012752533}]}, {"text": "(See Section 4.3 for an example verb with paraphrases.)", "labels": [], "entities": []}, {"text": "We again use the OOT accuracy measure.", "labels": [], "entities": [{"text": "OOT", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9771363139152527}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.8735700249671936}]}, {"text": "The results for paraphrase assessment on SEMCOR-PARA are shown in.", "labels": [], "entities": [{"text": "paraphrase assessment", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.9502175748348236}, {"text": "SEMCOR-PARA", "start_pos": 41, "end_pos": 52, "type": "DATASET", "confidence": 0.7984200119972229}]}, {"text": "The numbers are substantially lower than for LEXSUB-PARA.", "labels": [], "entities": [{"text": "LEXSUB-PARA", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.5461664795875549}]}, {"text": "This is first and foremost a consequence of the higher \"polysemy\" of the pseudo-paraphrases.", "labels": [], "entities": []}, {"text": "In LEXSUB-PARA, the average numbers of possible paraphrases per target word is 20; in SEMCOR-PARA, 54.", "labels": [], "entities": [{"text": "LEXSUB-PARA", "start_pos": 3, "end_pos": 14, "type": "METRIC", "confidence": 0.8108308911323547}, {"text": "SEMCOR-PARA", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.7398885488510132}]}, {"text": "This is to be expected and also reflected in the much lower random baseline (19.6% OOT).", "labels": [], "entities": [{"text": "random baseline", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.9425033926963806}, {"text": "OOT", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9951643943786621}]}, {"text": "However, we also observe that the reduction in error rate over the baseline is considerably lower for SEMCOR-PARA than for LEXSUB-PARA (10% vs. 20% reduction).", "labels": [], "entities": [{"text": "error rate", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9688805043697357}, {"text": "SEMCOR-PARA", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.6909559965133667}, {"text": "LEXSUB-PARA", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.9172317385673523}]}, {"text": "Among the parameters of the model, we find the largest impact for the reweighting parameter.", "labels": [], "entities": [{"text": "reweighting", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.9131160378456116}]}, {"text": "The best results occur in the middle range(n = 2 and n = 5), with both lower and higher weights yielding considerably lower scores.", "labels": [], "entities": []}, {"text": "Apparently, it is more difficult to strike the right balance between the target and the expectations on this dataset.", "labels": [], "entities": []}, {"text": "This is also mirrored in the smaller improvement of the target type vector baseline over the random baseline.", "labels": [], "entities": []}, {"text": "As for vector combination functions, we find the best results for the more \"intersection\"-like mult and min combinations, with somewhat lower results for add; however, the differences are rather small.", "labels": [], "entities": []}, {"text": "Finally, combination with obj works better than combination with subj.", "labels": [], "entities": []}, {"text": "At least among the best results, both is able to improve over the use of either individual relation.", "labels": [], "entities": []}, {"text": "The best result uses mult-both, with an OOT accuracy of 25.6.", "labels": [], "entities": [{"text": "OOT", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9984493255615234}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.8133354783058167}]}], "tableCaptions": [{"text": " Table 1: OOT accuracy on the LEXSUB-PARA dataset  across models and reweighting values (best results for  each model boldfaced). Random baseline: 53.7. Target  type vector baseline: 57.1. Pr : Numerical problem.", "labels": [], "entities": [{"text": "OOT", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9916986227035522}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9371633529663086}, {"text": "LEXSUB-PARA dataset", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.7813036143779755}, {"text": "Random baseline", "start_pos": 130, "end_pos": 145, "type": "METRIC", "confidence": 0.9687248170375824}, {"text": "Pr", "start_pos": 189, "end_pos": 191, "type": "METRIC", "confidence": 0.9560719728469849}]}, {"text": " Table 2: OOT accuracy on the SEMCOR-PARA dataset  across models and reweighting values (best results for  each line boldfaced). Random baseline: 19.6. Target  type vector baseline: 20.8", "labels": [], "entities": [{"text": "OOT", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9914174675941467}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.948761522769928}, {"text": "SEMCOR-PARA dataset", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.8824522197246552}, {"text": "Random baseline", "start_pos": 129, "end_pos": 144, "type": "METRIC", "confidence": 0.96107217669487}]}]}