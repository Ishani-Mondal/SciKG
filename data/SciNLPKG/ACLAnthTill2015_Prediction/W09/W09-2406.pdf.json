{"title": [{"text": "Large-scale Semantic Networks: Annotation and Evaluation", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.5649213194847107}]}], "abstractContent": [{"text": "We introduce a large-scale semantic-network annotation effort based on the MutliNet formalism.", "labels": [], "entities": []}, {"text": "Annotation is achieved via a process which incorporates several independent tools including a MultiNet graph editing tool, a semantic concept lexicon, a user-editable knowledge-base for semantic concepts, and a MultiNet parser.", "labels": [], "entities": [{"text": "MultiNet graph editing", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.5865765412648519}]}, {"text": "We present an evaluation metric for these semantic networks, allowing us to determine the quality of annotations in terms of inter-annotator agreement.", "labels": [], "entities": []}, {"text": "We use this metric to report the agreement rates fora pilot annotation effort involving three annota-tors.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we propose an annotation framework which integrates the MultiNet semantic network formalism) and the syntacticosemantic formalism of the Prague Dependency Treebank) (PDT).", "labels": [], "entities": [{"text": "Prague Dependency Treebank) (PDT)", "start_pos": 151, "end_pos": 184, "type": "DATASET", "confidence": 0.9455778769084385}]}, {"text": "The primary goal of this task is to increase the interoperability of these two frameworks in order to facilitate efforts to annotate at the semantic level while preserving intrasentential semantic and syntactic annotations as are found in the PDT.", "labels": [], "entities": []}, {"text": "The task of annotating text with global semantic interactions (e.g., semantic interactions within some discourse) presents a cognitively demanding problem.", "labels": [], "entities": []}, {"text": "As with many other annotation formalisms, * Part of this work was completed while at the Johns Hopkins University Center for Language and Speech Processing in Baltimore, MD USA.", "labels": [], "entities": [{"text": "Language and Speech Processing", "start_pos": 125, "end_pos": 155, "type": "TASK", "confidence": 0.5717506408691406}]}, {"text": "we propose a technique that builds from cognitively simpler tasks such as syntactic and semantic annotations at the sentence level including rich morphological analysis.", "labels": [], "entities": []}, {"text": "Rather than constraining the semantic representations to those compatible with the sentential annotations, our procedure provides the syntacitco-semantic tree as a reference; the annotators are free to select nodes from this tree to create nodes in the network.", "labels": [], "entities": []}, {"text": "We do not attempt to measure the influence this procedure has on the types of semantic networks generated.", "labels": [], "entities": []}, {"text": "We believe that using a soft-constraint such as the syntactico-semantic tree, allows us to better generate human labeled semantic networks with links to the interpretations of the individual sentence analyses.", "labels": [], "entities": []}, {"text": "In this paper, we present a procedure for computing the annotator agreement rate for MultiNet graphs.", "labels": [], "entities": [{"text": "annotator agreement rate", "start_pos": 56, "end_pos": 80, "type": "METRIC", "confidence": 0.7103333175182343}]}, {"text": "Note that a MultiNet graph does not represent the same semantics as a syntactico-semantic dependency tree.", "labels": [], "entities": []}, {"text": "The nodes of the MultiNet graph are connected based on a corpus-wide interpretation of the entities referred to in the corpus.", "labels": [], "entities": []}, {"text": "These global connections are determined by the intra-sentential interpretation but are not restricted to that interpretation.", "labels": [], "entities": []}, {"text": "Therefore, the procedure for computing annotator agreement differs from the standard approaches to evaluating syntactic and semantic dependency treebanks (e.g., dependency link agreement, label agreement, predicate-argument structure agreement).", "labels": [], "entities": []}, {"text": "As noted in, \"Even though the design of annotation schemes has been initiated for single semantic phenomena, there exists no annotation scheme (as far as I know) that aims to inte-grate a wide range of semantic phenomena all at once.", "labels": [], "entities": []}, {"text": "It would be welcome to have such a resource at ones disposal, and ideally a semantic annotation scheme should be multi-layered, where certain semantic phenomena can be properly analysed or left simply unanalysed.\"", "labels": [], "entities": []}, {"text": "In Section 1 we introduce the theoretical background of the frameworks on which our annotation tool is based: MultiNet and the Tectogrammatical Representation (TR) of the PDT.", "labels": [], "entities": []}, {"text": "Section 2 describes the annotation process in detail, including an introduction to the encyclopedic tools available to the annotators.", "labels": [], "entities": []}, {"text": "In Section 3 we present an evaluation metric for MultiNet/TR labeled data.", "labels": [], "entities": [{"text": "MultiNet/TR labeled data", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.49109597206115724}]}, {"text": "We also present an evaluation of the data we have had annotated using the proposed procedure.", "labels": [], "entities": []}, {"text": "Finally, we conclude with a short discussion of the problems observed during the annotation process and suggest improvements as future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street Journal (covering those annotated at the syntactic level in the Penn Treebank ().", "labels": [], "entities": [{"text": "The Wall Street Journal", "start_pos": 110, "end_pos": 133, "type": "DATASET", "confidence": 0.7687138989567757}, {"text": "Penn Treebank", "start_pos": 190, "end_pos": 203, "type": "DATASET", "confidence": 0.9816624522209167}]}, {"text": "We use the annotation from the Prague Czech-English Dependency Treebank), which contains a large portion of the WSJ Treebank annotated according to the PDT annotation scheme (including all layers of the FGD formalism).", "labels": [], "entities": [{"text": "Prague Czech-English Dependency Treebank", "start_pos": 31, "end_pos": 71, "type": "DATASET", "confidence": 0.8762046545743942}, {"text": "WSJ Treebank", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.977963775396347}, {"text": "FGD", "start_pos": 203, "end_pos": 206, "type": "DATASET", "confidence": 0.9362528324127197}]}, {"text": "We reserved a small set of data to be used to train our annotators and have excluded these articles from the evaluation.", "labels": [], "entities": []}, {"text": "Three native English-speaking annotators were trained and then asked to annotate sentences from the corpus.", "labels": [], "entities": []}, {"text": "We have a sample of 67 sentences (1793 words) annotated by two of the annotators; of those, 46 sentences (1236 words) were annotated by three annotators.", "labels": [], "entities": []}, {"text": "2 Agreement is measured for each individual sentences in two steps.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9985002279281616}]}, {"text": "First, the best match between the two annotators' graphs is found and then the F-measure is computed.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.987675666809082}]}, {"text": "In order to determine the optimal graph match between two graphs, we make use of the fact that the annotators have the tectogrammatical tree from which they can select nodes as concepts in the MultiNet graph.", "labels": [], "entities": []}, {"text": "Many of the nodes in the annotated graphs remain linked to the tectogrammatical tree, therefore we have a unique identifier for these nodes.", "labels": [], "entities": []}, {"text": "When matching the nodes of two different annotations, we assume anode represents an identical concept if both annotators linked the node to the same tectogrammatical node.", "labels": [], "entities": []}, {"text": "For the remaining nodes, we consider all possible one-to-one mappings and construct the optimal mapping with respect to the Fmeasure.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.5475883483886719}]}, {"text": "Formally, we start with a set of tectogrammatical trees containing a set of nodes N . The annotation is a tuple G = (V, E, T, A), where V are the vertices, E \u2286 V \u00d7 V \u00d7 P are the directed edges and their labels (e.g., agent of an action: AGT \u2208 P ), T \u2286 V \u00d7N is the mapping from vertices to the tectogrammatical nodes, and finally A are attributes of the nodes, which we ignore in this initial evaluation.", "labels": [], "entities": []}, {"text": "Analogously, G = (V , E , T , A ) is another annotation of the same sentence and our goal is to measure the similarity s(G, G ) \u2208 [0, 1] of G and G . To measure the similarity we need a set \u03a6 of admissible one-to-one mappings between vertices in the two annotations.", "labels": [], "entities": []}, {"text": "A mapping is admissible if it connects vertices which are indicated by the annotators as representing the same tectogrammatical node: In Equation 1, the first condition ensures that \u03a6 is constrained by the mapping induced by the links to the tectogrammatical layer.", "labels": [], "entities": []}, {"text": "The remaining two conditions guarantee that \u03a6 is a one-to-one mapping.", "labels": [], "entities": []}, {"text": "We define the annotation agreement s as: where F is the F1-measure: |E| + |E | where m(\u03c6) is the number of edges that match given the mapping \u03c6.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9960410594940186}]}, {"text": "We use four versions of m, which gives us four versions of F and consequently four scores s for every sentence: These four m(\u03c6) functions give us four possible Fm measures, which allows us to have four scores for every sentence: s du , s uu , s dl and s ul . shows that the inter-annotator agreement is not significantly correlated with the position of the sentence in the annotation process.", "labels": [], "entities": []}, {"text": "This suggests that the annotations for each annotator had achieved a stable point (primarily due to the annotator training process).", "labels": [], "entities": []}, {"text": "sentences are not more difficult than short sentences.", "labels": [], "entities": []}, {"text": "The variance decreases with the sentence length as expected.", "labels": [], "entities": [{"text": "variance", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9435688257217407}]}, {"text": "In we show the comparison of directed and labeled evaluations with the undirected unlabeled case.", "labels": [], "entities": []}, {"text": "By definition the undirected unlabeled score is the upper bound for all the other scores.", "labels": [], "entities": []}, {"text": "The directed score is well correlated and not very different from the undirected score, indicating that the annotators did not have much trouble with determining the correct direction of the edges.", "labels": [], "entities": []}, {"text": "This might be, in part, due to support from the formalism and its tool cedit: each relation type is specified by a semantic-concept type signature; a relation that violates its signature is reported immediately to the annotator.", "labels": [], "entities": []}, {"text": "On the other hand, labeled score is significantly lower than the unlabeled score, which suggests that the annotators have difficulties in assigning the correct relation types.", "labels": [], "entities": [{"text": "labeled score", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.8551542162895203}]}, {"text": "The correlation coefficient between s uu and s ul (approx. 0.75) is also much lower than than the correlation coefficient between s uu and s du (approx. 0.95).", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9741642773151398}]}, {"text": "A more detailed comparison of individual annotator pairs is depicted in.", "labels": [], "entities": []}, {"text": "The graph shows that there is a significant positive correlation between scores, i.e. if two annotators can agree on the", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement in percents. The re- sults come from the two samples described in the first  paragraph of Section 3.", "labels": [], "entities": [{"text": "re- sults", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9429545402526855}]}]}