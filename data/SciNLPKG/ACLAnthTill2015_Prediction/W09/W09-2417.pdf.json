{"title": [{"text": "SemEval-2010 Task 10: Linking Events and Their Participants in Discourse", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe the SemEval-2010 shared task on \"Linking Events and Their Participants in Discourse\".", "labels": [], "entities": [{"text": "SemEval-2010 shared task", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8415766358375549}, {"text": "Linking Events and Their Participants in Discourse", "start_pos": 60, "end_pos": 110, "type": "TASK", "confidence": 0.8307414054870605}]}, {"text": "This task is a variant of the classical semantic role labelling task.", "labels": [], "entities": [{"text": "semantic role labelling task", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.7254546359181404}]}, {"text": "The novel aspect is that we focus on linking local semantic argument structures across sentence boundaries.", "labels": [], "entities": []}, {"text": "Specifically, the task aims at linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist).", "labels": [], "entities": []}, {"text": "This task is potentially beneficial fora number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction.", "labels": [], "entities": [{"text": "semantic role labelling community", "start_pos": 131, "end_pos": 164, "type": "TASK", "confidence": 0.6981135606765747}, {"text": "co-reference resolution", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.7302846163511276}, {"text": "information extraction", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.8265627324581146}]}], "introductionContent": [{"text": "Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (.", "labels": [], "entities": [{"text": "Semantic role labelling (SRL)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8656881352265676}]}, {"text": "Semantic roles describe the function of the participants in an event.", "labels": [], "entities": []}, {"text": "Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc.", "labels": [], "entities": []}, {"text": "SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval, and CoNLL (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.531825065612793}]}, {"text": "The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures can lead to tangible performance gains in NLP applications such as information extraction (, question answering or recognising textual entailment).", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.650266170501709}, {"text": "information extraction", "start_pos": 233, "end_pos": 255, "type": "TASK", "confidence": 0.7833362221717834}, {"text": "question answering", "start_pos": 259, "end_pos": 277, "type": "TASK", "confidence": 0.8305439352989197}]}, {"text": "However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.668102482954661}, {"text": "text understanding", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.7470256686210632}, {"text": "information extraction", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.8112128973007202}, {"text": "summarisation", "start_pos": 230, "end_pos": 243, "type": "TASK", "confidence": 0.9772909283638}, {"text": "question answering", "start_pos": 248, "end_pos": 266, "type": "TASK", "confidence": 0.8513387441635132}]}, {"text": "The reason for this is that SRL has traditionally been viewed as a sentence-internal task.", "labels": [], "entities": [{"text": "SRL", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9920719265937805}]}, {"text": "Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic information.", "labels": [], "entities": []}, {"text": "This view of SRL as a sentence-internal task is partly due to the fact that large-scale manual annotation projects such as FrameNet and PropBank 2 typically present their annotations lexicographically by lemma rather than by source text.", "labels": [], "entities": [{"text": "SRL", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9891391396522522}]}, {"text": "Furthermore, in the case of FrameNet, the annotation effort did not start outwith the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from a very large corpus, which did not allow fora view of the data as 'full-text annotation'.", "labels": [], "entities": []}, {"text": "It is clear that there is an interplay between local argument structure and the surrounding discourse.", "labels": [], "entities": []}, {"text": "In early work, discussed filling null complements from context by using knowledge about individual predicates and ten-dencies of referential chaining across sentences.", "labels": [], "entities": [{"text": "filling null complements from context", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8300498962402344}, {"text": "referential chaining across sentences", "start_pos": 129, "end_pos": 166, "type": "TASK", "confidence": 0.7911533415317535}]}, {"text": "But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semantic relations between the predicates involved.", "labels": [], "entities": []}, {"text": "Two notable exceptions are and.", "labels": [], "entities": []}, {"text": "analyse a short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions of how to model this.", "labels": [], "entities": []}, {"text": "provide a detailed analysis of the links between the local semantic argument structures in a short text; however their system is not fully implemented either.", "labels": [], "entities": []}, {"text": "In the shared task, we intend to make a first step towards taking SRL beyond the domain of individual sentences by linking local semantic argument structures to the wider discourse context.", "labels": [], "entities": [{"text": "SRL", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9930723309516907}]}, {"text": "In particular, we address the problem of finding fillers for roles which are neither instantiated as direct dependents of our target predicates nor displaced through long-distance dependency or coinstantatiation constructions.", "labels": [], "entities": []}, {"text": "Often a referent for an uninstantiated role can be found in the wider context, i.e. in preceding or following sentences.", "labels": [], "entities": []}, {"text": "An example is given in (1), where the CHARGES role (ARG2 in PropBank) of cleared is left empty but can be linked to murder in the previous sentence.", "labels": [], "entities": [{"text": "CHARGES", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.8452916741371155}, {"text": "ARG2", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.5851201415061951}]}, {"text": "(1) Ina lengthy court case the defendant was tried for murder.", "labels": [], "entities": []}, {"text": "In the end, he was cleared.", "labels": [], "entities": []}, {"text": "Another very rich example is provided by (2), where, for instance, the experiencer and the object of jealousy are not overtly expressed as syntactic dependents of the noun jealousy but can be inferred to be Watson and the speaker, Holmes, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned above we allow participants to address either the full role recognition and labelling task plus the linking of null instantiations or to make use of the gold standard semantic argument structure and look only at the null instantiations.", "labels": [], "entities": [{"text": "role recognition and labelling task", "start_pos": 68, "end_pos": 103, "type": "TASK", "confidence": 0.7534549176692963}]}, {"text": "We also permit systems to perform either FrameNet or PropBank style SRL.", "labels": [], "entities": []}, {"text": "Hence, systems can be entered for four subtasks which will be evaluated separately: \u2022 full task, FrameNet \u2022 null instantiations, FrameNet \u2022 full task, PropBank \u2022 null instantiations, PropBank The focus for the proposed task is on the null instantiation linking, however, for completeness, we also evaluate the standard SRL task.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 183, "end_pos": 191, "type": "DATASET", "confidence": 0.9332451224327087}]}, {"text": "For role recognition and labelling we use a standard evaluation set-up, i.e., for role recognition we will evaluate the accuracy with respect to the manually created gold standard, for role labelling we will evaluate precision, recall, and F-Score.", "labels": [], "entities": [{"text": "role recognition", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.902065247297287}, {"text": "role recognition", "start_pos": 82, "end_pos": 98, "type": "TASK", "confidence": 0.8738569021224976}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9991576671600342}, {"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9996864795684814}, {"text": "recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9995181560516357}, {"text": "F-Score", "start_pos": 240, "end_pos": 247, "type": "METRIC", "confidence": 0.997700035572052}]}, {"text": "The null instantiation linkings are evaluated slightly differently.", "labels": [], "entities": []}, {"text": "In the gold standard, we will identify referents for null instantiations in the discourse context.", "labels": [], "entities": []}, {"text": "In some cases, more than one referent might be appropriate, e.g., because the omitted argument refers to an entity that is mentioned multiple times in the context.", "labels": [], "entities": []}, {"text": "In this case, a system should be given credit if the null instantiation is linked to any of these expressions.", "labels": [], "entities": []}, {"text": "To achieve this we create equivalence sets for the referents of null instantiations.", "labels": [], "entities": []}, {"text": "If the null instantiation is linked to any item in the equivalence set, the link is counted as a true positive.", "labels": [], "entities": []}, {"text": "We can then define NI linking precision as the number of all true positive links divided by the number of links made by a system, and NI linking recall as the number of true positive links divided by the number of links between a null instantiation and its equivalence set in the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.5260180830955505}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.6310605406761169}]}, {"text": "NI linking F-Score is then the harmonic mean between NI linking precision and recall.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.47211673855781555}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.7360415458679199}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9966481328010559}]}, {"text": "Since it may sometimes be difficult to determine the correct extend of the filler of an NI, we score an automatic annotation as correct if it includes the head of the gold standard filler in the predicted filler.", "labels": [], "entities": []}, {"text": "However, in order to not favour systems which link NIs to excessively large spans of text to maximise the likelihood of linking to a correct referent, we introduce a second evaluation measure, which computes the overlap (Dice coefficient) between the words in the predicted filler (P) of a null instantiation and the words in the gold standard one (G): Example illustrates this point.", "labels": [], "entities": [{"text": "Dice coefficient)", "start_pos": 221, "end_pos": 238, "type": "METRIC", "confidence": 0.9793490767478943}]}, {"text": "The verb won in the second sentence evokes the Finish competition frame whose COMPETITION role is null instantiated.", "labels": [], "entities": []}, {"text": "From the context it is clear that the competition role is semantically filled by their first TV debate (head: debate) and last night's debate (head: debate) in the previous sentences.", "labels": [], "entities": []}, {"text": "These two expressions makeup the equivalence set for the COMPETITION role in the last sentence.", "labels": [], "entities": []}, {"text": "Any system that would predict a linkage to a filler that covers the head of either of these two expressions would score a true positive for this NI.", "labels": [], "entities": []}, {"text": "However, a system that linked to last night's debate would have an NI linking overlap of 1 (i.e., 2*3/(3+3)) while a system linking the whole second sentence Last night's debate was eagerly anticipated to the NI would have an NI linking overlap of 0.67 (i.e., 2*3/(6+3))", "labels": [], "entities": [{"text": "NI linking overlap", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.9234490791956583}, {"text": "NI", "start_pos": 209, "end_pos": 211, "type": "DATASET", "confidence": 0.9074112772941589}, {"text": "NI linking overlap", "start_pos": 226, "end_pos": 244, "type": "METRIC", "confidence": 0.7871995568275452}]}], "tableCaptions": []}