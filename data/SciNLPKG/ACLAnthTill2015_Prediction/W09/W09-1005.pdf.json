{"title": [{"text": "GREAT: a finite-state machine translation toolkit implementing a Grammatical Inference Approach for Transducer Inference (GIATI)", "labels": [], "entities": [{"text": "GREAT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.4711840748786926}, {"text": "finite-state machine translation", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.7212299108505249}, {"text": "Transducer Inference (GIATI)", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.8147603154182435}]}], "abstractContent": [{"text": "GREAT is a finite-state toolkit which is devoted to Machine Translation and that learns structured models from bilingual data.", "labels": [], "entities": [{"text": "GREAT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6493449211120605}, {"text": "Machine Translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8370038568973541}]}, {"text": "The training procedure is based on grammatical inference techniques to obtain stochastic transducers that model both the structure of the languages and the relationship between them.", "labels": [], "entities": []}, {"text": "The inference of grammars from natural language causes the models to become larger when a less restrictive task is involved; even more if a bilingual modelling is being considered.", "labels": [], "entities": []}, {"text": "GREAT has been successful to implement the GIATI learning methodology, using different scalability issues to be able to deal with corpora of high volume of data.", "labels": [], "entities": [{"text": "GREAT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9460687637329102}]}, {"text": "This is reported with experiments on the EuroParl corpus, which is a state-of-the-art task in Statistical Machine Translation.", "labels": [], "entities": [{"text": "EuroParl corpus", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9940339922904968}, {"text": "Statistical Machine Translation", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.8608572284380595}]}], "introductionContent": [{"text": "Over the last years, grammatical inference techniques have not been widely employed in the machine translation area.", "labels": [], "entities": [{"text": "grammatical inference", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8465389311313629}, {"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7598709464073181}]}, {"text": "Nevertheless, it is not unknown that researchers are trying to include some structured information into their models in order to capture the grammatical regularities that there are in languages together with their own relationship.", "labels": [], "entities": []}, {"text": "GIATI) is a grammatical inference methodology to infer stochastic transducers in a bilingual modelling approach for statistical machine translation.", "labels": [], "entities": [{"text": "GIATI)", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7174696028232574}, {"text": "statistical machine translation", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.7081820964813232}]}, {"text": "From a statistical point of view, the translation problem can be stated as follows: given a source sentence s = s 1 . .", "labels": [], "entities": [{"text": "translation", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9849495887756348}]}, {"text": "s J , the goal is to find a target sentenc\u00ea t = t 1 . .", "labels": [], "entities": []}, {"text": "t \u02c6 I , among all possible target strings t, that maximises the posterior probability: The conditional probability Pr(t|s) can be replaced by a joint probability distribution Pr(s, t) which is modelled by a stochastic transducer being inferred through the GIATI methodology): This paper describes GREAT, a software package for bilingual modelling from parallel corpus.", "labels": [], "entities": []}, {"text": "GREAT is a finite-state toolkit which was born to overcome the computational problems that previous implementations of GIATI ( had in practice when huge amounts of data were used.", "labels": [], "entities": [{"text": "GREAT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7941807508468628}]}, {"text": "Even more, GREAT is the result of a very meticulous study of GIATI models, which improves the treatment of smoothing transitions in decoding time, and that also reduces the required time to translate an input sentence by means of an analysis that will depend on the granularity of the symbols.", "labels": [], "entities": [{"text": "GREAT", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.6692916750907898}]}, {"text": "Experiments fora state-of-the-art, voluminous translation task, such as the EuroParl, are reported.", "labels": [], "entities": [{"text": "translation task", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.871859073638916}, {"text": "EuroParl", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9887059926986694}]}, {"text": "In (, the so called phrase-based finite-state transducers were concluded to be a better modelling option for this task than the ones that derive from a wordbased approach.", "labels": [], "entities": []}, {"text": "That is why the experiments here are exclusively related to this particular kind of GIATI-based transducers.", "labels": [], "entities": []}], "datasetContent": [{"text": "GREAT has been successfully employed to work with the French-English EuroParl corpus, that is, the benchmark corpus of the NAACL 2006 shared task of the Workshop on Machine Translation of the Association for Computational Linguistics.", "labels": [], "entities": [{"text": "GREAT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6316351294517517}, {"text": "French-English EuroParl corpus", "start_pos": 54, "end_pos": 84, "type": "DATASET", "confidence": 0.6885276834170023}, {"text": "NAACL 2006 shared task of the Workshop on Machine Translation", "start_pos": 123, "end_pos": 184, "type": "TASK", "confidence": 0.6483188688755035}]}, {"text": "The corpus characteristics can be seen in.", "labels": [], "entities": []}, {"text": "The EuroParl corpus is built on the proceedings of the European Parliament, which are published on its web and are freely available.", "labels": [], "entities": [{"text": "EuroParl corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9867210686206818}]}, {"text": "Because of its nature, this corpus has a large variability and complexity, since the translations into the different official languages are performed by groups of human translators.", "labels": [], "entities": []}, {"text": "The fact that not all translators agree in their translation criteria implies that a given source sentence can be translated in various different ways throughout the corpus.", "labels": [], "entities": []}, {"text": "Since the proceedings are not available in every language as a whole, a different subset of the corpus is extracted for every different language pair, thus evolving into somewhat a different corpus for each pair of languages.", "labels": [], "entities": []}, {"text": "We evaluated the performance of our methods by using the following evaluation measures: This indicator computes the precision of unigrams, bigrams, trigrams, and tetragrams with respect to a set of reference translations, with a penalty for too short sentences).", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9978188276290894}]}, {"text": "BLEU measures accuracy, not error rate.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9861189126968384}, {"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995195865631104}, {"text": "error rate", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9758733212947845}]}, {"text": "WER (Word Error Rate): The WER criterion calculates the minimum number of editions (substitutions, insertions or deletions) that are needed to convert the system hypothesis into the sentence considered ground truth.", "labels": [], "entities": [{"text": "WER (Word Error Rate)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.7294863512118658}, {"text": "WER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9310295581817627}]}, {"text": "Because of its nature, this measure is very pessimistic.", "labels": [], "entities": []}, {"text": "It refers to the average time (in milliseconds) to translate one word from the test corpus, without considering loading times.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Characteristics of the Fr-En EuroParl.", "labels": [], "entities": [{"text": "Fr-En EuroParl", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.8962911665439606}]}, {"text": " Table 2: Results for the two smoothing criteria.", "labels": [], "entities": []}, {"text": " Table 3: Results for a word-based analysis.", "labels": [], "entities": []}, {"text": " Table 4: Results for a phrase-based analysis.", "labels": [], "entities": []}, {"text": " Table 5: Number of trained and survived n-grams.", "labels": [], "entities": []}, {"text": " Table 6: Decoding time for several windows sizes.", "labels": [], "entities": []}]}