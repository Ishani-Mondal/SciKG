{"title": [{"text": "Biomedical Event Annotation with CRFs and Precision Grammars", "labels": [], "entities": [{"text": "Biomedical Event Annotation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7364329099655151}]}], "abstractContent": [{"text": "This work describes a system for the tasks of identifying events in biomedical text and marking those that are speculative or negated.", "labels": [], "entities": []}, {"text": "The architecture of the system relies on both Machine Learning (ML) approaches and hand-coded precision grammars.", "labels": [], "entities": []}, {"text": "We submitted the output of our approach to the event extraction shared task at BioNLP 2009, where our methods suffered from low recall, although we were one of the few teams to provide answers for task 3.", "labels": [], "entities": [{"text": "event extraction shared task", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.8575953841209412}, {"text": "BioNLP 2009", "start_pos": 79, "end_pos": 90, "type": "DATASET", "confidence": 0.7889254093170166}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9992293119430542}]}], "introductionContent": [{"text": "We present in this paper our techniques for the tasks 1 and 3 of the event extraction shared task at BioNLP 2009.", "labels": [], "entities": [{"text": "event extraction shared task", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.8448742777109146}, {"text": "BioNLP 2009", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.7278786897659302}]}, {"text": "We make use of both Machine Learning (ML) approaches and hand-coded precision grammars in an architecture that combines multiple dedicated modules.", "labels": [], "entities": []}, {"text": "In the third task on negation/speculation, we extract extract rich linguistic features resulting from our HPSG high-precision grammar to train an ML classifier.", "labels": [], "entities": [{"text": "negation/speculation", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.9523380200068156}, {"text": "ML classifier", "start_pos": 146, "end_pos": 159, "type": "TASK", "confidence": 0.7976798117160797}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Trigger-word detection performance over train- ing data. Results for the look-up tagger and CRFs with  the full feature set and when removing one feature type  at a time, for 3 and 4 word windows. The best results per  column are shown in bold.", "labels": [], "entities": [{"text": "Trigger-word detection", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.955621212720871}]}, {"text": " Table 2: Performance of selected feature and window- size combinations over development data. Best results  per column are given in bold.", "labels": [], "entities": []}, {"text": " Table 4: Results (exact match) over development data  for task 3 using gold-standard event/trigger annotations  and selected other annotations for task 1. Feature sets  described in", "labels": [], "entities": []}, {"text": " Table 5: Task 1 results with approximate span matching,  recursive evaluation (our final submission is in bold)", "labels": [], "entities": []}, {"text": " Table 6: Results for the different events from our com- bined system. Averaged scores for single events, regula- tions, and all.", "labels": [], "entities": [{"text": "Averaged scores", "start_pos": 71, "end_pos": 86, "type": "METRIC", "confidence": 0.9559910595417023}]}, {"text": " Table 7: Results over test data for task 3 using gold- standard event annotations (approx recursive matching),  showing which set of trigger word classifications from  task 1 was used as input (submitted results in bold). Fea- ture sets described in", "labels": [], "entities": []}]}