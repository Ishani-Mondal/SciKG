{"title": [{"text": "A Study of Convolution Tree Kernel with Local Alignment", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper discusses anew convolu-tion tree kernel by introducing local alignments.", "labels": [], "entities": []}, {"text": "The main idea of the new kernel is to allow some syntactic alternations during each match between subtrees.", "labels": [], "entities": []}, {"text": "In this paper, we give an algorithm to calculate the composite kernel.", "labels": [], "entities": []}, {"text": "The experiment results show promising improvements on two tasks: semantic role labeling and question classification.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7560844222704569}, {"text": "question classification", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7999503314495087}]}], "introductionContent": [{"text": "Recently kernel-based methods have become a state-of-art technique and been widely used in natural language processing applications.", "labels": [], "entities": []}, {"text": "In this method, a key problem is how to design a proper kernel function in terms of different data representations.", "labels": [], "entities": []}, {"text": "So far, there are two kinds of data representations.", "labels": [], "entities": []}, {"text": "One is to encode an object with a flat vector whose element correspond to an extracted feature from the object.", "labels": [], "entities": []}, {"text": "However the feature vector is sensitive to the structural variations.", "labels": [], "entities": []}, {"text": "The extraction schema is heavily dependent on different problems.", "labels": [], "entities": []}, {"text": "On the other hand, kernel function can be directly calculated on the object.", "labels": [], "entities": []}, {"text": "The advantages are that the original topological information is to a large extent preserved and the introduction of additional noise maybe avoided.", "labels": [], "entities": []}, {"text": "Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction(, named entity recognition(), semantic role labeling( and soon.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.8422795832157135}, {"text": "named entity recognition", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.6507712999979655}, {"text": "semantic role labeling", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.7147461573282877}]}, {"text": "To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called\" Convolution kernel\".", "labels": [], "entities": []}, {"text": "Based on this work, proposed a tree kernel calculation by counting the common subtrees.", "labels": [], "entities": []}, {"text": "In other words, two trees are considered if and only if these two trees are exactly same.", "labels": [], "entities": []}, {"text": "In real sentences, some structural alternations within a given phrase are permitted without changing its usage.", "labels": [], "entities": []}, {"text": "Therefore, proposed partial trees to partially match between subtrees.", "labels": [], "entities": []}, {"text": "generalize the tree kernel to labeled order tree kernel with more flexible match.", "labels": [], "entities": []}, {"text": "And from the idea of introducing linguistical knowledge, proposed a grammar-driven tree kernel, in which two subtrees are same if and only if the corresponding two productions are in the same manually defined set.", "labels": [], "entities": []}, {"text": "In addition, the problem of hard matching can be alleviated by processing or mapping the trees.", "labels": [], "entities": [{"text": "hard matching", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.7077198624610901}]}, {"text": "For example, Tai mapping () generalized the kernel from counting subtrees to counting the function of mapping.", "labels": [], "entities": []}, {"text": "Moreover multi-source knowledge can benefit kernel calculation, such as using dependency information to dynamically determine the tree span (.", "labels": [], "entities": [{"text": "kernel calculation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8247806131839752}]}, {"text": "In this paper, we propose a tree kernel calculation algorithm by allowing variations in productions.", "labels": [], "entities": [{"text": "tree kernel calculation", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.65921550989151}]}, {"text": "The variation is measured with local alignment score between two derivative POS sequences.", "labels": [], "entities": []}, {"text": "To reduce the computation complexity, we use the dynamic programming algorithm to compute the score of any alignment.", "labels": [], "entities": []}, {"text": "And the top n alignments are considered in the kernel.", "labels": [], "entities": []}, {"text": "Another problem in Collins and Duffy's tree kernel is context-free.", "labels": [], "entities": [{"text": "Collins and Duffy's tree kernel", "start_pos": 19, "end_pos": 50, "type": "DATASET", "confidence": 0.8117489218711853}]}, {"text": "It does not consider any semantic information located at the leaf nodes of the parsing trees.", "labels": [], "entities": []}, {"text": "To lexicalized tree kernel, considered the associated term similarity by virtue of WordNet.", "labels": [], "entities": [{"text": "similarity", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9458719491958618}, {"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9786255359649658}]}, {"text": "constructed a separate lexical feature containing words on a given path and merged into the kernel in linear combination.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the commonly used tree kernel.", "labels": [], "entities": []}, {"text": "In section 3, we propose our method to make use of the local alignment information in kernel calculation.", "labels": [], "entities": [{"text": "kernel calculation", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.842197984457016}]}, {"text": "Section 4 presents the results of our experiments for two different applications ( Semantic Role Labeling and Question Classification).", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.8229895035425822}, {"text": "Question Classification", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7676916420459747}]}, {"text": "Finally section 5 provides our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The classification performance is evaluated with respect to accuracy, precision(p), recall(r) and Accuracy(%) ( 84.35 86.72 ( 87.96 Our Kernel 88.48 compares the performance of our method and other three famous kernels on WSJ test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9994545578956604}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9993640780448914}, {"text": "recall(r)", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9388691931962967}, {"text": "Accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9993908405303955}, {"text": "WSJ test data", "start_pos": 222, "end_pos": 235, "type": "DATASET", "confidence": 0.9674046834309896}]}, {"text": "We implemented these three methods with the same settings described in the papers.", "labels": [], "entities": []}, {"text": "It shows that our kernel achieves the best performance with 88.48% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9958509206771851}]}, {"text": "The advantages of our approach are: 1).", "labels": [], "entities": []}, {"text": "the alignments allow soft syntactic structure match; 2).", "labels": [], "entities": []}, {"text": "threshold can avoid overgeneration and selected salient alignments.", "labels": [], "entities": []}, {"text": "gives our performance on data sets and the detail result on WSJ test data.", "labels": [], "entities": [{"text": "detail", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9982156753540039}, {"text": "WSJ test data", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9408989747365316}]}, {"text": "In this set of experiment, we evaluate different types of kernels for Question Classification(QC) task.: Classification accuracy of different kernels on different data sets this paper we use the same dataset as introduced in().", "labels": [], "entities": [{"text": "Question Classification(QC) task.", "start_pos": 70, "end_pos": 103, "type": "TASK", "confidence": 0.8054559032122294}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.8593618869781494}]}, {"text": "The dataset is divided 4 into 5500 questions for training and 500 questions from TREC 20 for testing.", "labels": [], "entities": [{"text": "TREC 20", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.8019301891326904}]}, {"text": "The total training samples are randomly divided into 5 subsets with sizes 1,000, 2,000, 3,000, 4,000 and 5,500 respectively.", "labels": [], "entities": []}, {"text": "In this paper, we compare the linear kernel based on bag-of-word (BOW), the original tree kernel (TK), the local alignment tree kernel (section 3, LATK) and its correspondences with LSA similarity and a set of semanticenriched LATK with different similarity metrics.", "labels": [], "entities": []}, {"text": "To obtain the parse tree, we use Charniak parser 5 for every question.", "labels": [], "entities": []}, {"text": "Like the previous experiment, SVM-Light-TK software and the OVA strategy are implemented.", "labels": [], "entities": []}, {"text": "In all experiments, we use the default parameter in SVM(e.g. margin parameter) and set \u03b1 = 1.", "labels": [], "entities": [{"text": "margin", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9679036736488342}]}, {"text": "In LSA model, we set k = 50.", "labels": [], "entities": []}, {"text": "Finally, we use multi-classification accuracy to evaluate http://l2r.cs.uiuc.edu/ cogcomp/Data/QA/QC/ 5 ftp://ftp.cs.brown.edu/pub/nlparser/ the performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9414882063865662}]}, {"text": "gives the results of the experiments.", "labels": [], "entities": []}, {"text": "We can see that the local alignment tree kernel increase the multi-classification accuracy of the basic tree kernel by about 0.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.970536470413208}]}, {"text": "The introduction of semantic information further improves accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9952117800712585}]}, {"text": "Among WordNet-based metrics, \"Wu and Palmer\" metric achieves the best result, i.e. 92.5%.", "labels": [], "entities": []}, {"text": "As a whole, the WordNet-based similarities perform better than LSA-based measurement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data sets statistics", "labels": [], "entities": []}, {"text": " Table 3: top: overall performance result on  data sets ; bottom: detail result on WSJ  data", "labels": [], "entities": [{"text": "detail", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9937589168548584}, {"text": "WSJ  data", "start_pos": 83, "end_pos": 92, "type": "DATASET", "confidence": 0.867871880531311}]}, {"text": " Table 5: Classification accuracy of different kernels on different data sets", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8961912393569946}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.7674710750579834}]}]}