{"title": [{"text": "Disambiguating \"DE\" for Chinese-English Machine Translation", "labels": [], "entities": [{"text": "DE", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8558791279792786}, {"text": "Chinese-English Machine Translation", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.5844293336073557}]}], "abstractContent": [{"text": "Linking constructions involving (DE) are ubiquitous in Chinese, and can be translated into En-glish in many different ways.", "labels": [], "entities": []}, {"text": "This is a major source of machine translation error, even when syntax-sensitive translation models are used.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7613962590694427}]}, {"text": "This paper explores how getting more information about the syntactic, semantic, and discourse context of uses of (DE) can facilitate producing an appropriate English translation strategy.", "labels": [], "entities": [{"text": "English translation", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.6229676753282547}]}, {"text": "We describe a finer-grained classification of (DE) constructions in Chinese NPs, construct a corpus of annotated examples , and then train a log-linear classifier, which contains linguistically inspired features.", "labels": [], "entities": [{"text": "classification of (DE) constructions", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.6766232450803121}]}, {"text": "We use the DE classifier to preprocess MT data by explicitly labeling (DE) constructions, as well as reordering phrases, and show that our approach provides significant BLEU point gains on MT02 (+1.24), MT03 (+0.88) and MT05 (+1.49) on a phrased-based system.", "labels": [], "entities": [{"text": "BLEU point", "start_pos": 169, "end_pos": 179, "type": "METRIC", "confidence": 0.9632094204425812}, {"text": "MT02", "start_pos": 189, "end_pos": 193, "type": "DATASET", "confidence": 0.8993455767631531}, {"text": "MT03", "start_pos": 203, "end_pos": 207, "type": "DATASET", "confidence": 0.9074859619140625}, {"text": "MT05", "start_pos": 220, "end_pos": 224, "type": "DATASET", "confidence": 0.933384358882904}]}, {"text": "The improvement persists when a hierarchical reordering model is applied.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English.", "labels": [], "entities": [{"text": "Machine translation (MT) from Chinese to English", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8940109941694472}, {"text": "BLEU", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9989582300186157}]}, {"text": "Many of these structural differences are related to the ubiquitous Chinese (DE) construction, used fora wide range of noun modification constructions (both single word and clausal) and other uses.", "labels": [], "entities": [{"text": "noun modification constructions", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.7471411029497782}]}, {"text": "Part of the solution to dealing with these ordering issues is hierarchical decoding, such as the Hiero system), a method motivated by (DE) examples like the one in.", "labels": [], "entities": [{"text": "Hiero system", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.8772928714752197}]}, {"text": "In this case, the translation goal is to rotate the noun head and the preceding relative clause around (DE), so that we can translate to \"[one of few countries] [have diplomatic relations with North Korea]\".", "labels": [], "entities": [{"text": "DE)", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9790180921554565}]}, {"text": "Hiero can learn this kind of lexicalized synchronous grammar rule.", "labels": [], "entities": []}, {"text": "But use of hierarchical decoders has not solved the DE construction translation problem.", "labels": [], "entities": [{"text": "DE construction translation", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.8481510678927103}]}, {"text": "We analyzed the errors of three state-of-the-art systems (the 3 DARPA GALE phase 2 teams' systems), and even though all three use some kind of hierarchical system, we found many remaining errors related to reordering.", "labels": [], "entities": [{"text": "DARPA GALE phase 2 teams'", "start_pos": 64, "end_pos": 89, "type": "DATASET", "confidence": 0.6332065999507904}]}, {"text": "An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (;).", "labels": [], "entities": []}, {"text": "For example introduced a set of rules to decide if a (DE) construction should be reordered or not before translating to English: \u2022 For DNPs (consisting of\"XP+DEG\"): -Reorder if XP is PP or LCP; -Reorder if XP is a non-pronominal NP \u2022 For CPs (typically formed by \"IP+DEC\"): -Reorder to align with the \"that+clause\" structure of English.", "labels": [], "entities": []}, {"text": "Although this and previous reordering work has led to significant improvements, errors still remain.", "labels": [], "entities": []}, {"text": "Indeed, found that the precision of their NP rules is only about 54.6% on a small human-judged set.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9996356964111328}]}, {"text": "One possible reason the (DE) construction remains unsolved is that previous work has paid insufficient attention to the many ways the (DE) construction can be translated and the rich structural cues to the translation. classes.", "labels": [], "entities": []}, {"text": "But our investigation shows that there are many strategies for translating Chinese [A B] phrases into English, including the patterns in Table 1, only some involving reversal.", "labels": [], "entities": [{"text": "translating Chinese [A B] phrases", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.7937646167618888}]}, {"text": "Notice that the presence of reordering is only one part of the rich structure of these examples.", "labels": [], "entities": []}, {"text": "Some reorderings are relative clauses, while others involve prepositional phrases, but not all prepositional phrase uses involve reorderings.", "labels": [], "entities": []}, {"text": "These examples suggest that capturing finer-grained translation patterns could help achieve higher accuracy both in reordering and in lexical choice.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9974790215492249}]}, {"text": "In this work, we propose to use a statistical classifier trained on various features to predict fora given Chinese (DE) construction both whether it will reorder in English and which construction it will translate to in English.", "labels": [], "entities": []}, {"text": "We suggest that the necessary classificatory features can be extracted from Chinese, rather than English.", "labels": [], "entities": []}, {"text": "in Chinese has a unified meaning of 'noun modification', and the choice of reordering and construction realization are mainly a consequence of facts of English noun modification.", "labels": [], "entities": [{"text": "noun modification", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7377480119466782}, {"text": "construction realization", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.7292799949645996}]}, {"text": "Nevertheless, most of the features that determine the choice of a felicitous translation are available in the Chinese source.", "labels": [], "entities": []}, {"text": "Noun modification realization has been widely studied in English (e.g.,), and many of the important determinative properties (e.g., topicality, animacy, prototypicality) can be detected working in the source language.", "labels": [], "entities": [{"text": "Noun modification realization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8842399716377258}]}, {"text": "We first present some corpus analysis characterizing different DE constructions based on how they get translated into English (Section 2).", "labels": [], "entities": []}, {"text": "We then train a classifier to label DEs into the 5 different categories that we define (Section 3).", "labels": [], "entities": []}, {"text": "The fine-grained DEs, together with reordering, are then used as input to a statistical MT system (Section 4).", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9219501614570618}]}, {"text": "We find that classifying DEs into finergrained tokens helps MT performance, usually at least twice as much as just doing phrasal reordering.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9933905601501465}]}], "datasetContent": [{"text": "For our MT experiments, we used a reimplementation of Moses (), a state-of-the-art phrase-based system.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9924887418746948}]}, {"text": "The alignment is done by the Berkeley word aligner () and then we symmetrized the word alignment using the grow-diag heuristic.", "labels": [], "entities": []}, {"text": "For features, we incorporate Moses' standard eight features as well as the lexicalized reordering model.", "labels": [], "entities": []}, {"text": "Parameter tuning is done with Minimum Error Rate Training (MERT).", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.721862256526947}, {"text": "Minimum Error Rate Training (MERT)", "start_pos": 30, "end_pos": 64, "type": "METRIC", "confidence": 0.9222412364823478}]}, {"text": "The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences.", "labels": [], "entities": [{"text": "MERT", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.9514577388763428}, {"text": "NIST MT06 data set", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.9222688376903534}]}, {"text": "We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences).", "labels": [], "entities": [{"text": "MT02", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.7591210007667542}, {"text": "MT03", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.8597036004066467}, {"text": "MT05", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8450745344161987}]}, {"text": "Our MT training corpus contains 1,560,071 sentence pairs from various parallel corpora from LDC.", "labels": [], "entities": [{"text": "MT training", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8925550580024719}]}, {"text": "There are 12,259,997 words on the English side.", "labels": [], "entities": [{"text": "English side", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9174515604972839}]}, {"text": "Chinese word segmentation is done by the Stanford Chinese segmenter (.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5690516531467438}]}, {"text": "After segmentation, there are 11,061,792 words on the Chinese side.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9766531586647034}]}, {"text": "We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules.", "labels": [], "entities": [{"text": "AFP", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8546220064163208}, {"text": "Gigaword corpus", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.8920708298683167}, {"text": "NIST08", "start_pos": 183, "end_pos": 189, "type": "DATASET", "confidence": 0.9596299529075623}]}, {"text": "Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed.", "labels": [], "entities": [{"text": "MT02", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9732262492179871}, {"text": "MT03", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8785545825958252}, {"text": "MT05", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.8828917741775513}, {"text": "MT06", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9551503658294678}]}, {"text": "To run the DE classifier, we also need to parse the Chinese texts.", "labels": [], "entities": []}, {"text": "We use the Stanford Chinese parser ( to parse the Chinese side of the MT training data and the tuning and test sets.", "labels": [], "entities": [{"text": "Stanford Chinese parser", "start_pos": 11, "end_pos": 34, "type": "DATASET", "confidence": 0.8894801735877991}, {"text": "MT training data", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.7369228800137838}]}, {"text": "We have two different settings as baseline experiments.", "labels": [], "entities": []}, {"text": "The first is without reordering or DE annotation on the Chinese side; we simply align the parallel texts, extract phrases and tune parameters.", "labels": [], "entities": []}, {"text": "This experiment is referred to as BASELINE.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8967998027801514}]}, {"text": "Also, we reorder the training data, the tuning and the test sets with the NP rules in ( and compare our results with this second baseline (WANG-NP).", "labels": [], "entities": [{"text": "WANG-NP", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.7255374789237976}]}, {"text": "The NP reordering preprocessing (WANG-NP) showed consistent improvement in on all test sets, with BLEU point gains ranging from 0.15 to 0.40.", "labels": [], "entities": [{"text": "BLEU point", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.9752524793148041}]}, {"text": "This confirms that having reordering around DEs in NP helps Chinese-English MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.7699848413467407}]}, {"text": "We use the best setting of the DE classifier described in Section 3 to annotate DEs in NPs in the MT training data as well as the NIST tuning and test sets.", "labels": [], "entities": [{"text": "MT training data", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.8154855171839396}, {"text": "NIST tuning and test sets", "start_pos": 130, "end_pos": 155, "type": "DATASET", "confidence": 0.8350210070610047}]}, {"text": "If a DE is in an NP, we use the annotation of AB , AsB , BprepA , relc , or AprepB to replace the original DE character.", "labels": [], "entities": [{"text": "AB", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.9912561178207397}, {"text": "BprepA", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9723894000053406}, {"text": "AprepB", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.8791609406471252}]}, {"text": "Once we have the DEs labeled, we preprocess the Chinese sentences by reordering them.", "labels": [], "entities": []}, {"text": "11 Note that not all DEs in the Chinese data are in NPs, therefore not all DEs are annotated with the extra labels.", "labels": [], "entities": [{"text": "Chinese data", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8793292939662933}]}, {"text": "After this preprocessing, we restart the whole MT pipeline -align the preprocessed data, extract phrases, run MERT and evaluate.", "labels": [], "entities": [{"text": "MT pipeline", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.8484541177749634}, {"text": "MERT", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8240596055984497}]}, {"text": "This setting is referred to as DE-Annotated in.", "labels": [], "entities": [{"text": "DE-Annotated", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.7292050123214722}]}], "tableCaptions": [{"text": " Table 4: The confusion matrix for 5-class DE classification", "labels": [], "entities": [{"text": "5-class DE classification", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7060036261876425}]}, {"text": " Table 6: The number of different DE classes labeled for the  MT training data.", "labels": [], "entities": [{"text": "MT training data", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.882587711016337}]}, {"text": " Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER  metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the  approximate randomization test in (", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.991176962852478}, {"text": "NIST MT evaluation datasets", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.6638427898287773}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.998855471611023}, {"text": "TER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9842139482498169}, {"text": "BASELINE", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9771071076393127}]}]}