{"title": [{"text": "New Features for FrameNet -WordNet Mapping", "labels": [], "entities": []}], "abstractContent": [{"text": "Many applications in the context of natural language processing or information retrieval maybe largely improved if they were able to fully exploit the rich semantic information annotated in high-quality, publicly available resources such as the FrameNet and the Word-Net databases.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.768280953168869}, {"text": "Word-Net databases", "start_pos": 262, "end_pos": 280, "type": "DATASET", "confidence": 0.9243531823158264}]}, {"text": "Nevertheless, the practical use of similar resources is often biased by the limited coverage of semantic phenomena that they provide.", "labels": [], "entities": []}, {"text": "A natural solution to this problem would be to automatically establish anchors between these resources that would allow us 1) to jointly use the encoded information, thus possibly overcoming limitations of the individual corpora, and 2) to extend each resource coverage by exploiting the information encoded in the others.", "labels": [], "entities": []}, {"text": "In this paper, we present a supervised learning framework for the mapping of FrameNet lexical units onto WordNet synsets based on a reduced set of novel and semantically rich features.", "labels": [], "entities": []}, {"text": "The automatically learnt mapping, which we call MapNet, can be used 1) to extend frame sets in the English FrameNet, 2) to populate frame sets in the Italian FrameNet via MultiWordNet and 3) to add frame labels to the MultiSemCor corpus.", "labels": [], "entities": [{"text": "English FrameNet", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.8183050751686096}, {"text": "MultiSemCor corpus", "start_pos": 218, "end_pos": 236, "type": "DATASET", "confidence": 0.9271947741508484}]}, {"text": "Our evaluation on these tasks shows that the proposed approach is viable and can result in accurate automatic annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the integration of manually-built lexical resources into NLP systems has received growing interest.", "labels": [], "entities": []}, {"text": "In particular, resources annotated with the surface realization of semantic roles, like FrameNet () have shown to convey an improvement in several NLP tasks, from question answering to textual entailment ( and shallow semantic parsing ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.8195905685424805}, {"text": "textual entailment", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.7064873576164246}, {"text": "shallow semantic parsing", "start_pos": 210, "end_pos": 234, "type": "TASK", "confidence": 0.6325759987036387}]}, {"text": "Nonetheless, the main limitation of such resources is their poor coverage, particularly as regards FrameNet.", "labels": [], "entities": [{"text": "coverage", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9723792672157288}, {"text": "FrameNet", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9059252738952637}]}, {"text": "Indeed, the latest FrameNet release (v. 1.3) contains 10,195 lexical units (LUs), 3,380 of which are described only by a lexicographic definition without any example sentence.", "labels": [], "entities": [{"text": "FrameNet release", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.8528696596622467}]}, {"text": "In order to cope with this lack of data, it would be useful to map frame information onto other lexical resources with a broader coverage.", "labels": [], "entities": []}, {"text": "We believe that WordNet, with 210,000 entries in version 3.0, can represent a suitable resource for this task.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 16, "end_pos": 23, "type": "DATASET", "confidence": 0.9692920446395874}]}, {"text": "In fact, both FrameNet and WordNet group together semantically similar words, and provide a hierarchical representation of the lexical knowledge (in WordNet the relations between synsets, in FrameNet between frames, see).", "labels": [], "entities": []}, {"text": "On the other hand, WordNet provides a more extensive coverage particularly for adjectives and nouns denoting artifacts and natural kinds, that are mostly neglected in FrameNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9567729234695435}, {"text": "FrameNet", "start_pos": 167, "end_pos": 175, "type": "DATASET", "confidence": 0.9130019545555115}]}, {"text": "In this paper, we present an approach using Support Vector Machines (SVM) to map FrameNet lexical units to WordNet synsets.", "labels": [], "entities": []}, {"text": "The proposed approach addresses some of the limitations of previous works on the same task (see for example De and).", "labels": [], "entities": []}, {"text": "Most notably, as we do not train the SVM on a per-frame basis, our model is able to cope also with those frames that have little or no annotated sentences to support the frame description.", "labels": [], "entities": []}, {"text": "After learning a very fast model on a small set of annotated lexical unit-synset pairs, we can automatically establish new mappings in never-seen-before pairs and use them for our applications.", "labels": [], "entities": []}, {"text": "We will evaluate the effect of the induced mappings on two tasks: the automatic enrichment of lexical unit sets in the English and Italian FrameNet via MultiWordNet (), and the annotation of the MultiSemCor corpus () with frame labels.", "labels": [], "entities": [{"text": "MultiSemCor corpus", "start_pos": 195, "end_pos": 213, "type": "DATASET", "confidence": 0.873809278011322}]}], "datasetContent": [{"text": "In order to train and test the classifier, we created a gold standard by manually annotating 2,158 LUsynset pairs as positive or negative examples.", "labels": [], "entities": []}, {"text": "We don't have data about inter-annotator agreement because the dataset was developed only by one annotator, but De Cao et al.", "labels": [], "entities": []}, {"text": "(2008) report 0.90 as Cohen's Kappa computed over 192 LU-synset pairs for the same mapping task.", "labels": [], "entities": []}, {"text": "This confirms that senses and lexical units are highly correlated and that the mapping is semantically motivated.", "labels": [], "entities": []}, {"text": "The annotation process can be carried out in reasonable time.", "labels": [], "entities": []}, {"text": "It took approximately two work days to an expert annotator to manually annotate the 2,158 pairs that makeup our gold standard.", "labels": [], "entities": []}, {"text": "The lexical units were randomly selected from the FrameNet database regardless of their part of speech or amount of annotated data in the FrameNet database.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 50, "end_pos": 67, "type": "DATASET", "confidence": 0.949310690164566}, {"text": "FrameNet database", "start_pos": 138, "end_pos": 155, "type": "DATASET", "confidence": 0.9511007964611053}]}, {"text": "For each lexical unit, we extracted from WordNet the synsets where the LU appears, and for each of them we assigned a positive label in case the LU-synset pairs share the same meaning, and a negative label otherwise.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9784297943115234}]}, {"text": "Statistics about the dataset are reported in  The 386 frames that are present in the dataset represent about one half of all lexicalized frames in the FrameNet database.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 151, "end_pos": 168, "type": "DATASET", "confidence": 0.93007493019104}]}, {"text": "This proves that, despite the limited size of the dataset, it is well representative of FrameNet characteristics.", "labels": [], "entities": []}, {"text": "This is confirmed by the distribution of the part of speech.", "labels": [], "entities": []}, {"text": "In fact, in the FrameNet database about 41% of the LUs are nouns, 40% are verbs, 17% are adjectives and <1% are adverbs (the rest are prepositions, which are not included in our experiment because they are not present in WordNet).", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.931563138961792}, {"text": "WordNet", "start_pos": 221, "end_pos": 228, "type": "DATASET", "confidence": 0.9610787630081177}]}, {"text": "In our dataset, the percentage of nouns is higher, but the PoS ranking by frequency is the same, with nouns being the most frequent PoS and adverbs the less represented.", "labels": [], "entities": [{"text": "PoS", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.8427260518074036}]}, {"text": "The average polysemy corresponds to the average number of candidate synsets for every LU in the dataset.", "labels": [], "entities": []}, {"text": "Note that the high number of lexical units with only one candidate does not imply a more straightforward mapping, because in some cases the only candidate represents a negative example.", "labels": [], "entities": []}, {"text": "In fact, a LU could be encoded in a frame that does not correspond to the sense expressed by the synset.", "labels": [], "entities": []}, {"text": "To evaluate our methodology we carried out a 10-fold cross validation using the available data, splitting them in 10 non-overlapping sets.", "labels": [], "entities": []}, {"text": "For each iteration, 70% of the data was used for training, 30% for testing.", "labels": [], "entities": []}, {"text": "All the splits were generated so as to maintain a balance between positive and negative examples in the training and test sets.", "labels": [], "entities": []}, {"text": "We used the SVM optimizer SVMLight 2), and applied polynomial kernels (poly) of different degrees (i.e. 1 through 4) in order to select the configuration with the best generalization capabilities.", "labels": [], "entities": []}, {"text": "The accuracy is measured in terms of Precision, Recall and F 1 measure, i.e. the harmonic average between Precision and Recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996702671051025}, {"text": "Precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.8747330904006958}, {"text": "Recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9673964381217957}, {"text": "F 1 measure", "start_pos": 59, "end_pos": 70, "type": "METRIC", "confidence": 0.9905243118604025}]}, {"text": "For the sake of annotation, it is important that an automatic system be very precise, thus not producing wrong annotations.", "labels": [], "entities": []}, {"text": "On the other hand, the higher the recall, the larger the amount of data that the system will be able to annotate.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996854066848755}]}, {"text": "The macro-average of the classifier accuracy for the different configurations is shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9596007466316223}]}, {"text": "We report results for linear kernel (i.e. poly 1), maximizing recall and f-measure, and for polynomial kernel of degree 2 (i.e. poly 2), scoring the highest precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9576167464256287}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9968659281730652}]}, {"text": "In general , we notice that all our models have a higher precision than recall, but overall are quite balanced.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9990350008010864}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9984537363052368}]}, {"text": "Different polynomial kernels (i.e. conjunction of features) do not produce very relevant differences in the results, suggesting that the features that we employed encode significant information and have a relevance if considered independently.", "labels": [], "entities": []}, {"text": "As a comparison, we also carried out the same evaluation by setting a manual threshold and considering a LU-synset pair as a positive example if the sum of the feature values was above the threshold.", "labels": [], "entities": []}, {"text": "We chose two different threshold values, the first (Row 1 in) selected so as to have comparable precision with the most precise SVM model (i.e. poly2), the second (Row 2) selected to have recall comparable with poly1, i.e. the SVM model with highest recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.998471200466156}, {"text": "recall", "start_pos": 188, "end_pos": 194, "type": "METRIC", "confidence": 0.9987747073173523}, {"text": "recall", "start_pos": 250, "end_pos": 256, "type": "METRIC", "confidence": 0.9975292086601257}]}, {"text": "In the former case, the model has a recall that is less than half than poly2, i.e. 0.214 vs. 0.569, meaning that such model would establish a half of the mappings while making the same percentage of mistakes.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9985594153404236}]}, {"text": "In the latter, the precision of the SVM classifier is 0.114 points higher, i.e. 0.794 vs. 0.680, meaning the SVM can retrieve as many mappings but making 15% less errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.999358594417572}]}, {"text": "In order to investigate the impact of different features on the classifier performance, we also considered three different groups of features separately: the ones based on stem overlap, those computed for prevalent domain and synset, and the features for simple and extended frame -synset overlap.", "labels": [], "entities": []}, {"text": "We did not take into account cross-lingual parallelism because it is one single feature whose coverage strongly relies on the parallel corpus available.", "labels": [], "entities": []}, {"text": "As a consequence, it is not possible to test the feature in isolation due to data sparseness.", "labels": [], "entities": []}, {"text": "Results are shown in, in the second group of rows.", "labels": [], "entities": []}, {"text": "Also in this case, we carried out a 10-fold cross validation using a polynomial kernel of degree 2.", "labels": [], "entities": []}, {"text": "The stem overlap features, which to our best knowledge are an original contribution of our approach, score the highest recall among the three groups.", "labels": [], "entities": [{"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9995831847190857}]}, {"text": "This confirms our intuition that LU defini-tions and WordNet glosses can help extending the number of mapped LUs, including those that are poorly annotated.", "labels": [], "entities": []}, {"text": "For instance, if we consider the KNOT CREATION frame, having only tie.v as LU, the features about prevalent domain & synset and about synset-frame overlap would hardly be informative, while stem overlap generally achieves a consistent performance regardless of the LU set.", "labels": [], "entities": [{"text": "KNOT CREATION frame", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.6718514164288839}]}, {"text": "In fact, tie.v is correctly mapped to synset v#00095054 based on their similar definition (respectively \"to form a knot\" and \"form a knot or bow in\").", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics on the dataset", "labels": [], "entities": []}]}