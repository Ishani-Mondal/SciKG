{"title": [{"text": "Modeling User Satisfaction with Hidden Markov Models", "labels": [], "entities": [{"text": "Modeling User Satisfaction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7937719424565634}]}], "abstractContent": [{"text": "Models for predicting judgments about the quality of Spoken Dialog Systems have been used as overall evaluation metric or as optimization functions in adaptive systems.", "labels": [], "entities": [{"text": "predicting judgments", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9004265367984772}]}, {"text": "We describe anew approach to such models, using Hidden Markov Models (HMMs).", "labels": [], "entities": []}, {"text": "The user's opinion is regarded as a continuous process evolving overtime.", "labels": [], "entities": []}, {"text": "We present the data collection method and results achieved with the HMM model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken Dialog Systems (SDSs) are now widely used, and are becoming more complex as a result of the increased solidity of advanced techniques, mainly in the realm of natural language understanding ().", "labels": [], "entities": [{"text": "Spoken Dialog Systems (SDSs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8121870656808218}, {"text": "natural language understanding", "start_pos": 165, "end_pos": 195, "type": "TASK", "confidence": 0.6715815365314484}]}, {"text": "At the same time, the evaluation of such systems increasingly demands for testing the entire system, as components for speech recognition, language understanding and dialog management are interacting more deeply.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7589059174060822}, {"text": "language understanding", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.7429438233375549}, {"text": "dialog management", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.7521273791790009}]}, {"text": "For example, the system might search for web content on the basis of meaning extracted from an n-best list, and generate the reply and speech recognition grammars depending on the content found ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.6992683410644531}]}, {"text": "The performance of single components strongly depends on each other component in this case.", "labels": [], "entities": []}, {"text": "While performance parameters become less meaningful in such a system, the system's overall quality, which can only be measured by asking the user, gains interest for the evaluation.", "labels": [], "entities": []}, {"text": "Typically, users fill out questionnaires after the interaction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system; ITU-T Rec.", "labels": [], "entities": [{"text": "ITU-T Rec.", "start_pos": 188, "end_pos": 198, "type": "DATASET", "confidence": 0.9631394942601522}]}, {"text": "Judgments of the system's overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction.", "labels": [], "entities": []}, {"text": "Thus, the complexity of the evaluation task is reduced.", "labels": [], "entities": []}, {"text": "In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g..", "labels": [], "entities": []}, {"text": "In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g.).", "labels": [], "entities": []}, {"text": "Currently, prediction models for user judgments are based on the PARADISE framework introduced by.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.6932883858680725}]}, {"text": "PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.7469170689582825}]}, {"text": "Therefore, a linear regression function can be trained with interaction parameters describing dialog costs and task success as predictors, and satisfaction ratings as the target.", "labels": [], "entities": []}, {"text": "The resulting equation can then be used to predict user satisfaction with unseen dialogs.", "labels": [], "entities": []}, {"text": "In follow-up studies, it could be shown that such models are to some degree generalizable ().", "labels": [], "entities": []}, {"text": "However, also limitations of the models in predicting judgments for other user groups, or for systems with different levels of ASR performance, were reported ().", "labels": [], "entities": [{"text": "predicting judgments", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.8991194665431976}, {"text": "ASR", "start_pos": 127, "end_pos": 130, "type": "TASK", "confidence": 0.9720974564552307}]}, {"text": "In the same study, prediction functions for user satisfaction were proposed to serve as optimization function in a system adapting its dialog strategy during the interaction.", "labels": [], "entities": []}, {"text": "This idea is taken up by.", "labels": [], "entities": []}, {"text": "The prediction accuracy of PARADISE functions typically lies around an R 2 of 0.5, meaning that 50% of the variance in the judgments is explained by the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9250226616859436}, {"text": "PARADISE", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.6113056540489197}, {"text": "R 2", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9705570638179779}]}, {"text": "While this number is not absolutely satisfying, it could be shown that mean values for groups of dialogs (e.g. with a specific system configuration) can be predicted more accurately than single dialogs with the same models.", "labels": [], "entities": []}, {"text": "Low R 2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree.", "labels": [], "entities": [{"text": "R 2", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9908835589885712}]}, {"text": "Such differences have been described, and may concern the actual perception of the judged issue, or the way the perception is described by the participant We have tested the PARADISE framework extensively, using different classifier models and interaction parameters.", "labels": [], "entities": []}, {"text": "Precise and general models are hard to achieve, even if the set of parameters describing the interaction is widely extended.", "labels": [], "entities": []}, {"text": "In an effort to improve such prediction models, we developed two ideas: \u2022 Predict the distribution of ratings which can be expected fora representative group of users given the same stimulus.", "labels": [], "entities": []}, {"text": "This takes into account that inmost cases the relevant user characteristics determining the judgment cannot be tracked, or even are unknown.", "labels": [], "entities": []}, {"text": "\u2022 Consider the time relations between events by modeling user opinion as a variable evolving over the course of the dialog.", "labels": [], "entities": []}, {"text": "This way, time relations like cooccurrence of events, which affect quality perception, attention, or memory can be modeled most effectively.", "labels": [], "entities": [{"text": "memory", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9471699595451355}]}, {"text": "In this paper, we present anew modeling approach considering these ideas.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the topology of the model.", "labels": [], "entities": []}, {"text": "Following this, we report how training data for the model were obtained from user tests in Section 3.", "labels": [], "entities": []}, {"text": "Evaluation results are presented in Section 4 and discussed in Section 5, before we conclude with some remarks on follow-up research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3. Evaluation of predictions of training dialogs (mean squared error and mean absolute error  in predicting the most probable state at each turn). Baseline results are given in brackets. The feature  combinations with which results were obtained are also reported.", "labels": [], "entities": [{"text": "mean squared error", "start_pos": 57, "end_pos": 75, "type": "METRIC", "confidence": 0.8780779441197714}, {"text": "mean absolute error", "start_pos": 80, "end_pos": 99, "type": "METRIC", "confidence": 0.7560261090596517}]}, {"text": " Table 4. Evaluation of predictions of training dialogs (tah=model trained on users with high  technical affinity; rf=user speech act feature exclude from analysis)", "labels": [], "entities": []}]}