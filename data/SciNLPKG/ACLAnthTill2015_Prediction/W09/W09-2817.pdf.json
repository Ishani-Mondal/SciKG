{"title": [{"text": "The GREC Named Entity Generation Challenge 2009: Overview and Evaluation Results", "labels": [], "entities": [{"text": "GREC Named Entity Generation Challenge 2009", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.7763400773207346}]}], "abstractContent": [{"text": "The GREC-NEG Task at Generation Challenges 2009 required participating systems to select coreference chains for all people entities mentioned in short en-cyclopaedic texts about people collected from Wikipedia.", "labels": [], "entities": [{"text": "GREC-NEG Task at Generation Challenges 2009", "start_pos": 4, "end_pos": 47, "type": "DATASET", "confidence": 0.6373304178317388}]}, {"text": "Three teams submitted six systems in total, and we additionally created four baseline systems.", "labels": [], "entities": []}, {"text": "Systems were tested automatically using a range of existing intrinsic metrics.", "labels": [], "entities": []}, {"text": "We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.8592241108417511}]}, {"text": "In addition, systems were tested in an intrinsic evaluation involving human judges.", "labels": [], "entities": []}, {"text": "This report describes the GREC-NEG Task and the evaluation methods applied, gives brief descriptions of the participating systems , and presents the evaluation results.", "labels": [], "entities": [{"text": "GREC-NEG Task", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.5808796584606171}]}], "introductionContent": [{"text": "The GREC-NEG task is about how to generate appropriate references to people entities in the context of apiece of discourse longer than a sentence.", "labels": [], "entities": [{"text": "GREC-NEG", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.6030469536781311}]}, {"text": "Rather than requiring participants to generate referring expressions (REs) from scratch, the GREC-NEG data provides sets of possible REs for selection.", "labels": [], "entities": [{"text": "GREC-NEG data", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.8950698673725128}]}, {"text": "This was the first time we ran a shared task using this data.", "labels": [], "entities": []}, {"text": "GREC-NEG is a step further from the related GREC-MSR Task in that it requires systems to generate plural as well as singular references, for all people entities mentioned in a text (GREC-MSR in contrast only had singular references to a single entity).", "labels": [], "entities": [{"text": "GREC-NEG", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8809815645217896}]}, {"text": "Moreover in GREC-NEG, possible REs for each entity are provided as one set for each entity (rather than one set for each context), so the task of selecting an appropriate RE fora given context is harder than in GREC-MSR.", "labels": [], "entities": [{"text": "GREC-NEG", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.8421531915664673}, {"text": "GREC-MSR", "start_pos": 211, "end_pos": 219, "type": "DATASET", "confidence": 0.8859390020370483}]}, {"text": "The main aim for participating systems in GREC-NEG'09 was to select an appropriate type of RE (name, common noun, pronoun, or empty reference).", "labels": [], "entities": [{"text": "GREC-NEG'09", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.8121024966239929}, {"text": "RE", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.942728579044342}]}, {"text": "The immediate motivating application context for the GREC Tasks is the improvement of referential clarity and coherence in extractive summaries and multiply edited texts (such as Wikipedia articles) by regenerating REs contained in them.", "labels": [], "entities": []}, {"text": "The motivating theoretical interest for the GREC Tasks is to discover what kind of information is useful in the input when making decisions about different properties of referring expressions when such expressions are being generated in context (this is in contrast to most traditional referring expression generation work in NLG which views the REG task as context-independent).", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 286, "end_pos": 317, "type": "TASK", "confidence": 0.7025627295176188}]}, {"text": "The GREC-NEG data is derived from the newly created GREC-People corpus which consists of 1,000 annotated introduction sections from Wikipedia articles in the category People.", "labels": [], "entities": [{"text": "GREC-NEG data", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8869161307811737}, {"text": "GREC-People corpus", "start_pos": 52, "end_pos": 70, "type": "DATASET", "confidence": 0.8885579407215118}]}, {"text": "Nine teams from seven countries registered for the GREC-NEG'09 Task, of which three teams ultimately submitted six systems in total (see Table 1).", "labels": [], "entities": [{"text": "GREC-NEG'09 Task", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.5335171669721603}]}, {"text": "We also used the corpus texts themselves as 'system' outputs, and created four baseline systems.", "labels": [], "entities": []}, {"text": "We evaluated the resulting 11 systems using a range of intrinsic and extrinsic evaluation methods.", "labels": [], "entities": []}, {"text": "This report presents the results of all evaluations (Section 6), along with descriptions of the GREC-NEG data (Sections 2) and task (Section 3), the test sets and evaluation methods (Section 4), and the participating systems (Section 5).", "labels": [], "entities": [{"text": "GREC-NEG data", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.8572002947330475}]}], "datasetContent": [{"text": "The GREC-NEG data set was divided into training, development and test data.", "labels": [], "entities": [{"text": "GREC-NEG data set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9080745577812195}]}, {"text": "We performed evaluations on the test data, using a range of different evaluation methods, including intrinsic and extrinsic, automatically assessed and human-evaluated, as described in the following sections.", "labels": [], "entities": []}, {"text": "Participants computed evaluation scores on the development set, using the geval-2.0.pl code provided by us which computes Word String Accuracy, REG'08-Type Recall and Precision, stringedit distance and BLEU.", "labels": [], "entities": [{"text": "stringedit distance", "start_pos": 178, "end_pos": 197, "type": "METRIC", "confidence": 0.6849931180477142}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9958160519599915}]}, {"text": "The chief humanlikeness measures we computed were REG08-Type Recall and Precision.", "labels": [], "entities": [{"text": "REG08-Type Recall", "start_pos": 50, "end_pos": 67, "type": "METRIC", "confidence": 0.8283509910106659}, {"text": "Precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.958938717842102}]}, {"text": "REG08-Type Precision is defined as the proportion of REFEXs selected by a participating system which match the reference REFEXs (where match counts are obtained as explained in the preceding section).", "labels": [], "entities": [{"text": "REG08-Type Precision", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.4150717109441757}]}, {"text": "REG08-Type Recall is defined as the proportion of reference REFEXs for which a participating system has produced a match.", "labels": [], "entities": [{"text": "REG08-Type Recall", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.4443197101354599}]}, {"text": "The reason why we use REG08-Type Recall and Precision for GREC-NEG rather than REG08-Type Accuracy as in GREC-MSR is that in GREC-NEG (unlike in GREC-MSR) there maybe a different number of REFEXs in system outputs and the reference texts in the test set (because there are embedded references in GREC-People, and systems may select REFEXs with or without embedded references for any given REF).", "labels": [], "entities": [{"text": "REG08-Type Recall", "start_pos": 22, "end_pos": 39, "type": "METRIC", "confidence": 0.7945423126220703}, {"text": "Precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.50545334815979}]}, {"text": "We also computed String Accuracy, defined as the proportion of word strings selected by a participating system that match those in the reference texts.", "labels": [], "entities": []}, {"text": "This was computed on complete, 'flattened' word strings contained in the outermost REFEX i.e. embedded REFEX word strings were not considered separately.", "labels": [], "entities": []}, {"text": "We also computed BLEU-3, NIST, string-edit distance and length-normalised string-edit distance, all on word strings defined as for String Accuracy.", "labels": [], "entities": [{"text": "BLEU-3", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9969993829727173}, {"text": "NIST", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.7885259389877319}, {"text": "length-normalised string-edit distance", "start_pos": 56, "end_pos": 94, "type": "METRIC", "confidence": 0.8704400857289633}]}, {"text": "BLEU and NIST are designed for multiple output versions, and for the string-edit metrics we computed the mean of means over the three textlevel scores (computed against the three versions of a text).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.953423798084259}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.9273927807807922}]}, {"text": "For details, see GREC-MSR report in this volume.", "labels": [], "entities": [{"text": "GREC-MSR report", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.7752497494220734}]}, {"text": "Given that the motivating application context for the GREC-NEG Task is improving referential clarity and coherence in multiply edited texts, we designed the human-assessed intrinsic evaluation as a preference-judgment test where subjects expressed their preference, in terms of two criteria, for either the original Wikipedia text or the version of it with system-generated referring expressions in it.", "labels": [], "entities": []}, {"text": "The intrinsic human evaluation involved outputs for 30 randomly selected items from the test set from 5 of the 6 participating systems, 1 the four baselines and the original corpus texts (10 systems in total).", "labels": [], "entities": []}, {"text": "We used a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from each system and for each test set item.", "labels": [], "entities": []}, {"text": "There were three 10x10 squares, and a total of 600 individual judgments in this evaluation (60 per system: 2 criteria x 3 articles x 10 evaluators).", "labels": [], "entities": []}, {"text": "We recruited 10 native speakers of English from among students currently completing a linguistics-related degree at Kings College London and University College London.", "labels": [], "entities": []}, {"text": "Following detailed instructions, subjects did two practice examples, followed by the 30 texts to be evaluated, in random order.", "labels": [], "entities": []}, {"text": "Subjects carried out the evaluation over the internet, at a time and place of their choosing.", "labels": [], "entities": []}, {"text": "They were allowed to interrupt and resume the experiment (though discouraged from doing so).", "labels": [], "entities": []}, {"text": "shows what subjects saw during the evaluation of an individual text pair.", "labels": [], "entities": []}, {"text": "The place (left/right) of the original Wikipedia article was randomly determined for each individual evaluation of a text pair.", "labels": [], "entities": []}, {"text": "People references are highlighted in yellow/orange, those that are identical in both texts are yellow, those that are different are orange.", "labels": [], "entities": []}, {"text": "The evaluator's task is to express their preference in terms of each quality criterion by moving the slider pointers.", "labels": [], "entities": []}, {"text": "Moving the slider to the left means expressing a preference for the text on the left, moving it to the right means preferring the text on the right; the further to the left/right the slider is moved, the stronger the preference.", "labels": [], "entities": []}, {"text": "The two criteria were explained in the introduction as follows (the wording of the first is from DUC): It was not evident to the evaluators that sliders were associated with numerical values.", "labels": [], "entities": [{"text": "DUC", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.8617256283760071}]}, {"text": "Slider pointers started out in the middle of the scale (no preference).", "labels": [], "entities": [{"text": "Slider pointers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.762305498123169}]}, {"text": "The values associated with the points on the slider ranged from -10.0 to +10.0.", "labels": [], "entities": []}, {"text": "An evaluation we piloted in REG'08 was an automatic approach to extrinsic evaluation (for a more detailed description, seethe GREC-MSR results report elsewhere in this volume).", "labels": [], "entities": [{"text": "REG'08", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.8472523093223572}, {"text": "extrinsic evaluation", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.8077230453491211}, {"text": "GREC-MSR", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.5733178853988647}]}, {"text": "The basic premise is that poorly chosen reference chains seem likely to affect the reader's ability to resolve REs.", "labels": [], "entities": [{"text": "resolve REs", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.7098470330238342}]}, {"text": "In our automatic extrinsic method, the role of the reader is played by an automatic coreference resolution tool and the expectation is that the tool performs worse (is less able to identify coreference chains) with more poorly chosen referential expressions.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.8475450277328491}]}, {"text": "To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolversthose included in LingPipe 2 and OpenNLP)-and averaged results.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.7531632781028748}, {"text": "OpenNLP", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.8248091340065002}]}, {"text": "For the same reason we used three different performance measures: MUC-6 (, CEAF, and B-CUBED ().", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.5940629839897156}, {"text": "CEAF", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.5716974139213562}, {"text": "B-CUBED", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9908635020256042}]}], "tableCaptions": [{"text": " Table 2: Numbers of person entities (hence coreference chains) in texts in the training/development data,  e.g. there are 38 texts which mention exactly 5 person entities.", "labels": [], "entities": []}, {"text": " Table 5: Self-reported evaluation scores for devel- opment set.", "labels": [], "entities": []}, {"text": " Table 3: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set and  for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.", "labels": [], "entities": [{"text": "REG08-Type Precision", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.6329437792301178}, {"text": "Recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9914116859436035}]}, {"text": " Table 4: REG08-Type Recall and Precision scores against human topline version of Test Set for complete  set and for subdomains; homogeneous subsets (Tukey HSD, alpha = .05) for complete set only.", "labels": [], "entities": [{"text": "REG08-Type Recall", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.755437970161438}, {"text": "Precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9694560170173645}]}, {"text": " Table 6: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1a (systems  in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy  only.", "labels": [], "entities": [{"text": "Word String Accuracy", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.6631325682004293}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9964281916618347}, {"text": "NIST", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.6086859703063965}, {"text": "Tukey HSD", "start_pos": 158, "end_pos": 167, "type": "DATASET", "confidence": 0.8544483482837677}]}, {"text": " Table 7: Word String Accuracy, BLEU, NIST, and string-edit scores, computed on Test Set 1b (systems  in order of Word String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy.", "labels": [], "entities": [{"text": "Word String Accuracy", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.6520866354306539}, {"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9963996410369873}, {"text": "Test Set 1b", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.7760814726352692}, {"text": "Tukey HSD", "start_pos": 158, "end_pos": 167, "type": "DATASET", "confidence": 0.8547712564468384}]}, {"text": " Table 8: Results for Clarity and Fluency preference judgement experiment. Mean = mean of individual  scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred; \u2212 =  number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.", "labels": [], "entities": [{"text": "Clarity and Fluency preference judgement", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.6550051510334015}, {"text": "Mean", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9870333671569824}]}, {"text": " Table 9: MUC, CEAF and B-CUBED F-Scores for  all systems; homogeneous subsets (Tukey HSD),  alpha = .05, for mean of F-Scores.", "labels": [], "entities": [{"text": "MUC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7824918031692505}, {"text": "CEAF", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.812452495098114}, {"text": "B-CUBED F-Scores", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.8214060366153717}, {"text": "F-Scores", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.8923341035842896}]}]}