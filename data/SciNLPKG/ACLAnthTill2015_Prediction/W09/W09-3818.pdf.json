{"title": [{"text": "Constructing parse forests that include exactly the n-best PCFG trees", "labels": [], "entities": [{"text": "Constructing parse", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7341523766517639}]}], "abstractContent": [{"text": "This paper describes and compares two algorithms that take as input a shared PCFG parse forest and produce shared forests that contain exactly then most likely trees of the initial forest.", "labels": [], "entities": []}, {"text": "Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG f-structure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest.", "labels": [], "entities": []}, {"text": "We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9829809963703156}]}], "introductionContent": [{"text": "The output of a CFG parser based on dynamic programming, such as an Earley parser, is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest.", "labels": [], "entities": [{"text": "CFG parser", "start_pos": 16, "end_pos": 26, "type": "TASK", "confidence": 0.6899427473545074}]}, {"text": "It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure.", "labels": [], "entities": []}, {"text": "This forest can be used for further processing, as reranking or machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7776384651660919}]}, {"text": "When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only then most likely trees of the forest.", "labels": [], "entities": []}, {"text": "Standard state-of-the-art algorithms that extract then best parses) produce a collection of trees, losing the factorization that has been achieved by the parser, and reproduce some identical sub-trees in several parses.", "labels": [], "entities": []}, {"text": "This situation is not satisfactory since postparsing processes, such as reranking algorithms or attribute computation, cannot take advantage of this lost factorization and may reproduce some identical work on common sub-trees, with a computational cost that can be exponentally high.", "labels": [], "entities": []}, {"text": "One way to solve the problem is to prune the forest by eliminating sub-forests that do not contribute to any of then most likely trees.", "labels": [], "entities": []}, {"text": "But this over-generates: the pruned forest contains more than then most likely trees.", "labels": [], "entities": []}, {"text": "This is particularly costly for post-parsing processes that may require in the worst cases an exponential execution time w.r.t. the number of trees in the forest, such as LFG f-structures construction or some advanced reranking techniques.", "labels": [], "entities": []}, {"text": "The experiments detailed in the last part of this paper show that the overgeneration factor of pruned sub-forest is more or less constant (see 6): after pruning the forest so as to keep then best trees, the resulting forest contains approximately 10 3 n trees.", "labels": [], "entities": []}, {"text": "At least for some post-parsing processes, this overhead is highly problematic.", "labels": [], "entities": []}, {"text": "For example, although LFG parsing can be achieved by computing LFG f-structures on top of a c-structure parse forest with a reasonable efficiency), it is clear that a 10 3 factor drastically affects the overall speed of the LFG parser.", "labels": [], "entities": [{"text": "LFG parsing", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.6332945227622986}]}, {"text": "Therefore, simply pruning the forest is not an adequate solution.", "labels": [], "entities": []}, {"text": "However, it will prove useful for comparison purposes.", "labels": [], "entities": []}, {"text": "The new direction that we explore in this paper is the production of shared forests that contain exactly then most likely trees, avoiding both the explicit construction of n different trees and the over-generation of pruning techniques.", "labels": [], "entities": []}, {"text": "This can be seen as a transduction which is applied on a forest and produces another forest.", "labels": [], "entities": []}, {"text": "The transduction applies some local transformations on the structure of the forest, developing some parts of the forest when necessary.", "labels": [], "entities": []}, {"text": "The structure of this paper is the following.", "labels": [], "entities": []}, {"text": "Section 2 defines the basic objects we will be dealing with.", "labels": [], "entities": []}, {"text": "Section 3 describes how to prune a shared forest, and introduces two approaches for building shared forests that contain exactly then most likely parses.", "labels": [], "entities": []}, {"text": "Section 4 describes experiments that were carried out on the Penn Treebank and section 5 concludes the paper.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9904006719589233}]}], "datasetContent": [{"text": "The methods described in section 3 have been tested on a PCFG G extracted from the Penn Treebank (.", "labels": [], "entities": [{"text": "PCFG G extracted from the Penn Treebank", "start_pos": 57, "end_pos": 96, "type": "DATASET", "confidence": 0.8827016353607178}]}, {"text": "G has been extracted naively: the trees have been decomposed into binary context free rules, and the probability of every rule has been estimated by its relative frequency (number of occurrences of the rule divided by the number of occurrences of its left hand side).", "labels": [], "entities": []}, {"text": "Rules occurring less than 3 times and rules with probabilities lower than 3 \u00d7 10 \u22124 have been eliminated.", "labels": [], "entities": []}, {"text": "The grammar produced contains 932 non terminals and 3, 439 rules.", "labels": [], "entities": []}, {"text": "The parsing has been realized using the SYN-TAX system which implements, and optimizes, the Earley algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.977454662322998}]}, {"text": "The evaluation has been conducted on the 1, 845 sentences of section 1, which constitute our test set.", "labels": [], "entities": []}, {"text": "For every sentence and for increasing values of n, an n-best sub-forest has been built using the rankset and the rectangles method.", "labels": [], "entities": []}, {"text": "The performances of the algorithms have been measured by the average compression rate they achieve for different values of n.", "labels": [], "entities": []}, {"text": "The compression rate is obtained by dividing the size of the n-best sub-forest of a sentence, as defined in section 2, by the size of the (unfolded) n-best forest.", "labels": [], "entities": []}, {"text": "The latter is the sum of the sizes of all trees in the forest, where every tree is seen as an instantiated grammar, its size is therefore the size of the corresponding instantiated grammar.", "labels": [], "entities": []}, {"text": "The size of the n-best forest constitutes a natural upper bound for the representation of the n-best trees.", "labels": [], "entities": []}, {"text": "Unfortunately, we have no natural lower bound for the size of such an object.", "labels": [], "entities": []}, {"text": "Nevertheless, we have computed the compression rates of the pruned n-best forest and used it as an imperfect lower bound.", "labels": [], "entities": []}, {"text": "As already mentioned, its imperfection comes from the fact that a pruned n-best forest contains more trees than then best ones.", "labels": [], "entities": []}, {"text": "This overgeneration appears clearly in which shows, for increasing values of n, the average number of trees in the n-best pruned forest for all sentences in our test set.", "labels": [], "entities": []}, {"text": "shows the average compression rates achieved by the three methods (forest pruning, rectangles and ranksets) on the test set for increasing values of n.", "labels": [], "entities": []}, {"text": "As predicted, the performances lie between 1 (no compression) and the compression of the n-best pruned forest.", "labels": [], "entities": []}, {"text": "The rectangle method outperforms the ranksets algorithm for every value of n.", "labels": [], "entities": []}, {"text": "The time needed to build an 100-best forest with the rectangle and the ranksets algorithms is shown in.", "labels": [], "entities": []}, {"text": "This figure shows the average parsing time for sentences of a given length, as well as the average time necessary for building the 100-best forest using the two aforementioned algorithms.", "labels": [], "entities": []}, {"text": "This time includes the parsing time i.e. it is the time necessary for parsing a sentence and building the 100-best forest.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9655255079269409}, {"text": "parsing a sentence", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8724441925684611}]}, {"text": "As shown by the figure, the time complexities of the two methods are very close.", "labels": [], "entities": []}], "tableCaptions": []}