{"title": [{"text": "Monte Carlo inference and maximization for phrase-based translation", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7440986633300781}]}], "abstractContent": [{"text": "Recent advances in statistical machine translation have used beam search for approximate NP-complete inference within probabilistic translation models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6419152120749155}]}, {"text": "We present an alternative approach of sampling from the posterior distribution defined by a translation model.", "labels": [], "entities": []}, {"text": "We define a novel Gibbs sampler for sampling translations given a source sentence and show that it effectively explores this posterior distribution.", "labels": [], "entities": []}, {"text": "In doing so we overcome the limitations of heuristic beam search and obtain theoretically sound solutions to inference problems such as finding the maximum probability translation and minimum expected risk training and decoding.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e * that maximises the conditional posterior probability p(e|f ).", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.792948787411054}]}, {"text": "This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes -a direct result of employing a mathematical framework that we can reason about independently of any particular model.", "labels": [], "entities": [{"text": "translation", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9588170647621155}]}, {"text": "For example, we can train SMT models using maximum likelihood estimation ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9965642094612122}]}, {"text": "Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (.", "labels": [], "entities": []}, {"text": "We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models ().", "labels": [], "entities": []}, {"text": "Most models define multiple derivations for each translation; the probability of a translation is thus the sum overall of its derivations.", "labels": [], "entities": []}, {"text": "Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting.", "labels": [], "entities": [{"text": "NPhard", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.8711268901824951}]}, {"text": "It is thus necessary to resort to approximations for this sum and the search for its maximum e * . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP).", "labels": [], "entities": []}, {"text": "Though effective for some problems, it has many serious drawbacks for probabilistic inference: 1.", "labels": [], "entities": []}, {"text": "It typically differs from the true model maximum.", "labels": [], "entities": []}, {"text": "2. It often requires additional approximations in search, leading to further error.", "labels": [], "entities": []}, {"text": "3. It introduces restrictions on models, such as use of only local features.", "labels": [], "entities": []}, {"text": "4. It provides no good solution to compute the normalization factor Z(f ) required by many probabilistic algorithms.", "labels": [], "entities": []}, {"text": "In this work, we solve these problems using a Monte Carlo technique with none of the above drawbacks.", "labels": [], "entities": []}, {"text": "Our technique is based on a novel Gibbs sampler that draws samples from the posterior distribution of a phrase-based translation model () but operates in linear time with respect to the number of input words (Section 2).", "labels": [], "entities": []}, {"text": "We show that it is effective for both decoding (Section 3) and minimum risk training (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "To verify that our sampler was behaving as expected, we computed the KL divergence between its inferred distribution\u02c6qdistribution\u02c6 distribution\u02c6q(e|f ) and the true distribution over a single sentence).", "labels": [], "entities": []}, {"text": "We computed the true posterior distribution p(e|f ) under an Arabic-English phrase-based translation model with parameters trained to maximise expected BLEU (Section 4), summing out the derivations for identical translations and computing the partition term Z(f ).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9992659687995911}]}, {"text": "As the number of iterations increases, the KL divergence between the distributions approaches zero.", "labels": [], "entities": []}, {"text": "During preliminary experiments with training, we observed on a held-out data set (portions of MT04) that the magnitude of the weights vector increased steadily (effectively sharpening the distribution), but without any obvious change in the objective.", "labels": [], "entities": [{"text": "MT04", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.8464124202728271}]}, {"text": "Since this resulted in poor generalization we added a regularization term of || \u03b8 \u2212 \u00b5|| 2 /2\u03c3 2 to L.", "labels": [], "entities": []}, {"text": "We initially set the means to zero, but after further observing that the translations under all decoding criteria tended to be shorter than the reference (causing a significant drop in performance when evaluated using BLEU), we found that performance could be improved by setting \u00b5 WP = \u22120.5, indicating a preference fora lower weight on this parameter.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9981276392936707}, {"text": "\u00b5 WP", "start_pos": 280, "end_pos": 284, "type": "METRIC", "confidence": 0.9187745451927185}]}, {"text": "compares the performance on Arabic to English translation of systems tuned with MERT (maximizing corpus BLEU) with systems tuned to maximise expected sentence-level BLEU.", "labels": [], "entities": [{"text": "MERT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9940420985221863}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.5576918721199036}, {"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9370558261871338}]}, {"text": "Although the performance of the minimum risk model under all decoding criteria is lower than that of the original MERT model, we note that the positive effect of marginalizing over derivations as well as using minimum risk decoding for obtaining good results on this model.", "labels": [], "entities": []}, {"text": "A full exploration of minimum risk training is beyond the scope of this paper, but these initial experiments should help emphasise the versatility of the sampler and its utility in solving a variety of problems.", "labels": [], "entities": []}, {"text": "In the conclusion, we will, however, discuss some possible future directions that can betaken to make this style of training more competitive with standard baseline systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of the BLEU score of the Moses  decoder with the sampler running in max-derivation  (MaxD), max-translation (MaxT) and minumum Bayes  risk (MBR) modes. The test sets are TEST2007 (in) and  NEWS-DEV2009B (out)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.998647153377533}, {"text": "minumum Bayes  risk (MBR)", "start_pos": 140, "end_pos": 165, "type": "METRIC", "confidence": 0.7637246946493784}, {"text": "TEST2007", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.4930877387523651}, {"text": "NEWS-DEV2009B", "start_pos": 210, "end_pos": 223, "type": "DATASET", "confidence": 0.9362338781356812}]}, {"text": " Table 2: Decoding with minimum risk trained systems,  compared with decoding with MERT-trained systems on  Arabic to English MT03 data", "labels": [], "entities": [{"text": "MT03 data", "start_pos": 126, "end_pos": 135, "type": "DATASET", "confidence": 0.7967608273029327}]}]}