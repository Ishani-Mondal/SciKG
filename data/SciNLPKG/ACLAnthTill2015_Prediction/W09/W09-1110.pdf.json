{"title": [{"text": "Interactive Feature Space Construction using Semantic Information", "labels": [], "entities": [{"text": "Interactive Feature Space Construction", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5520826280117035}]}], "abstractContent": [{"text": "Specifying an appropriate feature space is an important aspect of achieving good performance when designing systems based upon learned classifiers.", "labels": [], "entities": []}, {"text": "Effectively incorporating information regarding semantically related words into the feature space is known to produce robust, accurate classifiers and is one apparent motivation for efforts to automatically generate such resources.", "labels": [], "entities": []}, {"text": "However, naive incorporation of this semantic information may result in poor performance due to increased ambiguity.", "labels": [], "entities": []}, {"text": "To overcome this limitation, we introduce the interactive feature space construction protocol, where the learner identifies inadequate regions of the feature space and in coordination with a domain expert adds descriptiveness through existing semantic resources.", "labels": [], "entities": [{"text": "feature space construction", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.6720480720202128}]}, {"text": "We demonstrate effectiveness on an entity and relation extraction system including both performance improvements and ro-bustness to reductions in annotated data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6825842410326004}]}], "introductionContent": [{"text": "An important natural language processing (NLP) task is the design of learning systems which perform well over a wide range of domains with limited training data.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.7952205042044321}]}, {"text": "While the NLP community has along tradition of incorporating linguistic information into statistical systems, machine learning approaches to these problems often emphasize learning sophisticated models oversimple, mostly lexical, features.", "labels": [], "entities": []}, {"text": "This trend is not surprising as a primary motivation for machine learning solutions is to reduce the manual effort required to achieve state of the art performance.", "labels": [], "entities": []}, {"text": "However, one notable advantage of discriminative classifiers is the capacity to encode arbitrarily complex features, which partially accounts for their popularity.", "labels": [], "entities": []}, {"text": "While this flexibility is powerful, it often overwhelms the system designer causing them to resort to simple features.", "labels": [], "entities": []}, {"text": "This work presents a method to partially automate feature engineering through an interactive learning protocol.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7391985356807709}]}, {"text": "While it is widely accepted that classifier performance is predicated on feature engineering, designing good features requires significant effort.", "labels": [], "entities": []}, {"text": "One underutilized resource for descriptive features are existing semantically related word lists (SRWLs), generated both manually and automatically ().", "labels": [], "entities": []}, {"text": "Consider the following named entity recognition (NER) example: His father was rushed to ORG , an arm of ORG , in west suburban LOC . For such tasks, it is helpful to know that west is a member of the SRWL and other such designations.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.7800344079732895}, {"text": "ORG", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9368076920509338}, {"text": "ORG", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.8613330721855164}, {"text": "LOC", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.754898726940155}]}, {"text": "If extracting features using this information, we would require observing only a subset of the SRWL in the data to learn the corresponding parameter.", "labels": [], "entities": [{"text": "SRWL", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.8101431131362915}]}, {"text": "This statement suggests that one method for learning robust classifiers is to incorporate semantic information through features extracted from the more descriptive representation: His father was rushed to Westlake, an  Deriving discriminative features from this representation often results in more informative features and a correspondingly simpler classification task.", "labels": [], "entities": [{"text": "Westlake", "start_pos": 205, "end_pos": 213, "type": "DATASET", "confidence": 0.9679234027862549}, {"text": "Deriving discriminative", "start_pos": 219, "end_pos": 242, "type": "TASK", "confidence": 0.9185760617256165}]}, {"text": "Although effective approaches along this vein have been shown to induce more accurate classifiers (), naive approaches may instead result in higher sample complexity due to increased ambiguity introduced through these semantic resources.", "labels": [], "entities": []}, {"text": "Features based upon SRWLs must therefore balance the tradeoff between descriptiveness and noise.", "labels": [], "entities": []}, {"text": "This paper introduces the interactive feature space construction (IFSC) protocol, which facilitates coordination between a domain expert and learning algorithm to interactively define the feature space during training.", "labels": [], "entities": [{"text": "interactive feature space construction (IFSC)", "start_pos": 26, "end_pos": 71, "type": "TASK", "confidence": 0.7720737116677421}]}, {"text": "This paper describes the particular instance of the IFSC protocol where semantic information is introduced through abstraction of lexical terms in the feature space with their SRWL labels.", "labels": [], "entities": []}, {"text": "Specifically, there are two notable contributions of this work: (1) an interactive method for the expert to directly encode semantic knowledge into the feature space with minimal effort and (2) a querying function which uses both the current state of the learner and properties of the available SRWLs to select informative instances for presentation to the expert.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of this protocol on an entity and relation extraction task in terms of performance and labeled data requirements.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.7734803160031637}]}], "datasetContent": [{"text": "To demonstrate the IFSC protocol on a practical application, we examine a three-stage pipeline model for entity and relation extraction, where the task is decomposed into sequential stages of segmentation, entity classification, and relation classification.", "labels": [], "entities": [{"text": "entity and relation extraction", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.616087406873703}, {"text": "entity classification", "start_pos": 206, "end_pos": 227, "type": "TASK", "confidence": 0.7104986906051636}, {"text": "relation classification", "start_pos": 233, "end_pos": 256, "type": "TASK", "confidence": 0.790252685546875}]}, {"text": "Extending the standard classification task, a pipeline model decomposes the overall classification into a sequence of D stages such that each stage d = 1, . .", "labels": [], "entities": []}, {"text": "Entity classification begins with the results of the segmentation classifier and classifies each segment into Y \u2208 {person, location, organization}.", "labels": [], "entities": [{"text": "Entity classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8620306849479675}]}, {"text": "Finally, relation classification labels each predicted entity pair with Y \u2208 {located in, work for, org based in, live in, kill} \u00d7 {lef t, right} + no relation.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.8341266512870789}]}, {"text": "The data used for empirical evaluation was taken from) and consists of 1436 sentences, which is split into a 1149 (80%) sentence training set and a 287 (20%) sentence testing set such that all have at least one active relation.", "labels": [], "entities": []}, {"text": "SRWLs are provided by) and experiments were conducted using a custom graphical user interface (GUI) designed specifically for the IFSC protocol.", "labels": [], "entities": [{"text": "SRWLs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6833128929138184}, {"text": "IFSC protocol", "start_pos": 130, "end_pos": 143, "type": "DATASET", "confidence": 0.7047016620635986}]}, {"text": "The learning algorithm used for each stage of the classification task is a regularized variant of the structured Perceptron).", "labels": [], "entities": []}, {"text": "Resources used to perform experiments are available at http://L2R.cs.uiuc.edu/\u223ccogcomp/.", "labels": [], "entities": []}, {"text": "We extract features in a method similar to, except that we do not include gazetteer features in \u03a6 as we will include this type of external information interactively.", "labels": [], "entities": []}, {"text": "Secondly, we use SRWL features as introduced.", "labels": [], "entities": [{"text": "SRWL", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.4657812714576721}]}, {"text": "The segmentation features include the word/SRWL itself along with the word/SRWL of three words before and two words after, bigrams of the word/SRWL surrounding the word, capitalization of the word, and capitalization of its neighbor on each side.", "labels": [], "entities": []}, {"text": "Entity classification uses the segment size, the word/SRWL members within the segment, and a window of two word/SRWL elements on each side.", "labels": [], "entities": [{"text": "Entity classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8301043808460236}]}, {"text": "Relation classification uses the same features as entity classification along with the entity labels, the length of the entities, and the number of tokens between them.", "labels": [], "entities": [{"text": "Relation classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9591535925865173}, {"text": "entity classification", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7349465191364288}]}], "tableCaptions": [{"text": " Table 1: Relative performance of the stated experiments conducted over the entire available dataset. The interactive  feature construction protocol outperforms all non-interactive baselines, particularly for later stages of the pipeline  while requiring only 50 interactions.", "labels": [], "entities": []}]}