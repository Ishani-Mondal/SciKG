{"title": [{"text": "Findings of the 2009 Workshop on Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7825721104939779}]}], "abstractContent": [{"text": "This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task.", "labels": [], "entities": [{"text": "WMT09 shared tasks", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.5238916377226511}, {"text": "translation task", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.8939574658870697}]}, {"text": "We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.741893857717514}]}, {"text": "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics.", "labels": [], "entities": []}, {"text": "We present anew evaluation technique whereby system output is edited and judged for correctness.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (.", "labels": [], "entities": [{"text": "EACL Workshop on Statistical Machine Translation", "start_pos": 64, "end_pos": 112, "type": "TASK", "confidence": 0.5853839914004008}]}, {"text": "There were three shared tasks this year: a translation task between English and five other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics.", "labels": [], "entities": [{"text": "translation task", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.9007484018802643}]}, {"text": "The performance on each of these shared task was determined after a comprehensive human evaluation.", "labels": [], "entities": []}, {"text": "There were a number of differences between this year's workshop and last year's workshop: \u2022 Larger training sets -In addition to annual increases in the Europarl corpus, we released a French-English parallel corpus verging on 1 billion words.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 153, "end_pos": 168, "type": "DATASET", "confidence": 0.9917882680892944}]}, {"text": "We also provided large monolingual training sets for better language modeling of the news translation task.", "labels": [], "entities": [{"text": "news translation task", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7828889489173889}]}, {"text": "\u2022 Reduced number of conditions -Previous workshops had many conditions: 10 language pairs, both in-domain and out-ofdomain translation, and three types of manual evaluation.", "labels": [], "entities": []}, {"text": "This year we eliminated the in-domain Europarl test set and defined sentence-level ranking as the primary type of manual evaluation.", "labels": [], "entities": [{"text": "Europarl test set", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.9865459402402242}]}, {"text": "\u2022 Editing to evaluate translation qualityBeyond ranking the output of translation systems, we evaluated translation quality by having people edit the output of systems.", "labels": [], "entities": []}, {"text": "Later, we asked annotators to judge whether those edited translations were correct when shown the source and reference translation.", "labels": [], "entities": []}, {"text": "The primary objectives of this workshop are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8413358926773071}, {"text": "machine translation", "start_pos": 237, "end_pos": 256, "type": "TASK", "confidence": 0.8272717595100403}]}, {"text": "All of the data, translations, and human judgments produced for our workshop are publicly available.", "labels": [], "entities": []}, {"text": "We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.690511037906011}, {"text": "system combination", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7302986085414886}]}], "datasetContent": [{"text": "As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.", "labels": [], "entities": []}, {"text": "It is our contention: The number of items that were judged for each task during the manual evaluation. that automatic measures are an imperfect substitute for human assessment of translation quality.", "labels": [], "entities": []}, {"text": "Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics.", "labels": [], "entities": []}, {"text": "Manual evaluation is time consuming, and it requires a large effort to conduct it on the scale of our workshop.", "labels": [], "entities": [{"text": "Manual evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7027135193347931}]}, {"text": "We distributed the workload across a number of people, including shared-task participants, interested volunteers, and a small number of paid annotators.", "labels": [], "entities": []}, {"text": "More than 160 people participated in the manual evaluation, with 100 people putting in more than an hour's worth of effort, and 30 putting in more than four hours.", "labels": [], "entities": []}, {"text": "A collective total of 479 hours of labor was invested.", "labels": [], "entities": []}, {"text": "We asked people to evaluate the systems' output in two different ways: \u2022 Ranking translated sentences relative to each other.", "labels": [], "entities": []}, {"text": "This was our official determinant of translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8743048310279846}]}, {"text": "\u2022 Editing the output of systems without displaying the source or a reference translation, and then later judging whether edited translations were correct.", "labels": [], "entities": []}, {"text": "The total number of judgments collected for the different modes of annotation is given in.", "labels": [], "entities": []}, {"text": "In all cases, the output of the various translation outputs were judged on equal footing; the output of system combinations was judged alongside that of the individual system, and the constrained and unconstrained systems were judged together.", "labels": [], "entities": []}, {"text": "In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating the automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "Last year, NIST began running a similar \"Metrics for MAchine TRanslation\" challenge (Metrics-MATR), and presented their findings at a workshop at AMTA (.", "labels": [], "entities": [{"text": "NIST", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.9156412482261658}, {"text": "AMTA", "start_pos": 146, "end_pos": 150, "type": "DATASET", "confidence": 0.9264984726905823}]}, {"text": "In this year's shared task we evaluated a number of different automatic metrics: \u2022 Bleu ()-Bleu remains the de facto standard in machine translation evaluation.", "labels": [], "entities": [{"text": "Bleu ()-Bleu", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.8715325991312662}, {"text": "machine translation evaluation", "start_pos": 129, "end_pos": 159, "type": "TASK", "confidence": 0.8736533522605896}]}, {"text": "It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as away of capturing some of the allowable variation in translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9830080270767212}]}, {"text": "We use a single reference translation in our experiments.", "labels": [], "entities": []}, {"text": "\u2022 Meteor (Agarwal and Lavie, 2008)-Meteor measures precision and recall for unigrams and applies a fragmentation penalty.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9986685514450073}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9984075427055359}]}, {"text": "It uses flexible word matching based on stemming and WordNet-synonymy.", "labels": [], "entities": [{"text": "word matching", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7128341048955917}, {"text": "WordNet-synonymy", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9343372583389282}]}, {"text": "meteor-ranking is optimized for correlation with ranking judgments.", "labels": [], "entities": []}, {"text": "\u2022 Translation Error Rate ()-TER calculates the number of edits required to change a hypothesis translation into a reference translation.", "labels": [], "entities": [{"text": "Translation Error Rate", "start_pos": 2, "end_pos": 24, "type": "METRIC", "confidence": 0.6048561930656433}, {"text": "TER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.6129624843597412}]}, {"text": "The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words.", "labels": [], "entities": []}, {"text": "\u2022 MaxSim (Chan and Ng, 2008)-MaxSim calculates a similarity score by comparing items in the translation against the reference.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 49, "end_pos": 65, "type": "METRIC", "confidence": 0.9525347948074341}]}, {"text": "Unlike most metrics which do strict matching, MaxSim computes a similarity score for non-identical items.", "labels": [], "entities": []}, {"text": "To find a maximum weight matching that matches each system item to at most one reference item, the items are then modeled as nodes in a bipartite graph.", "labels": [], "entities": []}, {"text": "\u2022 wcd6p4er ()-a measure based on cder with word-based substitution costs.", "labels": [], "entities": []}, {"text": "also submitted two contrastive metrics: bleusp4114, a modified version of BLEU-S (), with tuned n-gram weights, and bleusp, with constant weights.", "labels": [], "entities": [{"text": "bleusp4114", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.8951088190078735}, {"text": "BLEU-S", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9965707063674927}, {"text": "bleusp", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9199490547180176}]}, {"text": "wcd6p4er is an error measure and bleusp is a quality score.", "labels": [], "entities": [{"text": "error measure", "start_pos": 15, "end_pos": 28, "type": "METRIC", "confidence": 0.9702104926109314}, {"text": "bleusp", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9578920006752014}, {"text": "quality score", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9423195719718933}]}, {"text": "\u2022 RTE ()-The RTE metric follows a semantic approach which applies recent work in rich textual entailment to the problem of MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.9790283739566803}]}, {"text": "Its predictions are based on a regression model over a feature set adapted from an entailment systems.", "labels": [], "entities": []}, {"text": "The features primarily model alignment quality and (mis-)matches of syntactic and semantic structures.", "labels": [], "entities": []}, {"text": "\u2022 ULC ()-ULC is an arithmetic mean over other automatic metrics.", "labels": [], "entities": [{"text": "ULC ()-ULC", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.7253722747166952}]}, {"text": "The set of metrics used include Rouge, Meteor, measures of overlap between constituent parses, dependency parses, semantic roles, and discourse representations.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.6897076368331909}]}, {"text": "The ULC metric had the strongest correlation with human judgments in WMT08).", "labels": [], "entities": [{"text": "WMT08", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8179429173469543}]}, {"text": "\u2022 wpF and wpBleu (Popovic and Ney, 2009) -These metrics are based on words and part of speech sequences.", "labels": [], "entities": []}, {"text": "wpF is an n-gram based Fmeasure which takes into account both word n-grams and part of speech n-grams.", "labels": [], "entities": [{"text": "wpF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9224854707717896}, {"text": "Fmeasure", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8092308640480042}]}, {"text": "wp-BLEU is a combnination of the normal Blue score and apart of speech-based Bleu score.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.8254004418849945}]}, {"text": "\u2022 SemPOS () -the Sem-POS metric computes overlapping words, as defined in (, with respect to their semantic part of speech.", "labels": [], "entities": []}, {"text": "Moreover, it does not use the surface representation of words but their underlying forms obtained from the TectoMT framework.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The number of items that were judged for each task during the manual evaluation.", "labels": [], "entities": []}, {"text": " Table 4: Inter-and intra-annotator agreement for  the two types of manual evaluation", "labels": [], "entities": []}, {"text": " Table 7: The system-level correlation of the au- tomatic evaluation metrics with the human judg- ments for translation into English.", "labels": [], "entities": []}, {"text": " Table 8: The system-level correlation of the au- tomatic evaluation metrics with the human judg- ments for translation out of English.", "labels": [], "entities": [{"text": "translation out of English", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.8809747844934464}]}, {"text": " Table 9: The system-level correlation for auto- matic metrics ranking five English-Czech systems", "labels": [], "entities": []}, {"text": " Table 10: Sentence-level consistency of the auto- matic metrics with human judgments for transla- tions into English. Italicized numbers fall below  the random-choice baseline.", "labels": [], "entities": []}, {"text": " Table 11: Sentence-level consistency of the auto- matic metrics with human judgments for transla- tions out of English. Italicized numbers fall below  the random-choice baseline.", "labels": [], "entities": []}, {"text": " Table 12: Consistency of the automatic met- rics when their system-level ranks are treated as  sentence-level scores. Oracle shows the consis- tency of using the system-level human ranks that  are given in", "labels": [], "entities": []}, {"text": " Table 13: Consistency of the automatic met- rics when their system-level ranks are treated as  sentence-level scores. Oracle shows the consis- tency of using the system-level human ranks that  are given in", "labels": [], "entities": []}, {"text": " Table 14: Sentence-level ranking for the WMT09 German-English News Task", "labels": [], "entities": [{"text": "WMT09 German-English News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9155187010765076}]}, {"text": " Table 15: Sentence-level ranking for the WMT09 English-German News Task", "labels": [], "entities": [{"text": "WMT09 English-German News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9285535216331482}]}, {"text": " Table 16: Sentence-level ranking for the WMT09 Spanish-English News Task", "labels": [], "entities": [{"text": "WMT09 Spanish-English News Task", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.9356339126825333}]}, {"text": " Table 17: Sentence-level ranking for the WMT09 English-Spanish News Task", "labels": [], "entities": [{"text": "WMT09 English-Spanish News Task", "start_pos": 42, "end_pos": 73, "type": "DATASET", "confidence": 0.9452704787254333}]}, {"text": " Table 18: Sentence-level ranking for the WMT09 French-English News Task", "labels": [], "entities": [{"text": "WMT09 French-English News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9269029200077057}]}, {"text": " Table 19: Sentence-level ranking for the WMT09 English-French News Task", "labels": [], "entities": [{"text": "WMT09 English-French News Task", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9428203254938126}]}, {"text": " Table 20: Sentence-level ranking for the WMT09 Czech-English News Task", "labels": [], "entities": [{"text": "WMT09 Czech-English News Task", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.9231450855731964}]}, {"text": " Table 21: Sentence-level ranking for the WMT09 English-Czech News Task", "labels": [], "entities": [{"text": "WMT09 English-Czech News Task", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.9475086033344269}]}, {"text": " Table 22: Sentence-level ranking for the WMT09 Hungarian-English News Task", "labels": [], "entities": [{"text": "WMT09 Hungarian-English News Task", "start_pos": 42, "end_pos": 75, "type": "DATASET", "confidence": 0.9107519686222076}]}, {"text": " Table 23: Sentence-level ranking for the WMT09 All-English News Task", "labels": [], "entities": [{"text": "WMT09 All-English News Task", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.9042049050331116}]}, {"text": " Table 24: Sentence-level ranking for the WMT09 Multisource-English News Task", "labels": [], "entities": [{"text": "WMT09 Multisource-English News Task", "start_pos": 42, "end_pos": 77, "type": "DATASET", "confidence": 0.8727903664112091}]}]}