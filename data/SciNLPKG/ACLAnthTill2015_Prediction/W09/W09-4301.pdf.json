{"title": [{"text": "Summarizing Blog Entries versus News Texts", "labels": [], "entities": [{"text": "Summarizing Blog Entries", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9142012993494669}]}], "abstractContent": [{"text": "As more and more people are expressing their opinions on the web in the form of weblogs (or blogs), research on the blogosphere is gaining popularity.", "labels": [], "entities": []}, {"text": "As the outcome of this research, different natural language tools such as query-based opinion summarizers have been developed to mine and organize opinions on a particular event or entity in blog entries.", "labels": [], "entities": [{"text": "query-based opinion summarizers", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7210063139597574}]}, {"text": "However , the variety of blog posts and the informal style and structure of blog entries pose many difficulties for these natural language tools.", "labels": [], "entities": []}, {"text": "In this paper, we identify and categorize errors which typically occur in opinion summarization from blog entries and compare blog entry summaries with traditional news text summaries based on these error types to quantify the differences between these two genres of texts for the purpose of summariza-tion.", "labels": [], "entities": [{"text": "opinion summarization from blog entries", "start_pos": 74, "end_pos": 113, "type": "TASK", "confidence": 0.8443956136703491}]}, {"text": "For evaluation, we used summaries from participating systems of the TAC 2008 opinion summarization track and updated summa-rization track.", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization track", "start_pos": 68, "end_pos": 104, "type": "DATASET", "confidence": 0.8195652723312378}]}, {"text": "Our results show that some errors are much more frequent to blog entries (e.g. topic irrelevant information) compared to news texts; while other error types, such as content overlap, seem to be comparable.", "labels": [], "entities": []}, {"text": "These findings can be used to prioritize these error types and give clear indications as to where we should put effort to improve blog summarization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Everyday, people express their opinions on a variety of topics ranging from politics, movies, music to newly launched products on the web in weblogs (or blogs), wikis, online-forums, review sites, and social networking web sites.", "labels": [], "entities": []}, {"text": "As more and more people are expressing their opinions on the web, the Internet is becoming a popular and dynamic source of opinions.", "labels": [], "entities": []}, {"text": "Natural language tools for automatically mining and organizing these opinions on various events will be very useful for individuals, organizations, and governments.", "labels": [], "entities": [{"text": "automatically mining and organizing these opinions on various events", "start_pos": 27, "end_pos": 95, "type": "TASK", "confidence": 0.7558111283514235}]}, {"text": "Various natural language tools to process and utilize event-related information from texts have already been developed.", "labels": [], "entities": []}, {"text": "Event-based question answering systems and event-based summarization systems are only a few examples.", "labels": [], "entities": [{"text": "Event-based question answering", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5639175077279409}, {"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.656950056552887}]}, {"text": "However, most of the event-based systems have been developed to process events from traditional news texts.", "labels": [], "entities": []}, {"text": "Blog entries are different in style and structure compared to news texts.", "labels": [], "entities": []}, {"text": "As a result, successful natural language approaches that deal with news texts might not be as successful for processing blog entries; thus adaptation of existing successful NLP approaches for news texts to process blog entries is an interesting and challenging task.", "labels": [], "entities": []}, {"text": "The first step towards this adaptation is to identify the differences between these two textual genres in order to develop approaches to handle this new genre of texts (blogs) with greater accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9871353507041931}]}, {"text": "In this study, we compare automatically generated summaries of blog entries with summaries of news texts with the goal of improving opinion summarization from blog entries.", "labels": [], "entities": []}, {"text": "In particular, we compared summaries for these two genres of texts on the basis of various errors which typically occur in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 123, "end_pos": 136, "type": "TASK", "confidence": 0.9640975594520569}]}, {"text": "In this paper, we first investigate what kind of errors typically occur in query-based opinionated summary for blog entries.", "labels": [], "entities": []}, {"text": "The errors that we have identified are categorized and then used to compare blog summaries with news texts summaries.", "labels": [], "entities": []}, {"text": "For evaluation, we used summaries from participating systems at the TAC 2008 opinion summarization track and updated summarization track.", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization track", "start_pos": 68, "end_pos": 104, "type": "DATASET", "confidence": 0.8310469508171081}]}, {"text": "Summaries of the TAC 2008 opinion summarization track and updated summarization track were generated from blogs entries and traditional news texts, respectively.", "labels": [], "entities": [{"text": "TAC 2008 opinion summarization", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.815687745809555}]}, {"text": "The systems participating in the TAC opinion summarization track and in the updated summarization track are quite different in several aspects, as they are targeted to resolve two different tasks.", "labels": [], "entities": [{"text": "TAC opinion summarization track", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7051297426223755}]}, {"text": "The systems participating in the updated summarization track were mainly required to find the answers to given queries and detect redundant information while the systems participating in the opinion summarization track were required to perform opinion mining and polarity classification in addition.", "labels": [], "entities": [{"text": "summarization track", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8850874900817871}, {"text": "opinion mining", "start_pos": 244, "end_pos": 258, "type": "TASK", "confidence": 0.7817176282405853}, {"text": "polarity classification", "start_pos": 263, "end_pos": 286, "type": "TASK", "confidence": 0.7624406814575195}]}, {"text": "Moreover, the systems participating in the opinion summarization track were provided optional snippets (described in section 3.1) and were restricted with a maximum summary length which were much higher compared to the updated summarization track.", "labels": [], "entities": [{"text": "summary length", "start_pos": 165, "end_pos": 179, "type": "METRIC", "confidence": 0.8073264062404633}]}, {"text": "Despite these differences, these two datasets were used in our work because they are the most comparable datasets for our task.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of blog summaries use the same criteria as for traditional news text summarization.", "labels": [], "entities": [{"text": "news text summarization", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.8060502211252848}]}, {"text": "The quality of a summary is assessed mostly on its content and linguistic quality.", "labels": [], "entities": []}, {"text": "Content evaluation of a querybased summary is performed based on the relevance assessment (with the topic and query) and inclusion of important contents from the input documents.", "labels": [], "entities": []}, {"text": "Currently, the automatic evaluation tool ROUGE is the most popular evaluation approach for content evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9126260876655579}, {"text": "content evaluation", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7621189951896667}]}, {"text": "ROUGE automatically compares system generated summaries with a set of model summaries (human generated) by computing n-gram word overlaps between them.", "labels": [], "entities": []}, {"text": "Conferences and workshops such as TAC and DUC (Document Understanding Conference) use ROUGE.", "labels": [], "entities": [{"text": "TAC", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.5344281792640686}]}, {"text": "The pyramid method is also used for content evaluation.", "labels": [], "entities": [{"text": "content evaluation", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8687565624713898}]}, {"text": "In the pyramid method, multiple human generated summaries are analyzed manually to generated a gold standard.", "labels": [], "entities": []}, {"text": "In this process, summary analysis is done semantically such that information with the same meaning (expressed using different wording) is marked as summary content unit (SCU).", "labels": [], "entities": [{"text": "summary analysis", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.938585489988327}]}, {"text": "A weight is assigned for each SCU based on the number of human summarizers that express it in their summaries.", "labels": [], "entities": []}, {"text": "In this method, the pyramid score fora system generated summary is calculated as follows: score = (the sum of weights of SCUs expressed in a generated summary) / (the sum of weights of an ideally informative summary with the same number of SCUs) The linguistic quality of a summary is evaluated manually based on how it structures and presents the contents.", "labels": [], "entities": []}, {"text": "Grammaticality, non-redundancy, referential clarity, focus, structure and coherence are the commonly used factors considered to evaluate the linguistic quality.", "labels": [], "entities": [{"text": "Grammaticality", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.836180567741394}]}, {"text": "Mainly, subjective evaluation is done to assess the linguistic quality of an automatically generated summary.", "labels": [], "entities": []}, {"text": "In this process, human assessors directly assign scores on a scale based on agreement or disagreement with predefined set of questions such as \"Are they ungrammatical?", "labels": [], "entities": []}, {"text": "\", \"Do they contain redundant information?", "labels": [], "entities": []}, {"text": "\". The assessments are done without reference to any model summaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average TAC-2008 Summarization Results  -Blogs vs. News Texts", "labels": [], "entities": [{"text": "TAC-2008 Summarization", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.5734632462263107}]}, {"text": " Table 2: Summary-Level Errors -Blogs vs. News  Texts", "labels": [], "entities": [{"text": "Summary-Level Errors -Blogs vs. News  Texts", "start_pos": 10, "end_pos": 53, "type": "TASK", "confidence": 0.6795340861592974}]}, {"text": " Table 3: Sentence-Level Errors -Blogs vs. News  Texts", "labels": [], "entities": []}, {"text": " Table 4: Intra-Sentence-Level Errors -Blogs vs.  News Texts", "labels": [], "entities": []}]}