{"title": [{"text": "Workshop Adaptation of Language Resources and Technology to New Domains", "labels": [], "entities": []}], "abstractContent": [{"text": "This extended abstract focuses on the main points we will be touching upon during our talk, the aim of which is to present in a concise manner our group's work on enhancing robustness of lexicalised grammars for real-life applications and thus also on enabling their adaptation to new domains in its entirety.", "labels": [], "entities": []}], "introductionContent": [{"text": "At present, various wide-coverage symbolic parsing systems for different languages exist and have been integrated into real-world NLP applications, such as IE, QA, grammar checking, MT and intelligent IR.", "labels": [], "entities": [{"text": "symbolic parsing", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7380602061748505}, {"text": "IE", "start_pos": 156, "end_pos": 158, "type": "TASK", "confidence": 0.9368693232536316}, {"text": "grammar checking", "start_pos": 164, "end_pos": 180, "type": "TASK", "confidence": 0.8116620182991028}, {"text": "MT", "start_pos": 182, "end_pos": 184, "type": "TASK", "confidence": 0.9816628098487854}]}, {"text": "This integration, though, has reminded us of the shortcomings of symbolic systems, in particular lack of coverage, one consequence of which relates to enormous and sometimes insurmountable difficulties with porting and re-using such systems to new domains.", "labels": [], "entities": []}, {"text": "When the hand-crafted grammars which usually lie at the heart of symbolic parsing systems are applied to naturally occurring text, we often find that they are underperforming.", "labels": [], "entities": [{"text": "symbolic parsing", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.6990611702203751}]}, {"text": "Typical sources of coverage deficiency include unknown words, words for which the dictionary did not contain the relevant category, Multiword Expressions (MWEs), but also more general grammatical knowledge, such as grammar rules and word ordering constraints.", "labels": [], "entities": []}, {"text": "Currently, grammars and their accompanying lexica often need to be extended manually.", "labels": [], "entities": []}, {"text": "In this talk, we offer the overview to a range of machine learning-based methods which enable us to derive linguistic knowledge from corpora, for instance, in order to solve problems of coverage and efficiency deficiency of large-scale lexicalised grammars, ensuring this way their portability and re-usability and aiming at domain-independent linguistic processing.", "labels": [], "entities": []}, {"text": "In particular, we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and ensuring maintainability and re-usability of lexicalised grammars.", "labels": [], "entities": []}, {"text": "To this effect, we focus on enhancing robustness and ensuring maintainability and re-usability for two largescale \"deep\" grammars, one of English (ERG;) and one of German (GG;), developed in the framework of Head-driven Phrase Structure Grammar (HPSG).", "labels": [], "entities": [{"text": "Head-driven Phrase Structure Grammar (HPSG)", "start_pos": 208, "end_pos": 251, "type": "TASK", "confidence": 0.6497269230229514}]}, {"text": "Specifically, we show that the incorporation of detailed linguistic information into the process of automatic extension of the lexicon of such language resources enhances their performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammars when employed in practical real-life applications.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}