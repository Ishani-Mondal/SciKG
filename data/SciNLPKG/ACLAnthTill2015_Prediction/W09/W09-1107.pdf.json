{"title": [{"text": "A Method for Stopping Active Learning Based on Stabilizing Predictions and the Need for User-Adjustable Stopping", "labels": [], "entities": [{"text": "Stopping Active Learning", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8412698904673258}]}], "abstractContent": [{"text": "A survey of existing methods for stopping active learning (AL) reveals the needs for methods that are: more widely applicable; more aggressive in saving annotations; and more stable across changing datasets.", "labels": [], "entities": [{"text": "stopping active learning (AL)", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.9048052430152893}]}, {"text": "A new method for stopping AL based on stabilizing predictions is presented that addresses these needs.", "labels": [], "entities": [{"text": "stopping AL", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.7907933592796326}]}, {"text": "Furthermore, stopping methods are required to handle abroad range of different annota-tion/performance tradeoff valuations.", "labels": [], "entities": []}, {"text": "Despite this, the existing body of work is dominated by conservative methods with little (if any) attention paid to providing users with control over the behavior of stopping methods.", "labels": [], "entities": []}, {"text": "The proposed method is shown to fill a gap in the level of aggressiveness available for stopping AL and supports providing users with control over stopping behavior.", "labels": [], "entities": [{"text": "stopping AL", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.8590422868728638}]}], "introductionContent": [{"text": "The use of Active Learning (AL) to reduce NLP annotation costs has generated considerable interest recently (e.g.).", "labels": [], "entities": []}, {"text": "To realize the savings in annotation efforts that AL enables, we must have a mechanism for knowing when to stop the annotation process. is intended to motivate the value of stopping at the right time.", "labels": [], "entities": []}, {"text": "The x-axis measures the number of human annotations that have been requested and ranges from 0 to 70,000.", "labels": [], "entities": [{"text": "x-axis", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9872642755508423}]}, {"text": "The y-axis measures * This research was conducted while the first author was a PhD student at the University of Delaware.", "labels": [], "entities": []}, {"text": "performance in terms of F-Measure.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9969570636749268}]}, {"text": "As can be seen from the figure, the issue is that if we stop too early while useful generalizations are still being made, we windup with a lower performing system but if we stop too late after all the useful generalizations have been made, we just windup wasting human annotation effort.", "labels": [], "entities": []}, {"text": "The terms aggressive and conservative will be used throughout the rest of this paper to describe the behavior of stopping methods.", "labels": [], "entities": []}, {"text": "Conservative methods tend to stop further to the right in.", "labels": [], "entities": []}, {"text": "They are conservative in the sense that they're very careful not to risk losing significant amounts of Fmeasure, even if it means annotating many more examples than necessary.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.6250544190406799}]}, {"text": "Aggressive methods, on the other hand, tend to stop further to the left in.", "labels": [], "entities": []}, {"text": "They are aggressively trying to reduce unnecessary annotations.", "labels": [], "entities": []}, {"text": "There has been a flurry of recent work tackling the problem of automatically determining when to stop AL (see Section 2).", "labels": [], "entities": [{"text": "automatically determining when to stop AL", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.47421257197856903}]}, {"text": "There are three areas where this body of work can be improved: applicability Several of the leading methods are restricted to only being used in certain situations, e.g., they can't be used with some base learners, they have to select points in certain batch sizes during AL, etc.", "labels": [], "entities": [{"text": "applicability", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9329649806022644}]}, {"text": "(See Section 2 for discussion of the exact applicability constraints of existing methods.) lack of aggressive stopping The leading methods tend to find stop points that are too far to the right in.", "labels": [], "entities": []}, {"text": "(See Section 4 for empirical confirmation of this.) instability Some of the leading methods work well on some datasets but then can completely breakdown on other datasets, either stopping way too late and wasting enormous amounts of annotation effort or stopping way too early and losing large amounts of F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 305, "end_pos": 314, "type": "METRIC", "confidence": 0.89618319272995}]}, {"text": "(See Section 4 for empirical confirmation of this.)", "labels": [], "entities": []}, {"text": "This paper presents anew stopping method based on stabilizing predictions that addresses each of these areas and provides user-adjustable stopping behavior.", "labels": [], "entities": []}, {"text": "The essential idea behind the new method is to test the predictions of the recently learned models (during AL) on examples which don't have to be labeled and stop when the predictions have stabilized.", "labels": [], "entities": []}, {"text": "Some of the main advantages of the new method are that: it requires no additional labeled data, it's widely applicable, it fills a need fora method which can aggressively save annotations, it has stable performance, and it provides users with control over how aggressively/conservatively to stop AL.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 explains our Stabilizing Predictions (SP) stopping criterion in detail.", "labels": [], "entities": [{"text": "Stabilizing Predictions (SP) stopping", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.6329644471406937}]}, {"text": "Section 4 evaluates the SP method and discusses results.", "labels": [], "entities": [{"text": "SP", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9611201882362366}]}, {"text": "present stopping criteria based on the gradient of performance estimates and the gradient of confidence estimates.", "labels": [], "entities": [{"text": "stopping", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9338166117668152}]}, {"text": "Their technique with gradient of performance estimates is only applicable when probabilistic base learners are used.", "labels": [], "entities": []}, {"text": "The gradient of confidence estimates method is more generally applicable (e.g., it can be applied with our experiments where we use SVMs as the base learner).", "labels": [], "entities": []}, {"text": "This method, denoted by LS2008 in confidence over a window of recent points and when the gradient falls below a threshold, AL is stopped.", "labels": [], "entities": [{"text": "AL", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9969885945320129}]}], "datasetContent": [{"text": "We evaluate the Stabilizing Predictions (SP) stopping method on multiple datasets for Text Classification (TC) and Named Entity Recognition (NER) tasks.", "labels": [], "entities": [{"text": "Stabilizing Predictions (SP) stopping", "start_pos": 16, "end_pos": 53, "type": "TASK", "confidence": 0.7499671479066213}, {"text": "Text Classification (TC)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.8154233455657959}, {"text": "Named Entity Recognition (NER) tasks", "start_pos": 115, "end_pos": 151, "type": "TASK", "confidence": 0.8033517939703805}]}, {"text": "All of the datasets are freely and publicly available and have been used in many past works.", "labels": [], "entities": []}, {"text": "For Text Classification, we use two publicly available spam corpora: the spamassassin corpus used in and the TREC spam corpus trec05p-1/ham25 described in).", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7782075107097626}, {"text": "spamassassin corpus", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.7577921152114868}, {"text": "TREC spam corpus trec05p-1", "start_pos": 109, "end_pos": 135, "type": "DATASET", "confidence": 0.8106673806905746}]}, {"text": "For both of these corpora, the task is a binary classification task and we perform 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We also use the Reuters dataset, in particular the Reuters-21578 Distribution 1.0 ModApte split 2 . Since a document may belong to more than one category, each category is treated as a separate binary classification problem, as in.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9767252206802368}, {"text": "Reuters-21578 Distribution 1.0 ModApte split", "start_pos": 51, "end_pos": 95, "type": "DATASET", "confidence": 0.9540986180305481}]}, {"text": "Consistent with, results are reported for the ten largest categories.", "labels": [], "entities": []}, {"text": "Other TC datasets we use are the 20Newsgroups 3 newsgroup article classification and the WebKB web page classification datasets.", "labels": [], "entities": [{"text": "TC datasets", "start_pos": 6, "end_pos": 17, "type": "DATASET", "confidence": 0.8323964774608612}, {"text": "20Newsgroups 3 newsgroup article classification", "start_pos": 33, "end_pos": 80, "type": "TASK", "confidence": 0.8212468028068542}, {"text": "WebKB web page classification datasets", "start_pos": 89, "end_pos": 127, "type": "DATASET", "confidence": 0.9264489769935608}]}, {"text": "For WebKB, as in) we use the four largest categories.", "labels": [], "entities": [{"text": "WebKB", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9391455054283142}]}, {"text": "For all of our TC datasets, we use binary features for every word that occurs in the training data at least three times.", "labels": [], "entities": [{"text": "TC datasets", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.8231036067008972}]}, {"text": "For NER, we use the publicly available GENIA corpus . Our features, based on those from (), are surface features such as the words in the named entity and two words on each side, suffix information, and positional information.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8589844703674316}, {"text": "GENIA corpus", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9351581633090973}]}, {"text": "We assume a two-phase model where boundary identification has already been performed, as in ().", "labels": [], "entities": [{"text": "boundary identification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7562409341335297}]}, {"text": "SVMs deliver high performance for the datasets we use so we employ SVMs as our base learner in the bulk of our experiments (maximum entropy models are used in Subsection 4.3).", "labels": [], "entities": []}, {"text": "For selection of points to query, we use the approach that was used in () of selecting the points that are closest to the current hyperplane.", "labels": [], "entities": []}, {"text": "We use SVM light (Joachims, 1999) for training the SVMs.", "labels": [], "entities": []}, {"text": "For the smaller datasets (less than 50,000 examples in total), a batch size of 20 was used with an initial training set of size 100 and for the larger datasets (greater than 50,000 examples in total), a batch size of 200 was used with an initial training set of size 1000.", "labels": [], "entities": []}, {"text": "shows the results for all of our datasets.", "labels": [], "entities": []}, {"text": "For each dataset, we report the average number of annotations 5 requested by each of the stopping methods as well as the average F-measure achieved by each of the stopping methods.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9982170462608337}]}, {"text": "There are two facts worth keeping in mind.", "labels": [], "entities": []}, {"text": "First, the numbers in are averages and therefore, sometimes two methods could have very similar average numbers of annotations but wildly different average F-measures (because one of the methods was consistently stopping around its average whereas the other was stopping way too early and way too late).", "labels": [], "entities": [{"text": "F-measures", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9916130304336548}]}, {"text": "Second, sometimes a method with a higher average number of annotations has a lower Better evaluation metrics would use more refined measures of annotation effort than the number of annotations because not all annotations require the same amount of effort to annotate but lacking such a refined model for our datasets, we use number of annotations in these experiments.", "labels": [], "entities": []}, {"text": "Tests of statistical significance are performed using matched pairs t tests at a 95% confidence level.", "labels": [], "entities": []}, {"text": "All of the additional experiments in this subsection were conducted on our least computationally demanding dataset, Spamassassin.", "labels": [], "entities": [{"text": "Spamassassin", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.9616014361381531}]}, {"text": "The results in show how varying the intensity cutoff and the longevity requirement, respectively, of SP enable a user to control stopping behavior.", "labels": [], "entities": [{"text": "intensity cutoff", "start_pos": 36, "end_pos": 52, "type": "METRIC", "confidence": 0.9430674910545349}]}, {"text": "Both methods enable a user to adjust stopping in a controlled fashion (without radical changes in behavior).", "labels": [], "entities": []}, {"text": "Areas of future work include: combining the intensity and longevity methods for controlling behavior; and developing precise expectations on the change in behavior corresponding to changes in the intensity and longevity settings.", "labels": [], "entities": []}, {"text": "The results in show results for different stop set sizes.", "labels": [], "entities": []}, {"text": "Even with random selection of a stop set as small as 500, SP's performance holds fairly steady.", "labels": [], "entities": [{"text": "SP", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.8539838194847107}]}, {"text": "This plus the fact that random selection of stop sets of size 2000 worked across all the folds of all the datasets in show that in practice perhaps the simple heuristic of choosing a fairly large random set of points works well.", "labels": [], "entities": []}, {"text": "Nonetheless, we think the size necessary will depend on the dataset and other factors such as the feature representation so more principled methods of determining the size and/or the makeup of the stop set are an area for future: Controlling the behavior of stopping through the use of longevity.", "labels": [], "entities": []}, {"text": "For window length k longevity levels in {1, 2, 3, 4, 5, 6}, the 10-fold average number of annotations at the automatically determined stopping points and the 10-fold average F-measure at the automatically determined stopping points are displayed for the Spamassassin dataset.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9863895773887634}, {"text": "Spamassassin dataset", "start_pos": 254, "end_pos": 274, "type": "DATASET", "confidence": 0.9901220798492432}]}, {"text": "could be developed to create stop sets with high representativeness (in terms of feature space) density (meaning representativeness of stop set divided by size of stop set).", "labels": [], "entities": []}, {"text": "For example, a possibility is to cluster examples before AL begins and then make sure the stop set contains examples from each of the clusters.", "labels": [], "entities": []}, {"text": "Another possibility is to use a greedy algorithm where the stop set is iteratively grown whereon each iteration the center of mass of the stop set in feature space is computed and an example in the unlabeled pool that is maximally far in feature space from this center of mass is selected for inclusion in the stop set.", "labels": [], "entities": []}, {"text": "This could be useful for efficiency (in terms of getting the same stopping performance with a smaller stop set as could be achieved with a larger stop set) and also as away to ensure adequate representation of the task space.", "labels": [], "entities": []}, {"text": "The latter can be accom-: Methods for stopping AL with maximum entropy as the base learner.", "labels": [], "entities": []}, {"text": "For each stopping method, the average number of annotations at the automatically determined stopping point and the average F-measure at the automatically determined stopping point are displayed.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9961638450622559}]}, {"text": "Bold entries are statistically significantly different than SP (and non-bold entries are not).", "labels": [], "entities": []}, {"text": "SC2000, the margin exhaustion method, is not shown since it can't be used with a non-margin-based learner.", "labels": [], "entities": [{"text": "SC2000", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.91337651014328}, {"text": "margin exhaustion", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7907033860683441}]}, {"text": "The final column (labeled \"All\") represents standard fully supervised passive learning with the entire set of training data.", "labels": [], "entities": []}, {"text": "Stop  plished by perhaps continuing to add examples to the stop set until adding new examples is no longer increasing the representativeness of the stop set.", "labels": [], "entities": []}, {"text": "As one of the advantages of SP is that it's widely applicable, shows the results when using maximum entropy models as the base learner during AL (the query points selected are those which the model is most uncertain about).", "labels": [], "entities": [{"text": "SP", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9728436470031738}]}, {"text": "The results reinforce our conclusions from the SVM experiments, with SP performing aggressively and all statistically significant differences in performance being in SP's favor.", "labels": [], "entities": []}, {"text": "shows the graph fora representative fold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Controlling the behavior of stopping through the  use of intensity. For Kappa intensity levels in {97.0, 97.5,  98.0, 98.5, 99.0, 99.5}, the 10-fold average number of an- notations at the automatically determined stopping points  and the 10-fold average F-measure at the automatically  determined stopping points are displayed for the Spamas- sassin dataset.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 264, "end_pos": 273, "type": "METRIC", "confidence": 0.9912309646606445}, {"text": "Spamas- sassin dataset", "start_pos": 345, "end_pos": 367, "type": "DATASET", "confidence": 0.8444459438323975}]}, {"text": " Table 3: Controlling the behavior of stopping through the  use of longevity. For window length k longevity levels in  {1, 2, 3, 4, 5, 6}, the 10-fold average number of annota- tions at the automatically determined stopping points and  the 10-fold average F-measure at the automatically deter- mined stopping points are displayed for the Spamassassin  dataset.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9919689297676086}, {"text": "Spamassassin  dataset", "start_pos": 338, "end_pos": 359, "type": "DATASET", "confidence": 0.9888051748275757}]}, {"text": " Table 4: Investigating the sensitivity to stop set size. For  stop set sizes in {2500, 2000, 1500, 1000, 500}, the 10- fold average number of annotations at the automatically  determined stopping points and the 10-fold average F- measure at the automatically determined stopping points  are displayed for the Spamassassin dataset.", "labels": [], "entities": [{"text": "F- measure", "start_pos": 228, "end_pos": 238, "type": "METRIC", "confidence": 0.9910920461018881}, {"text": "Spamassassin dataset", "start_pos": 310, "end_pos": 330, "type": "DATASET", "confidence": 0.9889626801013947}]}]}