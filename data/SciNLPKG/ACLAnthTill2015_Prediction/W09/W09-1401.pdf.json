{"title": [{"text": "Overview of BioNLP'09 Shared Task on Event Extraction", "labels": [], "entities": [{"text": "Event Extraction", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.6492575407028198}]}], "abstractContent": [{"text": "The paper presents the design and implementation of the BioNLP'09 Shared Task, and reports the final results with analysis.", "labels": [], "entities": [{"text": "BioNLP'09 Shared Task", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.517798900604248}]}, {"text": "The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity.", "labels": [], "entities": [{"text": "bio-molecular event extraction", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.6204102536042532}]}, {"text": "The data was developed based on the GENIA event corpus.", "labels": [], "entities": [{"text": "GENIA event corpus", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.9598555564880371}]}, {"text": "The shared task was run over 12 weeks, drawing initial interest from 42 teams.", "labels": [], "entities": []}, {"text": "Of these teams, 24 submitted final results.", "labels": [], "entities": []}, {"text": "The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges.", "labels": [], "entities": []}], "introductionContent": [{"text": "The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC, TREC and ACE () events, have significantly contributed to the progress of their respective fields.", "labels": [], "entities": [{"text": "text mining (TM)", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.9180320262908935}, {"text": "MUC, TREC and ACE", "start_pos": 125, "end_pos": 142, "type": "DATASET", "confidence": 0.6840309739112854}]}, {"text": "This has also been the casein bio-TM.", "labels": [], "entities": []}, {"text": "Examples include the TREC Genomics track), JNLPBA (), LLL, and BioCreative (.", "labels": [], "entities": [{"text": "TREC Genomics track", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.7140800356864929}, {"text": "JNLPBA", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.818946897983551}]}, {"text": "While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules.", "labels": [], "entities": [{"text": "information retrieval)", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7926693558692932}, {"text": "named entity recognition)", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.7799437642097473}, {"text": "information extraction)", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.8309683005015055}]}, {"text": "With the emergence of NER systems with performance capable of supporting practical applications, the recent interest of the bio-TM community is shifting toward IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.9855304956436157}]}, {"text": "Similarly to LLL and BioCreative, the BioNLP'09 Shared Task (the BioNLP task, hereafter) also addresses bio-IE, but takes a definitive step further toward finer-grained IE.", "labels": [], "entities": []}, {"text": "While LLL and BioCreative focus on a rather simple representation of relations of bio-molecules, i.e. protein-protein interactions (PPI), the BioNLP task concerns the detailed behavior of bio-molecules, characterized as bio-molecular events (bio-events).", "labels": [], "entities": []}, {"text": "The difference in focus is motivated in part by different applications envisioned as being supported by the IE methods.", "labels": [], "entities": []}, {"text": "For example, BioCreative aims to support curation of PPI databases such as MINT (, fora longtime one of the primary tasks of bioinformatics.", "labels": [], "entities": []}, {"text": "The BioNLP task aims to support the development of more detailed and structured databases, e.g. pathway () or Gene Ontology Annotation (GOA)) databases, which are gaining increasing interest in bioinformatics research in response to recent advances in molecular biology.", "labels": [], "entities": []}, {"text": "As the first shared task of its type, the BioNLP task aimed to define a bounded, well-defined bioevent extraction task, considering both the actual needs and the state of the art in bio-TM technology and to pursue it as a community-wide effort.", "labels": [], "entities": [{"text": "bioevent extraction task", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.7931522826353709}]}, {"text": "The key challenge was in finding a good balance between the utility and the feasibility of the task, which was also limited by the resources available.", "labels": [], "entities": []}, {"text": "Special consideration was given to providing evaluation at diverse levels and aspects, so that the results can drive continuous efforts in relevant directions.", "labels": [], "entities": []}, {"text": "The paper discusses the design and implementation of the BioNLP task, and reports the results with analysis.", "labels": [], "entities": [{"text": "BioNLP task", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.5813277065753937}]}, {"text": "T(P/Ev), C(P/Ev) Site, CSite Positive regulation T(P/Ev), C(P/Ev) Site, CSite Negative regulation T(P/Ev), C(P/Ev) Site, CSite: Event types and their arguments.", "labels": [], "entities": []}, {"text": "The type of the filler entity is specified in parenthesis.", "labels": [], "entities": []}, {"text": "The filler entity of the secondary arguments are all of Entity type which represents any entity but proteins: T=Theme, C=Cause, P=Protein, Ev=Event.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation, the participants were given the test data with gold annotation only for proteins.", "labels": [], "entities": []}, {"text": "The evaluation was then carried out by comparing the annotation predicted by each participant to the gold annotation.", "labels": [], "entities": []}, {"text": "For the comparison, equality of annotations is defined as described in Section 4.1.", "labels": [], "entities": [{"text": "equality", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9780323505401611}]}, {"text": "The evaluation results are reported using the standard recall/precision/f-score metrics, under different criteria defined through the equalities.", "labels": [], "entities": [{"text": "recall/precision/f-score metrics", "start_pos": 55, "end_pos": 87, "type": "METRIC", "confidence": 0.7645642558733622}]}, {"text": "Various evaluation modes can be defined by varying equivalence criteria.", "labels": [], "entities": []}, {"text": "In the following, we describe three fundamental variants applied in the evaluation.", "labels": [], "entities": []}, {"text": "Strict matching The strict matching mode requires exact equality, as defined in section 4.1.", "labels": [], "entities": [{"text": "Strict matching", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7013038098812103}]}, {"text": "As some of its requirements maybe viewed as unnecessarily precise, practically motivated relaxed variants, described in the following, are also applied.", "labels": [], "entities": []}, {"text": "Approximate span matching The approximate span matching mode is defined by relaxing the requirement for text span matching for t-entities.", "labels": [], "entities": [{"text": "Approximate span matching", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6120607852935791}, {"text": "approximate span matching", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6516981025536855}, {"text": "text span matching", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.6184556285540262}]}, {"text": "Specifically, a given span is equivalent to a gold span if it is entirely contained within an extension of the gold span by one word both to the left and to the right, that is, beg1 \u2265 ebeg2 and end1 \u2264 eend2, where (beg1, end1) is the given span and (ebeg2, eend2) is the extended gold span.", "labels": [], "entities": []}, {"text": "Approximate recursive matching In strict matching, fora regulation event to be correct, the events it refers to as theme or cause must also be be strictly correct.", "labels": [], "entities": [{"text": "Approximate", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.971510112285614}]}, {"text": "The approximate recursive matching mode is defined by relaxing the requirement for recursive event matching, so that an event can match even if the events it refers to are only partially correct.", "labels": [], "entities": [{"text": "recursive event matching", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6755991180737814}]}, {"text": "Specifically, for partial matching, only Theme arguments are considered: events can match even if referred events differ in non-Theme arguments.", "labels": [], "entities": []}, {"text": "The final evaluation results of Task 1 are shown in.", "labels": [], "entities": []}, {"text": "The results on the five event types involv-   ing only a single primary theme argument are shown in one merged class, \"Simple Event\".", "labels": [], "entities": []}, {"text": "The broad performance range (31% -70%) indicates even the extraction of simple events is not a trivial task.", "labels": [], "entities": []}, {"text": "However, the top-ranked systems show encouraging performance, achieving or approaching 70% f-score.", "labels": [], "entities": [{"text": "f-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9895360469818115}]}, {"text": "The performance ranges for Binding (5% -44%) and Regulation (1% -40%) events show their extraction to be clearly more challenging.", "labels": [], "entities": [{"text": "Binding", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9696678519248962}, {"text": "Regulation", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.7819258570671082}]}, {"text": "It is interesting that while most systems show better performance for binding over regulation events, the systems and are better for regulation, showing somewhat reduced performance for Binding events.", "labels": [], "entities": []}, {"text": "This is in particular contrast to the following two systems, and, which show far better performance for Binding than Regulation events.", "labels": [], "entities": [{"text": "Binding", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.975752055644989}]}, {"text": "As one possible explanation, we find that the latter two differentiate binding events by their number of themes, while the former two give no specific treatment to multi-theme binding events.", "labels": [], "entities": []}, {"text": "Such observations and comparisons area clear benefit of a community-wide shared task.", "labels": [], "entities": []}, {"text": "shows the evaluation results for the teams who participated in Task 2.", "labels": [], "entities": []}, {"text": "The \"All\" column shows the overall performance of the systems for Task 2, while the \"All Second Args.\" column shows the performance of finding only the secondary arguments.", "labels": [], "entities": [{"text": "All Second Args.\" column", "start_pos": 85, "end_pos": 109, "type": "METRIC", "confidence": 0.7735955476760864}]}, {"text": "The evaluation results show considerable differences between the criteria.", "labels": [], "entities": []}, {"text": "For example, the system shows performance comparable to the top ranked system in finding secondary arguments, although its overall performance for Task 2 is more limited.", "labels": [], "entities": []}, {"text": "show performance at a practical level in particular in finding specific sites of phosphorylation.", "labels": [], "entities": []}, {"text": "As shown in, the performance range for Task 3 is very low although the representation of the task is as simple as the simple events.", "labels": [], "entities": []}, {"text": "We attribute the reason to the fact that Task 3 is the only task of which the annotation is not bound to textual clue, thus no text-bound annotation was provided.", "labels": [], "entities": []}, {"text": "shows a scatter plot of the performance of the participating systems during the system development period.", "labels": [], "entities": []}, {"text": "The performance evaluation comes from the log of the online evaluation system on the development data.", "labels": [], "entities": []}, {"text": "It shows the best performance and the average performance of the participating systems were trending upwards up until the deadline of final submission, which indicates there is still much potential for improvement.", "labels": [], "entities": []}, {"text": "shows experimental results of a system ensemble using the final submissions.", "labels": [], "entities": []}, {"text": "For the experiments, the top 3-10 systems were chosen, and the output of each system treated as a weighted vote . Three weighting schemes were used; \"Equal\" weights each vote equally; \"Averaged\" weights each We used the 'ensemble' function of U-Compare.", "labels": [], "entities": []}, {"text": "vote by the overall f-score of the system; \"Event Type\" weights each vote by the f-score of the system for the specific event type.", "labels": [], "entities": []}, {"text": "The best score, 55.96%, was obtained by the \"Event Type\" weighting scheme, showing a 4% unit improvement over the best individual system.", "labels": [], "entities": []}, {"text": "While using the final scores for weighting uses data that would not be available in practice, similar weighting could likely be obtained e.g. using performance on the development data.", "labels": [], "entities": []}, {"text": "The experiment demonstrates that an f-score better than 55% can be achieved simply by combining the strengths of the systems.", "labels": [], "entities": [{"text": "f-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9546589255332947}]}], "tableCaptions": [{"text": " Table 2: Statistics of the data sets.  For events,  Task1/Task2 shown separately as secondary arguments  may introduce additional differentiation of events.", "labels": [], "entities": []}, {"text": " Table 4: Profiles of the participants: GTag=GENIAtagger, MLN=Markov Logic Network, UMLS=UMLS SPE- CIALIST Lexicon/tools, MC=McClosky-Charniak, GDep=Genia Dependency Parser, Stanford=Stanford Parser,  CBR=Case-Based Reasoning, CM=ConceptMapper.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation results of Task 1 (recall / precision / f-score).", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9989249110221863}, {"text": "precision / f-score", "start_pos": 49, "end_pos": 68, "type": "METRIC", "confidence": 0.699986477692922}]}, {"text": " Table 6: Evaluation results for Task 2.", "labels": [], "entities": []}, {"text": " Table 7: Evaluation results for Task 3.", "labels": [], "entities": []}, {"text": " Table 8: Experimental results of system ensemble.", "labels": [], "entities": []}]}