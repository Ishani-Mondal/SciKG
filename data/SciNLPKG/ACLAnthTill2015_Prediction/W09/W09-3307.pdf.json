{"title": [{"text": "Construction of Disambiguated Folksonomy Ontologies Using Wikipedia", "labels": [], "entities": []}], "abstractContent": [{"text": "One of the difficulties in using Folk-sonomies in computational systems is tag ambiguity: tags with multiple meanings.", "labels": [], "entities": []}, {"text": "This paper presents a novel method for building Folksonomy tag ontologies in which the nodes are disambiguated.", "labels": [], "entities": []}, {"text": "Our method utilizes a clustering algorithm called DSCBC, which was originally developed in Natural Language Processing (NLP), to derive committees of tags, each of which corresponds to one meaning or domain.", "labels": [], "entities": []}, {"text": "In this work, we use Wikipedia as the external knowledge source for the domains of the tags.", "labels": [], "entities": []}, {"text": "Using the committees , an ambiguous tag is identified as one which belongs to more than one committee.", "labels": [], "entities": []}, {"text": "Then we apply a hierarchical agglom-erative clustering algorithm to build an on-tology of tags.", "labels": [], "entities": []}, {"text": "The nodes in the derived ontology are disambiguated in that an ambiguous tag appears in several nodes in the ontology, each of which corresponds to one meaning of the tag.", "labels": [], "entities": []}, {"text": "We evaluate the derived ontology for its ontological density (how close similar tags are placed), and its usefulness in applications, in particular fora personalized tag retrieval task.", "labels": [], "entities": [{"text": "personalized tag retrieval task", "start_pos": 153, "end_pos": 184, "type": "TASK", "confidence": 0.683522991836071}]}, {"text": "The results showed marked improvements over other approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, there has been a rapid growth in social tagging systems -so-called Folksonomies where users assign keywords or tags to categorize resources.", "labels": [], "entities": []}, {"text": "Typically, the sources of folksonomies are web resources, and virtually any kind of information available on the Internet, ranging from web pages (e.g. Delicious (delicious.com)), scientific articles (e.g. Bibsonomy (www.bibsonomy.org)) to media resources (e.g. Flickr (www.flickr.com), Last.fm (www.last.fm)).", "labels": [], "entities": [{"text": "Last.fm", "start_pos": 287, "end_pos": 294, "type": "DATASET", "confidence": 0.7649734020233154}]}, {"text": "Although tags in folksonomies are essentially semantic concepts, they have distinct characteristics as compared to conventional semantic resources which are often used in Natural Language Processing (NLP), such as WordNet.", "labels": [], "entities": []}, {"text": "First, folksonomy tags are unrestricted -users are free to choose any words or set of characters to formulate tags.", "labels": [], "entities": []}, {"text": "One significant problem arising from such free-formedness is tag ambiguity: tags that have several meanings (e.g. \"Java\" as coffee or a programming language or an island in Indonesia).", "labels": [], "entities": []}, {"text": "Second, folksonomy tags are unstructured -tags assigned to a given resource are simply enumerated in a list (although oftentimes using a varying font size to indicate popularity), and no special organization or categorization of the tags is made (by the Folksonomy site).", "labels": [], "entities": []}, {"text": "There have been several work recently which extracted structures from folksonomy tags and constructed ontologies (e.g. (), ().", "labels": [], "entities": []}, {"text": "However, most of them evaluate the effect of the extracted structures only in the context of specific applications, for instance generating user recommendations (e.g. ().", "labels": [], "entities": []}, {"text": "In this work, we develop a novel method for constructing ontologies from folksonomy tags.", "labels": [], "entities": []}, {"text": "In particular, we employ a clustering algorithm called Domain Similarity Clustering By Committee (DSCBC) (.", "labels": [], "entities": []}, {"text": "DSCBC is an extension of an algorithm called CBC (), and was originally developed for lexical semantics in NLP to automatically derive single/unambiguous word meanings (as committees) from ambiguous words.", "labels": [], "entities": [{"text": "DSCBC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9062859416007996}]}, {"text": "In this work, DSCBC is effectively adopted to derive disambiguated folksonomy tag committees, where a committee in this context is a cluster of tags in which the members share the same or very similar concept in one of their meanings.", "labels": [], "entities": []}, {"text": "By using DSCBC, an ambiguous tag is identified as one which belongs to more than one committee.", "labels": [], "entities": [{"text": "DSCBC", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.9424864053726196}]}, {"text": "One of the key ideas in DSCBC is the notion of feature domain similarity: the similarity between the features themselves, obtained a priori from sources external to the dataset used at hand.", "labels": [], "entities": []}, {"text": "For example, if data instances x and y are represented by features f1 and f2, the feature domain similarity refers to the similarity between f1 and f2 (not between x and y).", "labels": [], "entities": []}, {"text": "DSCBC utilizes this feature domain similarity to derive clusters whose domains are 'close', thereby producing unambiguous committees.", "labels": [], "entities": [{"text": "DSCBC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.956271767616272}]}, {"text": "In this work, we incorporate Wikipedia as the external knowledge resource, and use the similarity between Wikipedia articles to derive the committees of disambiguated tags.", "labels": [], "entities": []}, {"text": "Finally using the tag committees derived by DSCBC, we build an ontology of tags by using a modified hierarchical agglomerative clustering algorithm.", "labels": [], "entities": [{"text": "DSCBC", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.9746413826942444}]}, {"text": "Ambiguous tags are mapped to several nodes in this ontology.", "labels": [], "entities": []}, {"text": "Note that in this paper, we refer to the structure derived by the hierarchical clustering algorithm as an 'ontology' instead of a 'taxonomy'.", "labels": [], "entities": []}, {"text": "That is because, in the algorithm, the parent-child relation is determined by a similarity measure only, therefore sometimes does not correspond to the subsumption relation in the strict sense.", "labels": [], "entities": []}, {"text": "For evaluation, we construct an ontology from the Delicious tags, and measure the quality (ontological density) of the derived ontology by comparing with the ontologies obtained without using Wikipedia.", "labels": [], "entities": []}, {"text": "We also use the derived ontology in a personalized information retrieval task.", "labels": [], "entities": [{"text": "personalized information retrieval task", "start_pos": 38, "end_pos": 77, "type": "TASK", "confidence": 0.6783885657787323}]}, {"text": "The results show that our method achieved marked improvements over other approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied our proposed algorithm to data from a real-world social tagging system Delicious and derived a tag ontology.", "labels": [], "entities": []}, {"text": "Then we evaluated the derived ontology on two aspects: the density of the ontology, and the usefulness of the ontology in a personalized Information Retrieval (IR) task.", "labels": [], "entities": [{"text": "personalized Information Retrieval (IR) task", "start_pos": 124, "end_pos": 168, "type": "TASK", "confidence": 0.8097355280603681}]}, {"text": "Note that in the experiments, we determined the values for all tuning coefficients in the algorithms during the preliminary test runs.", "labels": [], "entities": []}, {"text": "We first crawled the Delicious site and obtained data consisting of 29,918 users, 6,403,442 resources and 1,035,177 tags.", "labels": [], "entities": [{"text": "Delicious site", "start_pos": 21, "end_pos": 35, "type": "DATASET", "confidence": 0.903544694185257}]}, {"text": "In this data, 47,184,492 annotations were made by just one user, or for one resource, or by one tag.", "labels": [], "entities": []}, {"text": "This distribution followed the Zipf's law -small numbers of tags were infrequent use and large numbers of tags were rarely used.", "labels": [], "entities": []}, {"text": "Our intuitions were that the effect of using the semantic/encyclopedia knowledge from Wikipedia would probably be better reflected in the low frequency \"long tail\" part of the Zipf's distribution rather than the high frequency part.", "labels": [], "entities": []}, {"text": "Likewise for users, we have discovered in our previous research that search personalization algorithms often produce different results for users with rich profiles and for users who have sparse profiles.", "labels": [], "entities": []}, {"text": "This problem is known as the \"Cold Start\" problem in search personalization: anew user has very little information/history in the profile, therefore the system cannot reliably infer his/her interests.", "labels": [], "entities": []}, {"text": "Since our experiments included a personalized IR task, we decided to extract two subsets from the data: one set containing high frequency tags assigned by users with rich profiles (randomly selected 1,000 most frequent tags entered by 100 high profile users), and another containing low frequency tags assigned by users with sparse profiles (randomly selected 1,000 least frequent tags entered by 100 sparse profile users).", "labels": [], "entities": [{"text": "IR task", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9000897109508514}]}, {"text": "We refer to the former set as the \"Frequent Set\" and the latter set as the \"Long Tail Set\".", "labels": [], "entities": [{"text": "Frequent Set", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.8005784153938293}, {"text": "Long Tail Set", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.8856915632883707}]}, {"text": "The total number of resources in each dataset was 16,635 and 3,356 respectively.", "labels": [], "entities": []}, {"text": "Then for both datasets, we applied a partof speech tagger to all resources and extracted all nouns (and discarded all other parts of speech).", "labels": [], "entities": []}, {"text": "We also applied the Porter Stemmer (tartarus.org/\u223cmartin/PorterStemmer) to eliminate terms with inflectional variations.", "labels": [], "entities": [{"text": "Porter Stemmer (tartarus.org/\u223cmartin/PorterStemmer)", "start_pos": 20, "end_pos": 71, "type": "DATASET", "confidence": 0.8789140118492974}]}, {"text": "Finally, we repre-sented each resource page as a vector of stemmed terms, and the values were term frequencies.", "labels": [], "entities": []}, {"text": "As for Wikipedia, we used its English version available from BitTorrent Network (www.bittorrent.com).", "labels": [], "entities": [{"text": "BitTorrent Network", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.875697910785675}]}, {"text": "The original data (the most recent dump, as of 24 July, 2008) contained 13,916,311 pages.", "labels": [], "entities": []}, {"text": "In order to reduce the size to make the computation feasible, we randomly chose 75,000 pages (which contained at least 50 words) and applied the Maximal Complete Link clustering algorithm to further reduce the size.", "labels": [], "entities": [{"text": "Maximal Complete Link clustering", "start_pos": 145, "end_pos": 177, "type": "TASK", "confidence": 0.5256020724773407}]}, {"text": "After clustering, we obtained a total of 43,876 clusters, most of which contained one or two Wiki articles, but some of which had several articles.", "labels": [], "entities": []}, {"text": "We call such a Wiki article cluster Wiki concept.", "labels": [], "entities": []}, {"text": "As with the tag datasets, for each Wiki article we applied the Porter Stemmer to reduce the number of the terms.", "labels": [], "entities": []}, {"text": "Then we represented each Wiki concept page as a vector of stemmed terms, and the values were term frequencies.", "labels": [], "entities": []}, {"text": "For the first evaluation, we evaluated the derived Delicious tag ontology directly by measuring the topological closeness of similar semantic concepts in the ontology.", "labels": [], "entities": []}, {"text": "To that end, we developed a notion of ontological density: all tags assigned to a specific resource should be located close to each other in the ontology.", "labels": [], "entities": []}, {"text": "For instance, a web resource java.sun.com in Delicious is assigned with various tags such as 'Java', 'Programming' and 'Technology'.", "labels": [], "entities": []}, {"text": "Those tags should be concentrated in one place rather than scattered over various sections in the ontology.", "labels": [], "entities": []}, {"text": "By measuring the distance as the number of edges in the ontology between tags assigned to a specific resource, we can obtain an estimate of the ontology density for the resource.", "labels": [], "entities": []}, {"text": "Then finding the average density of all resources can give us an approximation of the overall density of the ontology's quality.", "labels": [], "entities": []}, {"text": "But here a difficulty arises for ambiguous tags -when a tag is ambiguous and located in several places in the ontology.", "labels": [], "entities": []}, {"text": "In those cases, we chose the sense (an ontology node) which is the closest to the unambiguous tags assigned to the same resource.", "labels": [], "entities": []}, {"text": "For example, shows apart of the ontology where an ambiguous tag 'NLP' (with two senses) is mapped: 1) Natural Language Processing (the left one in the, and 2) Neurolinguistic programming (the right one in the figure).", "labels": [], "entities": []}, {"text": "The target web resource is tagged with three tags: two unambiguous tags 'POS' and 'Porter', and an ambiguous tag 'NLP'.", "labels": [], "entities": []}, {"text": "To identify the sense of 'NLP' for this resource, we count the number of edges from the two unambiguous tags ('POS', 'Porter') to both 'NLP' tag nodes, and select the one which has the shortest distance.", "labels": [], "entities": []}, {"text": "In the figure, the first sense has the total distance of 4 (= 2 edges from 'Pos' + 2 edges from 'Porter'), while the second sense has the distance 10 (= 5 edges from 'Pos' + 5 edges from 'Porter').", "labels": [], "entities": []}, {"text": "Therefore, we select the first sense ('Natural Language Processing') as the meaning of 'NLP' for this resource.", "labels": [], "entities": []}, {"text": "Formally we define the density of the ontology T for the set of resources R, denoted Dens(T, R), as the average density overall resources in R, as follows.", "labels": [], "entities": []}, {"text": "We computed the density value for the ontology derived by our approach ('Ontology Enhanced with Wiki Concepts') and compared with the ontologies obtained by using only the resources (where a tag vector is presented by the stemmed terms in the resources to which the tag is assigned), and only the tags (where a tag vector is presented by the resource to which they were assigned).", "labels": [], "entities": []}, {"text": "show the results, for the two datasets.", "labels": [], "entities": []}, {"text": "For both datasets, the differences between the three ontologies were statistically significant (at p=0.05), indicating that the encyclopedia knowledge obtained from Wikipedia was indeed effective in deriving a semantically dense ontology.", "labels": [], "entities": []}, {"text": "Here, one observation is that the relative improvement was more significant for the \"Frequent Set\" than the \"Long Tail Set\".", "labels": [], "entities": [{"text": "Frequent Set", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.8124617636203766}, {"text": "Long Tail Set", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.8241296410560608}]}, {"text": "The reason is because frequent tags are generally more ambiguous than less frequent tags (as with words in general), therefore the effect of tag disambiguation by DSCBC was more salient, relatively, for the frequent tags.", "labels": [], "entities": [{"text": "tag disambiguation", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.716064989566803}, {"text": "DSCBC", "start_pos": 163, "end_pos": 168, "type": "DATASET", "confidence": 0.9399576783180237}]}, {"text": "For the second evaluation, we used the derived Delicious ontology in an IR task and measured its utility.", "labels": [], "entities": [{"text": "IR task", "start_pos": 72, "end_pos": 79, "type": "TASK", "confidence": 0.9107617139816284}]}, {"text": "In particular, we personalized the search results fora given user by utilizing the tag ontology as away to present the user profile and infer his/her information needs.", "labels": [], "entities": []}, {"text": "Using the derived ontology, we search in the ontology for the query tag entered by a specific user.", "labels": [], "entities": []}, {"text": "We first match the ontology with the user's profile and derive a score distribution for the nodes in the tree which reflects the user's general interest.", "labels": [], "entities": []}, {"text": "To do so, we take each tag in the user's profile as the initial activation point, then spread the activation up and down the ontology tree, for all tags.", "labels": [], "entities": []}, {"text": "To spread activation from a given node, we use two parameters: decay factor, which determines the amount of the interest to be transfered to the parent/child of the current node; and damping threshold -if the interest score becomes less than this value we stop further iteration.", "labels": [], "entities": [{"text": "decay factor", "start_pos": 63, "end_pos": 75, "type": "METRIC", "confidence": 0.9661390483379364}]}, {"text": "Thus the resulting score distribution of the tree is effectively personalized to the user's general interest.", "labels": [], "entities": []}, {"text": "Using the obtained score distribution of a given user, we search the tree fora query tag (of this user).", "labels": [], "entities": []}, {"text": "In the same way as the tags in the profile, we spread activation over the ontology from the node to which the tag belongs, but this time we add a weight to emphasize the relative importance of the query tag compared to the tags from the profile, because the query reflects the user's current information needs.", "labels": [], "entities": []}, {"text": "Finally we feed the preference vector to the modified FolkRank algorithm () to retrieve and rank the relevant web resources which reflect the user-specific preferences.", "labels": [], "entities": []}, {"text": "shows the overall scheme of the personalized ranked retrieval using an ontological user profile.", "labels": [], "entities": []}], "tableCaptions": []}