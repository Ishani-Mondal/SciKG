{"title": [{"text": "WikiWalk: Random walks on Wikipedia for Semantic Relatedness", "labels": [], "entities": [{"text": "Semantic Relatedness", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7923559546470642}]}], "abstractContent": [{"text": "Computing semantic relatedness of natural language texts is a key component of tasks such as information retrieval and sum-marization, and often depends on knowledge of abroad range of real-world concepts and relationships.", "labels": [], "entities": [{"text": "Computing semantic relatedness of natural language texts", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.8862506832395282}, {"text": "information retrieval", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.7889383137226105}]}, {"text": "We address this knowledge integration issue by computing semantic relatedness using person-alized PageRank (random walks) on a graph derived from Wikipedia.", "labels": [], "entities": [{"text": "knowledge integration", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7407651245594025}]}, {"text": "This paper evaluates methods for building the graph, including link selection strategies, and two methods for representing input texts as distributions over the graph nodes: one based on a dictionary lookup, the other based on Explicit Semantic Analysis.", "labels": [], "entities": [{"text": "link selection", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.6805226802825928}]}, {"text": "We evaluate our techniques on standard word relatedness and text similarity datasets, finding that they capture similarity information complementary to existing Wikipedia-based relatedness measures, resulting in small improvements on a state-of-the-art measure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many problems in NLP call for numerical measures of semantic relatedness, including document summarization, information retrieval, and textual entailment.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.6408577859401703}, {"text": "information retrieval", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.8018123507499695}, {"text": "textual entailment", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7303941547870636}]}, {"text": "Often, measuring the relatedness of words or text passages requires world knowledge about entities and concepts that are beyond the scope of any single word in the document.", "labels": [], "entities": []}, {"text": "Consider, for instance, the following pair: 1.", "labels": [], "entities": []}, {"text": "Gettysburg Address To correctly assess that these examples are related requires knowledge of the United States Civil War found neither in the examples themselves nor in traditional lexical resources such as WordNet.", "labels": [], "entities": [{"text": "Gettysburg Address", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9444673359394073}, {"text": "WordNet", "start_pos": 207, "end_pos": 214, "type": "DATASET", "confidence": 0.9651081562042236}]}, {"text": "Fortunately, a massive collaboratively constructed knowledge resource is available that has specific articles dedicated to both.", "labels": [], "entities": []}, {"text": "Wikipedia is an online encyclopedia containing around one million articles on a wide variety of topics maintained by over one hundred thousand volunteer editors with quality comparable to that of traditional encyclopedias.", "labels": [], "entities": []}, {"text": "Recent work has shown that Wikipedia can be used as the basis of successful measures of semantic relatedness between words or text passages (;.", "labels": [], "entities": []}, {"text": "The most successful measure, Explicit Semantic Analysis (ESA) (, treats each article as its own dimension in a vector space.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis (ESA)", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.6970097770293554}]}, {"text": "Texts are compared by first projecting them into the space of Wikipedia articles and then comparing the resulting vectors.", "labels": [], "entities": []}, {"text": "In addition to article text, Wikipedia stores a great deal of information about the relationships between the articles in the form of hyperlinks, info boxes, and category pages.", "labels": [], "entities": []}, {"text": "Despite along history of research demonstrating the effectiveness of incorporating link information into relatedness measures based on the WordNet graph), previous work on Wikipedia has made limited use of this relationship information, using only category links () or just the actual links in a page (.", "labels": [], "entities": [{"text": "WordNet graph", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.9723008275032043}]}, {"text": "In this work, we combine previous approaches by converting Wikipedia into a graph, mapping input texts into the graph, and performing random walks based on Personalized PageRank) to obtain stationary distributions that characterize each text.", "labels": [], "entities": []}, {"text": "Semantic relatedness between two texts is computed by comparing their distributions.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we explore the use of all these link types when con-structing the Wikipedia graph, the intuition being these links, or some combination of them, contain additional information that would allow again over methods that use only just the article text.", "labels": [], "entities": [{"text": "Wikipedia graph", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.9208811819553375}]}, {"text": "We also discuss two methods for performing the initial mapping of input texts to the graph, using techniques from previous studies that utilized WordNet graphs and Wikipedia article text.", "labels": [], "entities": []}, {"text": "We find that performance is signficantly affected by the strategy used to initialize the graph walk, as well as the links selected when constructing the Wikipedia graph.", "labels": [], "entities": []}, {"text": "Our best system combines an ESA-initialized vector with random walks, improving on state-of-the-art results over the ( dataset.", "labels": [], "entities": []}, {"text": "An analysis of the output demonstrates that, while the gains are small, the random walk adds complementary relatedness information not present in the page text.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the two methods of initialization as well as several types of edges.", "labels": [], "entities": []}, {"text": "For a set of pairs, system performance is evaluated by how well the generated scores correlate with the gold scores.", "labels": [], "entities": []}, {"text": "tion coefficient on the () documentpair dataset.", "labels": [], "entities": [{"text": "documentpair dataset", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.8430888652801514}]}], "tableCaptions": [{"text": " Table 1: Graph and dictionary sizes. Avg. Articles  column details the average number of articles per  entry.", "labels": [], "entities": [{"text": "Avg", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.8972519636154175}]}, {"text": " Table 3: Spearman correlation on the WordSim- 353 dataset with dictionary-based initialization.  Refer to Section 3 for explanation of dictionary  and graph building methods. Between parenthe- sis, results excluding pairs which had a word with  an empty dictionary entry.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7437857687473297}, {"text": "WordSim- 353 dataset", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.9458773583173752}]}, {"text": " Table 5: Pearson correlation on the (Lee et al.,  2005) dataset when walking on various types of  links. Note that walking tends to hurt performance  overall, with Infobox links by far the least harm- ful.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9144282937049866}]}, {"text": " Table 6: Pearson correlation on the (", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8714970648288727}]}, {"text": " Table 8: Statistics for first concept match length,  by run and walk type.", "labels": [], "entities": []}, {"text": " Table 9: Return probability vs. correlation, on tex- tual similarity data (", "labels": [], "entities": [{"text": "Return probability", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.94595006108284}, {"text": "correlation", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9317219853401184}]}]}