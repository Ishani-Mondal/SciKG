{"title": [{"text": "Effective Analysis of Causes and Inter-dependencies of Parsing Errors", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose two methods for analyzing errors in parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9528862237930298}]}, {"text": "One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement.", "labels": [], "entities": []}, {"text": "The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model.", "labels": [], "entities": []}, {"text": "The first method uses patterns of errors to associate them with categories of causes for those errors, such as errors in scope determination of coordination, PP-attachment, identification of antecedent of relative clauses, etc.", "labels": [], "entities": [{"text": "scope determination of coordination", "start_pos": 121, "end_pos": 156, "type": "TASK", "confidence": 0.7746360749006271}, {"text": "identification of antecedent of relative clauses", "start_pos": 173, "end_pos": 221, "type": "TASK", "confidence": 0.8382130662600199}]}, {"text": "On the other hand, the second method, which is based on re-parsing with one of observed errors corrected , assesses inter-dependencies among errors by examining which other errors were to be corrected as a result if a specific error was corrected.", "labels": [], "entities": []}, {"text": "Experiments show that these two methods are complementary and by being combined , they can provide useful clues as to how to improve a given grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "In any kind of complex systems, analyzing causes of errors is a crucial step for improving its performance.", "labels": [], "entities": []}, {"text": "In recent sophisticated parsing technologies, the step of error analysis has been becoming more and more convoluted and time-consuming, if not impossible.", "labels": [], "entities": [{"text": "parsing", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9658358693122864}, {"text": "error analysis", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.753906786441803}]}, {"text": "While common performance evaluation measures such as F-values are useful to compare the performance of systems or evaluate improvement of a system, they hardly give useful clues as to how to improve a system.", "labels": [], "entities": [{"text": "F-values", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9915803074836731}]}, {"text": "Evaluation measures usually assume uniform units such as the number of correctly or incorrectly recognized constituent boundaries and their labels, or in a similar vein, dependency links among words and their labels, and then compute single values such as the Fvalue.", "labels": [], "entities": [{"text": "Fvalue", "start_pos": 260, "end_pos": 266, "type": "METRIC", "confidence": 0.8789201378822327}]}, {"text": "These values do not give any insights as to where the weaknesses exist in a parsing model.", "labels": [], "entities": []}, {"text": "As a result, the improvement process takes the form of time consuming trial-error cycles.", "labels": [], "entities": []}, {"text": "Once grammar developers know the actual distribution of errors across different categories such as PP-attachment, complement/adjunct distinction, gerund/participle distinction, etc., they can think of focused and systematic improvement of a parsing model.", "labels": [], "entities": []}, {"text": "Another problem of the F-value in terms of uniform units is that it does not take interdependencies among errors into consideration.", "labels": [], "entities": [{"text": "F-value", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9913021326065063}]}, {"text": "In particular, for parsers based on grammar formalisms such as LFG (, HPSG, or CCG), units (eg. single predicate-argument links) are inter-related through hierarchical structures and structure sharing assumed by these formalisms.", "labels": [], "entities": []}, {"text": "Single errors are inherently propagated to other sets of errors.", "labels": [], "entities": []}, {"text": "This is also the case, though to a lesser extent, for parsing models in which shallow parsing is followed by another component for semantic label assignment.", "labels": [], "entities": [{"text": "semantic label assignment", "start_pos": 131, "end_pos": 156, "type": "TASK", "confidence": 0.6467540065447489}]}, {"text": "In order to address these two issues, we propose two methods in this paper.", "labels": [], "entities": []}, {"text": "One is to recognize cause categories of errors and the other is to capture inter-dependencies among errors.", "labels": [], "entities": []}, {"text": "The former method defines various patterns of errors to identify categories of error causes.", "labels": [], "entities": []}, {"text": "The latter method re-parses a sentence with a single target error corrected, and regards the errors which are corrected in re-parse as errors dependent on the target.", "labels": [], "entities": []}, {"text": "Although these two methods are implemented fora specific parser using HPSG), the same ideas can be applied to any type of parsing models.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9348689317703247}]}, {"text": "Word: has Grammatical nature: auxiliary # of arguments: 2", "labels": [], "entities": []}], "datasetContent": [{"text": "A parser is a system which interprets given sentences in terms of structures derived from syntactic or in some cases semantic viewpoints, and structures constructed as a result are used as essential information for various tasks of natural language processing such as information extraction, machine translation, and soon.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 268, "end_pos": 290, "type": "TASK", "confidence": 0.7865698635578156}, {"text": "machine translation", "start_pos": 292, "end_pos": 311, "type": "TASK", "confidence": 0.8230876922607422}]}, {"text": "In this paper, we address issues involved in improving the performance of a parser which produces structural representations deeper than surface constituent structures.", "labels": [], "entities": []}, {"text": "Such a parser is called a \"deep parser.\"", "labels": [], "entities": []}, {"text": "In many deep parsers, the output structure is defined by a linguistics-based grammar framework such as CFG, CCG), LFG: Descriptions for predicate types.", "labels": [], "entities": []}, {"text": "Alternatively, some deep parsing models assume staged processing in which a stage of shallow parsing is followed by a stage of semantic role labeling, which assigns labels indicating semantic relationships between predicates and their arguments.", "labels": [], "entities": []}, {"text": "In either case, we assume a parser to produce a single \"deep\" structural representation fora given sentence, which is chosen from a set of possible interpretations as the most probable one by a disambiguation model.", "labels": [], "entities": []}, {"text": "For evaluation of the performance of a parser, various metrics have been introduced according to the structure captured by a given grammar formalism or a system of semantic labels.", "labels": [], "entities": []}, {"text": "In most cases, instead of examining correctness fora whole structure, a parser is evaluated in terms of the F-value which shows how correctly it recognizes relationships among words and assigns \"labels\" to the relationships in the structure.", "labels": [], "entities": [{"text": "F-value", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9856767654418945}]}, {"text": "In this paper, we assume a certain type of \"predicateargument relation.\"", "labels": [], "entities": []}, {"text": "In this measurement, a structure given fora sentence is decomposed into a set of predicative words and their arguments.", "labels": [], "entities": []}, {"text": "A predicate takes other words as its arguments.", "labels": [], "entities": []}, {"text": "In our representation, the arguments are labeled by semantically neutral labels such as ARGn(n = 1...5) and MOD.", "labels": [], "entities": [{"text": "ARGn", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9781508445739746}, {"text": "MOD", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.925175130367279}]}, {"text": "In this representation, a basic unit is a triplet, such as <Predicate:PredicateType, ArgumentLabel, Argument>, where \"Predicate\" and \"Argument\" are surface words.", "labels": [], "entities": [{"text": "Argument", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9351805448532104}]}, {"text": "As shown in the examples in Section 4, \"PredicateType\" bears extra information concerning the syntactic construction in which the triplet is embedded.", "labels": [], "entities": []}, {"text": "ARG1-ARG5 express relations between a Head and its complement, while MOD expresses a relation between an Adjunct and its modifiee.", "labels": [], "entities": []}, {"text": "Since all dependency relations are expressed by triplets, triplets contain not only semantic de-: An example of parsing performance evaluations pendencies but also many dependencies which are essentially syntactic in nature.", "labels": [], "entities": [{"text": "parsing performance evaluations pendencies", "start_pos": 112, "end_pos": 154, "type": "TASK", "confidence": 0.8545282483100891}]}, {"text": "shows an example used in and.", "labels": [], "entities": []}, {"text": "This example shows predicate-argument relations for \"John has come.\"", "labels": [], "entities": []}, {"text": "There are two predicates in this sentence, \"has\" and \"come\".", "labels": [], "entities": []}, {"text": "The word \"has\", which is used as an auxiliary verb, takes two words, \"John\" and \"come\", as its arguments, and therefore two triplets of predicateargument relation, <has ARG1 John> and <has ARG2 come>.", "labels": [], "entities": []}, {"text": "As for the predicative word \"come\", we have one triplet <come ARG1 John>.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.6825327277183533}]}, {"text": "Note that, in this HPSG analysis, the auxiliary verb \"has\" is analyzed in such away that it takes one NP as subject and one VP as complement, and that the subject of the auxiliary verb is shared by the verb (\"come\") in VP as its subject.", "labels": [], "entities": []}, {"text": "The fact that \"has\" in this sentence is an auxiliary verb is indicated by the \"PredicateType\", aux 2args.", "labels": [], "entities": [{"text": "PredicateType", "start_pos": 79, "end_pos": 92, "type": "METRIC", "confidence": 0.9255808591842651}]}, {"text": "A \"PredicateType\" consists of a type and the number of arguments it takes).", "labels": [], "entities": []}, {"text": "shows an example of the evaluation of the parser based on these predicate-argument relations.", "labels": [], "entities": []}, {"text": "Note that the predicate types are abbreviated in this figure.", "labels": [], "entities": []}, {"text": "In the sentence \"I saw a girl with a telescope\", there should be four triplets for the two predicates, \"saw\" and \"with,\" each of which takes Figure 5: Parsing errors around one relative clause attachment two arguments.", "labels": [], "entities": []}, {"text": "Although the parser output does indeed contain four triplets, the first argument of \"with\" is not the correct one.", "labels": [], "entities": []}, {"text": "Thus, this output is erroneous, with the F-value of 75%.", "labels": [], "entities": [{"text": "F-value", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.999362051486969}]}, {"text": "We applied our methods to the analyses of actual errors produced by Enju.", "labels": [], "entities": [{"text": "Enju", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.8802574872970581}]}, {"text": "This version of Enju was trained on the Penn Treebank (Marcus et al., 1994) Section 2-21.", "labels": [], "entities": [{"text": "Penn Treebank (Marcus et al., 1994) Section 2-21", "start_pos": 40, "end_pos": 88, "type": "DATASET", "confidence": 0.912500565702265}]}], "tableCaptions": [{"text": " Table 3: Errors classified into cause categories", "labels": [], "entities": []}, {"text": " Table 5: Correction propagations between errors for each cause category and the other errors", "labels": [], "entities": []}, {"text": " Table 6: Summary of parsing performances for domain and model variations", "labels": [], "entities": []}, {"text": " Table 7: Error distributions for domain and model variations", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9394957423210144}]}]}