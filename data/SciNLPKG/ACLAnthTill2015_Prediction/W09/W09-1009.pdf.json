{"title": [{"text": "Upper Bounds for Unsupervised Parsing with Unambiguous Non-Terminally Separated Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference.", "labels": [], "entities": [{"text": "Unambiguous Non-Terminally Separated (UNTS) grammars", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7635713602815356}, {"text": "grammatical inference", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7029083669185638}]}, {"text": "However, these properties do not state the maximal performance they can achieve when they are evaluated against a gold treebank that is not produced by an UNTS grammar.", "labels": [], "entities": [{"text": "UNTS grammar", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.7458023428916931}]}, {"text": "In this paper we investigate such an upper bound.", "labels": [], "entities": []}, {"text": "We develop a method to find an upper bound for the unlabeled F 1 performance that any UNTS grammar can achieve over a given tree-bank.", "labels": [], "entities": []}, {"text": "Our strategy is to characterize all possible versions of the gold treebank that UNTS grammars can produce and to find the one that optimizes a metric we define.", "labels": [], "entities": [{"text": "UNTS", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.8166869282722473}]}, {"text": "We show away to translate this score into an upper bound for the F 1.", "labels": [], "entities": [{"text": "F 1", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9923923015594482}]}, {"text": "In particular, we show that the F 1 parsing score of any UNTS grammar cannot be beyond 82.2% when the gold treebank is the WSJ10 corpus .", "labels": [], "entities": [{"text": "F 1 parsing score", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.8582890033721924}, {"text": "UNTS grammar", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8485903143882751}, {"text": "WSJ10 corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.9881620407104492}]}], "introductionContent": [{"text": "Unsupervised learning of natural language has received a lot of attention in the last years, e.g.,, and.", "labels": [], "entities": []}, {"text": "Most of them use sentences from a treebank for training and trees from the same treebank for evaluation.", "labels": [], "entities": []}, {"text": "As such, the best model for unsupervised parsing is the one that reports the best performance.", "labels": [], "entities": []}, {"text": "Unambiguous Non-Terminally Separated (UNTS) grammars have properties that make them attractive for grammatical inference.", "labels": [], "entities": [{"text": "Unambiguous Non-Terminally Separated (UNTS) grammars", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7635713602815356}, {"text": "grammatical inference", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7029083669185638}]}, {"text": "These grammars have been shown to be PAC-learnable in polynomial time), meaning that under certain circumstances, the underlying grammar can be learned from a sample of the underlying language.", "labels": [], "entities": []}, {"text": "Moreover, UNTS grammars have been successfully used to induce grammars from unannotated corpora in competitions of learnability of formal languages.", "labels": [], "entities": []}, {"text": "UNTS grammars can be used for modeling natural language.", "labels": [], "entities": [{"text": "UNTS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.809883713722229}]}, {"text": "They can be induced using any training material, the induced models can be evaluated using trees from a treebank, and their performance can be compared against state-of-theart unsupervised models.", "labels": [], "entities": []}, {"text": "Different learning algorithms might produce different grammars and, consequently, different scores.", "labels": [], "entities": []}, {"text": "The fact that the class of UNTS grammars is PAC learnable does not convey any information on the possible scores that different UNTS grammars might produce.", "labels": [], "entities": []}, {"text": "From a performance oriented perspective it might be possible to have an upper bound over the set of possible scores of UNTS grammars.", "labels": [], "entities": []}, {"text": "Knowing an upper bound is complementary to knowing that the class of UNTS grammars is PAC learnable.", "labels": [], "entities": []}, {"text": "Such upper bound has to be defined specifically for UNTS grammars and has to take into account the treebank used as test set.", "labels": [], "entities": [{"text": "UNTS grammars", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.6413623988628387}]}, {"text": "The key question is how to compute it.", "labels": [], "entities": []}, {"text": "Suppose that we want to evaluate the performance of a given UNTS grammar using a treebank.", "labels": [], "entities": []}, {"text": "The candidate grammar produces a tree for each sentence and those trees are compared to the original treebank.", "labels": [], "entities": []}, {"text": "We can think that the candidate grammar has produced anew version of the treebank, and that the score of the grammar is a measure of the closeness of the new treebank to the original treebank.", "labels": [], "entities": []}, {"text": "Finding the best upper bound is equivalent to finding the closest UNTS version of the treebank to the original one.", "labels": [], "entities": []}, {"text": "Such bounds are difficult to find for most classes of languages because the search space is the set of all possible versions of the treebank that might have been produced by any grammar in the class understudy.", "labels": [], "entities": []}, {"text": "In order to make the problem tractable, we need the formalism to have an easy way to characterize all the versions of a treebank it might produce.", "labels": [], "entities": []}, {"text": "UNTS grammars have a special characterization that makes the search space easy to define but whose exploration is NP-hard.", "labels": [], "entities": [{"text": "UNTS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7801483869552612}]}, {"text": "In this paper we present away to characterize UNTS grammars and a metric function to measure the closeness between two different version of a treebank.", "labels": [], "entities": []}, {"text": "We show that the problem of finding the closest UNTS version of the treebank can be described as Maximum Weight Independent Set (MWIS) problem, a well known NP-hard problem.", "labels": [], "entities": [{"text": "Maximum Weight Independent Set (MWIS) problem", "start_pos": 97, "end_pos": 142, "type": "METRIC", "confidence": 0.8666949272155762}]}, {"text": "The exploration algorithm returns aversion of the treebank that is the closest to the gold standard in terms of our own metric.", "labels": [], "entities": []}, {"text": "We show that the F 1-measure is related to our measure and that it is possible to find and upper bound of the F 1-performance for all UNTS grammars.", "labels": [], "entities": [{"text": "UNTS", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.8293654918670654}]}, {"text": "Moreover, we compute this upper bound for the WSJ10, a subset of the Penn Treebank using POS tags as the alphabet.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9502989053726196}, {"text": "Penn Treebank", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9939812123775482}]}, {"text": "The upper bound we found is 82.2% for the F 1 measure.", "labels": [], "entities": [{"text": "F 1 measure", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9386936624844869}]}, {"text": "Our result suggest that UNTS grammars area formalism that has the potential to achieve state-of-the-art unsupervised parsing performance but does not guarantee that there exists a grammar that can actually achieve the 82.2%.", "labels": [], "entities": [{"text": "UNTS", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7096333503723145}]}, {"text": "To the best of our knowledge, there is no previous research on finding upper bounds for performance over a concrete class of grammars.", "labels": [], "entities": []}, {"text": "In, the authors compute an upper bound for parsing with binary trees a gold treebank that is not binary.", "labels": [], "entities": []}, {"text": "This upper bound, that is 88.1% for the WSJ10, is for any parser that returns binary trees, including the concrete models developed in the same work.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9528301358222961}]}, {"text": "But their upper bound does not use any specific information of the concrete models that may help them to find better ones.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents our characterization of UNTS grammars.", "labels": [], "entities": [{"text": "UNTS grammars", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.5711311399936676}]}, {"text": "Section 3 introduces the metric we optimized and explains how the closest version of the treebank is found.", "labels": [], "entities": []}, {"text": "Section 4 explains how the upper bound for our metric is translated to an upper bound of the F 1 score.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9729988972345988}]}, {"text": "Section 5 presents our bound for UNTS grammars using the WSJ10 and finally Section 6 concludes the paper.", "labels": [], "entities": [{"text": "UNTS grammars", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.5695914030075073}, {"text": "WSJ10", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9716118574142456}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Figures for the WSJ10 and its graph.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.9535832405090332}]}, {"text": " Table 2: Summary of the scores for C max .", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7703198194503784}]}, {"text": " Table 3: Performance on the WSJ10 of the most  recent unsupervised parsers, and our upper bounds  on UNTS.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9106845855712891}, {"text": "UNTS", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.872176468372345}]}]}