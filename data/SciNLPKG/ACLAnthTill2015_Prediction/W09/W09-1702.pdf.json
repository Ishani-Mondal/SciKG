{"title": [{"text": "Utilizing Contextually Relevant Terms in Bilingual Lexicon Extraction", "labels": [], "entities": [{"text": "Utilizing Contextually Relevant Terms in Bilingual Lexicon Extraction", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.6929488182067871}]}], "abstractContent": [{"text": "This paper demonstrates one efficient technique in extracting bilingual word pairs from non-parallel but comparable corpora.", "labels": [], "entities": [{"text": "extracting bilingual word pairs", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.8328300714492798}]}, {"text": "Instead of using the common approach of taking high frequency words to buildup the initial bilingual lexicon, we show contextually relevant terms that co-occur with cognate pairs can be efficiently utilized to build a bilingual dictionary.", "labels": [], "entities": []}, {"text": "The result shows that our models using this technique have significant improvement over baseline models especially when highest-ranked translation candidate per word is considered .", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual lexicons or dictionaries are invaluable knowledge resources for language processing tasks.", "labels": [], "entities": [{"text": "language processing tasks", "start_pos": 74, "end_pos": 99, "type": "TASK", "confidence": 0.7532645563284556}]}, {"text": "The compilation of such bilingual lexicons remains as a substantial issue to linguistic fields.", "labels": [], "entities": []}, {"text": "In general practice, many linguists and translators spend huge amounts of money and effort to compile this type of knowledge resources either manually, semiautomatically or automatically.", "labels": [], "entities": []}, {"text": "Thus, obtaining the data is expensive.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate a technique that utilizes contextually relevant terms that co-occur with cognate pairs to expand an initial bilingual lexicon.", "labels": [], "entities": []}, {"text": "We use unannotated resources that are freely available such as English-Spanish Europarl corpus ( and another different set of cognate pairs as seed words.", "labels": [], "entities": [{"text": "English-Spanish Europarl corpus", "start_pos": 63, "end_pos": 94, "type": "DATASET", "confidence": 0.7056183815002441}]}, {"text": "We show that this technique is able to achieve high precision score for bilingual lexicon extracted from non-parallel but comparable corpora.", "labels": [], "entities": [{"text": "precision score", "start_pos": 52, "end_pos": 67, "type": "METRIC", "confidence": 0.9508500695228577}]}, {"text": "Our model using this technique with spelling similarity approach obtains 85.4 percent precision at 50.0 percent recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9983170032501221}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9954028129577637}]}, {"text": "Precision of 79.0 percent at 50.0 percent recall is recorded when using this technique with context similarity approach.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9956321120262146}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.996733546257019}]}, {"text": "Furthermore, by using a string edit-distance vs. precision curve, we also reveal that the latter model is able to capture words efficiently compared to a baseline model.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9974033236503601}]}, {"text": "Section 2 is dedicated to mention some of the related works.", "labels": [], "entities": []}, {"text": "In Section 3, the technique that we used is explained.", "labels": [], "entities": []}, {"text": "Section 4 describes our experimental setup followed by the evaluation results in Section 5.", "labels": [], "entities": []}, {"text": "Discussion and conclusion are in Section 6 and 7 respectively.", "labels": [], "entities": []}, {"text": "describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features.", "labels": [], "entities": []}, {"text": "In reporting our work, we treat both identical word pairs and similar spelling word pairs as cognate pairs.", "labels": [], "entities": []}, {"text": "map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct.", "labels": [], "entities": []}, {"text": "They propose to restrict the word length, at least of length 6, to increase the accuracy of the collected word pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9988585710525513}]}, {"text": "mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio and string edit distance).", "labels": [], "entities": [{"text": "longest common subsequence ratio", "start_pos": 92, "end_pos": 124, "type": "METRIC", "confidence": 0.8765522539615631}, {"text": "string edit distance", "start_pos": 129, "end_pos": 149, "type": "METRIC", "confidence": 0.5573710898558298}]}, {"text": "However, point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair.", "labels": [], "entities": []}, {"text": "mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.999413251876831}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9979660511016846}]}, {"text": "Instead, they propose assigning a feature to each substring of length of three or less for each word.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted our evaluation lexicon from Word Reference * free online dictionary . For this work, the word types are not restricted but mostly are content words.", "labels": [], "entities": [{"text": "Word Reference * free online dictionary", "start_pos": 41, "end_pos": 80, "type": "DATASET", "confidence": 0.8828904728094736}]}, {"text": "We have two sets of evaluation.", "labels": [], "entities": []}, {"text": "In one, we take high ranked candidate pairs where W s could have multiple translations.", "labels": [], "entities": []}, {"text": "In the other, we only consider highest-ranked W t for each W s . For evaluation purposes, we take only the top 2000 candidate ranked-pairs from the output.", "labels": [], "entities": []}, {"text": "From that list, only candidate pairs with words found in the evaluation lexicon are proposed.", "labels": [], "entities": []}, {"text": "We use F1-measure to evaluate proposed lexicon against the evaluation lexicon.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9969481825828552}]}, {"text": "The recall is defined as the proportion of the high ranked candidate pairs.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9992761015892029}]}, {"text": "The precision is given as the number of correct candidate pairs divided by the total number of proposed candidate pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995286464691162}]}, {"text": "For the first evaluation, candidate pairs are ranked after being measured either with cosine for context similarity or edit distance for spelling similarity.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 119, "end_pos": 132, "type": "METRIC", "confidence": 0.8990162312984467}]}, {"text": "Using either context or spelling similarity approach on Sand T (labeled ECS and ESS respectively), our models achieved about 51.2 percent of best F1 measure.", "labels": [], "entities": [{"text": "ECS", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.7674212455749512}, {"text": "ESS", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9279046654701233}, {"text": "F1 measure", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.984582930803299}]}, {"text": "Those are not a significant improvement with only 1.0 to 2.0 percent error reduction over the baseline models (labeled CS and SS).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 69, "end_pos": 84, "type": "METRIC", "confidence": 0.9797199666500092}]}, {"text": "For the second evaluation, we take the first 2000 of {W s , W t } pairs where W s may only have the highest ranked W t as translation candidates (See).", "labels": [], "entities": []}, {"text": "This time, both of our models (with context similarity and spelling similarity, labeled ECST and ESST respectively) yielded almost 60.0 percent of best F1 measure.", "labels": [], "entities": [{"text": "ECST", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.6567379832267761}, {"text": "ESST", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9438149333000183}, {"text": "F1 measure", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9836090207099915}]}, {"text": "It is noted that using ESST alone recorded a significant improvement of 20.0 percent in the F1 score compared to SST baseline model.", "labels": [], "entities": [{"text": "ESST", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.838658332824707}, {"text": "F1 score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9863354563713074}]}, {"text": "ESST obtained 85.4 percent precision at 50.0 percent recall.", "labels": [], "entities": [{"text": "ESST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.839713454246521}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993641972541809}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9982398748397827}]}, {"text": "Precision of 79.0 percent at 50.0 percent recall is recorded when using ECST.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9976418018341064}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9967494010925293}, {"text": "ECST", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.681104302406311}]}, {"text": "However, the ECST has not recorded a significant difference over CST baseline model (57.1 and 52.6 percent respectively) in the second evaluation.", "labels": [], "entities": [{"text": "ECST", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.7258146405220032}]}, {"text": "The overall performances, represented by precision scores for different 14 It is important to seethe inner performance of the ECST model with further analysis.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.998818576335907}]}, {"text": "We present a string edit distance value (EDv) vs. precision curve for ECST and CST in to measure the performance of the ECST model in capturing bilingual pairs with less similar orthographic features, those that may not be captured using spelling similarity.", "labels": [], "entities": [{"text": "string edit distance value (EDv)", "start_pos": 13, "end_pos": 45, "type": "METRIC", "confidence": 0.7951880395412445}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9966232776641846}]}, {"text": "The graph in shows that even though CST has higher precision score than ECST at EDv of 2, it is not significant (the difference is less than 5.0 percent) and the spelling is still similar.", "labels": [], "entities": [{"text": "precision score", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.985174834728241}, {"text": "ECST", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.5246626138687134}, {"text": "EDv", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9921509027481079}, {"text": "spelling", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9846599698066711}]}, {"text": "On the other hand, precision for proposed lexicon with EDv above 3 (where the W sand the proposed translation equivalent W t spelling becoming more dissimilar) using ECST is higher than CST.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9995498061180115}, {"text": "EDv", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.964317798614502}, {"text": "CST", "start_pos": 186, "end_pos": 189, "type": "DATASET", "confidence": 0.8154069781303406}]}, {"text": "The most significant difference of the precision is almost 35.0 percent, where ECST achieved almost 75.0 percent precision compared to CST with 40.0 percent precision at EDv of 4.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994755387306213}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9988338351249695}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9968582391738892}, {"text": "EDv", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.9145806431770325}]}, {"text": "It is followed by ECST with almost 50.0 percent precision compared to CST with precision less than 35.0 percent, offering about 15.0 percent precision improvement at EDv of 5.", "labels": [], "entities": [{"text": "ECST", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.4460834264755249}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9994331002235413}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9983102083206177}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.999396562576294}, {"text": "EDv", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.9955955147743225}]}], "tableCaptions": [{"text": " Table 2: Performance of baseline and our model for top  2000 candidates of top 1", "labels": [], "entities": []}]}