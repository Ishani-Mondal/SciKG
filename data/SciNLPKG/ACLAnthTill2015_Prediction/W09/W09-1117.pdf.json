{"title": [{"text": "Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences", "labels": [], "entities": [{"text": "Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts", "start_pos": 0, "end_pos": 88, "type": "TASK", "confidence": 0.8964614331722259}]}], "abstractContent": [{"text": "This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses.", "labels": [], "entities": [{"text": "induction of translation lexicons", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.735110267996788}]}, {"text": "We introduce a dependency-based context model that incorporates long-range dependencies, variable context sizes, and reordering.", "labels": [], "entities": []}, {"text": "It provides a 16% relative improvement over the baseline approach that uses a fixed context window of adjacent words.", "labels": [], "entities": []}, {"text": "Its Top 10 accuracy for noun translation is higher than that of a statistical translation model trained on a Spanish-English parallel corpus containing 100,000 sentence pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9084416031837463}, {"text": "noun translation", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.903395801782608}]}, {"text": "We generalize the evaluation to other word-types, and show that the performance can be increased to 18% relative by preserving part-of-speech equiva-lencies during translation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent trends in machine translation illustrate that highly accurate word and phrase translations can be learned automatically given enough parallel training data (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.81182461977005}, {"text": "word and phrase translations", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6914862543344498}]}, {"text": "However, large parallel corpora exist for only a small fraction of the world's languages, leading to a bottleneck for building translation systems in low-density languages such as Swahili, Uzbek or Punjabi.", "labels": [], "entities": []}, {"text": "While parallel training data is uncommon for such languages, more readily available resources include small translation dictionaries, comparable corpora, and large amounts of monolingual data.", "labels": [], "entities": []}, {"text": "The marked difference in the availability of monolingual vs parallel corpora has led several researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora; or by exploiting the cross-language evidence of closely related \"bridge\" languages that have more resources.", "labels": [], "entities": []}, {"text": "This paper investigates new ways of learning translations from monolingual corpora.", "labels": [], "entities": [{"text": "learning translations from monolingual corpora", "start_pos": 36, "end_pos": 82, "type": "TASK", "confidence": 0.6891965568065643}]}, {"text": "We extend the Rapp (1999) model of context vector projection using a seed lexicon.", "labels": [], "entities": [{"text": "context vector projection", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.662745068470637}]}, {"text": "It is based on the intuition that translations will have similar lexical context, even in unrelated corpora.", "labels": [], "entities": []}, {"text": "For example, in order to translate the word \"airplane\", the algorithm builds a context vector which might contain terms such as \"passengers\", \"runway\", \"airport\", etc. and words in target language that have their translations (obtained via seed lexicon) in surrounding context can be considered as likely translations.", "labels": [], "entities": [{"text": "translate the word \"airplane\"", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.8351240158081055}]}, {"text": "We extend the basic approach by formulating a context model that uses dependency trees.", "labels": [], "entities": []}, {"text": "The use of dependencies has the following advantages: \u2022 Long distance dependencies allow associated words to be included in the context vector even if they fall outside of the fixed-window used in the baseline model.", "labels": [], "entities": []}, {"text": "\u2022 Using relationships like parent and child instead of absolute positions alleviates problems when projecting vectors between languages with different word orders.", "labels": [], "entities": []}, {"text": "\u2022 It achieves better performance than baseline context models across the board, and better performance than statistical translation models on Top-10 accuracy for noun translation when trained on identical data.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.6489890962839127}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9326802492141724}, {"text": "noun translation", "start_pos": 162, "end_pos": 178, "type": "TASK", "confidence": 0.8633785545825958}]}, {"text": "We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns).", "labels": [], "entities": [{"text": "partof-speech clustering", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7826530933380127}, {"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9987205862998962}]}], "datasetContent": [{"text": "For our initial set of experiments we compared several different vector-based context models: \u2022 Adj bow -A baseline model which used bag of words model with a fixed window of 4 words, two on either side of the word to be translated.", "labels": [], "entities": []}, {"text": "\u2022 Adj posn -A second baseline that used a fixed window of 4 words but which took positional into account.", "labels": [], "entities": []}, {"text": "\u2022 Dep bow -A dependency model which did not distinguish between grandparent, parent, child and grandparent relations, analogous to the bag of words model.", "labels": [], "entities": []}, {"text": "\u2022 Dep posn -A dependency model which did include such relationships, and was analogous to the position-based baseline.", "labels": [], "entities": []}, {"text": "\u2022 Dep posn + rev -The above Dep posn model applied in both directions (Spanish-to-English and English-to-Spanish) using their sum as the final translation score.", "labels": [], "entities": []}, {"text": "We contrasted the accuracy of the above methods, which use monolingual corpora, with a statistical model trained on bilingual parallel corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9994102716445923}]}, {"text": "We refer to that model as Moses en-es-100k , because it was trained using the Moses toolkit ().", "labels": [], "entities": []}, {"text": "The models were evaluated in terms of exact-match translation accuracy of the 1000 most frequent nouns in a English-Spanish dictionary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.7386603951454163}]}, {"text": "The accuracy was calculated by counting how many mappings exactly match one of the entries in the dictionary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996253252029419}]}, {"text": "This evaluation criterion is similar to the setup used by.", "labels": [], "entities": []}, {"text": "We compute the Top N accuracy in the standard way as the number of Spanish test words whose Top N English translation candidates contain a lexicon translation entry out of the total number of Spanish words that can be mapped correctly using the lexicon entries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9110393524169922}]}, {"text": "Thus if \"crecimiento, growth\" is the correct mapping based on the lexicon entries, the translation for \"crecimiento\" will be counted as correct if \"growth\" occurs in the Top N English translation candidates for \"crecimiento\".", "labels": [], "entities": []}, {"text": "Note that the exact-match accuracy is a conservative estimate as it is possible that the algorithm may propose a reasonable translation for the given   Spanish word but is marked incorrect if it does not exist in the lexicon.", "labels": [], "entities": [{"text": "exact-match", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9016536474227905}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.5370286107063293}]}, {"text": "Because it would be intractable to compare each projected vector against the vectors for all possible English words, we limited ourselves to comparing the projected vector from each Spanish word against the vectors for the 1000 most frequent English nouns, following along the lines of previous work ().", "labels": [], "entities": []}, {"text": "gives the Top 1 and Top 10 accuracy for each of the models on their ability to translate Spanish nouns into English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9880899786949158}, {"text": "translate Spanish nouns into English", "start_pos": 79, "end_pos": 115, "type": "TASK", "confidence": 0.8493548393249511}]}, {"text": "Examples of the top 10 translations using the best performing baseline and dependency-based models are shown in  latter disregards the position information in the context vector and simply uses a bag of words instead.", "labels": [], "entities": []}, {"text": "shows that Adj bow gains using this simplification.", "labels": [], "entities": []}, {"text": "A bag of words vector approach pools counts together, which helps to reduce data sparsity.", "labels": [], "entities": []}, {"text": "In the position based model the vector is four times as long.", "labels": [], "entities": []}, {"text": "Additionally, the bag of words model can help when there is local re-ordering between the two languages.", "labels": [], "entities": []}, {"text": "For instance, Spanish adjectives often follow nouns whereas in English the the ordering is reversed.", "labels": [], "entities": []}, {"text": "Thus, one can either learn position mappings, that is, position +1 for adjectives in Spanish is the same as position -1 in English or just add the the word counts from different positions into one common vector as considered in the bag of words approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top 10 translation candidates for the spanish word", "labels": [], "entities": [{"text": "spanish word", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.9226380884647369}]}, {"text": " Table 3: Performance of various context-based models", "labels": [], "entities": []}, {"text": " Table 5: Performance of dependency context-based model", "labels": [], "entities": []}]}