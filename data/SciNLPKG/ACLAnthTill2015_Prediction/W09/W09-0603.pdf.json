{"title": [{"text": "System Building Cost vs. Output Quality in Data-To-Text Generation", "labels": [], "entities": [{"text": "Data-To-Text Generation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.7410644292831421}]}], "abstractContent": [{"text": "Data-to-text generation systems tend to be knowledge-based and manually built, which limits their reusability and makes them time and cost-intensive to create and maintain.", "labels": [], "entities": [{"text": "Data-to-text generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7123463749885559}]}, {"text": "Methods for automating (part of) the system building process exist , but do such methods risk a loss in output quality?", "labels": [], "entities": []}, {"text": "In this paper, we investigate the cost/quality trade-off in generation system building.", "labels": [], "entities": []}, {"text": "We compare four new data-to-text systems which were created by predominantly automatic techniques against six existing systems for the same domain which were created by predominantly manual techniques.", "labels": [], "entities": []}, {"text": "We evaluate the ten systems using intrinsic automatic metrics and human quality ratings.", "labels": [], "entities": []}, {"text": "We find that increasing the degree to which system building is automated does not necessarily result in a reduction in output quality.", "labels": [], "entities": []}, {"text": "We find furthermore that standard automatic evaluation metrics underestimate the quality of handcrafted systems and overestimate the quality of automatically created systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional Natural Language Generation (NLG) systems tend to be handcrafted knowledge-based systems.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8359787861506144}]}, {"text": "Such systems tend to be brittle, expensive to create and hard to adapt to new domains or applications.", "labels": [], "entities": []}, {"text": "Over the last decade or so, in particular following Knight and Langkilde's work on n-gram-based generate-and-select surface realisation (), NLG researchers have become increasingly interested in systems that are automatically trainable from data.", "labels": [], "entities": []}, {"text": "Systems that have a trainable component tend to be easier to adapt to new domains and applications, and increased automation is often taken as self-evidently a good thing.", "labels": [], "entities": [{"text": "automation", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9707143902778625}]}, {"text": "The question is, however, whether reduced system building cost and increased adaptability are achieved at the price of a reduction in output quality, and if so, how great the price is.", "labels": [], "entities": []}, {"text": "This in turn raises the question of how to evaluate output quality so that a potential decrease can be detected and quantified.", "labels": [], "entities": []}, {"text": "In this paper we set about trying to find answers to these questions.", "labels": [], "entities": []}, {"text": "We start, in the following section, we briefly describing the SUMTIME corpus of weather forecasts which we used in our experiments.", "labels": [], "entities": [{"text": "SUMTIME corpus of weather forecasts", "start_pos": 62, "end_pos": 97, "type": "DATASET", "confidence": 0.8575256884098053}]}, {"text": "In the next section (Section 2), we outline four different approaches to building data-to-text generation systems which involve different combinations of manual and automatic techniques.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.772385835647583}]}, {"text": "Next (Section 4) we describe ten systems in the four categories that generate weather forecast texts in the SUMTIME domain.", "labels": [], "entities": [{"text": "SUMTIME domain", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.875432014465332}]}, {"text": "In Section 5 we describe the human-assessed and automatically computed evaluation methods we used to comparatively evaluate the quality of the outputs of the ten systems.", "labels": [], "entities": []}, {"text": "We then present the evaluation results and discuss implications of discrepancies we found between the results of the human and automatic evaluations (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "The two automatic metrics used in the evaluations, NIST 2 and BLEU 3 , have been shown to correlate well with expert judgments (Pearson's r = 0.82 and 0.79 respectively) in the SUMTIME domain).", "labels": [], "entities": [{"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.7919086813926697}, {"text": "BLEU 3", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9756847620010376}, {"text": "Pearson's r", "start_pos": 128, "end_pos": 139, "type": "METRIC", "confidence": 0.9254020849863688}]}, {"text": "BLEU-x is an n-gram based string comparison measure, originally proposed by for evaluation of MT systems.", "labels": [], "entities": [{"text": "BLEU-x", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9822620153427124}, {"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9759648442268372}]}, {"text": "It computes the proportion of word n-grams of length x and less that a system output shares with several reference outputs.", "labels": [], "entities": []}, {"text": "Setting x = 4 (i.e. considering all ngrams of length \u2264 4) is standard.", "labels": [], "entities": []}, {"text": "NIST) is aversion of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams, and the range of NIST scores depends on the size of the test set.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8192847371101379}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9985910058021545}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9957740902900696}]}, {"text": "Some research has shown NIST to correlate with human judgments more highly than BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9982554316520691}]}, {"text": "We designed an experiment in which participants were asked to rate forecast texts for Clarity and Readability on scales of 1-7.", "labels": [], "entities": []}, {"text": "Clarity was explained as indicating how understandable a forecast was, and Readability as indicating how fluent and readable it was.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9682641625404358}]}, {"text": "After an introduction and detailed explanations, participants carried out the evaluations over the web.", "labels": [], "entities": []}, {"text": "They were able to interrupt and resume the evaluation at anytime.", "labels": [], "entities": []}, {"text": "We randomly selected 22 forecast dates and used outputs from all 10 systems for those dates (as well as the corresponding forecasts in the corpus) in the evaluation, i.e. a total of 242 forecast texts.", "labels": [], "entities": []}, {"text": "We used a repeated Latin squares design where each combination of forecast date and system is assigned two trials.", "labels": [], "entities": []}, {"text": "As there were 2 evaluation criteria, there were 968 individual ratings in this experiment.", "labels": [], "entities": []}, {"text": "An evaluation session started with three training examples; the real trials were then presented in random order.", "labels": [], "entities": []}, {"text": "We recruited 22 participants from among our university colleagues whose first language was English and who had no experience of NLP.", "labels": [], "entities": []}, {"text": "We did not try to recruit master mariners as in earlier experiments reported by, because these experiments also demonstrated that the correlation between the ratings by such expert evaluators and lay-people is very strong in the SUMTIME domain (Pearson's r = 0.845).", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 245, "end_pos": 256, "type": "METRIC", "confidence": 0.7392228245735168}]}], "tableCaptions": [{"text": " Table 4: Mean Clarity and Readability ratings  from human evaluation; NIST and BLEU scores  on same 22 forecasts as used in human evaluation.", "labels": [], "entities": [{"text": "Mean Clarity", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9157166182994843}, {"text": "NIST", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.9001361131668091}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9974391460418701}]}]}