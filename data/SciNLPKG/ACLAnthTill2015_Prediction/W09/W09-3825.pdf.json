{"title": [{"text": "Transition-Based Parsing of the Chinese Treebank using a Global Discriminative Model", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.943276435136795}]}], "abstractContent": [{"text": "Transition-based approaches have shown competitive performance on constituent and dependency parsing of Chinese.", "labels": [], "entities": [{"text": "dependency parsing of Chinese", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.7532285898923874}]}, {"text": "State-of-the-art accuracies have been achieved by a deterministic shift-reduce parsing model on parsing the Chinese Treebank 2 data (Wang et al., 2006).", "labels": [], "entities": [{"text": "Chinese Treebank 2 data", "start_pos": 108, "end_pos": 131, "type": "DATASET", "confidence": 0.9681552797555923}]}, {"text": "In this paper, we propose a global discriminative model based on the shift-reduce parsing process, combined with a beam-search decoder, obtaining competitive accuracies on CTB2.", "labels": [], "entities": [{"text": "CTB2", "start_pos": 172, "end_pos": 176, "type": "DATASET", "confidence": 0.962854266166687}]}, {"text": "We also report the performance of the parser on CTB5 data, obtaining the highest scores in the literature fora dependency-based evaluation.", "labels": [], "entities": [{"text": "CTB5 data", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.9641870558261871}]}], "introductionContent": [{"text": "Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions).", "labels": [], "entities": [{"text": "Transition-based statistical parsing", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5428734123706818}]}, {"text": "The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9723053574562073}]}, {"text": "One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9803637862205505}]}, {"text": "The alternative approach, exemplified by and, is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse.", "labels": [], "entities": []}, {"text": "For English dependency parsing, the two approaches give similar results ().", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.588481475909551}]}, {"text": "For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9952901899814606}]}, {"text": "In contrast, for Chinese, the best dependency parsers are currently transition-based (.", "labels": [], "entities": []}, {"text": "For constituent-based parsing using the Chinese Treebank (CTB), have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process.", "labels": [], "entities": [{"text": "constituent-based parsing", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.4821707457304001}, {"text": "Chinese Treebank (CTB)", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.9730618953704834}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9651610255241394}]}, {"text": "In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it withs approach.", "labels": [], "entities": [{"text": "Chinese shift-reduce parsing", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.5920190910498301}]}, {"text": "We apply the same shift-reduce procedure as, but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.", "labels": [], "entities": []}, {"text": "We apply beam search to decoding instead of greedy search.", "labels": [], "entities": []}, {"text": "The parser still operates in linear time, but the use of beam-search allows the correction of local decision errors by global comparison.", "labels": [], "entities": []}, {"text": "Using CTB2, our model achieved Parseval F-scores comparable tos approach.", "labels": [], "entities": [{"text": "CTB2", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.9118880033493042}, {"text": "Parseval F-scores", "start_pos": 31, "end_pos": 48, "type": "METRIC", "confidence": 0.8105166554450989}]}, {"text": "We also present accuracy scores for the much larger CTB5, using both a constituent-based and dependency-based evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9992387294769287}, {"text": "CTB5", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8724856972694397}]}, {"text": "The scores for the dependency-based evaluation were higher than the state-of-the-art dependency parsers for the CTB5 data.", "labels": [], "entities": [{"text": "CTB5 data", "start_pos": 112, "end_pos": 121, "type": "DATASET", "confidence": 0.9797306656837463}]}, {"text": "The process assumes binary-branching trees; section 2.1 explains how these are obtained from the arbitrary-branching trees in the Chinese Treebank.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 130, "end_pos": 146, "type": "DATASET", "confidence": 0.9810171723365784}]}, {"text": "The input is assumed to be segmented and POS tagged, and the word-POS pairs waiting to be processed are stored in a queue.", "labels": [], "entities": []}, {"text": "A stack holds the partial parse trees that are built during the parsing process.", "labels": [], "entities": []}, {"text": "A parse state is defined as a stack,queue pair.", "labels": [], "entities": []}, {"text": "Parser actions, including SHIFT and various kinds of REDUCE, define functions from states to states by shifting word-POS pairs onto the stack and building partial parse trees.", "labels": [], "entities": []}, {"text": "This action is novel in our parser. and only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node.", "labels": [], "entities": []}, {"text": "However, there area small number of sentences (14 out of 3475 from the training data) that have unary-branching roots.", "labels": [], "entities": []}, {"text": "For these sentences, Wang's parser will be unable to produce the unary-branching roots because the parsing process terminates as soon as the root is found.", "labels": [], "entities": []}, {"text": "We define a separate action to terminate parsing, allowing unary reduces to be applied to the root item before parsing finishes.", "labels": [], "entities": []}, {"text": "The trees built by the parser are lexicalized, using the head-finding rules from.", "labels": [], "entities": []}, {"text": "The left (L) and right (R) versions of the REDUCE-binary rules indicate whether the head of: the binarization algorithm with input T the new node is to betaken from the left or right child.", "labels": [], "entities": []}, {"text": "Note also that, since the parser is building binary trees, the X label in the REDUCE rules can be one of the temporary constituent labels, such as NP * , which are needed for the binarization process described in Section 2.1.", "labels": [], "entities": []}, {"text": "Hence the number of left and right binary reduce rules is the number of constituent labels in the binarized grammar.", "labels": [], "entities": []}, {"text": "give a detailed example showing how a segmented and POS-tagged sentence can be incrementally processed using the shift-reduce actions to produce a binary tree.", "labels": [], "entities": []}, {"text": "We show this example in.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data.", "labels": [], "entities": [{"text": "Chinese Treebank 2", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.9717259407043457}, {"text": "Chinese Treebank 5 data", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9751375764608383}]}, {"text": "Standard data preparation was performed before the experiments: empty terminal nodes were removed; any non-terminal nodes with no children were removed; any unary X \u2192 X nodes resulting from the previous steps were collapsed into one X node.", "labels": [], "entities": []}, {"text": "For all experiments, we used the EVALB tool 1 for evaluation, and used labeled recall (LR), labeled precision (LP ) and F 1 score (which is the  harmonic mean of LR and LP ) to measure parsing accuracy.", "labels": [], "entities": [{"text": "recall (LR)", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9150659143924713}, {"text": "labeled precision (LP )", "start_pos": 92, "end_pos": 115, "type": "METRIC", "confidence": 0.8320163011550903}, {"text": "F 1 score", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.984728475411733}, {"text": "parsing", "start_pos": 185, "end_pos": 192, "type": "TASK", "confidence": 0.9655026793479919}, {"text": "accuracy", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.8303016424179077}]}, {"text": "shows the accuracy curves using different beam-sizes for the decoder.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993268251419067}]}, {"text": "The number of training iterations is on the x-axis with F -score on the y-axis.", "labels": [], "entities": [{"text": "F -score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9925885995229086}]}, {"text": "The tests were performed using the development test data and gold-standard POStags.", "labels": [], "entities": [{"text": "POStags", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.8286947011947632}]}, {"text": "The figure shows the benefit of using abeam size greater than 1, with comparatively little accuracy gain being obtained beyond abeam size of 8.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9992252588272095}]}], "tableCaptions": [{"text": " Table 2: The standard split of CTB2 data", "labels": [], "entities": [{"text": "CTB2 data", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.9383794069290161}]}, {"text": " Table 3: Accuracies on CTB2 with gold-standard", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9988694787025452}, {"text": "CTB2", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9507593512535095}]}, {"text": " Table 4: Accuracies on CTB2 with automatically assigned tags", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9969702959060669}, {"text": "CTB2", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9492499232292175}]}, {"text": " Table 5: Accuracies on CTB5 using gold-standard and automatically assigned POS-tags", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9926466941833496}, {"text": "CTB5", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.9518283605575562}]}, {"text": " Table 6: Training sets with different sizes", "labels": [], "entities": []}, {"text": " Table 7: Standard split of CTB5 data", "labels": [], "entities": [{"text": "CTB5 data", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.9445253312587738}]}, {"text": " Table 8: Comparison with state-of-the-art depen- dency parsing using CTB5 data", "labels": [], "entities": [{"text": "depen- dency parsing", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.71482253074646}, {"text": "CTB5 data", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9297181367874146}]}]}