{"title": [{"text": "Representing words as regions in vector space", "labels": [], "entities": [{"text": "Representing words", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8959236741065979}]}], "abstractContent": [{"text": "Vector space models of word meaning typically represent the meaning of a word as a vector computed by summing overall its corpus occurrences.", "labels": [], "entities": []}, {"text": "Words close to this point in space can be assumed to be similar to it in meaning.", "labels": [], "entities": []}, {"text": "But how far around this point does the region of similar meaning extend?", "labels": [], "entities": []}, {"text": "In this paper we discuss two models that represent word meaning as regions in vector space.", "labels": [], "entities": []}, {"text": "Both representations can be computed from traditional point representations in vector space.", "labels": [], "entities": []}, {"text": "We find that both models perform at over 95% F-score on a token classification task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9986357092857361}, {"text": "token classification task", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.8707837661107382}]}], "introductionContent": [{"text": "Vector space models of word meaning) represent words as points in a highdimensional semantic space.", "labels": [], "entities": []}, {"text": "The dimensions of the space represent the contexts in which each target word has been observed.", "labels": [], "entities": []}, {"text": "Distance between vectors in semantic space predicts the degree of semantic similarity between the corresponding words, as words with similar meaning tend to occur in similar contexts.", "labels": [], "entities": []}, {"text": "Because of this property, vector space models have been used successfully both in computational linguistics; and in cognitive science).", "labels": [], "entities": []}, {"text": "Given the known problems with defining globally appropriate senses, vector space models are especially interesting for their ability to represent word meaning without relying on dictionary senses.", "labels": [], "entities": []}, {"text": "Vector space models typically compute one vector per target word (what we will call word type vectors), summing co-occurrence counts overall corpus tokens of the target.", "labels": [], "entities": []}, {"text": "If the target word is polysemous, the representation will constitute a union over the uses or senses of the word.", "labels": [], "entities": []}, {"text": "Such a model does not provide information on the amount of variance in each dimension: Do values on each dimension vary a lot across occurrences of the target?", "labels": [], "entities": []}, {"text": "Also, it does not provide information on co-occurrences of feature values in occurrences of the target.", "labels": [], "entities": []}, {"text": "To encode these two types of information, we study richer models of word meaning in vector space beyond single point representations.", "labels": [], "entities": []}, {"text": "Many models of categorization in psychology represent a concept as a region, characterized by feature vectors with dimension weights.", "labels": [], "entities": []}, {"text": "Taking our cue from these approaches, we study two models that represent a word as a region in vector space rather than a point.", "labels": [], "entities": []}, {"text": "The first model is one that we have recently introduced for representing hyponymy in vector space.", "labels": [], "entities": []}, {"text": "We now test its suitability as a general region model for word meaning.", "labels": [], "entities": [{"text": "word meaning", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.7460371851921082}]}, {"text": "This model can be viewed as a prototypestyle model that induces a region surrounding a central vector.", "labels": [], "entities": []}, {"text": "As it does not record co-occurrences of feature values, we contrast it with a second model, an exemplar-style model using a k-nearest neighbor analysis, which can represent both degree of variance in each dimension and value co-occurrences.", "labels": [], "entities": []}, {"text": "Both models induce regions representations without labeled data.", "labels": [], "entities": []}, {"text": "The idea on which both models are based is to use word token vectors to estimate a region representation.", "labels": [], "entities": []}, {"text": "We evaluate the two region models on a task of token classification: Given a point in vector space, the task is predict the word of which it is a token vector.", "labels": [], "entities": [{"text": "token classification", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8858345150947571}]}, {"text": "By representing the meaning of words as regions in vector space, we can describe areas in which points encode similar meanings.", "labels": [], "entities": []}, {"text": "This description is flexible, depending on the target word in question, rather than uniform for all words through a fixed distance threshold from the target's type vector.", "labels": [], "entities": []}, {"text": "One possible application of region models of word meaning is in the task of determining the appropriateness of a paraphrase in a given context.", "labels": [], "entities": []}, {"text": "This task is highly relevant for textual entailment (.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8106317520141602}]}, {"text": "Current vector space approaches typically compare the target word's token vector to the type vector of the potential paraphrase (.", "labels": [], "entities": []}, {"text": "A region model could instead test the target's token vector for inclusion in the potential paraphrase's region.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports on experiments that test the performance of the two region models of word meaning in vector space that we have presented in Sec.", "labels": [], "entities": []}, {"text": "3, the centered and the distributed model.", "labels": [], "entities": []}, {"text": "In the first experiment, we test whether the two region models can identify novel tokens of the monosemous verbs in Mon.", "labels": [], "entities": []}, {"text": "The task is the one described in Sec.", "labels": [], "entities": []}, {"text": "4. We focus on monosemous verbs first because we suspect that the centered model should do better here than on polysemous verbs.", "labels": [], "entities": []}, {"text": "Both models were trained using token vectors computed from the training half of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.9667014479637146}]}, {"text": "Token vectors of the target verb were treated as positive data, and token vectors of other verbs as negative data.", "labels": [], "entities": []}, {"text": "We used resampling to restrict the number of negative items used during training, using 3% of the negative items, randomly sampled.", "labels": [], "entities": []}, {"text": "We use for testing only those direct objects that do not also appear in the training data, yielding 6,339 positive and 1,396,552 negative test items summed overall target verbs.", "labels": [], "entities": []}, {"text": "The case of supersede discussed in Sec.", "labels": [], "entities": []}, {"text": "4 is an example of a monosemous verb according to WordNet 3.0.", "labels": [], "entities": []}, {"text": "summarizes precision, recall and F-score results.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9997231364250183}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995680451393127}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9988824725151062}]}, {"text": "Both models easily beat the random base-4 This simplification breaks down for 6 of the 120 verbs (5%), which are in fact synonyms.", "labels": [], "entities": []}, {"text": "We consider this an acceptable level of noise.", "labels": [], "entities": []}, {"text": "The number of 3% was determined on a development set constructed by further splitting the training set into training and development portion.   line.", "labels": [], "entities": []}, {"text": "The centered model shows better performance overall than the distributed one, and the avg method of computing token vector worked best for both models.", "labels": [], "entities": []}, {"text": "The centered model has extremely high precision throughout, while the distributed model has better recall for conditions avg and min.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9992695450782776}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9994674324989319}]}, {"text": "Table 2 breaks down the results by the frequency of the target verb, measured in the number of different verb/object tokens in the training data.", "labels": [], "entities": []}, {"text": "We now test how the centered and distributed models fare on the same task, but with a mixture of monosemous and polysemous verbs.", "labels": [], "entities": []}, {"text": "We use the verbs in Hyp, which in WordNet 3.0 have on average 6.79 senses.", "labels": [], "entities": [{"text": "WordNet 3.0", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.909713625907898}]}, {"text": "For example, follow is a WordNet hypernym of the monosemous supersede.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9334765672683716}]}, {"text": "It has 24 senses, among them comply and postdate.", "labels": [], "entities": [{"text": "comply", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9985541701316833}]}, {"text": "Among its training tokens are follow instruction and follow dinner.", "labels": [], "entities": []}, {"text": "The first is probably the comply sense of follow, the second the postdate sense.", "labels": [], "entities": [{"text": "comply", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9946960210800171}, {"text": "follow", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.5289735198020935}]}, {"text": "An example of a test token (i.e., occurring in the test but not the train-ing data) is follow tea.", "labels": [], "entities": []}, {"text": "(If tea is teatime, this is also the postdate sense.)", "labels": [], "entities": []}, {"text": "We computed type vectors for the Hyp verbs and their objects from the training half of the BNC, and computed token vectors using the best method from Exp.", "labels": [], "entities": [{"text": "BNC", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9117695689201355}]}, {"text": "Again, we use for testing only those tokens that do not also appear in the training data.", "labels": [], "entities": []}, {"text": "Due to the larger amount of data, we used resampling in the training as well as the test data, using only a random 3% of negative tokens for testing.", "labels": [], "entities": []}, {"text": "This yielded 25,736 positive and 670,630 negative test items.", "labels": [], "entities": []}, {"text": "shows the results: The first line has the overall results, and the following lines breakdown the results by the number of senses each lemma has in WordNet 3.0. 6 Both models, centered and distributed, easily beat the random baseline.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 147, "end_pos": 154, "type": "DATASET", "confidence": 0.9309149980545044}]}, {"text": "The centered model has comparable results for the Hyp as for the Mon verbs (cf., while the distributed model has better results for this dataset, and better results than the centered model.", "labels": [], "entities": []}, {"text": "The centered model shows a marked improvement in recall as the number of senses increases.", "labels": [], "entities": [{"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9995217323303223}]}, {"text": "We first proposed the centered model as a method for encoding hyponymy information in a vector space representation.", "labels": [], "entities": []}, {"text": "Hyponymy information from another source, in this case WordNet, was encoded in a centered region representation of a target verb by using tokens of the verb itself as well as tokens from its direct hyponyms in training the model.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.9612155556678772}]}, {"text": "Negative data consisted of training data tokens that were not occurrences of the target verb or its direct hyponyms.", "labels": [], "entities": []}, {"text": "In the example of the verb follow, the positive training data would contain tokens of follow along with tokens of supersede and guard, another direct hyponym of follow.", "labels": [], "entities": []}, {"text": "Negative training tokens would include, for example, tokens of the word destroy.", "labels": [], "entities": []}, {"text": "The resulting centered model, in this case of follow, was then tested on previously unseen tokens, for example guard purpose (a token of a hyponym) and destroy lawn (a token of a nonhyponym), with the task of predicting whether they were tokens of direct hyponyms of follow or not.", "labels": [], "entities": []}, {"text": "The one-sense items in   We now repeat this experiment with the distributed model.", "labels": [], "entities": []}, {"text": "We use the direct hypernyms of the verbs in Mon, with the same frequency restrictions as above.", "labels": [], "entities": []}, {"text": "We refer to this set of 273 verbs as DHyp.", "labels": [], "entities": []}, {"text": "We train one centered and one distributed model for each verb win DHyp.", "labels": [], "entities": []}, {"text": "Positive training tokens for training a model fora verb w \u2208 DHyp are tokens of wand of all sufficiently frequent children of win WordNet 3.0.", "labels": [], "entities": []}, {"text": "Negative training tokens are tokens of other verbs in DHyp and their children.", "labels": [], "entities": []}, {"text": "We again sample a random 3% of the negative data during both training and testing.", "labels": [], "entities": []}, {"text": "Both models again beat the baseline.", "labels": [], "entities": []}, {"text": "The distributed model shows slightly better results overall, while the centered model has by far the highest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9980968832969666}]}], "tableCaptions": [{"text": " Table 1: Results: token classification for monosemous  verbs. Random baseline: Prec 0.8, Rec 49.8, F 1.6.", "labels": [], "entities": [{"text": "token classification", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.777199387550354}, {"text": "Random baseline", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.9515939652919769}, {"text": "Prec", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.8120385408401489}, {"text": "F 1.6", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9533304274082184}]}, {"text": " Table 2: Results: token classification for monosemous  verbs, by target frequency", "labels": [], "entities": [{"text": "token classification", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.9285621345043182}]}, {"text": " Table 3: Results: Token classification for polysemous  verbs, avg token computation. Random baseline: Prec  8.2, Rec 50.4, F 14.0.", "labels": [], "entities": [{"text": "Token classification", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.9224444627761841}, {"text": "F", "start_pos": 124, "end_pos": 125, "type": "METRIC", "confidence": 0.94651859998703}]}]}