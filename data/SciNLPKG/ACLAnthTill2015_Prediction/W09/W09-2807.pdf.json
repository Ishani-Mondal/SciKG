{"title": [{"text": "A Classification Algorithm for Predicting the Structure of Summaries", "labels": [], "entities": [{"text": "Predicting the Structure of Summaries", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.8917803883552551}]}], "abstractContent": [{"text": "We investigate the problem of generating the structure of short domain independent abstracts.", "labels": [], "entities": []}, {"text": "We apply a supervised machine learning approach trained over a set of abstracts collected from abstracting services and automatically annotated with a text analysis tool.", "labels": [], "entities": []}, {"text": "We design a set of features for learning inspired from past research in content selection, information ordering , and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts.", "labels": [], "entities": [{"text": "content selection", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.735545814037323}, {"text": "information ordering", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7261180281639099}, {"text": "rhetorical analysis", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8576942980289459}]}, {"text": "The proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60% of the cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Mani (2001) defines an abstract as \"a summary at least some of whose material is not present in the input\".", "labels": [], "entities": []}, {"text": "Ina study of professional abstracting, concluded that professional abstractors produce abstracts by \"cut-andpaste\" operations, and that standard sentence patterns are used in their production.", "labels": [], "entities": [{"text": "professional abstracting", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.6744620501995087}]}, {"text": "Examples of abstracts produced by a professional abstractor are shown in Figures 1 and 2.", "labels": [], "entities": []}, {"text": "They contain fragments \"copied\" from the input documents together with phrases (underlined in the figures) inserted by the professional abstractors.", "labels": [], "entities": []}, {"text": "Ina recent study inhuman abstracting (restricted to the amendment of authors abstracts) noted that professional abstractors prepend third person singular verbs in present tense and without subject to the author abstract, a phenomenon related -yet different -from the problem we are investigating in this paper.", "labels": [], "entities": [{"text": "inhuman abstracting", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.6960465610027313}]}, {"text": "Note that the phrases or predicates prepended to the selected sentence fragments copied from the input document have a communicative function: Presents a model instructional session that was prepared and taught by librarians to introduce college students, faculty, and staff to the Internet by teaching them how to join listservs and topic-centered discussion groups.", "labels": [], "entities": []}, {"text": "Describes the sessions' audience, learning objectives, facility, and course design.", "labels": [], "entities": []}, {"text": "Presents a checklist for preparing an Internet instruction session.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments reported here correspond to the use of different features as input for the classifier.", "labels": [], "entities": []}, {"text": "In these experiments we have used a subset of the collected abstracts, they contain predicates which appeared at least 5 times in the corpus.", "labels": [], "entities": []}, {"text": "With this restriction in place the original set of predicates used to create the discourse structure is reduced to sixteen (See), however, the number of possible structures in the reduced corpus is still considerable with a total of 179 different structures.", "labels": [], "entities": []}, {"text": "In the experiments we compare several classifiers: \u2022 Random Generation selects a predicate at random at each iteration of the algorithm; \u2022 Predicate-based Generation is a SVM classifier which uses the two previous predicates to generate the current predicate ignoring sentence content; \u2022 Position-based Generation is a SVM classifier which also ignores sentence content but uses as features for classification the absolute position of the sentence to be generated;", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Classification Confusion Table for a Sub- set of Predicates in the Corpus (Average Fre- quency).", "labels": [], "entities": [{"text": "Fre- quency)", "start_pos": 93, "end_pos": 105, "type": "METRIC", "confidence": 0.920191153883934}]}, {"text": " Table 6: Predicate Classification Accuracy", "labels": [], "entities": [{"text": "Predicate Classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7591118812561035}, {"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9288706183433533}]}]}