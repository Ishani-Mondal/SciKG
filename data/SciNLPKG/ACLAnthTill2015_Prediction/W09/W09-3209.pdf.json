{"title": [{"text": "Ranking and Semi-supervised Classification on Large Scale Graphs Using Map-Reduce", "labels": [], "entities": []}], "abstractContent": [{"text": "Label Propagation, a standard algorithm for semi-supervised classification, suffers from scalability issues involving memory and computation when used with large-scale graphs from real-world datasets.", "labels": [], "entities": [{"text": "semi-supervised classification", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.7178463935852051}]}, {"text": "In this paper we approach Label Propagation as solution to a system of linear equations which can be implemented as a scalable parallel algorithm using the map-reduce framework.", "labels": [], "entities": []}, {"text": "In addition to semi-supervised classification, this approach to Label Propagation allows us to adapt the algorithm to make it usable for ranking on graphs and derive the theoretical connection between Label Propagation and PageRank.", "labels": [], "entities": []}, {"text": "We provide empirical evidence to that effect using two natural language tasks-lexical relat-edness and polarity induction.", "labels": [], "entities": []}, {"text": "The version of the Label Propagation algorithm presented here scales linearly in the size of the data with a constant main memory requirement , in contrast to the quadratic cost of both in traditional approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language data often lend themselves to a graph-based representation.", "labels": [], "entities": []}, {"text": "Words can be linked by explicit relations as in WordNet, and documents can be linked to one another via hyperlinks.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9384909272193909}]}, {"text": "Even in the absence of such a straightforward representation it is possible to derive meaningful graphs such as the nearest neighbor graphs, as done in certain manifold learning methods, e.g.;.", "labels": [], "entities": []}, {"text": "Typically, these graphs share the following properties: \u2022 They are edge-weighted.", "labels": [], "entities": []}, {"text": "\u2022 The edge weight encodes some notion of relatedness between the vertices.", "labels": [], "entities": []}, {"text": "\u2022 The relation represented by edges is at least weakly transitive.", "labels": [], "entities": []}, {"text": "Examples of such relations include, \"is similar to\", \"is more general than\", and soon.", "labels": [], "entities": []}, {"text": "It is important that the relations selected are transitive for the graphbased learning methods using random walks.", "labels": [], "entities": []}, {"text": "Such graphs present several possibilities for solving natural language problems involving ranking, classification, and clustering.", "labels": [], "entities": []}, {"text": "Graphs have been successfully employed in machine learning in a variety of supervised, unsupervised, and semisupervised tasks.", "labels": [], "entities": []}, {"text": "Graph based algorithms perform better than their counterparts as they capture the latent structure of the problem.", "labels": [], "entities": []}, {"text": "Further, their elegant mathematical framework allows simpler analysis to gain a deeper understanding of the problem.", "labels": [], "entities": []}, {"text": "Despite these advantages, implementations of most graph-based learning algorithms do not scale well on large datasets from real world problems in natural language processing.", "labels": [], "entities": []}, {"text": "With large amounts of unlabeled data available, the graphs can easily grow to millions of nodes and most existing non-parallel methods either fail to work due to resource constraints or find the computation intractable.", "labels": [], "entities": []}, {"text": "In this paper we describe a scalable implementation of Label Propagation, a popular random walk based semi-supervised classification method.", "labels": [], "entities": []}, {"text": "We show that our framework can also be used for ranking on graphs.", "labels": [], "entities": []}, {"text": "Our parallel formulation shows a theoretical connection between Label Propagation and PageRank.", "labels": [], "entities": []}, {"text": "We also confirm this empirically using the lexical relatedness task.", "labels": [], "entities": []}, {"text": "The proposed Parallel Label Propagation scales up linearly in the data and the number of processing elements available.", "labels": [], "entities": []}, {"text": "Also, the main memory required by the method does not grow with the size of the graph.", "labels": [], "entities": []}, {"text": "The outline of this paper is as follows: Section 2 introduces the manifold assumption and explains why graph-based learning algorithms perform better than their counterparts.", "labels": [], "entities": []}, {"text": "Section 3 motivates the random walk based approach for learning on graphs.", "labels": [], "entities": []}, {"text": "Section 4 introduces the Label Propagation method by.", "labels": [], "entities": [{"text": "Label Propagation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7772981524467468}]}, {"text": "In Section 5 we describe a method to scale up Label Propagation using Map-Reduce.", "labels": [], "entities": []}, {"text": "Section 6 shows how Label Propagation could be used for ranking on graphs and derives the relation between Label Propagation and PageRank.", "labels": [], "entities": []}, {"text": "Parallel Label Propagation is evaluated on ranking and semi-supervised classification problems in natural language processing in Section 8.", "labels": [], "entities": [{"text": "Parallel Label Propagation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6650873223940531}]}, {"text": "We study scalability of this algorithm in Section 9 and describe related work in the area of parallel algorithms and machine learning in Section 10.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the Parallel Label Propagation algorithm for both ranking and semi-supervised classification.", "labels": [], "entities": []}, {"text": "In ranking our goal is to rank the vertices of a graph with respect to a given node called the pivot/query node.", "labels": [], "entities": []}, {"text": "In semi-supervised classification, we are given a graph with some vertices Algorithm 3: Graph Construction map(key, value): begin edgeEntry = value; Node n(edgeEntry); Emit(n.id, n); end reduce(key, values): begin Emit(key, serialize(values)); end labeled and would like to predict labels for the remaining vertices.", "labels": [], "entities": [{"text": "semi-supervised classification", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.6769779026508331}]}, {"text": "We present some experiments to study the scalability of the algorithm presented.", "labels": [], "entities": []}, {"text": "All our experiments were performed on an experimental cluster of four machines to test the concept.", "labels": [], "entities": []}, {"text": "The machines were Intel Xeon 2.4 GHz with 1Gb main memory.", "labels": [], "entities": []}, {"text": "All performance measures were averaged over 20 runs.", "labels": [], "entities": []}, {"text": "shows scaleup of the algorithm which measures how well the algorithm handles increasing data sizes.", "labels": [], "entities": []}, {"text": "For this experiment, we used all nodes in the cluster.", "labels": [], "entities": []}, {"text": "As observed, the increase in time is at most linear in the size of the data.", "labels": [], "entities": []}, {"text": "shows speedup of the algorithm.", "labels": [], "entities": []}, {"text": "Speedup shows how well the algorithm performs with increase in resources fora fixed input size.", "labels": [], "entities": []}, {"text": "In this case, we progressively increase the number of nodes in the cluster.", "labels": [], "entities": []}, {"text": "Again, the speedup achieved is linear in the number of processing elements (CPUs).", "labels": [], "entities": []}, {"text": "An appealing factor of Algorithm 2 is that the memory used by each mapper process is fixed regardless of the size of the graph.", "labels": [], "entities": []}, {"text": "This makes the algorithm feasible for use with large-scale graphs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Lexical-relatedness results: Comparision  of PageRank and \u03b2-corrected Parallel Label Prop- agation", "labels": [], "entities": [{"text": "Comparision", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9663912057876587}, {"text": "\u03b2-corrected Parallel Label Prop- agation", "start_pos": 68, "end_pos": 108, "type": "METRIC", "confidence": 0.7398605595032374}]}, {"text": " Table 3: Polarity induction results (F-scores)", "labels": [], "entities": [{"text": "Polarity induction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7077728807926178}, {"text": "F-scores", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9883812069892883}]}]}