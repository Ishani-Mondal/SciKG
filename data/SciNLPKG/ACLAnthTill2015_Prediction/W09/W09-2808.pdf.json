{"title": [{"text": "Syntax-Driven Sentence Revision for Broadcast News Summarization", "labels": [], "entities": [{"text": "Syntax-Driven Sentence Revision", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8843175570170084}, {"text": "Summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.7294454574584961}]}], "abstractContent": [{"text": "We propose a method of revising lead sentences in a news broadcast.", "labels": [], "entities": []}, {"text": "Unlike many other methods proposed so far, this method does not use the corefer-ence relation of noun phrases (NPs) but rather, insertion and substitution of the phrases modifying the same head chunk in lead and other sentences.", "labels": [], "entities": []}, {"text": "The method borrows an idea from the sentence fusion methods and is more general than those using NP coreferencing as ours includes them.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7399671673774719}]}, {"text": "We show in experiments the method was able to find semantically appropriate revisions thus demonstrating its basic feasibility.", "labels": [], "entities": []}, {"text": "We also show that that parsing errors mainly degraded the sentential completeness such as grammaticality and redundancy.", "labels": [], "entities": []}], "introductionContent": [{"text": "We address the problem of revising the lead sentence in a broadcast news text to increase the amount of background information in the lead.", "labels": [], "entities": []}, {"text": "This is one of the draft and revision approaches to summarization, which has received keen attention in the research community.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9924719333648682}]}, {"text": "Unlike many other methods that directly utilize noun phrase (NP) coreference), we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences.", "labels": [], "entities": [{"text": "noun phrase (NP) coreference", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6048462291558584}]}, {"text": "We also show its effectiveness in a revision experiment.", "labels": [], "entities": [{"text": "revision", "start_pos": 36, "end_pos": 44, "type": "TASK", "confidence": 0.9848612546920776}]}, {"text": "As is well known, the extractive summary that has been extensively studied from the early days of summarization history suffers from various drawbacks.", "labels": [], "entities": [{"text": "summarization history", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.9057387411594391}]}, {"text": "These include the problems of a break in cohesion in the summary text such as dangling anaphora and a sudden shift in topic.", "labels": [], "entities": []}, {"text": "To ameliorate these problems, the idea of revising the extracted sentences was proposed in a single document summarization study.", "labels": [], "entities": []}, {"text": "found that human summarization can be traced back to six cut-andpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part.", "labels": [], "entities": [{"text": "summarization", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7233293652534485}, {"text": "sentence reduction", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.724848285317421}, {"text": "sentence extraction", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.6985244601964951}]}, {"text": "Mani and colleagues (1999) proposed a summarization system based on \"draft and revision\" together with sentence extraction.", "labels": [], "entities": [{"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9752286672592163}, {"text": "sentence extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7511714398860931}]}, {"text": "The revision part is achieved with the sentence aggregation and smoothing modules.", "labels": [], "entities": []}, {"text": "The cohesion break problem becomes particularly conspicuous in multi-document summarization.", "labels": [], "entities": []}, {"text": "To ameliorate this, revision of the extracted sentences is also thought to be effective, and many ideas and methods have been proposed so far.", "labels": [], "entities": []}, {"text": "For example, Otterbacher and colleagues analyzed manually revised extracts and factored out cohesion problems.", "labels": [], "entities": []}, {"text": "proposed a revision idea that utilizes noun coreference with linguistic quality improvements in mind.", "labels": [], "entities": [{"text": "noun coreference", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7930181622505188}]}, {"text": "Other than the break in cohesion, multidocument summarization faces the problem of information overlap particularly when the document set consists of similar sentences.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7074219882488251}, {"text": "information overlap", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7094419300556183}]}, {"text": "proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a nonoverlapping summary sentence.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.717223733663559}]}, {"text": "Their algorithm firstly analyzes the sentences to obtain the dependency trees and sets a basis tree by finding the centroid of the dependency trees.", "labels": [], "entities": []}, {"text": "It next augments the basis tree with the sub-trees in other sentences and finally prunes the predefined constituents.", "labels": [], "entities": []}, {"text": "Their algorithm was further modified and applied to the German biographies by.", "labels": [], "entities": []}, {"text": "Like the work of and, our work was inspired by the summarization method used by human abstractors.", "labels": [], "entities": []}, {"text": "Actually, our abstractors first extract important sentences, which is called lead identification, and then revise them, which is referred to as phrase elaboration or specification.", "labels": [], "entities": [{"text": "lead identification", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7962207198143005}, {"text": "phrase elaboration or specification", "start_pos": 144, "end_pos": 179, "type": "TASK", "confidence": 0.7574395090341568}]}, {"text": "In this paper, we concentrate on the revision part.", "labels": [], "entities": []}, {"text": "Our work can be viewed as an application of the sentence fusion method to the draft and revision approach to a single Japanese news document summarization.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7126640826463699}, {"text": "Japanese news document summarization", "start_pos": 118, "end_pos": 154, "type": "TASK", "confidence": 0.532161071896553}]}, {"text": "Actually, our dependency structure alignment is almost the same as that of, and our lead sentence plays the role of a basis tree in the.", "labels": [], "entities": []}, {"text": "Though the idea of sentence fusion was developed mainly for suppressing the overlap in multi-document summarization, we consider this effective in augmenting the extracts in a single-document summarization task where we faceless overlap among sentences.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7250753790140152}]}, {"text": "Before explaining the method in detail, we will briefly introduce the Japanese dependency 1 structure on which our idea is based.", "labels": [], "entities": []}, {"text": "The dependency structure is constructed based on the bunsetsu chunk, which we call \"chunk\" for simplicity.", "labels": [], "entities": []}, {"text": "The chunk usually consists of one content-bearing word and a series of function words.", "labels": [], "entities": []}, {"text": "All the chunks in a sentence except for the last one modify a chunk in the right direction.", "labels": [], "entities": []}, {"text": "We call the modifying chunk the modifier and the modified chunk the head.", "labels": [], "entities": []}, {"text": "We usually span a directed edge from a modifier chunk to the head chunk 2 . Our dependency tree has no syntactic information such as subject or object.", "labels": [], "entities": []}, {"text": "showed that most Japanese broadcast news texts are written with a three-part structure, i.e., the lead, body, and supplement.", "labels": [], "entities": []}, {"text": "The most important information is succinctly mentioned in the lead, which is the opening sentence(s) of a news story, referred to as an \"article\" here.", "labels": [], "entities": []}, {"text": "Proper names and details are sometimes avoided in favor of more abstract expressions such as \"big insurance company.\"", "labels": [], "entities": []}, {"text": "The lead is then detailed in the body by answering who, what, when, where, why, and how, and proper names only alluded to in the lead appear here.", "labels": [], "entities": []}, {"text": "Necessary information that was not covered in the lead or the body is placed in the supplement.", "labels": [], "entities": []}, {"text": "The research also reports that professional news abstractors who are hired for digital text services summarize articles in a two-step approach.", "labels": [], "entities": [{"text": "summarize articles", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.8176398873329163}]}, {"text": "First, they identify the lead sentences and set it (them) as the starting point of the summary.", "labels": [], "entities": []}, {"text": "As the average lead length is 95 characters and the al-lowed summary length is about 115 characters (or 150 characters depending on the screen design), they revise the lead sentences using expressions from the remainder of the story.", "labels": [], "entities": []}, {"text": "We see here that the extraction and revision strategy that has been extensively studied by many researchers for various reasons was actually applied by human abstractors, and therefore, the strategy can be used as areal summarization model.", "labels": [], "entities": [{"text": "extraction and revision", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7301038503646851}]}, {"text": "Inspired by this, we decided to study a news summarization system based on the above approach.", "labels": [], "entities": [{"text": "news summarization", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.6671449095010757}]}, {"text": "To develop a complete summarization system, we have to solve three problems: 1) identifying the lead, body, and supplement structure in each article, 2) finding the lead revision candidates, and 3) generating a final summary by selecting and combining the candidates.", "labels": [], "entities": []}, {"text": "We have already studied problem 1) and showed that automatic recognition of three tags with a decision tree algorithm reached a precision over 92% ().", "labels": [], "entities": [{"text": "automatic recognition of three tags", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.7634821832180023}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9990083575248718}]}, {"text": "We then moved to problem 2), which we discuss extensively in the rest of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To see how problem 2) in the previous section could be solved, we conducted a manual leadrevision experiment.", "labels": [], "entities": []}, {"text": "We asked a native Japanese speaker to revise the lead sentences of 15 news articles using expressions from the body section of each article with cut-and-paste operations (insertion and substitution) of bunsetsu chunk sequences.", "labels": [], "entities": []}, {"text": "We refer to chunk sequences as phrases.", "labels": [], "entities": []}, {"text": "We also asked the reviser to find as many revisions as possible.", "labels": [], "entities": []}, {"text": "In the interview with her, we found that she took advantage of the syntactic structure to revise the lead sentences.", "labels": [], "entities": []}, {"text": "Actually, she first searched for the \"same\" chunks in the lead and the body and checked whether the modifier phrases to these chunks could be used for revision.", "labels": [], "entities": [{"text": "revision", "start_pos": 151, "end_pos": 159, "type": "TASK", "confidence": 0.9757025837898254}]}, {"text": "To see what makes these chunks the \"same,\" we compared the syntactic head chunk of the lead and body phrases used for substitution and insertion.", "labels": [], "entities": [{"text": "substitution and insertion", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.6922489404678345}]}, {"text": "summarizes the results of the comparison in three categories: perfect match, partial match (content word match), and different.", "labels": [], "entities": [{"text": "perfect", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.965840220451355}]}, {"text": "The table indicates that nearly half of the head chunks were exactly the same, and the rest contained some differences.", "labels": [], "entities": []}, {"text": "The second row shows the number where the syntactic heads had the same content words but not the same function words.", "labels": [], "entities": []}, {"text": "The pair \u4f1a\u8ac7\u3057 kaidan-shi 'talked' and \u4f1a\u8ac7\u3057\u307e\u3057\u305f kaidan-shi-mashi-ta 'talked' is an The third row represents cases where the syntactic heads had no common surface words.", "labels": [], "entities": []}, {"text": "We found that even in this case, though, the syntactic heads were close in someway.", "labels": [], "entities": []}, {"text": "In one example, there was accordance in the distant heads, for instance, in the pair \u898b \u3064 \u304b \u3063 \u305f mitsuka-tta 'found' and \u4e00\u90e8\u306e ichibu-no 'part of.'", "labels": [], "entities": [{"text": "accordance", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9427884221076965}]}, {"text": "In this case, we can find the chunk \u898b\u3064\u304b\u3063\u305f mitsuka-tta 'found' at a short edge distance from \u4e00 \u90e8\u306e ichibu-no 'part of.'", "labels": [], "entities": []}, {"text": "Based on the findings, we devised a lead sentence revision algorithm.", "labels": [], "entities": [{"text": "sentence revision", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7382802963256836}]}, {"text": "\u2022 Purpose We conducted a lead revision experiment with three purposes.", "labels": [], "entities": []}, {"text": "The first one was to empirically evaluate the validity of our simplified assump-tions: trigger identification and concreteness increase evaluation.", "labels": [], "entities": [{"text": "trigger identification", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.8060564398765564}]}, {"text": "For trigger identification, we basically viewed the identical chunks as triggers and added some amendments for light verbs (nouns) and verb inflections.", "labels": [], "entities": [{"text": "trigger identification", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.861691802740097}]}, {"text": "For the check of an increase in concreteness, we assumed that phrases with more chunks were more concrete.", "labels": [], "entities": []}, {"text": "However, these simplifications should be verified in experiments.", "labels": [], "entities": []}, {"text": "The second purpose was to check the validity of using the revision phrases only in body sentences and not in the supplemental sentences.", "labels": [], "entities": [{"text": "validity", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9854176044464111}]}, {"text": "The last one was to determine how ineffective the result is if the syntactic parsing fails.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.6738928556442261}]}, {"text": "With these purposes in mind, we designed our experiment as follows.", "labels": [], "entities": []}, {"text": "\u2022 Data A total of 257 articles from news programs broadcast on 20 Jan., 20 Apr., and 20 July in 2004 were tagged with lead, body, and supplement tags by a native Japanese evaluator.", "labels": [], "entities": []}, {"text": "The articles were morphologically analyzed by Mecab ( and syntactically parsed by Cabocha ().", "labels": [], "entities": []}, {"text": "\u2022  We prepared an evaluation interface that presents a lead with one revision point (insertion or substitution) that was obtained using the body and supplemental sentences to an evaluator.", "labels": [], "entities": []}, {"text": "A Japanese native speaker evaluated the results one by one with the above interface.", "labels": [], "entities": []}, {"text": "We planned a linguistic evaluation like DUC2005).", "labels": [], "entities": [{"text": "DUC2005", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9483814239501953}]}, {"text": "Since their five-type evaluation is intended for multi-document summarization, whereas our task is single-document summarization, and we are interested in evaluating our questions mentioned above, we carried out the evaluation as follows.", "labels": [], "entities": []}, {"text": "In future, we plan to increase the number of evaluation items and the number of evaluators..", "labels": [], "entities": []}, {"text": "Sentential completeness E1) The evaluator judged if the revision was obtained from the lead and body sentences with or without parsing errors.", "labels": [], "entities": [{"text": "Sentential completeness E1", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.693156361579895}]}, {"text": "Here, errors that did not affect the revision were not considered.", "labels": [], "entities": [{"text": "revision", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.5411332845687866}]}, {"text": "E2) Second, she checked whether the revision was semantically corrector revised information matching the fact described in the lead sentence.", "labels": [], "entities": []}, {"text": "Here, she did not care about the grammaticality or the improvements in concreteness of the revision; if the revision was problematic but manually correctable, it was judged as OK.", "labels": [], "entities": []}, {"text": "This step evaluated the correctness of the trigger selection; wrong triggers, i.e., those referring to different facts produce semantically inconsistent revisions as they mix up different facts.", "labels": [], "entities": []}, {"text": "The following evaluation was done for those judged correct in evaluation step E2, as we found that revisions that were semantically inconsistent with the lead's facts were often too difficult to evaluate further.", "labels": [], "entities": []}, {"text": "E3) Third, she evaluated the change in concreteness after revision with the revisions that passed evaluation E2.", "labels": [], "entities": []}, {"text": "She judged whether or not the revision increased the concreteness of the lead in three categories ().", "labels": [], "entities": []}, {"text": "Notice that original lead sentences are supposed to have an average score of 1.", "labels": [], "entities": []}, {"text": "E4) Last, she checked the sentential completeness of the revision result that passed evaluation E2.", "labels": [], "entities": []}, {"text": "They still contained problems such as grammatical errors and improper insertion position.", "labels": [], "entities": []}, {"text": "Rather than evaluating these items separately, we measured them together for sentential completeness.", "labels": [], "entities": []}, {"text": "At this time, we measured in terms of the number of operations (insertion, deletion, substitution) needed to make the sentence complete 5 . As shown in, revisions requiring more than two operations are categorized as \"poor,\" those requiring one operation are \"acceptable,\" and those requiring no operations are \"perfect.\"", "labels": [], "entities": []}, {"text": "We employed this measure because we found that grading detailed items such as grammaticality and insertion positions at fine levels was rather difficult.", "labels": [], "entities": []}, {"text": "We also found that native Japanese speakers can correct errors easily.", "labels": [], "entities": []}, {"text": "Notice the lead sentences are perfect and are supposed to have an average score of 2 in sentential completeness.", "labels": [], "entities": []}, {"text": "Since the revision does not improve the completeness further but elicits defects such as grammatical errors, it usually produces a score below 2.", "labels": [], "entities": []}, {"text": "Some examples of the results with their scores are shown below.", "labels": [], "entities": []}, {"text": "The underlined parts are the inserted body chunk phrases, and the parenthesized parts are the deleted lead chunks.", "labels": [], "entities": []}, {"text": "Example 1 is the perfect substitution and had scores of 2 for both concreteness increase and completeness.", "labels": [], "entities": []}, {"text": "Actually, the originally vaguely mentioned term 'event' was replaced by a more concrete phrase with proper names, 'Korean Peninsula Peace Forum sponsored by Korea Society and others.'", "labels": [], "entities": [{"text": "Korean Peninsula Peace Forum sponsored by Korea Society", "start_pos": 115, "end_pos": 170, "type": "DATASET", "confidence": 0.8967084884643555}]}, {"text": "Notice that this can be achieved by NP coreference based methods if they can identify that these two different phrases are coreferential.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.6809491515159607}]}, {"text": "Our method does this through the dependency on the same trigger \u51fa\u5e2d\u3059\u308b shusseki-suru 'attend.'", "labels": [], "entities": []}, {"text": "Example 2 is a perfect sentence, but its concreteness stayed at the same level.", "labels": [], "entities": []}, {"text": "As a result, the scores were 1 for concreteness increase and 2 for completeness..", "labels": [], "entities": [{"text": "concreteness increase", "start_pos": 35, "end_pos": 56, "type": "METRIC", "confidence": 0.6282587349414825}]}, {"text": "Results of sentential completeness Actually, the original sentence that meant \"They found a crack in the parts\" was revised to \"They found there was a crack in the parts,\" which did not add useful information.", "labels": [], "entities": [{"text": "sentential completeness", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7179060280323029}]}, {"text": "Example 3 has a grammatical problem although the revision supplied useful information.As a result, it had scores of 2 for concreteness increase and 0 for completeness.", "labels": [], "entities": []}, {"text": "The added kara-case phrase (from phrase) \u5730\u4e0a\u4e8c\u5341\u30e1\u30fc\u30c8\u30eb\u306e\u9ad8\u3055\u304b\u3089 chijouniju-metoru-no-takasa-kara 'from 20 meters high' is useful, but since the original sentence already has the kara-case \u30d8\u30ea\u30b3\u30d7\u30bf\u30fc\u304b\u3089 herikoputa-kara 'from helicopter,' the insertion invited a double kara-case, which is forbidden in Japanese.", "labels": [], "entities": []}, {"text": "To correct the error, we need at least two operations, and thus, a completeness score of 0 was assigned.", "labels": [], "entities": [{"text": "completeness score", "start_pos": 67, "end_pos": 85, "type": "METRIC", "confidence": 0.9715488255023956}]}, {"text": "presents the results of evaluation E2, the semantic correctness with the parsing status of evaluation E1 and the source sentence category from which the phrases for revision were obtained.", "labels": [], "entities": []}, {"text": "Columns 2 and 3 list the number of revisions (insertions and substitutions) that were correct and incorrect and column 4 shows the correctness ratio.", "labels": [], "entities": []}, {"text": "We obtained a total of 603 revisions and found that 30% (180/603) of them were derived with syntactic errors.", "labels": [], "entities": []}, {"text": "The semantic correctness ratio was unchanged regardless of the parsing success.", "labels": [], "entities": []}, {"text": "On the contrary, it was affected by the source sentence type.", "labels": [], "entities": []}, {"text": "The correctness ratio with the supplemental sentence was significantly 6 lower than that with the body sentence.", "labels": [], "entities": [{"text": "correctness ratio", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9757183790206909}]}, {"text": "lists the results of the concreteness improvements with the parsing status and the source sentence type.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9612281322479248}]}, {"text": "Columns 2, 3 and 4 list the number of revisions that fell in the scores (0-2) listed in the first row.", "labels": [], "entities": []}, {"text": "The average score in this table again was not affected by the parsing failure but was significantly affected by the source sentence category.", "labels": [], "entities": [{"text": "parsing", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9629820585250854}]}, {"text": "The result with the supplement sentences was significantly worse than that with body sentences.", "labels": [], "entities": []}, {"text": "lists the results of the sentential completeness in the same fashion as.", "labels": [], "entities": []}, {"text": "The sentential completeness was significantly worsened by both the parsing failure and source sentence category.", "labels": [], "entities": [{"text": "sentential completeness", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8201623260974884}, {"text": "parsing", "start_pos": 67, "end_pos": 74, "type": "TASK", "confidence": 0.9678549766540527}]}, {"text": "These results indicate that the answers to the questions posed at the beginning of this section are as follows.", "labels": [], "entities": []}, {"text": "From the semantic correctness evaluation, we infer that our trigger selection strategy worked well especially when the source sentence category was limited to the body.", "labels": [], "entities": []}, {"text": "From the concreteness-increase evaluation, the assumption that we made also worked reasonably well when the source sentence category was limited to the body.", "labels": [], "entities": []}, {"text": "The effect of parsing was much more limited than we had anticipated in that it did not degrade either the semantic correctness or the concreteness improvements.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9779036641120911}]}, {"text": "Parsing failure, however, degraded the sentential completeness of the revised sentences.", "labels": [], "entities": []}, {"text": "This seems quite reasonable: parsing errors elicit problems such as wrong phrase attachment and wrong maximum phrase identification.", "labels": [], "entities": [{"text": "wrong phrase attachment", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.6172313888867696}, {"text": "wrong maximum phrase identification", "start_pos": 96, "end_pos": 131, "type": "TASK", "confidence": 0.6289462000131607}]}, {"text": "The revisions with these errors invite incomplete sentences that need corrections.", "labels": [], "entities": []}, {"text": "It is worth noting that cases sometimes occurred where a parsing error did not cause any problem in the revision.", "labels": [], "entities": []}, {"text": "We found that the phrases governed by a trigger pair in many cases were quite similar, and therefore, the parser makes the same error.", "labels": [], "entities": []}, {"text": "In that case, the errors are often offset and cause no problems superficially.", "labels": [], "entities": []}, {"text": "We consider that the sentential completeness needs further improvements to make an automatic summarization system, although the semantic correctness and concreteness increase are at an almost satisfactory level.", "labels": [], "entities": [{"text": "summarization", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.9662142395973206}]}, {"text": "Our dependencybased revision is expected to be potentially useful to develop a summarization system.", "labels": [], "entities": [{"text": "summarization", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.984889805316925}]}], "tableCaptions": [{"text": " Table 1. Degree of syntactic head agreement", "labels": [], "entities": [{"text": "syntactic head agreement", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6793463031450907}]}, {"text": " Table 3. Evaluation of increased concreteness", "labels": [], "entities": [{"text": "concreteness", "start_pos": 34, "end_pos": 46, "type": "TASK", "confidence": 0.6772052049636841}]}, {"text": " Table 5. Results of semantic correctness", "labels": [], "entities": [{"text": "semantic correctness", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.836538165807724}]}, {"text": " Table 7. Results of sentential completeness", "labels": [], "entities": [{"text": "sentential completeness", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.9157336354255676}]}]}