{"title": [{"text": "Inference Rules for Recognizing Textual Entailment", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.8866754770278931}]}], "abstractContent": [{"text": "In this paper, we explore the application of inference rules for recognizing textual entailment (RTE).", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 65, "end_pos": 101, "type": "TASK", "confidence": 0.83281276623408}]}, {"text": "We start with an automatically acquired collection and then propose methods to refine it and obtain more rules using a hand-crafted lexical resource.", "labels": [], "entities": []}, {"text": "Following this, we derive a dependency-based representation from texts, which aims to provide a proper base for the inference rule application.", "labels": [], "entities": []}, {"text": "The evaluation of our approach on the RTE data shows promising results on precision and the error analysis suggests future improvements.", "labels": [], "entities": [{"text": "RTE data", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.855536937713623}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9996563196182251}, {"text": "error", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9585074782371521}]}], "introductionContent": [{"text": "Textual inference plays an important role in many natural language processing (NLP) tasks, such as question answering.", "labels": [], "entities": [{"text": "Textual inference", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7138361930847168}, {"text": "question answering", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.9015874564647675}]}, {"text": "In recent years, the recognizing textual entailment (RTE) challenge, which focuses on detecting semantic inference, has attracted a lot of attention.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.8735718925793966}, {"text": "detecting semantic inference", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.8153281807899475}]}, {"text": "Given a text T (several sentences) and a hypothesis H (one sentence), the goal is to detect if H can be inferred from T.", "labels": [], "entities": []}, {"text": "Studies such as attest that lexical substitution (e.g. synonyms, antonyms) or simple syntactic variation accounts for the entailment only in a small number of pairs.", "labels": [], "entities": []}, {"text": "Thus, one essential issue is to identify more complex expressions which, inappropriate contexts, convey the same (or similar) meaning.", "labels": [], "entities": []}, {"text": "More generally, we are also interested in pairs of expressions in which only a uni-directional inference relation holds . A typical example is the following RTE pair in which accelerate to in H is used as an alternative formulation for reach speed of in T.", "labels": [], "entities": []}, {"text": "T: The high-speed train, scheduled fora trial run on Tuesday, is able to reach a maximum speed of up to 430 kilometers per hour, or 119 meters per second.", "labels": [], "entities": [{"text": "T", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9844740629196167}]}, {"text": "H: The train accelerates to 430 kilometers per hour.", "labels": [], "entities": []}, {"text": "One way to deal with textual inference is through rule representation, such as X wrote Y \u2248 X is author of Y.", "labels": [], "entities": []}, {"text": "However, manually building collections of inference rules is time-consuming and it is unlikely that humans can exhaustively enumerate all the rules encoding the knowledge needed in reasoning with natural languages.", "labels": [], "entities": []}, {"text": "Instead, an alternative is to acquire these rules automatically from large corpora.", "labels": [], "entities": []}, {"text": "Furthermore, given such a rule collection, how to successfully use it in NLP applications is the next step to be focused on.", "labels": [], "entities": []}, {"text": "For the first aspect, we extend and refine an existing collection of inference rules acquired based on the Distributional Hypothesis (DH).", "labels": [], "entities": []}, {"text": "One of the main advantages of using DH is that the only input needed is a large corpus of (parsed) text 2 . For this purpose, a hand-crafted lexical resource is used for augmenting the original inference rule collection and excluding some of the incorrect rules.", "labels": [], "entities": []}, {"text": "For the second aspect, we focus on applying these rules to the RTE task.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 63, "end_pos": 71, "type": "TASK", "confidence": 0.9151968657970428}]}, {"text": "In particular, we use a structure representation derived from the dependency parse trees of T and H, which aims to capture the essential information they convey.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 introduces the inference rule collection we use, based on the Discovery of Inference Rules from Text (henceforth DIRT) algorithm; we also discuss previous work on applying it to the RTE task.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 239, "end_pos": 247, "type": "TASK", "confidence": 0.9310263395309448}]}, {"text": "Section 3 presents our analyses on the RTE data and discusses two issues: the lack of rules and the difficulty of finding proper ways of applying them.", "labels": [], "entities": [{"text": "RTE data", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8193311095237732}]}, {"text": "Section 4 proposes methods to extend and refine the rule collection aiming at the former issue.", "labels": [], "entities": [{"text": "rule collection", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.8418911695480347}]}, {"text": "To address the latter issue, Section 5 describes the structure representation we use to identify the appropriate context for the rule application.", "labels": [], "entities": []}, {"text": "The experiments will be presented in Section 6, followed by an error analysis and discussions in Section 7.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 63, "end_pos": 77, "type": "METRIC", "confidence": 0.9197760820388794}]}, {"text": "Finally, Section 8 will conclude the paper and point out some future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments consist in predicting positive entailment in a very straightforward rule-based manner.", "labels": [], "entities": [{"text": "predicting positive entailment", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.9016255935033163}]}, {"text": "For each collection we select the RTE pairs in which we find a tree skeleton and match an inference rule.", "labels": [], "entities": []}, {"text": "The first number in our table entries represents how many of such pairs we have identified, out of 1600 development and test pairs.", "labels": [], "entities": []}, {"text": "For these pairs we simply predict positive entailment and the second entry represents what percentage of these pairs are indeed true entailment.", "labels": [], "entities": []}, {"text": "Our work does not focus on building a complete RTE system but we also combine our method with a bag of words baseline to seethe effects on the entire data set.", "labels": [], "entities": []}, {"text": "In the first two columns: Dirt T Sand Dirt+WN T S ) we consider DIRT in its original state and DIRT with rules generated with WordNet as described in Section 4; all precisions are higher than 63% 4 . After adding WordNet, tree skeletons and rules are matched in approximately twice as many pairs, while the precision is not harmed.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.9686269760131836}, {"text": "precisions", "start_pos": 165, "end_pos": 175, "type": "METRIC", "confidence": 0.9603379964828491}, {"text": "WordNet", "start_pos": 213, "end_pos": 220, "type": "DATASET", "confidence": 0.948866605758667}, {"text": "precision", "start_pos": 307, "end_pos": 316, "type": "METRIC", "confidence": 0.9992419481277466}]}, {"text": "This may indicate that our method of adding rules does not decrease precision of an RTE system.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9992793202400208}]}, {"text": "In the third column we report the results of using a set of rules containing only the trivial identity ones (Id T S ).", "labels": [], "entities": []}, {"text": "For our current system, this can be seen as a precision upper bound for all the other collections, in concordance  with the fact that identical rules are nothing but inference rules of highest possible confidence.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.998830258846283}]}, {"text": "The fourth column (Dirt+Id+WN T S ) contains what can be considered our best setting.", "labels": [], "entities": [{"text": "Id+WN T S )", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.6607441405455271}]}, {"text": "In this setting three times as many pairs are covered using a collection containing DIRT and identity rules with WordNet extension.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9279360771179199}]}, {"text": "Although the precision results with this setting are encouraging (65% for RTE2 data and 69% for RTE3 data), the coverage is still low, 8% for RTE2 and 6% for RTE3.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9995852112770081}, {"text": "RTE2 data", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.8666836023330688}, {"text": "coverage", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9959445595741272}]}, {"text": "This aspect together with an error analysis we performed are the focus of Section 7.", "labels": [], "entities": []}, {"text": "Another experiment aimed at improving the precision of our predictions.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.999197781085968}]}, {"text": "For this we further restrict our method: we have a true entailment only if applying the inference rule to a TS leaves no unmatched lexical items in the fragment of the dependency path where it has been identified.", "labels": [], "entities": []}, {"text": "The more restricted method (Dirt+Id+WN * T S ) gives, as expected, better precision with an approximately 30% loss in coverage.", "labels": [], "entities": [{"text": "Dirt+Id+WN * T S )", "start_pos": 28, "end_pos": 46, "type": "METRIC", "confidence": 0.5399904714690315}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994205236434937}, {"text": "coverage", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9917138814926147}]}, {"text": "At last, we also integrate our method with a bag of words baseline, which calculates the ratio of overlapping words in T and H.", "labels": [], "entities": []}, {"text": "For the pairs that our method covers, we overrule the baseline's decision.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "On the full data set, the improvement is still small due to the low coverage of our method, however on the pairs that are covered by our method, there is a significant improvement over the overlap baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on tree skeletons with various rule collections", "labels": [], "entities": []}]}