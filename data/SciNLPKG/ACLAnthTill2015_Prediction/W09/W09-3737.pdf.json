{"title": [{"text": "Towards a Cognitive Approach for the Automated Detection of Connotative Meaning", "labels": [], "entities": [{"text": "Automated Detection of Connotative Meaning", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.8210688531398773}]}], "abstractContent": [], "introductionContent": [{"text": "The goal of the research described here is to automate the recognition of connotative meaning in text using a range of linguistic and non-linguistic features.", "labels": [], "entities": [{"text": "recognition of connotative meaning in text", "start_pos": 59, "end_pos": 101, "type": "TASK", "confidence": 0.8704961836338043}]}, {"text": "Pilot results are used to illustrate the potential of an integrated multidisciplinary approach to semantic text analysis that combines cognitiveoriented human subject experimentation with Machine Learning (ML) based Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "semantic text analysis", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.7607292731602987}]}, {"text": "The research presented here was funded through the Advanced Question and Answering for Intelligence (AQUAINT) Project of the U.S. federal government's Intelligence Advanced Research Projects Activity (IARPA) Office.", "labels": [], "entities": [{"text": "Advanced Question and Answering for Intelligence (AQUAINT)", "start_pos": 51, "end_pos": 109, "type": "TASK", "confidence": 0.7070694168408712}]}, {"text": "Funded as an exploratory \"Blue Sky\" project, this award enabled us to develop an extensible experimental setup and to make progress towards training a machine learning system.", "labels": [], "entities": [{"text": "Blue Sky\"", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.8466360171635946}]}, {"text": "Automated understanding of connotative meaning of text requires an understanding of both the mechanics of text and the human behaviors involved in the disambiguation of text.", "labels": [], "entities": []}, {"text": "We glean more from text than what can be explicitly parsed from parts of speech or named entities.", "labels": [], "entities": []}, {"text": "There are other aspects of meaning that humans takeaway from text, such as a sincere apology, an urgent request for help, a serious warning, or a perception of personal threat.", "labels": [], "entities": []}, {"text": "Merging cognitive and social cognitive psychology research with sophisticated machine learning could extend current NLP systems to account for these aspects.", "labels": [], "entities": []}, {"text": "Building on current natural language processing research, this pilot project encapsulates an end-to-end research methodology that begins by 1) establishing a human-understanding baseline for the distinction between connotative and denotative meaning, 2) then extends the analysis of the mechanics of literal versus non-literal meaning by applying NLP tools to the human-annotated text, and 3) uses these cumulative results to feed a machine learning system that will be taught to recognize the potential for connotative meaning at the sentence level, across a much broader corpus.", "labels": [], "entities": []}, {"text": "This paper describes the preliminary iteration of this methodology and suggests ways that this approach could be improved for future applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "Next, our efforts focused on designing a reusable and scalable online evaluation tool that would allow us to systematically gather multiple judgments for each sentence using a much larger pool of stimulus text.", "labels": [], "entities": []}, {"text": "Scaling up the human evaluations also allowed us to decipher between responses that were either systematically patterned or more idiosyncratic (or random).", "labels": [], "entities": []}, {"text": "According to our forced-choice design, each online participant was presented with a series of 32 pairs of sentences, one pair at a time, and asked to identify the sentence that provided more of an opportunity to read between the lines.", "labels": [], "entities": []}, {"text": "Half the participants were presented with a positive prompt (which sentence provides the most opportunity) and half were presented with a negative prompt (which sentence provides the least opportunity).", "labels": [], "entities": []}, {"text": "Positive/negative assignment was determined randomly.", "labels": [], "entities": []}, {"text": "The 16 sentences selected during the first round were re-paired in a second round.", "labels": [], "entities": []}, {"text": "This continued until 4 sentences remained, representing sentences that were more strongly connotative or denotative, depending on the prompt.", "labels": [], "entities": []}, {"text": "Final sentence scores were averaged across all evaluations received.", "labels": [], "entities": []}, {"text": "The forced choice scenario requires a sample of only 13 participants to evaluate 832 sentences.", "labels": [], "entities": []}, {"text": "This was a significant improvement over previous methods, increasing the number of sentences and the number of evaluations per sentence and therefore increasing the reliability of our findings.", "labels": [], "entities": [{"text": "reliability", "start_pos": 165, "end_pos": 176, "type": "METRIC", "confidence": 0.9913020133972168}]}, {"text": "For example, using this scalable setup on a set of 832 sentences we need only 26 participants to generate two evaluations per sentence in the set, 39 participants to yield three evaluations per sentence, etc.", "labels": [], "entities": []}, {"text": "We ran the system with a randomly selected sample of both sentences and participants with the intent to eventually make direct comparison among more controlled samples of sentences and participants.", "labels": [], "entities": []}, {"text": "This has direct implication for the evaluation phase of our pilot.", "labels": [], "entities": []}, {"text": "Because sentences were selected at random, without guarantee of a certain number of each type of sentence, our goal was to achieve results on a par with chance.", "labels": [], "entities": []}, {"text": "Anything else would reveal systematic bias in the experiment design or implementation.", "labels": [], "entities": []}, {"text": "This also provides us with a baseline for future investigations where the stimulus text would be more wilfully controlled.", "labels": [], "entities": []}, {"text": "In the first iteration of the pilot setup, each of 832 sentences were viewed by six different participants, three assigned to a positive group and three to a negative group, as described above.", "labels": [], "entities": []}, {"text": "The denotative condition ranged in ratings from 0 to -3 while the connotative condition ranged in rating from 0 to 3.", "labels": [], "entities": []}, {"text": "These were then averaged to achieve an overall score for each sentence.", "labels": [], "entities": []}, {"text": "Because they were randomly selected, each sentence had predictable chance of ultimately being identified as connotative or denotative.", "labels": [], "entities": []}, {"text": "In other words, each sentence had an equal chance of being identified as connotative.", "labels": [], "entities": []}, {"text": "Having established a baseline based on chance, we can next control for various features and evaluate the relative impact as systematic differences from the baseline.", "labels": [], "entities": []}, {"text": "We will be able to say with a relatively high degree of certainty that \"x,\" \"y\" or \"z\" feature, sentence structure, behavior, etc. was responsible for skewing the odds in a reliable manner because we will be able to control for these variables across various experimental scenarios.", "labels": [], "entities": []}, {"text": "This, combined with improved validity resulting from an increased number of human judgments and an increased number of sentences viewed, marks the strength of this methodology.", "labels": [], "entities": [{"text": "validity", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9971393346786499}]}, {"text": "Additionally, we will be able to compare sentences within each scenario even when an overall chance outcome occurs.", "labels": [], "entities": []}, {"text": "For example, in the initial run of our sentences, we achieved an overall chance outcome.", "labels": [], "entities": []}, {"text": "However, \"anomalies\" emerged, sentences that were strongly skewed towards being assigned a neutral evaluation score or towards an extreme score (either distinctly connotative or distinctly denotative).", "labels": [], "entities": []}, {"text": "This allowed us to gather a reliable and valid subset of data that can be utilized in ML experiments.", "labels": [], "entities": [{"text": "ML", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9868337512016296}]}, {"text": "See below fora very shortlist of sample sentences grouped according to the overall scores they received determine by the six human reviewers: Denotative examples- \u2022 The equipment was a radar system.", "labels": [], "entities": []}, {"text": "\u2022 Kosovo has been part of modern day Serbia since 1912.", "labels": [], "entities": []}, {"text": "\u2022 The projected figure for 2007 is about $ 3100.", "labels": [], "entities": []}, {"text": "Connotative examples- \u2022 In fact, do what you bloody well like . \u2022 But it's pretty interesting , in a depressing sort of way . \u2022 It's no more a language than American English or Quebecois French  Our preliminary analysis suggests that humans are consistent in recognizing the extremes of connotative and denotative sentences and an automatic recognition system could be built to identify when a text is likely to convey connotative meaning.", "labels": [], "entities": []}, {"text": "Machine Learning (ML) techniques could be used to enable a system to first classify a text according to whether it conveys a connotative or denotative level of meaning, and eventually, identify specific connotations.", "labels": [], "entities": []}, {"text": "ML techniques usually assume a feature space within which the system learns the relative importance of features to use in classification.", "labels": [], "entities": [{"text": "ML", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9631989002227783}]}, {"text": "Since humans process language at various levels (morphological, lexical, syntactic, semantic, discourse and pragmatic), some multi-level combination of features is helping them reach consistent conclusions.", "labels": [], "entities": []}, {"text": "Hence, the initial machine learning classification decision will be made based on a class of critical features, as cognitive and social-cognitive theory suggests happens inhuman interpretation of text.", "labels": [], "entities": [{"text": "machine learning classification", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.656409869591395}]}, {"text": "TextTagger, an Information Extraction System developed at Syracuse University's Center for Natural Language Processing, currently can identify sentence boundaries, part-of-speech tag words, stem and lemmatize words, identify various types of phrases, categorize named entities and common nouns, recognize relations, and resolve co-references in text.", "labels": [], "entities": []}, {"text": "We are in the process of designing a ML framework that utilizes these tags and can learn from a few examples provided by the human subject experiments described above, then train on other sets of similar data marked by analysts as possessing the features illustrated by the sentences consistently identified as conveying connotative meaning.", "labels": [], "entities": []}, {"text": "For preliminary ML-based analysis, the data collection included 266 sentences (from the original 832 used inhuman subject experiments), 145 tagged as strongly connotative and 121 tagged as strongly denotative by subjects.", "labels": [], "entities": [{"text": "ML-based analysis", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.9344102740287781}]}, {"text": "Fifty sentences from each set became a test collection and the remaining 95 connotative and 71 denotative sentences were used for training.", "labels": [], "entities": []}, {"text": "Our baseline results (without TextTagger annotations) were: Precision: 44.77 ; Recall: 60; F: 51.28.", "labels": [], "entities": [{"text": "Precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.997210681438446}, {"text": "Recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9935004115104675}, {"text": "F", "start_pos": 91, "end_pos": 92, "type": "METRIC", "confidence": 0.994785487651825}]}, {"text": "After tagging, when we only use proper names and common nouns the results improved: Precision: 51.61 Recall: 92; F: 67.13.", "labels": [], "entities": [{"text": "Precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9986898303031921}, {"text": "Recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9671118259429932}, {"text": "F", "start_pos": 113, "end_pos": 114, "type": "METRIC", "confidence": 0.9981728792190552}]}, {"text": "Although these results are not as high as some categorization results reported in the literature for simpler categorization tasks such as document labeling or spam identification, we believe that using higher level linguistic features extracted by our NLP technology will significantly improve them.", "labels": [], "entities": [{"text": "document labeling", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7097561955451965}, {"text": "spam identification", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.840257316827774}]}, {"text": "More sophisticated analysis will be conducted during future applications of this methodology.", "labels": [], "entities": []}], "tableCaptions": []}