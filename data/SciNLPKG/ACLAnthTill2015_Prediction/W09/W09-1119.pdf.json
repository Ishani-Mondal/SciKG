{"title": [{"text": "Design Challenges and Misconceptions in Named Entity Recognition * \u2020 \u2021", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6790840029716492}]}], "abstractContent": [{"text": "We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system.", "labels": [], "entities": [{"text": "NER", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.945247232913971}]}, {"text": "In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system.", "labels": [], "entities": []}, {"text": "In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F 1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.", "labels": [], "entities": [{"text": "90.8 F 1 score", "start_pos": 151, "end_pos": 165, "type": "METRIC", "confidence": 0.8542306572198868}, {"text": "CoNLL-2003 NER shared task", "start_pos": 173, "end_pos": 199, "type": "DATASET", "confidence": 0.7928410321474075}]}], "introductionContent": [{"text": "Natural Language Processing applications are characterized by making complex interdependent decisions that require large amounts of prior knowledge.", "labels": [], "entities": []}, {"text": "In this paper we investigate one such applicationNamed Entity Recognition (NER).", "labels": [], "entities": [{"text": "applicationNamed Entity Recognition (NER)", "start_pos": 38, "end_pos": 79, "type": "TASK", "confidence": 0.7453176379203796}]}, {"text": "illustrates the necessity of using prior knowledge and non-local decisions in NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9401934146881104}]}, {"text": "In the absence of mixed case information it is difficult to understand that * The system and the Webpages dataset are available at: http://l2r.cs.uiuc.edu/\u223ccogcomp/software.php \u2020 This work was supported by NSF grant NSF SoD-HCER-0613885, by MIAS, a DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC and by an NDIIPP project from the National Library of Congress.", "labels": [], "entities": []}, {"text": "\u2021 We thank Nicholas Rizzolo for the baseline LBJ NER system, Xavier Carreras for suggesting the word class models, and multiple reviewers for insightful comments.", "labels": [], "entities": []}, {"text": "\"BLINKER\" is a person.", "labels": [], "entities": [{"text": "BLINKER", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.9849886894226074}]}, {"text": "Likewise, it is not obvious that the last mention of \"Wednesday\" is an organization (in fact, the first mention of \"Wednesday\" can also be understood as a \"comeback\" which happens on Wednesday).", "labels": [], "entities": []}, {"text": "An NER system could take advantage of the fact that \"blinker\" is also mentioned later in the text as the easily identifiable \"Reggie Blinker\".", "labels": [], "entities": [{"text": "Reggie Blinker", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.7806917130947113}]}, {"text": "It is also useful to know that Udinese is a soccer club (an entry about this club appears in Wikipedia), and the expression \"both Wednesday and Udinese\" implies that \"Wednesday\" and \"Udinese\" should be assigned the same label.", "labels": [], "entities": []}, {"text": "The above discussion focuses on the need for external knowledge resources (for example, that Udinese can be a soccer club) and the need for nonlocal features to leverage the multiple occurrences of named entities in the text.", "labels": [], "entities": []}, {"text": "While these two needs have motivated some of the research in NER in the last decade, several other fundamental decisions must be made.", "labels": [], "entities": [{"text": "NER", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9467018842697144}]}, {"text": "These include: what model to use for sequential inference, how to represent text chunks and what inference (decoding) algorithm to use.", "labels": [], "entities": []}, {"text": "Despite the recent progress in NER, the effort has been dispersed in several directions and there are no published attempts to compare or combine the recent advances, leading to some design misconceptions and less than optimal performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9394538402557373}]}, {"text": "In this paper we analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system.", "labels": [], "entities": [{"text": "NER", "start_pos": 143, "end_pos": 146, "type": "TASK", "confidence": 0.947636067867279}]}, {"text": "We find that BILOU representation of text chunks significantly outperforms the widely adopted BIO.", "labels": [], "entities": [{"text": "BILOU", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.7519471645355225}, {"text": "BIO", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.6428001523017883}]}, {"text": "Surprisingly, naive greedy inference performs comparably to beamsearch or Viterbi, while being considerably more computationally efficient.", "labels": [], "entities": []}, {"text": "We analyze several approaches for modeling non-local dependencies proposed in the literature and find that none of them clearly outperforms the others across several datasets.", "labels": [], "entities": []}, {"text": "However, as we show, these contributions are, to a large extent, independent and, as we show, the approaches can be used together to yield better results.", "labels": [], "entities": []}, {"text": "Our experiments corroborate recently published results indicating that word class models learned on unlabeled text can significantly improve the performance of the system and can bean alternative to the traditional semi-supervised learning paradigm.", "labels": [], "entities": []}, {"text": "Combining recent advances, we develop a publicly available NER system that achieves 90.8 F 1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.", "labels": [], "entities": [{"text": "90.8 F 1 score", "start_pos": 84, "end_pos": 98, "type": "METRIC", "confidence": 0.9060736894607544}, {"text": "CoNLL-2003 NER shared task", "start_pos": 106, "end_pos": 132, "type": "DATASET", "confidence": 0.8146364986896515}]}, {"text": "Our system is robust -it consistently outperforms all publicly available NER systems (e.g., the Stanford NER system) on all three datasets.", "labels": [], "entities": [{"text": "Stanford NER system", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.8364025155703226}]}], "datasetContent": [{"text": "NER system should be robust across multiple domains, as it is expected to be applied on a diverse set of documents: historical texts, news articles, patent applications, webpages etc.", "labels": [], "entities": []}, {"text": "Therefore, we have considered three datasets: CoNLL03 shared task data, MUC7 data and a set of Webpages we have annotated manually.", "labels": [], "entities": [{"text": "CoNLL03 shared task data", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.7829268872737885}, {"text": "MUC7 data", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.7950828373432159}]}, {"text": "In the experiments throughout the paper, we test the ability of the tagger to adapt to new test domains.", "labels": [], "entities": []}, {"text": "Throughout this work, we train on the CoNLL03 data and test on the other datasets without retraining.", "labels": [], "entities": [{"text": "CoNLL03 data", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9732247292995453}]}, {"text": "The differences in annotation schemes across datasets created evaluation challenges.", "labels": [], "entities": []}, {"text": "We discuss the datasets and the evaluation methods below.", "labels": [], "entities": []}, {"text": "The CoNLL03 shared task data is a subset of Reuters 1996 news corpus annotated with 4 entity types: PER,ORG, LOC, MISC.", "labels": [], "entities": [{"text": "CoNLL03 shared task data", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8213215619325638}, {"text": "Reuters 1996 news corpus", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.9400985538959503}, {"text": "PER,ORG", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9797659516334534}, {"text": "LOC", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.8581345081329346}, {"text": "MISC", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.8656005263328552}]}, {"text": "It is important to notice that both the training and the development datasets are news feeds from August 1996, while the test set contains news feeds from December 1996.", "labels": [], "entities": []}, {"text": "The named entities mentioned in the test dataset are considerably different from those that appear in the training or the development set.", "labels": [], "entities": []}, {"text": "As a result, the test dataset is considerably harder than the development set.", "labels": [], "entities": []}, {"text": "Evaluation: Following the convention, we report phrase-level F 1 score.", "labels": [], "entities": [{"text": "phrase-level F 1 score", "start_pos": 48, "end_pos": 70, "type": "METRIC", "confidence": 0.8357921689748764}]}, {"text": "The MUC7 dataset is a subset of the North American News Text Corpora annotated with a wide variety of entities including people, locations, organizations, temporal events, monetary units, and soon.", "labels": [], "entities": [{"text": "MUC7 dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9596824645996094}, {"text": "North American News Text Corpora annotated", "start_pos": 36, "end_pos": 78, "type": "DATASET", "confidence": 0.8846295475959778}]}, {"text": "Since there was no direct mapping from temporal events, monetary units, and other entities from MUC7 and the MISC label in the CoNLL03 dataset, we measure performance only on PER,ORG and LOC.", "labels": [], "entities": [{"text": "MUC7", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.9401389360427856}, {"text": "MISC label", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.814494401216507}, {"text": "CoNLL03 dataset", "start_pos": 127, "end_pos": 142, "type": "DATASET", "confidence": 0.9307284653186798}, {"text": "PER,ORG", "start_pos": 175, "end_pos": 182, "type": "METRIC", "confidence": 0.9167912602424622}]}, {"text": "Evaluation: There are several sources of inconsistency in annotation between MUC7 and CoNLL03.", "labels": [], "entities": [{"text": "MUC7", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.7826782464981079}]}, {"text": "For example, since the MUC7 dataset does not contain the MISC label, in the sentence \"balloon, called the Virgin Global Challenger\" , the expression Virgin Global Challenger should be labeled as MISC according to CoNLL03 guidelines.", "labels": [], "entities": [{"text": "MUC7 dataset", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.9662477076053619}, {"text": "Virgin Global Challenger", "start_pos": 106, "end_pos": 130, "type": "DATASET", "confidence": 0.9720824360847473}, {"text": "Virgin Global Challenger", "start_pos": 149, "end_pos": 173, "type": "DATASET", "confidence": 0.9684915343920389}, {"text": "CoNLL03", "start_pos": 213, "end_pos": 220, "type": "DATASET", "confidence": 0.9184262752532959}]}, {"text": "However, the gold annotation in MUC7 is \"balloon, called the [ORG Virgin] Global Challenger\".", "labels": [], "entities": [{"text": "MUC7", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9293519854545593}, {"text": "ORG Virgin] Global Challenger", "start_pos": 62, "end_pos": 91, "type": "DATASET", "confidence": 0.9274202704429626}]}, {"text": "These and other annotation inconsistencies have prompted us to relax the requirements of finding the exact phrase boundaries and measure performance using token-level F 1 . Webpages -we have assembled and manually annotated a collection of 20 webpages, including personal, academic and computer-science conference homepages.", "labels": [], "entities": []}, {"text": "The dataset contains 783 entities.", "labels": [], "entities": []}, {"text": "Evaluation: The named entities in the webpages were highly ambiguous and very different from the named entities seen in the training data.", "labels": [], "entities": []}, {"text": "For example, the data included sentences such as : \"Hear, O Israel, the Lord our God, the Lord is one.\"", "labels": [], "entities": []}, {"text": "We could not agree on whether \"O Israel\" should be labeled as ORG, LOC, or PER.", "labels": [], "entities": [{"text": "ORG", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9940335154533386}, {"text": "LOC", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7914435267448425}, {"text": "PER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9916015267372131}]}, {"text": "Similarly, we could not agree on whether \"God\" and \"Lord\" is an ORG or PER.", "labels": [], "entities": [{"text": "ORG", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9936121106147766}, {"text": "PER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9028669595718384}]}, {"text": "These issues led us to report token-level entity-identification F 1 score for this dataset.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.8350240190823873}]}, {"text": "That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Phrase-level F1 performance of different inference", "labels": [], "entities": [{"text": "Phrase-level F1 performance", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.6955256462097168}]}, {"text": " Table 2: End system performance with BILOU and BIO", "labels": [], "entities": [{"text": "End", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9248246550559998}, {"text": "BILOU", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9924801588058472}, {"text": "BIO", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9764304757118225}]}, {"text": " Table 3: The utility of non-local features. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and", "labels": [], "entities": [{"text": "CoNLL03 data", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9781312048435211}, {"text": "CoNNL03", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9685049653053284}, {"text": "MUC7", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.940814197063446}]}, {"text": " Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages.", "labels": [], "entities": [{"text": "CoNLL03 data", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9782188832759857}, {"text": "CoNNL03", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9742708206176758}, {"text": "MUC7", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.8966489434242249}]}, {"text": " Table 5: End system performance by component. Results confirm that NER is a knowledge-intensive task.", "labels": [], "entities": [{"text": "NER", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9757409691810608}]}]}