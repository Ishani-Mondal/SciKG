{"title": [{"text": "A Re-examination of Lexical Association Measures", "labels": [], "entities": [{"text": "Re-examination", "start_pos": 2, "end_pos": 16, "type": "TASK", "confidence": 0.9372153878211975}, {"text": "Lexical Association Measures", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.7378695607185364}]}], "abstractContent": [{"text": "We review lexical Association Measures (AMs) that have been employed by past work in extracting multiword expressions.", "labels": [], "entities": [{"text": "lexical Association Measures (AMs)", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.7357861349980036}]}, {"text": "Our work contributes to the understanding of these AMs by categorizing them into two groups and suggesting the use of rank equivalence to group AMs with the same ranking performance.", "labels": [], "entities": [{"text": "understanding of these AMs", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.6692667976021767}]}, {"text": "We also examine how existing AMs can be adapted to better rank English verb particle constructions and light verb constructions.", "labels": [], "entities": []}, {"text": "Specifically, we suggest normalizing (Pointwise) Mutual Information and using marginal frequencies to construct penalization terms.", "labels": [], "entities": []}, {"text": "We empirically validate the effectiveness of these modified AMs in detection tasks in English, performed on the Penn Treebank, which shows significant improvement over the original AMs.", "labels": [], "entities": [{"text": "detection tasks", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.893168717622757}, {"text": "Penn Treebank", "start_pos": 112, "end_pos": 125, "type": "DATASET", "confidence": 0.9960392117500305}]}], "introductionContent": [{"text": "Recently, the NLP community has witnessed a renewed interest in the use of lexical association measures in extracting Multiword Expressions (MWEs).", "labels": [], "entities": [{"text": "extracting Multiword Expressions (MWEs)", "start_pos": 107, "end_pos": 146, "type": "TASK", "confidence": 0.8424311975638071}]}, {"text": "Lexical Association Measures (hereafter, AMs) are mathematical formulas which can be used to capture the degree of connection or association between constituents of a given phrase.", "labels": [], "entities": [{"text": "Lexical Association Measures (hereafter, AMs)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7352619133889675}]}, {"text": "Well-known AMs include Pointwise Mutual Information (PMI), Pearson's 2 \u03c7 and the Odds Ratio.", "labels": [], "entities": [{"text": "Pointwise Mutual Information (PMI)", "start_pos": 23, "end_pos": 57, "type": "TASK", "confidence": 0.5646446744600931}, {"text": "Pearson's 2 \u03c7", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.6905801594257355}, {"text": "Odds", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9828143119812012}]}, {"text": "These AMs have been applied in many different fields of study, from information retrieval to hypothesis testing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7939757704734802}, {"text": "hypothesis testing", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.9283259212970734}]}, {"text": "In the context of MWE extraction, many published works have been devoted to comparing their effectiveness.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.9952068030834198}]}, {"text": "evaluate Mutual Information (MI), Dice, Pearson's 2 \u03c7 , log-likelihood ratio and the T score.", "labels": [], "entities": [{"text": "Mutual Information (MI)", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.5111421883106232}, {"text": "Pearson's 2 \u03c7", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.8566333651542664}, {"text": "T score", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.986828476190567}]}, {"text": "In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated.", "labels": [], "entities": [{"text": "Z score", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8912538886070251}, {"text": "Pointwise MI", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.7927429974079132}, {"text": "odds ratio", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9661314785480499}]}, {"text": "Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher's exact tests, various coefficients such as evaluated MI, Pearson's 2 \u03c7 and Permutation Entropy.", "labels": [], "entities": [{"text": "AMs", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9567733407020569}, {"text": "MI", "start_pos": 168, "end_pos": 170, "type": "METRIC", "confidence": 0.9655900597572327}]}, {"text": "Probably the most comprehensive evaluation of AMs was presented in, where 82 AMs were assembled and evaluated over Czech collocations.", "labels": [], "entities": [{"text": "AMs", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9557697772979736}]}, {"text": "These collocations contained a mix of idiomatic expressions, technical terms, light verb constructions and stock phrases.", "labels": [], "entities": []}, {"text": "In their work, the best combination of AMs was selected using machine learning.", "labels": [], "entities": [{"text": "AMs", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9531763792037964}]}, {"text": "While the previous works have evaluated AMs, there have been few details on why the AMs perform as they do.", "labels": [], "entities": [{"text": "AMs", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9455852508544922}]}, {"text": "A detailed analysis of why these AMs perform as they do is needed in order to explain their identification performance, and to help us recommend AMs for future tasks.", "labels": [], "entities": []}, {"text": "This weakness of previous works motivated us to address this issue.", "labels": [], "entities": []}, {"text": "In this work, we contribute to further understanding of association measures, using two different MWE extraction tasks to motivate and concretize our discussion.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.9609026312828064}]}, {"text": "Our goal is to be able to predict, a priori, what types of AMs are likely to perform well fora particular MWE class.", "labels": [], "entities": []}, {"text": "We focus on the extraction of two common types of English MWEs that can be captured by bigram model: Verb Particle Constructions (VPCs) and Light Verb Constructions (LVCs).", "labels": [], "entities": []}, {"text": "VPCs consist of a verb and one or more particles, which can be prepositions (e.g. put on, bolster up), adjectives (cut short) or verbs (make do).", "labels": [], "entities": []}, {"text": "For simplicity, we focus only on bigram VPCs that take prepositional particles, the most common class of VPCs.", "labels": [], "entities": []}, {"text": "A special characteristic of VPCs that affects their extraction is the mobility of noun phrase complements in transitive VPCs.", "labels": [], "entities": []}, {"text": "They can appear after the particle (Take off your hat) or between the verb and the particle (Take your hat off).", "labels": [], "entities": []}, {"text": "However, a pronominal complement can only appear in the latter configuration (Take it off).", "labels": [], "entities": []}, {"text": "In comparison, LVCs comprise of a verb and a complement, which is usually a noun phrase (make a presentation, give a demonstration).", "labels": [], "entities": []}, {"text": "Their meanings come mostly from their complements and, as such, verbs in LVCs are termed semantically light, hence the name light verb.", "labels": [], "entities": []}, {"text": "This explains why modifiers of LVCs modify the complement instead of the verb (make a serious mistake vs. *make a mistake seriously).", "labels": [], "entities": []}, {"text": "This phenomenon also shows that an LVC's constituents may not occur contiguously.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will first present how VPC and LVC candidates are extracted and used to form our evaluation data set.", "labels": [], "entities": []}, {"text": "Second, we will discuss how performances of AMs are measured in our experiments.", "labels": [], "entities": [{"text": "AMs", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9092262983322144}]}, {"text": "In this study, we employ the Wall Street Journal (WSJ) section of one million words in the Penn Tree Bank.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section", "start_pos": 29, "end_pos": 62, "type": "DATASET", "confidence": 0.9462973986353193}, {"text": "Penn Tree Bank", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.9172030289967855}]}, {"text": "To create the evaluation data set, we first extract the VPC and LVC candidates from our corpus as described below.", "labels": [], "entities": []}, {"text": "We note here that the mobility property of both VPC and LVC constituents have been used in the extraction process.", "labels": [], "entities": []}, {"text": "For VPCs, we first identify particles using a pre-compiled set of 38 particles based on (Appendix A).", "labels": [], "entities": []}, {"text": "Here we do not use the WSJ particle tag to avoid possible inconsistencies pointed out in.", "labels": [], "entities": [{"text": "WSJ particle tag", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.8480250835418701}]}, {"text": "Next, we search to the left of the located particle for the nearest verb.", "labels": [], "entities": []}, {"text": "As verbs and particles in transitive VPCs may not occur contiguously, we allow an intervening NP of up to 5 words, similar to and, since longer NPs tend to be located after particles.", "labels": [], "entities": []}, {"text": "Extraction of LVCs is carried out in a similar fashion.", "labels": [], "entities": []}, {"text": "First, occurrences of light verbs are located based on the following set of seven frequently used English light verbs: do, get, give, have, make, put and take.", "labels": [], "entities": []}, {"text": "Next, we search to the right of the light verbs for the nearest noun,  To evaluate the performance of AMs, we can use the standard precision and recall measures, as in much past work.", "labels": [], "entities": [{"text": "AMs", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9658583402633667}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9995105266571045}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9978184700012207}]}, {"text": "We note that the ranked list of candidates generated by an AM is often used as a classifier by setting a threshold.", "labels": [], "entities": []}, {"text": "However, setting a threshold is problematic and optimal threshold values vary for different AMs.", "labels": [], "entities": []}, {"text": "Additionally, using the list of ranked candidates directly as a classifier does not consider the confidence indicated by actual scores.", "labels": [], "entities": []}, {"text": "Another way to avoid setting threshold values is to measure precision and recall of only then most likely candidates (the n-best method).", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9989744424819946}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9987711310386658}]}, {"text": "However, as discussed in, this method depends heavily on the choice of n.", "labels": [], "entities": []}, {"text": "In this paper, we opt for average precision (AP), which is the average of precisions at all possible recall values.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 26, "end_pos": 48, "type": "METRIC", "confidence": 0.9113634109497071}, {"text": "precisions", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9382341504096985}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9595084190368652}]}, {"text": "This choice also makes our results comparable to those of Pecina and Schlesinger (2006).", "labels": [], "entities": []}, {"text": "gives the two average precision profiles of the 82 AMs presented in Pecina and Schlesinger (2006) when we replicated their experiments over our English VPC and LVC datasets.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9873964190483093}, {"text": "English VPC and LVC datasets", "start_pos": 144, "end_pos": 172, "type": "DATASET", "confidence": 0.6126694262027741}]}, {"text": "We observe that the average precision profile for VPCs is slightly concave while the one for LVCs is more convex.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9986851811408997}]}, {"text": "This can be interpreted as VPCs being more sensitive to the choice of AM than LVCs.", "labels": [], "entities": []}, {"text": "Another point we observed is that avast majority of Class I AMs, including PMI, its variants and association coefficients (excluding hypothesis tests), perform reasonably well in our application.", "labels": [], "entities": []}, {"text": "In contrast, the performances of most of context-based and hypothesis test AMs are very modest.", "labels": [], "entities": [{"text": "hypothesis test AMs", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.584098627169927}]}, {"text": "Their mediocre performance indicates their inapplicability to our VPC and LVC tasks.", "labels": [], "entities": []}, {"text": "In particular, the high frequencies of particles in VPCs and light verbs in LVCs both undermine their contexts' discriminative power and skew the difference between observed and expected frequencies that are relied on in hypothesis tests.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Evaluation data sizes (type count, not token).", "labels": [], "entities": []}, {"text": " Table 4. AP performance of PMI and its variants. Best  alpha settings shown in parentheses.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9905039668083191}]}, {"text": " Table 5. AP performance of normalized (P)MI versus  standard (P)MI. Best alpha settings shown in  parentheses.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.991497814655304}]}, {"text": " Table 6. Replacing min() with max() in selected AMs.", "labels": [], "entities": []}, {"text": " Table 7. Performance of Fager and its adjusted  version.", "labels": [], "entities": [{"text": "Performance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9785380959510803}, {"text": "Fager", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.4877831041812897}]}, {"text": " Table 8. AP performance of suggested VPCs'  penalization terms and AMs.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.5789443850517273}, {"text": "AMs", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9907171130180359}]}, {"text": " Table 9. AP performance of suggested LVCs'  penalization terms and AMs.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9258933663368225}, {"text": "AMs", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9952320456504822}]}]}