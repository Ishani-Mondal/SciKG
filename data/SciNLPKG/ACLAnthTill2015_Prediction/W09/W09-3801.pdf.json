{"title": [{"text": "Parsing Algorithms based on Tree Automata", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate several algorithms related to the parsing problem for weighted au-tomata, under the assumption that the input is a string rather than a tree.", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.9172038435935974}]}, {"text": "This assumption is motivated by several natural language processing applications.", "labels": [], "entities": []}, {"text": "We provide algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and prefix probability.", "labels": [], "entities": []}, {"text": "Our algorithms are obtained by extending to weighted tree au-tomata the Bar-Hillel technique, as defined for context-free grammars.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tree automata are finite-state devices that recognize tree languages, that is, sets of trees.", "labels": [], "entities": []}, {"text": "There is a growing interest nowadays in the natural language parsing community, and especially in the area of syntax-based machine translation, for probabilistic tree automata (PTA) viewed as suitable representations of grammar models.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.6929429570833842}, {"text": "syntax-based machine translation", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.6824361880620321}]}, {"text": "In fact, probabilistic tree automata are generatively more powerful than probabilistic context-free grammars (PCFGs), when we consider the latter as devices that generate tree languages.", "labels": [], "entities": []}, {"text": "This difference can be intuitively understood if we consider that a computation by a PTA uses hidden states, drawn from a finite set, that can be used to transfer information within the tree structure being recognized.", "labels": [], "entities": []}, {"text": "As an example, in written English we can empirically observe different distributions in the expansion of so-called noun phrase (NP) nodes, in the contexts of subject and direct-object positions, respectively.", "labels": [], "entities": []}, {"text": "This can be easily captured using some states of a PTA that keep a record of the different contexts.", "labels": [], "entities": []}, {"text": "In contrast, PCFGs are unable to model these effects, because NP node expansion should be independent of the context in the derivation.", "labels": [], "entities": []}, {"text": "This problem for PCFGs is usually solved by resorting to so-called parental annotations), but this, of course, results in a different tree language, since these annotations will appear in the derived tree.", "labels": [], "entities": []}, {"text": "Most of the theoretical work on parsing and estimation based on PTA has assumed that the input is a tree (, in accordance with the very definition of these devices.", "labels": [], "entities": [{"text": "parsing and estimation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7203089793523153}]}, {"text": "However, both in parsing as well as in machine translation, the input is most often represented as a string rather than a tree.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.717636913061142}]}, {"text": "When the input is a string, some trick is applied to map the problem back to the case of an input tree.", "labels": [], "entities": []}, {"text": "As an example in the context of machine translation, assume a probabilistic tree transducer T as a translation model, and an input string w to be translated.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7951666414737701}]}, {"text": "One can then intermediately construct a tree automaton M w that recognizes the set of all possible trees that have was yield, with internal nodes from the input alphabet of T . This automaton M w is further transformed into a tree transducer implementing a partial identity translation, and such a transducer is composed with T (relation composition).", "labels": [], "entities": []}, {"text": "This is usually called the 'cascaded' approach.", "labels": [], "entities": []}, {"text": "Such an approach can be easily applied also to parsing problems.", "labels": [], "entities": [{"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9866665005683899}]}, {"text": "In contrast with the cascaded approach above, which maybe rather inefficient, in this paper we investigate a more direct technique for parsing strings based on weighted and probabilistic tree automata.", "labels": [], "entities": []}, {"text": "We do this by extending to weighted tree automata the well-known Bar-Hillel construction defined for context-free grammars () and for weighted context-free grammars (.", "labels": [], "entities": []}, {"text": "This provides an abstract framework under which several parsing algorithms can be directly derived, based on weighted tree automata.", "labels": [], "entities": []}, {"text": "We discuss several applications of our results, including algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and prefix probability.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}