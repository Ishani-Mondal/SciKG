{"title": [{"text": "Classifying Japanese Polysemous Verbs based on Fuzzy C-means Clustering", "labels": [], "entities": [{"text": "Fuzzy C-means Clustering", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.740493098894755}]}], "abstractContent": [{"text": "This paper presents a method for classifying Japanese polysemous verbs using an algorithm to identify overlapping nodes with more than one cluster.", "labels": [], "entities": [{"text": "classifying Japanese polysemous verbs", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.8210810869932175}]}, {"text": "The algorithm is a graph-based unsupervised clustering algorithm, which combines a generalized modularity function, spectral mapping , and fuzzy clustering technique.", "labels": [], "entities": []}, {"text": "The modularity function for measuring cluster structure is calculated based on the frequency distributions over verb frames with selectional preferences.", "labels": [], "entities": []}, {"text": "Evaluations are made on two sets of verbs including pol-ysemies.", "labels": [], "entities": []}], "introductionContent": [{"text": "There has been quite a lot of research concerned with automatic clustering of semantically similar words or automatic retrieval of collocations among them from corpora.", "labels": [], "entities": [{"text": "automatic clustering of semantically similar words", "start_pos": 54, "end_pos": 104, "type": "TASK", "confidence": 0.8124187290668488}]}, {"text": "Most of this work is based on similarity measures derived from the distribution of words in corpora.", "labels": [], "entities": []}, {"text": "However, the facts that a single word does have more than one sense and that the distribution of a word in a corpus is a mixture of usages of different senses of the same word often hamper such attempts.", "labels": [], "entities": []}, {"text": "In general, restriction of the subject domain makes the problem of polysemy less problematic.", "labels": [], "entities": []}, {"text": "However, even in texts from a restricted domain such as economics or sports, one encounters quite a large number of polysemous words.", "labels": [], "entities": []}, {"text": "Therefore, semantic classification of polysemies has been an interest since the earliest days when a number of large scale corpora have become available.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8576473891735077}]}, {"text": "In this paper, we focus on Japanese polysemous verbs, and present a method for polysemous verb classification.", "labels": [], "entities": [{"text": "polysemous verb classification", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.722769558429718}]}, {"text": "We used a graph-based unsupervised clustering algorithm.", "labels": [], "entities": []}, {"text": "The algorithm combines the idea of modularity function Q, spectral relaxation and fuzzy c-means clustering method to identify overlapping nodes with more than one cluster.", "labels": [], "entities": []}, {"text": "The modularity function measures the quality of a cluster structure.", "labels": [], "entities": []}, {"text": "Spectral mapping performs a dimensionality reduction which makes it possible to cluster in the very high dimensional spaces.", "labels": [], "entities": [{"text": "Spectral mapping", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8703541457653046}]}, {"text": "The fuzzy c-means allows for the detection of nodes with more than one cluster.", "labels": [], "entities": []}, {"text": "We applied the algorithm to cluster polysemous verbs.", "labels": [], "entities": []}, {"text": "The modularity function for measuring the quality of a cluster structure is calculated based on the frequency distributions over verb frames with selectional preferences.", "labels": [], "entities": []}, {"text": "We collected semantic classes from IPAL Japanese dictionary, and used them as a gold standard data.", "labels": [], "entities": [{"text": "IPAL Japanese dictionary", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.9288312196731567}]}, {"text": "IPAL lists about 900 Japanese basic verbs, and categorizes each verb into multiple senses.", "labels": [], "entities": [{"text": "IPAL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8911512494087219}]}, {"text": "Moreover, the categorization is based on verbal syntax with respect to the choice of its arguments.", "labels": [], "entities": []}, {"text": "Therefore, if the clustering algorithm induces a polysemous verb classification on the basis of verbal syntax, then the resulting classification should agree the IPAL classes.", "labels": [], "entities": []}, {"text": "We used a large Japanese newspaper corpus and EDR (Electronic Dictionary Research) dictionary to obtain verbs and their subcategorization frames with selectional preferences . The results obtained using two data sets were better than the baseline, EM algorithm.", "labels": [], "entities": [{"text": "Japanese newspaper corpus", "start_pos": 16, "end_pos": 41, "type": "DATASET", "confidence": 0.6819360454877218}, {"text": "EDR (Electronic Dictionary Research) dictionary", "start_pos": 46, "end_pos": 93, "type": "DATASET", "confidence": 0.6778461422239032}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section presents related work.", "labels": [], "entities": []}, {"text": "After describing Japanese verb with selectional preferences, we present a distributional similarity in Section 4, and a graph-based unsupervised clustering algorithm in Section 5.", "labels": [], "entities": []}, {"text": "Results using two data sets are reported in Section 6.", "labels": [], "entities": []}, {"text": "We give our conclusion in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created test verbs using two sets of Japanese Mainichi newspaper corpus.", "labels": [], "entities": [{"text": "Mainichi newspaper corpus", "start_pos": 49, "end_pos": 74, "type": "DATASET", "confidence": 0.9571568965911865}]}, {"text": "One is a set consisting one year (2007) newspapers (We call it a set from 2007), and another is a set of 17 years (from 1991 to 2007) Japanese Mainichi newspapers (We call it a set from 1991 2007).", "labels": [], "entities": [{"text": "Mainichi newspapers", "start_pos": 143, "end_pos": 162, "type": "DATASET", "confidence": 0.8770911991596222}]}, {"text": "For each set, all Japanese documents were parsed using the syntactic analyzer Cabocha (.", "labels": [], "entities": []}, {"text": "We selected verbs, each frequency f (v) is, 500 \u2264 f (v) \u2264 10,000.", "labels": [], "entities": []}, {"text": "As a result, we obtained 279 verbs fora set from 2007 and 1,692 verbs fora set from 1991 2007.", "labels": [], "entities": []}, {"text": "From these verbs, we chose verbs which appeared in the machine readable dictionary, IPAL.", "labels": [], "entities": [{"text": "IPAL", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.711597740650177}]}, {"text": "This selection resulted in a total of 81 verbs fora set from 2007, and 170 verbs, fora set from 1991 2007.", "labels": [], "entities": []}, {"text": "We obtained Japanese verb frames with selectional preferences using these two sets.", "labels": [], "entities": []}, {"text": "We extracted sentence patterns with their frequencies.", "labels": [], "entities": []}, {"text": "Noun words within each sentence were tagged sense identifier by using the EDR Japanese sense dictionary.", "labels": [], "entities": [{"text": "EDR Japanese sense dictionary", "start_pos": 74, "end_pos": 103, "type": "DATASET", "confidence": 0.9551543444395065}]}, {"text": "As a result, we obtained 56,400 verb frame patterns fora set from 2007, and 300,993 patterns fora set from 1991 2007.", "labels": [], "entities": []}, {"text": "We created the gold standard data, verb classes, using IPAL.", "labels": [], "entities": []}, {"text": "IPAL lists about 900 Japanese verbs and categorizes each verb into multiple senses, based on verbal syntax and semantics.", "labels": [], "entities": []}, {"text": "It also listed synonym verbs.", "labels": [], "entities": []}, {"text": "shows a fragment of the entry associated with the Japanese verb taberu.", "labels": [], "entities": []}, {"text": "The verb \"taberu\" has two senses, \"eat\" and \"live\".", "labels": [], "entities": []}, {"text": "\"pattern\" refers to the case frame(s) associated with each verb sense.", "labels": [], "entities": []}, {"text": "According to the IPAL, we obtained verb classes, each class corresponds to a sense of each verb.", "labels": [], "entities": []}, {"text": "There are 87 classes fora set from 2007, and 152 classes fora set from 1991 2007.", "labels": [], "entities": []}, {"text": "The examples of the test verbs and their senses are shown in.", "labels": [], "entities": []}, {"text": "For evaluation of verb classification, we used the precision, recall, and F-score, which were defined by), especially to capture how many verbs does the algorithm actually detect more than just the predominant sense.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.718996599316597}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9997519850730896}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9985350370407104}, {"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9994088411331177}]}, {"text": "For comparison against polysemies, we utilized the EM algorithm which is widely used as a soft clustering technique.", "labels": [], "entities": []}, {"text": "We followed the method presented in.", "labels": [], "entities": []}, {"text": "We used a probability distribution over verb frames with selectional preferences.", "labels": [], "entities": []}, {"text": "The initial probabilities  were often determined randomly.", "labels": [], "entities": []}, {"text": "We set the initial probabilities by using the result of the standard k-means.", "labels": [], "entities": []}, {"text": "For k-means, we used 50 random replications of the initialization, each time initializing the cluster center with k randomly chosen.", "labels": [], "entities": []}, {"text": "We used up to 20 iterations to learn the model probabilities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for a set from 2007", "labels": [], "entities": []}, {"text": " Table 4: Results against each measure", "labels": [], "entities": []}, {"text": " Table 5: Results for a set from 1991 2007", "labels": [], "entities": []}]}