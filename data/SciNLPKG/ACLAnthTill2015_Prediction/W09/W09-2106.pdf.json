{"title": [{"text": "An Application of Latent Semantic Analysis to Word Sense Discrimination for Words with Related and Unrelated Meanings", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.6189416249593099}, {"text": "Word Sense Discrimination", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6907601455847422}]}], "abstractContent": [{"text": "We present an application of Latent Semantic Analysis to word sense discrimination within a tutor for English vocabulary learning.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.5769674479961395}, {"text": "word sense discrimination", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.7695475816726685}, {"text": "English vocabulary learning", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6588848729928335}]}, {"text": "We attempt to match the meaning of a word in a document with the meaning of the same word in a fill-in-the-blank question.", "labels": [], "entities": []}, {"text": "We compare the performance of the Lesk algorithm to Latent Semantic Analysis.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.6633533934752146}]}, {"text": "We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.5923653046290079}]}], "introductionContent": [{"text": "In this paper, we present an application of Latent Semantic Analysis (LSA) to word sense discrimination (WSD) within a tutor for English vocabulary learning for non-native speakers.", "labels": [], "entities": [{"text": "word sense discrimination (WSD)", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.8162010063727697}, {"text": "English vocabulary learning", "start_pos": 129, "end_pos": 156, "type": "TASK", "confidence": 0.6282948950926462}]}, {"text": "This tutor retrieves documents from the Web that contain target words a student needs to learn and that are at an appropriate reading level).", "labels": [], "entities": []}, {"text": "It presents a document to the student and then follows the document reading with practice questions that measure how the student's knowledge has evolved.", "labels": [], "entities": []}, {"text": "It is important that the fill-in-the-blank questions (also known as cloze questions) that we ask to the students allow us to determine their vocabulary knowledge accurately.", "labels": [], "entities": []}, {"text": "An example of cloze question is shown in.", "labels": [], "entities": []}, {"text": "Some words have more than one meaning and so the cloze question we give could be about a different meaning than the one that the student learned in the document.", "labels": [], "entities": []}, {"text": "This is something that can lead to confusion and must be avoided.", "labels": [], "entities": []}, {"text": "To do this, we need to use some automatic measure of semantic similarity.", "labels": [], "entities": []}, {"text": "To define the problem formally, given a target word w, a string r (the reading) containing wand n strings q 1 , ..., q n (the sentences used for the questions) each containing w, find the strings q i where the meaning of w is closest to its meaning in r.", "labels": [], "entities": []}, {"text": "We make the problem simpler by selecting only one question.", "labels": [], "entities": []}, {"text": "This problem is challenging because the context defined by cloze questions is short.", "labels": [], "entities": []}, {"text": "Furthermore, a word can have only slight variations in meaning that even humans find sometimes difficult to distinguish.", "labels": [], "entities": []}, {"text": "LSA was originally applied to Information Retrieval (.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.6943412125110626}]}, {"text": "It was shown to be able to match short queries to relevant documents even when there were no exact matches between the words.", "labels": [], "entities": []}, {"text": "Therefore LSA would seem to bean appropriate technique for matching a short context, such as a question, with a whole document.", "labels": [], "entities": [{"text": "LSA", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6307610869407654}]}, {"text": "So we are looking to first discriminate between the meanings of words, such as \"compound\", that have several very different meanings (a chemical compound or a set of buildings) and then to disambiguate words that have senses that are closely related such as \"comprise\" (\"be composed of\" or \"compose\").", "labels": [], "entities": []}, {"text": "In the following sections, we present LSA and some of its applications, then we present some experimental results that compare a baseline to the use of LSA for both tasks we have just described.", "labels": [], "entities": []}, {"text": "We expect the task to be easier on words with unrelated meanings.", "labels": [], "entities": []}, {"text": "In addition, we expect that LSA will perform better when we use context selection on the documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a database of 62 manually generated cloze questions covering 16 target words . We manually annotated the senses of the target words in these questions using WordNet senses.", "labels": [], "entities": [{"text": "WordNet senses", "start_pos": 165, "end_pos": 179, "type": "DATASET", "confidence": 0.9255255162715912}]}, {"text": "For each word and for each sense, we manually gathered documents from the Web containing the target word with the corresponding sense.", "labels": [], "entities": []}, {"text": "There were 84 documents in total.", "labels": [], "entities": []}, {"text": "We added 97 documents extracted from the tutor database of documents that contained at least one target word but we did not annotate their meaning.", "labels": [], "entities": [{"text": "tutor database of documents", "start_pos": 41, "end_pos": 68, "type": "DATASET", "confidence": 0.9016475081443787}]}, {"text": "We wanted to evaluate the performances of LSA for WSD for words with unrelated meanings and for words with both related and unrelated meanings.", "labels": [], "entities": [{"text": "WSD", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8912926912307739}]}, {"text": "For the first type of evaluation, we retained four target words.", "labels": [], "entities": []}, {"text": "For the second type of evaluation, all 16 words were included.", "labels": [], "entities": []}, {"text": "We also wanted to evaluate the influence of the size of the context of the target words.", "labels": [], "entities": []}, {"text": "We therefore considered two matrices: a term-document matrix and a term-context matrix where context designates five sentences around the target word in the document.", "labels": [], "entities": []}, {"text": "In both cases each cell of the matrix had a tf-idf weight.", "labels": [], "entities": []}, {"text": "Finally, we wanted to investigate the influence of the dimension reduction on performance.", "labels": [], "entities": []}, {"text": "In our experiments, we explored these three directions.", "labels": [], "entities": []}], "tableCaptions": []}