{"title": [{"text": "Probabilistic Text Structuring: Experiments with Sentence Ordering", "labels": [], "entities": [{"text": "Sentence Ordering", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8091917634010315}]}], "abstractContent": [{"text": "Ordering information is a critical task for natural language generation applications.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.682814359664917}]}, {"text": "In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.8110285401344299}, {"text": "text-to-text generation", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7356978356838226}]}, {"text": "We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives.", "labels": [], "entities": []}, {"text": "We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task.", "labels": [], "entities": []}, {"text": "We also assess the appropriateness of such a model for multidocument summa-rization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Structuring a set of facts into a coherent text is a non-trivial task which has received much attention in the area of concept-to-text generation (see Reiter and Dale 2000 for an overview).", "labels": [], "entities": [{"text": "Structuring a set of facts into a coherent text", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8004708025190566}, {"text": "concept-to-text generation", "start_pos": 119, "end_pos": 145, "type": "TASK", "confidence": 0.7831418812274933}]}, {"text": "The structured text is typically assumed to be a tree (i.e., to have a hierarchical structure) whose leaves express the content being communicated and whose nodes specify how this content is grouped via rhetorical or discourse relations (e.g., contrast, sequence, elaboration).", "labels": [], "entities": []}, {"text": "For domains with large numbers of facts and rhetorical relations, there can be more than one possible tree representing the intended content.", "labels": [], "entities": []}, {"text": "These different trees will be realized as texts with different sentence orders or even paragraph orders and different levels of coherence.", "labels": [], "entities": []}, {"text": "Finding the tree that yields the best possible text is effectively a search problem.", "labels": [], "entities": []}, {"text": "One way to address it is by narrowing down the search space either exhaustively or heuristically.", "labels": [], "entities": []}, {"text": "argues that global coherence can be achieved if constraints on local coherence are satisfied.", "labels": [], "entities": []}, {"text": "The latter are operationalized as weights on the ordering and adjacency of facts and are derived from a corpus of naturally occurring texts.", "labels": [], "entities": []}, {"text": "A constraint satisfaction algorithm is used to find the tree with maximal weights from the space of all possible trees.", "labels": [], "entities": []}, {"text": "advocate stochastic search as an alternative to exhaustively examining the search space.", "labels": [], "entities": []}, {"text": "Rather than requiring a global optimum to be found, they use a genetic algorithm to select a tree that is coherent enough for people to understand (local optimum).", "labels": [], "entities": []}, {"text": "The problem of finding an acceptable ordering does not arise solely in concept-to-text generation but also in the emerging field of text-to-text generation.", "labels": [], "entities": [{"text": "concept-to-text generation", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7685535848140717}, {"text": "text-to-text generation", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.7308462709188461}]}, {"text": "Examples of applications that require some form of text structuring, are single-and multidocument summarization as well as question answering.", "labels": [], "entities": [{"text": "single-and multidocument summarization", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.5539885461330414}, {"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.897856742143631}]}, {"text": "Note that these applications do not typically assume rich semantic knowledge organized in tree-like structures or communicative goals as is often the casein concept-to-text generation.", "labels": [], "entities": [{"text": "casein concept-to-text generation", "start_pos": 150, "end_pos": 183, "type": "TASK", "confidence": 0.6360560854276022}]}, {"text": "Although in single document summarization the position of a sentence in a document can provide cues with respect to its ordering in the summary, this is not the casein multidocument summarization where sentences are selected from different documents and must be somehow ordered so as to produce a coherent summary ().", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.5821059743563334}, {"text": "multidocument summarization", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.518183708190918}]}, {"text": "Answering a question may also involve the extraction, potentially summarization, and ordering of information across multiple information sources.", "labels": [], "entities": [{"text": "summarization", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.903910756111145}]}, {"text": "address the problem of information ordering in multidocument summarization and show that naive ordering algorithms such as majority ordering (selects most frequent orders across input documents) and chronological ordering (orders facts according to publication date) do not always yield coherent summaries although the latter produces good results when the information is eventbased.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7582009732723236}, {"text": "multidocument summarization", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.5439723432064056}]}, {"text": "Barzilay et al. further conduct a study where subjects are asked to produce a coherent text from the output of a multidocument summarizer.", "labels": [], "entities": []}, {"text": "Their results reveal that although the generated orders differ from subject to subject, topically related sentences always appear together.", "labels": [], "entities": []}, {"text": "Based on the human study they propose an algorithm that first identifies topically related groups of sentences and then orders them according to chronological information.", "labels": [], "entities": []}, {"text": "In this paper we introduce an unsupervised probabilistic model for text structuring that learns ordering constraints from a large corpus.", "labels": [], "entities": [{"text": "text structuring", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7716900706291199}]}, {"text": "The model operates on sentences rather than facts in a knowledge base and is potentially useful for text-to-text generation applications.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.7210764139890671}]}, {"text": "For example, it can be used to order the sentences obtained from a multidocument summarizer or a question answering system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.7665815651416779}]}, {"text": "Sentences are represented by a set of informative features (e.g., a verb and its subject, a noun and its modifier) that can be automatically extracted from the corpus without recourse to manual annotation.", "labels": [], "entities": []}, {"text": "The model learns which sequences of features are likely to co-occur and makes predictions concerning preferred orderings.", "labels": [], "entities": []}, {"text": "Local coherence is thus operationalized by sentence proximity in the training corpus.", "labels": [], "entities": []}, {"text": "Global coherence is obtained by greedily searching through the space of possible orders.", "labels": [], "entities": []}, {"text": "As in the case of we construct an acceptable ordering rather than the best possible one.", "labels": [], "entities": []}, {"text": "We propose an automatic method of evaluating the orders generated by our model by measuring closeness or distance from the gold standard, a collection of orders produced by humans.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces our model and an algorithm for producing a possible order.", "labels": [], "entities": []}, {"text": "Section 3 describes our corpus and the estimation of the model parameters.", "labels": [], "entities": []}, {"text": "Our experiments are detailed in Section 4.", "labels": [], "entities": []}, {"text": "We conclude with a discussion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experiments with the model and the features introduced in the previous sections.", "labels": [], "entities": []}, {"text": "We first evaluate the model by attempting to reproduce the structure of unseen texts from the BLLIP corpus, i.e., the corpus on which the model is trained on.", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8779941499233246}]}, {"text": "We next obtain an upper bound for the task by conducting a sentence ordering experiment with humans and comparing the model against the human data.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7370661348104477}]}, {"text": "Finally, we assess whether this model can be used for multi-document summarization using data from.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.6582060754299164}]}, {"text": "But before we outline the details of our experiments we discuss our choice of metric for comparing different orders.", "labels": [], "entities": []}, {"text": "Our task is to produce an ordering for the sentences of a given text.", "labels": [], "entities": []}, {"text": "We can think of the sentences as objects for which a ranking must be produced.", "labels": [], "entities": []}, {"text": "gives an example of a text containing 10 sentences (A-J) and the orders (i.e., rankings) produced by three hypothetical models.", "labels": [], "entities": []}, {"text": "A number of metrics can be used to measure the distance between two rankings such as Spearman's correlation coefficient for ranked data, Cayley distance, or Kendall's \u03c4 (see Lebanon and Lafferty 2002 for details).", "labels": [], "entities": [{"text": "Spearman's correlation coefficient", "start_pos": 85, "end_pos": 119, "type": "METRIC", "confidence": 0.639323428273201}, {"text": "Cayley distance", "start_pos": 137, "end_pos": 152, "type": "METRIC", "confidence": 0.6510975062847137}, {"text": "Kendall's \u03c4", "start_pos": 157, "end_pos": 168, "type": "METRIC", "confidence": 0.6399215658505758}]}, {"text": "Kendall's \u03c4 is based on the number of inversions in the rankings and is defined in (6): where N is the number of objects (i.e., sentences) being ranked and inversions are the number of interchanges of consecutive elements necessary to arrange them in their natural order.", "labels": [], "entities": []}, {"text": "If we think in terms of permutations, then \u03c4 can be interpreted as the minimum number of adjacent transpositions needed to bring one order to the other.", "labels": [], "entities": []}, {"text": "In the number of inversions can be calculated by counting the number of intersections of the lines.", "labels": [], "entities": []}, {"text": "The metric ranges from \u22121 (inverse ranks) to 1 (identical ranks).", "labels": [], "entities": []}, {"text": "The \u03c4 for Model 1 and Model 2 in is .822.", "labels": [], "entities": [{"text": "\u03c4", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9861240386962891}]}, {"text": "Kendall's \u03c4 seems particularly appropriate for the tasks considered in this paper.", "labels": [], "entities": []}, {"text": "The metric is sensitive to the fact that some sentences maybe always ordered next to each other even though their absolute orders might differ.", "labels": [], "entities": []}, {"text": "It also penalizes inverse rankings.", "labels": [], "entities": []}, {"text": "Comparison between Model 1 and Model 3 would give a \u03c4 of 0.244 even though the orders between the two models are identical modulo the beginning and the end.", "labels": [], "entities": []}, {"text": "This seems appropriate given that flipping the introduction in a document with the conclusions seriously disrupts coherence.", "labels": [], "entities": []}, {"text": "The model from Section 2.1 was trained on the BLLIP corpus and tested on 20 held-out randomly selected unseen texts (average length 15.3).", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.96302330493927}]}, {"text": "We also used 20 randomly chosen texts (disjoint from the test data) for development purposes (average length 16.2).", "labels": [], "entities": []}, {"text": "All our results are reported on the test set.", "labels": [], "entities": []}, {"text": "The input to the the greedy algorithm (see Section 2.2) was a text with a randomized sentence ordering.", "labels": [], "entities": []}, {"text": "The ordered output was compared against the original authored text using \u03c4. gives the average \u03c4 (T ) for all 20 test texts when the following features are used: lemmatized verbs (V L ), tensed verbs (V T ), lemmatized nouns (N L ), lemmatized verbs and nouns (V L N L ), tensed verbs and lemmatized nouns (V TN L ), verb-related dependencies (V D ), noun-related dependencies (N D ), verb and noun dependencies (V D ND ), and all available dependencies (A D ).", "labels": [], "entities": []}, {"text": "For comparison we also report the naive baseline of generating a random oder (B R ).", "labels": [], "entities": []}, {"text": "As can be seen from the best performing features are N Land VD ND . This is not surprising given that N L encapsulates notions of entity-based coherence, which is relatively important for our domain.", "labels": [], "entities": []}, {"text": "A lot of texts are about a particular entity (company or individual) and their properties.", "labels": [], "entities": []}, {"text": "The feature VD ND subsumes several other features and does expectedly better: it captures entity-based coherence, the interrelations among verbs, the structure of sentences and also preserves information about argument structure (who is doing what to whom).", "labels": [], "entities": []}, {"text": "The distance between the orders produced by the model and the original texts increases when all types of dependencies are: Comparison between original BLLIP texts and model generated variants taken into account.", "labels": [], "entities": []}, {"text": "The feature space becomes too big, there are too many spurious feature pairs, and the model can't distinguish informative from noninformative features.", "labels": [], "entities": []}, {"text": "We carried out a one-way Analysis of Variance (ANOVA) to examine the effect of different feature types.", "labels": [], "entities": [{"text": "Analysis of Variance (ANOVA)", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.4877055635054906}]}, {"text": "The ANOVA revealed a reliable effect of feature type (F(9, 171) = 3.31; p < 0.01).", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.5502026677131653}, {"text": "F", "start_pos": 54, "end_pos": 55, "type": "METRIC", "confidence": 0.9873327612876892}]}, {"text": "We performed Post-hoc Tukey tests to further examine whether there are any significant differences among the different features and between our model and the baseline.", "labels": [], "entities": []}, {"text": "We found out that N L , VT N L , VD , and VD ND are significantly better than BR (\u03b1 = 0.01), whereas N Land VD ND are not significantly different from each other.", "labels": [], "entities": [{"text": "BR", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9878430366516113}]}, {"text": "However, they are significantly better than all other features (\u03b1 = 0.05).", "labels": [], "entities": []}, {"text": "In this experiment we compare our model's performance against human judges.", "labels": [], "entities": []}, {"text": "Twelve texts were randomly selected from the 20 texts in our test data.", "labels": [], "entities": []}, {"text": "The texts were presented to subjects with the order of their sentences scrambled.", "labels": [], "entities": []}, {"text": "Participants were asked to reorder the sentences so as to produce a coherent text.", "labels": [], "entities": []}, {"text": "Each participant saw three texts randomly chosen from the pool of 12 texts.", "labels": [], "entities": []}, {"text": "A random order of sentences was generated for every text the participants saw.", "labels": [], "entities": []}, {"text": "Sentences were presented verbatim, pronouns and connectives were retained in order to make ordering feasible.", "labels": [], "entities": []}, {"text": "Notice that this information is absent from the features the model takes into account.", "labels": [], "entities": []}, {"text": "The study was conducted remotely over the Internet using a variant of reports pairwise \u03c4 averaged over 12 texts for all participants (H H ) and the average \u03c4 between the model and each of the subjects for all features used in Experiment 1.", "labels": [], "entities": []}, {"text": "The average distance in the orderings produced by our subjects is .58.", "labels": [], "entities": []}, {"text": "The distance between the humans and the best features is .51 for N Land .55 for VD ND . An ANOVA yielded a significant effect of feature type (F(9, 99) = 5.213; and VT N L perform significantly worse than H H (\u03b1 = 0.01), whereas N Land VD ND are not significantly different from H H (\u03b1 = 0.01).", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.4667740762233734}, {"text": "F(9, 99)", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.8958466748396555}]}, {"text": "This is in agreement with Experiment 1 and points to the importance of lexical and structural information for the ordering task.", "labels": [], "entities": []}, {"text": "(2002) collected a corpus of multiple orderings in order to study what makes an order cohesive.", "labels": [], "entities": []}, {"text": "Their goal was to improve the ordering strategy of MULTIGEN () a multidocument summarization system that operates on news articles describing the same event.", "labels": [], "entities": [{"text": "MULTIGEN", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.7282679677009583}]}, {"text": "MULTIGEN identifies text units that convey similar information across documents and clusters them into themes.", "labels": [], "entities": []}, {"text": "Each theme is next syntactically analysed into predicate argument structures; the structures that are repeated often enough are chosen to be included into the summary.", "labels": [], "entities": []}, {"text": "A language generation system outputs a sentence (per theme) from the selected predicate argument structures.", "labels": [], "entities": []}, {"text": "collected ten sets of articles each consisting of two to three articles reporting the same event and simulated MULTIGEN by manually selecting the sentences to be included in the final summary.", "labels": [], "entities": [{"text": "MULTIGEN", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.8380247950553894}]}, {"text": "This way they ensured that orderings were not influenced by mistakes their system could have made.", "labels": [], "entities": []}, {"text": "Explicit references and connectives were removed from the sentences so as not to reveal clues about the sentence ordering.", "labels": [], "entities": []}, {"text": "Ten subjects provided orders for each summary which had an average length of 8.8.", "labels": [], "entities": [{"text": "length", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9178633093833923}]}, {"text": "We simulated the participants' task by using the model from Section 2.1 to produce an order for each candidate summary . We then compared the differences in the orderings generated by the model and participants using the best performing features from Experiment 2 (i.e., N Land VD ND ).", "labels": [], "entities": [{"text": "N Land VD ND", "start_pos": 271, "end_pos": 283, "type": "DATASET", "confidence": 0.7649393603205681}]}, {"text": "Note that the model was trained on the BLLIP corpus, whereas the sentences to be ordered were taken from news articles describing the same event.", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9371382594108582}]}, {"text": "Not only were the news articles unseen but also their syntactic structure was unfamiliar to the model.", "labels": [], "entities": []}, {"text": "The results are shown in table 5, again average pairwise \u03c4 is reported.", "labels": [], "entities": []}, {"text": "We also give the naive baseline of choosing a random order (B R ).", "labels": [], "entities": []}, {"text": "The average distance in the orderings produced by  Although N L performed adequately in Experiments 1 and 2, it failed to outperform the baseline in the summarization task.", "labels": [], "entities": [{"text": "summarization", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.9876328706741333}]}, {"text": "This maybe due to the fact that entity-based coherence is not as important as temporal coherence for the news articles summaries.", "labels": [], "entities": []}, {"text": "Recall that the summaries describe events across documents.", "labels": [], "entities": []}, {"text": "This information is captured more adequately by VD ND and not by N L that only keeps a record of the entities in the sentence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dependencies for sentence (2) in Figure 3", "labels": [], "entities": []}, {"text": " Table 2: Example of rankings for a 10 sentence text", "labels": [], "entities": []}, {"text": " Table 3: Comparison between original BLLIP texts  and model generated variants", "labels": [], "entities": [{"text": "BLLIP texts", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.6564204543828964}]}, {"text": " Table 4: Comparison between orderings produced by  humans and the model on BLLIP texts", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.569551408290863}]}, {"text": " Table 5: Comparison between orderings produced by  humans and the model on multidocument summaries", "labels": [], "entities": []}]}