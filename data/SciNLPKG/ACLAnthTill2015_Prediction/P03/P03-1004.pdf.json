{"title": [{"text": "Fast Methods for Kernel-based Text Analysis", "labels": [], "entities": [{"text": "Kernel-based Text Analysis", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.7910025715827942}]}], "abstractContent": [{"text": "Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 109, "end_pos": 142, "type": "TASK", "confidence": 0.6906815767288208}]}, {"text": "In NLP, although feature combinations are crucial to improving performance, they are heuris-tically selected.", "labels": [], "entities": []}, {"text": "Kernel methods change this situation.", "labels": [], "entities": []}, {"text": "The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs.", "labels": [], "entities": []}, {"text": "Kernel-based text analysis shows an excellent performance in terms inaccuracy; however, these methods are usually too slow to apply to large-scale text analysis.", "labels": [], "entities": [{"text": "Kernel-based text analysis", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6478139857451121}, {"text": "text analysis", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.7196668386459351}]}, {"text": "In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier.", "labels": [], "entities": [{"text": "Basket Mining", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.938685953617096}]}, {"text": "Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classi-fiers are about 30 to 300 times faster than the standard kernel-based classifiers.", "labels": [], "entities": [{"text": "Japanese Word Segmentation", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.5340146819750468}, {"text": "Japanese Dependency Parsing", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.5577024122079214}]}], "introductionContent": [{"text": "Kernel methods (e.g., Support Vector Machines) attract a great deal of attention recently.", "labels": [], "entities": []}, {"text": "In the field of Natural Language Processing, many successes have been reported.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6363114217917124}]}, {"text": "Examples include Part-of-Speech tagging () Text Chunking (), Named Entity Recognition (, and Japanese Dependency Parsing ().", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7656983137130737}, {"text": "Named Entity Recognition", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6249604423840841}, {"text": "Japanese Dependency Parsing", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.58106929063797}]}, {"text": "It is known in NLP that combination of features contributes to a significant improvement inaccuracy.", "labels": [], "entities": []}, {"text": "For instance, in the task of dependency parsing, it would be hard to confirm a correct dependency relation with only a single set of features from either ahead or its modifier.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8587906956672668}]}, {"text": "Rather, dependency relations should be determined by at least information from both of two phrases.", "labels": [], "entities": []}, {"text": "In previous research, feature combination has been selected manually, and the performance significantly depended on these selections.", "labels": [], "entities": []}, {"text": "This is not the case with kernel-based methodology.", "labels": [], "entities": []}, {"text": "For instance, if we use a polynomial kernel, all feature combinations are implicitly expanded without loss of generality and increasing the computational costs.", "labels": [], "entities": []}, {"text": "Although the mapped feature space is quite large, the maximal margin strategy of SVMs gives us a good generalization performance compared to the previous manual feature selection.", "labels": [], "entities": []}, {"text": "This is the main reason why kernel-based learning has delivered great results to the field of NLP.", "labels": [], "entities": []}, {"text": "Kernel-based text analysis shows an excellent performance in terms inaccuracy; however, its inefficiency in actual analysis limits practical application.", "labels": [], "entities": [{"text": "Kernel-based text analysis", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6785895129044851}]}, {"text": "For example, an SVM-based NE-chunker runs at a rate of only 85 byte/sec, while previous rulebased system can process several kilobytes per second ().", "labels": [], "entities": []}, {"text": "Such slow execution time is inadequate for Information Retrieval, Question Answering, or Text Mining, where fast analysis of large quantities of text is indispensable.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8204507231712341}, {"text": "Question Answering", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8222230076789856}, {"text": "Text Mining", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.8058353364467621}]}, {"text": "This paper presents two novel methods that make the kernel-based text analyzers substantially faster.", "labels": [], "entities": []}, {"text": "These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector.", "labels": [], "entities": []}, {"text": "More specifically, we focus on a Polynomial Kernel of degree d, which can attain feature combinations that are crucial to improving the performance of tasks in NLP.", "labels": [], "entities": []}, {"text": "Second, we introduce two fast classification algorithms for this kernel.", "labels": [], "entities": []}, {"text": "One is PKI (Polynomial Kernel Inverted), which is an extension of Inverted Index in Information Retrieval.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.6928810924291611}]}, {"text": "The other is PKE (Polynomial Kernel Expanded), where all feature combinations are explicitly expanded.", "labels": [], "entities": []}, {"text": "By applying PKE, we can convert a kernel-based classifier into a simple and fast liner classifier.", "labels": [], "entities": []}, {"text": "In order to build PKE, we extend the PrefixSpan (), an efficient Basket Mining algorithm, to enumerate effective feature combinations from a set of support examples.", "labels": [], "entities": [{"text": "PrefixSpan", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.8555108308792114}, {"text": "Basket Mining", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.941045343875885}]}, {"text": "Experiments on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that PKI and PKE perform respectively 2 to 13 times and 30 to 300 times faster than standard kernel-based systems, without a discernible change inaccuracy.", "labels": [], "entities": [{"text": "Japanese Word Segmentation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.5269895444313685}, {"text": "Japanese Dependency Parsing", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.5478300054868063}]}], "datasetContent": [{"text": "To demonstrate performances of PKI and PKE, we examined three NLP tasks: English BaseNP Chunking (EBC), Japanese Word Segmentation (JWS) and: \u2126 in TRIE representation Japanese Dependency Parsing (JDP).", "labels": [], "entities": [{"text": "Japanese Word Segmentation (JWS)", "start_pos": 104, "end_pos": 136, "type": "TASK", "confidence": 0.674097994963328}, {"text": "TRIE representation Japanese Dependency Parsing (JDP", "start_pos": 147, "end_pos": 199, "type": "TASK", "confidence": 0.5831670675958905}]}, {"text": "A more detailed description of each task, training and test data, the system parameters, and feature sets are presented in the following subsections.", "labels": [], "entities": []}, {"text": "summarizes the detail information of support examples (e.g., size of SVs, size of feature set etc.).", "labels": [], "entities": [{"text": "detail", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.959154486656189}]}, {"text": "Our preliminary experiments show that a Quadratic Kernel performs the best in EBC, and a Cubic Kernel performs the best in JWS and JDP.", "labels": [], "entities": [{"text": "EBC", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8464120626449585}, {"text": "JWS", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.8445734977722168}, {"text": "JDP", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.8201139569282532}]}, {"text": "The experiments using a Cubic Kernel are suitable to evaluate the effectiveness of the basket mining approach applied in the PKE, since a Cubic Kernel projects the original feature space F into F 3 space, which is too large to be handled only using a naive exhaustive method.", "labels": [], "entities": [{"text": "basket mining", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.7958962321281433}, {"text": "PKE", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.6559902429580688}]}, {"text": "All experiments were conducted under Linux using XEON 2.4 Ghz dual processors and 3.5 Gbyte of main memory.", "labels": [], "entities": []}, {"text": "All systems are implemented in C++.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of Data Set", "labels": [], "entities": [{"text": "Details of Data Set", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.6462340950965881}]}, {"text": " Table 2: Results of EBC  PKE", "labels": [], "entities": [{"text": "EBC  PKE", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8950562179088593}]}, {"text": " Table 3: Results of JWS  PKE", "labels": [], "entities": [{"text": "JWS  PKE", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8364679217338562}]}, {"text": " Table 4: Results of JDP  PKE", "labels": [], "entities": [{"text": "JDP  PKE", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.757007360458374}]}, {"text": " Table 5: Frequency-based pruning (JWS)  PKE  time  Speedup Acc.(%)  |\u2126|  \u03be (sec./sent.)", "labels": [], "entities": [{"text": "PKE  time  Speedup Acc.", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.7325322329998016}]}, {"text": " Table 6: Frequency-based pruning (JDP)  PKE  time  Speedup Acc.(%)  |\u2126|  \u03be (sec./sent.)", "labels": [], "entities": [{"text": "PKE  time  Speedup Acc.", "start_pos": 41, "end_pos": 64, "type": "METRIC", "confidence": 0.7244685888290405}]}]}