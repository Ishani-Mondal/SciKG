{"title": [{"text": "A Probability Model to Improve Word Alignment", "labels": [], "entities": [{"text": "Improve Word Alignment", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.7848173975944519}]}], "abstractContent": [{"text": "Word alignment plays a crucial role in statistical machine translation.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7330288141965866}, {"text": "statistical machine translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7434782683849335}]}, {"text": "Word-aligned corpora have been found to bean excellent source of translation-related knowledge.", "labels": [], "entities": []}, {"text": "We present a statistical model for computing the probability of an alignment given a sentence pair.", "labels": [], "entities": []}, {"text": "This model allows easy integration of context-specific features.", "labels": [], "entities": []}, {"text": "Our experiments show that this model can bean effective tool for improving an existing word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.7280969023704529}]}], "introductionContent": [{"text": "Word alignments were first introduced as an intermediate result of statistical machine translation systems (.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7245795875787735}, {"text": "statistical machine translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6244027018547058}]}, {"text": "Since their introduction, many researchers have become interested in word alignments as a knowledge source.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7628462910652161}]}, {"text": "For example, alignments can be used to learn translation lexicons, transfer rules, and classifiers to find safe sentence segmentation points (.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7144189476966858}]}, {"text": "In addition to the IBM models, researchers have proposed a number of alternative alignment methods.", "labels": [], "entities": []}, {"text": "These methods often involve using a statistic such as \u03c6 2 ( or the log likelihood ratio to create a score to measure the strength of correlation between source and target words.", "labels": [], "entities": [{"text": "\u03c6 2", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9080964028835297}]}, {"text": "Such measures can then be used to guide a constrained search to produce word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7018686532974243}]}, {"text": "It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the alignment.", "labels": [], "entities": []}, {"text": "For example Melamed uses competitive linking along with an explicit noise model in) to produce anew scoring metric, which in turn creates better alignments.", "labels": [], "entities": []}, {"text": "In this paper, we present a simple, flexible, statistical model that is designed to capture the information present in a baseline alignment.", "labels": [], "entities": []}, {"text": "This model allows us to compute the probability of an alignment fora given sentence pair.", "labels": [], "entities": []}, {"text": "It also allows for the easy incorporation of context-specific knowledge into alignment probabilities.", "labels": [], "entities": []}, {"text": "A critical reader may pose the question, \"Why invent anew statistical model for this purpose, when existing, proven models are available to train on a given word alignment?\"", "labels": [], "entities": []}, {"text": "We will demonstrate experimentally that, for the purposes of refinement, our model achieves better results than a comparable existing alternative.", "labels": [], "entities": []}, {"text": "We will first present this model in its most general form.", "labels": [], "entities": []}, {"text": "Next, we describe an alignment algorithm that integrates this model with linguistic constraints in order to produce high quality word alignments.", "labels": [], "entities": []}, {"text": "We will follow with our experimental results and discussion.", "labels": [], "entities": []}, {"text": "We will close with a look at how our work relates to other similar systems and a discussion of possible future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We adopted the same evaluation methodology as in, which compared alignment outputs with manually aligned sentences.", "labels": [], "entities": []}, {"text": "Och and Ney classify manual alignments into two categories: Sure (S) and Possible (P ) (S\u2286P ).", "labels": [], "entities": []}, {"text": "They defined the following metrics to evaluate an alignment A: We trained our alignment program with the same 50K pairs of sentences as) and tested it on the same 500 manually aligned sentences.", "labels": [], "entities": []}, {"text": "Both the training and testing sentences are from the Hansard corpus.", "labels": [], "entities": [{"text": "Hansard corpus", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9845965206623077}]}, {"text": "We parsed the training and testing corpora with Minipar.", "labels": [], "entities": [{"text": "Minipar", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9738962054252625}]}, {"text": "We then ran the training procedure in Section 4 for three iterations.", "labels": [], "entities": []}, {"text": "We conducted three experiments using this methodology.", "labels": [], "entities": []}, {"text": "The goal of the first experiment is to compare the algorithm in Section 3 to a state-of-theart alignment system.", "labels": [], "entities": []}, {"text": "The second will determine the contributions of the features.", "labels": [], "entities": []}, {"text": "The third experiment aims to keep all factors constant except for the model, in an attempt to determine its performance when compared to an obvious alternative.", "labels": [], "entities": []}, {"text": "compares the results of our algorithm with the results in, where an HMM model is used to bootstrap IBM Model 4.", "labels": [], "entities": []}, {"text": "The rows IBM-4 F\u2192E and IBM-4 E\u2192F are the results obtained by IBM Model 4 when treating French as the source and English as the target or vice versa.", "labels": [], "entities": []}, {"text": "The row IBM-4 Intersect shows the results obtained by taking the intersection of the alignments produced by IBM-4 E\u2192F and IBM-4 F\u2192E.", "labels": [], "entities": [{"text": "IBM-4", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8068695664405823}, {"text": "Intersect", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.561303973197937}]}, {"text": "The row IBM-4 Refined shows results obtained by refining the intersection of alignments in order to increase recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9978631138801575}]}, {"text": "Even though we have compared our algorithm to alignments created using IBM statistical models, it is not clear if our model is essential to our performance.", "labels": [], "entities": []}, {"text": "This experiment aims to determine if we could have achieved similar results using the same initial alignment and search algorithm with an alternative model.", "labels": [], "entities": []}, {"text": "Without using any features, our model is similar to IBM's Model 1, in that they both take into account only the word types that participate in a given link.", "labels": [], "entities": []}, {"text": "IBM Model 1 uses P (f |e), the probability off being generated bye, while our model uses P (l|e, f ), the probability of a link existing between e and f . In this experiment, we set Model 1 translation probabilities according to our initial \u03c6 2 alignment, sampling as we described in Section 4.2.", "labels": [], "entities": []}, {"text": "We then use then j=1 P (f j |e a j ) to evaluate candidate alignments in a search that is otherwise identical to our algorithm.", "labels": [], "entities": []}, {"text": "We ran Model 1 refinement for three iterations and that refining our initial \u03c6 2 alignment using IBM's Model 1 is less effective than using our model in the same manner.", "labels": [], "entities": []}, {"text": "In fact, the Model 1 refinement receives a lower score than our initial alignment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison with (Och and Ney, 2000)", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of Features", "labels": [], "entities": []}, {"text": " Table 4: P (l|e, f ) vs. P (f |e)", "labels": [], "entities": []}]}