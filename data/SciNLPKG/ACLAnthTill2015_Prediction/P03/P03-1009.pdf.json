{"title": [{"text": "Clustering Polysemic Subcategorization Frame Distributions Semantically", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.", "labels": [], "entities": []}, {"text": "We describe anew approach which involves clustering subcategoriza-tion frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we particularly focus on clustering polysemic verbs.", "labels": [], "entities": [{"text": "clustering polysemic verbs", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.8745608528455099}]}, {"text": "A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.).", "labels": [], "entities": []}, {"text": "While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.", "labels": [], "entities": []}, {"text": "* This work was partly supported by UK EPSRC project GR/N36462/93: 'Robust Accurate Statistical Parsing (RASP)'.", "labels": [], "entities": [{"text": "UK EPSRC project GR/N36462/93", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.8776648342609406}, {"text": "Statistical Parsing (RASP)'", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.8116701722145081}]}, {"text": "Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation, document classification (, word sense disambiguation and subcategorization acquisition.", "labels": [], "entities": [{"text": "Verb classifications", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7241383939981461}, {"text": "language generation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7259994596242905}, {"text": "machine translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.8055331110954285}, {"text": "document classification", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.7968398332595825}, {"text": "word sense disambiguation", "start_pos": 180, "end_pos": 205, "type": "TASK", "confidence": 0.6776806314786276}, {"text": "subcategorization acquisition", "start_pos": 210, "end_pos": 239, "type": "TASK", "confidence": 0.8020745813846588}]}, {"text": "One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour.", "labels": [], "entities": []}, {"text": "In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data; Schulte im . In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner.", "labels": [], "entities": []}, {"text": "Previous research has demonstrated that clustering can be useful in inferring Levin-style semantic classes from both English and German verb subcategorization information ; Schulte im; Schulte im ).", "labels": [], "entities": []}, {"text": "We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of and (ii) applying a clustering mechanism to this information.", "labels": [], "entities": []}, {"text": "We use clustering methods that process raw distributional data directly, avoiding complex preprocessing steps required by many advanced methods (e.g. Brew and Schulte im ).", "labels": [], "entities": []}, {"text": "In contrast to earlier work, we give special emphasis to polysemy.", "labels": [], "entities": []}, {"text": "Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not).", "labels": [], "entities": []}, {"text": "The relatively good clustering results obtained suggest that many polysemic verbs do have some predominating sense in corpus data.", "labels": [], "entities": []}, {"text": "However, this sense can vary across corpora (), and assuming a single sense is inadequate for an important group of medium and high frequency verbs whose distribution of senses in balanced corpus data is flat rather than zipfian ().", "labels": [], "entities": []}, {"text": "To allow for sense variation, we introduce anew evaluation scheme against a polysemic gold standard.", "labels": [], "entities": []}, {"text": "This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated SCF data semantically.", "labels": [], "entities": [{"text": "clustering undisambiguated SCF data semantically", "start_pos": 100, "end_pos": 148, "type": "TASK", "confidence": 0.8286354303359985}]}, {"text": "We discuss our gold standards and the choice of test verbs in section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the method for subcategorization acquisition and section 4 presents the approach to clustering.", "labels": [], "entities": [{"text": "subcategorization acquisition", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.822333037853241}]}, {"text": "Details of the experimental evaluation are supplied in section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes with directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first evaluated the clusters against the predominant sense, i.e. using the monosemous gold standard.", "labels": [], "entities": []}, {"text": "The results, shown in, demonstrate that both clustering methods perform significantly Our definition differs by a factor of 2 from that of Schulte im   better on the task than our random clustering baseline.", "labels": [], "entities": []}, {"text": "Both methods show clearly better performance with fine-grained SCFs (with prepositions, +PP) than with coarse-grained ones (-PP).", "labels": [], "entities": []}, {"text": "Surprisingly, the simple NN method performs very similarly to the more sophisticated IB.", "labels": [], "entities": []}, {"text": "Being based on pairwise similarities, it shows better performance than IB on the pairwise measure.", "labels": [], "entities": [{"text": "IB", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9805607795715332}]}, {"text": "The IB is, however, slightly better according to the global measure (2% with K = 42).", "labels": [], "entities": [{"text": "IB", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9969630837440491}, {"text": "K", "start_pos": 77, "end_pos": 78, "type": "METRIC", "confidence": 0.9638726711273193}]}, {"text": "The fact that the NN method performs better than the IB with similar K values (NN K = 24 vs. IB K = 25) seems to suggest that the JS divergence provides a better model for the predominant class than the compression model of the IB.", "labels": [], "entities": [{"text": "JS divergence", "start_pos": 130, "end_pos": 143, "type": "TASK", "confidence": 0.716983437538147}]}, {"text": "However, it is likely that the IB performance suffered due to our choice of test data.", "labels": [], "entities": [{"text": "IB", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.849963366985321}]}, {"text": "As the method is global, it performs better when the target classes are represented by a high number of verbs.", "labels": [], "entities": []}, {"text": "In our experiment, many semantic classes were represented by two verbs only (section 2).", "labels": [], "entities": []}, {"text": "Nevertheless, the IB method has the clear advantage that it allows for more clusters to be produced.", "labels": [], "entities": []}, {"text": "At best it classified half of the verbs correctly according to their predominant sense (mPUR = 50%).", "labels": [], "entities": [{"text": "mPUR", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9126275777816772}]}, {"text": "Although this leaves room for improvement, the result compares favourably to previously published results . We argue, however, that evaluation against a monosemous gold standard reveals only part of the picture.", "labels": [], "entities": []}, {"text": "25 12% 18% (14% + 5\u03c3) 39% 48% (43%+ 3\u03c3) 35 14% 20% (16% + 6\u03c3) 47% 59% (50%+ 4\u03c3) 42 15% 19% (16% + 3\u03c3) 50% 59% (54%+ 2\u03c3): Evaluation against the monosemous (Pred.) and polysemous (Multiple) gold standards.", "labels": [], "entities": []}, {"text": "The figures in parentheses are results of evaluation on randomly polysemous data + significance of the actual Results were obtained with finegrained SCFs (including prepositions).", "labels": [], "entities": [{"text": "significance", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9552810788154602}]}, {"text": "In evaluation against the polysemic gold standard, we assume that a verb which is polysemous in our corpus data may appear in a cluster with verbs that share any of its senses.", "labels": [], "entities": []}, {"text": "In order to evaluate the clusters against polysemous data, we assigned each polysemic verb Va single sense: the one it shares with the highest number of verbs in the cluster K(V ).", "labels": [], "entities": []}, {"text": "shows the results against polysemic and monosemous gold standards.", "labels": [], "entities": []}, {"text": "The former are noticeably better than the latter (e.g. IB with K = 42 is 9% better).", "labels": [], "entities": [{"text": "IB", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9684163928031921}]}, {"text": "Clearly, allowing for multiple gold standard classes makes it easier to obtain better results with evaluation.", "labels": [], "entities": []}, {"text": "In order to show that polysemy makes a nontrivial contribution in shaping the clusters, we measured the improvement that can be due to pure chance by creating randomly polysemous gold standards.", "labels": [], "entities": []}, {"text": "We constructed 100 sets of random gold standards.", "labels": [], "entities": []}, {"text": "In each iteration, the verbs kept their original predominant senses, but the set of additional senses was taken entirely from another verb -chosen at random.", "labels": [], "entities": []}, {"text": "By doing so, we preserved the dominant sense of each verb, the total frequency of all senses and the correlations between the additional senses.", "labels": [], "entities": []}, {"text": "The results included in table 3 indicate, with 99.5% confidence (3\u03c3 and above), that the improvement obtained with the polysemous gold standard is not artificial (except in two cases with 95% confidence).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test verbs and their monosemous/polysemic gold standard senses", "labels": [], "entities": []}, {"text": " Table 2: Clustering performance on the predominant senses,", "labels": [], "entities": []}, {"text": " Table 3: Evaluation against the monosemous (Pred.) and pol-", "labels": [], "entities": [{"text": "Pred.", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.8483381867408752}]}, {"text": " Table 4: The fraction of verb pairs clustered together, as a", "labels": [], "entities": []}, {"text": " Table 5: The fraction of verb pairs clustered together, as a", "labels": [], "entities": []}]}