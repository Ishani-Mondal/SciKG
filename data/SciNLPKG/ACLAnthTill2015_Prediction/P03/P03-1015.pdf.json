{"title": [{"text": "Combining Deep and Shallow Approaches in Parsing German", "labels": [], "entities": []}], "abstractContent": [{"text": "The paper describes two parsing schemes: a shallow approach based on machine learning and a cascaded finite-state parser with a hand-crafted grammar.", "labels": [], "entities": []}, {"text": "It discusses several ways to combine them and presents evaluation results for the two individual approaches and their combination.", "labels": [], "entities": []}, {"text": "An underspecification scheme for the output of the finite-state parser is introduced and shown to improve performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In several areas of Natural Language Processing, a combination of different approaches has been found to give the best results.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6220627923806509}]}, {"text": "It is especially rewarding to combine deep and shallow systems, where the former guarantees interpretability and high precision and the latter provides robustness and high recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9974862337112427}, {"text": "recall", "start_pos": 172, "end_pos": 178, "type": "METRIC", "confidence": 0.9979277849197388}]}, {"text": "This paper investigates such a combination consisting of an n-gram based shallow parser and a cascaded finite-state parser 1 with hand-crafted grammar and morphological checking.", "labels": [], "entities": []}, {"text": "The respective strengths and weaknesses of these approaches are brought to light in an in-depth evaluation on a treebank of German newspaper texts () containing ca.", "labels": [], "entities": []}, {"text": "340,000 tokens in 19,546 sentences.", "labels": [], "entities": []}, {"text": "The evaluation format chosen (dependency tuples) is used as the common denominator of the systems Although not everyone would agree that finite-state parsers constitute a 'deep' approach to parsing, they still are knowledge-based, require efforts of grammar-writing, a complex linguistic lexicon, manage without training data, etc.", "labels": [], "entities": []}, {"text": "in building a hybrid parser with improved performance.", "labels": [], "entities": []}, {"text": "An underspecification scheme allows the finite-state parser partially ambiguous output.", "labels": [], "entities": []}, {"text": "It is shown that the other parser can inmost cases successfully disambiguate such information.", "labels": [], "entities": []}, {"text": "Section 2 discusses the evaluation format adopted (dependency structures), its advantages, but also some of its controversial points.", "labels": [], "entities": []}, {"text": "Section 3 formulates a classification problem on the basis of the evaluation format and applies a machine learner to it.", "labels": [], "entities": []}, {"text": "Section 4 describes the architecture of the cascaded finite-state parser and its output in a novel underspecification format.", "labels": [], "entities": []}, {"text": "Section 5 explores several combination strategies and tests them on several variants of the two base components.", "labels": [], "entities": []}, {"text": "Section 6 provides an in-depth evaluation of the component systems and the hybrid parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "The simplest method to evaluate a parser is to count the parse trees it gets correct.", "labels": [], "entities": []}, {"text": "This measure is, however, not very informative since most applications do not require one hundred percent correct parse trees.", "labels": [], "entities": []}, {"text": "Thus, an important question in parser evaluation is how to breakdown parsing results.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9483470022678375}]}, {"text": "In the PARSEVAL evaluation scheme, partially correct parses are gauged by the number of nodes they produce and have in common with the gold standard (measured in precision and recall).", "labels": [], "entities": [{"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9990580677986145}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9954255223274231}]}, {"text": "Another figure (crossing brackets) only counts those incorrect nodes that change the partial order induced by the tree.", "labels": [], "entities": []}, {"text": "A problematic aspect of the PARSEVAL approach is that the weight given to particular constructions is again grammar-specific, since some grammars may need more nodes to describe them than others.", "labels": [], "entities": []}, {"text": "Further, the approach does not pay sufficient heed to the fact that parsing decisions are often intricately twisted: One wrong decision may produce a whole series of other wrong decisions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.9661878347396851}]}, {"text": "Both these problems are circumvented when parsing results are evaluated on a more abstract level, viz.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9620530605316162}]}, {"text": "Dependency structure generally follows predicateargument structure, but departs from it in that the basic building blocks are words rather than predicates.", "labels": [], "entities": [{"text": "Dependency structure", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9018333852291107}]}, {"text": "In terms of parser evaluation, the first property guarantees independence of decisions (every link is relevant also for the interpretation level), while the second property makes fora better empirical justification.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8669671416282654}]}, {"text": "Dependency structure can be modelled by a directed acylic graph, with word tokens at the nodes.", "labels": [], "entities": [{"text": "Dependency structure", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8445396721363068}]}, {"text": "In labelled dependency structure, the links are furthermore classified into a certain set of grammatical roles.", "labels": [], "entities": []}, {"text": "Dependency can be easily determined from constituent structure if in every phrase structure rule a constituent is singled out as the head.", "labels": [], "entities": []}, {"text": "To derive a labelled dependency structure, all non-head constituents in a rule must be labelled with the grammatical role that links their head tokens to the head token of the head constituent.", "labels": [], "entities": []}, {"text": "There are two cases where the divergence between predicates and word tokens makes trouble: (1) predicates expressed by more than one token, and (2) predicates expressed by no token (as they occur in ellipsis).", "labels": [], "entities": []}, {"text": "Case 1 frequently occurs within the verb complex (of both English and German).", "labels": [], "entities": []}, {"text": "The solution proposed in the literature () is to define a normal form for dependency structure, where every adjunct or argument attaches to some distinguished part of the verb complex.", "labels": [], "entities": [{"text": "dependency structure", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.8312671184539795}]}, {"text": "The underlying assumption is that those cases where scope decisions in the verb complex are semantically relevant (e.g. with modal verbs) are not resolvable in syntax anyway.", "labels": [], "entities": []}, {"text": "There is no generally accepted solution for case 2 (ellipsis).", "labels": [], "entities": []}, {"text": "Most authors in the evaluation literature neglect it, perhaps due to its infrequency (in the NEGRA corpus, ellipsis only occurs in 1.2% of all dependency relations).", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.8739758431911469}]}, {"text": "proposes to promote one of the dependents (preferably an obligatory one) (1a) or even all dependents (1b) to head status.", "labels": [], "entities": []}, {"text": "(1) a. the very brave b.", "labels": [], "entities": []}, {"text": "John likes tea and Harry coffee.", "labels": [], "entities": []}, {"text": "A more sweeping solution to these problems is to abandon dependency structure at all and directly go for predicate-argument structure).", "labels": [], "entities": []}, {"text": "But as we argued above, moving to a more theoretical level is detrimental to comparability across grammatical frameworks.", "labels": [], "entities": []}, {"text": "We used -grams (3-grams and 5-grams) of POS tags as context and C4.5 for machine learning.", "labels": [], "entities": []}, {"text": "All results were subjected to 10-fold cross validation.", "labels": [], "entities": []}, {"text": "The learning algorithm always returns a result.", "labels": [], "entities": []}, {"text": "We counted a result as not assigned, however, if it referred to ahead token outside the sentence.", "labels": [], "entities": []}, {"text": "See for results 4 of the learner.", "labels": [], "entities": []}, {"text": "The left column shows performance with POS tags from the treebank (ideal tags, I-tags), the right column values obtained with POS tags as generated automatically by a tagger with an accuracy of 95% (tagger tags, T-tags).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.998828113079071}]}, {"text": "The nth-tag head representation outperforms the distance representation by 10%.", "labels": [], "entities": []}, {"text": "Considering acyclicity (cover) slightly improves performance, but the gain is not statistically significant (t-test with 99%).", "labels": [], "entities": []}, {"text": "The results are quite impressive as they stand, in particular the nth-tag 5-gram version seems to achieve quite good results.", "labels": [], "entities": []}, {"text": "It should, however, be stressed that most of the dependencies correctly determined by the n-gram methods extend over no more than 3 tokens.", "labels": [], "entities": []}, {"text": "With the distance method, such 'short' dependencies makeup 98.90% of all dependencies correctly found, with the nth-tag method still 82%, but only 79.63% with the finite-state parser (see section 4) and 78.91% in the treebank.", "labels": [], "entities": []}], "tableCaptions": []}