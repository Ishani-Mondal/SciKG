{"title": [{"text": "Closing the Gap: Learning-Based Information Extraction Rivaling Knowledge-Engineering Methods", "labels": [], "entities": [{"text": "Learning-Based Information Extraction Rivaling Knowledge-Engineering Methods", "start_pos": 17, "end_pos": 93, "type": "TASK", "confidence": 0.7971707930167516}]}], "abstractContent": [{"text": "In this paper, we present a learning approach to the scenario template task of information extraction, where information filling one template could come from multiple sentences.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.7817753553390503}]}, {"text": "When tested on the MUC-4 task, our learning approach achieves accuracy competitive to the best of the MUC-4 systems, which were all built with manually engineered rules.", "labels": [], "entities": [{"text": "MUC-4 task", "start_pos": 19, "end_pos": 29, "type": "DATASET", "confidence": 0.8527320921421051}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9994813799858093}]}, {"text": "Our analysis reveals that our use of full parsing and state-of-the-art learning algorithms have contributed to the good performance.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first research to have demonstrated that a learning approach to the full-scale information extraction task could achieve performance rivaling that of the knowledge-engineering approach.", "labels": [], "entities": [{"text": "information extraction task", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.765033890803655}]}], "introductionContent": [{"text": "The explosive growth of online texts written in natural language has prompted much research into information extraction (IE), the task of automatically extracting specific information items of interest from natural language texts.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.865276300907135}, {"text": "automatically extracting specific information items of interest from natural language texts", "start_pos": 138, "end_pos": 229, "type": "TASK", "confidence": 0.48492584987120196}]}, {"text": "The extracted information is used to fill database records, also known as templates in the IE literature.", "labels": [], "entities": [{"text": "IE literature", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.7456332743167877}]}, {"text": "Research efforts on IE tackle a variety of tasks.", "labels": [], "entities": [{"text": "IE", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9910953044891357}]}, {"text": "They include extracting information from semistructured texts, such as seminar announcements, rental and job advertisements, etc., as well as from free texts, such as newspaper articles.", "labels": [], "entities": []}, {"text": "IE from semi-structured texts is easier than from free texts, since the layout and format of a semi-structured text provide additional useful clues  to aid in extraction.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9822412133216858}]}, {"text": "Several benchmark data sets have been used to evaluate IE approaches on semistructured texts.", "labels": [], "entities": [{"text": "IE", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9950922727584839}]}, {"text": "For the task of extracting information from free texts, a series of Message Understanding Conferences (MUC) provided benchmark data sets for evaluation.", "labels": [], "entities": [{"text": "extracting information from free texts", "start_pos": 16, "end_pos": 54, "type": "TASK", "confidence": 0.8255707859992981}, {"text": "Message Understanding Conferences (MUC)", "start_pos": 68, "end_pos": 107, "type": "TASK", "confidence": 0.7129290004571279}]}, {"text": "Several subtasks for IE from free texts have been identified.", "labels": [], "entities": [{"text": "IE", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.991061806678772}]}, {"text": "The named entity (NE) task extracts person names, organization names, location names, etc.", "labels": [], "entities": []}, {"text": "The template element (TE) task extracts information centered around an entity, like the acronym, category, and location of a company.", "labels": [], "entities": []}, {"text": "The template relation (TR) task extracts relations between entities.", "labels": [], "entities": []}, {"text": "Finally, the full-scale IE task, the scenario template (ST) task, deals with extracting generic information items from free texts.", "labels": [], "entities": [{"text": "IE task", "start_pos": 24, "end_pos": 31, "type": "TASK", "confidence": 0.9188210368156433}, {"text": "extracting generic information items from free texts", "start_pos": 77, "end_pos": 129, "type": "TASK", "confidence": 0.829717252935682}]}, {"text": "To tackle the full ST task, an IE system needs to merge information from multiple sentences in general, since the information needed to fill one template can come from multiple sentences, and thus discourse processing is needed.", "labels": [], "entities": [{"text": "IE", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9661305546760559}]}, {"text": "The full-scale ST task is considerably harder than all the other IE tasks or subtasks outlined above.", "labels": [], "entities": [{"text": "ST task", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.8892067670822144}, {"text": "IE tasks", "start_pos": 65, "end_pos": 73, "type": "TASK", "confidence": 0.904959499835968}]}, {"text": "As is the case with many other natural language processing (NLP) tasks, there are two main approaches to IE, namely the knowledge-engineering approach and the learning approach.", "labels": [], "entities": [{"text": "IE", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9937666654586792}]}, {"text": "Most early IE systems adopted the knowledge-engineering ap-  proach, where manually engineered rules were used for IE.", "labels": [], "entities": [{"text": "IE", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9687612056732178}]}, {"text": "More recently, machine learning approaches have been used for IE from semi-structured texts, named entity extraction (, template element extraction, and template relation extraction).", "labels": [], "entities": [{"text": "IE", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9944860339164734}, {"text": "entity extraction", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.742522120475769}, {"text": "template element extraction", "start_pos": 120, "end_pos": 147, "type": "TASK", "confidence": 0.6224234600861868}, {"text": "template relation extraction", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.6136049528916677}]}, {"text": "These machine learning approaches have been successful for these tasks, achieving accuracy comparable to the knowledge-engineering approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9993732571601868}]}, {"text": "However, for the full-scale ST task of generic IE from free texts, the best reported method to date is still the knowledge-engineering approach.", "labels": [], "entities": [{"text": "ST task", "start_pos": 28, "end_pos": 35, "type": "TASK", "confidence": 0.9104931652545929}, {"text": "IE", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.8099712133407593}]}, {"text": "For example, almost all participating IE systems in MUC used the knowledge-engineering approach for the full-scale ST task.", "labels": [], "entities": [{"text": "MUC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8834159970283508}, {"text": "ST task", "start_pos": 115, "end_pos": 122, "type": "TASK", "confidence": 0.8682079315185547}]}, {"text": "The one notable exception is the work of UMass at MUC-6 ().", "labels": [], "entities": [{"text": "UMass", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.914463996887207}, {"text": "MUC-6", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.5351718068122864}]}, {"text": "Unfortunately, their learning approach did considerably worse than the best MUC-6 systems.", "labels": [], "entities": []}, {"text": "Soderland (1999) and Chieu and Ng (2002a) attempted machine learning approaches fora scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.", "labels": [], "entities": []}, {"text": "In this paper, we present a learning approach to the full-scale ST task of extracting information from free texts.", "labels": [], "entities": [{"text": "ST task of extracting information from free texts", "start_pos": 64, "end_pos": 113, "type": "TASK", "confidence": 0.7649155631661415}]}, {"text": "The task we tackle is considerably more complex than that of), since we need to deal with merging information from multiple sentences to fill one template.", "labels": [], "entities": []}, {"text": "We evaluated our learning approach on the MUC-4 task of extracting terrorist events from free texts.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 42, "end_pos": 47, "type": "TASK", "confidence": 0.5058079361915588}, {"text": "extracting terrorist events from free texts", "start_pos": 56, "end_pos": 99, "type": "TASK", "confidence": 0.7366170883178711}]}, {"text": "We chose the MUC-4 task since manually prepared templates required for training are available.", "labels": [], "entities": [{"text": "MUC-4 task", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.592661589384079}]}, {"text": "When trained and tested on the official benchmark data of MUC-4, our learning approach achieves accuracy competitive with the best MUC-4 systems, which were all built using manually engineered rules.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.5720896124839783}, {"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9993796348571777}]}, {"text": "To our knowledge, our work is the first learning-based approach to have achieved performance competitive with the knowledge-engineering approach on the full-scale ST task.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are 1,300 training documents, of which 700 are relevant (i.e., have one or more event templates).", "labels": [], "entities": []}, {"text": "There are two official test sets, i.e., TST3 and TST4, containing 100 documents each.", "labels": [], "entities": [{"text": "TST3", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.6765925288200378}, {"text": "TST4", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.6650603413581848}]}, {"text": "We trained our system ALICE using the 700 documents with relevant templates, and then tested it on the two official test sets.", "labels": [], "entities": [{"text": "ALICE", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.988814651966095}]}, {"text": "The output templates were scored using the scorer provided on the official website.", "labels": [], "entities": []}, {"text": "The accuracy figures of ALICE (with different learning algorithms) on string slots and all slots are listed in, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995254278182983}]}, {"text": "Accuracy is measured in terms of recall (R), precision (P), and F-measure (F).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9900375604629517}, {"text": "recall (R)", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9477265328168869}, {"text": "precision (P)", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9553933888673782}, {"text": "F-measure (F)", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9657602161169052}]}, {"text": "We also list in the two tables the accuracy figures of the top 7 (out of a total of 17) systems that participated in MUC-4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9995817542076111}, {"text": "MUC-4", "start_pos": 117, "end_pos": 122, "type": "TASK", "confidence": 0.5775572061538696}]}, {"text": "The accuracy figures in the two tables are obtained by running the official scorer on the output templates of ALICE, and those of the MUC-4 participating systems (available   on the official web site).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997163414955139}, {"text": "ALICE", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.6733060479164124}, {"text": "MUC-4", "start_pos": 134, "end_pos": 139, "type": "DATASET", "confidence": 0.8568692803382874}]}, {"text": "The same history file downloaded from the official website is uniformly used for scoring the output templates of all systems (the history file contains the arbitration decisions for ambiguous cases).", "labels": [], "entities": []}, {"text": "We conducted statistical significance test, using the approximate randomization method adopted in MUC-4.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.8821514844894409}]}, {"text": "shows the systems that are not significantly different from Alice-ME.", "labels": [], "entities": []}, {"text": "Our system ALICE-ME, using a learning approach, is able to achieve accuracy competitive to the best of the MUC-4 participating systems, which were all built using manually engineered rules.", "labels": [], "entities": [{"text": "ALICE-ME", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.6833454966545105}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9994418025016785}, {"text": "MUC-4", "start_pos": 107, "end_pos": 112, "type": "DATASET", "confidence": 0.8336523175239563}]}, {"text": "We also observed that ME and SVM, the more recent machine learning algorithms, performed better than DT and NB.", "labels": [], "entities": [{"text": "ME", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.7085504531860352}]}, {"text": "To illustrate the benefit of full parsing, we conducted experiments using a subset of features, with and without full parsing.", "labels": [], "entities": []}, {"text": "We used ME as the learning algorithm in these experiments.", "labels": [], "entities": []}, {"text": "The results on string slots are summarized in: Accuracy of string slots with and without full parsing baseline system used only two features, headword (H) and named entity class (NE).", "labels": [], "entities": []}, {"text": "Next, we added three features, VAg, VPa, and V-Prep.", "labels": [], "entities": []}, {"text": "Without full parsing, these verbs were obtained based on the immediately preceding (or following) verb of a noun phrase, and the voice of the verb.", "labels": [], "entities": []}, {"text": "With full parsing, these verbs were obtained based on traversing the full parse tree.", "labels": [], "entities": []}, {"text": "The results indicate that verb features contribute to the performance of the system, even without full parsing.", "labels": [], "entities": []}, {"text": "With full parsing, verbs can be determined more accurately, leading to better overall performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy of string slots on the TST3 and  TST4 test set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9980074763298035}, {"text": "TST3", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.8091691732406616}, {"text": "TST4 test set", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.8678206404050192}]}, {"text": " Table 4: Accuracy of all slots on the TST3 and TST4  test set", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991884827613831}, {"text": "TST3", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.8245364427566528}, {"text": "TST4  test set", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.8656970461209615}]}, {"text": " Table 5: Systems whose F-measures are not signif- icantly different from Alice-ME at the 0.10 signifi- cance level with 0.99 confidence", "labels": [], "entities": [{"text": "F-measures", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.918099582195282}]}, {"text": " Table 6: Accuracy of string slots with and without  full parsing", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9947524070739746}, {"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.6197168231010437}]}]}