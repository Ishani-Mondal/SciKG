{"title": [], "abstractContent": [{"text": "Truecasing is the process of restoring case information to badly-cased or non-cased text.", "labels": [], "entities": []}, {"text": "This paper explores truecas-ing issues and proposes a statistical, language modeling based truecaser which achieves an accuracy of \u223c98% on news articles.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.999390721321106}]}, {"text": "Task based evaluation shows a 26% F-measure improvement in named entity recognition when using truecasing.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9986661672592163}, {"text": "named entity recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6176053484280905}]}, {"text": "In the context of automatic content extraction , mention detection on automatic speech recognition text is also improved by a factor of 8.", "labels": [], "entities": [{"text": "automatic content extraction", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6595126291116079}, {"text": "mention detection", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7543751299381256}]}, {"text": "Truecasing also enhances machine translation output legibility and yields a BLEU score improvement of 80.2%.", "labels": [], "entities": [{"text": "machine translation output legibility", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.7724742144346237}, {"text": "BLEU score", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9813312292098999}]}, {"text": "This paper argues for the use of truecasing as a valuable component in text processing applications.", "labels": [], "entities": [{"text": "text processing", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.796142190694809}]}], "introductionContent": [{"text": "While it is true that large, high quality text corpora are becoming a reality, it is also true that the digital world is flooded with enormous collections of low quality natural language text.", "labels": [], "entities": []}, {"text": "Transcripts from various audio sources, automatic speech recognition, optical character recognition, online messaging and gaming, email, and the web are just a few examples of raw text sources with content often produced in a hurry, containing misspellings, insertions, deletions, grammatical errors, neologisms, jargon terms \u2660 Work done at IBM TJ Watson Research Center etc.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.7313313285509745}, {"text": "optical character recognition", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.696709156036377}, {"text": "IBM TJ Watson Research Center", "start_pos": 341, "end_pos": 370, "type": "DATASET", "confidence": 0.7656406223773956}]}, {"text": "We want to enhance the quality of such sources in order to produce better rule-based systems and sharper statistical models.", "labels": [], "entities": []}, {"text": "This paper focuses on truecasing, which is the process of restoring case information to raw text.", "labels": [], "entities": []}, {"text": "Besides text rEaDaBILiTY, truecasing enhances the quality of case-carrying data, brings into the picture new corpora originally considered too noisy for various NLP tasks, and performs case normalization across styles, sources, and genres.", "labels": [], "entities": [{"text": "case normalization", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.7516194880008698}]}, {"text": "Consider the following mildly ambiguous sentence \"us rep.", "labels": [], "entities": []}, {"text": "james pond showed up riding an it and going to a now meeting\".", "labels": [], "entities": []}, {"text": "The case-carrying alternative \"US Rep.", "labels": [], "entities": [{"text": "US Rep", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9328856468200684}]}, {"text": "James Pond showed up riding an IT and going to a NOW meeting\" is arguably better fit to be subjected to further processing.", "labels": [], "entities": [{"text": "NOW", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8875749707221985}]}, {"text": "Broadcast news transcripts contain casing errors which reduce the performance of tasks such as named entity tagging.", "labels": [], "entities": [{"text": "named entity tagging", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.6403754651546478}]}, {"text": "Automatic speech recognition produces non-cased text.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7172598242759705}]}, {"text": "Headlines, teasers, section headers -which carry high information content -are not properly cased for tasks such as question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.866866409778595}]}, {"text": "Truecasing is an essential step in transforming these types of data into cleaner sources to be used by NLP applications.", "labels": [], "entities": []}, {"text": "\"the president\" and \"the President\" are two viable surface forms that correctly convey the same information in the same context.", "labels": [], "entities": []}, {"text": "Such discrepancies are usually due to differences in news source, authors, and stylistic choices.", "labels": [], "entities": []}, {"text": "Truecasing can be used as a normalization tool across corpora in order to produce consistent, context sensitive, case information; it consistently reduces expressions to their statistical canonical form.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to show the benefits of truecasing in general as a valuable building block for NLP applications rather than promoting a specific implementation.", "labels": [], "entities": []}, {"text": "We explore several truecasing issues and propose a statistical, language modeling based truecaser, showing its performance on news articles.", "labels": [], "entities": []}, {"text": "Then, we present a straightforward application of truecasing on machine translation output.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.7657654285430908}]}, {"text": "Finally, we demonstrate the considerable benefits of truecasing through task based evaluations on named entity tagging and automatic content extraction.", "labels": [], "entities": [{"text": "named entity tagging", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.6946592132250468}, {"text": "content extraction", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.6787526458501816}]}], "datasetContent": [{"text": "Both the unigram model and the language model based truecaser were trained on the AQUAINT (ARDA) and TREC (NIST) corpora, each consisting of 500M token news stories from various news agencies.", "labels": [], "entities": [{"text": "AQUAINT", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.860469400882721}]}, {"text": "The truecaser was built using IBM's ViaVoice TM language modeling tools.", "labels": [], "entities": [{"text": "ViaVoice TM language", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.8844470779101054}]}, {"text": "These tools implement trigram language models using deleted interpolation for backing off if the trigram is not found in the training data.", "labels": [], "entities": []}, {"text": "The resulting model's perplexity is 108.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9586086869239807}]}, {"text": "Since there is no absolute truth when truecasing a sentence, the experiments need to be built with some reference in mind.", "labels": [], "entities": []}, {"text": "Our assumption is that professionally written news articles are very close to an intangible absolute truth in terms of casing.", "labels": [], "entities": []}, {"text": "Furthermore, we ignore the impact of diverging stylistic forms, assuming the differences are minor.", "labels": [], "entities": []}, {"text": "Based on the above assumptions we judge the truecasing methods on four different test sets.", "labels": [], "entities": []}, {"text": "The first test set (APR) consists of the August 25, 2002 * top 20 news stories from Associated Press and Reuters excluding titles, headlines, and section headers which together form the second test set (APR+).", "labels": [], "entities": []}, {"text": "The third test set (ACE) consists of ear- * Randomly chosen test date lier news stories from AP and New York Times belonging to the ACE dataset.", "labels": [], "entities": [{"text": "ear- * Randomly chosen test date lier", "start_pos": 37, "end_pos": 74, "type": "METRIC", "confidence": 0.8376907408237457}, {"text": "AP and New York Times", "start_pos": 93, "end_pos": 114, "type": "DATASET", "confidence": 0.7917033314704895}, {"text": "ACE dataset", "start_pos": 132, "end_pos": 143, "type": "DATASET", "confidence": 0.9517651796340942}]}, {"text": "The last test set (MT) includes a set of machine translation references (i.e. human translations) of news articles from the Xinhua agency.", "labels": [], "entities": [{"text": "machine translation references (i.e. human translations) of news articles from the Xinhua agency", "start_pos": 41, "end_pos": 137, "type": "TASK", "confidence": 0.7531681815783183}]}, {"text": "The sizes of the data sets are as follows: APR -12k tokens, ACE -90k tokens, and MT -63k tokens.", "labels": [], "entities": [{"text": "APR", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.6232560276985168}]}, {"text": "For both truecasing methods, we computed the agreement with the original news story considered to be the ground truth.", "labels": [], "entities": []}, {"text": "Case restoration and normalization can be employed for more complex tasks.", "labels": [], "entities": [{"text": "Case restoration", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8904138803482056}]}, {"text": "We have successfully leveraged truecasing in improving named entity recognition and automatic content extraction.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.6875909864902496}, {"text": "automatic content extraction", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.6401194532712301}]}], "tableCaptions": [{"text": " Table 1: BLEU score for several truecasing strategies. (truecasing+ methods additionally employ the \"first  sentence letter uppercased\" rule adjustment).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999395489692688}]}, {"text": " Table 2: Named Entity Recognition performance with truecasing and without (baseline).", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.5395152866840363}]}, {"text": " Table 3: Results of ACE mention detection with and without truecasing.", "labels": [], "entities": [{"text": "ACE", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.7596326470375061}]}]}