{"title": [{"text": "Evaluation challenges in large-scale document summarization", "labels": [], "entities": [{"text": "summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.518652617931366}]}], "abstractContent": [{"text": "We present a large-scale meta evaluation of eight evaluation measures for both single-document and multi-document summarizers.", "labels": [], "entities": []}, {"text": "To this end we built a corpus consisting of (a) 100 Million automatic summaries using six summarizers and baselines at ten summary lengths in both English and Chinese, (b) more than 10,000 manual abstracts and extracts, and (c) 200 Million automatic document and summary retrievals using 20 queries.", "labels": [], "entities": []}, {"text": "We present both qualitative and quantitative results showing the strengths and drawbacks of all evaluation methods and how they rank the different summarizers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic document summarization is afield that has seen increasing attention from the NLP community in recent years.", "labels": [], "entities": [{"text": "Automatic document summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7410827477773031}]}, {"text": "In part, this is because summarization incorporates many important aspects of both natural language understanding and natural language generation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9831716418266296}, {"text": "natural language understanding", "start_pos": 83, "end_pos": 113, "type": "TASK", "confidence": 0.6682776908079783}, {"text": "natural language generation", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.652752141157786}]}, {"text": "In part it is because effective automatic summarization would be useful in a variety of areas.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.8982070684432983}]}, {"text": "Unfortunately, evaluating automatic summarization in a standard and inexpensive way is a difficult task ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9043989777565002}]}, {"text": "Traditional large-scale evaluations are either too simplistic (using measures like precision, recall, and percent agreement which (1) don't take chance agreement into account and (2) don't account for the fact that human judges don't agree which sentences should be in a summary) or too expensive (an approach using manual judgements can scale up to a few hundred summaries but not to tensor hundreds of thousands).", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9991617202758789}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9984385371208191}]}, {"text": "In this paper, we present a comparison of six summarizers as well as a meta-evaluation including eight measures: Precision/Recall, Percent Agreement, Kappa, Relative Utility, Relevance Correlation, and three types of Content-Based measures (cosine, longest common subsequence, and word overlap).", "labels": [], "entities": [{"text": "Precision/Recall", "start_pos": 113, "end_pos": 129, "type": "METRIC", "confidence": 0.8389090696970621}, {"text": "Percent Agreement", "start_pos": 131, "end_pos": 148, "type": "METRIC", "confidence": 0.703577846288681}]}, {"text": "We found that while all measures tend to rank summarizers in different orders, measures like Kappa, Relative Utility, Relevance Correlation and Content-Based each offer significant advantages over the more simplistic methods.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 93, "end_pos": 98, "type": "METRIC", "confidence": 0.9737645983695984}]}], "datasetContent": [{"text": "We performed our experiments on the Hong Kong News corpus provided by the Hong Kong SAR of the People's Republic of China (LDC catalog number LDC2000T46).", "labels": [], "entities": [{"text": "Hong Kong News corpus provided by the Hong Kong SAR of the People's Republic of China (LDC catalog number LDC2000T46)", "start_pos": 36, "end_pos": 153, "type": "DATASET", "confidence": 0.9143129768578903}]}, {"text": "It contains 18,146 pairs of parallel documents in English and Chinese.", "labels": [], "entities": []}, {"text": "The texts are not typical news articles.", "labels": [], "entities": []}, {"text": "The Hong Kong Newspaper mainly publishes announcements of the local administration and descriptions of municipal events, such as an anniversary of the fire department, or seasonal festivals.", "labels": [], "entities": [{"text": "Hong Kong Newspaper", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.903666079044342}]}, {"text": "We tokenized the corpus to identify headlines and sentence boundaries.", "labels": [], "entities": []}, {"text": "For the English text, we used a lemmatizer for nouns and verbs.", "labels": [], "entities": []}, {"text": "We also segmented the Chinese documents using the tool provided at http://www.mandarintools.com.", "labels": [], "entities": []}, {"text": "asked LDC to build a set of queries ().", "labels": [], "entities": [{"text": "LDC", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.9268765449523926}]}, {"text": "Each of these queries produced a cluster of relevant documents.", "labels": [], "entities": []}, {"text": "Twenty of these clusters were used in the experiments in this paper.", "labels": [], "entities": []}, {"text": "Additionally, we needed manual summaries or extracts for reference.", "labels": [], "entities": []}, {"text": "The LDC annotators produced summaries for each document in all clusters.", "labels": [], "entities": []}, {"text": "In order to produce human extracts, our judges also labeled sentences with \"relevance judgements\", which indicate the relevance of sentence to the topic of the document.", "labels": [], "entities": []}, {"text": "The relevance judgements for sentences range from 0 (irrelevant) to 10 (essential).", "labels": [], "entities": []}, {"text": "As in (), in order to create an extract of a certain length, we simply extract the top scoring sentences that add up to that length.", "labels": [], "entities": []}, {"text": "For each target summary length, we produce an extract using a summarizer or baseline.", "labels": [], "entities": []}, {"text": "Then we compare the output of the summarizer or baseline with the extract produced from the human relevance judgements.", "labels": [], "entities": []}, {"text": "Both the summarizers and the evaluation measures are described in greater detail in the next two sections.", "labels": [], "entities": []}, {"text": "We used three general types of evaluation measures: co-selection, content-based similarity, and relevance correlation.", "labels": [], "entities": []}, {"text": "Co-selection measures include precision and recall of co-selected sentences, relative utility (), and.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9990411400794983}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9974938631057739}]}, {"text": "Co-selection methods have some restrictions: they only work for extractive summarizers.", "labels": [], "entities": []}, {"text": "Two manual summaries of the same input do not in general share many identical sentences.", "labels": [], "entities": []}, {"text": "We address this weakness of co-selection: All runs performed (X = 20 clusters, x = 10 clusters).", "labels": [], "entities": []}, {"text": "Language: E = English, C = Chinese, X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.; S = sentence-based, W = word-based; #dj = number of \"docjudges\" (ranked lists of documents and summaries).", "labels": [], "entities": []}, {"text": "Target lengths above 50% are not shown in this table for lack of space.", "labels": [], "entities": []}, {"text": "Each run is available using two different retrieval schemes.", "labels": [], "entities": []}, {"text": "We report results using the cross-lingual retrievals in a separate paper.", "labels": [], "entities": []}, {"text": "measures with several content-based similarity measures.", "labels": [], "entities": []}, {"text": "The similarity measures we use are word overlap, longest common subsequence, and cosine.", "labels": [], "entities": []}, {"text": "One advantage of similarity measures is that they can compare manual and automatic extracts with manual abstracts.", "labels": [], "entities": []}, {"text": "To our knowledge, no systematic experiments about agreement on the task of summary writing have been performed before.", "labels": [], "entities": [{"text": "summary writing", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.8555641770362854}]}, {"text": "We use similarity measures to measure interjudge agreement among three judges per topic.", "labels": [], "entities": []}, {"text": "We also apply the measures between human extracts and summaries, which answers the question if human extracts are more similar to automatic extracts or to human summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 54, "end_pos": 63, "type": "TASK", "confidence": 0.957904577255249}]}, {"text": "The third group of evaluation measures includes relevance correlation.", "labels": [], "entities": []}, {"text": "It shows the relative performance of a summary: how much the performance of document retrieval decreases when indexing summaries rather than full texts.", "labels": [], "entities": []}, {"text": "Task-based evaluations (e.g.,), DUC), or () measure human performance using the summaries fora certain task (after the summaries are created).", "labels": [], "entities": [{"text": "DUC", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.926779568195343}]}, {"text": "Although they can be a very effective way of measuring summary quality, task-based evaluations are prohibitively expensive at large scales.", "labels": [], "entities": []}, {"text": "In this project, we didn't perform any task-based evaluations as they would not be appropriate at the scale of millions of summaries.", "labels": [], "entities": []}, {"text": "For each document and target length we produce three extracts from the three different judges, which we label throughout as J1, J2, and J3.", "labels": [], "entities": []}, {"text": "We used the rates 5%, 10%, 20%, 30%, 40% for most experiments.", "labels": [], "entities": []}, {"text": "For some experiments, we also consider summaries of 50%, 60%, 70%, 80% and 90% of the original length of the documents.", "labels": [], "entities": []}, {"text": "shows some abbreviations for co-selection that we will use throughout this section.", "labels": [], "entities": []}], "tableCaptions": []}