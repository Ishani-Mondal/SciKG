{"title": [{"text": "Constructing Semantic Space Models from Parsed Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning.", "labels": [], "entities": []}, {"text": "In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account.", "labels": [], "entities": []}, {"text": "We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector-based models of word co-occurrence have proved a useful representational framework fora variety of natural language processing (NLP) tasks such as word sense discrimination), text segmentation (), contextual spelling correction, automatic thesaurus extraction, and notably information retrieval (.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 154, "end_pos": 179, "type": "TASK", "confidence": 0.6785535116990408}, {"text": "text segmentation", "start_pos": 182, "end_pos": 199, "type": "TASK", "confidence": 0.758717268705368}, {"text": "contextual spelling correction", "start_pos": 204, "end_pos": 234, "type": "TASK", "confidence": 0.65104079246521}, {"text": "automatic thesaurus extraction", "start_pos": 236, "end_pos": 266, "type": "TASK", "confidence": 0.6596827904383341}, {"text": "information retrieval", "start_pos": 280, "end_pos": 301, "type": "TASK", "confidence": 0.8417034447193146}]}, {"text": "Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements) to semantic priming () and text comprehension.", "labels": [], "entities": []}, {"text": "In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important information about its meaning.", "labels": [], "entities": []}, {"text": "The semantic properties of words are represented by vectors that are constructed from the observed distributional patterns of co-occurrence of their neighbouring words.", "labels": [], "entities": []}, {"text": "Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context.", "labels": [], "entities": []}, {"text": "Contexts are defined as a small number of words surrounding the target word) or as entire paragraphs, even documents.", "labels": [], "entities": []}, {"text": "Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account.", "labels": [], "entities": []}, {"text": "A word can be thus viewed as a point in an n-dimensional semantic space.", "labels": [], "entities": []}, {"text": "The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance.", "labels": [], "entities": []}, {"text": "In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink ) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface forms).", "labels": [], "entities": []}, {"text": "Minimal assumptions are made with respect to syntactic dependencies among words.", "labels": [], "entities": []}, {"text": "In fact it is assumed that all context words within a certain distance from the target word are semantically relevant.", "labels": [], "entities": []}, {"text": "The lack of syntactic information makes the building of semantic space models relatively straightforward and language independent (all that is needed is a corpus of written or spoken text).", "labels": [], "entities": []}, {"text": "However, this entails that contextual information contributes indiscriminately to a word's meaning.", "labels": [], "entities": []}, {"text": "Some studies have tried to incorporate syntactic information into vector-based models.", "labels": [], "entities": []}, {"text": "In this view, the semantic space is constructed from words that bear a syntactic relationship to the target word of interest.", "labels": [], "entities": []}, {"text": "This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant.", "labels": [], "entities": []}, {"text": "However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g.,) or collapse all types of syntactic relations available fora given target word.", "labels": [], "entities": []}, {"text": "Although syntactic information is now used to select a word's appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing.", "labels": [], "entities": []}, {"text": "A commonly raised criticism for both types of semantic space models (i.e., word-based and syntaxbased) concerns the notion of semantic similarity.", "labels": [], "entities": []}, {"text": "Proximity between two words in the semantic space cannot indicate the nature of the lexical relations between them.", "labels": [], "entities": []}, {"text": "Distributionally similar words can be antonyms, synonyms, hyponyms or in some cases semantically unrelated.", "labels": [], "entities": []}, {"text": "This limits the application of semantic space models for NLP tasks which require distinguishing between lexical relations.", "labels": [], "entities": []}, {"text": "In this paper we generalise semantic space models by proposing a flexible conceptualisation of context which is parametrisable in terms of syntactic relations.", "labels": [], "entities": []}, {"text": "We develop a general framework for vectorbased models which can be optimised for different tasks.", "labels": [], "entities": []}, {"text": "Our framework allows the construction of semantic space to take place over words or syntactic relations thus bridging the distance between wordbased and syntax-based models.", "labels": [], "entities": []}, {"text": "Furthermore, we show how our model can incorporate well-defined, informative contexts in a principled way which retains information about the syntactic relations available fora given target word.", "labels": [], "entities": []}, {"text": "We first evaluate our model on semantic priming, a phenomenon that has received much attention in computational psycholinguistics and is typically modelled using word-based semantic spaces.", "labels": [], "entities": []}, {"text": "We next conduct a study that shows that our model is sensitive to different types of lexical relations.", "labels": [], "entities": []}], "datasetContent": [{"text": "A large number of modelling studies in psycholinguistics have focused on simulating semantic priming studies.", "labels": [], "entities": [{"text": "simulating semantic priming studies", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.7313076108694077}]}, {"text": "The semantic priming paradigm provides a natural test bed for semantic space models as it concentrates on the semantic similarity or dissimilarity between a prime and its target, and it is precisely this type of lexical relations that vectorbased models capture.", "labels": [], "entities": []}, {"text": "In this experiment we focus on Balota and Lorch's (1986) mediated priming study.", "labels": [], "entities": []}, {"text": "In semantic priming transient presentation of a prime word like tiger directly facilitates pronunciation or lexical decision on a target word like lion.", "labels": [], "entities": [{"text": "semantic priming transient presentation of a prime word", "start_pos": 3, "end_pos": 58, "type": "TASK", "confidence": 0.775375235825777}]}, {"text": "Mediated priming extends this paradigm by additionally allowing indirectly related words as primes -like stripes, which is only related to lion by means of the intermediate concept tiger.", "labels": [], "entities": []}, {"text": "obtained small mediated priming effects for pronunciation tasks but not for lexical decision.", "labels": [], "entities": []}, {"text": "For the pronunciation task, reaction times were reduced significantly for both direct and mediated primes, however the effect was larger for direct primes.", "labels": [], "entities": [{"text": "pronunciation task", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.9273739457130432}]}, {"text": "There are at least two semantic space simulations that attempt to shed light on the mediated priming effect.", "labels": [], "entities": []}, {"text": "replicated both the direct and mediated priming effects, whereas could only replicate direct priming.", "labels": [], "entities": []}, {"text": "In their study, mediated primes were farther from their targets than unrelated words.", "labels": [], "entities": []}, {"text": "In this experiment we examine whether dependencybased models construct a semantic space that encapsulates different lexical relations.", "labels": [], "entities": []}, {"text": "More specifically, we will assess whether word pairs capturing different types of semantic relations (e.g., hyponymy, synonymy) can be distinguished in terms of their distances in the semantic space.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Size of direct and mediated priming effects", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9701537489891052}]}, {"text": " Table 3: Mean skew divergences and Tukey test re- sults for model 7", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9549254179000854}, {"text": "Tukey test re-", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.7902676239609718}]}]}