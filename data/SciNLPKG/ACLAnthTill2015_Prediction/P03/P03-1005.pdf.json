{"title": [{"text": "Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data", "labels": [], "entities": [{"text": "Hierarchical Directed Acyclic Graph Kernel", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.6251512825489044}]}], "abstractContent": [{"text": "This paper proposes the \"Hierarchical Directed Acyclic Graph (HDAG) Kernel\" for structured natural language data.", "labels": [], "entities": []}, {"text": "The HDAG Kernel directly accepts several levels of both chunks and their relations, and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs.", "labels": [], "entities": []}, {"text": "We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function.", "labels": [], "entities": [{"text": "question classification", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7687466144561768}, {"text": "sentence alignment tasks", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.7592375775178274}]}, {"text": "The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods.", "labels": [], "entities": [{"text": "HDAG Kernel", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.7395888268947601}]}], "introductionContent": [{"text": "As it has become easy to get structured corpora such as annotated texts, many researchers have applied statistical and machine learning techniques to NLP tasks, thus the accuracies of basic NLP tools, such as POS taggers, NP chunkers, named entities taggers and dependency analyzers, have been improved to the point that they can realize practical applications in NLP.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 209, "end_pos": 220, "type": "TASK", "confidence": 0.6220547407865524}]}, {"text": "The motivation of this paper is to identify and use richer information within texts that will improve the performance of NLP applications; this is in contrast to using feature vectors constructed by a bagof-words (.", "labels": [], "entities": []}, {"text": "We now are focusing on the methods that use numerical feature vectors to represent the features of natural language data.", "labels": [], "entities": []}, {"text": "In this case, since the original natural language data is symbolic, researchers convert the symbolic data into numeric data.", "labels": [], "entities": []}, {"text": "This process, feature extraction, is ad-hoc in nature and differs with each NLP task; there has been no neat formulation for generating feature vectors from the semantic and grammatical structures inside texts.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7588278651237488}]}, {"text": "Kernel methods) suitable for NLP have recently been devised.", "labels": [], "entities": [{"text": "NLP", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8859024047851562}]}, {"text": "Convolution Kernels) demonstrate how to build kernels over discrete structures such as strings, trees, and graphs.", "labels": [], "entities": []}, {"text": "One of the most remarkable properties of this kernel methodology is that it retains the original representation of objects and algorithms manipulate the objects simply by computing kernel functions from the inner products between pairs of objects.", "labels": [], "entities": []}, {"text": "This means that we do not have to map texts to the feature vectors by explicitly representing them, as long as an efficient calculation for the inner products between a pair of texts is defined.", "labels": [], "entities": []}, {"text": "The kernel method is widely adopted in Machine Learning methods, such as the Support Vector Machine (SVM).", "labels": [], "entities": []}, {"text": "In addition, kernel function . The Tree Kernel) and String Subsequence Kernel (SSK) (), developed in the NLP field, are examples of Convolution Kernels instances.", "labels": [], "entities": []}, {"text": "An explicit definition of both the Tree Kernel and SSK weighted according to the length of the subsequence.", "labels": [], "entities": []}, {"text": "These two kernels make polynomialtime calculations, based on efficient recursive calculation, possible, see equation (1).", "labels": [], "entities": []}, {"text": "Our proposed method uses the framework of Convolution Kernels.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the performance of the proposed method in an actual application of NLP; the data set is written in Japanese.", "labels": [], "entities": []}, {"text": "We compared HDAG and DAG (the latter had no hierarchy structure) to the String Subsequence Kernel (SSK) for word sequence, Dependency Structure  We expanded SSK and DSK to improve the total performance of the experiments.", "labels": [], "entities": []}, {"text": "We denote them as SSK' and DSK' respectively.", "labels": [], "entities": [{"text": "DSK", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.751448929309845}]}, {"text": "The original SSK treats only exact \u009c string combinations based on parameter \u009c . We consider string combinations of up to \u009c for SSK'.", "labels": [], "entities": []}, {"text": "The original DSK was specifically constructed for parse tree use.", "labels": [], "entities": []}, {"text": "We expanded it to be able to treat the \u009c combinations of nodes and the free order of child node matching.", "labels": [], "entities": []}, {"text": "shows some input objects for each evaluated kernel, (a) for HDAG, (b) for DAG and DSK', and (c) for SSK'.", "labels": [], "entities": [{"text": "DSK", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8566567897796631}]}, {"text": "Note, though DAG and DSK' treat the same input objects, their kernel calculation methods differ as do the return values.", "labels": [], "entities": [{"text": "DSK", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.8763296604156494}]}, {"text": "We used the words and semantic information of \"Goi-taikei\" (, which is similar to WordNet in English, as the attributes of the node.", "labels": [], "entities": []}, {"text": "The chunks and their relations in the texts were analyzed by cabocha (), and named entities were analyzed by the method of ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Similarity values of", "labels": [], "entities": [{"text": "Similarity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9209445118904114}]}, {"text": " Table 3: Results of the performance as a similarity  measure for question classification", "labels": [], "entities": [{"text": "question classification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.892532467842102}]}, {"text": " Table 4: Results of the performance as a similarity  measure for sentence alignment", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7654695808887482}]}, {"text": " Table 5: Results of question classification by SVM  with comparison kernel functions", "labels": [], "entities": [{"text": "question classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8785769939422607}]}]}