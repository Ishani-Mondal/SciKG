{"title": [{"text": "Feedback Cleaning of Machine Translation Rules Using Automatic Evaluation", "labels": [], "entities": [{"text": "Feedback Cleaning of Machine Translation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8283990025520325}]}], "abstractContent": [{"text": "When rules of transfer-based machine translation (MT) are automatically acquired from bilingual corpora, incor-rect/redundant rules are generated due to acquisition errors or translation variety in the corpora.", "labels": [], "entities": [{"text": "transfer-based machine translation (MT)", "start_pos": 14, "end_pos": 53, "type": "TASK", "confidence": 0.8220682541529337}]}, {"text": "As anew countermeasure to this problem, we propose a feedback cleaning method using automatic evaluation of MT quality, which removes incor-rect/redundant rules as away to increase the evaluation score.", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.9538660645484924}]}, {"text": "BLEU is utilized for the automatic evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9830347299575806}]}, {"text": "The hill-climbing algorithm, which involves features of this task, is applied to searching for the optimal combination of rules.", "labels": [], "entities": []}, {"text": "Our experiments show that the MT quality improves by 10% in test sentences according to a subjective evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9764827489852905}]}, {"text": "This is considerable improvement over previous methods .", "labels": [], "entities": []}], "introductionContent": [{"text": "Along with the efforts made in accumulating bilingual corpora for many language pairs, quite a few machine translation (MT) systems that automatically acquire their knowledge from corpora have been proposed.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.8505409359931946}]}, {"text": "However, knowledge for transferbased MT acquired from corpora contains many incorrect/redundant rules due to acquisition errors or translation variety in the corpora.", "labels": [], "entities": [{"text": "transferbased MT acquired", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7774927417437235}]}, {"text": "Such rules conflict with other existing rules and cause implausible MT results or increase ambiguity.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9910981059074402}]}, {"text": "If incorrect rules could be avoided, MT quality would necessarily improve.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9729273915290833}]}, {"text": "There are two approaches to overcoming incorrect/redundant rules: \u2022 Selecting appropriate rules in a disambiguation process during the translation (on-line processing, ().", "labels": [], "entities": []}, {"text": "\u2022 Cleaning incorrect/redundant rules after automatic acquisition (off-line processing,).", "labels": [], "entities": []}, {"text": "We employ the second approach in this paper.", "labels": [], "entities": []}, {"text": "The cutoff by frequency) and the hypothesis test) have been applied to clean the rules.", "labels": [], "entities": []}, {"text": "The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9924610257148743}]}, {"text": "The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident.", "labels": [], "entities": []}, {"text": "Another current topic of machine translation is automatic evaluation of MT quality).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8347809910774231}, {"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.975721001625061}]}, {"text": "These methods aim to replace subjective evaluation in order to speedup the development cycle of MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9929633736610413}]}, {"text": "However, they can be utilized not only as developers' aids but also for automatic tuning of MT systems (.", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.932545006275177}]}, {"text": "We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method).", "labels": [], "entities": [{"text": "feedback cleaning", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7874617874622345}]}, {"text": "Our method evaluates the contribution of each rule to the MT results and removes inappropriate rules as away to increase the evaluation scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9882551431655884}]}, {"text": "Since the automatic evaluation correlates with a subjective evaluation, MT quality will improve after cleaning.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9807748794555664}]}, {"text": "Our method only evaluates MT results and does not consider various conditions of the MT engine, such as parameters, interference in dictionaries, disambiguation methods, and soon.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9785543084144592}, {"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9055836796760559}]}, {"text": "Even if an MT engine avoids incorrect/redundant rules by on-line processing, errors inevitably remain.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9776401519775391}]}, {"text": "Our method cleans the rules in advance by only focusing on the remaining errors.", "labels": [], "entities": []}, {"text": "Thus, our method complements on-line processing and adapts translation rules to the given conditions of the MT engine.", "labels": [], "entities": [{"text": "MT engine", "start_pos": 108, "end_pos": 117, "type": "TASK", "confidence": 0.8984919786453247}]}], "datasetContent": [{"text": "We utilize BLEU () for the automatic evaluation of MT quality in this paper.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9990299940109253}, {"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9906920790672302}]}, {"text": "BLEU measures the similarity between MT results and translation results made by humans (called Rule No. Syn. Cat.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9885576963424683}, {"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9937736988067627}, {"text": "Rule No. Syn. Cat.", "start_pos": 95, "end_pos": 113, "type": "DATASET", "confidence": 0.7258574018875757}]}, {"text": "Source Pattern VP Note that a sizeable set of MT results is necessary in order to calculate an accurate BLEU score.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9042402505874634}, {"text": "accurate", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9447802901268005}, {"text": "BLEU score", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.907051146030426}]}, {"text": "Although it is possible to calculate the BLEU score of a single MT result, it contains errors from the subjective evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9848596453666687}, {"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9559741020202637}]}, {"text": "BLEU cancels out individual errors by summing the similarities of MT results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9761028289794922}, {"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9734749794006348}]}, {"text": "Therefore, we need all of the MT results from the evaluation corpus in order to calculate an accurate BLEU score.", "labels": [], "entities": [{"text": "MT", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.9021564722061157}, {"text": "BLEU score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.946781188249588}]}, {"text": "One feature of BLEU is its use of multiple references fora single source sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.7914949655532837}]}, {"text": "However, one reference per sentence is used in this paper because an already existing bilingual corpus is applied to the cleaning.", "labels": [], "entities": []}, {"text": "In this section, the effects of feedback cleaning are evaluated by using English-to-Japanese translation.", "labels": [], "entities": [{"text": "feedback cleaning", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7259487807750702}]}, {"text": "Bilingual Corpora The corpus used in the following experiments is the Basic Travel Expression Corpus ().", "labels": [], "entities": [{"text": "Basic Travel Expression Corpus", "start_pos": 70, "end_pos": 100, "type": "DATASET", "confidence": 0.5327242165803909}]}, {"text": "This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists.", "labels": [], "entities": []}, {"text": "We divided it into sub-corpora for training, evaluation, and test as shown in.", "labels": [], "entities": []}, {"text": "The number of rules acquired from the training corpus (the base rule set size) was 105,588.", "labels": [], "entities": []}, {"text": "We used the following two methods to evaluate MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9946275353431702}]}, {"text": "In order to observe the characteristics of feedback cleaning, cleaning of the base rule set was carried out by using the evaluation corpus.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "This graph shows changes in the test corpus BLEU score, the evaluation corpus BLEU score, and the number of rules along with the number of iterations.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9726352393627167}, {"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9514783322811127}]}, {"text": "Consequently, the removed rules converged at nine iterations, and 6,220 rules were removed.", "labels": [], "entities": []}, {"text": "The evaluation corpus BLEU score was improved by increasing the number of iterations, demonstrating that the combinatorial optimization by the hill-climbing algorithm worked effectively.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9769174754619598}]}, {"text": "The test corpus BLEU score reached a peak score of 0.245 at the second iteration and slightly decreased after the third iteration due to overfitting.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9802537262439728}]}, {"text": "However, the final score was 0.244, which is almost the same as the peak score.", "labels": [], "entities": []}, {"text": "The test corpus BLEU score was lower than the evaluation corpus BLEU score because the rules used in the test corpus were not exhaustively checked by the evaluation corpus.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.980804830789566}, {"text": "BLEU score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9722778499126434}]}, {"text": "If the evaluation corpus size could be expanded, the test corpus score would improve.", "labels": [], "entities": []}, {"text": "About 37,000 sentences were translated on average in each iteration.", "labels": [], "entities": []}, {"text": "This means that the time for an iteration is estimated at about ten hours if translation speed is one second per sentence.", "labels": [], "entities": []}, {"text": "This is a short enough time for us because our method does not require real-time processing.", "labels": [], "entities": []}, {"text": "The idea of feedback cleaning is independent of BLEU.", "labels": [], "entities": [{"text": "feedback cleaning", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7796763479709625}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9872884750366211}]}, {"text": "Some automatic evaluation methods of MT quality other than BLEU have been proposed.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9922582507133484}, {"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.997820258140564}]}, {"text": "For example,,, and measure similarity between MT results and the references by DP matching (edit distances) and then output the evaluation scores.", "labels": [], "entities": [{"text": "similarity", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9833950996398926}, {"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9460501670837402}]}, {"text": "These automatic evaluation methods that output scores are applicable to feedback cleaning.", "labels": [], "entities": [{"text": "feedback cleaning", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8790284097194672}]}, {"text": "The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9951540231704712}, {"text": "MT", "start_pos": 161, "end_pos": 163, "type": "TASK", "confidence": 0.9437146782875061}]}, {"text": "Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sentence translations is an important technique for all of these methods.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9644039273262024}]}, {"text": "The effects of feedback cleaning depend on the characteristics of objective measures.", "labels": [], "entities": [{"text": "feedback cleaning", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.736081063747406}]}, {"text": "DP-based measures and BLEU have different characteristics (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9974821209907532}]}, {"text": "The exploration of several measures for feedback cleaning remains an interesting future work.", "labels": [], "entities": [{"text": "feedback cleaning", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.8608331978321075}]}], "tableCaptions": []}