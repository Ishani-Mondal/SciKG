{"title": [{"text": "Unsupervised Learning of Dependency Structure for Language Modeling", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7179231643676758}]}], "abstractContent": [{"text": "This paper presents a dependency language model (DLM) that captures linguistic constraints via a dependency structure, i.e., a set of probabilistic dependencies that express the relations between headwords of each phrase in a sentence by an acyclic, planar, undirected graph.", "labels": [], "entities": []}, {"text": "First, we incorporate the dependency structure into an n-gram language model to capture long distance word dependency.", "labels": [], "entities": []}, {"text": "Second, we present an unsuper-vised learning method that discovers the dependency structure of a sentence using a bootstrapping procedure.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the proposed models on a realistic application (Japanese Kana-Kanji conversion).", "labels": [], "entities": [{"text": "Kana-Kanji conversion", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.6156411916017532}]}, {"text": "Experiments show that the best DLM achieves an 11.3% error rate reduction over the word trigram model.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 53, "end_pos": 73, "type": "METRIC", "confidence": 0.9736654361089071}]}, {"text": "1 Introduction In recent years, many efforts have been made to utilize linguistic structure in language modeling, which for practical reasons is still dominated by trigram-based language models.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7057712376117706}]}, {"text": "There are two major obstacles to successfully incorporating linguistic structure into a language model: (1) capturing longer distance word dependencies leads to higher-order n-gram models, where the number of parameters is usually too large to estimate; (2) capturing deeper linguistic relations in a language model requires a large annotated training corpus and a decoder that assigns linguistic structure, which are not always available.", "labels": [], "entities": []}, {"text": "This paper presents anew dependency language model (DLM) that captures long distance linguistic constraints between words via a dependency structure, i.e., a set of probabilistic dependencies that capture linguistic relations between headwords of each phrase in a sentence.", "labels": [], "entities": []}, {"text": "To deal with the first obstacle mentioned above, we approximate long-distance linguistic dependency by a model that is similar to a skipping bigram model in which the prediction of a word is conditioned on exactly one other linguistically related word that lies arbitrarily far in the past.", "labels": [], "entities": []}, {"text": "This dependency model is then interpolated with a headword bigram model and a word trigram model, keeping the number of parameters of the combined model manageable.", "labels": [], "entities": []}, {"text": "To overcome the second obstacle, we used an unsu-pervised learning method that discovers the dependency structure of a given sentence using an Expectation-Maximization (EM)-like procedure.", "labels": [], "entities": []}, {"text": "In this method, no manual syntactic annotation is required, thereby opening up the possibility for building a language model that performs well on a wide variety of data and languages.", "labels": [], "entities": []}, {"text": "The proposed model is evaluated using Japanese Kana-Kanji conversion, achieving significant error rate reduction over the word trigram model.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.9589101473490397}]}, {"text": "2 Motivation A trigram language model predicts the next word based only on two preceding words, blindly discarding any other relevant word that may lie three or more positions to the left.", "labels": [], "entities": []}, {"text": "Such a model is likely to be linguistically implausible: consider the Eng-lish sentence in Figure 1(a), where a trigram model would predict cried from next seat, which does not agree with our intuition.", "labels": [], "entities": []}, {"text": "In this paper, we define a dependency structure of a sentence as a set of probabilistic dependencies that express linguistic relations between words in a sentence by an acyclic, planar graph, where two related words are connected by an undirected graph edge (i.e., we do not differentiate the modifier and the head in a de", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, many efforts have been made to utilize linguistic structure in language modeling, which for practical reasons is still dominated by trigram-based language models.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.716102734208107}]}, {"text": "There are two major obstacles to successfully incorporating linguistic structure into a language model: (1) capturing longer distance word dependencies leads to higher-order n-gram models, where the number of parameters is usually too large to estimate; (2) capturing deeper linguistic relations in a language model requires a large annotated training corpus and a decoder that assigns linguistic structure, which are not always available.", "labels": [], "entities": []}, {"text": "This paper presents anew dependency language model (DLM) that captures long distance linguistic constraints between words via a dependency structure, i.e., a set of probabilistic dependencies that capture linguistic relations between headwords of each phrase in a sentence.", "labels": [], "entities": []}, {"text": "To deal with the first obstacle mentioned above, we approximate long-distance linguistic dependency by a model that is similar to a skipping bigram model in which the prediction of a word is conditioned on exactly one other linguistically related word that lies arbitrarily far in the past.", "labels": [], "entities": []}, {"text": "This dependency model is then interpolated with a headword bigram model and a word trigram model, keeping the number of parameters of the combined model manageable.", "labels": [], "entities": []}, {"text": "To overcome the second obstacle, we used an unsupervised learning method that discovers the dependency structure of a given sentence using an Expectation-Maximization (EM)-like procedure.", "labels": [], "entities": []}, {"text": "In this method, no manual syntactic annotation is required, thereby opening up the possibility for building a language model that performs well on a wide variety of data and languages.", "labels": [], "entities": []}, {"text": "The proposed model is evaluated using Japanese Kana-Kanji conversion, achieving significant error rate reduction over the word trigram model.", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 92, "end_pos": 112, "type": "METRIC", "confidence": 0.9589101473490397}]}], "datasetContent": [{"text": "In this study, we evaluated language models on the application of Japanese Kana-Kanji conversion, which is the standard method of inputting Japanese text by converting the text of a syllabary-based Kana string into the appropriate combination of Kanji and Kana.", "labels": [], "entities": [{"text": "Kana-Kanji conversion", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.6660468578338623}]}, {"text": "This is a similar problem to speech recognition, except that it does not include acoustic ambiguity.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8104086518287659}]}, {"text": "Performance on this task is measured in terms of the character error rate (CER), given by the number of characters wrongly converted from the phonetic string divided by the number of characters in the correct transcript.", "labels": [], "entities": [{"text": "character error rate (CER)", "start_pos": 53, "end_pos": 79, "type": "METRIC", "confidence": 0.8512457311153412}]}, {"text": "For our experiments, we used two newspaper corpora, Nikkei and Yomiuri Newspapers, both of which have been pre-word-segmented.", "labels": [], "entities": [{"text": "Nikkei", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9803751111030579}, {"text": "Yomiuri Newspapers", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.8733049035072327}]}, {"text": "We built language models from a 36-million-word subset of the Nikkei Newspaper corpus, performed parameter optimization on a 100,000-word subset of the Yomiuri Newspaper (held-out data), and tested our models on another 100,000-word subset of the Yomiuri Newspaper corpus.", "labels": [], "entities": [{"text": "Nikkei Newspaper corpus", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.985086977481842}, {"text": "Yomiuri Newspaper", "start_pos": 152, "end_pos": 169, "type": "DATASET", "confidence": 0.9621727466583252}, {"text": "Yomiuri Newspaper corpus", "start_pos": 247, "end_pos": 271, "type": "DATASET", "confidence": 0.9548975427945455}]}, {"text": "The lexicon we used contains 167,107 entries.", "labels": [], "entities": []}, {"text": "Our evaluation was done within a framework of so-called \"N-best rescoring\" method, in which a list of hypotheses is generated by the baseline language model (a word trigram model in this study), which is then rescored using a more sophisticated language model.", "labels": [], "entities": []}, {"text": "We use the N-best list of N=100,\ud97b\udf59 whose \"oracle\" CER (i.e., the CER of the hypotheses with the minimum number of errors) is presented in, indicating the upper bound on performance.", "labels": [], "entities": [{"text": "CER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.8978811502456665}, {"text": "CER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9809433817863464}]}, {"text": "We also note in that the performance of the conversion using the baseline trigram model is much better than the state-of-the-art performance currently available in the marketplace, presumably due to the large amount of training data we used, and to the similarity between the training and the test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Comparison of CER results", "labels": [], "entities": []}]}