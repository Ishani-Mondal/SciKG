{"title": [{"text": "High-precision Identification of Discourse New and Unique Noun Phrases", "labels": [], "entities": [{"text": "Identification of Discourse New and Unique Noun Phrases", "start_pos": 15, "end_pos": 70, "type": "TASK", "confidence": 0.7933128103613853}]}], "abstractContent": [{"text": "Coreference resolution systems usually attempt to find a suitable antecedent for (al-most) every noun phrase.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8923743069171906}]}, {"text": "Recent studies, however, show that many definite NPs are not anaphoric.", "labels": [], "entities": []}, {"text": "The same claim, obviously, holds for the indefinites as well.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most coreference resolution systems proceed in the following way: they first identify all the possible markables (for example, noun phrases) and then check one by one candidate pairs , trying to find out whether the members of those pairs can be coreferent.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.9565122425556183}]}, {"text": "As the final step, the pairs are ranked using a scoring algorithm in order to find an appropriate partition of all the markables into coreference classes.", "labels": [], "entities": []}, {"text": "Those approaches require substantial processing: in the worst case one has to check is the total number of markables found by the system.", "labels": [], "entities": []}, {"text": "However, R. Vieira and M. Poesio have recently shown in) that such an exhaustive search is not needed, because many noun phrases are not anaphoric at all -about PR Q T S of definite NPs in their corpus have no prior referents.", "labels": [], "entities": []}, {"text": "Obviously, this number is even higher if one takes into account all the other types of NPs -for example, indefinites are almost always non-anaphoric.", "labels": [], "entities": []}, {"text": "We can conclude that a coreference resolution engine might benefit a lot from a pre-filtering algorithm for identifying non-anaphoric entities.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.950938493013382}]}, {"text": "First, we save much processing time by discarding at least half of the markables.", "labels": [], "entities": []}, {"text": "Second, we can hope to reduce the number of mistakes: without pre-filtering, our coreference resolution system might misclassify a discourse new entity as coreferent to some previous one.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.81752148270607}]}, {"text": "However, such a pre-filtering can also decrease the system's performance if too many anaphoric NPs are classified as discourse new: as those NPs are not processed by the main coreference resolution module at all, we cannot find correct antecedents for them.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.7890994250774384}]}, {"text": "Therefore, we are interested in an algorithm with a good precision, possibly sacrificing its recall to a reasonable extent.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9978523254394531}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9990847110748291}]}, {"text": "V. Ng and C. Cardie analysed in) the impact of such a prefiltering on their coreference resolution engine.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.9413934350013733}]}, {"text": "It turned out that an automatically induced increased and the recall decreased), the prefiltering resulted in improving the coreference resolution.", "labels": [], "entities": [{"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.999267041683197}, {"text": "coreference resolution", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.8217410743236542}]}, {"text": "Several algorithms for identifying discourse new entities have been proposed in the literature.", "labels": [], "entities": []}, {"text": "R. Vieira and M. Poesio use hand-crafted heuristics, encoding syntactic information.", "labels": [], "entities": []}, {"text": "For example, the noun phrase \"the inequities of the current land-ownership system\" is classified by their system as , because it contains the restrictive postmodification \"of the current landownership system\".", "labels": [], "entities": []}, {"text": "This approach leads to 72% precision and 69% recall for definite discourse new NPs.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996664524078369}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9996004700660706}]}, {"text": "The system described in) also makes use of syntactic heuristics.", "labels": [], "entities": []}, {"text": "But in addition the authors mine discourse new entities from the corpus.", "labels": [], "entities": []}, {"text": "Four types of entities can be classified as non-anaphoric: 1.", "labels": [], "entities": []}, {"text": "having specific syntactic structure, 2.", "labels": [], "entities": []}, {"text": "appearing in the first sentence of some text in the training corpus, 3.", "labels": [], "entities": []}, {"text": "exhibiting the same pattern as several expressions of type (2), 4.", "labels": [], "entities": []}, {"text": "appearing in the corpus at least 5 times and always with the definite article (\"definitesonly\").", "labels": [], "entities": []}, {"text": "Using various combinations of these methods, D. Bean and E. Riloff achieved an accuracy for definite non-anaphoric NPs of about , with various combinations of precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995104074478149}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9996134638786316}, {"text": "recall", "start_pos": 173, "end_pos": 179, "type": "METRIC", "confidence": 0.9985711574554443}]}, {"text": "1 This algorithm, however, has two limitations.", "labels": [], "entities": []}, {"text": "First, one needs a corpus consisting of many small texts.", "labels": [], "entities": []}, {"text": "Otherwise it is impossible to find enough non-anaphoric entities of type (2) and, hence, to collect enough patterns for the entities of type (3).", "labels": [], "entities": []}, {"text": "Second, for an entity to be recognized as \"definite-only\", it should be found in the corpus at least 5 times.", "labels": [], "entities": []}, {"text": "This automatically results in the data sparseness problem, excluding many infrequent nouns and NPs.", "labels": [], "entities": []}, {"text": "In our approach we use machine learning to identify non-anaphoric noun-phrases.", "labels": [], "entities": []}, {"text": "We combine syntactic heuristics with the \"definite probability\".", "labels": [], "entities": []}, {"text": "Unlike Bean and Riloff, we model definite probability using the Internet instead of the training corpus itself.", "labels": [], "entities": []}, {"text": "This helps us to overcome the data sparseness problem to a large extent.", "labels": [], "entities": []}, {"text": "As it has been shown recently in (), Internet counts produce reliable data for linguistic analysis, correlating well with corpus counts and plausibility judgements.", "labels": [], "entities": [{"text": "linguistic analysis", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7345566153526306}]}, {"text": "The rest of the paper is organised as follows: first we discuss our NPs classification.", "labels": [], "entities": [{"text": "NPs classification", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7471840083599091}]}, {"text": "In Section 3, we describe briefly various data sources we used.", "labels": [], "entities": []}, {"text": "Section 4 provides an explanation of our learning strategy and evaluation results.", "labels": [], "entities": []}, {"text": "The approach is summarised in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision, Recall, and F-score for the", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994431138038635}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9945328235626221}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9996060729026794}]}, {"text": " Table 2: Precision, Recall, and F-score for the", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994491934776306}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9952889680862427}, {"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.999602735042572}]}]}