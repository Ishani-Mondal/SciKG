{"title": [{"text": "Orthogonal Negation in Vector Spaces for Modelling Word-Meanings and Document Retrieval", "labels": [], "entities": [{"text": "Document Retrieval", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6837409138679504}]}], "abstractContent": [{"text": "Standard IR systems can process queries such as \"web NOT internet\", enabling users who are interested in arachnids to avoid documents about computing.", "labels": [], "entities": []}, {"text": "The documents retrieved for such a query should be irrelevant to the negated query term.", "labels": [], "entities": []}, {"text": "Most systems implement this by reprocessing results after retrieval to remove documents containing the unwanted string of letters.", "labels": [], "entities": []}, {"text": "This paper describes and evaluates a theoretically motivated method for removing unwanted meanings directly from the original query in vector models, with the same vector negation operator as used in quantum logic.", "labels": [], "entities": []}, {"text": "Irrelevance in vector spaces is modelled using orthogonality, so query vectors are made orthogonal to the negated term or terms.", "labels": [], "entities": []}, {"text": "As well as removing unwanted terms, this form of vector negation reduces the occurrence of synonyms and neighbours of the negated terms by as much as 76% compared with standard Boolean methods.", "labels": [], "entities": []}, {"text": "By altering the query vector itself, vector negation removes not only unwanted strings but un-wanted meanings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector spaces enjoy widespread use in information retrieval (; Baeza-Yates and * This research was supported in part by the Research Collaboration between the NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University, and by EC/NSF grant IST-1999-11438 for the MUCHMORE project.), and from this original application vector models have been applied to semantic tasks such as word-sense acquisition and disambiguation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7829309403896332}, {"text": "NTT Communication Science Laboratories", "start_pos": 159, "end_pos": 197, "type": "DATASET", "confidence": 0.8602429181337357}, {"text": "word-sense acquisition", "start_pos": 429, "end_pos": 451, "type": "TASK", "confidence": 0.7359936535358429}]}, {"text": "One benefit of these models is that the similarity between pairs of terms or between queries and documents is a continuous function, automatically ranking results rather than giving just a YES/NO judgment.", "labels": [], "entities": [{"text": "YES/NO judgment", "start_pos": 189, "end_pos": 204, "type": "METRIC", "confidence": 0.8274916857481003}]}, {"text": "In addition, vector models can be freely built from unlabelled text and so are both entirely unsupervised, and an accurate reflection of the way words are used in practice.", "labels": [], "entities": []}, {"text": "In vector models, terms are usually combined to form more complicated query statements by (weighted) vector addition.", "labels": [], "entities": []}, {"text": "Because vector addition is commutative, terms are combined in a \"bag of words\" fashion.", "labels": [], "entities": [{"text": "vector addition", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7098819464445114}]}, {"text": "While this has proved to be effective, it certainly leaves room for improvement: any genuine natural language understanding of query statements cannot rely solely on commutative addition for building more complicated expressions out of primitives.", "labels": [], "entities": []}, {"text": "Other algebraic systems such as Boolean logic and set theory have well-known operations for building composite expressions out of more basic ones.", "labels": [], "entities": []}, {"text": "Settheoretic models for the logical connectives 'AND', 'NOT' and 'OR' are completely understood by most researchers, and used by Boolean IR systems for assembling the results to complicated queries.", "labels": [], "entities": [{"text": "NOT", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.8144594430923462}, {"text": "OR", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.8480264544487}]}, {"text": "It is clearly desirable to develop a calculus which combines the flexible ranking of results in a vector model with the crisp efficiency of Boolean logic, a goal which has long been recognised (  and attempted mainly for conjunction and disjunction.", "labels": [], "entities": []}, {"text": "This paper proposes such a scheme for negation, based upon well-known linear algebra, and which also implies a vector form of disjunction.", "labels": [], "entities": [{"text": "negation", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.9779015779495239}]}, {"text": "It turns out that these vector connectives are precisely those used in quantum logic), a development which is discussed in much more detail in.", "labels": [], "entities": []}, {"text": "Because of its simplicity, our model is easy to understand and to implement.", "labels": [], "entities": []}, {"text": "Vector negation is based on the intuition that unrelated meanings should be orthogonal to one another, which is to say that they should have no features in common at all.", "labels": [], "entities": [{"text": "Vector negation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9151806235313416}]}, {"text": "Thus vector negation generates a 'meaning vector' which is completely orthogonal to the negated term.", "labels": [], "entities": []}, {"text": "Document retrieval experiments demonstrate that vector negation is not only effective at removing unwanted terms: it is also more effective than other methods at removing their synonyms and related terms.", "labels": [], "entities": [{"text": "Document retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7731437683105469}, {"text": "vector negation", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7398965358734131}]}, {"text": "This justifies the claim that, by producing a single query vector for \"a NOT b\", we remove not only unwanted strings but also unwanted meanings.", "labels": [], "entities": []}, {"text": "We describe the underlying motivation behind this model and define the vector negation and disjunction operations in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3 we review other ways negation is implemented in Information Retrieval, comparing and contrasting with vector negation.", "labels": [], "entities": [{"text": "negation", "start_pos": 34, "end_pos": 42, "type": "TASK", "confidence": 0.9681394696235657}, {"text": "Information Retrieval", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7639046609401703}]}, {"text": "In Section 4 we describe experiments demonstrating the benefits and drawbacks of vector negation compared with two other methods for negation.", "labels": [], "entities": [{"text": "vector negation", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.6361369788646698}, {"text": "negation", "start_pos": 133, "end_pos": 141, "type": "TASK", "confidence": 0.9694697856903076}]}], "datasetContent": [{"text": "word-senses Our first experiments with vector negation were to determine whether the negation operator could find different senses of ambiguous words by negating a word closely related to one of the meanings.", "labels": [], "entities": []}, {"text": "A vector space model was built using Latent Semantic Analysis, similar to the systems of.", "labels": [], "entities": []}, {"text": "The effect of LSA is to increase linear dependency between terms, and for this reason it is likely that LSA is a crucial step in our approach.", "labels": [], "entities": []}, {"text": "Terms were indexed depending on their co-occurrence with 1000 frequent \"content-bearing words\" in a 15 word context-window, giving each term 1000 coordinates.", "labels": [], "entities": []}, {"text": "This was reduced to 100 dimensions using singular value decomposition.", "labels": [], "entities": []}, {"text": "Later on, document vectors were assigned in the usual manner by summation of term vectors using tf-idf weighting (.", "labels": [], "entities": []}, {"text": "Vectors were normalised, so that the standard (Euclidean) scalar product and cosine similarity coincided.", "labels": [], "entities": []}, {"text": "This scalar product was used as a measure of term-term and term-document similarity throughout our experiments.", "labels": [], "entities": []}, {"text": "This method was used because it has been found to be effective at producing good term-term similarities for word-sense disambiguation and automatic lexical acquisition, and these similarities were used to generate interesting queries and to judge the effectiveness of different forms of negation.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.7497303187847137}, {"text": "automatic lexical acquisition", "start_pos": 138, "end_pos": 167, "type": "TASK", "confidence": 0.6869937380154928}]}, {"text": "More details on the building of this vector space model can be found in: First experiments with negation and wordsenses Two early results using negation to find senses of ambiguous words are given in, showing that vector negation is very effective for removing the 'legal' meaning from the word suit and the 'sporting' meaning from the wordplay, leaving respectively the 'clothing' and 'performance' meanings.", "labels": [], "entities": []}, {"text": "Note that removing a particular word also removes concepts related to the negated word.", "labels": [], "entities": []}, {"text": "This gives credence to the claim that our mathematical model is removing the meaning of a word, rather than just a string of characters.", "labels": [], "entities": []}, {"text": "This encouraged us to setup a larger scale experiment to test this hypothesis, which is described in Section 4.", "labels": [], "entities": []}, {"text": "This section describes experiments which compare the three methods of negation described above (postretrieval filtering, constant subtraction and vector negation) with the baseline alternative of no negation at all.", "labels": [], "entities": [{"text": "negation", "start_pos": 70, "end_pos": 78, "type": "TASK", "confidence": 0.966049313545227}]}, {"text": "The experiments were carried out using the vector space model described in Section 2.1.", "labels": [], "entities": []}, {"text": "To judge the effectiveness of different methods at removing unwanted meanings, with a large number of queries, we made the following assumptions.", "labels": [], "entities": []}, {"text": "A document which is relevant to the meaning of 'term a NOT term b' should contain as many references to term a and as few references to term b as possible.", "labels": [], "entities": []}, {"text": "Close neighbours and synonyms of term bare undesirable as well, since if they occur the document in question is likely to be related to the negated term even if the negated term itself does not appear.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: First experiments with negation and word- senses", "labels": [], "entities": [{"text": "word- senses", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.6624998648961385}]}, {"text": " Table 2: Table of results showing the percentage frequency of different terms in retrieved documents", "labels": [], "entities": []}]}