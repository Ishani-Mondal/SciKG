{"title": [{"text": "Self-Organizing Markov Models and Their Application to Part-of-Speech Tagging", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7045350074768066}]}], "abstractContent": [{"text": "This paper presents a method to develop a class of variable memory Markov models that have higher memory capacity than traditional (uniform memory) Markov models.", "labels": [], "entities": []}, {"text": "The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm.", "labels": [], "entities": []}, {"text": "A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task.", "labels": [], "entities": [{"text": "part-of-speech tagging task", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.7418258984883627}]}], "introductionContent": [{"text": "Many major NLP tasks can be regarded as problems of finding an optimal valuation for random processes.", "labels": [], "entities": []}, {"text": "For example, fora given word sequence, part-of-speech (POS) tagging involves finding an optimal sequence of syntactic classes, and NP chunking involves finding IOB tag sequences (each of which represents the inside, outside and beginning of noun phrases respectively).", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7061084628105163}, {"text": "NP chunking", "start_pos": 131, "end_pos": 142, "type": "TASK", "confidence": 0.7624551355838776}]}, {"text": "Many machine learning techniques have been developed to tackle such random process tasks, which include Hidden Markov Models (HMMs), Maximum Entropy Models (MEs), Support Vector Machines (SVMs), etc.", "labels": [], "entities": []}, {"text": "Among them, SVMs have high memory capacity and show high performance, especially when the target classification requires the consideration of various features.", "labels": [], "entities": []}, {"text": "On the other hand, HMMs have low memory capacity but they work very well, especially when the target task involves a series of classifications that are tightly related to each other and requires global optimization of them.", "labels": [], "entities": []}, {"text": "As for POS tagging, recent comparisons show that HMMs work better than other models when they are combined with good smoothing techniques and with handling of unknown words.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.9174715876579285}]}, {"text": "While global optimization is the strong point of HMMs, developers often complain that it is difficult to make HMMs incorporate various features and to improve them beyond given performances.", "labels": [], "entities": [{"text": "global optimization", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.7635389864444733}]}, {"text": "For example, we often find that in some cases a certain lexical context can improve the performance of an HMM-based POS tagger, but incorporating such additional features is not easy and it may even degrade the overall performance.", "labels": [], "entities": [{"text": "HMM-based POS tagger", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.5036680499712626}]}, {"text": "Because Markov models have the structure of tightly coupled states, an arbitrary change without elaborate consideration can spoil the overall structure.", "labels": [], "entities": []}, {"text": "This paper presents away of utilizing statistical decision trees to systematically raise the memory capacity of Markov models and effectively to make Markov models be able to accommodate various features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed a series of experiments to compare the performance of self-organizing Markov models with traditional Markov models.", "labels": [], "entities": []}, {"text": "Wall Street Journal as contained in Penn Treebank II is used as the reference material.", "labels": [], "entities": [{"text": "Wall Street Journal as contained in Penn Treebank II", "start_pos": 0, "end_pos": 52, "type": "DATASET", "confidence": 0.9528265727890862}]}, {"text": "As the experimental task is partof-speech tagging, all other annotations like syntactic bracketing have been removed from the corpus.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.733124852180481}, {"text": "syntactic bracketing", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.7332737147808075}]}, {"text": "Every figure (digit) in the corpus has been changed into a special symbol.", "labels": [], "entities": []}, {"text": "From the whole corpus, every 10'th sentence from the first is selected into the test corpus, and the remaining ones constitute the training corpus.", "labels": [], "entities": []}, {"text": "shows some basic statistics of the corpora.", "labels": [], "entities": []}, {"text": "We implemented several tagging models based on equation (3).", "labels": [], "entities": []}, {"text": "For the tag language model, we used We used 95% of confidence level to extend context.", "labels": [], "entities": []}, {"text": "In other words, only when there are enough evidences for improvement at 95% of confidence level, a context is extended.", "labels": [], "entities": []}], "tableCaptions": []}