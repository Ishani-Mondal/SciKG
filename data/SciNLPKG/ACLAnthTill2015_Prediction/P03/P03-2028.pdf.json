{"title": [{"text": "Spoken Interactive ODQA System: SPIQA", "labels": [], "entities": [{"text": "Spoken Interactive ODQA", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6956853667894999}]}], "abstractContent": [{"text": "We have been investigating an interactive approach for Open-domain QA (ODQA) and have constructed a spoken interactive ODQA system, SPIQA.", "labels": [], "entities": [{"text": "Open-domain QA (ODQA)", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.5673892319202423}]}, {"text": "The system derives disambiguating queries (DQs) that draw out additional information.", "labels": [], "entities": []}, {"text": "To test the efficiency of additional information requested by the DQs, the system reconstructs the user's initial question by combining the addition information with question.", "labels": [], "entities": []}, {"text": "The combination is then used for answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9388731718063354}]}, {"text": "Experimental results revealed the potential of the generated DQs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open-domain QA (ODQA), which extracts answers from large text corpora, such as newspaper texts, has been intensively investigated in the Text REtrieval Conference (TREC).", "labels": [], "entities": [{"text": "Open-domain QA (ODQA)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7254646241664886}, {"text": "Text REtrieval Conference (TREC)", "start_pos": 137, "end_pos": 169, "type": "TASK", "confidence": 0.8030440310637156}]}, {"text": "ODQA systems return an actual answer in response to a question written in a natural language.", "labels": [], "entities": []}, {"text": "However, the information in the first question input by a user is not usually sufficient to yield the desired answer.", "labels": [], "entities": []}, {"text": "Interactions for collecting additional information to accomplish QA are needed.", "labels": [], "entities": [{"text": "QA", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.8077612519264221}]}, {"text": "To construct more precise and user-friendly ODQA systems, a speech interface is used for the interaction between human beings and machines.", "labels": [], "entities": []}, {"text": "Our goal is to construct a spoken interactive ODQA system that includes an automatic speech recognition (ASR) system and an ODQA system.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8022143721580506}]}, {"text": "To clarify the problems presented in building such a system, the QA systems constructed so far have been classified into a number of groups, depending on their target domains, interfaces, and interactions to draw out additional information from users to accomplish set tasks, as is shown in.", "labels": [], "entities": []}, {"text": "In this table, text and speech denote text input and speech input, respectively.", "labels": [], "entities": []}, {"text": "The term \"addition\" represents additional information queried by the QA systems.", "labels": [], "entities": []}, {"text": "This additional information is separate to that derived from the user's initial questions.", "labels": [], "entities": []}, {"text": "To construct spoken interactive ODQA systems, the following problems must be overcome: 1.", "labels": [], "entities": []}, {"text": "System queries for additional information to extract answers and effective interaction strategies using such queries cannot be prepared before the user inputs the question.", "labels": [], "entities": []}, {"text": "2. Recognition errors degrade the performance of QA systems.", "labels": [], "entities": [{"text": "Recognition", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9298436045646667}]}, {"text": "Some information indispensable for extracting answers is deleted or substituted with other words.", "labels": [], "entities": []}, {"text": "Our spoken interactive ODQA system, SPIQA, copes with the first problem by adopting disambiguating users' questions using system queries.", "labels": [], "entities": []}, {"text": "In addition, a speech summarization technique is applied to handle recognition errors.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7112419605255127}]}, {"text": "shows the components of our system, and the data that flows through it.", "labels": [], "entities": []}, {"text": "This system comprises an ASR system (SOLON), a screening filter that uses a summarization method, and ODQA engine (SAIQA) fora Japanese newspaper text corpus, a Deriving Disambiguating Queries (DDQ) module, and a Text-to-Speech Synthesis (TTS) engine (FinalFluet).: Components and data flow in SPIQA.", "labels": [], "entities": [{"text": "ODQA engine (SAIQA)", "start_pos": 102, "end_pos": 121, "type": "METRIC", "confidence": 0.8990823030471802}]}], "datasetContent": [{"text": "Questions consisting of 69 sentences read aloud by seven male speakers were transcribed by our ASR system.", "labels": [], "entities": [{"text": "ASR", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9424316883087158}]}, {"text": "The question transcriptions were processed with a screening filter and input into the ODQA engine.", "labels": [], "entities": []}, {"text": "Each question consisted of about 19 morphemes on average.", "labels": [], "entities": []}, {"text": "The sentences were grammatically correct, formally structured, and had enough information for the ODQA engine to extract the correct answers.", "labels": [], "entities": []}, {"text": "The mean word recognition accuracy obtained by the ASR system was 76%.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.6084328591823578}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9608278274536133}, {"text": "ASR", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9575241804122925}]}, {"text": "The DQs generated by the DDQ module were evaluated in comparison with manual disambiguation queries.", "labels": [], "entities": []}, {"text": "Although the questions read by the seven speakers had sufficient information to extract exact answers, some recognition errors resulted in a loss of information that was indispensable for obtaining the correct answers.", "labels": [], "entities": []}, {"text": "The manual DQs were made by five subjects based on a comparison of the original written questions and the transcription results given by the ASR system.", "labels": [], "entities": [{"text": "DQs", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9198432564735413}]}, {"text": "The automatic DQs were categorized into two classes: APPRO-PRIATE when they had the same meaning as at least one of the five manual DQs, and INAPPRO-PRIATE when there was no match.", "labels": [], "entities": [{"text": "APPRO-PRIATE", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.9332607984542847}, {"text": "INAPPRO-PRIATE", "start_pos": 141, "end_pos": 155, "type": "METRIC", "confidence": 0.9512475728988647}]}, {"text": "The QA performance in using recognized (REC) and screened questions (SCRN) were evaluated by MRR (Mean Reciprocal Rank) (http://trec.nist.gov/data/qa.html).", "labels": [], "entities": [{"text": "MRR", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.996421217918396}, {"text": "Mean Reciprocal Rank)", "start_pos": 98, "end_pos": 119, "type": "METRIC", "confidence": 0.8938323259353638}]}, {"text": "SCRN was compared with the transcribed question that just had recognition errors removed (DEL).", "labels": [], "entities": [{"text": "recognition errors removed (DEL)", "start_pos": 62, "end_pos": 94, "type": "METRIC", "confidence": 0.6935029625892639}]}, {"text": "In addition, the questions reconstructed manually by merging these questions and additional information requested the DQs generated by using SCRN, (DQ) were also evaluated.", "labels": [], "entities": []}, {"text": "The additional information was extracted from the original users' question without recognition errors.", "labels": [], "entities": []}, {"text": "In this study, adding information by using the DQs was performed only once.", "labels": [], "entities": []}, {"text": "shows the evaluation results in terms of the appropriateness of the DQs and the QA-system MRRs.", "labels": [], "entities": [{"text": "DQs", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.8840236067771912}, {"text": "QA-system MRRs", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.6860804557800293}]}, {"text": "The results indicate that roughly 50% of the DQs generated by the DDQ module based on the screened results were APPROPRIATE.", "labels": [], "entities": [{"text": "APPROPRIATE", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.7335431575775146}]}, {"text": "The MRR for manual transcription (TRS) with no recognition errors was 0.43.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9979110360145569}, {"text": "manual transcription (TRS)", "start_pos": 12, "end_pos": 38, "type": "METRIC", "confidence": 0.6335238575935364}]}, {"text": "In addition, we could improve the MRR from 0.25 (REC) to 0.28 (DQ) by using the DQs only once.", "labels": [], "entities": [{"text": "MRR", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8095396161079407}, {"text": "REC", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9715802073478699}]}, {"text": "Experimental results revealed the potential of the generated DQs in compensating for the degradation of the QA performance due to recognition errors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results of disambiguating  queries generated by the DDQ module.", "labels": [], "entities": []}]}