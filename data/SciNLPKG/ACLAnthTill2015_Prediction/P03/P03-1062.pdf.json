{"title": [{"text": "Learning to predict pitch accents and prosodic boundaries in Dutch", "labels": [], "entities": [{"text": "predict pitch accents and prosodic boundaries", "start_pos": 12, "end_pos": 57, "type": "TASK", "confidence": 0.6833813289801279}]}], "abstractContent": [{"text": "We train a decision tree inducer (CART) and a memory-based classifier (MBL) on predicting prosodic pitch accents and breaks in Dutch text, on the basis of shallow , easy-to-compute features.", "labels": [], "entities": [{"text": "predicting prosodic pitch accents and breaks in Dutch text", "start_pos": 79, "end_pos": 137, "type": "TASK", "confidence": 0.87689737478892}]}, {"text": "We train the algorithms on both tasks individually and on the two tasks simultaneously.", "labels": [], "entities": []}, {"text": "The parameters of both algorithms and the selection of features are optimized per task with iterative deepening, an efficient wrapper procedure that uses progressive sampling of training data.", "labels": [], "entities": []}, {"text": "Results show a consistent significant advantage of MBL over CART, and also indicate that task combination can be done at the cost of little generalization score loss.", "labels": [], "entities": [{"text": "MBL", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.751340925693512}, {"text": "CART", "start_pos": 60, "end_pos": 64, "type": "TASK", "confidence": 0.826739490032196}]}, {"text": "Tests on cross-validated data and on held-out data yield F-scores of MBL on accent placement of 84 and 87, respectively, and on breaks of 88 and 91, respectively.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9990071654319763}, {"text": "MBL", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.997891366481781}]}, {"text": "Accent placement is shown to outperform an informed baseline rule; reliably predicting breaks other than those already indicated by intra-sentential punctuation, however, appears to be more challenging.", "labels": [], "entities": [{"text": "Accent", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9373980164527893}]}], "introductionContent": [{"text": "Any text-to-speech (TTS) system that aims at producing understandable and natural-sounding output needs to have on-board methods for predicting prosody.", "labels": [], "entities": [{"text": "predicting prosody", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.9062636494636536}]}, {"text": "Most systems start with generating a prosodic representation at the linguistic or symbolic level, followed by the actual phonetic realization in terms of (primarily) pitch, pauses, and segmental durations.", "labels": [], "entities": []}, {"text": "The first step involves placing pitch accents and inserting prosodic boundaries at the right locations (and may involve tune choice as well).", "labels": [], "entities": [{"text": "tune choice", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.6971628665924072}]}, {"text": "Pitch accents correspond roughly to pitch movements that lend emphasis to certain words in an utterance.", "labels": [], "entities": []}, {"text": "Prosodic breaks are audible interruptions in the flow of speech, typically realized by a combination of a pause, a boundary-marking pitch movement, and lengthening of the phrase-final segments.", "labels": [], "entities": []}, {"text": "Errors at this level may impede the listener in the correct understanding of the spoken utterance ().", "labels": [], "entities": []}, {"text": "Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints.", "labels": [], "entities": [{"text": "Predicting prosody", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.885424941778183}]}, {"text": "However, producing all this information -using full parsing, including establishing semanto-syntactic relations, and full discourse analysis -is currently infeasible fora realtime system.", "labels": [], "entities": []}, {"text": "Resolving this dilemma has been the topic of several studies in pitch accent placement) and in prosodic boundary placement ().", "labels": [], "entities": [{"text": "pitch accent placement", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.5655214885870615}, {"text": "prosodic boundary placement", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.707867960135142}]}, {"text": "The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents.", "labels": [], "entities": []}, {"text": "Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an annotated corpus of newspaper text, and machine learning algorithms to produce classifiers for both tasks.", "labels": [], "entities": [{"text": "prosodic boundary placement", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.6454369326432546}]}, {"text": "We address two questions that have been left open thus far in previous work: 1.", "labels": [], "entities": []}, {"text": "Is there an advantage in inducing decision trees for both tasks, or is it better to not abstract from individual instances and use a memory-based k-nearest neighbour classifier?", "labels": [], "entities": []}, {"text": "2. Is there an advantage in inducing classifiers for both tasks individually, or can both tasks be learned together.", "labels": [], "entities": []}, {"text": "The first question deals with a key difference between standard decision tree induction and memorybased classification: how to deal with exceptional instances.", "labels": [], "entities": [{"text": "decision tree induction", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.6725936929384867}]}, {"text": "Decision trees, CART (Classification and Regression Tree) in particular (, have been among the first successful machine learning algorithms applied to predicting pitch accents and prosodic boundaries for TTS.", "labels": [], "entities": [{"text": "predicting pitch accents and prosodic boundaries", "start_pos": 151, "end_pos": 199, "type": "TASK", "confidence": 0.8173471788565317}, {"text": "TTS", "start_pos": 204, "end_pos": 207, "type": "TASK", "confidence": 0.9037001132965088}]}, {"text": "Decision tree induction finds, through heuristics, a minimallysized decision tree that is estimated to generalize well to unseen data.", "labels": [], "entities": [{"text": "Decision tree induction", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.831303338209788}]}, {"text": "Its minimality strategy makes the algorithm reluctant to remember individual outlier instances that would take long paths in the tree: typically, these are discarded.", "labels": [], "entities": []}, {"text": "This may work well when outliers do not reoccur, but as demonstrated by), exceptions do typically reoccur in language data.", "labels": [], "entities": []}, {"text": "Hence, machine learning algorithms that retain a memory trace of individual instances, like memory-based learning algorithms based on the k-nearest neighbour classifier, outperform decision tree or rule inducers precisely for this reason.", "labels": [], "entities": []}, {"text": "Comparing the performance of machine learning algorithms is not straightforward, and deserves careful methodological consideration.", "labels": [], "entities": []}, {"text": "For a fair comparison, both algorithms should be objectively and automatically optimized for the task to be learned.", "labels": [], "entities": []}, {"text": "This point is made by ), who show that, for tasks such as word-sense disambiguation and part-of-speech tagging, tuning algorithms in terms of feature selection and classifier parameters gives rise to significant improvements in performance.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7336842119693756}, {"text": "part-of-speech tagging", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7015668302774429}]}, {"text": "In this paper, therefore, we optimize both CART and MBL individually and per task, using a heuristic optimization method called iterative deepening.", "labels": [], "entities": []}, {"text": "The second issue, that of task combination, stems from the intuition that the two tasks have a lot in common.", "labels": [], "entities": [{"text": "task combination", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7111503183841705}]}, {"text": "For instance, reports that knowledge of the location of breaks facilitates accent placement.", "labels": [], "entities": [{"text": "accent placement", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.848985105752945}]}, {"text": "Although pitch accents and breaks do not consistently occur at the same positions, they are to some extent analogous to phrase chunks and head words in parsing: breaks mark boundaries of intonational phrases, in which typically at least one accent is placed.", "labels": [], "entities": []}, {"text": "A learner may thus be able to learn both tasks at the same time.", "labels": [], "entities": []}, {"text": "Apart from the two issues raised, our work is also practically motivated.", "labels": [], "entities": []}, {"text": "Our goal is a good algorithm for real-time TTS.", "labels": [], "entities": [{"text": "TTS", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8729718327522278}]}, {"text": "This is reflected in the type of features that we use as input.", "labels": [], "entities": []}, {"text": "These can be computed in real-time, and are language independent.", "labels": [], "entities": []}, {"text": "We intend to show that this approach goes along way towards generating high-quality prosody, casting doubt on the need for more expensive sentence and discourse analysis.", "labels": [], "entities": [{"text": "sentence and discourse analysis", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.6517099589109421}]}, {"text": "The remainder of this paper has the following structure.", "labels": [], "entities": []}, {"text": "In Section 2 we define the task, describe the data, and the feature generation process which involves POS tagging, syntactic chunking, and computing several information-theoretic metrics.", "labels": [], "entities": [{"text": "feature generation", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7379716336727142}, {"text": "POS tagging", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.7829362750053406}]}, {"text": "Furthermore, a brief overview is given of the algorithms we used (CART and MBL).", "labels": [], "entities": [{"text": "MBL", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7499377131462097}]}, {"text": "Section 3 describes the experimental procedure (ten-fold iterative deepening) and the evaluation metrics (F-scores).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9712719917297363}]}, {"text": "Section 4 reports the results for predicting accents and major prosodic boundaries with both classifiers.", "labels": [], "entities": [{"text": "predicting accents", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8337802290916443}]}, {"text": "It also reports their performance on held-out data and on two fully independent test sets.", "labels": [], "entities": []}, {"text": "The final section offers some discussion and concluding remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Symbolic and numerical features and class for the sentence De bomen rondom de scheepswerf Verolme moeten verkassen,", "labels": [], "entities": []}, {"text": " Table 2: Precision, recall, and F-scores on accent, break", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988855719566345}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9984074234962463}, {"text": "F-scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9989849925041199}]}, {"text": " Table 4: Precision, recall, and F-scores on accent and break", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990488886833191}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9988071918487549}, {"text": "F-scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.998985230922699}, {"text": "accent", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9342865943908691}]}]}