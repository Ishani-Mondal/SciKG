{"title": [{"text": "A SNoW based Supertagger with Application to NP Chunking", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.669448584318161}]}], "abstractContent": [{"text": "Supertagging is the tagging process of assigning the correct elementary tree of LTAG, or the correct supertag, to each word of an input sentence 1.", "labels": [], "entities": []}, {"text": "In this paper we propose to use supertags to expose syntactic dependencies which are unavailable with POS tags.", "labels": [], "entities": []}, {"text": "We first propose a novel method of applying Sparse Network of Winnow (SNoW) to sequential models.", "labels": [], "entities": []}], "introductionContent": [{"text": "In Lexicalized Tree-Adjoining Grammar (LTAG)), each word in a sentence is associated with an elementary tree, or a supertag (.", "labels": [], "entities": [{"text": "Lexicalized Tree-Adjoining Grammar (LTAG))", "start_pos": 3, "end_pos": 45, "type": "TASK", "confidence": 0.6794192691644033}]}, {"text": "Supertagging is the process of assigning the correct supertag to each word of an input sentence.", "labels": [], "entities": []}, {"text": "The following two facts make supertagging attractive.", "labels": [], "entities": []}, {"text": "Firstly supertags encode much more syntactical information than POS tags, which makes supertagging a useful pre-parsing tool, so-called, almost parsing ().", "labels": [], "entities": []}, {"text": "On the other hand, as the term 'supertagging' suggests, the time complexity of supertagging is similar to that of POS tagging, which is linear in the length of the input sentence.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.7728355824947357}]}, {"text": "In this paper, we will focus on the NP chunking task, and use it as an application of supertagging.", "labels": [], "entities": [{"text": "NP chunking task", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.861080269018809}]}, {"text": "proposed a two-phase parsing model which includes chunking and attaching.", "labels": [], "entities": []}, {"text": "() approached chucking by using Transformation Based Learning(TBL).", "labels": [], "entities": []}, {"text": "Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (), SVMs (), CRFs (, Maximum Entropy Model, Memory Based Learning) and).", "labels": [], "entities": []}, {"text": "The previous best result on chunking in literature was achieved by Regularized Winnow (), which took some of the parsing results given by an English Slot Grammar-based parser as input to the chunker.", "labels": [], "entities": [{"text": "chunking", "start_pos": 28, "end_pos": 36, "type": "TASK", "confidence": 0.9734063744544983}]}, {"text": "The use of parsing results contributed \u00a5 \u00a4 \u00a3 absolute increase in F-score.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9700543284416199}, {"text": "F-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.995800793170929}]}, {"text": "However, this approach conflicts with the purpose of chunking.", "labels": [], "entities": [{"text": "chunking", "start_pos": 53, "end_pos": 61, "type": "TASK", "confidence": 0.9689971208572388}]}, {"text": "Ideally, a chunker geneates n-best results, and an attacher uses chunking results to construct a parse.", "labels": [], "entities": []}, {"text": "The dilemma is that syntactic constraints are useful in the chunking phase, but they are unavailable until the attaching phase.", "labels": [], "entities": []}, {"text": "The reason is that POS tags are not a good labeling system to encode enough linguistic knowledge for chunking.", "labels": [], "entities": []}, {"text": "However another labeling system, supertagging, can provide a great deal of syntactic information.", "labels": [], "entities": []}, {"text": "In an LTAG, each word is associated with a set of possible elementary trees.", "labels": [], "entities": []}, {"text": "An LTAG parser assigns the correct elementary tree to each word of a sentence, and uses the elementary trees of all the words to build a parse tree for the sentence.", "labels": [], "entities": []}, {"text": "Elementary trees, which we call supertags, contain more information than POS tags, and they help to improve the chunking accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9448345899581909}]}, {"text": "Although supertags are able to encode long distance dependence, supertaggers trained with local information in fact do not take full advantage of complex information available in supertags.", "labels": [], "entities": []}, {"text": "In order to exploit syntactic dependencies in a larger context, we propose anew model of supertagging based on Sparse Network of Winnow (SNoW).", "labels": [], "entities": []}, {"text": "We also propose a novel method of applying SNoW to sequential models in away analogous to the Projection-base Markov Model (PMM) used in).", "labels": [], "entities": []}, {"text": "In contrast to PMM, we construct a SNoW classifier for each POS tag.", "labels": [], "entities": [{"text": "PMM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.920939028263092}]}, {"text": "For each word of an input sentence, its POS tag, instead of the supertag of the previous word, is used to select the corresponding SNoW classifier.", "labels": [], "entities": []}, {"text": "This method helps to avoid the sparse data problem and forces SNoW to focus on difficult cases in the context of supertagging task.", "labels": [], "entities": []}, {"text": "Since PMM suffers from the label bias problem (), we have used two methods to cope with this problem.", "labels": [], "entities": [{"text": "PMM", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9607526659965515}]}, {"text": "One method is to skip the local normalization step, and the other is to combine the results of left-to-right scan and right-to-left scan.", "labels": [], "entities": []}, {"text": "We test our supertagger on both the hand-coded supertags used in) as well as the supertags extracted from Penn Treebank(PTB)).", "labels": [], "entities": [{"text": "Penn Treebank(PTB))", "start_pos": 106, "end_pos": 125, "type": "DATASET", "confidence": 0.9749842166900635}]}, {"text": "On the dataset used in), our supertagger achieves an accuracy of We then apply our supertagger to NP chunking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9996359348297119}, {"text": "NP chunking", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.7361351847648621}]}, {"text": "The purpose of this paper is to find a better way to exploit syntactic information which is useful in NP chunking, but not the machine learning part.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 102, "end_pos": 113, "type": "TASK", "confidence": 0.8810422420501709}]}, {"text": "So we just use TBL, a well-known algorithm in the community of text chunking, as the machine learning tool in our research.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.7655375599861145}]}, {"text": "Using TBL also allows us to easily evaluate the contribution of supertags with respect to Ramshaw and Marcus's original work, the de facto baseline of NP chunking.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 151, "end_pos": 162, "type": "TASK", "confidence": 0.8216684758663177}]}, {"text": "The use of supertags with TBL can be easily extended to other machine learning algorithms.", "labels": [], "entities": []}, {"text": "We repeat Ramshaw and Marcus' Transformation Based NP chunking algorithm by substituting supertags for POS tags in the dataset.", "labels": [], "entities": [{"text": "Transformation Based NP chunking", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.5370631441473961}]}, {"text": "The use of supertags gives rise to almost frame.", "labels": [], "entities": []}, {"text": "This confirms our claim that using supertagging as a labeling system helps to increase the overall performance of NP Chunking.", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 114, "end_pos": 125, "type": "DATASET", "confidence": 0.7972997725009918}]}, {"text": "The supertagger presented in this paper provides an opportunity for advanced machine learning techniques to improve their performance on chunking tasks by exploiting more syntactic information encoded in the supertags.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use the default settings of the SNoW promotion parameter, demotion parameter and the threshold value given by the SNoW system.", "labels": [], "entities": [{"text": "SNoW promotion", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7711573243141174}]}, {"text": "We train our model on the training data for 2 rounds, only counting the features that appear for at least 5 times.", "labels": [], "entities": []}, {"text": "We skip the normalization step in test, and we use beam search with the width of 5.", "labels": [], "entities": []}, {"text": "In our first experiment, we use the same dataset as that of) for our experiments.", "labels": [], "entities": []}, {"text": "We use WSJ section 00 through 24 expect section 20 as training data, and use section 20 as test data.", "labels": [], "entities": [{"text": "WSJ section 00", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.8592050274213155}]}, {"text": "Both training and test data are first tagged by Brill's POS tagger.", "labels": [], "entities": [{"text": "Brill's POS tagger", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.8185758590698242}]}, {"text": "We use the same pairwise voting algorithm as in , the best supertagging result to date.", "labels": [], "entities": []}, {"text": "shows the comparison with previous work.", "labels": [], "entities": []}, {"text": "Our algorithm, which is coded in Java, takes about 10 minutes to supertag the test data with a P3 1.13GHz processor.", "labels": [], "entities": []}, {"text": "However, in, the accuracy of \u00a2 \u00a4 \u00a3 \u00a6 \u00a5 \u00a3 \u00a4 was achieved by a Viterbi search program that took about 5 days to supertag the test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9996289014816284}]}, {"text": "The counterpart of our algorithm in) is the beam search on Model 8 with width of 5, which is the same as the beam width in our algorithm.", "labels": [], "entities": []}, {"text": "Compared with this program, our algorithm achieves an error reduction of \" # \u00a5 & \u00a9 . () achieved an accuracy of by combination of 5 distinct supertaggers.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.9698782563209534}, {"text": "\u00a9", "start_pos": 81, "end_pos": 82, "type": "METRIC", "confidence": 0.9853371977806091}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9994978904724121}]}, {"text": "However, our result is achieved by combining outputs of two homogeneous supertaggers, which only differ in scan direction.", "labels": [], "entities": []}, {"text": "Our next experiment is with the set of supertags abstracted from PTB with Fei Xia's LexTract.", "labels": [], "entities": [{"text": "PTB", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.971993625164032}, {"text": "LexTract", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.7060580849647522}]}, {"text": "Xia extracted an LTAG-style grammar from PTB, and repeated Srinivas' experiment (Srinivas, 1997) on her supertag set.", "labels": [], "entities": [{"text": "PTB", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.9005250930786133}]}, {"text": "There are 2920 elemenmodel acc Srinivas   tary trees in Xia's grammar \u0089 \u008a I , so that the supertags are more specialized and hence there is much more ambiguity in supertagging.", "labels": [], "entities": [{"text": "Xia's grammar \u0089 \u008a I", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.8671916325887045}]}, {"text": "We have experimented with our model on \u0089 \u008b I and her dataset.", "labels": [], "entities": [{"text": "\u0089 \u008b I and her dataset", "start_pos": 39, "end_pos": 60, "type": "DATASET", "confidence": 0.7306373914082845}]}, {"text": "We train our left-to-right model on WSJ section 02 through 21 of PTB, and test on section 22 and 23.", "labels": [], "entities": [{"text": "WSJ section 02", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.8493534525235494}, {"text": "PTB", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.5588652491569519}]}, {"text": "We achieve an average error reduction of \u00a9 ' \u00a6 \u00a5 \u0088 . The reason why the accuracy is rather low is that systems using \u0089 \u008a I have to cope with much more ambiguities due the large size of the supertag set.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 22, "end_pos": 37, "type": "METRIC", "confidence": 0.9710829854011536}, {"text": "\u00a9", "start_pos": 41, "end_pos": 42, "type": "METRIC", "confidence": 0.9819946885108948}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9995065927505493}, {"text": "\u0089 \u008a I", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.5897765060265859}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We test on both normalized and unnormalized models with both hand coded supertag set and autoextracted supertag set.", "labels": [], "entities": []}, {"text": "We use the left-to-right SNoW model in these experiments.", "labels": [], "entities": []}, {"text": "The results in show that skipping the local normalization improves performance in all the systems.", "labels": [], "entities": []}, {"text": "The effect of skipping normalization is more significant on auto-extracted tags.", "labels": [], "entities": []}, {"text": "We think this is because sparse tag set size norm?", "labels": [], "entities": []}, {"text": "acc: Experiments on normalized and unnormalized models using left-to-right SNoW supertagger.", "labels": [], "entities": []}, {"text": "size = size of the tag set. norm?", "labels": [], "entities": []}, {"text": "acc = percentage of accuracy on section 20, 22 and 23.", "labels": [], "entities": [{"text": "acc", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9929574728012085}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9919739365577698}]}, {"text": "auto = auto-extracted tag set.", "labels": [], "entities": []}, {"text": "hand = hand coded tag set.", "labels": [], "entities": []}, {"text": "data is more vulnerable to the label bias problem.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison with previous work. Training  data is WSJ section 00 thorough 24 except section  20 of PTB. Test data is WSJ section 20. Size of tag  set is 479. acc = percentage of accuracy. The num- ber of Srinivas(97) is based on footnote 1 of (Chen et  al., 1999). The number of Chen(01) width=5 is the  result of a beam search on Model 8 with the width  of 5.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9429962635040283}, {"text": "PTB", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9616500735282898}, {"text": "WSJ", "start_pos": 126, "end_pos": 129, "type": "DATASET", "confidence": 0.9561845660209656}, {"text": "Size", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9591729044914246}, {"text": "acc", "start_pos": 167, "end_pos": 170, "type": "METRIC", "confidence": 0.9576014876365662}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9984833598136902}]}, {"text": " Table 2: Results on auto-extracted LTAG grammar.  Training data is WSJ section 02 thorough 21 of PTB.  Test data is WSJ section 22 and 23. Size of supertag  set is 2920. acc = percentage of accuracy.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9479488730430603}, {"text": "PTB", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.9143082499504089}, {"text": "WSJ", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.9679536819458008}, {"text": "acc", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.9754644632339478}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9981156587600708}]}, {"text": " Table 4: Results on NP Chunking. Training data is  WSJ section 15-18 of PTB. Test data is WSJ section  20. A = Accuracy of IOB tagging. P = NP chunk  Precision. R = NP chunk Recall. F = F-score. Brill- POS = fast TBL with Brill's POS tags. Tri-STAG =  fast TBL with supertags given by Srinivas' trigram- based supertagger. SNoW-STAG = fast TBL with  supertags given by our SNoW supertagger. SNoW- STAG2 = fast TBL with augmented supertags given  by our SNoW supertagger. GOLD-POS = fast TBL  with gold standard POS tags. GOLD-STAG = fast  TBL with gold standard supertags.", "labels": [], "entities": [{"text": "NP Chunking", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.7861720025539398}, {"text": "WSJ", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.9529448747634888}, {"text": "PTB", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.890424370765686}, {"text": "WSJ", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9561923146247864}, {"text": "A", "start_pos": 108, "end_pos": 109, "type": "METRIC", "confidence": 0.9932522177696228}, {"text": "Accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9920136332511902}, {"text": "IOB tagging", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.5733353644609451}, {"text": "Recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.9146548509597778}, {"text": "F-score", "start_pos": 187, "end_pos": 194, "type": "METRIC", "confidence": 0.9527146816253662}, {"text": "Brill", "start_pos": 196, "end_pos": 201, "type": "METRIC", "confidence": 0.8212584257125854}]}]}