{"title": [{"text": "A Noisy-Channel Approach to Question Answering", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7866093814373016}]}], "abstractContent": [{"text": "We introduce a probabilistic noisy-channel model for question answering and we show how it can be exploited in the context of an end-to-end QA system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8784197568893433}]}, {"text": "Our noisy-channel system outperforms a state-of-the-art rule-based QA system that uses similar resources.", "labels": [], "entities": []}, {"text": "We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 187, "end_pos": 194, "type": "DATASET", "confidence": 0.9360935091972351}]}], "introductionContent": [{"text": "Current state-of-the-art Question Answering (QA) systems are extremely complex.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8758867859840394}]}, {"text": "They contain tens of modules that do everything from information retrieval, sentence parsing (), question-type pinpointing), semantic analysis (Xu et al.,), and reasoning ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7722240090370178}, {"text": "sentence parsing", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7101708054542542}, {"text": "semantic analysis", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.8311530351638794}]}, {"text": "They access external resources such as the WordNet (), the web (), structured, and semistructured databases ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9634403586387634}]}, {"text": "They contain feedback loops, ranking, and re-ranking modules.", "labels": [], "entities": []}, {"text": "Given their complexity, it is often difficult (and sometimes impossible) to understand what contributes to the performance of a system and what doesn't.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew approach to QA in which the contribution of various resources and components can be easily assessed.", "labels": [], "entities": [{"text": "QA", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.941718578338623}]}, {"text": "The fundamental insight of our approach, which departs significantly from the current architectures, is that, at its core, a QA system is a pipeline of only two modules: \u2022 An IR engine that retrieves a set of M documents/N sentences that may contain answers to a given question Q.", "labels": [], "entities": []}, {"text": "\u2022 And an answer identifier module that given a question Q and a sentence S (from the set of sentences retrieved by the IR engine) identifies a sub-string SA of S that is likely to bean answer to Q and assigns a score to it.", "labels": [], "entities": []}, {"text": "Once one has these two modules, one has a QA system because finding the answer to a question Q amounts to selecting the sub-string SA of highest score.", "labels": [], "entities": []}, {"text": "Although this view is not made explicit by QA researchers, it is implicitly present in all systems we are aware of.", "labels": [], "entities": []}, {"text": "In its simplest form, if one accepts a whole sentence as an answer (S A = S), one can assess the likelihood that a sentence S contains the answer to a question Q by measuring the cosine similarity between Q and S.", "labels": [], "entities": []}, {"text": "However, as research in QA demonstrates, word-overlap is not a good enough metric for determining whether a sentence contains the answer to a question.", "labels": [], "entities": []}, {"text": "Consider, for example, the question \"Who is the leader of France?\"", "labels": [], "entities": []}, {"text": "The sentence \"Henri Hadjenberg, who is the leader of France's Jewish community, endorsed confronting the specter of the Vichy past\" overlaps with all question terms, but it does not contain the correct answer; while the sentence \"Bush later met with French President Jacques Chirac\" does not overlap with any question term, but it does contain the correct answer.", "labels": [], "entities": [{"text": "confronting the specter of the Vichy past", "start_pos": 89, "end_pos": 130, "type": "TASK", "confidence": 0.8066011071205139}]}, {"text": "To circumvent this limitation of word-based similarity metrics, QA researchers have developed methods through which they first map questions and sentences that may contain answers in different spaces, and then compute the \"similarity\" between them there.", "labels": [], "entities": []}, {"text": "For example, the systems developed at IBM and ISI map questions and answer sentences into parse trees and surfacebased semantic labels and measure the similarity between questions and answer sentences in this syntactic/semantic space, using QA-motivated metrics.", "labels": [], "entities": []}, {"text": "The systems developed by CYC and LCC map questions and answer sentences into logical forms and compute the \"similarity\" between them using inference rules.", "labels": [], "entities": [{"text": "CYC", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9627729654312134}]}, {"text": "And systems such as those developed by IBM and BBN map questions and answers into feature sets and compute the similarity between them using maximum entropy models that are trained on question-answer corpora.", "labels": [], "entities": [{"text": "BBN", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8921608924865723}]}, {"text": "From this perspective then, the fundamental problem of question answering is that of finding spaces where the distance between questions and sentences that contain correct answers is small and where the distance between questions and sentences that contain incorrect answers is large.", "labels": [], "entities": [{"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.850922703742981}]}, {"text": "In this paper, we propose anew space and anew metric for computing this distance.", "labels": [], "entities": []}, {"text": "Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition, part of speech tagging, machine translation (, information retrieval, and text summarization (), we develop a noisy channel model for QA.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7749797999858856}, {"text": "speech tagging", "start_pos": 122, "end_pos": 136, "type": "TASK", "confidence": 0.7116833925247192}, {"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7714702486991882}, {"text": "information retrieval", "start_pos": 161, "end_pos": 182, "type": "TASK", "confidence": 0.7616864442825317}, {"text": "text summarization", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.7439928650856018}]}, {"text": "This model explains how a given sentence SA that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations.", "labels": [], "entities": []}, {"text": "Given a corpus of questionanswer pairs (Q, SA ), we can train a probabilistic model for estimating the conditional probability P(Q | SA ).", "labels": [], "entities": []}, {"text": "Once the parameters of this model are learned, given a question Q and the set of sentences \u03a3 returned by an IR engine, one can find the sentence Si \u2208 \u03a3 and an answer in it A i,j by searching for the S i,A i,j that maximizes the conditional probability P(Q | S i,A i,j ).", "labels": [], "entities": []}, {"text": "In Section 2, we first present the noisy-channel model that we propose for this task.", "labels": [], "entities": []}, {"text": "In Section 3, we describe how we generate training examples.", "labels": [], "entities": []}, {"text": "In Section 4, we describe how we use the learned models to answer factoid questions, we evaluate the performance of our system using a variety of experimental conditions, and we compare it with a rule-based system that we have previously used in several TREC evaluations.", "labels": [], "entities": []}, {"text": "In Section 5, we demonstrate that the framework we propose is flexible enough to accommodate a wide range of resources and techniques that have been employed in state-of-the-art QA systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the results by generating automatically the mean reciprocal rank (MRR) using the TREC 2002 patterns and QuizZone original answers when testing on TREC 2002 and QuizZone test sets respectively.", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 56, "end_pos": 82, "type": "METRIC", "confidence": 0.8810622692108154}, {"text": "TREC 2002 patterns", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.9140963355700175}, {"text": "TREC 2002", "start_pos": 158, "end_pos": 167, "type": "DATASET", "confidence": 0.9135877192020416}, {"text": "QuizZone test sets", "start_pos": 172, "end_pos": 190, "type": "DATASET", "confidence": 0.8764357566833496}]}, {"text": "Our baseline is a state of the art QA system, QA-base, which was ranked from second to seventh in the last 3 years at TREC.", "labels": [], "entities": [{"text": "TREC", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.8830026388168335}]}, {"text": "To ensure a fair comparison, we use the same Web-based IR system in all experiments with no answer retrofitting.", "labels": [], "entities": []}, {"text": "For the same reason, we use the QA-base system with the post-processing module disabled.", "labels": [], "entities": []}, {"text": "(This module re-ranks the answers produced by QA-base on the basis of their redundancy, frequency on the web, etc.) summarizes results of different combinations of training and test sets:  For the TREC 2002 corpus, the relatively low MRRs are due to the small answer coverage of the TREC 2002 patterns.", "labels": [], "entities": [{"text": "TREC 2002 corpus", "start_pos": 197, "end_pos": 213, "type": "DATASET", "confidence": 0.9133533438046774}, {"text": "MRRs", "start_pos": 234, "end_pos": 238, "type": "METRIC", "confidence": 0.8934639692306519}, {"text": "TREC 2002 patterns", "start_pos": 283, "end_pos": 301, "type": "DATASET", "confidence": 0.9043126503626505}]}, {"text": "For the KM corpus, the relatively low MRRs are explained by two factors: (i) for this corpus, each evaluation pattern consists of only one string -the original answer; (ii) the KM questions are more complex than TREC questions (What piece of furniture is associated with Modred, Percival,.", "labels": [], "entities": [{"text": "MRRs", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.7484273314476013}]}, {"text": "It is interesting to see that using only the TREC9-10 data as training (system A in), we are able to beat the baseline when testing on TREC 2002 questions; however, this is not true when testing on KM questions.", "labels": [], "entities": [{"text": "TREC9-10 data", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.9417127072811127}, {"text": "TREC 2002 questions", "start_pos": 135, "end_pos": 154, "type": "DATASET", "confidence": 0.8513285915056864}]}, {"text": "This can be explained by the fact that the TREC9-10 training set is similar to the TREC 2002 test set while it is significantly different from the KM test set.", "labels": [], "entities": [{"text": "TREC9-10 training set", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.881519079208374}, {"text": "TREC 2002 test set", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.9445482790470123}, {"text": "KM test set", "start_pos": 147, "end_pos": 158, "type": "DATASET", "confidence": 0.8053674598534902}]}, {"text": "We also notice that expanding the training to TREC9-10Web (System B) and then to Quiz-Zone (System C) improved the performance on both test sets, which confirms that both the variability across answer tagged sentences (Trec9-10Web) and the abundance of distinct questions (Quiz-Zone) contribute to the diversity of a QA training corpus, and implicitly to the performance of our system.", "labels": [], "entities": [{"text": "TREC9-10Web", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.5768977999687195}]}], "tableCaptions": [{"text": " Table 1: Size of Training Corpora", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.989452600479126}]}, {"text": " Table 2: Impact of training and test sets.", "labels": [], "entities": []}, {"text": " Table 3: WordNet synonyms and glosses impact.", "labels": [], "entities": [{"text": "WordNet synonyms", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.6560873985290527}]}, {"text": " Table 5: Factoid impact on system performance.", "labels": [], "entities": []}]}