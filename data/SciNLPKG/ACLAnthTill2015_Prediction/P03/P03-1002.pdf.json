{"title": [{"text": "Using Predicate-Argument Structures for Information Extraction", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7095129638910294}]}], "abstractContent": [{"text": "In this paper we present a novel, cus-tomizable IE paradigm that takes advantage of predicate-argument structures.", "labels": [], "entities": [{"text": "IE", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9189362525939941}]}, {"text": "We also introduce anew way of automatically identifying predicate argument structures, which is central to our IE paradigm.", "labels": [], "entities": [{"text": "automatically identifying predicate argument structures", "start_pos": 30, "end_pos": 85, "type": "TASK", "confidence": 0.6678568243980407}, {"text": "IE", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9695708751678467}]}, {"text": "It is based on: (1) an extended set of features; and (2) inductive decision tree learning.", "labels": [], "entities": [{"text": "decision tree learning", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.6514176925023397}]}, {"text": "The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.", "labels": [], "entities": [{"text": "IE", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.993316113948822}]}], "introductionContent": [{"text": "The goal of recent Information Extraction (IE) tasks was to provide event-level indexing into news stories, including news wire, radio and television sources.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.8556296944618225}]}, {"text": "In this context, the purpose of the HUB Event-99 evaluations) was to capture information on some newsworthy classes of events, e.g. natural disasters, deaths, bombings, elections, financial fluctuations or illness outbreaks.", "labels": [], "entities": [{"text": "HUB Event-99 evaluations", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.8834326068560282}]}, {"text": "The identification and selective extraction of relevant information is dictated by templettes.", "labels": [], "entities": []}, {"text": "Event templettes are frame-like structures with slots representing the event basic information, such as main event participants, event outcome, time and location.", "labels": [], "entities": []}, {"text": "For each type of event, a separate templette is defined.", "labels": [], "entities": []}, {"text": "The slots fills consist of excerpts from text with pointers back into the original source material.", "labels": [], "entities": []}, {"text": "Templettes are designed to support event-based browsing and search.", "labels": [], "entities": []}, {"text": "illustrates a templette defined for \"market changes\" as well as the source of the slot fillers.", "labels": [], "entities": []}, {"text": "To date, some of the most successful IE techniques are built around a set of domain relevant linguistic patterns based on select verbs (e.g. fall, gain or lose for the \"market change\" topic).", "labels": [], "entities": [{"text": "IE", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9923835396766663}]}, {"text": "These patterns are matched against documents for identifying and extracting domain-relevant information.", "labels": [], "entities": []}, {"text": "Such patterns are either handcrafted or acquired automatically.", "labels": [], "entities": []}, {"text": "A rich literature covers methods of automatically acquiring IE patterns.", "labels": [], "entities": [{"text": "IE patterns", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.8246930539608002}]}, {"text": "Some of the most recent methods were reported in).", "labels": [], "entities": []}, {"text": "To process texts efficiently and fast, domain patterns are ideally implemented as finite state automata (FSAs), a methodology pioneered in the FASTUS IE system (.", "labels": [], "entities": [{"text": "FASTUS IE system", "start_pos": 143, "end_pos": 159, "type": "DATASET", "confidence": 0.7979293266932169}]}, {"text": "Although this paradigm is simple and elegant, it has the disadvantage that it is not easily portable from one domain of interest to the next.", "labels": [], "entities": []}, {"text": "In contrast, anew, truly domain-independent IE paradigm maybe designed if we know (a) predicates relevant to a domain; and (b) which of their arguments fill templette slots.", "labels": [], "entities": []}, {"text": "Central to this new way of extracting information from texts are systems that label predicate-argument structures on the output of full parsers.", "labels": [], "entities": []}, {"text": "One such augmented parser, trained on data available from the PropBank project has been recently presented in (.", "labels": [], "entities": []}, {"text": "In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods: (1) the statistical method reported in (); and (2) anew method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data.", "labels": [], "entities": [{"text": "IE", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.8977155685424805}, {"text": "Fscore", "start_pos": 273, "end_pos": 279, "type": "METRIC", "confidence": 0.9996364116668701}]}, {"text": "The accuracy enhancement of predicate argument recognition determines up to 14% better IE results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991421699523926}, {"text": "predicate argument recognition", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.7699295282363892}, {"text": "IE", "start_pos": 87, "end_pos": 89, "type": "TASK", "confidence": 0.7058746218681335}]}, {"text": "These results enforce our claim that predicate argument information for IE needs to be recognized with high accuracy.", "labels": [], "entities": [{"text": "IE", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9572824835777283}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9940696954727173}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reports on the parser that produces predicate-argument labels and compares it against the parser introduced in ().", "labels": [], "entities": []}, {"text": "Section 3 describes the pattern-free IE paradigm and compares it against FSA-based IE methods.", "labels": [], "entities": [{"text": "IE", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9065632224082947}, {"text": "FSA-based IE", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.5001822859048843}]}, {"text": "Section 4 describes the integration of predicate-argument parsers into the IE paradigm and compares the results against a FSA-based IE system.", "labels": [], "entities": [{"text": "IE paradigm", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.8277257680892944}, {"text": "FSA-based IE", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.7687940299510956}]}, {"text": "Section 5 summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results presented in this paper were obtained by training on Proposition Bank (PropBank) release 2002/7/15 ().", "labels": [], "entities": [{"text": "Proposition Bank (PropBank) release 2002/7/15", "start_pos": 65, "end_pos": 110, "type": "DATASET", "confidence": 0.95105887001211}]}, {"text": "Syntactic information was extracted from the gold-standard parses in TreeBank Release 2.", "labels": [], "entities": []}, {"text": "As named entity information is not available in PropBank/TreeBank we tagged the training corpus with NE information using an open-domain NE recognizer, having 96% F-measure on the MUC6 1 data.", "labels": [], "entities": [{"text": "PropBank/TreeBank", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.8648799856503805}, {"text": "F-measure", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9985402822494507}, {"text": "MUC6 1 data", "start_pos": 180, "end_pos": 191, "type": "DATASET", "confidence": 0.9281660715738932}]}, {"text": "We reserved section 23 of PropBank/TreeBank for testing, and we trained on the rest.", "labels": [], "entities": [{"text": "PropBank/TreeBank", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.7581296761830648}]}, {"text": "Due to memory limitations on our hardware, for the argument finding task we trained on the first 150 KB of TreeBank (about 11% of TreeBank), and for the role assignment task on the first 75 KB of argument constituents (about 60% of PropBank annotations).", "labels": [], "entities": [{"text": "argument finding", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7792727947235107}, {"text": "role assignment", "start_pos": 153, "end_pos": 168, "type": "TASK", "confidence": 0.7847859263420105}]}, {"text": "shows the results obtained by our inductive learning approach.", "labels": [], "entities": []}, {"text": "The first column describes the feature sets used in each of the 7 experiments performed.", "labels": [], "entities": []}, {"text": "The following three columns indicate the precision (P), recall (R), and F-measure ( \u00a2 \u00a1 ) 2 obtained for the task of identifying argument constituents.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9474116563796997}, {"text": "recall (R)", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9522375464439392}, {"text": "F-measure ( \u00a2 \u00a1 ) 2", "start_pos": 72, "end_pos": 91, "type": "METRIC", "confidence": 0.9506486256917318}]}, {"text": "The last column shows the accuracy (A) for the role assignment task using known argument constituents.", "labels": [], "entities": [{"text": "accuracy (A)", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.8272723853588104}, {"text": "role assignment task", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.8298521439234415}]}, {"text": "The first row in lists the results obtained when using only the FS1 features.", "labels": [], "entities": [{"text": "FS1", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8904261589050293}]}, {"text": "The next five lines list the individual contributions of each of the newly added features when combined with the FS1 features.", "labels": [], "entities": [{"text": "FS1", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9207159280776978}]}, {"text": "The last line shows the results obtained when all features from FS1 and FS2 were used.", "labels": [], "entities": [{"text": "FS1", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9659901261329651}, {"text": "FS2", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.906348705291748}]}, {"text": "shows that the new features increase the argument identification F-measure by 3.61%, and the role assignment accuracy with 4.29%.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.706867516040802}, {"text": "F-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.7230668663978577}, {"text": "role assignment", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.7822125554084778}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9539556503295898}]}, {"text": "For the argument identification task, the head and content word features have a significant contribution for the task precision, whereas NE features contribute significantly to the task recall.", "labels": [], "entities": [{"text": "argument identification task", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.8427973786989847}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9869155883789062}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.947462260723114}]}, {"text": "For the role assignment task the best features from the feature set FS2 are the content word features (cw and cPos) and the Boolean NE flags, which show that semantic information, even if minimal, is important for role classification.", "labels": [], "entities": [{"text": "role assignment task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.902500569820404}, {"text": "FS2", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9379542469978333}, {"text": "Boolean NE flags", "start_pos": 124, "end_pos": 140, "type": "DATASET", "confidence": 0.8266260226567587}, {"text": "role classification", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.8411644697189331}]}, {"text": "Surprisingly, the phrasal verb collocation features did not help for any of the tasks, but they were useful for boosting the decision trees.", "labels": [], "entities": []}, {"text": "Decision tree learning provided by C5) has builtin support for boosting.", "labels": [], "entities": []}, {"text": "We used it and obtained improvements for both tasks.", "labels": [], "entities": []}, {"text": "The best Fmeasure obtained for argument constituent identification was 88.98% in the fifth iteration (a 0.76% improvement).", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9849163293838501}, {"text": "argument constituent identification", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.671686440706253}]}, {"text": "The best accuracy for role assignment was 83.74% in the eight iteration (a 0.69% improvement) . We further analyzed the boosted trees and noticed that phrasal verb collocation features were mainly responsible for the improvements.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9993732571601868}, {"text": "role assignment", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.8704305589199066}]}, {"text": "This is the rationale for including them in the FS2 set.", "labels": [], "entities": [{"text": "FS2 set", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9640986323356628}]}, {"text": "We also were interested in comparing the results These results, listed also on the last line of   of the decision-tree-based method against the results obtained by the statistical approach reported in ().", "labels": [], "entities": []}, {"text": "() report the results listed on the first line of.", "labels": [], "entities": []}, {"text": "Because no Fscores were reported for the argument identification task, we re-implemented the model and obtained the results listed on the second line.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9923213124275208}, {"text": "argument identification task", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.8461843132972717}]}, {"text": "It looks like we had some implementation differences, and our results for the argument role classification task were slightly worse.", "labels": [], "entities": [{"text": "argument role classification task", "start_pos": 78, "end_pos": 111, "type": "TASK", "confidence": 0.8012678623199463}]}, {"text": "However, we used our results for the statistical model for comparing with the inductive learning model because we used the same feature extraction code for both models.", "labels": [], "entities": []}, {"text": "Lines 3 and 4 list the results of the inductive learning model with boosting enabled, when the features were only from FS1, and from FS1 and FS2 respectively.", "labels": [], "entities": [{"text": "FS1", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.9081496000289917}, {"text": "FS1", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.9702557325363159}, {"text": "FS2", "start_pos": 141, "end_pos": 144, "type": "DATASET", "confidence": 0.8745493292808533}]}, {"text": "When comparing the results obtained for both models when using only features from FS1, we find that almost the same results were obtained for role classification, but an enhancement of almost 13% was obtained when recognizing argument constituents.", "labels": [], "entities": [{"text": "FS1", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8793779611587524}, {"text": "role classification", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.8138197958469391}]}, {"text": "When comparing the statistical model with the inductive model that uses all features, there is an enhancement of 17.12% for argument identification and 4.87% for argument role recognition.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.8304080665111542}, {"text": "argument role recognition", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.7123600641886393}]}, {"text": "Another significant advantage of our inductive learning approach is that it scales better to un- uses predicate lexical information at most levels in the probability lattice, hence its scalability to unknown predicates is limited.", "labels": [], "entities": []}, {"text": "In contrast, the decision tree approach uses predicate lexical information only for 5% of the branching decisions recorded when testing the role assignment task, and only for 0.01% of the branching decisions seen during the argument constituent identification evaluation.", "labels": [], "entities": [{"text": "role assignment task", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.7578757603963217}, {"text": "argument constituent identification evaluation", "start_pos": 224, "end_pos": 270, "type": "TASK", "confidence": 0.7345699891448021}]}, {"text": "To evaluate the proposed IE paradigm we selected two Event99 domains: \"market change\", which tracks changes in stock indexes, and \"death\", which extracts all manners of human deaths.", "labels": [], "entities": [{"text": "IE", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9710883498191833}]}, {"text": "These domains were selected because most of the domain information can be processed without needing entity or event coreference.", "labels": [], "entities": []}, {"text": "Moreover, one of the domains (market change) uses verbs commonly used in PropBank/TreeBank, while the other (death) uses relatively unknown verbs, so we can also evaluate how well the system scales to verbs unseen in training.", "labels": [], "entities": [{"text": "PropBank/TreeBank", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.8276133338610331}]}, {"text": "lists the F-scores for the two domains.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9962663054466248}]}, {"text": "The first line of the  The results obtained by the FSA-based IE were the best, but they were made possible by handcrafted patterns requiring an effort of 10 person days per domain.", "labels": [], "entities": [{"text": "FSA-based IE", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.8655436336994171}]}, {"text": "The only human effort necessary in the new IE paradigm was imposed by the generation of mappings between arguments and templette slots, accomplished in less than 2 hours per domain, given that the training templettes are known.", "labels": [], "entities": [{"text": "IE paradigm", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9259556233882904}]}, {"text": "Additionally, it is easier to automatically learn these mappings than to acquire FSA patterns.", "labels": [], "entities": []}, {"text": "also shows that the new IE paradigm performs better when the predicate argument structures are recognized with the inductive learning model.", "labels": [], "entities": [{"text": "IE", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.965620756149292}]}, {"text": "The cause is the substantial difference in quality of the argument identification task between the two models.", "labels": [], "entities": [{"text": "argument identification task", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.8031898935635885}]}, {"text": "The that the new IE paradigm with the inductive learning model achieves about 90% of the performance of the FSA-based system for both domains, even though one of the domains uses mainly verbs rarely seen in training (e.g. \"die\" appears 5 times in PropBank).", "labels": [], "entities": [{"text": "IE", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9511531591415405}, {"text": "PropBank", "start_pos": 247, "end_pos": 255, "type": "DATASET", "confidence": 0.9483374357223511}]}, {"text": "Another way of evaluating the integration of predicate argument structures in IE is by comparing the number of events identified by each architecture.", "labels": [], "entities": [{"text": "IE", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9433834552764893}]}, {"text": "Once again, the new IE paradigm performs better when the predicate argument structures are recognized with the inductive learning model.", "labels": [], "entities": [{"text": "IE", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.975899338722229}]}, {"text": "More events are missed by the statistical model which does not recognize argument constituents as well the inductive learning model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inductive learning results for argument  identification and role assignment", "labels": [], "entities": [{"text": "argument  identification", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8758717775344849}, {"text": "role assignment", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.7676443755626678}]}, {"text": " Table 2: Comparison of statistical and decision tree  learning models", "labels": [], "entities": []}, {"text": " Table 3: Templette F-measure (", "labels": [], "entities": [{"text": "F-measure", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8077090978622437}]}, {"text": " Table 4: Number of event structures (FSA patterns  or predicate argument structures) matched", "labels": [], "entities": []}]}