{"title": [{"text": "Classifying Recognition Results for Spoken Dialog Systems", "labels": [], "entities": [{"text": "Classifying Recognition", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8869276344776154}, {"text": "Spoken Dialog Systems", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.865600069363912}]}], "abstractContent": [{"text": "This paper investigates the correlation between acoustic confidence scores as returned by speech recognizers with recognition quality.", "labels": [], "entities": [{"text": "acoustic confidence scores", "start_pos": 48, "end_pos": 74, "type": "METRIC", "confidence": 0.7236940264701843}]}, {"text": "We report the results of two machine learning experiments that predict the word error rate of recognition hypotheses and the confidence error rate for individual words within them.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.7246750295162201}, {"text": "confidence error rate", "start_pos": 125, "end_pos": 146, "type": "METRIC", "confidence": 0.9430477023124695}]}], "introductionContent": [{"text": "Acoustic confidence scores as computed by speech recognizers play an important role in the design of spoken dialog systems.", "labels": [], "entities": []}, {"text": "Often, systems solely decide on the basis of an overall acoustic confidence score whether they should accept (consider correct), clarify (ask for confirmation), or reject (prompt for repeat/rephrase) the interpretation of an user utterance.", "labels": [], "entities": []}, {"text": "This behavior is usually achieved by setting two fixed confidence thresholds: if the confidence score of an utterance is above the upper threshold it is accepted, when it is below the lower threshold it is rejected, and clarification is initiated in case the confidence score lies in between the two thresholds.", "labels": [], "entities": []}, {"text": "The GoDiS spoken dialog system () is an example of such a system.", "labels": [], "entities": [{"text": "GoDiS spoken dialog system", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.8708861917257309}]}, {"text": "More elaborated and flexible system behavior can be achieved by making use of individual word confidence scores or slot-confidences 1 that allow more fine-grained de-1 Some recognition platforms allow the application programmer to associate semantic slot values with certain words of an input utterance.", "labels": [], "entities": []}, {"text": "The slot-confi dence is then defined as the acoustic confidence for the words that makeup this slot.", "labels": [], "entities": []}, {"text": "cisions as to which parts of an utterance are not sufficiently well understood.", "labels": [], "entities": []}, {"text": "The aim of this paper is to investigate how well acoustic confidences correlate with recognition quality and to use machine learning (ML) techniques to improve this correlation.", "labels": [], "entities": []}, {"text": "In particular, we will conduct two different experiments.", "labels": [], "entities": []}, {"text": "First, we try to predict the word error rate (WER) of a recognition result based on its overall confidence score and show that we can improve on this by using ML classifiers.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 29, "end_pos": 50, "type": "METRIC", "confidence": 0.8811023235321045}]}, {"text": "Second, we will consider individual word confidence scores and again show that ML techniques can be fruitfully applied to the task of deciding whether individual words were recognized correctly or not.", "labels": [], "entities": [{"text": "ML", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9606054425239563}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we explain the general experimental setup, introduce acoustic confidences, and explain how we labeled our data.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 report on the actual experiments.", "labels": [], "entities": []}, {"text": "Section 5 summarizes and concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the ATIS2 corpus as our speech data source.", "labels": [], "entities": [{"text": "ATIS2 corpus", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.9736616611480713}]}, {"text": "The corpus contains approx. 15.000 utterances and has a vocabulary size of about 1.000 words.", "labels": [], "entities": []}, {"text": "In order to get \"real\" recognition data, we trained and tested the commercial NUANCE8.0 2 recognition engine on the ATIS2 corpus.", "labels": [], "entities": [{"text": "NUANCE8.0 2 recognition", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7333203554153442}, {"text": "ATIS2 corpus", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.9754673838615417}]}, {"text": "To this end we first split the corpus into two distinct sets.", "labels": [], "entities": []}, {"text": "With the first set we trained a statistical language model (trigram) for the recognizer.", "labels": [], "entities": []}, {"text": "This model was then used to recognize the other set of utterances (using 1-best recognition).", "labels": [], "entities": []}, {"text": "Finally, we split the set of recognized utterances into three different sets.", "labels": [], "entities": []}, {"text": "A training set (75%), a test set (20%) and a development set (5%).", "labels": [], "entities": []}, {"text": "The purpose of the first experiment was to find out how well features that can be automatically derived from a recognition hypothesis can be used to predict its word error rate.", "labels": [], "entities": []}, {"text": "As already mentioned in the previous section, all recognized sentences were assigned to one of the following classes depending on their actual WER: WER0 (WER 0%, sentence correctly recognized), WER50 (sentences with a WER between 1% and 50%), and WER100 (sentences with a WER greater than 50%).", "labels": [], "entities": [{"text": "WER", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.9482131004333496}, {"text": "WER0", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9040101170539856}, {"text": "WER50", "start_pos": 194, "end_pos": 199, "type": "METRIC", "confidence": 0.5550481677055359}, {"text": "WER100", "start_pos": 247, "end_pos": 253, "type": "DATASET", "confidence": 0.5030646324157715}]}, {"text": "The motivation to split the data into these three classes was that they can be associated with the two fixed thresholds commonly used in spoken dialog systems to decide whether an utterance should be accepted, clarified, or rejected.", "labels": [], "entities": []}, {"text": "We are aware that this might not bean optimal setting.", "labels": [], "entities": []}, {"text": "Some spoken dialog systems only spot for keywords or key-phrases in an utterance.", "labels": [], "entities": []}, {"text": "For them it does not matter whether \"unimportant\" words were recognized correctly or not and a WER greater than zero is often acceptable.", "labels": [], "entities": [{"text": "WER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9969215989112854}]}, {"text": "The main problem is that what counts as a keyword or key-phrase is system and domain depended.", "labels": [], "entities": []}, {"text": "We cannot simply base our experiments on the WER for content words like nouns, verbs, and adjectives.", "labels": [], "entities": []}, {"text": "Ina travel agency application, for example, the prepositions 'to' and 'from' are quite important.", "labels": [], "entities": []}, {"text": "In home automation, quantifiers/determiners are important to distinguish between the commands 'switch off all lights' and 'switch off the hall lights' (this example is borrowed from David Milward).", "labels": [], "entities": []}, {"text": "For further examples see also).", "labels": [], "entities": []}, {"text": "The aim of the second experiment was to investigate whether we can improve the confidence error rate (CER) for the recognized data.", "labels": [], "entities": [{"text": "confidence error rate (CER)", "start_pos": 79, "end_pos": 106, "type": "METRIC", "confidence": 0.9732692639032999}]}, {"text": "The CER measures how good individual word confidence scores predict whether words are correctly recognized or not.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5399278998374939}]}, {"text": "A confidence threshold is set according to which all words are either tagged as corrector incorrect.", "labels": [], "entities": []}, {"text": "The CER is then simply defined as the number of incorrectly assigned tags divided by the total number of recognized words.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8988643884658813}]}, {"text": "The CER is a very simple measure that strongly depends on the tagging threshold and the prior probability of the classes correct and incorrect.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9424451589584351}]}, {"text": "Since we have a strong bias towards correct words in our data, we complement the CER evaluation with a second evaluation matrix, the detection-error tradeoff (DET) curve which plots the false acceptance rate (the number of incorrect words tagged as correct divided by the total number of incorrect words) over the false rejection rate (the number of correct words tagged as incorrect divided by the total number of correct words).", "labels": [], "entities": [{"text": "CER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8576482534408569}, {"text": "detection-error tradeoff (DET) curve", "start_pos": 133, "end_pos": 169, "type": "METRIC", "confidence": 0.957842210928599}, {"text": "false acceptance rate", "start_pos": 186, "end_pos": 207, "type": "METRIC", "confidence": 0.7318812012672424}, {"text": "false rejection rate", "start_pos": 314, "end_pos": 334, "type": "METRIC", "confidence": 0.8049635291099548}]}, {"text": "This curve is instructive because it shows the results for several different tagging thresholds and how they effect the prediction accuracy for the two classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9723930358886719}]}], "tableCaptions": [{"text": " Table 2: Recognition results grouped by WER", "labels": [], "entities": [{"text": "Recognition", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8720858097076416}, {"text": "WER", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.4163309335708618}]}, {"text": " Table 6: Features used by Ripper", "labels": [], "entities": [{"text": "Ripper", "start_pos": 27, "end_pos": 33, "type": "TASK", "confidence": 0.6526396870613098}]}, {"text": " Table 7. The machine learners were only run  with their default settings.", "labels": [], "entities": []}, {"text": " Table 8: Minority class classification", "labels": [], "entities": [{"text": "Minority class classification", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8368332783381144}]}, {"text": " Table 9: Features used by Ripper", "labels": [], "entities": [{"text": "Ripper", "start_pos": 27, "end_pos": 33, "type": "TASK", "confidence": 0.6406387090682983}]}]}