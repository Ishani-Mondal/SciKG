{"title": [{"text": "A Machine Learning Approach to Pronoun Resolution in Spoken Dialogue", "labels": [], "entities": [{"text": "Pronoun Resolution in Spoken Dialogue", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7338805198669434}]}], "abstractContent": [{"text": "We apply a decision tree based approach to pronoun resolution in spoken dialogue.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7423577755689621}]}, {"text": "Our system deals with pronouns with NP-and non-NP-antecedents.", "labels": [], "entities": []}, {"text": "We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features.", "labels": [], "entities": [{"text": "pronoun resolution in spoken dialogue", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.8215148091316223}]}, {"text": "We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron's (2002) manually tuned system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpus-based methods and machine learning techniques have been applied to anaphora resolution in written text with considerable success (.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7786730527877808}]}, {"text": "It has been demonstrated that systems based on these approaches achieve a performance that is comparable to hand-crafted systems.", "labels": [], "entities": []}, {"text": "Since they can easily be applied to new domains it seems also feasible to port a given corpus-based anaphora resolution system from written text to spoken dialogue.", "labels": [], "entities": [{"text": "corpus-based anaphora resolution", "start_pos": 87, "end_pos": 119, "type": "TASK", "confidence": 0.7833572228749593}]}, {"text": "This paper describes the extensions and adaptations needed for applying our anaphora resolution system) to pronoun resolution in spoken dialogue.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7538572549819946}, {"text": "pronoun resolution in spoken dialogue", "start_pos": 107, "end_pos": 144, "type": "TASK", "confidence": 0.7999292612075806}]}, {"text": "There are important differences between written text and spoken dialogue which have to be accounted for.", "labels": [], "entities": []}, {"text": "The most obvious difference is that in spoken dialogue there is an abundance of (personal and demonstrative) pronouns with non-NP-antecedents or no antecedents at all.", "labels": [], "entities": []}, {"text": "Corpus studies have shown that a significant amount of pronouns in spoken dialogue have non-NP-antecedents: report that about 50% of the pronouns in the TRAINS93 corpus have non-NP-antecedents.", "labels": [], "entities": [{"text": "TRAINS93 corpus", "start_pos": 153, "end_pos": 168, "type": "DATASET", "confidence": 0.9307326078414917}]}, {"text": "note that only about 45% of the pronouns in a set of Switchboard dialogues have NP-antecedents.", "labels": [], "entities": []}, {"text": "The remainder consists of 22% which have non-NP-antecedents and 33% without antecedents.", "labels": [], "entities": []}, {"text": "These studies suggest that the performance of a pronoun resolution algorithm can be improved considerably by enabling it to resolve also pronouns with non-NP-antecedents.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.818651020526886}]}, {"text": "Because of the difficulties a pronoun resolution algorithm encounters in spoken dialogue, previous approaches were applied only to tiny domains, they needed deep semantic analysis and discourse processing and relied on hand-crafted knowledge bases.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7470686733722687}]}, {"text": "In contrast, we build on our existing anaphora resolution system and incrementally add new features specifically devised for spoken dialogue.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7610633969306946}]}, {"text": "That way we are able to determine relatively powerful yet computationally cheap features.", "labels": [], "entities": []}, {"text": "To our knowledge the work presented here describes the first implemented system for corpus-based anaphora resolution dealing also with non-NP-antecedents.", "labels": [], "entities": [{"text": "corpus-based anaphora resolution", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.6396762331326803}]}], "datasetContent": [{"text": "All experiments were performed using the decision tree learner RPART, which is a CART ( reimplementation for the S-Plus and R statistical computing environments (we use R,, see http://www.r-project.org).", "labels": [], "entities": [{"text": "decision tree learner RPART", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.5512298718094826}]}, {"text": "We used the standard pruning and control settings for RPART (cp=0.0001, minsplit=20, minbucket=7).", "labels": [], "entities": [{"text": "RPART", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.4561088979244232}]}, {"text": "All results reported were obtained by performing 20-fold crossvalidation.", "labels": [], "entities": []}, {"text": "In the prediction phase, the trained classifier is exposed to unlabeled instances of test data.", "labels": [], "entities": []}, {"text": "The classifier's task is to label each instance.", "labels": [], "entities": []}, {"text": "When an instance is labeled as coreferring, the IDs of the anaphor and antecedent are kept in a response list for the evaluation according to.", "labels": [], "entities": []}, {"text": "For determining the relevant feature set we followed an iterative procedure similar to the wrapper approach for feature selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7449601590633392}]}, {"text": "We start with a model based on a set of predefined baseline features.", "labels": [], "entities": []}, {"text": "Then we train models combining the baseline with all additional features separately.", "labels": [], "entities": []}, {"text": "We choose the best performing feature (fmeasure according to), adding it to the model.", "labels": [], "entities": []}, {"text": "We then train classifiers combining the enhanced model with each of the remaining features separately.", "labels": [], "entities": []}, {"text": "We again choose the best performing classifier and add the corresponding new feature to the model.", "labels": [], "entities": []}, {"text": "This process is repeated as long as significant improvement can be observed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of npform Feature on Markables (w/o 1st and 2nd Persons)", "labels": [], "entities": []}, {"text": " Table 2: Distribution of Agreement Feature on Pronominal Expressions", "labels": [], "entities": []}, {"text": " Table 4: Results for Third Person Masculine and Feminine Pronouns (3mf)", "labels": [], "entities": [{"text": "Third Person Masculine and Feminine Pronouns", "start_pos": 22, "end_pos": 66, "type": "TASK", "confidence": 0.7154109279314677}]}, {"text": " Table 5: Results for Third Person Neuter Pronouns (3n)", "labels": [], "entities": []}, {"text": " Table 6: Results for Third Person Plural Pronouns (3p)", "labels": [], "entities": []}]}