{"title": [{"text": "A speech interface for open-domain question-answering", "labels": [], "entities": []}], "abstractContent": [{"text": "Speech interfaces to question-answering systems offer significant potential for finding information with phones and mobile networked devices.", "labels": [], "entities": []}, {"text": "We describe a demonstration of spoken question answering using a commercial dictation engine whose language models we have cus-tomized to questions, a Web-based text-prediction interface allowing quick correction of errors, and an open-domain question-answering system, AnswerBus, which is freely available on the Web.", "labels": [], "entities": [{"text": "spoken question answering", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6576097508271536}]}, {"text": "We describe a small evaluation of the effect of recognition errors on the precision of the answers returned and make some concrete recommendations for modifying a question-answering system for improving robustness to spoken input.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.995046854019165}]}], "introductionContent": [{"text": "This paper demonstrates a multimodal interface for asking questions and retrieving a set of likely answers.", "labels": [], "entities": []}, {"text": "Such an interface is particularly appropriate for mobile networked devices with screens that are too small to display general Web pages and documents.", "labels": [], "entities": []}, {"text": "Palm and Pocket PC devices, whose screens commonly display 10-15 lines, are candidates.", "labels": [], "entities": []}, {"text": "argue that for such devices question-answering is more appropriate than traditional document retrieval.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.6827008277177811}]}, {"text": "But until recently no method has existed for inputting questions in a reasonable amount of time.", "labels": [], "entities": [{"text": "inputting questions", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.9076841771602631}]}, {"text": "The study of concludes that questions tend to have a limited lexical structure that can be exploited for accurate speech recognition or text prediction.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.6905732303857803}, {"text": "text prediction", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.6828038543462753}]}, {"text": "In this demonstration we test whether this result can endow areal spoken question answering system with acceptable precision.", "labels": [], "entities": [{"text": "areal spoken question answering", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.5874966531991959}, {"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.997651994228363}]}, {"text": "at Xerox labs built one of the earliest spoken information retrieval systems, with a speaker-dependent isolated-word speech recognizer and an electronic encyclopedia.", "labels": [], "entities": [{"text": "spoken information retrieval", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6438244879245758}, {"text": "speaker-dependent isolated-word speech recognizer", "start_pos": 85, "end_pos": 134, "type": "TASK", "confidence": 0.5838565677404404}]}, {"text": "One reason they reported for the success of their system was their use of simple language models to exploit the observation that pairs of words co-occurring in a document source are likely to bespoken together as keywords in a query.", "labels": [], "entities": []}, {"text": "Later research at CMU built upon similar intuition by deriving the languagemodel of their Sphinx-II speech recognizer from the searched document source.", "labels": [], "entities": [{"text": "Sphinx-II speech recognizer", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.5218570729096731}]}, {"text": "Colineau and others (1999) developed a system as apart of the THISL project for retrieval from broadcast news to respond to news-related queries such as What do you have on . .", "labels": [], "entities": []}, {"text": "? and I am doing a report on . .", "labels": [], "entities": []}, {"text": "-can you help me?", "labels": [], "entities": []}, {"text": "The queries the authors addressed had a simple structure, and they successfully modelled them in two parts: a question-frame, for which they handwrote grammar rules; and a content-bearing string of keywords, for which they fitted standard lexical language-models from the news collection.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the accuracy of the system subject to spoken input using 200 test questions from the TREC 2002 QA track).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9992407560348511}, {"text": "TREC 2002 QA track", "start_pos": 98, "end_pos": 116, "type": "DATASET", "confidence": 0.9483387321233749}]}, {"text": "AnswerBus returns snippets from Web pages containing possible answers; we compared these with the refer- The interface for rapidly typing questions and correcting mistranscriptions from speech.", "labels": [], "entities": []}, {"text": "Available at speech.ftw.at/\u02dcejs/ answerbus ence answers used in the TREC competition, overriding about 5 negative judgments when we felt the answers were satisfactory but absent from the TREC scorecard.", "labels": [], "entities": [{"text": "TREC competition", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.7096289992332458}, {"text": "TREC scorecard", "start_pos": 187, "end_pos": 201, "type": "DATASET", "confidence": 0.9589233100414276}]}, {"text": "For each of these 200 questions we passed two strings to the AnswerBus engine, one typed verbatim, the other transcribed from the speech of one of the people described above.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: % of questions answered correctly from  perfect text versus misrecognized speech.", "labels": [], "entities": []}, {"text": " Table 3: # of answers degraded or improved by the  dodgy input.", "labels": [], "entities": []}]}