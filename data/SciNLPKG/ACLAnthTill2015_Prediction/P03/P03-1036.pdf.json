{"title": [{"text": "Unsupervised Segmentation of Words Using Prior Distributions of Morph Length and Frequency", "labels": [], "entities": [{"text": "Unsupervised Segmentation of Words", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7437799274921417}]}], "abstractContent": [{"text": "We present a language-independent and unsupervised algorithm for the segmenta-tion of words into morphs.", "labels": [], "entities": []}, {"text": "The algorithm is based on anew generative probabilis-tic model, which makes use of relevant prior information on the length and frequency distributions of morphs in a language.", "labels": [], "entities": []}, {"text": "Our algorithm is shown to out-perform two competing algorithms, when evaluated on data from a language with agglutinative morphology (Finnish), and to perform well also on English data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to artificially \"understand\" or produce natural language, a system presumably has to know the elementary building blocks, i.e., the lexicon, of the language.", "labels": [], "entities": []}, {"text": "Additionally, the system needs to model the relations between these lexical units.", "labels": [], "entities": []}, {"text": "Many existing NLP (natural language processing) applications make use of words as such units.", "labels": [], "entities": []}, {"text": "For instance, in statistical language modelling, probabilities of word sequences are typically estimated, and bag-of-word models are common in information retrieval.", "labels": [], "entities": [{"text": "statistical language modelling", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7388765811920166}]}, {"text": "However, for some languages it is infeasible to construct lexicons for NLP applications, if the lexicons contain entire words.", "labels": [], "entities": []}, {"text": "In especially agglutinative languages, 1 such as Finnish and Turkish, the number of possible different word forms is simply too high.", "labels": [], "entities": []}, {"text": "For example, in Finnish, a single verb may appear in thousands of different forms.", "labels": [], "entities": []}, {"text": "According to linguistic theory, words are built from smaller units, morphemes.", "labels": [], "entities": []}, {"text": "Morphemes are the smallest meaning-bearing elements of language and could be used as lexical units instead of entire words.", "labels": [], "entities": []}, {"text": "However, the construction of a comprehensive morphological lexicon or analyzer based on linguistic theory requires a considerable amount of work by experts.", "labels": [], "entities": []}, {"text": "This is both time-consuming and expensive and hardly applicable to all languages.", "labels": [], "entities": []}, {"text": "Furthermore, as language evolves the lexicon must be updated continuously in order to remain up-to-date.", "labels": [], "entities": []}, {"text": "Alternatively, an interesting field of research lies open: Minimally supervised algorithms can be designed that automatically discover morphemes or morpheme-like units from data.", "labels": [], "entities": []}, {"text": "There exist a number of such algorithms, some of which are entirely unsupervised and others that use some knowledge of the language.", "labels": [], "entities": []}, {"text": "In the following, we discuss recent unsupervised algorithms and refer the reader to) fora comprehensive survey of previous research in the whole field.", "labels": [], "entities": []}, {"text": "Many algorithms proceed by segmenting (i.e., splitting) words into smaller components.", "labels": [], "entities": []}, {"text": "Often the limiting assumption is made that words consist of only one stem followed by one (possibly empty) suffix).", "labels": [], "entities": []}, {"text": "This limitation is reduced in) by allowing a recursive structure, where stems can have inner structure, so that they in turn consist of a substem and a suffix.", "labels": [], "entities": []}, {"text": "However, for languages with agglutinative morphology this may not be enough.", "labels": [], "entities": []}, {"text": "In Finnish, a word can consist of lengthy sequences of alternating stems and affixes.", "labels": [], "entities": []}, {"text": "Some morphology discovery algorithms learn relationships between words by comparing the orthographic or semantic similarity of the words).", "labels": [], "entities": [{"text": "morphology discovery", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.7234335839748383}]}, {"text": "Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages.", "labels": [], "entities": []}, {"text": "We previously presented two segmentation algorithms suitable for agglutinative languages).", "labels": [], "entities": []}, {"text": "The algorithms learn a set of segments, which we call morphs, from a corpus.", "labels": [], "entities": []}, {"text": "Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.7357534915208817}, {"text": "word discovery", "start_pos": 151, "end_pos": 165, "type": "TASK", "confidence": 0.7398501634597778}]}, {"text": "However, we observed that for the corpus size studied (100 000 words), our two algorithms were somewhat prone to excessive segmentation of words.", "labels": [], "entities": []}, {"text": "In this paper, we aim at overcoming the problem of excessive segmentation, particularly when small corpora (up to 200 000 words) are used for training.", "labels": [], "entities": []}, {"text": "We present anew segmentation algorithm, which is language independent and works in an unsupervised fashion.", "labels": [], "entities": []}, {"text": "Since the results obtained suggest that the algorithm performs rather well, it could possibly be suitable for languages for which only small amounts of written text are available.", "labels": [], "entities": []}, {"text": "The model is formulated in a probabilistic Bayesian framework.", "labels": [], "entities": []}, {"text": "It makes use of explicit prior information in the form of probability distributions for morph length and morph frequency.", "labels": [], "entities": []}, {"text": "The model is based on the same kind of reasoning as the probabilistic model in.", "labels": [], "entities": []}, {"text": "While Brent's model displays a prior probability that exponentially decreases with word length (with one character as the most common length), our model uses a probability distribution that more accurately models the real length distribution.", "labels": [], "entities": []}, {"text": "Also Brent's frequency distribution differs from ours, which we derive from Mandelbrot's correction of Zipf's law (cf. Section 2.5).", "labels": [], "entities": []}, {"text": "Our model requires that the values of two parameters be set: (i) our prior belief of the most common morph length, and (ii) our prior belief of the proportion of morph types 2 that occur only once in the corpus.", "labels": [], "entities": []}, {"text": "These morph types are called hapax legomena.", "labels": [], "entities": []}, {"text": "While the former is a rather intuitive measure, the latter may not appear as intuitive.", "labels": [], "entities": []}, {"text": "However, the proportion of hapax legomena maybe interpreted as a measure of the richness of the text.", "labels": [], "entities": []}, {"text": "Also note that since the most common morph length is calculated for morph types, not tokens, it is not independent of the corpus size.", "labels": [], "entities": []}, {"text": "A larger corpus usually requires a higher average morph length, a fact that is stated for word lengths in.", "labels": [], "entities": []}, {"text": "As an evaluation criterion for the performance of our method and two reference methods we use a measure that reflects the ability to recognize real morphemes of the language by examining the morphs found by the algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "From the point of view of linguistic theory, it is possible to come up with different plausible suggestions for the correct location of morpheme boundaries.", "labels": [], "entities": []}, {"text": "Some of the solutions maybe more elegant than others, 8 but it is difficult to say if the most elegant scheme will work best in practice, when real NLP applications are concerned.", "labels": [], "entities": []}, {"text": "We utilize an evaluation method for segmentation of words presented in ().", "labels": [], "entities": []}, {"text": "In this method, segments are not compared to one single \"correct\" segmentation.", "labels": [], "entities": []}, {"text": "The evaluation criterion can rather be interpreted from the point of view of language \"understanding\".", "labels": [], "entities": []}, {"text": "A morph discovered by the segmentation algorithm is considered to be \"understood\", if there is a low-ambiguity mapping from the morph to a corresponding morpheme.", "labels": [], "entities": []}, {"text": "Alternatively, a morph may correspond to a sequence of morphemes, if these morphemes are very likely to occur together.", "labels": [], "entities": []}, {"text": "The idea is that if an entirely new word form is encountered, the system will \"understand\" it by decomposing it into morphs that it \"understands\".", "labels": [], "entities": []}, {"text": "A segmentation algorithm that segments words into too small parts will perform poorly due to high ambiguity.", "labels": [], "entities": []}, {"text": "At the other extreme, an algorithm that is reluctant at splitting words will have bad generalization ability to new word forms.", "labels": [], "entities": []}, {"text": "Reference morpheme sequences for the words are obtained using existing software for automatic morphological analysis based on the two-level morphology of.", "labels": [], "entities": []}, {"text": "For each word form, the analyzer outputs the base form of the word together with grammatical tags.", "labels": [], "entities": []}, {"text": "By filtering the output, we get a sequence of morpheme labels that appear in the correct order and represent correct morphemes rather closely.", "labels": [], "entities": []}, {"text": "Note, however, that the morpheme labels are not necessarily orthographically similar to the morphemes they represent.", "labels": [], "entities": []}, {"text": "The exact procedure for evaluating the segmentation of a set of words consists of the following steps: (1) Segment the words in the corpus using the automatic segmentation algorithm.", "labels": [], "entities": [{"text": "evaluating the segmentation of a set of words", "start_pos": 24, "end_pos": 69, "type": "TASK", "confidence": 0.7425985373556614}]}, {"text": "(2) Divide the segmented data into two parts of equal size.", "labels": [], "entities": []}, {"text": "Collect all segmented word forms from the first part into a training vocabulary and collect all segmented word forms from the second part into a test vocabulary.", "labels": [], "entities": []}, {"text": "(3) Align the segmentation of the words in the training vocabulary with the corresponding reference morpheme label sequences.", "labels": [], "entities": []}, {"text": "Each morph must be aligned with one or more consecutive morpheme labels and each morpheme label must be aligned with at least one morph; e.g., fora hypothetical segmentation of the English word winners': Morpheme labels win -ER PL GEN Morph sequence w inn er s' (4) Estimate conditional probabilities for the morph/morpheme mappings computed over the whole training vocabulary: p(morpheme | morph).", "labels": [], "entities": [{"text": "ER PL GEN Morph sequence", "start_pos": 225, "end_pos": 249, "type": "METRIC", "confidence": 0.8377463579177856}]}, {"text": "Re-align using the Viterbi algorithm and employ the Expectation-Maximization algorithm iteratively until convergence of the probabilities.", "labels": [], "entities": [{"text": "Expectation-Maximization", "start_pos": 52, "end_pos": 76, "type": "METRIC", "confidence": 0.9886425137519836}]}, {"text": "(5) The quality of the segmentation is evaluated on the test vocabulary.", "labels": [], "entities": []}, {"text": "The segmented words in the test vocabulary are aligned against their reference morpheme label sequences according to the conditional probabilities learned from the training vocabulary.", "labels": [], "entities": []}, {"text": "To measure the quality of the segmentation we compute the expectation of the proportion of correct mappings from morphs to morpheme labels, E{p(morpheme | morph)}: where N is the number of morph/morpheme mappings, and pi (\u00b7) is the probability associated with the ith mapping.", "labels": [], "entities": []}, {"text": "Thus, we measure the proportion of morphemes in the test vocabulary that we can expect to recognize correctly by examining the morph segments.", "labels": [], "entities": []}, {"text": "9  We have conducted experiments involving (i) three different segmentation algorithms, (ii) two corpora in different languages (Finnish and English), and (iii) data sizes ranging from 2000 words to 200 000 words.", "labels": [], "entities": []}], "tableCaptions": []}