{"title": [{"text": "Loosely Tree-Based Alignment for Machine Translation", "labels": [], "entities": [{"text": "Loosely Tree-Based Alignment", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.576667716105779}, {"text": "Machine Translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7465086579322815}]}], "abstractContent": [{"text": "We augment a model of translation based on reordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.", "labels": [], "entities": []}, {"text": "This is done by adding anew subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.", "labels": [], "entities": [{"text": "tree-to-tree alignment", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7239046096801758}]}], "introductionContent": [{"text": "Systems for automatic translation between languages have been divided into transfer-based approaches, which rely on interpreting the source string into an abstract semantic representation from which text is generated in the target language, and statistical approaches, pioneered by, which estimate parameters fora model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.", "labels": [], "entities": [{"text": "automatic translation between languages", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.7601794451475143}]}, {"text": "Only recently have hybrid approaches begun to emerge, which apply probabilistic models to a structured representation of the source text.", "labels": [], "entities": []}, {"text": "showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution.", "labels": [], "entities": []}, {"text": "also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer.", "labels": [], "entities": []}, {"text": "present an algorithm for estimating probabilistic parameters fora similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.", "labels": [], "entities": []}, {"text": "The use of explicit syntactic information for the target language in this model has led to excellent translation results, and raises the prospect of training a statistical system using syntactic information for both sides of the parallel corpus.", "labels": [], "entities": [{"text": "translation", "start_pos": 101, "end_pos": 112, "type": "TASK", "confidence": 0.9675776362419128}]}, {"text": "Tree-to-tree alignment techniques such as probabilistic tree substitution grammars) can be trained on parse trees from parallel treebanks.", "labels": [], "entities": [{"text": "Tree-to-tree alignment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7021209597587585}]}, {"text": "However, real bitexts generally do not exhibit parse-tree isomorphism, whether because of systematic differences between how languages express a concept syntactically, or simply because of relatively free translations in the training material.", "labels": [], "entities": []}, {"text": "In this paper, we introduce \"loosely\" tree-based alignment techniques to address this problem.", "labels": [], "entities": []}, {"text": "We present analogous extensions for both tree-to-string and tree-to-tree models that allow alignments not obeying the constraints of the original syntactic tree (or tree pair), although such alignments are dispreferred because they incur a cost in probability.", "labels": [], "entities": []}, {"text": "This is achieved by introducing a clone operation, which copies an entire subtree of the source language syntactic structure, moving it anywhere in the target language sentence.", "labels": [], "entities": []}, {"text": "Careful parameterization of the probability model allows it to be estimated at no additional cost in computational complexity.", "labels": [], "entities": []}, {"text": "We expect our relatively unconstrained clone operation to allow for various types of structural divergence by providing a sort of hybrid between tree-based and unstructured, IBM-style models.", "labels": [], "entities": []}, {"text": "We first present the tree-to-string model, followed by the tree-to-tree model, before moving onto alignment results fora parallel syntactically annotated Korean-English corpus, measured in terms of alignment perplexities on held-out test data, and agreement with human-annotated word-level alignments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our translation models both in terms agreement with human-annotated word-level alignments between the sentence pairs.", "labels": [], "entities": []}, {"text": "For scoring the viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of, which measures agreement at the level of pairs of words: 1  where A is the set of word pairs aligned by the automatic system, and G the set aligned in the gold standard.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 104, "end_pos": 130, "type": "METRIC", "confidence": 0.9152400294939677}]}, {"text": "We provide a comparison of the tree-based models with the sequence of successively more complex models of.", "labels": [], "entities": []}, {"text": "The error rates shown in represent the minimum over training iterations; training was stopped for each model when error began to increase.", "labels": [], "entities": []}, {"text": "IBM Models 1, 2, and 3 refer to.", "labels": [], "entities": []}, {"text": "\"Tree-to-String\" is the model of, and \"Tree-to-String, Clone\" allows the node cloning operation of Section 2.1.", "labels": [], "entities": []}, {"text": "\"Tree-to-Tree\" indicates the model of Section 3, while \"Tree-to-Tree, Clone\" adds the node cloning operation of Section 3.1.", "labels": [], "entities": []}, {"text": "Model 2 is initialized from the parameters of Model 1, and Model 3 is initialized from Model 2.", "labels": [], "entities": []}, {"text": "The lexical translation probabilities Pt (f |e) for each of our tree-based models are initialized from Model 1, and the node re-ordering probabilities are initialized uniformly.", "labels": [], "entities": []}, {"text": "shows the viterbi alignment produced by the \"Tree-to-String, Clone\" system on one sentence from our test set.", "labels": [], "entities": []}, {"text": "We found better agreement with the human alignments when fixing P ins (left) in the Tree-to-String model to a constant rather than letting it be determined through the EM training.", "labels": [], "entities": []}, {"text": "While the model learned by EM tends to overestimate the total number of aligned word pairs, fixing a higher probability for insertions results in fewer total aligned pairs and therefore a better trade-off between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.9988697171211243}, {"text": "recall", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.995072066783905}]}, {"text": "As seen for other tasks, the likelihood criterion used in EM training may not be optimal when evaluating a system against human labeling.", "labels": [], "entities": [{"text": "likelihood criterion", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.9646225571632385}, {"text": "EM training", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.9164199531078339}]}, {"text": "The approach of optimizing a small number of metaparameters has been applied to machine translation by.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7515904605388641}]}, {"text": "It is likely that the IBM models could similarly be optimized to minimize alignment error -an open question is whether the optimization with respect to alignment error will correspond to optimization for translation accuracy.", "labels": [], "entities": [{"text": "alignment error", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.7991935908794403}, {"text": "translation", "start_pos": 204, "end_pos": 215, "type": "TASK", "confidence": 0.9487621784210205}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.43210676312446594}]}, {"text": "Within the strict EM framework, we found roughly equivalent performance between the IBM models and the two tree-based models when making use of the cloning operation.", "labels": [], "entities": []}, {"text": "For both the tree-tostring and tree-to-tree models, the cloning operation improved results, indicating that adding the flexibility to handle structural divergence is important when using syntax-based models.", "labels": [], "entities": []}, {"text": "The improvement was particularly significant for the tree-to-tree model, because using syntactic trees on both sides of the translation pair, while desirable as an additional source of information, severely constrains possible alignments unless the cloning operation is allowed.", "labels": [], "entities": []}, {"text": "The tree-to-tree model has better theoretical complexity than the tree-to-string model, being quadratic rather than quartic in sentence length, and we found this to be a significant advantage in practice.", "labels": [], "entities": []}, {"text": "This improvement in speed allows longer sentences and more data to be used in training syntax-based models.", "labels": [], "entities": [{"text": "speed", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.981792151927948}]}, {"text": "We found that when training on sentences of up 60 words, the tree-to-tree alignment was 20 times faster than tree-to-string alignment.", "labels": [], "entities": []}, {"text": "For reasons of speed, limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus.", "labels": [], "entities": [{"text": "speed", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9692268371582031}]}], "tableCaptions": [{"text": " Table 2: Alignment error rate on Korean-English corpus", "labels": [], "entities": [{"text": "Alignment error rate", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7453162968158722}]}]}