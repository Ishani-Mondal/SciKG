{"title": [{"text": "An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition", "labels": [], "entities": [{"text": "Improved Extraction Pattern Representation", "start_pos": 3, "end_pos": 45, "type": "TASK", "confidence": 0.5950390547513962}, {"text": "IE Pattern Acquisition", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9424357215563456}]}], "abstractContent": [{"text": "Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction.", "labels": [], "entities": [{"text": "automatic unsupervised acquisition of patterns", "start_pos": 47, "end_pos": 93, "type": "TASK", "confidence": 0.7254050254821778}, {"text": "information extraction", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8232668042182922}]}, {"text": "Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain.", "labels": [], "entities": []}, {"text": "The effect of these alternative models has not been previously studied.", "labels": [], "entities": []}, {"text": "In this paper, we compare the prior models and introduce anew model, the Subtree model, based on arbitrary sub-trees of dependency trees.", "labels": [], "entities": []}, {"text": "We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns.", "labels": [], "entities": [{"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9984835982322693}]}], "introductionContent": [{"text": "Information Extraction (IE) is the process of identifying events or actions of interest and their participating entities from a text.", "labels": [], "entities": [{"text": "Information Extraction (IE) is the process of identifying events or actions of interest and their participating entities from a text", "start_pos": 0, "end_pos": 132, "type": "Description", "confidence": 0.7625900276682593}]}, {"text": "As the field of IE has developed, the focus of study has moved towards automatic knowledge acquisition for information extraction, including domain-specific lexicons) and extraction patterns).", "labels": [], "entities": [{"text": "IE", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9906840324401855}, {"text": "automatic knowledge acquisition", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6716669201850891}, {"text": "information extraction", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7324035912752151}]}, {"text": "In particular, methods have recently emerged for the acquisition of event extraction patterns without corpus annotation in view of the cost of manual labor for annotation.", "labels": [], "entities": [{"text": "acquisition of event extraction patterns", "start_pos": 53, "end_pos": 93, "type": "TASK", "confidence": 0.7919432163238526}]}, {"text": "However, there has been little study of alternative representation models of extraction patterns for unsupervised acquisition.", "labels": [], "entities": []}, {"text": "In the prior work on extraction pattern acquisition, the representation model of the patterns was based on a fixed set of pattern templates, or predicate-argument relations, such as subject-verb, and object-verb ().", "labels": [], "entities": [{"text": "extraction pattern acquisition", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.7804116010665894}]}, {"text": "The model of our previous work () was based on the paths from predicate nodes in dependency trees.", "labels": [], "entities": []}, {"text": "In this paper, we discuss the limitations of prior extraction pattern representation models in relation to their ability to capture the participating entities in scenarios.", "labels": [], "entities": [{"text": "prior extraction pattern representation", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.71220862865448}]}, {"text": "We present an alternative model based on subtrees of dependency trees, so as to extract entities beyond direct predicate-argument relations.", "labels": [], "entities": []}, {"text": "An evaluation on scenario-template tasks shows that the proposed Subtree model outperforms the previous models.", "labels": [], "entities": []}, {"text": "Section 2 describes the Subtree model for extraction pattern representation.", "labels": [], "entities": [{"text": "extraction pattern representation", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.828701376914978}]}, {"text": "Section 3 shows the method for automatic acquisition.", "labels": [], "entities": [{"text": "automatic acquisition", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7787033915519714}]}, {"text": "Section 4 gives the experimental results of the comparison to other methods and Section 5 presents an analysis of these results.", "labels": [], "entities": []}, {"text": "Finally, Section 6 provides some concluding remarks and perspective on future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment of this study is focused on comparing the performance of the earlier extraction pattern models to the proposed Subtree Model (SUBT).", "labels": [], "entities": []}, {"text": "The compared models are the direct predicate-argument model (PA) , and the Chain model (CH) in ().", "labels": [], "entities": []}, {"text": "The task for this experiment is entity extraction, which is to identify all the entities participating in relevant events in a set of given Japanese texts.", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7918874323368073}]}, {"text": "Note that all NEs in the test documents were identified manually, so that the task can measure only how well extraction patterns can distinguish the participating entities from the entities that are not related to any events.", "labels": [], "entities": []}, {"text": "This task does not involve grouping entities associated with the same event into a single template to avoid possible effect of merging failure on extraction performance for entities.", "labels": [], "entities": []}, {"text": "We accumulated the test set of documents of two scenarios; the Management Succession scenario of, with a simpler template structure, where corporate managers assumed and/or left their posts, and the Murderer Arrest scenario, where a law enforcement organization arrested a murder suspect.", "labels": [], "entities": [{"text": "Management Succession", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7139782607555389}, {"text": "Murderer Arrest", "start_pos": 199, "end_pos": 214, "type": "TASK", "confidence": 0.7971916198730469}]}, {"text": "The source document set from which the extraction patterns are learned consists of 117,109 Mainichi Newspaper articles from 1995.", "labels": [], "entities": [{"text": "Mainichi Newspaper articles from 1995", "start_pos": 91, "end_pos": 128, "type": "DATASET", "confidence": 0.9786411881446838}]}, {"text": "All the sentences are morphologically analyzed by JU-MAN ( and converted into dependency trees by.", "labels": [], "entities": [{"text": "JU-MAN", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.819130539894104}]}, {"text": "Regardless of the model of extraction patterns, the pattern acquisition follows the procedure described in Section 3.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7400097846984863}]}, {"text": "We retrieved 300 documents as a relevant document set.", "labels": [], "entities": []}, {"text": "The association of NE classes and slots in the template is made automatically; Person, Organization, Post (slots) correspond to C-PERSON, C-ORG, C-POST (NE-classes), respectively, in the Succession scenario, and Suspect, Arresting Agency, Charge (slots) correspond to C-PERSON, C-ORG, C-OFFENCE (NE-classes), respectively, in the Ar-  For each model, we get a list of the pattern candidates ordered by the ranking function discussed in Section 3.3 after filtering.", "labels": [], "entities": [{"text": "Ar", "start_pos": 330, "end_pos": 332, "type": "METRIC", "confidence": 0.9886101484298706}]}, {"text": "The result of the performance is shown as a precision-recall graph for each subset of top-ranked patterns where ranges from 1 to the number of the pattern candidates.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9906847476959229}]}, {"text": "The test set was accumulated from Mainichi Newspaper in 1996 by a simple keyword search, with some additional irrelevant documents.", "labels": [], "entities": [{"text": "Mainichi Newspaper in 1996", "start_pos": 34, "end_pos": 60, "type": "DATASET", "confidence": 0.962515115737915}]}, {"text": "(See for detail.)(a) shows the precision-recall curve of top-relevant extraction patterns for each model on the Succession Scenario.", "labels": [], "entities": [{"text": "precision-recall curve", "start_pos": 31, "end_pos": 53, "type": "METRIC", "confidence": 0.9756483733654022}]}, {"text": "At lower recall levels (up to 35%), all the models performed similarly.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.999612033367157}]}, {"text": "However, the precision of Chain patterns dropped suddenly by 20% at recall level 38%, while the SUBT patterns keep the precision significantly higher than Chain patterns until it reaches 58% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.999448835849762}, {"text": "recall level", "start_pos": 68, "end_pos": 80, "type": "METRIC", "confidence": 0.9738061130046844}, {"text": "precision", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9978773593902588}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.998286783695221}]}, {"text": "Even after SUBT hit the drop at 56%, SUBT is consistently a few percent higher in precision than Chain patterns for most recall levels.(a) also shows that although PA keeps high precision at low recall level it has a significantly lower ceiling of recall (52%) compared to other models.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9992703795433044}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9923646450042725}, {"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9900508522987366}, {"text": "recall", "start_pos": 195, "end_pos": 201, "type": "METRIC", "confidence": 0.9843927621841431}, {"text": "recall", "start_pos": 248, "end_pos": 254, "type": "METRIC", "confidence": 0.9677266478538513}]}, {"text": "shows the extraction performance on Since there is no subcategory of C-PERSON to distinguish Suspect and victim (which is not extracted in this experiment) for the Arrest scenario, the learned pattern candidates may extract victims as Suspect entities by mistake.", "labels": [], "entities": []}, {"text": "Again, the PredicateArgument model has a much lower recall ceiling (25%).", "labels": [], "entities": [{"text": "recall ceiling", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.9951690137386322}]}, {"text": "The difference in the performance between the Subtree model and the Chain model does not seem as obvious as in the Succession task.", "labels": [], "entities": []}, {"text": "However, it is still observable that the Subtree model gains a few percent precision over the Chain model at recall levels around 40%.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9992997646331787}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9989166259765625}]}, {"text": "A possible explanation of the subtleness in performance difference in this scenario is the smaller number of contributing patterns compared to the Succession scenario.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task Description and Statistics of Test Data", "labels": [], "entities": []}]}