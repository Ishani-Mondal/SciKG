{"title": [{"text": "Counter-Training in Discovery of Semantic Patterns", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a method for unsu-pervised discovery of semantic patterns.", "labels": [], "entities": [{"text": "unsu-pervised discovery of semantic patterns", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.6694331347942353}]}, {"text": "Semantic patterns are useful fora variety of text understanding tasks, in particular for locating events in text for information extraction.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7957850098609924}, {"text": "information extraction", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.8236577212810516}]}, {"text": "The method builds upon previously described approaches to iterative unsupervised pattern acquisition.", "labels": [], "entities": [{"text": "iterative unsupervised pattern acquisition", "start_pos": 58, "end_pos": 100, "type": "TASK", "confidence": 0.6621340289711952}]}, {"text": "One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9970303773880005}]}, {"text": "Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7740440666675568}]}, {"text": "This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9978269934654236}]}, {"text": "We discuss the results of experiments with several scenarios , and examine different aspects of the new procedure.", "labels": [], "entities": []}], "introductionContent": [{"text": "The work described in this paper is motivated by research into automatic pattern acquisition.", "labels": [], "entities": [{"text": "automatic pattern acquisition", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.6207208335399628}]}, {"text": "Pattern acquisition is considered important fora variety of \"text understanding\" tasks, though our particular reference will be to Information Extraction (IE).", "labels": [], "entities": [{"text": "Pattern acquisition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9266749620437622}, {"text": "text understanding\"", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8158827622731527}, {"text": "Information Extraction (IE)", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.8205502152442932}]}, {"text": "In IE, the objective is to search through text for entities and events of a particular kind-corresponding to the user's interest.", "labels": [], "entities": [{"text": "IE", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9706482291221619}]}, {"text": "Many current systems achieve this by pattern matching.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.8459876477718353}]}, {"text": "The problem of recall, or coverage, in IE can then be restated to a large extent as a problem of acquiring a comprehensive set of good patterns which are relevant to the scenario of interest, i.e., which describe events occurring in this scenario.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9981514811515808}, {"text": "coverage", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9263488054275513}, {"text": "IE", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.981590747833252}]}, {"text": "Among the approaches to pattern acquisition recently proposed, unsupervised methods 1 have gained some popularity, due to the substantial reduction in amount of manual labor they require.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.9036026298999786}]}, {"text": "We build upon these approaches for learning IE patterns.", "labels": [], "entities": [{"text": "IE patterns", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.8810437321662903}]}, {"text": "The focus of this paper is on the problem of convergence in unsupervised methods.", "labels": [], "entities": []}, {"text": "As with a variety of related iterative, unsupervised methods, the output of the system is a stream of patterns, in which the quality is high initially, but then gradually degrades.", "labels": [], "entities": []}, {"text": "This degradation is inherent in the trade-off, or tension, in the scoring metrics: between trying to achieve higher recall vs. higher precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9964633584022522}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9923616647720337}]}, {"text": "Thus, when the learning algorithm is applied against a reference corpus, the result is a ranked list of patterns, and going down the list produces a curve which trades off precision for recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.9985670447349548}, {"text": "recall", "start_pos": 186, "end_pos": 192, "type": "METRIC", "confidence": 0.9957595467567444}]}, {"text": "Simply put, the unsupervised algorithm does not know when to stop learning.", "labels": [], "entities": []}, {"text": "In the absence of a good stopping criterion, the resulting list of patterns must be manually reviewed by a human; otherwise one can set ad-hoc thresholds, e.g., on the number of allowed iterations, as in (, or else to resort to supervised training to determine such thresholds-which is unsatisfactory when our goal from the outset is to try to limit supervision.", "labels": [], "entities": []}, {"text": "Thus, the lack of natural stopping criteria renders these algorithms less unsupervised than one would hope.", "labels": [], "entities": []}, {"text": "More importantly, this lack makes the algorithms difficult to use in settings where training must be completely automatic, such as in a generalpurpose information extraction system, where the topic may not be known in advance.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.6974424868822098}]}, {"text": "At the same time, certain unsupervised learning algorithms in other domains exhibit inherently natural stopping criteria.", "labels": [], "entities": []}, {"text": "One example is the algorithm for word sense disambiguation in.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7266068458557129}]}, {"text": "Of particular relevance to our method are the algorithms for semantic classification of names or NPs described in).", "labels": [], "entities": [{"text": "semantic classification of names or NPs", "start_pos": 61, "end_pos": 100, "type": "TASK", "confidence": 0.8326232135295868}]}, {"text": "Inspired in part by these algorithms, we introduce the counter-training technique for unsupervised pattern acquisition.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7029250264167786}]}, {"text": "The main idea behind countertraining is that several identical simple learners run simultaneously to compete with one another in different domains.", "labels": [], "entities": []}, {"text": "This yields an improvement in precision, and most crucially, it provides a natural indication to the learner when to stop learning-namely, once it attempts to wander into territory already claimed by other learners.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9995241165161133}]}, {"text": "We review the main features of the underlying unsupervised pattern learner and related work in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the algorithm; 3.2 gives the details of the basic learner, and 3.3 introduces the counter-training framework which is super-imposed on it.", "labels": [], "entities": []}, {"text": "We present the results with and without counter-training on several domains, Section 4, followed by discussion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the algorithm on documents from the Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.9157810111840566}]}, {"text": "The training corpus consisted of 15,000 articles from 3 months between 1992 and We used the scenarios shown in to compete with each other in different combinations.", "labels": [], "entities": []}, {"text": "The seed patterns for the scenarios, and the number of documents initially picked up by the seeds are shown in the table.", "labels": [], "entities": []}, {"text": "The seeds were kept small, and they yielded high precision; it is evident that these scenarios are represented to a varying degree within the corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9989410042762756}]}, {"text": "We also introduced an additional \"negative\" scenario (the row labeled \"Don't care\"), seeded with patterns for earnings reports and interest rate fluctuations.", "labels": [], "entities": []}, {"text": "The last column shows the number of iterations before learning stopped.", "labels": [], "entities": []}, {"text": "A sample of the discovered patterns 3 appears in.", "labels": [], "entities": []}, {"text": "For an indirect evaluation of the quality of the learned patterns, we employ the text-filtering evaluation strategy, as in (.", "labels": [], "entities": []}, {"text": "As a by-product of pattern acquisition, the algorithm acquires a set of relevant documents (more precisely, a distribution of document relevance weights  as a quantitative measure of the goodness of the patterns.", "labels": [], "entities": [{"text": "pattern acquisition", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7532508671283722}]}, {"text": "To conduct the text-filtering evaluation we need a binary relevance judgement for each document.", "labels": [], "entities": []}, {"text": "This is obtained as follows.", "labels": [], "entities": []}, {"text": "We introduce a cutoff threshold as non-relevant.", "labels": [], "entities": []}, {"text": "The results of the pattern learner for the \"Management Succession\" scenario, with and without counter-training, are shown in.", "labels": [], "entities": [{"text": "Management Succession\"", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.887310246626536}]}, {"text": "The test sub-corpus consists of the 100 MUC-6 documents.", "labels": [], "entities": [{"text": "MUC-6 documents", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9335567653179169}]}, {"text": "The initial seed yields about 15% recall at 86% precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996607303619385}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.994929850101471}]}, {"text": "The curve labeled Mono shows the performance of the baseline algorithm up to 150 iterations.", "labels": [], "entities": []}, {"text": "It stops learning good patterns after 60 iterations, at 73% recall, from which point precision drops.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9977941513061523}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995012283325195}]}, {"text": "The reason the recall appears to continue improving is that, after this point, the learner begins to acquire patterns describing secondary events, derivative of or commonly co-occurring with the focal topic.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9978311657905579}]}, {"text": "Examples of such events are fluctuations in stock prices, revenue estimates, and other common business news elements.", "labels": [], "entities": []}, {"text": "The Baseline 54% is the precision we would expect to get by randomly marking the documents as relevant to the scenario.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8210188150405884}, {"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9991752505302429}]}, {"text": "The performance of the Management Succession learner counter-trained against other learners is traced by the curve labeled Counter.", "labels": [], "entities": [{"text": "Counter", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.8808107972145081}]}, {"text": "It is important to recall that the counter-trained algorithm terminates at the final point on the curve, whereas the The relevance cut-off parameter, t j u w v 4 x was set to 0.3 for mono-trained experiments, and to 0.2 for counter-training.", "labels": [], "entities": [{"text": "The relevance cut-off parameter", "start_pos": 117, "end_pos": 148, "type": "METRIC", "confidence": 0.9449359774589539}]}, {"text": "These numbers were obtained from empirical trials, which suggest that a lower confidence is acceptable in the presence of negative evidence.", "labels": [], "entities": []}, {"text": "Internal relevance measures, , are maintained by the algorithm, and the external, binary measures are used only for evaluation of performance.", "labels": [], "entities": []}, {"text": "We checked the quality of the discovered patterns by hand.", "labels": [], "entities": []}, {"text": "Termination occurs at 142 iterations.", "labels": [], "entities": [{"text": "Termination", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7485721707344055}]}, {"text": "We observed that after iteration 103 only 10% of the patterns are \"good\", the rest are secondary.", "labels": [], "entities": []}, {"text": "However, in the first 103 iterations, over 90% of the patterns are good Management Succession patterns.", "labels": [], "entities": [{"text": "Management Succession", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.7952853739261627}]}, {"text": "In the same experiment the behaviour of the learner of the \"Legal Action\" scenario is shown in.", "labels": [], "entities": [{"text": "Legal Action\"", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.8847385247548422}]}, {"text": "The test corpus for this learner consists of 250 documents: the 100 MUC-6 training documents and 150 WSJ documents which we retrieved using a set of keywords and categorized manually.", "labels": [], "entities": [{"text": "MUC-6 training documents", "start_pos": 68, "end_pos": 92, "type": "DATASET", "confidence": 0.8258369167645773}]}, {"text": "The curves labeled Mono, Counter and Baseline are as in the preceding We observe that the counter-training termination point is near the mono-trained curve, and has a good recall-precision trade-off.", "labels": [], "entities": [{"text": "recall-precision", "start_pos": 172, "end_pos": 188, "type": "METRIC", "confidence": 0.9919330477714539}]}, {"text": "However, the improvement from counter-training is less pronounced here than for the Succession scenario.", "labels": [], "entities": [{"text": "Succession", "start_pos": 84, "end_pos": 94, "type": "TASK", "confidence": 0.9657912850379944}]}, {"text": "This is due to a subtle interplay between the combination of scenarios, their distribution in the corpus, and the choice of seeds.", "labels": [], "entities": []}, {"text": "We return to this in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Scenarios in Competition", "labels": [], "entities": []}]}