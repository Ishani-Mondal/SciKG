{"title": [{"text": "Offline Strategies for Online Question Answering: Answering Questions Before They Are Asked", "labels": [], "entities": [{"text": "Online Question Answering", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.5983843803405762}, {"text": "Answering Questions Before They Are Asked", "start_pos": 50, "end_pos": 91, "type": "TASK", "confidence": 0.8922539154688517}]}], "abstractContent": [{"text": "Recent work in Question Answering has focused on web-based systems that extract answers using simple lexico-syntactic patterns.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7738919258117676}]}, {"text": "We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions.", "labels": [], "entities": []}, {"text": "We evaluate our strategy on a challenging subset of questions, i.e. \"Who is \u2026\" questions, against a state of the art web-based Question Answering system.", "labels": [], "entities": [{"text": "Who is \u2026\" questions", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.6089213043451309}, {"text": "Question Answering", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7157078385353088}]}, {"text": "Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many of the recent advances in Question Answering have followed from the insight that systems can benefit by exploiting the redundancy of information in large corpora.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7889189720153809}]}, {"text": "describe using the vast amount of data available on the World Wide Web to achieve impressive performance with relatively simple techniques.", "labels": [], "entities": []}, {"text": "While the Web is a powerful resource, its usefulness in Question Answering is not without limits.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8014105558395386}]}, {"text": "The Web, while nearly infinite in content, is not a complete repository of useful information.", "labels": [], "entities": []}, {"text": "Most newspaper texts, for example, do not remain accessible on the Web for more than a few weeks.", "labels": [], "entities": []}, {"text": "Further, while Information Retrieval techniques are relatively successful at managing the vast quantity of text available on the Web, the exactness required of Question Answering systems makes them too slow and impractical for ordinary users.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.8028906583786011}, {"text": "exactness", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9577984809875488}, {"text": "Question Answering", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.7671617269515991}]}, {"text": "In order to combat these inadequacies, we propose a strategy in which information is extracted automatically from electronic texts offline, and stored for quick and easy access.", "labels": [], "entities": []}, {"text": "We borrow techniques from Text Mining in order to extract semantic relations (e.g., concept-instance relations) between lexical items.", "labels": [], "entities": [{"text": "Text Mining", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.7097052782773972}]}, {"text": "We enhance these techniques by increasing the yield and precision of the relations that we extract.", "labels": [], "entities": [{"text": "yield", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9930161237716675}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9978355765342712}]}, {"text": "Our strategy is to collect a large sample of newspaper text (15GB) and use multiple part of speech patterns to extract the semantic relations.", "labels": [], "entities": []}, {"text": "We then filter out the noise from these extracted relations using a machine-learned classifier.", "labels": [], "entities": []}, {"text": "This process generates a high precision repository of information that can be accessed quickly and easily.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9686088562011719}]}, {"text": "We test the feasibility of this strategy on one semantic relation and a challenging subset of questions, i.e., \"Who is \u2026\" questions, in which either a concept is presented and an instance is requested (e.g., \"Who is the mayor of Boston?\"), or an instance is presented and a concept is requested (e.g., \"Who is Jennifer Capriati?\").", "labels": [], "entities": []}, {"text": "By choosing this subset of questions we are able to focus only on answers given by concept-instance relationships.", "labels": [], "entities": []}, {"text": "While this paper examines only this type of relation, the techniques we propose are easily extensible to other question types.", "labels": [], "entities": []}, {"text": "Evaluations are conducted using a set of \"Who is \u2026\" questions collected over the period of a few months from the commercial question-based search engine www.askJeeves.com.", "labels": [], "entities": [{"text": "Who is \u2026\" questions", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.5788012966513634}]}, {"text": "We extract approximately 2,000,000 concept-instance relations from newspaper text using syntactic patterns and machine-learned filters (e.g., \"president Bill Clinton\" and \"Bill Clinton, president of the USA,\").", "labels": [], "entities": []}, {"text": "We then compare answers based on these relations to answers given by TextMap (), a state of the art web-based question answering system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.6927956789731979}]}, {"text": "Finally, we discuss the results of this evaluation and the implications and limitations of our strategy.", "labels": [], "entities": []}], "datasetContent": [{"text": "A large number of questions were collected over the period of a few months from www.askJeeves.com.", "labels": [], "entities": []}, {"text": "100 questions of the form \"Who is x\" were randomly selected from this set.", "labels": [], "entities": []}, {"text": "The questions queried concept-instance relations through both instance centered queries (e.g., \"Who is Jennifer Capriati?\") and concept centered queries (e.g., \"Who is the mayor of Boston?\").", "labels": [], "entities": []}, {"text": "Answers to these questions were then automatically generated both by look-up in the 2,000,000 extracted concept-instance pairs and by TextMap, a state of the art web-based Question Answering system which ranked among the top 10 systems in the TREC 11 Question Answering track ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.7672616541385651}, {"text": "TREC 11 Question Answering", "start_pos": 243, "end_pos": 269, "type": "TASK", "confidence": 0.8141150623559952}]}, {"text": "Although both systems supply multiple possible answers fora question, evaluations were conducted on only one answer.", "labels": [], "entities": []}, {"text": "For TextMap, this answer is just the output with highest confidence, i.e., the system's first answer.", "labels": [], "entities": [{"text": "TextMap", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8887143135070801}]}, {"text": "For the extracted instances, the answer was that concept-instance pair that appeared most frequently in the list of extracted examples.", "labels": [], "entities": []}, {"text": "If all pairs appear with equal frequency, a selection is made at random.", "labels": [], "entities": []}, {"text": "Answers for both systems are then classified by hand into three categories based upon their information content.", "labels": [], "entities": []}, {"text": "Answers that unequivocally identify an instance's celebrity (e.g., \"Jennifer Capriati is a tennis star\") are marked correct.", "labels": [], "entities": []}, {"text": "Answers that provide some, but insufficient, evidence to identify the instance's celebrity (e.g., \"Jennifer Capriati is a defending champion\") are marked partially correct.", "labels": [], "entities": []}, {"text": "Answers that provide no information to identify the instance's celebrity (e.g., \"Jennifer Capriati is a daughter\") are marked incorrect..", "labels": [], "entities": []}, {"text": "Example answers and judgments of a state of the art system and look-up method using extracted concept-instance pairs on questions collected online.", "labels": [], "entities": []}, {"text": "Ratings were judged as either correct (C), partially correct (P), or incorrect (I).", "labels": [], "entities": [{"text": "incorrect (I)", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.8195995390415192}]}], "tableCaptions": []}