{"title": [{"text": "Language Model Based Arabic Word Segmentation", "labels": [], "entities": [{"text": "Language Model Based Arabic Word Segmentation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.5613732437292734}]}], "abstractContent": [{"text": "We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme).", "labels": [], "entities": []}, {"text": "Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus.", "labels": [], "entities": []}, {"text": "The algorithm uses a trigram language model to determine the most probable morpheme sequence fora given input.", "labels": [], "entities": []}, {"text": "The language model is initially estimated from a small manually segmented corpus of about 110,000 words.", "labels": [], "entities": []}, {"text": "To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.9692662954330444}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9038895964622498}]}, {"text": "The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens.", "labels": [], "entities": [{"text": "Arabic word segmentation", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6291755040486654}, {"text": "exact match", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.8476373851299286}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.6384639739990234}]}, {"text": "We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphologically rich languages like Arabic present significant challenges to many natural language processing applications because a word often conveys complex meanings decomposable into several morphemes.", "labels": [], "entities": []}, {"text": "By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation () and information retrieval.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7742764353752136}, {"text": "information retrieval", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.8110219538211823}]}, {"text": "In this paper, we present a general word segmentation algorithm for handling inflectional morphology capable of segmenting a word into a prefix*-stemsuffix* sequence, using a small manually segmented corpus and a table of prefixes/suffixes of the language.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7579433917999268}]}, {"text": "We do not address Arabic infix morphology where many stems correspond to the same root with various infix variations; we treat all the stems of a common root as separate atomic units.", "labels": [], "entities": []}, {"text": "The use of a stem as a morpheme (unit of meaning) is better suited than the use of a root for the applications we are considering in information retrieval and machine translation (e.g. different stems of the same root translate into different English words.)", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.7538503408432007}, {"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7127192318439484}]}, {"text": "Examples of Arabic words and their segmentation into prefix*-stem-suffix* are given in, where '#' indicates a morpheme being a prefix, and '+' a suffix.", "labels": [], "entities": []}, {"text": "1 As shown in, a word may include multiple prefixes, as in \u202b\ufedf\ufede\ufede\ufede\u202c (l: for, Al: the), or multiple suffixes, as in \u202b\ufe97\ufeea\ufeea\ufeea\ufeea\u202c (t: feminine singular, h: his).", "labels": [], "entities": []}, {"text": "A word may also consist only of a stem, as in \u202b\u0627\ufedf\ufef0\ufef0\ufef0\u202c (AlY, to/towards).", "labels": [], "entities": []}, {"text": "The algorithm implementation involves (i) language model training on a morphemesegmented corpus, (ii) segmentation of input text into a sequence of morphemes using the language model parameters, and (iii) unsupervised acquisition of new stems from a large unsegmented corpus.", "labels": [], "entities": []}, {"text": "The only linguistic resources required include a small manually segmented corpus ranging from 20,000 words to 100,000 words, a table of prefixes and suffixes of the language and a large unsegmented corpus.", "labels": [], "entities": []}, {"text": "In Section 2, we discuss related work.", "labels": [], "entities": []}, {"text": "In Section 3, we describe the segmentation algorithm.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.9820072054862976}]}, {"text": "In Section 4, we discuss the unsupervised algorithm for new stem acquisition.", "labels": [], "entities": [{"text": "new stem acquisition", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6270380020141602}]}, {"text": "In Section 5, we present experimental results.", "labels": [], "entities": []}, {"text": "In Section 6, we summarize the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size.", "labels": [], "entities": [{"text": "segmentation error rate", "start_pos": 76, "end_pos": 99, "type": "METRIC", "confidence": 0.7690952022870382}]}, {"text": "Segmentation error rate is defined in (9).", "labels": [], "entities": [{"text": "Segmentation error rate", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.7790236671765646}]}], "tableCaptions": [{"text": " Table 2 shows examples of atomic (e.g. \u202b,\u0627\u0644\u202c  \u202b)\u0627\u062a\u202c and multi-component (e.g. \u202b,\u0648\ufe91\ufe8e\ufe8e\ufe8e\ufe8e\u0644\u202c", "labels": [], "entities": []}, {"text": " Table 5 Impact of Core Algorithm and LM Vocabulary Size on Segmentation Error Rate", "labels": [], "entities": [{"text": "LM Vocabulary Size", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.6690598130226135}]}, {"text": " Table 6 Language Model Vocabulary Size and Out of Vocabulary Ratio", "labels": [], "entities": [{"text": "Out", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9778791069984436}]}]}