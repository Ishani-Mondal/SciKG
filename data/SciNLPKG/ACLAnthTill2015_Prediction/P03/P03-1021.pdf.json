{"title": [{"text": "Minimum Error Rate Training in Statistical Machine Translation", "labels": [], "entities": [{"text": "Minimum Error Rate", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.7051838636398315}, {"text": "Statistical Machine Translation", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.8598490556081136}]}], "abstractContent": [{"text": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.6506590743859609}]}, {"text": "A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.", "labels": [], "entities": []}, {"text": "In this paper, we analyze various training criteria which directly optimize translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9567938446998596}]}, {"text": "These training criteria make use of recently proposed automatic evaluation met-rics.", "labels": [], "entities": []}, {"text": "We describe anew algorithm for efficient training an unsmoothed error count.", "labels": [], "entities": []}, {"text": "We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many tasks in natural language processing have evaluation criteria that go beyond simply counting the number of wrong decisions the system makes.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.6664257049560547}]}, {"text": "Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked retrieval, and BLEU or multi-reference word error rate for statistical machine translation.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9908068776130676}, {"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9176578521728516}, {"text": "mean average precision", "start_pos": 66, "end_pos": 88, "type": "METRIC", "confidence": 0.8896280924479166}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.998695433139801}, {"text": "multi-reference word error rate", "start_pos": 123, "end_pos": 154, "type": "METRIC", "confidence": 0.5668579488992691}, {"text": "statistical machine translation", "start_pos": 159, "end_pos": 190, "type": "TASK", "confidence": 0.6840898891290029}]}, {"text": "The use of statistical techniques in natural language processing often starts outwith the simplifying (often implicit) assumption that the final scoring is based on simply counting the number of wrong decisions, for instance, the number of sentences incorrectly translated in machine translation.", "labels": [], "entities": []}, {"text": "Hence, there is a mismatch between the basic assumptions of the used statistical approach and the final evaluation criterion used to measure success in a task.", "labels": [], "entities": []}, {"text": "Ideally, we would like to train our model parameters such that the end-to-end performance in some application is optimal.", "labels": [], "entities": []}, {"text": "In this paper, we investigate methods to efficiently optimize model parameters with respect to machine translation quality as measured by automatic evaluation criteria such as word error rate and BLEU.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.6879265308380127}, {"text": "word error rate", "start_pos": 176, "end_pos": 191, "type": "METRIC", "confidence": 0.6487709184487661}, {"text": "BLEU", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9898706078529358}]}, {"text": "The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.", "labels": [], "entities": []}, {"text": "1 minimizes the number of decision errors.", "labels": [], "entities": []}, {"text": "Hence, under a so-called zero-one loss function this decision rule is optimal (.", "labels": [], "entities": []}, {"text": "Note that using a different loss function-for example, one induced by the BLEU metric-a different decision rule would be optimal.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9937856197357178}]}, {"text": "The optimization problem under this criterion has very nice properties: there is one unique global optimum, and there are algorithms (e.g. gradient descent) that are guaranteed to converge to the global optimum.", "labels": [], "entities": []}, {"text": "Yet, the ultimate goal is to obtain good translation quality on unseen test data.", "labels": [], "entities": [{"text": "translation", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9511752724647522}]}, {"text": "Experience shows that good results can be obtained using this approach, yet there is no reason to assume that an optimization of the model parameters using Eq.", "labels": [], "entities": []}, {"text": "4 yields parameters that are optimal with respect to translation quality.", "labels": [], "entities": []}, {"text": "The goal of this paper is to investigate alternative training criteria and corresponding training algorithms, which are directly related to translation quality measured with automatic evaluation criteria.", "labels": [], "entities": []}, {"text": "In Section 3, we review various automatic evaluation criteria used in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7154461542765299}]}, {"text": "In Section 4, we present two different training criteria which try to directly optimize an error count.", "labels": [], "entities": []}, {"text": "In Section 5, we sketch anew training algorithm which efficiently optimizes an unsmoothed error count.", "labels": [], "entities": []}, {"text": "In Section 6, we describe the used feature functions and our approach to compute the candidate translations that are the basis for our training procedure.", "labels": [], "entities": []}, {"text": "In Section 7, we evaluate the different training criteria in the context of several MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 84, "end_pos": 86, "type": "TASK", "confidence": 0.9894750118255615}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Effect of different error criteria in training on the development corpus. Note that better results  correspond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for  which the difference to the best result (indicated in bold) is not statistically significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9983227849006653}, {"text": "NIST", "start_pos": 140, "end_pos": 144, "type": "DATASET", "confidence": 0.577985405921936}]}, {"text": " Table 1: Characteristics of training corpus (Train),  manual lexicon (Lex), development corpus (Dev),  test corpus (Test).", "labels": [], "entities": []}, {"text": " Table 1.  We observe that if we choose a certain error crite- rion in training, we obtain in most cases the best re- sults using the same criterion as the evaluation met- ric on the test data. The differences can be quite  large: If we optimize with respect to word error rate,  the results are mWER=68.3%, which is better than", "labels": [], "entities": [{"text": "mWER", "start_pos": 296, "end_pos": 300, "type": "METRIC", "confidence": 0.956939697265625}]}, {"text": " Table 3: Effect of different error criteria used in training on the test corpus. Note that better results corre- spond to larger BLEU and NIST scores and to smaller error rates. Italic numbers refer to results for which  the difference to the best result (indicated in bold) is not statistically significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9983166456222534}, {"text": "NIST", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.473409503698349}]}]}