{"title": [{"text": "Towards a Resource for Lexical Semantics: A Large German Corpus with Extensive Semantic Annotation", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information , e.g. the construction of domain-independent lexica.", "labels": [], "entities": []}, {"text": "The backbone of the annotation are semantic roles in the frame semantics paradigm.", "labels": [], "entities": []}, {"text": "We report experiences and evaluate the annotated data from the first project stage.", "labels": [], "entities": []}, {"text": "On this basis , we discuss the problems of vagueness and ambiguity in semantic annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpus-based methods for syntactic learning and processing are well-established in computational linguistics.", "labels": [], "entities": [{"text": "syntactic learning and processing", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.8039292097091675}]}, {"text": "There are comprehensive and carefully worked-out corpus resources available fora number of languages, e.g. the Penn Treebank for English or the NEGRA corpus) for German.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 111, "end_pos": 124, "type": "DATASET", "confidence": 0.9922400712966919}, {"text": "NEGRA corpus", "start_pos": 144, "end_pos": 156, "type": "DATASET", "confidence": 0.9416256546974182}]}, {"text": "In semantics, the situation is different: Semantic corpus annotation is only in its initial stages, and currently only a few, mostly small, corpora are available.", "labels": [], "entities": [{"text": "Semantic corpus annotation", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7429147164026896}]}, {"text": "Semantic annotation has predominantly concentrated on word senses, e.g. in the SENSEVAL initiative), a notable exception being the Prague Treebank . As a consequence, most recent work in corpus-based semantics has taken an unsupervised approach, relying on statistical methods to extract semantic regularities from raw corpora, often using information from ontologies like WordNet (.", "labels": [], "entities": [{"text": "Semantic annotation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8611229360103607}, {"text": "Prague Treebank", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.989173024892807}, {"text": "WordNet", "start_pos": 373, "end_pos": 380, "type": "DATASET", "confidence": 0.9479135870933533}]}, {"text": "Meanwhile, the lack of large, domainindependent lexica providing word-semantic information is one of the most serious bottlenecks for language technology.", "labels": [], "entities": []}, {"text": "To train tools for the acquisition of semantic information for such lexica, large, extensively annotated resources are necessary.", "labels": [], "entities": []}, {"text": "In this paper, we present current work of the SALSA (SAarbr\u00fccken Lexical Semantics Annotation and analysis) project, whose aim is to provide such a resource and to investigate efficient methods for its utilisation.", "labels": [], "entities": [{"text": "SALSA (SAarbr\u00fccken Lexical Semantics Annotation and analysis)", "start_pos": 46, "end_pos": 107, "type": "TASK", "confidence": 0.6592274440659417}]}, {"text": "In the current project phase, the focus of our research and the backbone of the annotation are semantic role relations.", "labels": [], "entities": []}, {"text": "More specifically, our role annotation is based on the Berkeley FrameNet project ().", "labels": [], "entities": []}, {"text": "In addition, we selectively annotate word senses and anaphoric links.", "labels": [], "entities": []}, {"text": "The TIGER corpus (), a 1.5M word German newspaper corpus, serves as sound syntactic basis.", "labels": [], "entities": [{"text": "TIGER corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7270601242780685}]}, {"text": "Besides the sparse data problem, the most serious problem for corpus-based lexical semantics is the lack of specificity of the data: Word meaning is notoriously ambiguous, vague, and subject to contextual variance.", "labels": [], "entities": []}, {"text": "The problem has been recognised and discussed in connection with the SENSEVAL task).", "labels": [], "entities": [{"text": "SENSEVAL task", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.8719039261341095}]}, {"text": "Annotation of frame semantic roles compounds the problem as it combines word sense assignment with the assignment of semantic roles, a task that introduces vagueness and ambiguity problems of its own.", "labels": [], "entities": [{"text": "word sense assignment", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.6361585855484009}]}, {"text": "The problem can be alleviated by choosing a suitable resource as annotation basis.", "labels": [], "entities": []}, {"text": "FrameNet roles, which are local to particular frames (abstract situations), maybe better suited for the annotation task than the \"classical\" thematic roles concept with a small, universal and exhaustive set of roles like agent, patient, theme: The exact extension of the role concepts has never been agreed upon.", "labels": [], "entities": []}, {"text": "Furthermore, the more concrete frame semantic roles may make the annotators' task easier.", "labels": [], "entities": []}, {"text": "The FrameNet database itself, however, cannot betaken as evidence that reliable annotation is possible: The aim of the FrameNet project is essentially lexicographic and its annotation not exhaustive; it comprises representative examples for the use of each frame and its frame elements in the BNC.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9050627946853638}, {"text": "BNC", "start_pos": 293, "end_pos": 296, "type": "DATASET", "confidence": 0.9638572931289673}]}, {"text": "While the vagueness and ambiguity problem maybe mitigated by the using of a \"good\" resource, it will not disappear entirely, and an annotation format is needed that can cope with the inherent vagueness of word sense and semantic role assignment.", "labels": [], "entities": [{"text": "semantic role assignment", "start_pos": 220, "end_pos": 244, "type": "TASK", "confidence": 0.6432174444198608}]}, {"text": "In Section 2 we briefly introduce FrameNet and the TIGER corpus that we use as a basis for semantic annotation.", "labels": [], "entities": [{"text": "TIGER corpus", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.8128749132156372}]}, {"text": "Section 3 gives an overview of the aims of the SALSA project, and Section 4 describes the annotation with frame semantic roles.", "labels": [], "entities": [{"text": "SALSA project", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.8757723867893219}]}, {"text": "Section 5 evaluates the first annotation results and the suitability of FrameNet as an annotation resource, and Section 6 discusses the effects of vagueness and ambiguity on frame semantic role annotation.", "labels": [], "entities": []}, {"text": "Although the current amount of annotated data does not allow for definitive judgements, we can discuss tendencies.", "labels": [], "entities": []}], "datasetContent": [{"text": "Compared to the pilot study we previously reported, in which 3 annotators tagged 440 corpus instances of a single frame, resulting in 1,320 annotation instances, we now dispose of a considerably larger body of data.", "labels": [], "entities": []}, {"text": "It consists of 703 corpus instances for the two frames shown in, making up a total of 4,653 annotation instances.", "labels": [], "entities": []}, {"text": "For the frame REQUEST, we obtained 421 instances with 8-fold and 114 with 7-fold annotation.", "labels": [], "entities": [{"text": "REQUEST", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9169818758964539}]}, {"text": "The annotated lemmas comprise auffordern (to request), fordern, verlangen (to demand), zur\u00fcckfordern (demand back), the noun Forderung (demand), and compound nouns ending with -forderung.", "labels": [], "entities": []}, {"text": "For the frame CT we have 30, 40 and 98 instances with 5-, 3-, and 2-fold annotation respectively.", "labels": [], "entities": []}, {"text": "The annotated lemmas are kaufen (to buy), erwerben (to acquire), verbrauchen (to consume), and verkaufen (to sell).", "labels": [], "entities": []}, {"text": "Note that the corpora we are evaluating do not constitute a random sample: At the moment, we cover only two frames, and REQUEST seems to be relatively easy to annotate.", "labels": [], "entities": [{"text": "REQUEST", "start_pos": 120, "end_pos": 127, "type": "METRIC", "confidence": 0.9878679513931274}]}, {"text": "Also, the annotation results may not be entirely predictive for larger sample sizes: While the annotation guidelines were being developed, we used REQUEST as a \"calibration\" frame to be annotated by everybody.", "labels": [], "entities": [{"text": "REQUEST", "start_pos": 147, "end_pos": 154, "type": "METRIC", "confidence": 0.8597556948661804}]}, {"text": "As a result, in some cases reliability maybe too low because detailed guidelines were not available, and in others it maybe too high because controversial instances were discussed in project meetings.", "labels": [], "entities": [{"text": "reliability", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9987459182739258}]}, {"text": "Results 74.25% 90.30% 69.33%: Inter-annotator agreement on frames (top) and frame elements (below).", "labels": [], "entities": []}, {"text": "Due to the limited space in this paper, we only address the question of inter-annotator agreement or annotation reliability, since a reliable annotation is necessary for all further corpus uses.", "labels": [], "entities": []}, {"text": "shows the inter-annotator agreement on frame assignment and on frame element assignment, computed for pairs of annotators.", "labels": [], "entities": []}, {"text": "The \"average\" column shows the total agreement for all annotation instances, while \"best\" and \"worst\" show the figures for the (lemma-specific) subcorpora with highest and lowest agreement, respectively.", "labels": [], "entities": [{"text": "agreement", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9486748576164246}]}, {"text": "The upper half of the table shows agreement on the assignment of frames to FEEs, for which we performed 14,410 pairwise comparisons, and the lower half shows agreement on assigned frame elements (29,889 pairwise comparisons).", "labels": [], "entities": [{"text": "agreement", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9809294939041138}, {"text": "FEEs", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.5380952954292297}]}, {"text": "Agreement on frame elements is \"exact match\": both annotators have to tag exactly the same sequence of words.", "labels": [], "entities": [{"text": "exact match", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9121091067790985}]}, {"text": "In sum, we found that annotators agreed very well on frames.", "labels": [], "entities": []}, {"text": "Disagreement on frame elements was higher, in the range of 12-25%.", "labels": [], "entities": [{"text": "Disagreement", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9774551391601562}]}, {"text": "Generally, the numbers indicated considerable differences between the subcorpora.", "labels": [], "entities": []}, {"text": "To investigate this matter further, we computed the Alpha statistic for our annotation.", "labels": [], "entities": []}, {"text": "Like the widely used Kappa, \u03b1 is a chancecorrected measure of reliability.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 28, "end_pos": 29, "type": "METRIC", "confidence": 0.9530249834060669}, {"text": "reliability", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9884006977081299}]}, {"text": "It is defined as We chose Alpha over Kappa because it also indicates unreliabilities due to unequal coder preference for categories.", "labels": [], "entities": []}, {"text": "With an \u03b1 value of 1 signifying total agreement and 0 chance agreement, \u03b1 values above 0.8 are usually interpreted as reliable annotation.", "labels": [], "entities": []}, {"text": "shows single category reliabilities for the assignment of frame elements.", "labels": [], "entities": []}, {"text": "The graphs shows that not only did target lemmas vary in their difficulty, but that reliability of frame element assignment was also a matter of high variation.", "labels": [], "entities": [{"text": "frame element assignment", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.5857510070006052}]}, {"text": "Firstly, frames introduced by nouns (Forderung and -forderung) were more difficult to annotate than verbs.", "labels": [], "entities": []}, {"text": "Secondly, frame elements could be assigned to three groups: frame elements which were always annotated reliably, those whose reliability was highly dependent on the FEE, and the third group whose members were impossible to annotate reliably (these are not shown in the graphs).", "labels": [], "entities": [{"text": "FEE", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.5523144602775574}]}, {"text": "In the REQUEST frames, SPEAKER, MESSAGE and AD-DRESSEE belong to the first group, at least for verbal FEEs.", "labels": [], "entities": [{"text": "REQUEST", "start_pos": 7, "end_pos": 14, "type": "METRIC", "confidence": 0.93536776304245}, {"text": "SPEAKER", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.950596034526825}, {"text": "MESSAGE", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9869568347930908}, {"text": "AD-DRESSEE", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8914921879768372}, {"text": "FEEs", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9443265795707703}]}, {"text": "MEDIUM is a member of the second group, and TOPIC was annotated at chance level (\u03b1 \u2248 0).", "labels": [], "entities": [{"text": "MEDIUM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.4779430627822876}, {"text": "TOPIC", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9266321659088135}, {"text": "chance", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9897612929344177}]}, {"text": "In the COMMERCE frame, only BUYER and GOODS always show high reliability.", "labels": [], "entities": [{"text": "COMMERCE frame", "start_pos": 7, "end_pos": 21, "type": "DATASET", "confidence": 0.7062136679887772}, {"text": "BUYER", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.9957484602928162}, {"text": "GOODS", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9110494256019592}, {"text": "reliability", "start_pos": 61, "end_pos": 72, "type": "METRIC", "confidence": 0.9796134233474731}]}, {"text": "SELLER can only be reliably annotated for the target verkaufen.", "labels": [], "entities": [{"text": "SELLER", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9414242506027222}]}, {"text": "PURPOSE and REASON fall into the third group.", "labels": [], "entities": [{"text": "PURPOSE", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.928608775138855}, {"text": "REASON", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9711287021636963}]}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement on frames (top)  and frame elements (below).", "labels": [], "entities": []}]}