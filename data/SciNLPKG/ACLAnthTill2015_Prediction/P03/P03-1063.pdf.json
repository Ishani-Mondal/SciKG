{"title": [{"text": "Text Chunking by Combining Hand-Crafted Rules and Memory-Based Learning", "labels": [], "entities": [{"text": "Text Chunking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7040677517652512}]}], "abstractContent": [{"text": "This paper proposes a hybrid of hand-crafted rules and a machine learning method for chunking Korean.", "labels": [], "entities": [{"text": "chunking Korean", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.9251298010349274}]}, {"text": "In the partially free word-order languages such as Korean and Japanese, a small number of rules dominate the performance due to their well-developed postpositions and endings.", "labels": [], "entities": []}, {"text": "Thus, the proposed method is primarily based on the rules, and then the residual errors are corrected by adopting a memory-based machine learning method.", "labels": [], "entities": []}, {"text": "Since the memory-based learning is an efficient method to handle exceptions in natural language processing, it is good at checking whether the estimates are exceptional cases of the rules and revising them.", "labels": [], "entities": []}, {"text": "An evaluation of the method yields the improvement in F-score over the rules or various machine learning methods alone.", "labels": [], "entities": [{"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9990377426147461}]}], "introductionContent": [{"text": "Text chunking has been one of the most interesting problems in natural language learning community since the first work of) using a machine learning method.", "labels": [], "entities": [{"text": "Text chunking", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7761231362819672}]}, {"text": "The main purpose of the machine learning methods applied to this task is to capture the hypothesis that best determine the chunk type of a word, and such methods have shown relatively high performance in English ().", "labels": [], "entities": []}, {"text": "In order to do it, various kinds of information, such as lexical information, part-of-speech and grammatical relation, of the neighboring words is used.", "labels": [], "entities": []}, {"text": "Since the position of a word plays an important role as a syntactic constraint in English, the methods are successful even with local information.", "labels": [], "entities": []}, {"text": "However, these methods are not appropriate for chunking Korean and Japanese, because such languages have a characteristic of partially free wordorder.", "labels": [], "entities": [{"text": "chunking Korean and Japanese", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.8916587680578232}]}, {"text": "That is, there is a very weak positional constraint in these languages.", "labels": [], "entities": []}, {"text": "Instead of positional constraints, they have overt postpositions that restrict the syntactic relation and composition of phrases.", "labels": [], "entities": []}, {"text": "Thus, unless we concentrate on the postpositions, we must enlarge the neighboring window to get a good hypothesis.", "labels": [], "entities": []}, {"text": "However, enlarging the window size will cause the curse of dimensionality, which results in the deficiency in the generalization performance.", "labels": [], "entities": []}, {"text": "Especially in Korean, the postpositions and the endings provide important information for noun phrase and verb phrase chunking respectively.", "labels": [], "entities": [{"text": "noun phrase and verb phrase chunking", "start_pos": 90, "end_pos": 126, "type": "TASK", "confidence": 0.6177485038836797}]}, {"text": "With only a few simple rules using such information, the performance of chunking Korean is as good as the rivaling other inference models such as machine learning algorithms and statistics-based methods.", "labels": [], "entities": []}, {"text": "Though the rules are approximately correct for most cases drawn from the domain on which the rules are based, the knowledge in the rules is not necessarily well-represented for any given set of cases.", "labels": [], "entities": []}, {"text": "Since chunking is usually processed in the earlier step of natural language processing, the errors made in this step have a fatal influence on the following steps.", "labels": [], "entities": [{"text": "chunking", "start_pos": 6, "end_pos": 14, "type": "TASK", "confidence": 0.9651685357093811}]}, {"text": "Therefore, the exceptions that are ignored by the rules must be com-  pensated for by some special treatments of them for higher performance.", "labels": [], "entities": []}, {"text": "To solve this problem, we have proposed a combining method of the rules and the k-nearest neighbor (k-NN) algorithm).", "labels": [], "entities": []}, {"text": "The problem in this method is that it has redundant kNNs because it maintains a separate k-NN for each kind of errors made by the rules.", "labels": [], "entities": []}, {"text": "In addition, because it applies a k-NN and the rules to each examples, it requires more computations than other inference methods.", "labels": [], "entities": []}, {"text": "The goal of this paper is to provide anew method for chunking Korean by combining the hand-crafted rules and a machine learning method.", "labels": [], "entities": [{"text": "chunking Korean", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.9377330839633942}]}, {"text": "The chunk type of a word in question is determined by the rules, and then verified by the machine learning method.", "labels": [], "entities": []}, {"text": "The role of the machine learning method is to determine whether the current context is an exception of the rules.", "labels": [], "entities": []}, {"text": "Therefore, a memory-based learning (MBL) is used as a machine learning method that can handle exceptions efficiently).", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 explains how the proposed method works.", "labels": [], "entities": []}, {"text": "Section 3 describes the rule-based method for chunking Korean and Section 4 explains chunking by memory-based learning.", "labels": [], "entities": [{"text": "chunking", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.9726869463920593}, {"text": "chunking", "start_pos": 85, "end_pos": 93, "type": "TASK", "confidence": 0.9635478258132935}]}, {"text": "Section 5 presents the experimental results.", "labels": [], "entities": []}, {"text": "Section 6 introduces the issues for applying the proposed method to other problems.", "labels": [], "entities": []}, {"text": "Finally, Section 7 draws conclusions.", "labels": [], "entities": []}, {"text": "shows the structure of the chunking model for Korean.", "labels": [], "entities": []}, {"text": "The main idea of this model is to apply rules to determine the chunk type of a word w i in a sentence, and then to refer to a memory based classifier in order to check whether it is an exceptional case of the rules.", "labels": [], "entities": []}, {"text": "In the training phase, each sentence is analyzed by the rules and the predicted chunk type is compared with the true chunk type.", "labels": [], "entities": []}, {"text": "In case of misprediction, the error type is determined according to the true chunk type and the predicted chunk type.", "labels": [], "entities": []}, {"text": "The mispredicted chunks are stored in the error case library with their true chunk types.", "labels": [], "entities": []}, {"text": "Since the error case library accumulates only the exceptions of the rules, the number of cases in the library is small if the rules are general enough to represent the instance space well.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation of the proposed method, all experiments are performed on STEP 2000 Korean Chunking dataset (STEP 2000 dataset) 2 . This dataset is derived from the parsed corpus, which is a product of STEP 2000 project supported by Korean government.", "labels": [], "entities": [{"text": "STEP 2000 Korean Chunking dataset (STEP 2000 dataset)", "start_pos": 76, "end_pos": 129, "type": "DATASET", "confidence": 0.9015686929225921}]}, {"text": "The corpus consists of 12,092 sentences with 111,658 phrases and 321,328 words, and the vocabulary size is 16,808.", "labels": [], "entities": []}, {"text": "summarizes the information on the dataset.", "labels": [], "entities": []}, {"text": "The format of the dataset follows that of CoNLL-2000 dataset).", "labels": [], "entities": [{"text": "CoNLL-2000 dataset", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.980726420879364}]}, {"text": "shows an example sentence in the dataset . Each word in the dataset has two additional tags, which area part-ofspeech tag and a chunk tag.", "labels": [], "entities": []}, {"text": "The part-of-speech tags are based on KAIST tagset.", "labels": [], "entities": [{"text": "KAIST tagset", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9004173576831818}]}, {"text": "Each phrase can have two kinds of chunk types: B-XP and I-XP.", "labels": [], "entities": []}, {"text": "In addition to them, there is O chunk type that is used for words which are not part of any chunk.", "labels": [], "entities": []}, {"text": "Since there are four types of phrases and one additional chunk type O, there exist nine chunk types.", "labels": [], "entities": []}, {"text": "shows the error types by the rules and their distribution.", "labels": [], "entities": []}, {"text": "For example, the error type 'B-ADVP I-ADVP' contains the errors whose true label is B-ADVP and that are mislabeled by I-ADVP.", "labels": [], "entities": []}, {"text": "There are eight error types, but most errors are related with noun phrases.", "labels": [], "entities": []}, {"text": "We found two reasons for this:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The probability that an adverb sequence  forms a chunk.", "labels": [], "entities": []}, {"text": " Table 3: The simple statistics on STEP 2000 Korean  Chunking dataset.", "labels": [], "entities": [{"text": "STEP 2000 Korean  Chunking dataset", "start_pos": 35, "end_pos": 69, "type": "DATASET", "confidence": 0.89434974193573}]}, {"text": " Table 4: The experimental results when the rules are  only used.", "labels": [], "entities": []}, {"text": " Table 5: The error distribution according to the mis- labeled chunk type.", "labels": [], "entities": []}, {"text": " Table 6: The experimental results of various ma- chine learning algorithms.", "labels": [], "entities": []}, {"text": " Table 7: The weights of the attributes in IB1-IG. The  total sum of the weights is 2.48.", "labels": [], "entities": []}, {"text": " Table 8: The final result of the proposed method by  combining the rules and the memory-based learning.  The average accuracy is 98.21\u00b10.43.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9994377493858337}]}]}