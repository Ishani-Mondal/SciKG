{"title": [{"text": "Integrated Shallow and Deep Parsing: TopP meets HPSG", "labels": [], "entities": [{"text": "HPSG", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.7852511405944824}]}], "abstractContent": [{"text": "We present a novel, data-driven method for integrated shallow and deep parsing.", "labels": [], "entities": []}, {"text": "Mediated by an XML-based multi-layer annotation architecture, we interleave a robust, but accurate stochastic topological field parser of German with a constraint-based HPSG parser.", "labels": [], "entities": []}, {"text": "Our annotation-based method for dovetailing shallow and deep phrasal constraints is highly flexible, allowing targeted and fine-grained guidance of constraint-based parsing.", "labels": [], "entities": []}, {"text": "We conduct systematic experiments that demonstrate substantial performance gains.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the strong points of deep processing (DNLP) technology such as HPSG or LFG parsers certainly lies with the high degree of precision as well as detailed linguistic analysis these systems are able to deliver.", "labels": [], "entities": [{"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9977992177009583}]}, {"text": "Although considerable progress has been made in the area of processing speed, DNLP systems still cannot rival shallow and medium depth technologies in terms of throughput and robustness.", "labels": [], "entities": []}, {"text": "As a net effect, the impact of deep parsing technology on application-oriented NLP is still fairly limited.", "labels": [], "entities": []}, {"text": "With the advent of XML-based hybrid shallowdeep architectures as presented in;) it has become possible to integrate the added value of deep processing with the performance and robustness of shallow processing.", "labels": [], "entities": []}, {"text": "So far, integration has largely focused on the lexical level, to improve upon the most urgent needs in increasing the robustness and coverage of deep parsing systems, namely lexical coverage.", "labels": [], "entities": [{"text": "integration", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9710348844528198}]}, {"text": "While integration in) was still restricted to morphological and PoS information, () extended shallow-deep integration at the lexical level to lexico-semantic information, and named entity expressions, including multiword expressions.", "labels": [], "entities": []}, {"text": "() assume a vertical, 'pipeline' scenario where shallow NLP tools provide XML annotations that are used by the DNLP system as a preprocessing and lexical interface.", "labels": [], "entities": []}, {"text": "The perspective opened up by a multi-layered, data-centric architecture is, however, much broader, in that it encourages horizontal cross-fertilisation effects among complementary and/or competing components.", "labels": [], "entities": []}, {"text": "One of the culprits for the relative inefficiency of DNLP parsers is the high degree of ambiguity found in large-scale grammars, which can often only be resolved within a larger syntactic domain.", "labels": [], "entities": []}, {"text": "Within a hybrid shallow-deep platform one can take advantage of partial knowledge provided by shallow parsers to pre-structure the search space of the deep parser.", "labels": [], "entities": []}, {"text": "In this paper, we will thus complement the efforts made on the lexical side by integration at the phrasal level.", "labels": [], "entities": []}, {"text": "We will show that this may lead to considerable performance increase for the DNLP component.", "labels": [], "entities": [{"text": "DNLP", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8578043580055237}]}, {"text": "More specifically, we combine a probabilistic topological field parser for German ) with the HPSG parser of).", "labels": [], "entities": []}, {"text": "The HPSG grammar used is the one originally developed by), with significant performance enhancements by B. Crysmann.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss the mapping problem involved with syntactic integration of shallow and deep analyses and motivate our choice to combine the HPSG system with a topological parser.", "labels": [], "entities": []}, {"text": "Section 3 outlines our basic approach towards syntactic shallow-deep integration.", "labels": [], "entities": [{"text": "syntactic shallow-deep integration", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.6337783833344778}]}, {"text": "Section 4 introduces various confidence measures, to be used for fine-tuning of phrasal integration.", "labels": [], "entities": [{"text": "phrasal integration", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7215475291013718}]}, {"text": "Sections 5 and 6 report on experiments and results of integrated shallow-deep parsing, measuring the effect of various integration parameters on performance gains for the DNLP component.", "labels": [], "entities": [{"text": "shallow-deep parsing", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.5459888130426407}]}, {"text": "Section 7 concludes and discusses possible extensions, to address robustness issues.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the task of identifying the perfect matches from a set of parses we give the following standard definitions: precision is the proportion of selected parses that have a perfect match -thus being the perfect match rate, and recall is the proportion of perfect matches that the system selected.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9994113445281982}, {"text": "recall", "start_pos": 226, "end_pos": 232, "type": "METRIC", "confidence": 0.9995880722999573}]}, {"text": "Coverage is usually defined as the proportion of attempted analyses with at least one parse.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.7513602375984192}]}, {"text": "We extend this definition to treat successful analyses with a high tree entropy as being out of coverage.", "labels": [], "entities": []}, {"text": "shows the effect of decreasing entropy thresholds \ud97b\udf59 on precision, recall and coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.99935382604599}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9992951154708862}, {"text": "coverage", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9905903339385986}]}, {"text": "The unfiltered set of all sentences is found at \ud97b\udf59=1.", "labels": [], "entities": []}, {"text": "Lowering \ud97b\udf59 in- Results We use f-measure as a target function on the training set to determine a plausible \ud97b\udf59.", "labels": [], "entities": []}, {"text": "F-measure is maximal at \ud97b\udf59=0.236 with 88.9%, see.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9779289364814758}]}, {"text": "Precision and recall are 83.7% and 94.8% resp.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9942580461502075}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.998874843120575}, {"text": "resp", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9975005984306335}]}, {"text": "while coverage goes down to 83.0%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9982101917266846}]}, {"text": "Applying the same \ud97b\udf59 on the test set, we get the following results: 80.5% precision, 93.0% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9980886578559875}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9993921518325806}]}, {"text": "Coverage goes down to 80.6%.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9549662470817566}]}, {"text": "LP is 93.3%, LR is 91.2%.", "labels": [], "entities": [{"text": "LP", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9566285610198975}, {"text": "LR", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.8715428709983826}]}, {"text": "Confidence Measure We distribute the complement of the associated tree entropy of a parse tree \u00d8\u00d6 as a global confidence measure overall brackets \ud97b\udf59\u00d6 extracted from that parse: conf \ud97b\udf59\u00d2\u00d8\u00b4\ud97b\udf59\u00d6\u00b5\ud97b\udf59\u00d2\u00d8\u00b4\ud97b\udf59\u00d6\u00b5 \u00b5 \u00bd ent\u00b4\u00d8\u00d6\u00b5.", "labels": [], "entities": []}, {"text": "For the thresholded version of conf \ud97b\udf59\u00d2\u00d8\u00b4\ud97b\udf59\u00d6\u00b5\ud97b\udf59\u00d2\u00d8\u00b4\ud97b\udf59\u00d6\u00b5, we set the threshold to \u00bd \ud97b\udf59 \ud97b\udf59 \u00bd \u00bc\ud97b\udf59\u00be\u00bf\u00bf \u00bc\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59.", "labels": [], "entities": [{"text": "conf \ud97b\udf59\u00d2\u00d8\u00b4\ud97b\udf59\u00d6\u00b5\ud97b\udf59\u00d2\u00d8\u00b4\ud97b\udf59\u00d6\u00b5", "start_pos": 31, "end_pos": 50, "type": "METRIC", "confidence": 0.7423276702562968}]}, {"text": "Experimental Setup In the experiments we use the subset of the NEGRA corpus (5060 sents, 24.57%) that is currently parsed by the HPSG grammar.", "labels": [], "entities": [{"text": "NEGRA corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.8965684473514557}, {"text": "HPSG grammar", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.9631651341915131}]}, {"text": "Average sentence length is 8.94, ignoring punctuation; average lexical ambiguity is 3.05 entries/word.", "labels": [], "entities": []}, {"text": "As baseline, we performed a run without topological information, yet including PoS prioritisation from tagging.", "labels": [], "entities": []}, {"text": "A series of tests explores the effects of alternative parameter settings.", "labels": [], "entities": []}, {"text": "We further test the impact of chunk information.", "labels": [], "entities": []}, {"text": "To this end, phrasal fields determined by topological parsing were fed to the chunk parser of.", "labels": [], "entities": []}, {"text": "Extracted NP and PP bracket constraints are defined as left-matching bracket types, to compensate for the non-embedding structure of chunks.", "labels": [], "entities": []}, {"text": "Chunk brackets are tested in conjunction with topological brackets, and in isolation, using the labelled precision value of 71.1% in as a uniform confidence weight.", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.8340685963630676}]}, {"text": "14 Measures For all runs we measure the absolute time and the number of parsing tasks needed to compute the first reading.", "labels": [], "entities": []}, {"text": "The times in the individual runs were normalised according to the number of executed tasks per second.", "labels": [], "entities": []}, {"text": "We noticed that the coverage of some integrated runs decreased by up to 1% of the 5060 test items, with atypical loss of around 0.5%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9916737675666809}]}, {"text": "To warrant that we are not just trading coverage for speed, we derived two measures from the primary data: an upper bound, where we associated every unsuccessful parse with the time and number of tasks used when the limit of 70000 passive edges was hit, and a lower bound, where we removed the most expensive parses from each run, until we reached the same coverage.", "labels": [], "entities": []}, {"text": "Whereas the upper bound is certainly more realistic in an application context, the lower bound gives us a worst case estimate of expectable speed-up.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Disamb: correct (\u00b7) / tagger ( ) PoS input.  Eval. on atomic (vs. parameterised) category labels.", "labels": [], "entities": []}, {"text": " Table 2: Priority weight parameters and results", "labels": [], "entities": []}]}