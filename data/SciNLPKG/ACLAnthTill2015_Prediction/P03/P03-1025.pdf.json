{"title": [{"text": "Compounding and derivational morphology in a finite-state setting", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes the application of finite-state approximation techniques on a unification-based grammar of word formation fora language like German.", "labels": [], "entities": [{"text": "word formation", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.6676572561264038}]}, {"text": "A refinement of an RTN-based approximation algorithm is proposed, which extends the state space of the automaton by selectively adding distinctions based on the parsing history at the point of entering a context-free rule.", "labels": [], "entities": [{"text": "RTN-based approximation", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.8548032343387604}]}, {"text": "The selection of history items exploits the specific linguistic nature of word formation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.7545246481895447}]}, {"text": "As experiments show, this algorithm avoids an explosion of the size of the automaton in the approximation construction.", "labels": [], "entities": []}, {"text": "1 The locus of word formation rules in grammars for NLP In English orthography, compounds following productive word formation patterns are spelled with spaces or hyphens separating the components (e.g., classic car repair workshop).", "labels": [], "entities": [{"text": "word formation", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7234337329864502}]}, {"text": "This is convenient from an NLP perspective, since most aspects of word formation can be ignored from the point of view of the conceptually simpler token-internal processes of inflectional morphology, for which standard finite-state techniques can be applied.", "labels": [], "entities": [{"text": "word formation", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7577799260616302}]}, {"text": "(Let us assume that to a first approximation, spaces and punctuation are used to identify token boundaries.)", "labels": [], "entities": []}, {"text": "It makes it also very easy to access one or more of the components of a compound (like classic car in the example), which is required in many NLP techniques (e.g., in a vector space model).", "labels": [], "entities": []}, {"text": "If an NLP task for English requires detailed information about the structure of compounds (as complex multi-token units), it is natural to use the formalisms of computational syntax for English, i.e., context-free grammars, or possibly unification-based grammars.", "labels": [], "entities": []}, {"text": "This makes it possible to deal with the bracketing structure of compounding, which would be impossible to cover in full generality in the finite-state setting.", "labels": [], "entities": []}, {"text": "In languages like German, spelling conventions for compounds do not support such a convenient split between sub-token processing based on finite-state technology and multi-token processing based on context-free grammars or beyond-in German, even very complex compounds are written without spaces or hyphens: words like Verkehrswegepla-nungsbeschleunigungsgesetz ('law for speeding up the planning of traffic routes') appear in corpora.", "labels": [], "entities": []}, {"text": "So, fora fully adequate and general account, the token-level analysis in German has to be done at least with a context-free grammar: 1 For checking the selection features of derivational affixes, in the general case a tree or bracketing structure is required.", "labels": [], "entities": []}, {"text": "For instance, the prefix Fehl-combines with nouns (compare (1)); however, it can appear linearly adjacent with a verb, including its own prefix, and only then do we get the suffix-ung, which turns the verb into a noun.", "labels": [], "entities": []}, {"text": "(1) N NV N \u00a1 \u00a3 \u00a2 V \u00a1 \u00a3 \u00a2 V N \u00a1 \u00a3 \u00a2 Fehl ver arbeit ung mis work 'misprocessing' 1 For a fully general account of derivational morphology in English, the token-level analysis has to go beyond finite-state means too: the prefix non-in nonrealizability combines with the complex derived adjective realizable, not with the verbal stem realize (and non-could combine with a more complex form).", "labels": [], "entities": []}, {"text": "However, since in English there is much less token-level interaction between derivation and compounding, a finite-state approximation of the relevant facts at token-level is more straightforward than in German.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We implemented the selective history-based RTNconstruction in Prolog, as a conversion routine that takes as input a definite-clause grammar with compiled-out grounded feature values; it produces as output a Prolog representation of an FSA.", "labels": [], "entities": [{"text": "FSA", "start_pos": 235, "end_pos": 238, "type": "DATASET", "confidence": 0.7647929191589355}]}, {"text": "The resulting automaton is determinized and minimized, using the FSA library for Prolog by Gertjan van Noord.", "labels": [], "entities": [{"text": "FSA library", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.9015481770038605}]}, {"text": "Emphasis was put on identifying the most suitable strategy for dealing with word formation taking into account the relative size of the FSAs generated (other techniques than the selective history strategy were tried out and discarded).", "labels": [], "entities": [{"text": "word formation", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.8381486535072327}]}, {"text": "The algorithm was applied on a sample word formation grammar with 185 compiled-out context-free rules, displaying the principled mechanism of category and other feature selection, but not the full set of distinctions made in the DeKo project.", "labels": [], "entities": [{"text": "word formation grammar", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.8016260266304016}]}, {"text": "9 of the rules were compiled from the prefixation rule, and were thus marked as h-rules for the selective method.", "labels": [], "entities": []}, {"text": "We ran a comparison between aversion of the non-selective parameterized RTN-method of (Nederhof 2000) and the selective history method proposed in this paper.", "labels": [], "entities": []}, {"text": "An overview of the results is given in.", "labels": [], "entities": []}, {"text": "It should be noted that the optimizations of sec.", "labels": [], "entities": []}, {"text": "7 were applied in both methods (the non-selective method was simulated by mark- are identical for the selective method is an artefact of the sample grammar. ing all rules as h-rules).", "labels": [], "entities": []}, {"text": "As the size results show, the non-deterministic FSAs constructed by the selective method are more complex (and hence resource-intensive in minimization) than the ones produced by the \"plain\" parameterized version.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 48, "end_pos": 52, "type": "TASK", "confidence": 0.8674996495246887}]}, {"text": "However, the difference in exactness of the approximizations has to betaken into account.", "labels": [], "entities": [{"text": "exactness", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9896993041038513}]}, {"text": "As a tentative indication for this, note that the minimized FSA for in the plain version has only two states; so obviously too many distinctions from the context-free grammar have been lost.", "labels": [], "entities": [{"text": "FSA", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9895746111869812}]}, {"text": "In the plain version, all word formation operations are treated alike, hence the history list of length one or two is quickly filled up with items that need not be recorded.", "labels": [], "entities": [{"text": "word formation", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7444363236427307}]}, {"text": "A comparison of the number of different pairs of categories and history lists used in the construction shows that the selective method is more economical in the use of memory space as the depth parameter grows larger.", "labels": [], "entities": []}, {"text": "(For led to a non-deterministic FSA with 87,601 states, for which minimization could not be completed due to a memory overflow.", "labels": [], "entities": [{"text": "FSA", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.40399375557899475}]}, {"text": "It is one goal for future research to identify possible ways of breaking down the approximation construction into smaller subproblems for which minimization can be run separately (even though all categories belong to the same equivalence class of mutually recursive categories).", "labels": [], "entities": []}, {"text": "11 Another goal is to experiment with the use of transduction as a means of adding structural markings from which the analysis trees can be reconstructed (to the extent they are not underspecified by the finite-state approach); possible approaches are discussed in Inspection of the longest few hundred prefixcontaining word forms in a large German newspaper corpus indicates that prefix stacking is rare.", "labels": [], "entities": [{"text": "Inspection of the longest few hundred prefixcontaining word forms in a large German newspaper corpus", "start_pos": 265, "end_pos": 365, "type": "TASK", "confidence": 0.7485912839571635}, {"text": "prefix stacking", "start_pos": 381, "end_pos": 396, "type": "TASK", "confidence": 0.7434378564357758}]}, {"text": "(If there are several prefixes in a word form, this tends to arise through compounding.)", "labels": [], "entities": []}, {"text": "No instance of stacking of depth 3 was observed.", "labels": [], "entities": [{"text": "stacking", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9892330169677734}]}, {"text": "So, the range of phenomena for which the approximation is inexact is of little practical relevance.", "labels": [], "entities": []}, {"text": "For a full evaluation of the coverage and exactness of the approach, a comprehensive implementation of the morphological grammar would be required.", "labels": [], "entities": [{"text": "exactness", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9741725921630859}]}, {"text": "We ran a preliminary experiment with a small grammar, focusing on the cases that might be problematic: we extracted from the corpus a random sample of 100 word forms containing prefixes.", "labels": [], "entities": []}, {"text": "From these 100 forms, we generated about 3700 grammatical and ungrammatical test examples by omission, addition and permutation of stems and affixes.", "labels": [], "entities": []}, {"text": "After making sure that the required affixes and stems were included in the lexicon of the grammar, we ran a comparison of exact parsing with the unification-based grammar and the selective history-based RTN-approximation, with parameter 6 8 (which means that there is a history window of one item).", "labels": [], "entities": [{"text": "exact parsing", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.549674853682518}]}, {"text": "For 97% of the test items, the two methods agreed; 3% of the items were accepted by the approximation method, but not by the full grammar.", "labels": [], "entities": []}, {"text": "The approximation does not lose any A related possibility pointed out by a reviewer would be to expand features from the original unification-grammar only where necessary (cf. test items parsed by the full grammar.", "labels": [], "entities": []}, {"text": "Some obvious improvements should make it possible soon to run experiments with a larger history window, reaching exactness of the finite-state method for almost all relevant data.", "labels": [], "entities": [{"text": "exactness", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9955312609672546}]}], "tableCaptions": []}