{"title": [{"text": "Generalized Algorithms for Constructing Statistical Language Models", "labels": [], "entities": [{"text": "Constructing Statistical Language", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.8709342082341512}]}], "abstractContent": [{"text": "Recent text and speech processing applications such as speech mining raise new and more general problems related to the construction of language models.", "labels": [], "entities": [{"text": "speech mining", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.7861230671405792}]}, {"text": "We present and describe in detail several new and efficient algorithms to address these more general problems and report experimental results demonstrating their usefulness.", "labels": [], "entities": []}, {"text": "We give an algorithm for computing efficiently the expected counts of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton; describe anew technique for creating exact representations of \u00a2-gram language models by weighted automata whose size is practical for offline use even fora vocabulary size of about 500,000 words and an \u00a2-gram order \u00a2 \u00a4 \u00a3 \u00a6 \u00a5 ; and present a simple and more general technique for constructing class-based language models that allows each class to represent an arbitrary weighted automaton.", "labels": [], "entities": []}, {"text": "An efficient implementation of our algorithms and techniques has been incorporated in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities.", "labels": [], "entities": [{"text": "GRM Library", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.8867305517196655}]}, {"text": "1 Motivation Statistical language models are crucial components of many modern natural language processing systems such as speech recognition, information extraction, machine translation, or document classification.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7323160767555237}, {"text": "information extraction", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7873994410037994}, {"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.7555196583271027}, {"text": "document classification", "start_pos": 191, "end_pos": 214, "type": "TASK", "confidence": 0.759233146905899}]}, {"text": "In all cases, a language model is used in combination with other information sources to rank alternative hypotheses by assigning them some probabilities.", "labels": [], "entities": []}, {"text": "There are classical techniques for constructing language models such as \u00a2-gram models with various smoothing techniques (see Chen and Goodman (1998) and the references therein fora survey and comparison of these techniques).", "labels": [], "entities": []}, {"text": "In some recent text and speech processing applications, several new and more general problems arise that are related to the construction of language models.", "labels": [], "entities": []}, {"text": "We present new and efficient algorithms to address these more general problems.", "labels": [], "entities": []}, {"text": "Classical language models are constructed by deriving statistics from large input texts.", "labels": [], "entities": []}, {"text": "In speech mining applications or for adaptation purposes, one often needs to construct a language model based on the output of a speech recognition system.", "labels": [], "entities": [{"text": "speech mining", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.7527077496051788}]}, {"text": "But, the output of a recognition system is not just text.", "labels": [], "entities": []}, {"text": "Indeed, the word error rate of conversational speech recognition systems is still too high in many tasks to rely only on the one-best output of the recognizer.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.6218510270118713}, {"text": "conversational speech recognition", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.6045664846897125}]}, {"text": "Thus, the word lattice output by speech recognition systems is used instead because it contains the correct transcription inmost cases.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7205230295658112}]}, {"text": "A word lattice is a weighted finite automaton (WFA) output by the recognizer fora particular utterance.", "labels": [], "entities": []}, {"text": "It contains typically a very large set of alternative transcription sentences for that utterance with the corresponding weights or probabilities.", "labels": [], "entities": []}, {"text": "A necessary step for constructing a language model based on a word lattice is to derive the statistics for any given sequence from the lattices or WFAs output by the recognizer.", "labels": [], "entities": []}, {"text": "This cannot be done by simply enumerating each path of the lattice and counting the number of occurrences of the sequence considered in each path since the number of paths of even a small automaton maybe more than four billion.", "labels": [], "entities": []}, {"text": "We present a simple and efficient algorithm for computing the expected count of any given sequence in a WFA and report experimental results demonstrating its efficiency.", "labels": [], "entities": []}, {"text": "Representation of language models by WFAs.", "labels": [], "entities": []}, {"text": "Classical \u00a2-gram language models admit a natural representation by WFAs in which each state encodes a left context of width less than \u00a2.", "labels": [], "entities": []}, {"text": "However, the size of that representation makes it impractical for offline optimizations such as those used in large-vocabulary speech recognition or general information extraction systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7371818572282791}, {"text": "general information extraction", "start_pos": 149, "end_pos": 179, "type": "TASK", "confidence": 0.607833206653595}]}, {"text": "Most offline representations of these models are based instead on an approximation to limit their size.", "labels": [], "entities": []}, {"text": "We describe anew technique for creating an exact representation of \u00a2-gram language models by WFAs whose size is practical for offline use even in tasks with a vocabulary size of about 500,000 words and for \u00a2 \u00a7 \u00a3 \u00a8 \u00a5.", "labels": [], "entities": []}, {"text": "In many applications, it is natural and convenient to construct class-based language models, that is models based on classes of words (Brown et al., 1992).", "labels": [], "entities": []}, {"text": "Such models are also often more robust since they may include words that belong to a class but that were not found in the corpus.", "labels": [], "entities": []}, {"text": "Classical class-based models are based on simple classes such as a list of words.", "labels": [], "entities": []}, {"text": "But new clustering algorithms allow one to create more general and more complex classes that maybe regular languages.", "labels": [], "entities": []}, {"text": "Very large and complex classes can also be defined using regular expressions.", "labels": [], "entities": []}, {"text": "We present a simple and more general approach to class-based language models based on general weighted context-dependent rules", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Note that some of the new intermediate backoff states can be fully or partially merged, to reduce the space requirements of the model.", "labels": [], "entities": []}, {"text": "Finding the optimal configuration of these states, however, is an NP-hard problem.", "labels": [], "entities": []}, {"text": "For our experiments, we used a simple greedy approach to sharing structure, which helped reduce space dramatically.", "labels": [], "entities": []}, {"text": "shows our example bigram model, after application of the algorithm.", "labels": [], "entities": []}, {"text": "Notice that there are now two history-less states, which correspond to gives the sizes of three models in terms of transitions and states, for both the failure transition and v -transition encoding of the model.", "labels": [], "entities": []}, {"text": "The DARPA North American Business News (NAB) corpus contains 250 million words, with a vocabulary of 463,331 words.", "labels": [], "entities": [{"text": "DARPA North American Business News (NAB) corpus", "start_pos": 4, "end_pos": 51, "type": "DATASET", "confidence": 0.8373446994357638}]}, {"text": "The Switchboard training corpus has 3.1 million words, and a vocabulary of 45,643.", "labels": [], "entities": [{"text": "Switchboard training corpus", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.6315870682398478}]}, {"text": "The number of transitions needed for the exact offline representation in each case was between 2 and 3 times the number of transitions used in the representation with failure transitions, and the number of states was less than twice the original number of states.", "labels": [], "entities": []}, {"text": "This shows that our technique is practical even for very large tasks.", "labels": [], "entities": []}, {"text": "Efficient implementations of model building algorithms have been incorporated into the GRM library.", "labels": [], "entities": [{"text": "GRM library", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.9037310481071472}]}, {"text": "The GRM utility grmmake produces basic backoff models, using Katz or Absolute discounting or Stolcke (1998).", "labels": [], "entities": []}, {"text": "The utility grmconvert takes a backoff model produced by grmmake or grmshrink and converts it into an exact model using either failure transitions or the algorithm just described.", "labels": [], "entities": []}, {"text": "It also converts the model to an interpolated model for use in the tropical semiring.", "labels": [], "entities": []}, {"text": "As an example, the following command line: grmmake -n3 counts.fsm > model.fsm creates a basic Katz backoff trigram model from the counts produced by the command line example in the earlier section.", "labels": [], "entities": []}, {"text": "The command: grmshrink -c1 model.fsm > m.s1.fsm shrinks the trigram model using the weighted difference method) with a threshold of 1.", "labels": [], "entities": []}, {"text": "Finally, the command: grmconvert -tfail m.s1.fsm > f.s1.fsm outputs the model represented with failure transitions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of models (in thousands) built from the  NAB and Switchboard corpora, with failure transitions", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9422320127487183}, {"text": "NAB", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9712100028991699}, {"text": "Switchboard corpora", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.8128058016300201}]}]}