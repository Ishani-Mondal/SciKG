{"title": [], "abstractContent": [{"text": "We advance the state-of-the-art for discrimi-natively trained machine translation systems by presenting novel probabilistic inference and search methods for synchronous grammars.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.716486781835556}]}, {"text": "By approximating the intractable space of all candidate translations produced by intersecting an ngram language model with asynchronous grammar, we are able to train and decode models incorporating millions of sparse, heterogeneous features.", "labels": [], "entities": []}, {"text": "Further, we demonstrate the power of the discriminative training paradigm by extracting structured syntactic features, and achieving increases in translation performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.788637156287829}]}, {"text": "Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art.", "labels": [], "entities": []}, {"text": "Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (;), or rescaling a product of sub-models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9948118925094604}]}, {"text": "Recent work by has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9844198822975159}]}, {"text": "Their approach was globally optimised and discriminative trained.", "labels": [], "entities": []}, {"text": "However, a language model, an information source known to be crucial for obtaining good performance in SMT, was notably omitted.", "labels": [], "entities": [{"text": "SMT", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9952607750892639}]}, {"text": "This was because adding a language model would mean that the normalising partition function could no longer be exactly calculated, thereby preventing efficient parameter estimation.", "labels": [], "entities": []}, {"text": "Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model.", "labels": [], "entities": []}, {"text": "The key insight is that we can use Monte-Carlo methods to approximate the partition function, thereby allowing us to tackle the extra computational burden associated with adding the language model.", "labels": [], "entities": []}, {"text": "This approach is theoretically justified and means that the model continues to be both probabilistic and globally optimised.", "labels": [], "entities": []}, {"text": "As expected, using a language model dramatically increases translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9622239470481873}]}, {"text": "Our second major contribution is an exploitation of syntactic features.", "labels": [], "entities": []}, {"text": "By encoding source syntax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar (.", "labels": [], "entities": [{"text": "coverage", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9611682295799255}]}, {"text": "We report on translation gains using this approach.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9424954056739807}]}, {"text": "We begin by introducing the synchronous grammar approach to SMT in Section 2.", "labels": [], "entities": [{"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.994648277759552}]}, {"text": "In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations fora given source sentence.", "labels": [], "entities": []}, {"text": "In Section 4 we evaluate the ability of our model to effectively estimate the highly dependent weights for the sparse features and realvalued language model.", "labels": [], "entities": []}, {"text": "In addition we describe how our model can easily integrate rich features over source syntax trees and compare our training methods to a state-of-the-art benchmark.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on the IWSLT 2005 Chinese to English translation task, using the 2004 test set as development data for tuning the hyperparameters and MERT training the benchmark systems.", "labels": [], "entities": [{"text": "IWSLT 2005 Chinese to English translation task", "start_pos": 29, "end_pos": 75, "type": "TASK", "confidence": 0.7632292338779995}, {"text": "2004 test set", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.7754652500152588}, {"text": "MERT", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.8374425172805786}]}, {"text": "The statistics for this data are presented in.", "labels": [], "entities": []}, {"text": "The training data made available for this task consisted of 40k pairs of transcribed utterances, drawn from the travel domain.", "labels": [], "entities": []}, {"text": "The development and test data for this task are somewhat unusual in that each sentence has a single human translated reference, and fifteen paraphrases of this reference, provided by monolingual annotators.", "labels": [], "entities": []}, {"text": "Model performance is evaluated using the standard BLEU metric () which measures average n-gram precision, n \u2264 4, and we use the NIST definition of the brevity penalty for multiple reference test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9955046772956848}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9196416735649109}, {"text": "NIST", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.8665026426315308}]}, {"text": "We provide evaluation against both the entire multi-reference sets, and the single human translation.", "labels": [], "entities": []}, {"text": "Our translation grammar is induced using the standard alignment and rule extraction heuristics used in hierarchical translation models.", "labels": [], "entities": [{"text": "translation grammar", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9453228116035461}]}, {"text": "As these heuristics aren't based on a generative model, and don't guarantee that the target translation will be reachable from the source, we discard those sentence pairs for which we cannot produce a derivation, leaving 38,405 sentences for training.", "labels": [], "entities": []}, {"text": "Our base model contains a single feature for each rule which counts the number of times it appeared in a particular derivation.", "labels": [], "entities": []}, {"text": "For models which include a language model, we train a standard Kneser-Ney trigram model on the target side of the training corpus.", "labels": [], "entities": []}, {"text": "We also include a word penalty feature to compensate for the shortening effect of the language model.", "labels": [], "entities": []}, {"text": "In total our model contains 2.9M features.", "labels": [], "entities": []}, {"text": "The aims of our evaluation are: (1) to determine that our proposed training regimes are able to realise performance increase when training sparse rule features and areal valued language model feature together, (2) that the model is able to effectively use rich features over the source sentence, and to confirm that our model obtains performance competitive with the current state-of-the-art.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. IWSLT Chinese to English translation corpus statistics.", "labels": [], "entities": [{"text": "IWSLT Chinese to English translation corpus statistics", "start_pos": 10, "end_pos": 64, "type": "DATASET", "confidence": 0.7321945003100804}]}, {"text": " Table 2. Development set results for varying the approximation of the partition function in training, \u02dc  Z cb  \u039b (f ) vs. \u02dc  Z sam", "labels": [], "entities": []}, {"text": " Table 3. Test set results.", "labels": [], "entities": []}]}