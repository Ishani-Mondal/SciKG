{"title": [{"text": "Attacking Decipherment Problems Optimally with Low-Order N-gram Models", "labels": [], "entities": [{"text": "Attacking Decipherment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8564533293247223}]}], "abstractContent": [{"text": "We introduce a method for solving substitution ciphers using low-order letter n-gram models.", "labels": [], "entities": [{"text": "solving substitution ciphers", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.700298031171163}]}, {"text": "This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked.", "labels": [], "entities": []}, {"text": "We carryout extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9786155223846436}]}, {"text": "We also make an empirical investigation of Shannon's (1949) theory of uncertainty in decipherment.", "labels": [], "entities": []}], "introductionContent": [{"text": "A number of papers have explored algorithms for automatically solving letter-substitution ciphers.", "labels": [], "entities": []}, {"text": "Some use heuristic methods to search for the best deterministic key, often using word dictionaries to guide that search.", "labels": [], "entities": []}, {"text": "Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models ().", "labels": [], "entities": []}, {"text": "In this paper, we introduce an exact decipherment method based on integer programming.", "labels": [], "entities": []}, {"text": "We carryout extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9995550513267517}]}, {"text": "We also empirically explore the concepts in Shannon's (1949) paper on information theory as applied to cipher systems.", "labels": [], "entities": [{"text": "information theory", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.822631299495697}]}, {"text": "We provide quantitative plots for uncertainty in decipherment, including the famous unicity distance, which estimates how long a cipher must be to virtually eliminate such uncertainty.", "labels": [], "entities": []}, {"text": "We find the ideas in paper relevant to problems of statistical machine translation and transliteration.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.7252590457598368}]}, {"text": "When first exposed to the idea of statistical machine translation, many people naturally ask: (1) how much data is needed to get a good result, and (2) can translation systems be trained without parallel data?", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.6901049911975861}]}, {"text": "These are tough questions by any stretch, and it is remarkable that Shannon was already in the 1940s tackling such questions in the realm of code-breaking, creating analytic formulas to estimate answers.", "labels": [], "entities": []}, {"text": "Our novel contributions are as follows: \u2022 We outline an exact letter-substitution decipherment method which: -guarantees that no key is overlooked, and -can be executed with standard integer programming solvers \u2022 We present empirical results for decipherment which: -plot search-error-free decipherment results at various cipher lengths, and -demonstrate accuracy rates superior to EMbased methods \u2022 We carryout empirical testing of Shannon's formulas for decipherment uncertainty most probable according to some statistical language model, whose job is to assign some probability to any sequence of letters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 355, "end_pos": 363, "type": "METRIC", "confidence": 0.9979782700538635}]}, {"text": "According to a 1-gram model of English, the probability of a plaintext p 1 ...p n is given by: That is, we obtain the probability of a sequence by multiplying together the probabilities of the individual letters that make it up.", "labels": [], "entities": []}, {"text": "This model assigns a probability to any letter sequence, and the probabilities of all letter sequences sum to one.", "labels": [], "entities": []}, {"text": "We collect letter probabilities (including space) from 50 million words of text available from the Linguistic Data Consortium (.", "labels": [], "entities": [{"text": "Linguistic Data Consortium", "start_pos": 99, "end_pos": 125, "type": "DATASET", "confidence": 0.7762446105480194}]}, {"text": "We also estimate 2-and 3-gram models using the same resources: Unlike the 1-gram model, the 2-gram model will assign a low probability to the sequence \"ae\" because the probability P (e | a) is low.", "labels": [], "entities": []}, {"text": "Of course, all these models are fairly weak, as already known by.", "labels": [], "entities": []}, {"text": "When we stochastically generate text according to these models, we get, for example: 1-gram: ...", "labels": [], "entities": []}, {"text": "thdo detusar ii c ibt deg irn toihytrsen ...", "labels": [], "entities": []}, {"text": "We can further estimate the probability of a whole English sentence or phrase.", "labels": [], "entities": []}, {"text": "For example, the probabilities of two plaintext phrases \"het oxf\" and \"the fox\" (which have the same letter frequency distribution) is shown below.", "labels": [], "entities": []}, {"text": "The 1-gram model which counts only the frequency of occurrence of each letter in the phrase, estimates the same probability for both the phrases \"het oxf\" and \"the fox\", since the same letters occur in both phrases.", "labels": [], "entities": []}, {"text": "On the other hand, the 2-gram and 3-gram models, which take context into account, are able to distinguish between the English and non-English phrases better, and hence assign a higher probability to the English phrase \"the fox\".", "labels": [], "entities": []}, {"text": "Over a longer sequence X of length N , we can also calculate \u2212log 2 (P (X))/N , which (per Shannon) gives the compression rate permitted by the model, in bits per character.", "labels": [], "entities": []}, {"text": "In our case, we get: 1 1-gram: 4.19 2-gram: 3.51 3-gram: 2.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}