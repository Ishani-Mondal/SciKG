{"title": [{"text": "Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.8523560166358948}]}], "abstractContent": [{"text": "We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses.", "labels": [], "entities": []}, {"text": "We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices.", "labels": [], "entities": []}, {"text": "We introduce an approximation to the BLEU score (Pap-ineni et al., 2001) that satisfies these conditions.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9865382015705109}]}, {"text": "The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9965658783912659}]}, {"text": "Our experiments show that the Lattice MBR decoder yields moderate , consistent gains in translation performance over N-best MBR decoding on Arabic-to-English, Chinese-to-English and English-to-Chinese translation tasks.", "labels": [], "entities": [{"text": "Lattice MBR decoder", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.858883261680603}, {"text": "translation", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.948139488697052}]}, {"text": "We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.", "labels": [], "entities": [{"text": "MBR", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.6269561052322388}, {"text": "MBR", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.961554229259491}]}], "introductionContent": [{"text": "Statistical language processing systems for speech recognition, machine translation or parsing typically employ the Maximum A Posteriori (MAP) decision rule which optimizes the 0-1 loss function.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7690661251544952}, {"text": "machine translation or parsing", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7316287904977798}, {"text": "Maximum A Posteriori (MAP) decision rule", "start_pos": 116, "end_pos": 156, "type": "METRIC", "confidence": 0.843667097389698}]}, {"text": "In contrast, these systems are evaluated using metrics based on string-edit distance (Word Error Rate), ngram overlap (BLEU score ()), or precision/recall relative to human annotations.", "labels": [], "entities": [{"text": "Word Error Rate)", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.6675689369440079}, {"text": "ngram overlap (BLEU score", "start_pos": 104, "end_pos": 129, "type": "METRIC", "confidence": 0.8585970878601075}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9985156655311584}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9203829765319824}]}, {"text": "Minimum Bayes-Risk (MBR) decoding aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification.", "labels": [], "entities": [{"text": "Minimum Bayes-Risk (MBR) decoding", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6066513508558273}]}, {"text": "Thus it directly incorporates the loss function into the decision criterion.", "labels": [], "entities": []}, {"text": "The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition), machine translation (, bilingual word alignment (, and parsing.", "labels": [], "entities": [{"text": "MAP classifier", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7795306146144867}, {"text": "automatic speech recognition", "start_pos": 128, "end_pos": 156, "type": "TASK", "confidence": 0.7074568470319113}, {"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.8265246748924255}, {"text": "bilingual word alignment", "start_pos": 182, "end_pos": 206, "type": "TASK", "confidence": 0.6098158558209738}]}, {"text": "In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6597441832224528}, {"text": "MBR decoding", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.7842190265655518}]}, {"text": "show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function () gives gains on BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9294726848602295}]}, {"text": "This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU.", "labels": [], "entities": [{"text": "BLEU loss function", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.8977830608685812}]}, {"text": "A different MBR inspired decoding approach is pursued in for machine translation using Synchronous Context Free Grammars.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7906385660171509}]}, {"text": "A forest generated by an initial decoding pass is rescored using dynamic programming to maximize the expected count of synchronous constituents in the tree that corresponds to the translation.", "labels": [], "entities": []}, {"text": "Since each constituent adds anew 4-gram to the existing translation, this approach approximately maximizes the expected BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9984415173530579}]}, {"text": "In this paper we explore a different strategy to perform MBR decoding over Translation Lattices () that compactly encode a huge number of translation alternatives relative to an N -best list.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.8532488644123077}]}, {"text": "This is a model-independent approach in that the lattices could be produced by any statistical MT system -both phrase-based and syntaxbased systems would work in this framework.", "labels": [], "entities": []}, {"text": "We will introduce conditions on the loss functions that can be incorporated in Lattice MBR decoding.", "labels": [], "entities": [{"text": "Lattice MBR decoding", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.7922272284825643}]}, {"text": "We describe an approximation to the BLEU score) that will satisfy these conditions.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9846944808959961}]}, {"text": "Our Lattice MBR decoding is realized using Weighted Finite State Automata.", "labels": [], "entities": []}, {"text": "We expect Lattice MBR decoding to improve upon N -best MBR primarily because lattices contain many more candidate translations than the Nbest list.", "labels": [], "entities": []}, {"text": "This has been demonstrated in speech recognition).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8786281049251556}]}, {"text": "We conduct a range of translation experiments to analyze lattice MBR and compare it with N -best MBR.", "labels": [], "entities": []}, {"text": "An important aspect of our lattice MBR is the linear approximation to the BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9729845821857452}]}, {"text": "We will show that MBR decoding under this score achieves a performance that is at least as good as the performance obtained under sentence-level BLEU score.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.499702587723732}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9355298280715942}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We review MBR decoding in Section 2 and give the formulation in terms of again function.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.5137099623680115}]}, {"text": "In Section 3, we describe the conditions on the gain function for efficient decoding over a lattice.", "labels": [], "entities": []}, {"text": "The implementation of lattice MBR with Weighted Finite State Automata is presented in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we introduce the corpus BLEU approximation that makes it possible to perform efficient lattice MBR decoding.", "labels": [], "entities": [{"text": "BLEU approximation", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.9583489298820496}]}, {"text": "An example of lattice MBR with a toy lattice is presented in Section 6.", "labels": [], "entities": [{"text": "MBR", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.6965845227241516}]}, {"text": "We present lattice MBR experiments in Section 7.", "labels": [], "entities": []}, {"text": "A final discussion is presented in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now present experiments to evaluate MBR decoding on lattices under the linear corpus BLEU  gain.", "labels": [], "entities": [{"text": "MBR decoding", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.8023004233837128}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9811108708381653}]}, {"text": "We start with a description of the data sets and the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9914103150367737}]}], "tableCaptions": [{"text": " Table 1: Statistics over the development and test sets.", "labels": [], "entities": []}, {"text": " Table 2: Lattice MBR, N -best MBR & MAP decoding.  On zhen, Lattice MBR and N -best MBR do not show  statistically significant differences.", "labels": [], "entities": []}, {"text": " Table 3: Lattice and N-best MBR (with Sentence  BLEU/Sentence log BLEU) on a 1000-best list. In each  column, entries with an asterisk do not show statistically  significant differences.", "labels": [], "entities": [{"text": "Lattice", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8366240859031677}, {"text": "N-best MBR", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.7412120699882507}, {"text": "Sentence  BLEU/Sentence log BLEU", "start_pos": 39, "end_pos": 71, "type": "METRIC", "confidence": 0.5677963097890218}]}, {"text": " Table 4: Lattice MBR with restrictions on hypothesis and  evidence spaces. In each column, entries with an asterisk  do not show statistically significant differences.", "labels": [], "entities": [{"text": "Lattice MBR", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.5373267233371735}]}, {"text": " Table 5: Lattice MBR as a function of max n-gram order.", "labels": [], "entities": [{"text": "Lattice MBR", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.6545418798923492}]}]}