{"title": [], "abstractContent": [{"text": "Predicting possible code-switching points can help develop more accurate methods for automatically processing mixed-language text, such as multilingual language models for speech recognition systems and syntactic an-alyzers.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.7048270404338837}]}, {"text": "We present in this paper exploratory results on learning to predict potential code-switching points in Spanish-English.", "labels": [], "entities": []}, {"text": "We trained different learning algorithms using a transcription of code-switched discourse.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the classifiers, we used two different criteria: 1) measuring precision , recall, and F-measure of the predictions against the reference in the transcription , and 2) rating the naturalness of artificially generated code-switched sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9995118379592896}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9990494847297668}, {"text": "F-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.99946528673172}]}, {"text": "Average scores for the code-switched sentences generated by our machine learning approach were close to the scores of those generated by humans.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multilingual speakers often switchback and forth between languages when speaking or writing, mostly in informal settings.", "labels": [], "entities": []}, {"text": "The mixing of languages involves very elaborated patterns and forms and we usually use the term Code-Switching (CS) to encompass all of them.", "labels": [], "entities": []}, {"text": "Before the Internet era, CS was mainly used in its spoken form.", "labels": [], "entities": []}, {"text": "But with so many different informal interaction settings, such as chats, forums, blogs, and web sites like Myspace and Facebook, CS is being used more and more in written form.", "labels": [], "entities": []}, {"text": "For English and Spanish, CS has taken a step further.", "labels": [], "entities": []}, {"text": "It has become a hallmark of the chicano culture as it is evident by the growing number of chicano writers publishing work in Spanish-English CS.", "labels": [], "entities": []}, {"text": "We have not completely discovered the process of human language acquisition, especially dual language acquisition.", "labels": [], "entities": [{"text": "human language acquisition", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.6683758894602457}, {"text": "dual language acquisition", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.7538680036862692}]}, {"text": "Findings in linguistics, sociolinguistics, and psycholinguistics show that the production of code-switched discourse requires a very sophisticated knowledge of the languages being mixed.", "labels": [], "entities": []}, {"text": "Some theories suggest bilingual speakers might have a third grammar for processing this type of discourse.", "labels": [], "entities": []}, {"text": "The general agreement regarding CS is that switches do not take place at random and instead it is possible to identify rules that bilingual speakers adhere to.", "labels": [], "entities": []}, {"text": "Understanding the CS process can lead to accurate methods for the automatic processing of bilingual discourse, and corpus-driven studies about CS can also inform linguistic theories.", "labels": [], "entities": []}, {"text": "In this paper we present exploratory work on learning to predict CS points using a machine learning approach.", "labels": [], "entities": []}, {"text": "Such an approach can be used to reduce perplexity of language models for bilingual discourse.", "labels": [], "entities": []}, {"text": "We believe that CS behavior can be learned by a classifier and the results presented in this paper support our belief.", "labels": [], "entities": []}, {"text": "One of the difficult aspects of trying to predict CS points is how to evaluate the performance of the learner since switching is intrinsically motivated and there are no forced switches.", "labels": [], "entities": []}, {"text": "Therefore, standard classification measures for this task such as precision, recall, F-measure, or accuracy, are not the best approach for measuring the effectiveness of a CS predictor.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9994999170303345}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9976670742034912}, {"text": "F-measure", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9963905215263367}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9994606375694275}]}, {"text": "To comple-ment the evaluation of our approach, we designed a task involving human judgements on the naturalness of automatically generated code-switched sentences.", "labels": [], "entities": []}, {"text": "Both evaluations yielded encouraging results.", "labels": [], "entities": []}, {"text": "The next section discusses theories explaining the CS production process.", "labels": [], "entities": []}, {"text": "Then in Section 3 we present our framework for learning to predict CS points.", "labels": [], "entities": []}, {"text": "Section 4 discusses the empirical evaluation of the classifiers compared to the human reference.", "labels": [], "entities": []}, {"text": "In Section 5 we present results of human evaluations on automatically generated code-switched sentences.", "labels": [], "entities": []}, {"text": "Section 6 describes previous work related to the processing of code-switched text.", "labels": [], "entities": [{"text": "processing of code-switched text", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.7960082590579987}]}, {"text": "Finally, we conclude in Section 7 with a summary of our findings and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This is the standard evaluation of machine learning classifiers.", "labels": [], "entities": []}, {"text": "We randomly divided the data into sentences and grouped them into 10 subsets to perform a cross-validation.", "labels": [], "entities": []}, {"text": "show results for Naive Bayes (NB) and Value Feature Interval (VFI).", "labels": [], "entities": [{"text": "Value Feature Interval (VFI)", "start_pos": 38, "end_pos": 66, "type": "METRIC", "confidence": 0.5982365061839422}]}, {"text": "Using WEKA (Witten and Frank, 1999), we experimented with different subsets of the attributes and two context windows: using only the preceding word and using the previous two words.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.6919407844543457}]}, {"text": "The results presented here are overall averages of 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We also report standard deviations.", "labels": [], "entities": [{"text": "standard", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9502404928207397}]}, {"text": "It should be noted that the Spanglish data set is highly imbalanced, around 96% of the instances belong to the negative class.", "labels": [], "entities": [{"text": "Spanglish data set", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9336827198664347}]}, {"text": "Therefore, our comparisons are based on Precision, Recall, and F-measure, leaving accuracy aside, since a weak classifier predicting that all instances belong to the negative class will reach an accuracy of 96%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9991412162780762}, {"text": "Recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9678855538368225}, {"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9987547397613525}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9992886781692505}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9990159273147583}]}, {"text": "The performance measures shown on show that NB outperforms VFI inmost of the configurations tested.", "labels": [], "entities": [{"text": "NB", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.5101833939552307}, {"text": "VFI", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.431086003780365}]}, {"text": "In particular, NB yields the best results when using a 1 word context with no lexical forms nor lemmas as attributes (see row 3).", "labels": [], "entities": []}, {"text": "This is a fortunate finding -for most practical problems there will always be words in the test set that have not been observed in the training set.", "labels": [], "entities": []}, {"text": "For our small Spanglish data set that will certainly be the case.", "labels": [], "entities": [{"text": "Spanglish data set", "start_pos": 14, "end_pos": 32, "type": "DATASET", "confidence": 0.9300740361213684}]}, {"text": "In contrast, VFI achieves higher F-measures when using a context of two words and all the features are used.", "labels": [], "entities": [{"text": "VFI", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6121866106987}, {"text": "F-measures", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9986961483955383}]}, {"text": "Analyzing the predictions of the learners we noted that the NB classifier is heavily biased by the language attribute, close to 80% of the positive predictions made by NB are after seeing a word in Spanish.", "labels": [], "entities": []}, {"text": "This preference seems to support the assumption of the asymmetry between the two languages and the existence of an ML . This however is not the case for VFI, only a little over 50% of the positive predictions belong to this scenario.", "labels": [], "entities": [{"text": "VFI", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.6322005987167358}]}, {"text": "Another interesting finding is the learner's tendency to predict a code-switch after observing words like \"Yeah\", \"anyway\", \"no\", and \"shower\".", "labels": [], "entities": []}, {"text": "The first two seem to fit the pattern of idiomatic expressions.", "labels": [], "entities": []}, {"text": "According to Montes-Alcal\u00e1 this type of CS includes linguistic routines and fillers that are difficult to translate accurately, which might be the case of \"anyway\", and unconscious changes, which can explain the case of \"Yeah\".", "labels": [], "entities": []}, {"text": "The case of \"shower\" and \"no\" are more difficult to explain, they might be overfitting patterns from the learners.", "labels": [], "entities": []}, {"text": "We also found out that VFI learned to predict that a CS will take place right after seeing the sequence of words le dije (I said).", "labels": [], "entities": [{"text": "VFI", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.7709291577339172}]}, {"text": "This sequence of words is frequently used when the speaker is about to quote his/herself, and this quotation is one of the welldocumented CS functions.", "labels": [], "entities": []}, {"text": "A greedy search approach for attribute selection using WEKA showed that out of the 20 attributes (when using a two word context), the subset with the highest predictive value included the language identification for word w n\u22121 and w n\u22122 , the confidence threshold from the English tagger for word w n\u22122 , the lemma from the Spanish Tree tagger for w n\u22121 , and the lexical form of the word w n\u22121 . We expected the chunk information to be useful and this does not seem to be the case.", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7187926173210144}, {"text": "WEKA", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8843465447425842}, {"text": "Spanish Tree tagger", "start_pos": 324, "end_pos": 343, "type": "DATASET", "confidence": 0.8393094142278036}]}, {"text": "Another unexpected outcome is that higher F-measures are reached by adding features generated by the monolingual Tree taggers.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9752807021141052}]}, {"text": "Even though these features are noisy, they still carry useful information.", "labels": [], "entities": []}, {"text": "We only show results from NB and VFI.", "labels": [], "entities": [{"text": "NB", "start_pos": 26, "end_pos": 28, "type": "DATASET", "confidence": 0.9384792447090149}, {"text": "VFI", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.8235531449317932}]}, {"text": "Initial experiments with a subset of the data showed that these algorithms were the most promising for this task.", "labels": [], "entities": []}, {"text": "They both yielded higher F-measures, even when compared against Support Vector Machines (SVMs), C4.5, and neural networks.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9991657733917236}]}, {"text": "On this experiment all the discriminative classifiers reached a classification accuracy close to 96%, but an F-  measure on the positive class of around 0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9406479597091675}, {"text": "F-  measure", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9927724401156107}]}, {"text": "NB and VFI estimate predictions for each class separately, which makes them robust to imbalanced data sets.", "labels": [], "entities": [{"text": "NB", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8888658881187439}]}, {"text": "In addition, generative models are known to be better for smaller data sets since they reach their higher asymptotic error much faster than discriminative models).", "labels": [], "entities": [{"text": "generative", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9703530669212341}]}, {"text": "This might explain why Naive Bayes outperformed strong classifiers such as SVMs by a large margin.", "labels": [], "entities": []}, {"text": "The overall prediction performance is not very high.", "labels": [], "entities": []}, {"text": "However, we should remark that for this particular task expecting a high F-measure is unrealistic.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9954146146774292}]}, {"text": "Consider for example, a case where the learners predict a CS point where the speaker decided not to switch, this does not imply that particular point is not a good CS point.", "labels": [], "entities": []}, {"text": "And similarly, if the classifier missed an existing CS point in the reference data set the resulting sentence might still be grammatical and natural sounding.", "labels": [], "entities": []}, {"text": "This motivated the use of an alternative evaluation, which we discuss below.", "labels": [], "entities": []}, {"text": "The goal of this evaluation is to explore how humans perceive our automatically generated CS sentences, and in particular, how do they compare to the original sentences and to the randomly generated ones.", "labels": [], "entities": []}, {"text": "We selected 30 spontaneous and naturally occurring CS sentences from different sources.", "labels": [], "entities": []}, {"text": "Some of them were selected from the Spanglish Times Magazine 5 , some others from blogs found in).", "labels": [], "entities": [{"text": "Spanglish Times Magazine 5", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.9743130505084991}]}, {"text": "Other sentences were taken from a paper discussing CS on e-mails).", "labels": [], "entities": []}, {"text": "All of the sentences are true occurrences of written CS, from speakers different from the ones in the Spanglish data set.", "labels": [], "entities": [{"text": "Spanglish data set", "start_pos": 102, "end_pos": 120, "type": "DATASET", "confidence": 0.9407558639844259}]}, {"text": "The sentences were translated to standard English and Spanish and were manually aligned.", "labels": [], "entities": []}, {"text": "We will use this parallel set of sentences to predict CS points with our models.", "labels": [], "entities": []}, {"text": "Based on the model predictions we will generate code-switched sentences by combining monolingual fragments.", "labels": [], "entities": []}, {"text": "It should be noted that the Spanglish data set is a transcription of spoken CS.", "labels": [], "entities": [{"text": "Spanglish data set", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9597492615381876}]}, {"text": "In contrast, this new evaluation set contains only written CS.", "labels": [], "entities": []}, {"text": "Recent studies suggest written CS will adhere to the rules of spoken CS), but there is still some controversy on this issue.", "labels": [], "entities": []}, {"text": "From our perspective, both samples come from informal conversational interactions.", "labels": [], "entities": []}, {"text": "It is expected that both will have similar patterns and therefore will provide a good source for our evaluation.", "labels": [], "entities": []}, {"text": "We had a total of 18 subjects participating in the experiment.", "labels": [], "entities": []}, {"text": "All of them identified themselves as being able to read and write Spanish and English, and the majority of them said to have used CS at least some times.", "labels": [], "entities": []}, {"text": "We showed to the human subjects the 28 sets of sentences.", "labels": [], "entities": []}, {"text": "This time we included the original version of the sentence.", "labels": [], "entities": []}, {"text": "Therefore, each judge was given 4 versions of each of the 28 code-switched sentences: the one generated from NB predictions, the one from VFI, the randomly generated, and the original one.", "labels": [], "entities": [{"text": "VFI", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.9144496917724609}]}, {"text": "Then we asked them to rate each sentence with a number from 1 to 5 indicating how natural and human-like the sentence sounds.", "labels": [], "entities": []}, {"text": "A rating of 5 means that they strongly agree, 4 means they agree, 3 not sure, 2 disagree, 1 strongly disagree.", "labels": [], "entities": []}, {"text": "The average results are presented in.", "labels": [], "entities": []}, {"text": "The sentences generated by NB were scored considerably higher than those from VFI and random, and closer to the human sentences.", "labels": [], "entities": [{"text": "VFI", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8125900626182556}]}, {"text": "According to the paired t-test the difference between the NB score and the random one is significant (p=0.01).", "labels": [], "entities": [{"text": "NB score", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.7080063223838806}]}, {"text": "However the average score for VFI is lower than random.", "labels": [], "entities": [{"text": "VFI", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.3546852469444275}]}, {"text": "More experiments are needed to see if by choosing the setting where VFI had the highest F-measure would make a difference in this respect.", "labels": [], "entities": [{"text": "VFI", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.7602499723434448}, {"text": "F-measure", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9978148937225342}]}, {"text": "Overall the subjects rated the human-generated CS sentences lower than what we were expecting, although it is clear that they consider these sentences more natural sounding than the rest.", "labels": [], "entities": []}, {"text": "This low rating might be related to the attitude several evaluators expressed toward CS.", "labels": [], "entities": []}, {"text": "In the evaluation form we asked the judges to express their opinion on CS and several of them indicated feelings along the lines of \"we shouldn't codeswitch\".", "labels": [], "entities": []}, {"text": "There are several ways in which two parallel sentences can be combined in CS, and possibly several will sound natural, but from our results, it is clear that the NB algorithm was indeed able to generate a human-like CS behavior that was successfully differentiated from randomly-generated sentences.", "labels": [], "entities": []}, {"text": "By looking at the set of automatically generated code-switched sentences, we realized that the majority of the sentences are grammatical and natural sounding.", "labels": [], "entities": []}, {"text": "We believe that fora large number of the sentences it would be hard fora human to distinguish the sentences that were automatically generated from the human-generated ones.", "labels": [], "entities": []}, {"text": "One of the giveaway clues is when a multi-word expression is CS, or a tagline.", "labels": [], "entities": []}, {"text": "shows three examples from the sentences evaluated.", "labels": [], "entities": []}, {"text": "In the table there is an example in sentence 1c where the noun phrase is code-switched, the sentence is grammatical according to Spanish rules, but it sounds very odd to have the noun carta followed by the adjective in English, \"astrological\".", "labels": [], "entities": []}, {"text": "Other interesting features are present in example 3 where for the same noun phrase \"produce section\" we have both, the female marking determiner la and the masculine el.", "labels": [], "entities": []}, {"text": "The same thing happens for the noun phrase \"check-out line\".", "labels": [], "entities": []}, {"text": "We would need to have a larger occurrence of these instances in our test set to determine if on average one form is preferred over the other.", "labels": [], "entities": []}, {"text": "In another experiment, we measured the prediction performance of NB and VFI on the 30 codeswitched sentences used in this part of the evaluation.", "labels": [], "entities": [{"text": "VFI", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9770804643630981}]}, {"text": "The best results, an F-measure of 0.418, were achieved by NB when a context of 1 word was used, and no words, nor lemmas were included as features.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9988194108009338}, {"text": "NB", "start_pos": 58, "end_pos": 60, "type": "DATASET", "confidence": 0.8158112168312073}]}, {"text": "This is the same setting used for the generation process.", "labels": [], "entities": []}, {"text": "In contrast, VFI reached an F-measure of 0.351 on this same setting.", "labels": [], "entities": [{"text": "VFI", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.5184261202812195}, {"text": "F-measure", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9988287091255188}]}, {"text": "30 sentences represent a very small dataset but the results are very promising since the speakers are different in the training and testing dataset.", "labels": [], "entities": []}, {"text": "Moreover, these results support the claim that written and spoken CS obey similar rules.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Features explored in learning to predict CS  points.", "labels": [], "entities": []}, {"text": " Table 2: Prediction results of CS points with NB using different features. Column C indicates the size of the context  used, 1 indicates a 1 word context, and 2 indicates two words preceding the word boundary. Columns P, R, and  F 1 , show precision, recall, and F-measure, respectively. Numbers in parenthesis show standard deviations. The row  marked with a '*' shows the configuration used for the generation of CS sentences presented in Section 5.", "labels": [], "entities": [{"text": "precision", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.9987245202064514}, {"text": "recall", "start_pos": 252, "end_pos": 258, "type": "METRIC", "confidence": 0.9935793876647949}, {"text": "F-measure", "start_pos": 264, "end_pos": 273, "type": "METRIC", "confidence": 0.9841190576553345}]}, {"text": " Table 3: Prediction results of CS points with VFI using different features. The notation on this table is the same as in", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9098228216171265}]}, {"text": " Table 4: Average score of 18 judges over the set of 28  code-switched sentences rated.", "labels": [], "entities": [{"text": "Average score", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9715226292610168}]}]}