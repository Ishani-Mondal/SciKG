{"title": [{"text": "Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections.", "labels": [], "entities": [{"text": "Lexical gaps between queries and questions (documents)", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7993436257044474}, {"text": "question retrieval on large online question and answer (Q&A) collections", "start_pos": 82, "end_pos": 154, "type": "TASK", "confidence": 0.7021331467798778}]}, {"text": "Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques.", "labels": [], "entities": []}, {"text": "However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models , alack of noise control on the models can cause degradation of retrieval performance.", "labels": [], "entities": []}, {"text": "This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes.", "labels": [], "entities": []}, {"text": "Experiments conducted on areal world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.", "labels": [], "entities": [{"text": "areal world Q&A collection", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7377647956212362}]}], "introductionContent": [{"text": "Community-driven question answering services, such as Yahoo!", "labels": [], "entities": [{"text": "question answering", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.731143593788147}]}, {"text": "Answers 1 and Live Search QnA 2 , have been rapidly gaining popularity among Web users interested in sharing information online.", "labels": [], "entities": []}, {"text": "By inducing users to collaboratively submit questions and answer questions posed by other users, large amounts of information have been collected in the form of question and answer (Q&A) pairs in recent years.", "labels": [], "entities": []}, {"text": "This user-generated information is a valuable resource for many information seekers, because users can acquire information straightforwardly by searching through answered questions that satisfy their information need.", "labels": [], "entities": []}, {"text": "Retrieval models for such Q&A collections should manage to handle the lexical gaps or word mismatches between user questions (queries) and answered questions in the collection.", "labels": [], "entities": []}, {"text": "Consider the two following examples of questions that are semantically similar to each other: \u2022 \"Where can I get cheap airplane tickets?\"", "labels": [], "entities": []}, {"text": "\u2022 \"Any travel website for low airfares?\"", "labels": [], "entities": []}, {"text": "Conventional word-based retrieval models would fail to capture the similarity between the two, because they have no words in common.", "labels": [], "entities": []}, {"text": "To bridge the query-question gap, prior work on Q&A retrieval by implicitly expands queries with the use of pre-constructed translation models, which lets you generate query words not in a question by translation to alternate words that are related.", "labels": [], "entities": [{"text": "Q&A retrieval", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.6257803440093994}]}, {"text": "In practice, these translation models are often constructed using statistical machine translation techniques that primarily rely on word co-occurrence statistics obtained from parallel strings (e.g., question-answer pairs).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6774948040644327}]}, {"text": "A critical issue of the translation-based approaches is the quality of translation models constructed in advance.", "labels": [], "entities": []}, {"text": "If no noise control is conducted during the construction, it is possible for translation models to contain \"unnecessary\" translations (i.e., translating a word into an unimportant word, such as a non-topical or common word).", "labels": [], "entities": []}, {"text": "In the query expansion viewpoint, an attempt to identify and decrease the proportion of unnecessary translations in a translation model may produce an effect of \"selective\" implicit query expansion and result in improved retrieval.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.751983106136322}]}, {"text": "However, prior work on translation-based Q&A retrieval does not recognize this issue and uses the translation model as it is; essentially no attention seems to have been paid to improving the performance of the translation-based approach by enhancing the quality of translation models.", "labels": [], "entities": [{"text": "translation-based Q&A retrieval", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7382866740226746}]}, {"text": "In this paper, we explore a number of empirical methods for selecting and eliminating unimportant words from parallel strings to avoid unnecessary translations from being learned in translation models built for retrieval purposes.", "labels": [], "entities": []}, {"text": "We use the term compact translation models to refer to the resulting models, since the total number of parameters for modeling translations would be minimized naturally.", "labels": [], "entities": []}, {"text": "We also present experiments in which compact translation models are used in Q&A retrieval.", "labels": [], "entities": [{"text": "Q&A retrieval", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.7575985044240952}]}, {"text": "The main goal of our study is to investigate if and how compact translation models can improve the performance of Q&A retrieval.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section introduces a translation-based retrieval model and accompanying techniques used to retrieve query-relevant questions.", "labels": [], "entities": [{"text": "translation-based retrieval", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.8810539245605469}]}, {"text": "Section 3 presents a number of empirical ways to select and eliminate unimportant words from parallel strings for training compact translation models.", "labels": [], "entities": []}, {"text": "Section 4 summarizes the compact translation models we built for retrieval experiments.", "labels": [], "entities": []}, {"text": "Section 5 presents and discusses the results of retrieval experiments.", "labels": [], "entities": []}, {"text": "Section 6 presents related works.", "labels": [], "entities": []}, {"text": "Finally, the last section concludes the paper and discusses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments have been conducted on areal world Q&A collection to demonstrate the effectiveness of compact translation models on Q&A retrieval.", "labels": [], "entities": [{"text": "areal world Q&A collection", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.5719544192155203}, {"text": "Q&A retrieval", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.737037405371666}]}, {"text": "In this section, four experimental settings for the Q&A retrieval experiments are described in detail.", "labels": [], "entities": [{"text": "Q&A retrieval", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.7964505851268768}]}, {"text": "Data: For the experiments, Q&A data have been collected from the Science domain of Yahoo!", "labels": [], "entities": [{"text": "Science domain of Yahoo!", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.9369049310684204}]}, {"text": "Answers, one of the most popular community-based question answering service on the Web.", "labels": [], "entities": [{"text": "question answering", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7098487615585327}]}, {"text": "We have obtained a total of 43,001 questions with a best answer (selected either by the questioner or by votes of other users) by recursively traversing subcategories of the Science domain, with up to 1,000 question pages retrieved.", "labels": [], "entities": [{"text": "Science domain", "start_pos": 174, "end_pos": 188, "type": "DATASET", "confidence": 0.8731278777122498}]}, {"text": "Among the obtained Q&A pairs, 32 Q&A pairs have been randomly selected as the test set, and the remaining 42,969 questions have been the reference set to be retrieved.", "labels": [], "entities": []}, {"text": "Each Q&A pair has three text fields: question title, question content, and answer.", "labels": [], "entities": []}, {"text": "The fields of each Q&A pair in the test set are considered as various test queries; the question title, the question content, and the answer are regarded as a short query, along query, and a supplementary query, respectively.", "labels": [], "entities": []}, {"text": "We have used long queries and supplementary queries only in the relevance judgment procedure.", "labels": [], "entities": [{"text": "relevance judgment", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9095408916473389}]}, {"text": "All retrieval experiments have been conducted using short queries only.", "labels": [], "entities": []}, {"text": "Relevance judgments: To find relevant Q&A pairs given a short query, we have employed a pooling technique used in the TREC conference series.", "labels": [], "entities": [{"text": "TREC conference series", "start_pos": 118, "end_pos": 140, "type": "DATASET", "confidence": 0.8862714568773905}]}, {"text": "We have pooled the top 40 Q&A pairs from each retrieval results generated by varying the retrieval algorithms, the search field, and the query type.", "labels": [], "entities": []}, {"text": "Popular word-based models, including the Okapi BM25, query-likelihood language model, and previous translation-based models (), have been used.", "labels": [], "entities": [{"text": "Okapi BM25", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.7719133496284485}]}, {"text": "Relevance judgments have been done by two student volunteers (both fluent in English).", "labels": [], "entities": []}, {"text": "Since many community-based question answering services present their search results in a hierarchical fashion (i.e. a list of relevant questions is shown first, and then the user chooses a specific question from the list to see its answers), a Q&A pair has been judged as relevant if its question is semantically similar to the query; neither quality nor rightness of the answer has not been considered.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7333729863166809}]}, {"text": "When a disagreement has been made between two volunteers, one of the authors has made the final judgment.", "labels": [], "entities": []}, {"text": "As a result, 177 relevant Q&A pairs have been found in total for the 32 short queries.", "labels": [], "entities": []}, {"text": "Baseline retrieval models: The proposed ap-proach to Q&A retrieval using compact translation models (denoted as CTLM henceforth) is compared to three baselines: QLM: Query-likelihood language model for retrieval (equivalent to Equation 3, without use of translation models).", "labels": [], "entities": []}, {"text": "This model represents wordbased retrieval models widely used in practice.", "labels": [], "entities": []}, {"text": "TLM(QQ): Translation-based language model for question retrieval).", "labels": [], "entities": [{"text": "question retrieval", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8042362332344055}]}, {"text": "This model uses IBM Model 1 learned from the (QQ) corpus of which stopwords are removed.", "labels": [], "entities": [{"text": "IBM Model 1 learned from the (QQ) corpus", "start_pos": 16, "end_pos": 56, "type": "DATASET", "confidence": 0.8206155359745025}]}, {"text": "TLM(QA): A variant of the translation-based approach.", "labels": [], "entities": []}, {"text": "This model uses IBM model 1 learned from the (QA) corpus.", "labels": [], "entities": [{"text": "IBM model 1 learned from the (QA) corpus", "start_pos": 16, "end_pos": 56, "type": "DATASET", "confidence": 0.7581573247909545}]}, {"text": "Evaluation metrics: We have reported the retrieval performance in terms of Mean Average Precision (MAP) and Mean R-Precision (R-Prec).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 75, "end_pos": 103, "type": "METRIC", "confidence": 0.9708602031071981}, {"text": "Mean R-Precision (R-Prec)", "start_pos": 108, "end_pos": 133, "type": "METRIC", "confidence": 0.9174761533737182}]}, {"text": "Average Precision can be computed based on the precision at each relevant document in the ranking.", "labels": [], "entities": [{"text": "Precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9167284369468689}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9989901185035706}]}, {"text": "Mean Average Precision is defined as the mean of the Average Precision values across the set of all queries: where Q is the set of test queries, m q is the number of relevant documents fora query q, R k is the set of ranked retrieval results from the top until rank position k, and P recision(R k ) is the fraction of relevant documents in R k (.", "labels": [], "entities": [{"text": "P recision(R k )", "start_pos": 282, "end_pos": 298, "type": "METRIC", "confidence": 0.9277033706506094}]}, {"text": "R-Precision is defined as the precision after R documents have been retrieved where R is the number of relevant documents for the current query).", "labels": [], "entities": [{"text": "R-Precision", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9461288452148438}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999239444732666}]}, {"text": "Mean RPrecision is the mean of the R-Precisions across the set of all queries.", "labels": [], "entities": []}, {"text": "We take MAP as our primary evaluation metric.", "labels": [], "entities": [{"text": "MAP", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.5164957046508789}]}, {"text": "Preliminary retrieval experiments have been conducted using the baseline QLM and different fields of Q&A data as retrieval unit.", "labels": [], "entities": []}, {"text": "shows the effectiveness of each field.", "labels": [], "entities": []}, {"text": "The results imply that the question title field is the most important field in our Yahoo!", "labels": [], "entities": [{"text": "question title", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.815763384103775}]}, {"text": "Answers collection; this also supports the observation presented by: Comparisons with three baseline retrieval models.", "labels": [], "entities": [{"text": "Answers collection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8256666362285614}]}, {"text": "* indicates that it is equivalent to's approach.", "labels": [], "entities": []}, {"text": "MAP improvements of CTLMs have been tested to be statistically significant using paired t-test..", "labels": [], "entities": []}, {"text": "Based on the preliminary observations, all retrieval models tested in this paper have ranked Q&A pairs according to the similarity scores between queries and question titles.", "labels": [], "entities": []}, {"text": "presents the comparison results of three baseline retrieval models and the proposed CTLMs.", "labels": [], "entities": []}, {"text": "For each method, the best performance after empirical \u03bb parameter tuning according to MAP is presented.", "labels": [], "entities": []}, {"text": "Notice that both the TLMs and CTLMs have outperformed the word-based QLM.", "labels": [], "entities": []}, {"text": "This implies that word-based models that do not address the issue of lexical gaps between queries and questions often fail to retrieve relevant Q&A data that have little word overlap with queries, as noted by.", "labels": [], "entities": []}, {"text": "Moreover, notice that the proposed CTLMs have achieved significantly better performances in all evaluation metrics than both QLM and TLMs, regardless of the parallel corpus in which the incorporated translation models are trained from.", "labels": [], "entities": []}, {"text": "This is a clear indication that the use of compact translation models built with appropriate word elimination strategies is effective in closing the query-question lexical gaps   for improving the performance of question retrieval in the context of language modeling framework.", "labels": [], "entities": [{"text": "word elimination", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.7277047336101532}, {"text": "question retrieval", "start_pos": 212, "end_pos": 230, "type": "TASK", "confidence": 0.7278900146484375}]}, {"text": "Note that the retrieval performance varies by the type of training corpus; CTLM(QA) has outperformed CTLM(QQ) significantly.", "labels": [], "entities": []}, {"text": "This proves the statement we made earlier that the (QQ) corpus would contain much noise since the translation models learned from the (QQ) corpus tend to have smaller vocabulary sizes but significantly more average translations per word than the ones learned from the (QA) corpus.", "labels": [], "entities": []}, {"text": "show the effect of various word elimination strategies on the retrieval performance of CTLMs in which the incorporated compact translation models are trained from the (QQ) corpus and the (QA) corpus, respectively.", "labels": [], "entities": [{"text": "word elimination", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7917080521583557}]}, {"text": "It is interesting to note that the importance of modifications in word elimination strategies also varies by the type of training corpus.", "labels": [], "entities": [{"text": "word elimination", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.8142321407794952}]}, {"text": "The retrieval results indicate that when the translation model is trained from the \"less noisy\" (QA) corpus, eliminating a relatively large proportions of words may hurt the retrieval performance of CTLM.", "labels": [], "entities": []}, {"text": "In the case when the translation model is trained from the \"noisy\" (QQ) corpus, a better retrieval performance maybe achieved if words are eliminated appropriately to a certain extent.", "labels": [], "entities": []}, {"text": "In terms of weighting scheme, the TextRank approach, which is more \"strict\" than tf-idf in eliminating unimportant words, has led comparatively higher retrieval performances on all levels of removal quantity when the translation model has been trained from the \"noisy\" (QQ) corpus.", "labels": [], "entities": []}, {"text": "On the contrary, the \"less strict\" tf-idf approach has led better performances when the translation model has been trained from the \"less noisy\" (QA) corpus.", "labels": [], "entities": []}, {"text": "In summary, the results imply that the performance of translation-based retrieval models can be significantly improved when strategies for building of compact translation models are chosen properly, regarding the expected noise level of the parallel corpus for training the translation models.", "labels": [], "entities": []}, {"text": "Ina case where a noisy parallel corpus is given for training of translation models, it is better to get rid of noise as much as possible by using \"strict\" term weighting algorithms; when a less noisy parallel corpus is given for building the translation models, a tolerant approach would yield better retrieval performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Impact of various word elimination strategies on translation model construction using (QA) corpus.", "labels": [], "entities": [{"text": "word elimination", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7615630626678467}, {"text": "translation model construction", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.94169948498408}]}, {"text": " Table 2: Impact of various word elimination strategies on translation model construction using (QQ) corpus.", "labels": [], "entities": [{"text": "word elimination", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7588609158992767}, {"text": "translation model construction", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.9414617220560709}]}, {"text": " Table 3: Preliminary retrieval results.", "labels": [], "entities": []}, {"text": " Table 4: Comparisons with three baseline retrieval mod- els. * indicates that it is equivalent to", "labels": [], "entities": []}, {"text": " Table 5: Contributions of various word elimination strate- gies on MAP performance of CTLM(QQ).", "labels": [], "entities": [{"text": "word elimination strate", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7990245223045349}]}, {"text": " Table 6: Contributions of various word elimination strate- gies on MAP performance of CTLM(QA).", "labels": [], "entities": [{"text": "word elimination strate", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.798051635424296}]}]}