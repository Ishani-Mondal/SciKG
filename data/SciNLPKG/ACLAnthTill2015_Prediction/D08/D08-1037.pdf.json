{"title": [], "abstractContent": [{"text": "This paper introduces anew method for identifying named-entity (NE) transliterations in bilingual corpora.", "labels": [], "entities": [{"text": "identifying named-entity (NE) transliterations in bilingual corpora", "start_pos": 38, "end_pos": 105, "type": "TASK", "confidence": 0.7442622979482015}]}, {"text": "Recent works have shown the advantage of discriminative approaches to transliteration: given two strings (w s , w t) in the source and target language, a classifier is trained to determine if wt is the translitera-tion of w s.", "labels": [], "entities": []}, {"text": "This paper shows that the translit-eration problem can be formulated as a constrained optimization problem and thus take into account contextual dependencies and constraints among character bi-grams in the two strings.", "labels": [], "entities": []}, {"text": "We further explore several methods for learning the objective function of the optimization problem and show the advantage of learning it discriminately.", "labels": [], "entities": []}, {"text": "Our experiments show that the new framework results in over 50% improvement in translating English NEs to Hebrew.", "labels": [], "entities": [{"text": "translating English NEs to Hebrew", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.8824965834617615}]}], "introductionContent": [{"text": "Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language based on phonetic similarity between the entities.", "labels": [], "entities": [{"text": "Named entity (NE) transliteration", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5940888325373331}]}, {"text": "Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (.", "labels": [], "entities": [{"text": "Identifying transliteration pairs", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9123045802116394}, {"text": "machine translation", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.8065700829029083}, {"text": "multilingual information retrieval", "start_pos": 175, "end_pos": 209, "type": "TASK", "confidence": 0.6270569364229838}]}, {"text": "It may appear at first glance that identifying the phonetic correlation between names based on an orthographic analysis is a simple, straight-forward: Named entities transliteration pairs in English and Hebrew and the character level mapping between the two names.", "labels": [], "entities": []}, {"text": "The Hebrew names can be romanized as eeta-l-ya and a-ya task; however in many cases a consistent deterministic mapping between characters does not exist; rather, the mapping depends on the context the characters appear in and on transliteration conventions which may change across domains.", "labels": [], "entities": []}, {"text": "exhibits two examples of NE transliterations in English and Hebrew, with the correct mapping across the two scripts.", "labels": [], "entities": []}, {"text": "Although the two Hebrew names share a common prefix 1 , this prefix can be mapped into a single English character or into two different characters depending on the context it appears in.", "labels": [], "entities": []}, {"text": "Similarly, depending on the context it appears in, the English character a can be mapped into different characters or to an \"empty\" character.", "labels": [], "entities": []}, {"text": "In recent years, as it became clear that solutions that are based on linguistics rules are not satisfactory, machine learning approaches have been developed to address this problem.", "labels": [], "entities": []}, {"text": "The common approach adopted is therefore to view this problem as a classification problem () and train a discriminative classifier.", "labels": [], "entities": []}, {"text": "That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that determines if one is a transliteration of the other.", "labels": [], "entities": []}, {"text": "Several papers have followed upon this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the discriminative classifier (.", "labels": [], "entities": []}, {"text": "While it has been clear that the relevancy of pairwise features is context sensitive and that there are contextual constraints among them, the hope was that a discriminative approach will be sufficient to account for those by weighing features appropriately.", "labels": [], "entities": []}, {"text": "This has been shown to be difficult for language pairs which are very different, such as English and Hebrew.", "labels": [], "entities": []}, {"text": "In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem.", "labels": [], "entities": []}, {"text": "We formalize it as an optimization problem over a set of local pairwise features -character n-gram matches across the two string -and subject to legitimacy constraints.", "labels": [], "entities": []}, {"text": "We use a discriminatively trained classifier as away to learn the objective function for the global constrained optimization problem.", "labels": [], "entities": []}, {"text": "Our technical approach follows a large body of work developed over the last few years, following) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods).", "labels": [], "entities": []}, {"text": "We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (w s , wt ), and a vector of initial weights.", "labels": [], "entities": []}, {"text": "Our first baseline makes use of all features extracted from a pair, along with a simple counting method to determine initial weights.", "labels": [], "entities": []}, {"text": "We then use a method similar to) in order to discriminatively train a better weight vector for the objective function.", "labels": [], "entities": []}, {"text": "Our key contribution is that we use a constrained optimization approach also to determine a better feature representation fora given pair.", "labels": [], "entities": []}, {"text": "() attempted a related approach to restricting the set of features representing a transliteration candidate.", "labels": [], "entities": []}, {"text": "However, rather than directly aligning the two strings as done there, we exploit the expressiveness of the ILP formulation and constraints to generate a better representation of a pair.", "labels": [], "entities": []}, {"text": "This is the representation we then use to discriminatively learn a better weight vector for the objective function used in our final model.", "labels": [], "entities": []}, {"text": "Our experiments focus on Hebrew-English transliteration, which were shown to be very difficult in a previous work.", "labels": [], "entities": [{"text": "Hebrew-English transliteration", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6268593370914459}]}, {"text": "We show very significant improvements over existing work with the same data set, proving the advantage of viewing the transliteration decision as a global inference problem.", "labels": [], "entities": []}, {"text": "Furthermore, we show the importance of using a discriminatively trained objective function.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The main algorithmic contribution of this paper is described in Sec.", "labels": [], "entities": []}, {"text": "2. Our experimental study is describes in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approach over a corpus of 300 English-Hebrew transliteration pairs, and used another 250 different samples for training the models.", "labels": [], "entities": []}, {"text": "We constructed the test set by pairing each English name with all Hebrew names in the corpus.", "labels": [], "entities": []}, {"text": "The system was evaluated on its ability to correctly identify the 300 transliteration pairs out of all the possible transliteration candidates.", "labels": [], "entities": []}, {"text": "We measured performance using the Mean Reciprocal Rank (MRR) measure.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR) measure", "start_pos": 34, "end_pos": 68, "type": "METRIC", "confidence": 0.9626024620873588}]}, {"text": "This measure, originally introduced in the field of information retrieval, is used to evaluate systems that rank several options according to their probability of correctness.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7490895986557007}]}, {"text": "MRR is a natural measure in our settings and has been used previously for evaluating transliteration systems, for example by).", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6057572364807129}]}, {"text": "Given a set Q of queries and their respective responses ranked according to the system's confidence, we denote the rank of the correct response to a query q i \u2208 Q as rank(q i ).", "labels": [], "entities": []}, {"text": "MRR is then defined as the average of the multiplicative inverse of the rank of the correct answer, that is: In our experiments we solved an ILP problem for every transliteration candidate pairs, and computed MRR with respect to the confidence of our decision model across the candidates.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9723146557807922}, {"text": "MRR", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.9437115788459778}]}, {"text": "Although this required solving thousands of ILP instances, it posed no computational burden as these instances typically contained a small number of variables and constraints.", "labels": [], "entities": []}, {"text": "The entire test set is solved in less than 20 minutes using the publicly available GLPK package (http: //www.gnu.org/software/glpk/ ).", "labels": [], "entities": []}, {"text": "The performance of the different models is summarized in table 1, these results are based on a training set of 250 samples used to train the discriminative transliteration models and also to construct the initial weight vector W . shows performance over different number of training examples.", "labels": [], "entities": []}, {"text": "Our evaluation is concerns with the core transliteration and decision models presented here and does not consider any data set optimizations that were introduced in previous works, which we view as orthogonal additions, hence the difference with the results published in.", "labels": [], "entities": []}, {"text": "The results clearly show that our final model, model 5, outperform other models.", "labels": [], "entities": []}, {"text": "Interestingly, model 1, a simplistic model, significantly outperforms the discriminative model presented in).", "labels": [], "entities": []}, {"text": "We believe that this is due to two reasons.", "labels": [], "entities": []}, {"text": "It shows that discriminative training over the representation obtained using AF is not efficient; moreover, this phenomenon is accentuated given that we train over a very small data set, which favors generative estimation of weights.", "labels": [], "entities": [{"text": "generative estimation", "start_pos": 200, "end_pos": 221, "type": "TASK", "confidence": 0.9381675720214844}]}, {"text": "This is also clear when comparing the performance of model 1 to model 4, which shows that learning over the representation obtained using constrained optimization (IF) results in a very significant performance improvement.", "labels": [], "entities": []}, {"text": "The improvement of using IF Wis not automatic.", "labels": [], "entities": [{"text": "IF Wis", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.821856677532196}]}, {"text": "Model 3, which uses IF W , and model 1, which uses AF, converge to nearly the same result.", "labels": [], "entities": [{"text": "AF", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9234393239021301}]}, {"text": "Both these models use generative weights to make the transliteration decision, and this highlights the importance of discriminative training.", "labels": [], "entities": []}, {"text": "Both model 4 and model 5 use discriminatively trained weights and significantly outperform model 3.", "labels": [], "entities": []}, {"text": "These results indicate that using constraint optimization to generate the examples' representation in itself may not help; the objective function used in this inference has a significant role in improved performance.", "labels": [], "entities": []}, {"text": "The benefit of discriminatively training the objective function becomes even clearer when comparing the performance of model 5 to that of model 4, which uses the original weight vector when inferring the sample representation.", "labels": [], "entities": []}, {"text": "It can be assumed that this algorithm can benefit from further iterations -generating anew feature  representations, training a model on it, and using the resulting model as anew objective function.", "labels": [], "entities": []}, {"text": "However, it turns out that after a single round, improved weights due to additional training do not change the feature representation; the inference process does not yield a different outcome.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the different transliteration models,  trained using 250 samples. To facilitate readability (Kle- mentiev and Roth, 2006b; Goldwasser and Roth, 2008)  are referenced as KR'06 and GR'08 respectively.", "labels": [], "entities": [{"text": "GR'08", "start_pos": 200, "end_pos": 205, "type": "METRIC", "confidence": 0.6160897016525269}]}, {"text": " Table 2: Results of using model 5 with and without a  normalized objective function. Both models were trained  using 250 samples. The LN suffix in the model's name  indicate that the objective function used length normal- ization.", "labels": [], "entities": []}]}