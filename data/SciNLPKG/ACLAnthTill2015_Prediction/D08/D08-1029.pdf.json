{"title": [{"text": "Who is Who and What is What: Experiments in Cross-Document Co-Reference", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a language-independent, scalable system for both challenges of cross-document co-reference: name variation and entity disambiguation.", "labels": [], "entities": [{"text": "name variation", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.730679988861084}, {"text": "entity disambiguation", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.7078120708465576}]}, {"text": "We provide system results from the ACE 2008 evaluation in both English and Arabic.", "labels": [], "entities": [{"text": "ACE 2008 evaluation", "start_pos": 35, "end_pos": 54, "type": "DATASET", "confidence": 0.9156362613042196}]}, {"text": "Our English system's accuracy is 8.4% relative better than an exact match baseline (and 14.2% relative better over entities mentioned in more than one document).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995877146720886}]}, {"text": "Unlike previous evaluations, ACE 2008 evaluated both name variation and entity disambiguation over naturally occurring named mentions.", "labels": [], "entities": [{"text": "ACE 2008", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.6638685166835785}, {"text": "entity disambiguation", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.7006164789199829}]}, {"text": "An information extraction engine finds document entities in text.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.726267546415329}]}, {"text": "We describe how our architecture designed for the 10K document ACE task is scalable to an even larger corpus.", "labels": [], "entities": [{"text": "10K document ACE task", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.5164678543806076}]}, {"text": "Our cross-document approach uses the names of entities to find an initial set of document entities that could refer to the same real world entity and then uses an agglomerative clustering algorithm to disam-biguate the potentially co-referent document entities.", "labels": [], "entities": []}, {"text": "We analyze how different aspects of our system affect performance using ablation studies over the English evaluation set.", "labels": [], "entities": [{"text": "English evaluation set", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.6375453074773153}]}, {"text": "In addition to evaluating cross-document co-reference performance, we used the results of the cross-document system to improve the accuracy of within-document extraction, and measured the impact in the ACE 2008 within-document evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9988123178482056}, {"text": "within-document extraction", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.707927405834198}, {"text": "ACE 2008 within-document evaluation", "start_pos": 202, "end_pos": 237, "type": "DATASET", "confidence": 0.8959013670682907}]}], "introductionContent": [{"text": "Cross-document entity co-reference is the problem of identifying whether mentions from different documents refer to the same or distinct entities.", "labels": [], "entities": []}, {"text": "There are two principal challenges: the same entity can be referred to by more than one name string (e.g. Mahmoud Abbas and Abu Mazen) and the same name string can be shared by more than one entity (e.g. John Smith).", "labels": [], "entities": []}, {"text": "Algorithms for solving the cross-document co-reference problem are necessary for systems that build knowledge bases from text, question answering systems, and watch list applications.", "labels": [], "entities": [{"text": "question answering", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8211647868156433}]}, {"text": "There are several challenges in evaluating and developing systems for the cross-document coreference task.", "labels": [], "entities": [{"text": "cross-document coreference task", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7863669792811075}]}, {"text": "(1) The annotation process required for evaluation and for training is expensive; an annotator must cluster a large number of entities across a large number of documents.", "labels": [], "entities": []}, {"text": "The annotator must read the context around each instance of an entity to make reliable judgments.", "labels": [], "entities": []}, {"text": "(2) On randomly selected text, a baseline of exact string match will do quite well, making it difficult to evaluate progress.", "labels": [], "entities": [{"text": "exact string match", "start_pos": 45, "end_pos": 63, "type": "METRIC", "confidence": 0.898272713025411}]}, {"text": "(3) For a machine, there can easily be a scalability challenge since the system must cluster a large number of entities.", "labels": [], "entities": []}, {"text": "Because of the annotation challenges, many previous studies in cross-document co-reference have focused on only the entity disambiguation problem (where one can use string retrieval to collect many documents that contain same name); or have used artificially ambiguated data.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.7224919199943542}]}, {"text": "Section 2 describes related work; section 3 introduces ACE, where the work was evaluated; section 4 describes the underlying information extraction engine; sections 5 and 6 address the challenges of coping with name variation and disambiguating entities; sections 7, 8, and 9 present empirical results, improvement of entity extraction within documents using cross-document coreference, and a difference in performance on person versus organization entities.", "labels": [], "entities": [{"text": "ACE", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.6672869920730591}, {"text": "information extraction", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.7453967034816742}, {"text": "entity extraction within documents", "start_pos": 318, "end_pos": 352, "type": "TASK", "confidence": 0.829368069767952}]}, {"text": "Section 10 discusses the scalability challenge.", "labels": [], "entities": []}], "datasetContent": [{"text": "NIST's ACE evaluation measures system performance on a predetermined set of entities, relations, and events.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9325389266014099}]}, {"text": "For the 2008 global entity detection and recognition task (GEDR) , system performance was measured on named instances of person and organization entities.", "labels": [], "entities": [{"text": "global entity detection and recognition task (GEDR)", "start_pos": 13, "end_pos": 64, "type": "TASK", "confidence": 0.8135516345500946}]}, {"text": "The GEDR task was run over both English and Arabic documents.", "labels": [], "entities": [{"text": "GEDR task", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.6847493052482605}]}, {"text": "Participants processed over 10K documents for each language.", "labels": [], "entities": []}, {"text": "References were produced for about 400 documents per language.", "labels": [], "entities": []}, {"text": "The evaluation set included documents from several genres over a 10 year time period.", "labels": [], "entities": []}, {"text": "Document counts are provided in.", "labels": [], "entities": []}, {"text": "This evaluation differed from previous community cross-document coreference evaluations in that it (a) covered both organizations and people; (b) required processing a relatively large data set; (c) evaluated entity disambiguation and name variation simultaneously; and (d) measured cross-document co-reference over systemdetected document-level entities and mentions.", "labels": [], "entities": []}, {"text": "The evaluation set was selected to include interesting cases for cross-document co-reference (e.g cases with spelling variation and entities with shared names).", "labels": [], "entities": []}, {"text": "This is necessary because annotation is difficult to produce and naturally sampled data has a high percentage of entities resolvable with string match.", "labels": [], "entities": []}, {"text": "The selection techniques were unknown to ACE participants.", "labels": [], "entities": []}, {"text": "As noted previously, the ACE collection was selected to include challenging entities.", "labels": [], "entities": [{"text": "ACE collection", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.8811545372009277}]}, {"text": "The selection criteria of the corpus (which are not known by ACE participants) can affect the importance of features.", "labels": [], "entities": []}, {"text": "For example, a corpus that included very few transliterated names would make less use of features based on edit distance. and show performance (with value weighted F) on the eight conditions over system predicted within-document extraction and reference within-document extraction respectively.", "labels": [], "entities": [{"text": "reference within-document extraction", "start_pos": 244, "end_pos": 280, "type": "TASK", "confidence": 0.6434838970502218}]}, {"text": "also includes configuration 5 run overall 10K documents.", "labels": [], "entities": []}, {"text": "We provide two sets of results.", "labels": [], "entities": []}, {"text": "The first evaluates system performance overall entities.", "labels": [], "entities": []}, {"text": "The relatively high score of the 'No Link' baseline indicates that a high percentage of the document-level entities in the corpus are only mentioned in one document.", "labels": [], "entities": []}, {"text": "The second set of numbers measures system performance on those entities appearing in more than one reference document.", "labels": [], "entities": []}, {"text": "While this metric does not give a complete picture of the cross-document co-reference task (sometimes a singleton entity must be disambiguated from a large entity that shares the same name); it does provide useful insights given the frequency of singleton entities.", "labels": [], "entities": []}, {"text": "Overall system performance improved as features were added.", "labels": [], "entities": []}, {"text": "Configuration 1, which disambiguated entities with a small set of features, performed worse than a more aggressive exact string match strategy.", "labels": [], "entities": []}, {"text": "The nature of our agglomerative clustering algorithm leads to entity merges only when there is sufficient evidence for the merge.", "labels": [], "entities": []}, {"text": "The relatively high performance of the exact match strategy suggests that in the ACE corpus, most entities that shared a name string referred to the same entity, and therefore aggressive merging leads to better performance.", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9088069498538971}]}, {"text": "As additional features are added, our system becomes more confident and merges more document-level entities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. This evaluation differed from  previous community cross-document coreference  evaluations in that it (a) covered both organizations  and people; (b) required processing a relatively  large data set; (c) evaluated entity disambiguation  and name variation simultaneously; and (d) meas- ured cross-document co-reference over system- detected document-level entities and mentions.", "labels": [], "entities": []}, {"text": " Table 1: Documents per genre in ACE2008 test set", "labels": [], "entities": [{"text": "ACE2008 test set", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9702717264493307}]}, {"text": " Table 3: ACE 2008 Within-Document Results (LEDR)", "labels": [], "entities": [{"text": "ACE 2008 Within-Document Results (LEDR)", "start_pos": 10, "end_pos": 49, "type": "DATASET", "confidence": 0.8806547522544861}]}, {"text": " Table 7. These differences suggest that  part of the reason for the low performance on or- ganizations in GEDR is within-document accuracy.", "labels": [], "entities": [{"text": "GEDR", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.7288756370544434}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9696058630943298}]}, {"text": " Table 7: Performance on ORG and PER Entities", "labels": [], "entities": [{"text": "ORG", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8426924347877502}]}, {"text": " Table 8: Local Performance on Name Only Task", "labels": [], "entities": []}]}