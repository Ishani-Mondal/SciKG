{"title": [], "abstractContent": [{"text": "In this paper we describe research on summarizing conversations in the meetings and emails domains.", "labels": [], "entities": [{"text": "summarizing conversations", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.9234771132469177}]}, {"text": "We introduce a conversation summarization system that works in multiple domains utilizing general conversational features, and compare our results with domain-dependent systems for meeting and email data.", "labels": [], "entities": [{"text": "conversation summarization", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.7050130665302277}]}, {"text": "We find that by treating meetings and emails as conversations with general conversational features in common, we can achieve competitive results with state-of-the-art systems that rely on more domain-specific features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our lives are increasingly comprised of multimodal conversations with others.", "labels": [], "entities": []}, {"text": "We email for business and personal purposes, attend meetings in person and remotely, chat online, and participate in blog or forum discussions.", "labels": [], "entities": []}, {"text": "It is clear that automatic summarization can be of benefit in dealing with this overwhelming amount of interactional information.", "labels": [], "entities": [{"text": "summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.8785573840141296}]}, {"text": "Automatic meeting abstracts would allow us to prepare for an upcoming meeting or review the decisions of a previous group.", "labels": [], "entities": []}, {"text": "Email summaries would aid corporate memory and provide efficient indices into large mail folders.", "labels": [], "entities": []}, {"text": "When summarizing in each of these domains, there will be potentially useful domain-specific features -e.g. prosodic features for meeting speech, subject headers for emails -but there are also underlying similarites between these domains.", "labels": [], "entities": []}, {"text": "They are all multiparty conversations, and we hypothesize that effective summarization techniques can be designed that would lead to robust summarization performance on a wide array of such conversation types.", "labels": [], "entities": [{"text": "summarization", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9815526604652405}, {"text": "summarization", "start_pos": 140, "end_pos": 153, "type": "TASK", "confidence": 0.9705134630203247}]}, {"text": "Such a general conversation summarization system would make it possible to summarize a wide variety of conversational data without needing to develop unique summarizers in each domain and across modalities.", "labels": [], "entities": [{"text": "conversation summarization", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6601994335651398}]}, {"text": "While progress has been made in summarizing conversations in individual domains, as described below, little or no work has been done on summarizing unrestricted, multimodal conversations.", "labels": [], "entities": [{"text": "summarizing conversations", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.9167277812957764}, {"text": "summarizing unrestricted, multimodal conversations", "start_pos": 136, "end_pos": 186, "type": "TASK", "confidence": 0.8178293347358704}]}, {"text": "In this research we take an extractive approach to summarization, presenting a novel set of conversational features for locating the most salient sentences in meeting speech and emails.", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9927515983581543}]}, {"text": "We demonstrate that using these conversational features in a machine-learning sentence classification framework yields performance that is competitive or superior to more restricted domain-specific systems, while having the advantage of being portable across conversational modalities.", "labels": [], "entities": [{"text": "machine-learning sentence classification", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.6730309724807739}]}, {"text": "The robust performance of the conversation-based system is attested via several summarization evaluation techniques, and we give an in-depth analysis of the effectiveness of the individual features and feature subclasses used.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the classifier employed for our machine learning experiments, the corpora used, the relevant summarization annotations for each corpus, and the evaluation methods employed.", "labels": [], "entities": []}, {"text": "For evaluating our extractive summaries, we implement existing evaluation schemes from previous research, with somewhat similar methods for meetings versus emails.", "labels": [], "entities": []}, {"text": "These are described and compared below.", "labels": [], "entities": []}, {"text": "We also evaluate our extractive classifiers more generally by plotting the receiver operator characteristic (ROC) curve and calculating the area under the curve (AUROC).", "labels": [], "entities": [{"text": "receiver operator characteristic (ROC) curve", "start_pos": 75, "end_pos": 119, "type": "METRIC", "confidence": 0.8914426565170288}, {"text": "AUROC)", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9714453220367432}]}, {"text": "This allows us to gauge the true-positive/false-positive ratio as the posterior threshold is varied.", "labels": [], "entities": []}, {"text": "We use the differing evaluation metrics for emails versus meetings for two primary reasons.", "labels": [], "entities": []}, {"text": "First, the differing summarization annotations in the AMI and Enron corpora naturally lend themselves to slightly divergent metrics, one based on extractabstract links and the other based on the essential/option/uninformative distinction.", "labels": [], "entities": [{"text": "AMI", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.9233107566833496}, {"text": "Enron corpora", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.878821462392807}]}, {"text": "Second, and more importantly, using these two metrics allow us to compare our results with state-of-the-art results in the two fields of speech summarization and email summarization.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.6993702203035355}, {"text": "email summarization", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.7139942795038223}]}, {"text": "In future work we plan to use a single evaluation metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Weighted F-Scores and AUROCs for Meeting  Summaries", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.7790077924728394}, {"text": "AUROCs", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9337297081947327}, {"text": "Meeting  Summaries", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.5953356474637985}]}, {"text": " Table 3: Pyramid Precision and AUROCs for Email Sum- maries", "labels": [], "entities": [{"text": "AUROCs", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9073356986045837}, {"text": "Email Sum- maries", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8118808716535568}]}]}