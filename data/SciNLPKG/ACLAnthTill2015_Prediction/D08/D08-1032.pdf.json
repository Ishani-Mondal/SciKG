{"title": [{"text": "Selecting Sentences for Answering Complex Questions", "labels": [], "entities": [{"text": "Selecting Sentences", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8953055739402771}, {"text": "Answering Complex Questions", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8582177758216858}]}], "abstractContent": [{"text": "Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summa-rization.", "labels": [], "entities": []}, {"text": "In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: k-means and Expectation Maximization (EM), for computing relative importance of the sentences.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 129, "end_pos": 158, "type": "METRIC", "confidence": 0.6620847463607789}]}, {"text": "However, the performance of these approaches depends entirely on the feature set used and the weighting of these features.", "labels": [], "entities": []}, {"text": "We extracted different kinds of features (i.e. lexical , lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query.", "labels": [], "entities": []}, {"text": "We used a local search technique to learn the weights of the features.", "labels": [], "entities": []}, {"text": "For all our methods of generating summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features.", "labels": [], "entities": []}], "introductionContent": [{"text": "After having made substantial headway in factoid and list questions, researchers have turned their attention to more complex information needs that cannot be answered by simply extracting named entities (persons, organizations, locations, dates, etc.) from documents.", "labels": [], "entities": []}, {"text": "For example, the question: \"Describe the after-effects of cyclone Sidr-Nov 2007 in Bangladesh\" requires inferencing and synthesizing information from multiple documents.", "labels": [], "entities": [{"text": "Describe the after-effects of cyclone Sidr-Nov 2007 in Bangladesh", "start_pos": 28, "end_pos": 93, "type": "TASK", "confidence": 0.781539089149899}]}, {"text": "This information synthesis in NLP can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information.", "labels": [], "entities": []}, {"text": "In this paper, we experimented with one empirical and two well-known unsupervised statistical machine learning techniques: k-means and EM and evaluated their performance in generating topicoriented summaries.", "labels": [], "entities": []}, {"text": "However, the performance of these approaches depends entirely on the feature set used and the weighting of these features.", "labels": [], "entities": []}, {"text": "We extracted different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query.", "labels": [], "entities": []}, {"text": "We have used a gradient descent local search technique to learn the weights of the features.", "labels": [], "entities": []}, {"text": "Traditionally, information extraction techniques are based on the BOW approach augmented by language modeling.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.9117395281791687}]}, {"text": "But when the task requires the use of more complex semantics, the approaches based on only BOW are often inadequate to perform fine-level textual analysis.", "labels": [], "entities": [{"text": "BOW", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.5738641023635864}]}, {"text": "Some improvements on BOW are given by the use of dependency trees and syntactic parse trees (),),, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs.", "labels": [], "entities": []}, {"text": "Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models.", "labels": [], "entities": []}, {"text": "Attempting an application of syntactic and semantic information to complex QA hence seems natural, as pinpointing the answer to a question relies on a deep understanding of the semantics of both.", "labels": [], "entities": []}, {"text": "In more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions), to our knowledge no study uses tree kernel functions to encode syntactic/semantic information.", "labels": [], "entities": []}, {"text": "For all our methods of generating summaries (i.e. empirical, k-means and EM), we have shown the effects of syntactic and shallow-semantic features over the BOW features.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 focuses on the related work, Section 3 describes how the features are extracted, Section 4 discusses the scoring approaches, Section 5 discusses how we remove the redundant sentences before adding them to the summary, Section 6 describes our experimental study.", "labels": [], "entities": []}, {"text": "We conclude and give future directions in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the main task of Document Understanding Conference (DUC) 2007 for evaluation.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC) 2007", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.7898326090403965}]}, {"text": "The task was: \"Given a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, well-organized 250-word summary of the documents that answers the question(s) in the topic.\"", "labels": [], "entities": []}, {"text": "NIST assessors developed topics of interest to them and choose a set of 25 documents relevant (document cluster) to each topic.", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8935618698596954}]}, {"text": "Each topic and its document cluster were given to 4 different NIST assessors.", "labels": [], "entities": [{"text": "NIST assessors", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.8834460079669952}]}, {"text": "The assessor created a 250-word summary of the document cluster that satisfies the information need expressed in the topic statement.", "labels": [], "entities": []}, {"text": "These multiple \"reference summaries\" are used in the evaluation of summary content.", "labels": [], "entities": []}, {"text": "We carried out automatic evaluation of our summaries using ROUGE) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.", "labels": [], "entities": [{"text": "summaries", "start_pos": 43, "end_pos": 52, "type": "TASK", "confidence": 0.958488941192627}, {"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9108512997627258}, {"text": "DUC", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9140570759773254}, {"text": "summarization evaluation", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.942062646150589}]}, {"text": "It measures summary quality by counting overlapping units such as the n-grams (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary.", "labels": [], "entities": []}, {"text": "ROUGE parameters were set as the same as DUC 2007 evaluation setup.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9134466648101807}, {"text": "DUC 2007 evaluation", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9267071882883707}]}, {"text": "One purpose of our experiments is to study the impact of different features for complex question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7841779291629791}]}, {"text": "To accomplish this, we generated summaries for the topics of DUC 2007 by each of our seven systems defined as below: The LEX system generates summaries based on only lexical features: n-gram (n=1,2,3,4), LCS, WLCS, skip bi-gram, head, head synonym.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.9397948980331421}, {"text": "WLCS", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.5710281729698181}]}, {"text": "The LSEM system considers only lexical semantic features: synonym, hypernym/hyponym, gloss, dependency-based and proximity-based similarity.", "labels": [], "entities": []}, {"text": "The COS system generates summary based on the graph-based method.", "labels": [], "entities": []}, {"text": "The SYS1 system considers all the features except the BE, syntactic and semantic features.", "labels": [], "entities": [{"text": "BE", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9955759048461914}]}, {"text": "The SYS2 system considers all the features except the syntactic and semantic features.", "labels": [], "entities": []}, {"text": "The SYS3 considers all the features except the semantic and the ALL 6 system generates summaries taking all the features into account.", "labels": [], "entities": []}, {"text": "to to show the evaluation measures for k-means, EM and empirical approaches respectively.", "labels": [], "entities": []}, {"text": "As Table 1 shows, in k-means, SYS2 gets 0-21%, SYS3 gets 4-32% and ALL gets 3-36% improvement in ROUGE-2 scores over the SYS1 system.", "labels": [], "entities": [{"text": "ALL", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9226099848747253}, {"text": "ROUGE-2", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9962300658226013}]}, {"text": "We get best ROUGE-W scores for SYS2 (i.e. including BE) but SYS3 and ALL do not perform well in this case.", "labels": [], "entities": [{"text": "ROUGE-W", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9973511695861816}, {"text": "SYS2", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.7495181560516357}, {"text": "BE", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.998694121837616}, {"text": "ALL", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.92467200756073}]}, {"text": "SYS2 improves the ROUGE-W F-score by 1% over SYS1.", "labels": [], "entities": [{"text": "SYS2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8855438232421875}, {"text": "ROUGE-W", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9984748959541321}, {"text": "F-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.7625682353973389}]}, {"text": "We do not get any improvement in ROUGE-SU) scores when we include any kind of syntactic/semantic structures.", "labels": [], "entities": [{"text": "ROUGE-SU) scores", "start_pos": 33, "end_pos": 49, "type": "METRIC", "confidence": 0.9592894117037455}]}, {"text": "The case is different for EM and empirical approaches.", "labels": [], "entities": []}, {"text": "Here, in every case we get a significant amount of improvement when we include the syntactic and/or semantic features.", "labels": [], "entities": []}, {"text": "For EM), the ratio of improvement in F-scores over SYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 2-24% for ALL.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9942674040794373}]}, {"text": "In our empirical approach), SYS2, SYS3 and ALL improve the Fscores by 3-11%, 7-15% and 8-19% over SYS1 respectively.", "labels": [], "entities": [{"text": "ALL", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9581382870674133}, {"text": "Fscores", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.998740017414093}]}, {"text": "These results clearly indicate the positive impact of the syntactic/semantic features for complex question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 98, "end_pos": 121, "type": "TASK", "confidence": 0.7806074420611063}]}, {"text": "shows the F-scores of the ROUGE measures for one baseline system, the best system in DUC 2007 and our three scoring techniques considering all features.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976789355278015}, {"text": "ROUGE", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.7672346830368042}, {"text": "DUC 2007", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.9678694903850555}]}, {"text": "The baseline system gener-: ROUGE-SU in empirical approach ates summaries by returning all the leading sentences (up to 250 words) in the T EXT field of the most recent document(s).", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9941923022270203}]}, {"text": "It shows that the empirical approach outperforms the other two learning techniques and EM performs better than k-means algorithm.", "labels": [], "entities": [{"text": "EM", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.6654911637306213}]}, {"text": "EM improves the F-scores over k-means by 0.7-22.5%.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7056230902671814}, {"text": "F-scores", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.998381495475769}]}, {"text": "Empirical approach improves the Fscores over k-means and EM by 5.9-20.2% and 3.5-6.5% respectively.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9989107847213745}]}, {"text": "Comparing with the DUC 2007 participants our systems achieve top scores and for some ROUGE measures there is no statistically significant difference between our system and the best DUC 2007 system.", "labels": [], "entities": [{"text": "DUC 2007 participants", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.9569042126337687}, {"text": "ROUGE", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9185998439788818}, {"text": "DUC 2007 system", "start_pos": 181, "end_pos": 196, "type": "DATASET", "confidence": 0.9268116354942322}]}], "tableCaptions": [{"text": " Table 1: ROUGE-2 measures in k-means learning", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9835943579673767}]}, {"text": " Table 4: ROUGE-2 measures in EM learning", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9884336590766907}, {"text": "EM learning", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.8506469428539276}]}, {"text": " Table 6: ROUGE-SU measures in EM learning", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9914087653160095}]}, {"text": " Table 9: ROUGE-SU in empirical approach", "labels": [], "entities": [{"text": "ROUGE-SU", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9570658206939697}]}, {"text": " Table 10: F-measures for different systems", "labels": [], "entities": [{"text": "F-measures", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9531742334365845}]}]}