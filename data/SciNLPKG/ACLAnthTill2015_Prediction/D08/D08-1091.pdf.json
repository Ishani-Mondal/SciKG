{"title": [{"text": "Sparse Multi-Scale Grammars for Discriminative Latent Variable Parsing", "labels": [], "entities": [{"text": "Discriminative Latent Variable Parsing", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.5789056941866875}]}], "abstractContent": [{"text": "We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8020694851875305}]}, {"text": "The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories).", "labels": [], "entities": []}, {"text": "Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al.", "labels": [], "entities": []}, {"text": "In addition, our discriminative approach integrally admits features beyond local tree configurations.", "labels": [], "entities": []}, {"text": "We present a multi-scale training method along with an efficient CKY-style dynamic program.", "labels": [], "entities": []}, {"text": "On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.", "labels": [], "entities": []}], "introductionContent": [{"text": "In latent variable approaches to parsing), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9752023816108704}]}, {"text": "The parse trees represent the desired output of the system, while the derivation trees represent the typically much more complex underlying syntactic processes.", "labels": [], "entities": []}, {"text": "In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work).", "labels": [], "entities": []}, {"text": "In particular, in we exhibited a very accurate category-splitting approach, in which a coarse initial grammar is refined by iteratively splitting each grammar category into two subcategories using the EM algorithm.", "labels": [], "entities": []}, {"text": "Of course, each time the number of grammar categories is doubled, the number of binary productions is increased by a factor of eight.", "labels": [], "entities": []}, {"text": "As a result, while our final grammars used few categories, the number of total active (non-zero) productions was still substantial (see Section 7).", "labels": [], "entities": []}, {"text": "In addition, it is reasonable to assume that some generatively learned splits have little discriminative utility.", "labels": [], "entities": [{"text": "generatively learned splits", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.8881449103355408}]}, {"text": "In this paper, we present a discriminative approach which addresses both of these limitations.", "labels": [], "entities": []}, {"text": "We introduce multi-scale grammars, in which some productions reference fine categories, while others reference coarse categories (see).", "labels": [], "entities": []}, {"text": "We use the general framework of hidden variable CRFs (, where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations.", "labels": [], "entities": []}, {"text": "With multi-scale grammars, it is natural to refine productions rather than categories.", "labels": [], "entities": []}, {"text": "As a result, a category such as NP can be complex in some regions of the grammar while remaining simpler in other regions.", "labels": [], "entities": []}, {"text": "Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features), giving the benefit of some input features integrally in our dynamic program.", "labels": [], "entities": []}, {"text": "Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of.", "labels": [], "entities": []}, {"text": "In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages.", "labels": [], "entities": []}, {"text": "Discriminative parsing has been investigated before, such as in Johnson (2001),,,,,, and, most similarly, in.", "labels": [], "entities": [{"text": "Discriminative parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7904617786407471}]}, {"text": "However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used.", "labels": [], "entities": []}, {"text": "Only in combination with a generative model was a discriminative component able to produce high parsing accuracies.", "labels": [], "entities": []}, {"text": "Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 178, "end_pos": 185, "type": "TASK", "confidence": 0.9592177867889404}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9170901775360107}]}], "datasetContent": [{"text": "We ran experiments on a variety of languages and corpora using the standard training and test splits, as described in.", "labels": [], "entities": []}, {"text": "In each case, we start with a completely unannotated X-bar grammar, obtained from the raw treebank by a simple rightbranching binarization scheme.", "labels": [], "entities": []}, {"text": "We then train multiscale grammars of increasing latent complexity as described in Section 5, directly incorporating the additional features from Section 6 into the training procedure.", "labels": [], "entities": []}, {"text": "Hierarchical training starting from a raw treebank grammar and proceeding to our most refined grammars took three days in a parallel implementation using 8 CPUs.", "labels": [], "entities": []}, {"text": "At testing time we marginalize out the hidden structure and extract the tree with the highest number of expected correct productions, as in . We compare to a baseline of discriminatively trained latent variable grammars).", "labels": [], "entities": []}, {"text": "We also compare our discriminative multiscale grammars to their generative split-and-merge cousins, which have been shown to produce the state-of-the-art figures in terms of accuracy and efficiency on many corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9983071088790894}]}, {"text": "For those comparisons we use the grammars from .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora and standard experimental setups.", "labels": [], "entities": []}, {"text": " Table 2: Our final test set parsing accuracies compared to  the best previous work on English, French and German.", "labels": [], "entities": []}, {"text": " Table 3: Complexity of highly split phrasal categories in  generative and discriminative grammars. Note that sub- categories are compared to production parameters, indi- cating that the number of parameters grows cubicly in the  number of subcategories for generative grammars, while  growing linearly for multi-scale grammars.", "labels": [], "entities": [{"text": "generative and discriminative grammars", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.7812863439321518}]}]}