{"title": [{"text": "Scalable Language Processing Algorithms for the Masses: A Case Study in Computing Word Co-occurrence Matrices with MapReduce", "labels": [], "entities": [{"text": "Scalable Language Processing Algorithms", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6881241723895073}]}], "abstractContent": [{"text": "This paper explores the challenge of scaling up language processing algorithms to increasingly large datasets.", "labels": [], "entities": []}, {"text": "While cluster computing has been available in commercial environments for several years, academic researchers have fallen behind in their ability to work on large datasets.", "labels": [], "entities": [{"text": "cluster computing", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.8956610262393951}]}, {"text": "I discuss two barriers contributing to this problem: lack of a suitable programming model for managing concurrency and difficulty in obtaining access to hardware.", "labels": [], "entities": []}, {"text": "Hadoop, an open-source implementation of Google's MapReduce framework, provides a compelling solution to both issues.", "labels": [], "entities": []}, {"text": "Its simple programming model hides system-level details from the developer, and its ability to run on commodity hardware puts cluster computing within the reach of many academic research groups.", "labels": [], "entities": []}, {"text": "This paper illustrates these points with a case study in building word co-occurrence matrices from large corpora.", "labels": [], "entities": []}, {"text": "I conclude with an analysis of an alternative computing model based on renting instead of buying computer clusters.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past couple of decades, the field of computational linguistics (and more broadly, human language technologies) has seen the emergence and later dominance of empirical techniques and datadriven research.", "labels": [], "entities": []}, {"text": "Concomitant with this trend is a coherent research thread that focuses on exploiting increasingly-large datasets.  were among the first to demonstrate the importance of dataset size as a significant factor governing prediction accuracy in a supervised machine learning task.", "labels": [], "entities": []}, {"text": "In fact, they argued that size of training set was perhaps more important than the choice of machine learning algorithm itself.", "labels": [], "entities": []}, {"text": "Similarly, experiments in question answering have shown the effectiveness of simple pattern-matching techniques when applied to large quantities of data ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8625319600105286}]}, {"text": "More recently, this line of argumentation has been echoed in experiments with Web-scale language models.", "labels": [], "entities": []}, {"text": "showed that for statistical machine translation, a simple smoothing technique (dubbed Stupid Backoff) approaches the quality of the Kneser-Ney algorithm as the amount of training data increases, and with the simple method one can process significantly more data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.7390766739845276}]}, {"text": "Challenges in scaling algorithms to increasinglylarge datasets have become a serious issue for researchers.", "labels": [], "entities": []}, {"text": "It is clear that datasets readily available today and the types of analyses that researchers wish to conduct have outgrown the capabilities of individual computers.", "labels": [], "entities": []}, {"text": "The only practical recourse is to distribute the computation across multiple cores, processors, or machines.", "labels": [], "entities": []}, {"text": "The consequences of failing to scale include misleading generalizations on artificially small datasets and limited practical applicability in real-world contexts, both undesirable.", "labels": [], "entities": []}, {"text": "This paper focuses on two barriers to developing scalable language processing algorithms: challenges associated with parallel programming and access to hardware.", "labels": [], "entities": []}, {"text": "Google's MapReduce framework () provides an attractive programming model for developing scalable algorithms, and with the release of Hadoop, an open-source implementation of MapReduce lead by Yahoo, cost-effective cluster computing is within the reach of most academic research groups.", "labels": [], "entities": []}, {"text": "It is emphasized that this work focuses on largedata algorithms from the perspective of academiacolleagues in commercial environments have long enjoyed the advantages of cluster computing.", "labels": [], "entities": []}, {"text": "However, it is only recently that such capabilities have become practical for academic research groups.", "labels": [], "entities": []}, {"text": "These points are illustrated by a case study in building large word co-occurrence matrices, a simple task that underlies many NLP algorithms.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: the next section overviews the MapReduce framework and why it provides a compelling solution to the issues sketched above.", "labels": [], "entities": []}, {"text": "Section 3 introduces the task of building word co-occurrence matrices, which provides an illustrative case study.", "labels": [], "entities": []}, {"text": "Two separate algorithms are presented in Section 4.", "labels": [], "entities": []}, {"text": "The experimental setup is described in Section 5, followed by presentation of results in Section 6.", "labels": [], "entities": []}, {"text": "Implications and generalizations are discussed following that.", "labels": [], "entities": []}, {"text": "Before concluding, I explore an alternative model of computing based on renting instead of buying hardware, which makes cluster computing practical for everyone.", "labels": [], "entities": []}], "datasetContent": [{"text": "Work reported in this paper used the English Gigaword corpus (version 3), 2 which consists of newswire documents from six separate sources, totaling 7.15 million documents (6.8 GB compressed, 19.4 GB uncompressed).", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 37, "end_pos": 60, "type": "DATASET", "confidence": 0.8121126095453898}]}, {"text": "Some experiments used only documents from the Associated Press Worldstream (APW), which contains 2.27 million documents (1.8 GB compressed, 5.7 GB uncompressed).", "labels": [], "entities": [{"text": "Associated Press Worldstream (APW)", "start_pos": 46, "end_pos": 80, "type": "DATASET", "confidence": 0.883251965045929}]}, {"text": "By LDC's count, the entire collection contains approximately 2.97 billion words.", "labels": [], "entities": []}, {"text": "Prior to working with Hadoop, the corpus was first preprocessed.", "labels": [], "entities": []}, {"text": "All XML markup was removed, followed by tokenization and stopword removal using standard tools from the Lucene search engine.", "labels": [], "entities": [{"text": "Lucene search engine", "start_pos": 104, "end_pos": 124, "type": "DATASET", "confidence": 0.9228449066480001}]}, {"text": "All tokens were replaced with unique integers fora more efficient encoding.", "labels": [], "entities": []}, {"text": "The data was then packed into a Hadoop-specific binary file format.", "labels": [], "entities": []}, {"text": "The entire Gigaword corpus took up 4.69 GB in this format; the APW sub-corpus, 1.32 GB.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9223809540271759}, {"text": "APW", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.9349860548973083}]}, {"text": "Initial experiments used Hadoop version 0.16.0 running on a 20-machine cluster (1 master, 19 slaves).", "labels": [], "entities": []}, {"text": "This cluster was made available to the Uni-1 Implementations of both algorithms are included in Cloud , an open source Hadoop library that I have been developing to support research and education, available from my homepage.", "labels": [], "entities": []}, {"text": "2 LDC catalog number LDC2007T07 versity of Maryland as part of the Google/IBM Academic Cloud Computing Initiative.", "labels": [], "entities": [{"text": "LDC catalog number LDC2007T07", "start_pos": 2, "end_pos": 31, "type": "DATASET", "confidence": 0.6336480975151062}, {"text": "versity", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.643052339553833}]}, {"text": "Each machine has two single-core processors (running at either 2.4 GHz or 2.8 GHz), 4 GB memory.", "labels": [], "entities": []}, {"text": "The cluster has an aggregate storage capacity of 1.7 TB.", "labels": [], "entities": []}, {"text": "Hadoop ran on top of a virtualization layer, which has a small but measurable impact on performance; see (.", "labels": [], "entities": []}, {"text": "Section 6 reports experimental results using this cluster; Section 8 explores an alternative model of computing based on \"renting cycles\".", "labels": [], "entities": []}], "tableCaptions": []}