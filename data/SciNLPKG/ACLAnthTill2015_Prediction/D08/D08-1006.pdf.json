{"title": [{"text": "Refining Generative Language Models using Discriminative Learning", "labels": [], "entities": [{"text": "Refining Generative Language", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8750841816266378}]}], "abstractContent": [{"text": "We propose anew approach to language mod-eling which utilizes discriminative learning methods.", "labels": [], "entities": []}, {"text": "Our approach is an iterative one: starting with an initial language model, in each iteration we generate 'false' sentences from the current model, and then train a clas-sifier to discriminate between them and sentences from the training corpus.", "labels": [], "entities": []}, {"text": "To the extent that this succeeds, the classifier is incorporated into the model by lowering the probability of sentences classified as false, and the process is repeated.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of this approach on a natural language corpus and show it provides an 11.4% improvement in perplexity over a modified kneser-ney smoothed trigram.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling is a fundamental task in natural language processing and is routinely employed in a wide range of applications, such as speech recognition, machine translation, etc'.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7388143390417099}, {"text": "natural language processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6708184878031412}, {"text": "speech recognition", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7697678804397583}, {"text": "machine translation", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.8071040511131287}]}, {"text": "Traditionally, a language model is a probabilistic model which assigns a probability value to a sentence or a sequence of words.", "labels": [], "entities": []}, {"text": "We refer to these as generative language models.", "labels": [], "entities": [{"text": "generative language", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8942412436008453}]}, {"text": "Avery popular example of a generative language model is the n-gram, which conditions the probability of the next word on the previous (n-1)-words.", "labels": [], "entities": [{"text": "generative language", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.9042176008224487}]}, {"text": "Although simple and widely-applicable, it has proven difficult to allow n-grams, and other forms of generative language models as well, to take advantage of non-local and overlapping features.", "labels": [], "entities": [{"text": "generative language", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8900788128376007}]}, {"text": "These sorts of features, however, pose no problem for standard discriminative learning methods, e.g. large-margin classifiers.", "labels": [], "entities": []}, {"text": "For this reason, anew class of language model, the discriminative language model, has been proposed recently to augment generative language models (.", "labels": [], "entities": []}, {"text": "Instead of providing probability values, discriminative language models directly classify sentences as either corrector incorrect, where the definition of correctness depends on the application (e.g. grammatical / ungrammatical, correct translation / incorrect translation, etc').", "labels": [], "entities": []}, {"text": "Discriminative learning methods require negative samples.", "labels": [], "entities": []}, {"text": "Given that the corpora used for training language models contain only real sentences, i.e. positive samples, obtaining these can be problematic.", "labels": [], "entities": []}, {"text": "In most work on discriminative language modeling this was not a major issue as the work was concerned with specific applications, and these provided a natural definition of negative samples.", "labels": [], "entities": [{"text": "discriminative language modeling", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.628626803557078}]}, {"text": "For instance, proposed a discriminative language model fora speech recognition task.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.8192938168843588}]}, {"text": "Given an acoustic sequence, a baseline recognizer was used to generate a set of possible transcriptions.", "labels": [], "entities": []}, {"text": "The correct transcription was taken as a positive sample, while the rest were taken as negative samples.", "labels": [], "entities": []}, {"text": "More recently, however, showed that a discriminative language model can be trained independently of a specific application by using a generative language model to obtain the negative samples.", "labels": [], "entities": []}, {"text": "Using a non-linear large-margin learning algorithm, they successfully trained a classifier to discriminate real sentences from sentences generated by a trigram.", "labels": [], "entities": []}, {"text": "In this paper we extend this line of work to study the extent to which discriminative learning methods can lead to better generative language models per-se.", "labels": [], "entities": [{"text": "generative language", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.9020456373691559}]}, {"text": "The basic intuition is the following: if a classifier can be used to discriminate real sentences from 'false' sentences generated by a language model, then it can also be used to improve that language model by taking probability mass away from sentences classified as false and transferring it to sentences classified as real.", "labels": [], "entities": []}, {"text": "If the resulting language model can be efficiently sampled from, then this process can be repeated, until generated sentences can no longer be distinguished from real ones.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows: In the next section we formally develop this intuition, providing a quick overview of the whole-sentence maximum-entropy model and of self-supervised boosting, two previous works on which we rely.", "labels": [], "entities": []}, {"text": "We also present the method we use for sampling from the current model, which for the present work is far more efficient than the classical Gibbs sampling.", "labels": [], "entities": []}, {"text": "Our experimental results are presented in section 3, and section 4 concludes with a discussion and a future outlook.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our approach on the ATIS natural language corpus ().", "labels": [], "entities": [{"text": "ATIS natural language corpus", "start_pos": 30, "end_pos": 58, "type": "DATASET", "confidence": 0.9151609688997269}]}, {"text": "We split the corpus into a training set of 11,000 sentences, a held-out set containing 1,045 sentences, and a test set containing 1,000 sentences which were reserved for measuring perplexity.", "labels": [], "entities": []}, {"text": "The corpus was pre-processed so that every word appearing less than three times was replaced by a special UNK symbol.", "labels": [], "entities": []}, {"text": "The resulting lexicon contained 603 word types.", "labels": [], "entities": []}, {"text": "Our learning framework leaves open a number of design choices: 1.", "labels": [], "entities": []}, {"text": "Baseline language model: For P 0 we used a trigram with modified kneser-ney smoothing, which is still considered one of the best smoothing methods for n-gram language models.", "labels": [], "entities": []}, {"text": "2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.", "labels": [], "entities": [{"text": "Sentence representation", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7974070906639099}]}, {"text": "A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled.", "labels": [], "entities": []}, {"text": "The value of the n'th coordinate in the vector representation of sentence s was set to the number of times the corresponding n-gram appeared in s.", "labels": [], "entities": []}, {"text": "3. Type of classifiers: For our features we used large-margin classifiers trained using the online algorithm described in).", "labels": [], "entities": []}, {"text": "The code for the classifier was generously provided by Daisuke Okanohara.", "labels": [], "entities": []}, {"text": "This code was extensively optimized to take advantage of the very sparse sentence representation described above.", "labels": [], "entities": []}, {"text": "As shown in, using this representation, a linear classifier cannot distinguish sentences sampled from a trigram and real sentences.", "labels": [], "entities": []}, {"text": "Therefore, we used a 3rd order polynomial kernel, which was found to give good results.", "labels": [], "entities": []}, {"text": "No special effort was otherwise made in order to optimize the parameters of the classifiers.", "labels": [], "entities": []}], "tableCaptions": []}