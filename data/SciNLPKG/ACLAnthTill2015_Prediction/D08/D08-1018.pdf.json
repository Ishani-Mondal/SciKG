{"title": [{"text": "Better Binarization for the CKY Parsing", "labels": [], "entities": [{"text": "CKY Parsing", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.8013063669204712}]}], "abstractContent": [{"text": "We present a study on how grammar binariza-tion empirically affects the efficiency of the CKY parsing.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.7572835087776184}]}, {"text": "We argue that binarizations affect parsing efficiency primarily by affecting the number of incomplete constituents generated , and the effectiveness of binarization also depends on the nature of the input.", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.978420078754425}]}, {"text": "We propose a novel binarization method utilizing rich information learnt from training corpus.", "labels": [], "entities": []}, {"text": "Experimental results not only show that different binarizations have great impacts on parsing efficiency, but also confirm that our learnt binarization outperforms other existing methods.", "labels": [], "entities": [{"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9826866388320923}]}, {"text": "Furthermore we show that it is feasible to combine existing parsing speed-up techniques with our binarization to achieve even better performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Binarization, which transforms an n-ary grammar into an equivalent binary grammar, is essential for achieving an O(n 3 ) time complexity in the contextfree grammar parsing.", "labels": [], "entities": [{"text": "contextfree grammar parsing", "start_pos": 144, "end_pos": 171, "type": "TASK", "confidence": 0.6525762379169464}]}, {"text": "O(n 3 ) tabular parsing algorithms, such as the CKY algorithm, the GHR parser (, the Earley algorithm and the chart parsing algorithm) all convert their grammars into binary branching forms, either explicitly or implicitly.", "labels": [], "entities": [{"text": "O(n 3 ) tabular parsing", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5936315698283059}, {"text": "GHR parser", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.6774278879165649}, {"text": "chart parsing", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.7250410169363022}]}, {"text": "In fact, the number of all possible binarizations of a production with n + 1 symbols on its right * This work was done when Xinying Song and Shilin Ding were visiting students at Microsoft Research Asia.", "labels": [], "entities": []}, {"text": "hand side is known to be the nth Catalan Number . All binarizations lead to the same parsing accuracy, but maybe different parsing efficiency, i.e. parsing speed.", "labels": [], "entities": [{"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9576822519302368}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9193739891052246}, {"text": "parsing", "start_pos": 148, "end_pos": 155, "type": "TASK", "confidence": 0.9717740416526794}]}, {"text": "We are interested in investigating whether and how binarizations will affect the efficiency of the CKY parsing.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.7887778878211975}]}, {"text": "Do different binarizations lead to different parsing efficiency?", "labels": [], "entities": []}, {"text": "gives an example to help answer this question.(a) illustrates the correct parse of the phrase \"get the bag and go\".", "labels": [], "entities": []}, {"text": "We assume that NP \u2192 NP CC NP is in the original grammar.", "labels": [], "entities": []}, {"text": "The symbols enclosed in square brackets in the figure are intermediate symbols.", "labels": [], "entities": []}, {"text": "If a left binarized grammar is used, see, an extra constituent [N P CC] spanning \"the bag and\" will be produced.", "labels": [], "entities": []}, {"text": "Because rule [N P CC] \u2192 NP CC is in the left binarized grammar and there is an NP over \"the bag\" and a CC over the right adjacent \"and\".", "labels": [], "entities": []}, {"text": "Having this constituent is unnecessary, because it lacks an NP to the right to complete the production.", "labels": [], "entities": [{"text": "NP", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9879004955291748}]}, {"text": "However, if aright binarization is used, as shown in(c), such unnecessary constituent can be avoided.", "labels": [], "entities": []}, {"text": "One observation from this example is that different binarizations affect constituent generation, thus affect parsing efficiency.", "labels": [], "entities": [{"text": "parsing", "start_pos": 109, "end_pos": 116, "type": "TASK", "confidence": 0.9608458280563354}]}, {"text": "Another observation is that for rules like X \u2192 Y CC Y , it is more suitable to binarize them in aright branching way.", "labels": [], "entities": []}, {"text": "This can be seen as a linguistic nature: for \"and\", usually the right neighbouring word can indicate the correct parse.", "labels": [], "entities": []}, {"text": "A good binarization should reflect such liguistic nature.", "labels": [], "entities": []}, {"text": "In this paper, we aim to study the effect of binarization on the efficiency of the CKY parsing.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.789059042930603}]}, {"text": "To our knowledge, this is the first work on this problem.", "labels": [], "entities": []}, {"text": "We propose the problem to find the optimal binarization in terms of parsing efficiency (Section 3).", "labels": [], "entities": [{"text": "parsing", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.9627549648284912}]}, {"text": "We argue that binarizations affect parsing efficiency primarily by affecting the number of incomplete constituents generated, and the effectiveness of binarization also depends on the nature of the input (Section 4).", "labels": [], "entities": [{"text": "parsing", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9801774024963379}]}, {"text": "Therefore we propose a novel binarization method utilizing rich information learnt from training corpus (Section 5).", "labels": [], "entities": []}, {"text": "Experimental results show that our binarization outperforms other existing methods (Section 7.2).", "labels": [], "entities": []}, {"text": "Since binarization is usually a preprocessing step before parsing, we argue that better performance can be achieved by combining other parsing speed-up techniques with our binarization (Section 6).", "labels": [], "entities": [{"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9631787538528442}]}, {"text": "We conduct experiments to confirm this (Section 7.3).", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted two experiments on Penn Treebank II corpus.", "labels": [], "entities": [{"text": "Penn Treebank II corpus", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.9933344125747681}]}, {"text": "The first is to compare the effects of different binarizations on parsing and the second is to test the feasibility to combine our work with iterative CKY parsing () to achieve even better efficiency.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9726582169532776}, {"text": "CKY parsing", "start_pos": 151, "end_pos": 162, "type": "TASK", "confidence": 0.6952813863754272}]}, {"text": "Following conventions, we learnt the grammar from Wall Street Journal (WSJ) section 2 to 21 and modified it by discarding all functional tags and empty nodes.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section 2", "start_pos": 50, "end_pos": 85, "type": "DATASET", "confidence": 0.9619644731283188}]}, {"text": "The parser obtained this way is a pure unlexicalized context-free parser with the raw treebank grammar.", "labels": [], "entities": []}, {"text": "Its accuracy turns out to be 72.46% in terms of F1 measure, quite the same as 72.62% as stated in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996739625930786}, {"text": "F1 measure", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9451436698436737}]}, {"text": "We adopt this parser in our experiment not only because of simplicity but also because we focus on parsing efficiency.", "labels": [], "entities": []}, {"text": "For all sentences with no more than 40 words in section 22, we use the first 10% as the development set, and the last 90% as the test set.", "labels": [], "entities": []}, {"text": "There are 158 and 1,420 sentences in development set and test set, respectively.", "labels": [], "entities": []}, {"text": "We use the whole 2,416 sentences in section 23 as the training set.", "labels": [], "entities": []}, {"text": "We use the development set to determine the better form of the ranking function f as well as to tune its weights.", "labels": [], "entities": []}, {"text": "Both metrics of num and ctr are normalized before use.", "labels": [], "entities": []}, {"text": "Since there is only one free variable in \u03bb 1 and \u03bb 2 , we can just enumerate 0 \u2264 \u03bb 1 \u2264 1, and set \u03bb 2 = 1 \u2212 \u03bb 1 . The increasing step is firstly set to 0.05 for the approximate location of the optimal weight, then set to 0.001 to learn more precisely around the optimal.", "labels": [], "entities": []}, {"text": "We find that the optimal is 5,773,088 (constituents produced in parsing development set) with \u03bb 1 = 0.014 for linear form, while for log-linear form the optimal is 5,905,292 with \u03bb 1 = 0.691.", "labels": [], "entities": []}, {"text": "Therefore we determine that the better form for the ranking function is linear with \u03bb 1 = 0.014 and \u03bb 2 = 0.986.", "labels": [], "entities": []}, {"text": "The size of each binarized grammar used in the experiment is shown in.", "labels": [], "entities": []}, {"text": "\"Original\" refers to the raw treebank grammar.", "labels": [], "entities": []}, {"text": "\"Ours\" refers to the learnt binarized grammar by our approach.", "labels": [], "entities": []}, {"text": "For the rest please refer to Section 2.", "labels": [], "entities": []}, {"text": "We also tested whether the size of the training set would have significant effect.", "labels": [], "entities": []}, {"text": "We use the first 10%, 20%, \u00b7 \u00b7 \u00b7 , up to 100% of section 23 as the training set, respectively, and parse the development set.", "labels": [], "entities": []}, {"text": "We find that all sizes examined have a similar impact, since the numbers of constituents produced are all around 5,780,000.", "labels": [], "entities": []}, {"text": "It means the training corpus does not have to be very large.", "labels": [], "entities": []}, {"text": "In this part, we use CKY to parse the entire test set and evaluate the efficiency of different binarizations.", "labels": [], "entities": []}, {"text": "The for-statement implementation of the innermost loop of CKY will affect the parsing time though it won't affect the number of constituents produced as discussed in Section 3.2.", "labels": [], "entities": [{"text": "CKY", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9204698204994202}, {"text": "parsing", "start_pos": 78, "end_pos": 85, "type": "TASK", "confidence": 0.9565294981002808}]}, {"text": "The best implementations maybe different for different binarized grammars.", "labels": [], "entities": []}, {"text": "We examine M1\u223cM4, testing their parsing time on the development set.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9455450177192688}]}, {"text": "Results show that for right binarization the best method is M3, while for the rest the best is M2.", "labels": [], "entities": []}, {"text": "We use the best method for each binarized grammar when comparing the parsing time in Experiment 1.", "labels": [], "entities": []}, {"text": "reports the total number of constituents and total time required for parsing the entire test set.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9826880097389221}]}, {"text": "It shows that different binarizations have great impacts on the efficiency of CKY.", "labels": [], "entities": []}, {"text": "With our binarization, the number of constituents produced is nearly 20% of that required by right binarization and nearly 25% of that by the widely-used left binarization.", "labels": [], "entities": []}, {"text": "As for the parsing time, CKY with our binarization is about 2.5 times as fast as with right binarization and about 1.75 times as fast as with left binarization.", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9769095778465271}]}, {"text": "This illustrates that our binarization can significantly improve the efficiency of the CKY parsing.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.7704825103282928}]}, {"text": "reports the detailed number of complete constituents, successful incomplete constituents and failed incomplete constituents produced in parsing.", "labels": [], "entities": []}, {"text": "The result proves that our binarization can significantly reduce the number of failed incomplete constituents, by a factor of 10 in contrast with left binarization.", "labels": [], "entities": []}, {"text": "Meanwhile, the number of successful in-complete constituents is also reduced by a factor of 2 compared to left binarization.", "labels": [], "entities": []}, {"text": "Another interesting observation is that parsing with a smaller grammar does not always yield a higher efficiency.", "labels": [], "entities": []}, {"text": "Our binarized grammar is more than twice the size of compact binarization, but ours is more efficient.", "labels": [], "entities": []}, {"text": "It proves that parsing efficiency is related to both the size of grammar in use as well as the number of constituents produced.", "labels": [], "entities": [{"text": "parsing", "start_pos": 15, "end_pos": 22, "type": "TASK", "confidence": 0.9681320786476135}]}, {"text": "In this part, we test the performance of combining our binarization with the iterative CKY () (henceforth T&T) algorithm.", "labels": [], "entities": []}, {"text": "Iterative CKY is a procedure of multiple passes of normal CKY: in each pass, it uses a threshold to prune bad constituents; if it cannot find a successful parse in one pass, it will relax the threshold and start another; this procedure is repeated until a successful parse is returned.", "labels": [], "entities": []}, {"text": "T&T used left binarization.", "labels": [], "entities": [{"text": "T&T", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.94329305489858}]}, {"text": "We re-implement their experiments and combine iterative CKY with our binarization.", "labels": [], "entities": []}, {"text": "Note that iterative CKY is an exact inference algorithm that guarantees to return the optimal parse.", "labels": [], "entities": []}, {"text": "As discussed in Section 6, the parsing accuracy is not changed in this experiment.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9708530306816101}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9273297190666199}]}, {"text": "T&T used a held-out set to learn the best step of threshold decrease.", "labels": [], "entities": [{"text": "T&T", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.885582685470581}]}, {"text": "They reported that the best step was 11 (in log-probability).", "labels": [], "entities": []}, {"text": "We found that the best step was indeed 11 for left binarization; for our binarizaiton, the best step was 17.", "labels": [], "entities": []}, {"text": "T&T used M4 as the for-statement implementation of CKY.", "labels": [], "entities": [{"text": "T&T", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9638792077700297}, {"text": "CKY", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9278646111488342}]}, {"text": "In this part, we follow the same method.", "labels": [], "entities": []}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "We can see that iterative CKY can achieve better performance by using a better binarization.", "labels": [], "entities": []}, {"text": "We also see that the reduction by binarization with pruning is less significant than without pruning.", "labels": [], "entities": []}, {"text": "It seems that the pruning itself in iterative CKY can counteract the reduction effect of binarization to some extent.", "labels": [], "entities": []}, {"text": "Still the best performance is archieved by combining iterative CKY with a better binarization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. \"Original\" refers  to the raw treebank grammar. \"Ours\" refers to the  learnt binarized grammar by our approach. For the  rest please refer to Section 2.", "labels": [], "entities": []}, {"text": " Table 3: Grammar size of different binarizations", "labels": [], "entities": []}, {"text": " Table 4: Performance on test set", "labels": [], "entities": []}, {"text": " Table 5. We can see that  iterative CKY can achieve better performance by us- ing a better binarization. We also see that the reduc- tion by binarization with pruning is less significant  than without pruning. It seems that the pruning itself  in iterative CKY can counteract the reduction effect  of binarization to some extent. Still the best per- formance is archieved by combining iterative CKY  with a better binarization.", "labels": [], "entities": []}, {"text": " Table 5: Combining with iterative CKY parsing", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.6651332080364227}]}]}