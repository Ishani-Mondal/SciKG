{"title": [{"text": "A Structured Vector Space Model for Word Meaning in Context", "labels": [], "entities": [{"text": "Word Meaning in Context", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.8211320489645004}]}], "abstractContent": [{"text": "We address the task of computing vector space representations for the meaning of word occurrences , which can vary widely according to context.", "labels": [], "entities": []}, {"text": "This task is a crucial step towards a robust, vector-based compositional account of sentence meaning.", "labels": [], "entities": [{"text": "vector-based compositional account of sentence meaning", "start_pos": 46, "end_pos": 100, "type": "TASK", "confidence": 0.7185290853182474}]}, {"text": "We argue that existing models for this task do not take syntactic structure sufficiently into account.", "labels": [], "entities": []}, {"text": "We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions.", "labels": [], "entities": []}, {"text": "This makes it possible to integrate syntax into the computation of word meaning in context.", "labels": [], "entities": []}, {"text": "In addition, the model performs at and above the state of the art for mod-eling the contextual adequacy of paraphrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic spaces area popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors.", "labels": [], "entities": [{"text": "representation of word meaning", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.7495432943105698}]}, {"text": "In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus.", "labels": [], "entities": []}, {"text": "These vectors are able to provide a robust model of semantic similarity that has been used in NLP ( and to model experimental results in cognitive science).", "labels": [], "entities": []}, {"text": "Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems.", "labels": [], "entities": []}, {"text": "Ina default semantic space as described above, each vector represents one lemma, averaging overall its possible usages.", "labels": [], "entities": []}, {"text": "Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context.", "labels": [], "entities": []}, {"text": "There have been several approaches in the literature) that compute meaning in context from lemma vectors.", "labels": [], "entities": []}, {"text": "Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = ab.", "labels": [], "entities": [{"text": "vector composition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7197540402412415}]}, {"text": "The context b can consist of as little as one word, as shown in Example (1).", "labels": [], "entities": []}, {"text": "In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract.", "labels": [], "entities": []}, {"text": "Conversely, verbs can influence the interpretation of nouns: In (1a), ball is understood as a spherical object, and in (1c) as a dancing event.", "labels": [], "entities": []}, {"text": "(1) a. catch a ball b. catch a disease c. attend a ball In this paper, we argue that models of word meaning relying on this procedure of vector composition are limited both in their scope and scalability.", "labels": [], "entities": []}, {"text": "The underlying shortcoming is a failure to consider syntax in two important ways.", "labels": [], "entities": []}, {"text": "The syntactic relation is ignored.", "labels": [], "entities": []}, {"text": "The first problem concerns the manner of vector composition, which ignores the relation between the target a and its context b.", "labels": [], "entities": [{"text": "vector composition", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7412589192390442}]}, {"text": "This relation can have a decisive influence on their interpretation, as Example (2) shows: a. a horse draws b. draw a horse In (2a), the meaning of the verb draw can be paraphrased as pull, while in (2b) it is similar to sketch.", "labels": [], "entities": []}, {"text": "This difference in meaning is due to the difference in relation: in (2a), horse is the subject, while in (2b) it is the object.", "labels": [], "entities": []}, {"text": "On the modeling side, however, a vector combination function that ignores the relation will assign the same representation to (2a) and (2b).", "labels": [], "entities": []}, {"text": "Thus, existing models are systematically unable to capture this class of phenomena.", "labels": [], "entities": []}, {"text": "Single vectors are too weak to represent phrases.", "labels": [], "entities": []}, {"text": "The second problem arises in the context of the important open question of how semantic spaces can \"scale up\" to provide interesting meaning representations for entire sentences.", "labels": [], "entities": []}, {"text": "We believe that the current vector composition methods, which result in a single vector c, are not informative enough for this purpose.", "labels": [], "entities": [{"text": "vector composition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7275288999080658}]}, {"text": "One proposal for \"scaling up\" is to straightforwardly interpret c = ab as the meaning of the phrase a + b).", "labels": [], "entities": []}, {"text": "The problem is that the vector c can only encode a fixed amount of structural information if its dimensionality is fixed, but there is no upper limit on sentence length, and hence on the amount of structure to be encoded.", "labels": [], "entities": []}, {"text": "It is difficult to conceive how c could encode deeper semantic properties, like predicateargument structure (distinguishing \"dog bites man\" and \"man bites dog\"), that are crucial for sentencelevel semantic tasks such as the recognition of textual entailment ().", "labels": [], "entities": [{"text": "recognition of textual entailment", "start_pos": 224, "end_pos": 257, "type": "TASK", "confidence": 0.8581243455410004}]}, {"text": "An alternative approach to sentence meaning would be to use the vector space representation only for representing word meaning, and to represent sentence structure separately.", "labels": [], "entities": [{"text": "sentence meaning", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.723938837647438}]}, {"text": "Unfortunately, present models cannot provide this grounding either, since they compute a single vector c that provides the same representations for both the meanings of a and bin context.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew, structured vector space model for word meaning (SVS) that addresses these problems.", "labels": [], "entities": [{"text": "word meaning (SVS)", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8285933494567871}]}, {"text": "A SVS representation of a lemma comprises several vectors representing the word's lexical meaning as well as the selectional preferences that it has for its argument positions.", "labels": [], "entities": []}, {"text": "The meaning of word a in context b is computed by combining a with b's selectional preference vector specific to the relation between a and b, addressing the first problem above.", "labels": [], "entities": []}, {"text": "In an expression a + b, the meanings of a and bin this context are computed as two separate vectors a and b . These vectors can then be combined with a representation of the structure's expression (e.g., a parse tree), to address the second problem discussed above.", "labels": [], "entities": []}, {"text": "We test the SVS model on the task of recognizing contextually appropriate paraphrases, finding that SVS performs at and above the state-ofthe-art.", "labels": [], "entities": []}, {"text": "Section 2 reviews related work.", "labels": [], "entities": []}, {"text": "Section 3 presents the SVS model for word meaning in context.", "labels": [], "entities": [{"text": "word meaning in context", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.7882156074047089}]}, {"text": "Sections 4 to 6 relate experiments on the paraphrase appropriateness task.", "labels": [], "entities": [{"text": "paraphrase appropriateness task", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.9305866758028666}]}], "datasetContent": [{"text": "This section provides the background to the following experimental evaluation of SVS, including parameters used for computing the SVS representations that will be used in the experiments.", "labels": [], "entities": [{"text": "SVS", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.8107914924621582}]}, {"text": "In this paper, we evaluate the SVS model against the task of predicting, given a predicate-argument pair, how appropriate a paraphrase (of either the predicate or the argument) is in that context.", "labels": [], "entities": []}, {"text": "We perform two experiments that both use the paraphrase task, but differ in their emphasis.", "labels": [], "entities": []}, {"text": "Experiment 1 replicates an existing evaluation against human judgments.", "labels": [], "entities": []}, {"text": "This evaluation uses synthetic dataset, limited to one particular construction, and constructed to provide maximally distinct paraphrase candidates.", "labels": [], "entities": []}, {"text": "Experiment 2 considers a broader class of constructions along with annotator-generated paraphrase candidates that are not screened for distinctness.", "labels": [], "entities": []}, {"text": "In both experiments, we compare the SVS model against the state-of-theart model by Mitchell and Lapata 2008 (henceforth M&L; cf. Sec. 2 for model details).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment 1: Mean cosine similarity for items  with high-and low-similarity landmarks; correlation with  human judgements (\u03c1). (**: p < 0.01)", "labels": [], "entities": [{"text": "Mean cosine similarity", "start_pos": 24, "end_pos": 46, "type": "METRIC", "confidence": 0.8532489736874899}]}, {"text": " Table 3: Experiment 2: Mean \"out of ten\" precision (P OOT )", "labels": [], "entities": [{"text": "precision (P OOT )", "start_pos": 42, "end_pos": 60, "type": "METRIC", "confidence": 0.7986973047256469}]}]}