{"title": [{"text": "Text 29( )20 108", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel method for discovering and modeling the relationship between informal Chinese expressions (including collo-quialisms and instant-messaging slang) and their formal equivalents.", "labels": [], "entities": []}, {"text": "Specifically, we proposed a bootstrapping procedure to identify a list of candidate informal phrases in web corpora.", "labels": [], "entities": []}, {"text": "Given an informal phrase, we retrieve contextual instances from the web using a search engine, generate hypotheses of formal equivalents via this data, and rank the hypotheses using a conditional log-linear model.", "labels": [], "entities": []}, {"text": "In the log-linear model, we incorporate as feature functions both rule-based intuitions and data co-occurrence phenomena (ei-ther as an explicit or indirect definition, or through formal/informal usages occurring in free variation in a discourse).", "labels": [], "entities": []}, {"text": "We test our system on manually collected test examples, and find that the (formal-informal) relationship discovery and extraction process using our method achieves an average 1-best precision of 62%.", "labels": [], "entities": [{"text": "relationship discovery and extraction", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.8049204349517822}, {"text": "precision", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.6735416054725647}]}, {"text": "Given the ubiquity of informal conversational style on the internet, this work has clear applications for text normalization in text-processing systems including machine translation aspiring to broad coverage.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7859139144420624}, {"text": "machine translation", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.8071933686733246}]}], "introductionContent": [{"text": "Informal text (e.g., newsgroups, online chat, blogs, etc.) is the majority of all text appearing on the Internet.", "labels": [], "entities": []}, {"text": "Informal text tends to have very different style from formal text (e.g., newswire, magazine, etc.).", "labels": [], "entities": []}, {"text": "In particular, they are different in vocabulary, syntactic structure, semantic interpretation, discourse  structure, and soon.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.711620569229126}]}, {"text": "On the other hand, certain relations exist between the informal and formal text, and informal text often has a viable formal equivalent.", "labels": [], "entities": []}, {"text": "shows several naturally occurring examples of informal expressions in Chinese, and Table 2 provides a more detailed inventory and characterization of this phenomena . The first example of informal phrase \"88\" is used very often in Chinese on-line chat when a person wants to say \"bye-bye\" to the other person.", "labels": [], "entities": []}, {"text": "This can be explained as follows.", "labels": [], "entities": []}, {"text": "In Chinese, the standard equivalent to \"byebye\" is \" \" whose PinYin is \"BaiBai\".", "labels": [], "entities": []}, {"text": "Coincidentally, the PinYin of \"88\" is \"BaBa\".", "labels": [], "entities": []}, {"text": "Because \"BaBa\" and \"BaiBai\" are near homophones, people often use \"88\" to represent \"\", either for input convenience or just for fun.", "labels": [], "entities": []}, {"text": "The other relations in are formed due to similar processes as will be described later.", "labels": [], "entities": []}, {"text": "Due to the often substantial divergence between informal and formal text, a text-processing system trained on formal text does not typically work well on informal genres.", "labels": [], "entities": []}, {"text": "For example, in a machine translation system (, if the bilingual training data does not contain the word \" \" (the second example in), it leaves the word untranslated.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7318064868450165}]}, {"text": "On the other hand, if the word \"\" does appear in the training data but it has only a translation \"gruel\" as that is the meaning in the formal text, the translation system may wrongly translate \"\" into \"gruel\" for the informal text where the word \" \" is more likely to mean \"like\".", "labels": [], "entities": []}, {"text": "Therefore, as a text-normalization step, it is desirable to transform the informal text into its standard formal equivalent before feeding it into a general-purpose text-processing system.", "labels": [], "entities": []}, {"text": "Unfortunately, there are many processes for generating informal expressions in common use today.", "labels": [], "entities": []}, {"text": "Such transformations are highly flexible/diverse, and new phrases are invented on the Internet everyday due to major news events, popular movies, TV shows, radio talks, political activities, and soon.", "labels": [], "entities": []}, {"text": "Therefore, it is of great interest to have a data-driven method that can automatically find the relations between informal and formal expressions.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel method for discovering and modeling the relationship between informal Chinese expressions found in web corpora and their formal equivalents.", "labels": [], "entities": []}, {"text": "Specifically, we implement a bootstrapping procedure to identify a list of candidate informal phrases.", "labels": [], "entities": []}, {"text": "Given an individual informal phrase, we retrieve contextual instances from the web using a search engine (in this case, www.baidu.com), generate hypotheses of formal equivalents via this data, and rank the hypotheses using a conditional log-linear model.", "labels": [], "entities": []}, {"text": "In the loglinear model, we incorporate as feature functions both rule-based intuitions and data co-occurrence phenomena (either as an explicit or indirect definition, or through formal/informal usages occurring in free variation in a discourse).", "labels": [], "entities": []}, {"text": "We test our system on manually collected test examples 2 , and find that the (formal-informal) relationship discovery and extraction process using our method achieves an average precision of more than 60%.", "labels": [], "entities": [{"text": "relationship discovery and extraction", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.8033265769481659}, {"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9976638555526733}]}, {"text": "This work has applica-tions for text normalization in many general-purpose text-processing tasks, e.g., machine translation.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7839941680431366}, {"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.8305937349796295}]}, {"text": "To the best of our knowledge, our work is the first published machine-learning approach to productively model the broad types of relationships between informal and formal expressions in Chinese using web corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "Recall that in Section 2 we categorize the formalinformal relations based on the manually collected relations.", "labels": [], "entities": []}, {"text": "In this section, we use a subset of them for training and testing.", "labels": [], "entities": []}, {"text": "In particular, we use 252 examples to train the log-linear model that is described in Section 4, and use 249 examples as test data to compute the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.998325526714325}]}, {"text": "4 shows the weights 5 learned for the various feature functions described in Section 4.4.", "labels": [], "entities": []}, {"text": "Clearly, different feature functions get quite different weights.", "labels": [], "entities": []}, {"text": "This is intuitive as the feature functions may differ in the scale of the feature values or in their importance in ranking the hypotheses.", "labels": [], "entities": []}, {"text": "In fact, this shows the importance of using the log-linear model to learn the optimal weights in a principled and automatic manner, instead of manually tuning the weights in an ad-hoc way.", "labels": [], "entities": []}, {"text": "show the precision results for different categories as described in Section 2, using the ruledriven, data-driven, or both rule and data-driven features, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9992870688438416}]}, {"text": "In the tables, the precision corresponding to the \"top-N \" is computed in the following way: if the true hypothesis is among the top-N hypotheses ranked by the model, we tag the classification as correct, otherwise as wrong.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.999049723148346}]}, {"text": "Clearly, the  larger the N is, the higher the precision is.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9997039437294006}]}, {"text": "Computing the top-N precision (instead of just computing the usual top-1 precision) is meaningful especially when we consider our relation extractor as an intermediate step in an end-to-end text-processing system (e.g., machine translation) since the final decision can be delayed to later stage based on more evidence.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9227768182754517}, {"text": "relation extractor", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.7183929085731506}, {"text": "machine translation)", "start_pos": 220, "end_pos": 240, "type": "TASK", "confidence": 0.8347827990849813}]}, {"text": "In general, our model gets quite respectably high precision for such a task (e.g., more than 60% for top-1 and more than 85% for top-100) when using both data and rule-driven features, as shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9985718727111816}]}, {"text": "Moreover, the data-driven features are more helpful than the rule-driven features (e.g, 25.3% absolute improvement in 1-best precision), while the combination of these features does boost the performance of any individual feature set (e.g., 10.4% absolute improvement in 1-best precision over the case using data-driven features only).", "labels": [], "entities": [{"text": "1-best precision", "start_pos": 118, "end_pos": 134, "type": "METRIC", "confidence": 0.6902296245098114}, {"text": "precision", "start_pos": 278, "end_pos": 287, "type": "METRIC", "confidence": 0.5930420160293579}]}, {"text": "We also carried out experiments (see) in the bootstrapping procedure described in Section 4.1.", "labels": [], "entities": []}, {"text": "In particular, we start from a seed set having 130 relations.", "labels": [], "entities": []}, {"text": "We identify the frequent patterns from the data retrieved from the web for these seed examples.", "labels": [], "entities": []}, {"text": "Then, we use these patterns to identify many more new possible formal-informal relations.", "labels": [], "entities": []}, {"text": "After the first iteration, we select the top 3000 pairs of relations matched by the patterns.", "labels": [], "entities": []}, {"text": "The recall of a manually collected test set (having 750 pairs) on these 3000 pairs is around 30%, which is quite promising given the highly noisy data.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9996452331542969}]}], "tableCaptions": [{"text": " Table 6: Optimal Weights in the Log-linear Model", "labels": [], "entities": []}, {"text": " Table 7: Rule-driven Features only: Precision on Chinese Formal-informal Relation Extraction", "labels": [], "entities": [{"text": "Chinese Formal-informal Relation Extraction", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.5763222277164459}]}, {"text": " Table 8: Data-driven Features only: Precision on Chinese Formal-informal Relation Extraction", "labels": [], "entities": [{"text": "Chinese Formal-informal Relation Extraction", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.6189666837453842}]}, {"text": " Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction", "labels": [], "entities": [{"text": "Chinese Formal-informal Relation Extraction", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.6361850947141647}]}, {"text": " Table 10: Recall of Test Set on a Candidate Set Extracted  by a Bootstrapping Procedure", "labels": [], "entities": []}]}