{"title": [{"text": "Learning with Probabilistic Features for Improved Pipeline Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel learning framework for pipeline models aimed at improving the communication between consecutive stages in a pipeline.", "labels": [], "entities": []}, {"text": "Our method exploits the confidence scores associated with outputs at any given stage in a pipeline in order to compute prob-abilistic features used at other stages downstream.", "labels": [], "entities": []}, {"text": "We describe a simple method of integrating probabilistic features into the linear scoring functions used by state of the art machine learning algorithms.", "labels": [], "entities": []}, {"text": "Experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline models , especially when upstream stages in the pipeline exhibit low accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8365930914878845}, {"text": "named entity recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6571836670239767}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.995261013507843}]}], "introductionContent": [{"text": "Machine learning algorithms are used extensively in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.6480106910069784}]}, {"text": "Applications range from fundamental language tasks such as part of speech (POS) tagging or syntactic parsing, to higher level applications such as information extraction (IE), semantic role labeling (SRL), or question answering (QA).", "labels": [], "entities": [{"text": "part of speech (POS) tagging", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6112591283661979}, {"text": "syntactic parsing", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7649534940719604}, {"text": "information extraction (IE)", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.8476871371269226}, {"text": "semantic role labeling (SRL)", "start_pos": 176, "end_pos": 204, "type": "TASK", "confidence": 0.7845091968774796}, {"text": "question answering (QA)", "start_pos": 209, "end_pos": 232, "type": "TASK", "confidence": 0.8812426686286926}]}, {"text": "Learning a model fora particular language processing problem often requires the output from other natural language tasks.", "labels": [], "entities": []}, {"text": "Syntactic parsing and dependency parsing usually start with a textual input that is tokenized, split in sentences and POS tagged.", "labels": [], "entities": [{"text": "Syntactic parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8943388760089874}, {"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8407008945941925}]}, {"text": "In information extraction, named entity recognition (NER), coreference resolution, and relation extraction (RE) have been shown to benefit from features that use POS tags and syntactic dependencies.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.8405570089817047}, {"text": "named entity recognition (NER)", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.7906922350327173}, {"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9521058797836304}, {"text": "relation extraction (RE)", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.7453289270401001}]}, {"text": "Similarly, most SRL approaches assume a parse tree representation of the input sentences.", "labels": [], "entities": [{"text": "SRL", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9835286140441895}]}, {"text": "The common practice in modeling such dependencies is to use a pipeline organization, in which the output of one task is fed as input to the next task in the sequence.", "labels": [], "entities": []}, {"text": "One advantage of this model is that it is very simple to implement; it also allows fora modular approach to natural language processing.", "labels": [], "entities": []}, {"text": "The key disadvantage is that errors propagate between stages in the pipeline, significantly affecting the quality of the final results.", "labels": [], "entities": []}, {"text": "One solution is to solve the tasks jointly, using the principled framework of probabilistic graphical models.", "labels": [], "entities": []}, {"text": "use factorial Conditional Random Fields (CRFs) () to jointly predict POS tags and segment noun phrases, improving on the cascaded models that perform the two tasks in sequence.) describe a CRF model that integrates the tasks of citation segmentation and citation matching.", "labels": [], "entities": [{"text": "citation segmentation", "start_pos": 228, "end_pos": 249, "type": "TASK", "confidence": 0.8134595155715942}, {"text": "citation matching", "start_pos": 254, "end_pos": 271, "type": "TASK", "confidence": 0.877595067024231}]}, {"text": "Their empirical results show the superiority of the integrated model over the pipeline approach.", "labels": [], "entities": []}, {"text": "While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the right representations, the associated inference algorithms and their computational complexity.", "labels": [], "entities": []}, {"text": "Recent negative results on the integration of syntactic parsing with SRL () provide additional evidence for the difficulty of this general approach.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.715272381901741}]}, {"text": "When dependencies between the tasks can be formulated in terms of constraints between their outputs, a simpler approach is to solve the tasks separately and integrate the constraints in a linear programming formulation, as proposed by for the simultaneous learning of named entities and relations between them.", "labels": [], "entities": []}, {"text": "More recently, model the linguistic pipelines as Bayesian networks on which they perform Monte Carlo inference in order to find the most likely output for the final stage in the pipeline.", "labels": [], "entities": []}, {"text": "In this paper, we present anew learning method for pipeline models that mitigates the problem of error propagation between the tasks.", "labels": [], "entities": []}, {"text": "Our method exploits the probabilities output by any given stage in the pipeline as weights for the features used at other stages downstream.", "labels": [], "entities": []}, {"text": "We show a simple method of integrating probabilistic features into linear scoring functions, which makes our approach applicable to state of the art machine learning algorithms such as).", "labels": [], "entities": []}, {"text": "Experimental results on dependency parsing and named entity recognition show useful improvements over the baseline pipeline models, especially when the basic pipeline components exhibit low accuracy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8472769260406494}, {"text": "named entity recognition", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6858651538689932}, {"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9930157661437988}]}], "datasetContent": [{"text": "We test the pipeline model M \u2032 3 versus the traditional model M 1 on the task of detecting mentions of person entities in the ACE dataset 2 . We use the standard training -testing split of the ACE 2002 dataset in which the training dataset is also augmented with the documents from the ACE 2003 dataset.", "labels": [], "entities": [{"text": "ACE dataset", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.9733536541461945}, {"text": "ACE 2002 dataset", "start_pos": 193, "end_pos": 209, "type": "DATASET", "confidence": 0.9505276282628378}, {"text": "ACE 2003 dataset", "start_pos": 286, "end_pos": 302, "type": "DATASET", "confidence": 0.9509783585866293}]}, {"text": "The combined dataset contains 674 documents for training and 97 for testing.", "labels": [], "entities": []}, {"text": "We implemented the CRF model in MALLET using three different sets of features: Tree, Flat, and Full corresponding to the union of all flat and tree features.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.6994268298149109}]}, {"text": "The POS tagger and the dependency parser were trained on sections 2-21 of the Penn Treebank, followed by an isotonic regression step on section 23 for the dependency parser.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.5924098044633865}, {"text": "Penn Treebank", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9957460761070251}]}, {"text": "We compute precision recall (PR) graphs by varying a threshold on the token level confidence output by the CRF tagger, and summarize the tagger performance using the area under the curve.", "labels": [], "entities": [{"text": "precision recall (PR", "start_pos": 11, "end_pos": 31, "type": "METRIC", "confidence": 0.8811712861061096}]}, {"text": "tures consistently outperforms the traditional model, especially when only tree features are used.", "labels": [], "entities": []}, {"text": "Dependency parsing is significantly less accurate than POS tagging.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8586181998252869}, {"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.77320197224617}]}, {"text": "Consequently, the improvement for the tree based model is more substantial than for the flat model, confirming our expectation that probabilistic features are more useful when upstream stages in the pipeline are less accurate.", "labels": [], "entities": []}, {"text": "shows the PR curves obtained for the tree-based models, on which we see a significant 5% improvement in precision over a wide range of recall values.", "labels": [], "entities": [{"text": "PR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.8406577706336975}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9992445707321167}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9968357682228088}]}], "tableCaptions": [{"text": " Table 3: Dependency parsing results.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8503358066082001}]}, {"text": " Table 4: Mention detection results.", "labels": [], "entities": [{"text": "Mention detection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9036961495876312}]}]}