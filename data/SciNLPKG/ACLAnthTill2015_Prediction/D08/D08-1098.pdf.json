{"title": [{"text": "CoCQA: Co-Training Over Questions and Answers with an Application to Predicting Question Subjectivity Orientation", "labels": [], "entities": [{"text": "Predicting Question Subjectivity Orientation", "start_pos": 69, "end_pos": 113, "type": "TASK", "confidence": 0.7392944842576981}]}], "abstractContent": [{"text": "An increasingly popular method for finding information online is via the Community Question Answering (CQA) portals such as Yahoo!", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 73, "end_pos": 107, "type": "TASK", "confidence": 0.7652397702137629}]}, {"text": "Answers , Naver, and Baidu Knows.", "labels": [], "entities": []}, {"text": "Searching the CQA archives, and ranking , filtering, and evaluating the submitted answers requires intelligent processing of the questions and answers posed by the users.", "labels": [], "entities": [{"text": "CQA archives", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.9815276265144348}]}, {"text": "One important task is automatically detecting the question's subjectivity orientation: namely, whether a user is searching for subjective or objective information.", "labels": [], "entities": [{"text": "detecting the question's subjectivity orientation", "start_pos": 36, "end_pos": 85, "type": "TASK", "confidence": 0.7580819924672445}]}, {"text": "Unfortunately, real user questions are often vague, ill-posed, poorly stated.", "labels": [], "entities": []}, {"text": "Furthermore, there has been little labeled training data available for real user questions.", "labels": [], "entities": []}, {"text": "To address these problems , we present CoCQA, a co-training system that exploits the association between the questions and contributed answers for question analysis tasks.", "labels": [], "entities": [{"text": "question analysis tasks", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.7812354365984598}]}, {"text": "The co-training approach allows CoCQA to use the effectively unlimited amounts of unlabeled data readily available in CQA archives.", "labels": [], "entities": []}, {"text": "In this paper we study the effectiveness of CoCQA for the question subjectivity classification task by experimenting over thousands of real users' questions.", "labels": [], "entities": [{"text": "question subjectivity classification task", "start_pos": 58, "end_pos": 99, "type": "TASK", "confidence": 0.8083541095256805}]}], "introductionContent": [{"text": "Automatic question answering (QA) has been one of the long-standing goals of natural language processing, information retrieval, and artificial intelligence research.", "labels": [], "entities": [{"text": "Automatic question answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8384277572234472}, {"text": "natural language processing", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.657194604476293}, {"text": "information retrieval", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7815134525299072}]}, {"text": "For a natural language question we would like to respond with a specific, accurate, and complete answer that addresses the question.", "labels": [], "entities": []}, {"text": "Although much progress has been made, answering complex, opinion, and even many factual questions automatically is still beyond the current state-of-the-art.", "labels": [], "entities": []}, {"text": "At the same time, the rise of popularity in social media and collaborative content creation services provides a promising alternative to web search or completely automated QA.", "labels": [], "entities": []}, {"text": "The explicit support for social interactions between participants, such as posting comments, rating content, and responding to questions and comments makes this medium particularly amenable to Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.8251248598098755}]}, {"text": "Some very successful examples of Community Question Answering (CQA) sites are Yahoo!", "labels": [], "entities": [{"text": "Community Question Answering (CQA)", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.7538664390643438}, {"text": "Yahoo!", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.9071084856987}]}, {"text": "Answers 1 and Naver 2 , and Baidu Knows . Yahoo!", "labels": [], "entities": [{"text": "Baidu Knows . Yahoo!", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.7373406410217285}]}, {"text": "Answers alone has already amassed hundreds of millions of answers posted by millions of participants on thousands of topics.", "labels": [], "entities": []}, {"text": "The questions posted to such CQA portals are typically complex, subjective, and rely on human interpretation to understand the corresponding information need.", "labels": [], "entities": []}, {"text": "At the same time, the questions are also usually ill-phrased, vague, and often subjective in nature.", "labels": [], "entities": []}, {"text": "Hence, analysis of the questions (and of the corresponding user intent) in this setting is a particularly difficult task.", "labels": [], "entities": []}, {"text": "At the same time, CQA content incorporates the relationships between questions and the corresponding answers.", "labels": [], "entities": []}, {"text": "Because of the various incentives provided by the CQA sites, answers posted by users tend to be, at least to some degree, responsive to the question.", "labels": [], "entities": []}, {"text": "This observation suggests investigating whether the relation-ship between questions and answers can be exploited to improve automated analysis of the CQA content and the user intent behind the questions posted.", "labels": [], "entities": []}, {"text": "To this end, we exploit the ideas of cotraining, a general semi-supervised learning approach naturally applicable to cases of complementary views on a domain, for example, web page links and content.", "labels": [], "entities": []}, {"text": "In our setting, we focus on the complimentary views fora question, namely the text of the question and the text of the associated answers.", "labels": [], "entities": []}, {"text": "As a concrete case-study of our approach we focus on one particularly important aspect of intent detection: the subjectivity orientation.", "labels": [], "entities": [{"text": "intent detection", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7184334546327591}]}, {"text": "We attempt to predict whether a question posted in a CQA site is subjective or objective.", "labels": [], "entities": []}, {"text": "Objective questions are expected to be answered with reliable or authoritative information, typically published online and possibly referenced as part of the answer, whereas subjective questions seek answers containing private states, e.g. personal opinions, judgment, experiences.", "labels": [], "entities": []}, {"text": "If we could automatically predict the orientation of a question, we would be able to better rank or filter the answers, improve search over the archives, and more accurately identify similar questions.", "labels": [], "entities": []}, {"text": "For example, if a question is objective, we could try to find a few highly relevant articles as references, whereas if a question is subjective, useful answers are not expected to be found in authoritative sources and tend to rank low with current question answering and CQA search techniques.", "labels": [], "entities": [{"text": "question answering", "start_pos": 248, "end_pos": 266, "type": "TASK", "confidence": 0.7477519810199738}]}, {"text": "Finally, learning how to identify question orientation is a crucial component of inferring user intent, a long-standing problem in web information access settings.", "labels": [], "entities": [{"text": "identify question orientation", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.669064849615097}]}, {"text": "In particular, we focus on the following research questions: \u2022 Can we utilize the inherent structure of the CQA interactions and use the unlimited amounts of unlabeled data to improve classification performance, and/or reduce the amount of manual labeling required?", "labels": [], "entities": []}, {"text": "\u2022 Can we automatically predict question subjectivity in Community Question Answering -and which features are useful for this task in the real CQA setting?", "labels": [], "entities": [{"text": "Community Question Answering", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.610703299442927}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first overview the community question answering setting, and state the question orientation classification problem, which we use as the motivating application for our system, more precisely.", "labels": [], "entities": [{"text": "community question answering setting", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.6687702536582947}, {"text": "question orientation classification", "start_pos": 74, "end_pos": 109, "type": "TASK", "confidence": 0.86687038342158}]}, {"text": "We then introduce our CoCQA system for semi-supervised classification of questions and answers in CQA communities (Section 3).", "labels": [], "entities": [{"text": "semi-supervised classification of questions and answers in CQA communities", "start_pos": 39, "end_pos": 113, "type": "TASK", "confidence": 0.6941092312335968}]}, {"text": "We report the results of our experiments over thousands of real user questions in Section 4, showing the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "Finally, we review related work in Section 5, and discuss our conclusions and future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with supervised and semisupervised methods on a relatively large data set from Yahoo!", "labels": [], "entities": [{"text": "data set from Yahoo!", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.700023478269577}]}, {"text": "To our knowledge, there is no standard dataset of real questions and answers posted by online users, labeled for subjectivity orientation.", "labels": [], "entities": []}, {"text": "Hence, we had to create a dataset ourselves.", "labels": [], "entities": []}, {"text": "To create our dataset, we downloaded more than 30,000 resolved questions from each of the following top-level categories of Yahoo!", "labels": [], "entities": []}, {"text": "Answers: Arts, Education, Health, Science, and Sports.", "labels": [], "entities": []}, {"text": "We randomly chose 200 questions from each category to create a raw dataset with 1,000 questions total.", "labels": [], "entities": []}, {"text": "Then, we labeled the dataset with annotators from the Amazon's Mechanical Turk service 4 . For annotation, each question was judged by 5 Mechanical Turk workers who passed a qualification test of 10 questions (labeled by ourselves) with at least 9 of them correctly marked.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk service", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.9478178143501281}]}, {"text": "The qualification test was required to ensure that the raters were sufficiently competent to make reasonable judgments.", "labels": [], "entities": [{"text": "qualification", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.8836525082588196}]}, {"text": "We grouped the tasks into 25 question batches, where the whole batch was submitted as the Mechanical Turk's Human Intelligence Task (HIT).", "labels": [], "entities": [{"text": "Mechanical Turk's Human Intelligence Task (HIT)", "start_pos": 90, "end_pos": 137, "type": "TASK", "confidence": 0.6923087239265442}]}, {"text": "The batching of questions was done to easily detect the \"random\" ratings produced by irresponsible workers.", "labels": [], "entities": []}, {"text": "That is, each worker rated a batch of 25 questions.", "labels": [], "entities": []}, {"text": "While precise definition of subjectivity is elusive, we decided to take the practical perspective, namely the \"majority\" interpretation.", "labels": [], "entities": []}, {"text": "The annotators were instructed to guess orientation according to how the question would be answered by most people.", "labels": [], "entities": []}, {"text": "We did not deal with multi-part questions: if any part of question was subjective, the whole question was labeled as subjective.", "labels": [], "entities": []}, {"text": "The gold standard was thus derived with the majority strategy, followed by manual inspection as a \"sanity check\".", "labels": [], "entities": []}, {"text": "At this stage we removed 22 questions with undeterminable meaning, including gems such as \"Upward Soccer   Use C Q ,j-1 to predict labels for U and choose top K items with highest confidence \ud97b\udf59 E Q, , j-1 Use CA ,j-1 to predict labels for U and choose top K items with highest confidence \ud97b\udf59 E A, , j-1  For the subjectivied experiments to follow, we attempt to capture the linguistic characteristics identified in previous work (Section 5) in a lightweight and robust manner, due to the informal and noisy nature of CQA.", "labels": [], "entities": []}, {"text": "In particular, we use the following feature classes, computed separately over question and answer content: \ud97b\udf59  \ud97b\udf59 Word and POS n-gram (n<=3, i.e. ). We use the character 3-grams features to overcome spelling errors and problems of illformatted or ungrammatical questions, and the POS information to capture common patterns across domains, as words, especially the content words, are quite diverse in different topical domains.", "labels": [], "entities": [{"text": "\ud97b\udf59  \ud97b\udf59 Word", "start_pos": 107, "end_pos": 116, "type": "DATASET", "confidence": 0.5604472756385803}]}, {"text": "For word and character 3-gram features, we consider two different versions: case-sensitive and case-insensitive.", "labels": [], "entities": []}, {"text": "Case-insensitive features are assumed to be helpful for mitigating negative effects of illformatted text.", "labels": [], "entities": []}, {"text": "Moreover, we experimented with three term weighting schemes: Binary, TF, and TF*IDF.", "labels": [], "entities": [{"text": "TF*IDF", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.7044688860575358}]}, {"text": "Term frequency (TF) exhibited better performance in our development experiments, so we use this weighting scheme for all the experiments in Section 4.", "labels": [], "entities": [{"text": "Term frequency (TF)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.930515193939209}]}, {"text": "Regarding features: both words and structure of the text (e.g., word order) can be used to infer subjectivity.", "labels": [], "entities": []}, {"text": "Therefore, the features we employ, such as words and word n-grams, are expected to be useful as a (coarse) proxy to grammatical and phrase features.", "labels": [], "entities": []}, {"text": "Unlike traditional work on news-like text, the text of CQA and has poor spelling, grammar, and heavily uses non-standard abbreviations, hence our decision to use character n-grams.", "labels": [], "entities": [{"text": "CQA", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9074706435203552}]}, {"text": "Metrics: Since the prediction on both subjective questions and objective questions is equally important, we use the macroaveraged F1 measure as the evaluation metric.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.9578307867050171}]}, {"text": "This is computed as the macro average of F1 measures computed for the Subjective and Objective classes individually.", "labels": [], "entities": [{"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9951064586639404}]}, {"text": "The Methods compared: We compare our approach with both the base supervised learning, as well as GE, a state-of-the-art semisupervised method: \ud97b\udf59 Supervised: we use the LibSVM implementation) with linear kernel.", "labels": [], "entities": [{"text": "GE", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.6122741103172302}]}, {"text": "\ud97b\udf59 GE: This is a state-of-the-art semisupervised learning algorithm, Generalized Expectation (GE), introduced in () that incorporates model expectations into the objective functions for parameter estimation.", "labels": [], "entities": [{"text": "\ud97b\udf59", "start_pos": 0, "end_pos": 1, "type": "DATASET", "confidence": 0.7305898070335388}, {"text": "GE", "start_pos": 2, "end_pos": 4, "type": "DATASET", "confidence": 0.46691250801086426}]}, {"text": "\ud97b\udf59 CoCQA: Our method (Section 3).", "labels": [], "entities": [{"text": "\ud97b\udf59", "start_pos": 0, "end_pos": 1, "type": "DATASET", "confidence": 0.8650757074356079}, {"text": "CoCQA", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.5779457688331604}]}, {"text": "For semi-supervised learning experiments, we selected a random subset of 2,000 unlabeled questions for each of the topical categories, for the total of 10,000 unlabeled questions.", "labels": [], "entities": []}, {"text": "First we report the performance of our Supervised baseline system with a variety of features, reporting the average results of 5-fold cross validation.", "labels": [], "entities": []}, {"text": "Then we investigate the performance to our new CoCQA framework under a variety of settings.", "labels": [], "entities": []}, {"text": "reports the classification performance for varying units of representation (e.g., question text vs. answer text) and the varying feature sets.", "labels": [], "entities": []}, {"text": "We used case-insensitive features and TF (term frequency within the text unit) as feature weights, as these two settings achieved the best results in our development experiments.", "labels": [], "entities": [{"text": "TF", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9913875460624695}]}, {"text": "The rows show performance considering only the question text (question), the best answer (best_ans), text of all answers to a question (all_ans), the text of the question and the best answer (q_bestans), and the text of the question with all answers (q_allans), respectively.", "labels": [], "entities": []}, {"text": "In particular, using the words in the question alone achieves F1 of 0.717, compared to using words in the question and the best answers text (F1 of 0.695).", "labels": [], "entities": [{"text": "F1", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.9998137354850769}, {"text": "F1", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9980340600013733}]}, {"text": "For comparison, a na\u00efve baseline that always guesses the majority class (Subjective) obtains F1 of 0.398.", "labels": [], "entities": [{"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9995978474617004}]}], "tableCaptions": [{"text": " Table 1: Labeled dataset statistics.", "labels": [], "entities": []}, {"text": " Table 2. Performance of predicting question  orientation on the mixed dataset with varying  feature sets (Supervised).", "labels": [], "entities": [{"text": "predicting question  orientation", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.9089466134707133}]}, {"text": " Table 3. Experiment results on sub-categories  with supervised SVM (q_bestans features).", "labels": [], "entities": []}, {"text": " Table 4. Performance of CoCQA, GE, and Su- pervised with the same feature and data settings.", "labels": [], "entities": [{"text": "GE", "start_pos": 32, "end_pos": 34, "type": "DATASET", "confidence": 0.7987531423568726}]}]}