{"title": [{"text": "Integrating Multi-level Linguistic Knowledge with a Unified Framework for Mandarin Speech Recognition", "labels": [], "entities": [{"text": "Mandarin Speech Recognition", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.6923260688781738}]}], "abstractContent": [{"text": "To improve the Mandarin large vocabulary continuous speech recognition (LVCSR), a unified framework based approach is introduced to exploit multi-level linguistic knowledge.", "labels": [], "entities": [{"text": "Mandarin large vocabulary continuous speech recognition (LVCSR)", "start_pos": 15, "end_pos": 78, "type": "TASK", "confidence": 0.6531920797295041}]}, {"text": "In this framework, each knowledge source is represented by a Weighted Finite State Transducer (WFST), and then they are combined to obtain a so-called analyzer for integrating multi-level knowledge sources.", "labels": [], "entities": []}, {"text": "Due to the uniform transducer representation, any knowledge source can be easily integrated into the analyzer, as long as it can be encoded into WFSTs.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 145, "end_pos": 150, "type": "DATASET", "confidence": 0.9267153739929199}]}, {"text": "Moreover, as the knowledge in each level is modeled independently and the combination is processed in the model level, the information inherently in each knowledge source has a chance to be thoroughly exploited.", "labels": [], "entities": []}, {"text": "By simulations, the effectiveness of the analyzer is investigated, and then a LVCSR system embedding the presented ana-lyzer is evaluated.", "labels": [], "entities": []}, {"text": "Experimental results reveal that this unified framework is an effective approach which significantly improves the performance of speech recognition with a 9.9% relative reduction of character error rate on the HUB-4 test set, a widely used Mandarin speech recognition task.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.8282977342605591}, {"text": "character error rate", "start_pos": 182, "end_pos": 202, "type": "METRIC", "confidence": 0.6392835179964701}, {"text": "HUB-4 test set", "start_pos": 210, "end_pos": 224, "type": "DATASET", "confidence": 0.9364826679229736}, {"text": "Mandarin speech recognition task", "start_pos": 240, "end_pos": 272, "type": "TASK", "confidence": 0.6626312434673309}]}], "introductionContent": [{"text": "Language modeling is essential for large vocabulary continuous speech recognition (LVCSR), which aims to determine the prior probability of a supposed word string W , p(W ).", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6718970388174057}, {"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 35, "end_pos": 89, "type": "TASK", "confidence": 0.7025011740624905}]}, {"text": "Although the word-based ngram language model remains the mainstream for Recently, structured language models have been introduced to make use of syntactic hierarchical characteristics).", "labels": [], "entities": []}, {"text": "Nevertheless, the computational complexity of decoding will be heavily increased, as they are parser-based models.", "labels": [], "entities": []}, {"text": "In contrast, the classbased language model groups the words that have similar functions of syntax or semantics into meaningful classes.", "labels": [], "entities": []}, {"text": "As a result, it handles the questions of data sparsity and generalization of unseen event.", "labels": [], "entities": []}, {"text": "In practice, the part-of-speech (POS) information, capturing the syntactic role of words, has been widely used in clustering words ().", "labels": [], "entities": []}, {"text": "In Heeman's POS language model, the joint probability of word sequence and associated POS sequence was estimated directly, which has been demonstrated to be superior to the conditional probability previously used in the class-based models).", "labels": [], "entities": []}, {"text": "Moreover, a SuperARV language model was presented (), in which lexical features and syntactic constraints were tightly integrated into a linguistic structure of SuperARV serving as a class in the model.", "labels": [], "entities": []}, {"text": "Thus, these knowledge was integrated in the representation level, and then the joint probabilities of words and corresponding SuperARVs were estimated.", "labels": [], "entities": []}, {"text": "However, in the class-based language models, words are taken as the model units, while other units smaller or larger than words are unfeasible for modeling simultaneously, such as the Chinese characters for Chinese names.", "labels": [], "entities": []}, {"text": "Usually, speech recognition systems can only recognize the words within a predefined dictionary.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7156312018632889}]}, {"text": "With the increase of unknown words, i.e., out-ofvocabulary (OOV) words, the performance will degrade dramatically.", "labels": [], "entities": []}, {"text": "This is because not only those unknown words cannot be recognized correctly, but the words surrounding them will be affected.", "labels": [], "entities": []}, {"text": "Thus, many efforts have been made to deal with the issue of OOV words), and various model units smaller than words have been examined to recognize OOVs from speech, such as phonemes (, variable-length phoneme sequence (), syllable) and sub-word (.", "labels": [], "entities": []}, {"text": "Since the proper name is atypical category of OOV words and usually takes a very large proportion among all kinds of OOV words, it has been specially addressed in (;).", "labels": [], "entities": []}, {"text": "All those attempts mentioned above succeed in utilizing linguistic knowledge in language modeling in some degree respectively.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7168621569871902}]}, {"text": "In this study, a unified framework based approach, which aims to exploit information from multi-level linguistic knowledge, is presented.", "labels": [], "entities": []}, {"text": "Here, the Weighted Finite State Transducer (WFST) turns to bean ideal choice for our purpose.", "labels": [], "entities": []}, {"text": "WFSTs were formerly introduced to simplify the integration of models in speech recognition, including acoustic models, phonetic models and word n-gram).", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8074471354484558}, {"text": "speech recognition", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7251963466405869}]}, {"text": "In recent years, the WFST has been successfully applied in several state-of-the-art speech recognition systems, such as systems developed by the AMI project (), IBM () and AT&T (, and in various fields of natural language processing, such as smoothed n-gram model, partial parsing, named entities recognition (), semantic interpretation () and machine translation).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7511741518974304}, {"text": "partial parsing", "start_pos": 265, "end_pos": 280, "type": "TASK", "confidence": 0.7133353650569916}, {"text": "named entities recognition", "start_pos": 282, "end_pos": 308, "type": "TASK", "confidence": 0.6917601625124613}, {"text": "semantic interpretation", "start_pos": 313, "end_pos": 336, "type": "TASK", "confidence": 0.7513745129108429}, {"text": "machine translation", "start_pos": 344, "end_pos": 363, "type": "TASK", "confidence": 0.7916451394557953}]}, {"text": "In, the WFST has been further used for language model adaptation, where language models of different vocabularies that represented different styles were integrated through the framework of speech translation.", "labels": [], "entities": [{"text": "WFST", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.8610122203826904}, {"text": "language model adaptation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6949921250343323}, {"text": "speech translation", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.7432312965393066}]}, {"text": "In WFST-based systems, all of the models are represented uniformly by WFSTs, and the general composition algorithm () combines these representations flexibly and efficiently.", "labels": [], "entities": []}, {"text": "Thereby, rather than integrating the models step by step in decoding stage, a complete search network is constructed in advance.", "labels": [], "entities": []}, {"text": "The combined WFST will be more efficient by optimizing with determinization, minimization and pushing algorithms of WFSTs (.", "labels": [], "entities": [{"text": "WFST", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8872244954109192}]}, {"text": "Besides, the researches on optimizing the search space and improving WFST-based speech recognition has been carried out, especially on how to perform on-the-fly WFSTs composition more efficiently.", "labels": [], "entities": [{"text": "WFST-based speech recognition", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.6584288279215494}, {"text": "WFSTs composition", "start_pos": 161, "end_pos": 178, "type": "TASK", "confidence": 0.7539046108722687}]}, {"text": "In this study, we extend the linguistic knowledge used in speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7966342866420746}]}, {"text": "As WFSTs provide a common and natural representation for lexical constraints, n-gram language model, Hidden Markov Model models and context-dependency, multi-level knowledge sources can be encoded into WFSTs under the uniform transducer representation.", "labels": [], "entities": []}, {"text": "Then this group of WFSTs is flexibly combined together to obtain an analyzer representing knowledge of person and location names as well as POS information.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.7874061465263367}]}, {"text": "Afterwards, the presented analyzer is incorporated into LVCSR to evaluate the linguistic correctness of recognition candidates by an n-best rescoring.", "labels": [], "entities": []}, {"text": "Unlike other methods, this approach holds two distinct features.", "labels": [], "entities": []}, {"text": "Firstly, as all multi-level knowledge sources are modeled independently, the model units such as character, words, phrase, etc., can be chosen freely.", "labels": [], "entities": []}, {"text": "Meanwhile, the integration of these information sources is conducted in the model level rather than the representation level.", "labels": [], "entities": []}, {"text": "This setup will help to model each knowledge source sufficiently and may promote the accuracy of speech recognition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9985785484313965}, {"text": "speech recognition", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8729044198989868}]}, {"text": "Secondly, under this unified framework, it is easy to combine additional knowledge source into the framework with the only requirement that the new knowledge source can be represented by WFSTs.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 187, "end_pos": 192, "type": "DATASET", "confidence": 0.9328588247299194}]}, {"text": "Moreover, since all knowledge sources are finally represented by a single WFST, additional efforts are not required for decoding the new knowledge source.", "labels": [], "entities": [{"text": "WFST", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9158483743667603}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In section 2, we introduce our analyzer in detail, and incorporate it into a Mandarin speech recognition system.", "labels": [], "entities": [{"text": "Mandarin speech recognition", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.532380590836207}]}, {"text": "In section 3, the simulations are performed to evaluate the analyzer and test its effectiveness when being applied to LVCSR.", "labels": [], "entities": [{"text": "LVCSR", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.9003919959068298}]}, {"text": "The conclusion appears in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Considering the function of the analyzer, cascaded subtasks of word segmentation, names recognition and POS tagging can be processed jointly, while they are traditionally handled in a pipeline manner.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.733566626906395}, {"text": "names recognition", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.8802989721298218}, {"text": "POS tagging", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.765198290348053}]}, {"text": "Hence, a comparison between the analyzer and the pipeline system can be used to evaluate the effectiveness of the introduced framework for knowledge integration.", "labels": [], "entities": [{"text": "knowledge integration", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.7379604876041412}]}, {"text": "As illustrated in, two systems based on the presented analyzer and the pipeline manner are constructed respectively.", "labels": [], "entities": []}, {"text": "The evaluation data came from the People's Daily of China in 1998 from January to June (annotated by the Institute of Computational Linguistics of Peking University 1 ), among which the January to May data was taken as the training set, and the June data was taken as the test set (consisted of 21,143 sentences and about 1.2 million words).", "labels": [], "entities": [{"text": "People's Daily of China in 1998", "start_pos": 34, "end_pos": 65, "type": "DATASET", "confidence": 0.8619323628289359}]}, {"text": "The first two thousand sentences from the June data were extracted as the development set, used to fix the composition weight \u03b1 in equation 2.", "labels": [], "entities": [{"text": "June data", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.9048680365085602}]}, {"text": "A dictionary including about 113,000 words was extracted from the training data, in which a person or location name was accounted as a word in vocabulary, only when the number of its appearances was no less than three.", "labels": [], "entities": []}, {"text": "In, the analyzer is compared with the pipeline system, where the analyzer outperforms the pipeline manner on all the subtasks in terms of F 1-score metric.", "labels": [], "entities": []}, {"text": "Furthermore to detect the differences, the statistical significance test using approximate randomization approach) is done on the word segmentation results.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.6913630068302155}]}, {"text": "Since there are more than 21,000 sentences in the test set, which is not appropriate for approximate randomization test, ten sets (500 sentences for each) are randomly selected from the test corpus.", "labels": [], "entities": []}, {"text": "For each set, we run 1048576 shuffles twice and calculate the significance level pvalue according to the shuffled results.", "labels": [], "entities": [{"text": "significance level pvalue", "start_pos": 62, "end_pos": 87, "type": "METRIC", "confidence": 0.859005888303121}]}, {"text": "It has been shown that all p-value are less than 0.001 on the ten sets.", "labels": [], "entities": []}, {"text": "Accordingly the improvement is statistically significant.", "labels": [], "entities": []}, {"text": "Actually, this significant improvement is reasonable, since the joint processing avoids error propagation and provides the opportunity of sharing information between different level knowledge sources.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7314864993095398}]}, {"text": "The superiority of this analyzer also shows that the integration of multi-level linguistic knowledge under the unified framework is effective, which may lead to improved LVCSR.", "labels": [], "entities": []}, {"text": "In the baseline speech recognition system, the acoustic models consisted of context-dependent Initial-Final models, in which the left-to-right model topology was used to represent each unit.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7141227722167969}]}, {"text": "According to the phonetic structures, the number of states in each model was set to 2 or 3 for initials, and 4 or 5 for tonal finals.", "labels": [], "entities": []}, {"text": "Each state was trained to have 32 Gaussian mixtures.", "labels": [], "entities": []}, {"text": "The used 39-dimension feature vector comprised 12 MFCC coefficients, energy, and their first-order and second-order deltas.", "labels": [], "entities": []}, {"text": "Since in this work we focused on modeling knowledge of language in Mandarin LVCSR, only clean male acoustic models were trained with a speech database that contained about 360 hours speech of over 750 male speakers.", "labels": [], "entities": []}, {"text": "This training data was picked up from three continuous Mandarin speech corpora: the 863-I, 863-II and Intel corpora.", "labels": [], "entities": []}, {"text": "The brief information about these three speech corpora was listed in.", "labels": [], "entities": []}, {"text": "As in this work, the evaluation data was the 1997 HUB-4 Mandarin broadcast news evaluation data (HUB-4 test set), to better fit this task, the acoustic models were adapted by the approach of maximum a posterior (MAP) adaptation.", "labels": [], "entities": [{"text": "HUB-4 Mandarin broadcast news evaluation data (HUB-4 test set", "start_pos": 50, "end_pos": 111, "type": "DATASET", "confidence": 0.937323659658432}, {"text": "maximum a posterior (MAP)", "start_pos": 191, "end_pos": 216, "type": "METRIC", "confidence": 0.7688831041256586}]}, {"text": "The adaption data was drawn from the HUB4 training set, excluding the HUB-4 develop-  ing set, where only the cleaned male speech data (data under condition f0 defined as) was used.", "labels": [], "entities": [{"text": "adaption", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.942984402179718}, {"text": "HUB4 training set", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9719515840212504}, {"text": "HUB-4 develop-  ing set", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.7430192470550537}]}, {"text": "The partition for the clean data was done with the acoustic segmentation software CMUseg 0.5 2 (), and finally 8.6 hours adaptation data was obtained.", "labels": [], "entities": [{"text": "acoustic segmentation", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.6917300671339035}, {"text": "CMUseg 0.5 2", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.9106619755427042}]}, {"text": "The language model was a word-based trigram built on 60,000 words entries and trained with a corpus about 1.5 billion characters.", "labels": [], "entities": []}, {"text": "The training set consisted of broadcast news data from the Xinhua News Agency released by LDC (Xinhua part of Chinese Gigaword), seven years data of People's Daily of China from 1995 to 2002 released by People's Daily Online 3 , and some other data from news websites, such as yahoo, sina and soon.", "labels": [], "entities": [{"text": "Chinese Gigaword)", "start_pos": 110, "end_pos": 127, "type": "DATASET", "confidence": 0.8721809983253479}, {"text": "People's Daily of China from 1995 to 2002 released by People's Daily Online 3", "start_pos": 149, "end_pos": 226, "type": "DATASET", "confidence": 0.7741952426731586}]}, {"text": "In addition, the analyzer incorporated in speech recognition was trained with a larger corpus from People's Daily of China, including the data in 1998 from January to June and the data in 2000 from January to November (annotated by the Institute of Computational Linguistics of Peking University).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8035108745098114}, {"text": "People's Daily of China", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.8950562357902527}]}, {"text": "The December data in 2000 was taken as the development set used to fix the composition weight \u03b1 in equation 2.", "labels": [], "entities": [{"text": "December data in 2000", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.8632265478372574}]}, {"text": "In our experiments, the clean male speech data from the Hub-4 test set was used, and 238 sentences were finally extracted for testing.", "labels": [], "entities": [{"text": "Hub-4 test set", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9765733083089193}]}, {"text": "The weight of the analyzer was empirically derived from the development set, including 649 clean male sentences from the devSet of HUB-4 Evaluation.", "labels": [], "entities": [{"text": "HUB-4 Evaluation", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.9003672897815704}]}, {"text": "The recognition results are shown in  achieved.", "labels": [], "entities": [{"text": "recognition", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7836427688598633}]}, {"text": "Furthermore, we ran the statistical significance test to detect the performance improvement, in which the approximate randomization approach) was modified to output the significance level, p-value, for the CER metric.", "labels": [], "entities": [{"text": "CER metric", "start_pos": 206, "end_pos": 216, "type": "DATASET", "confidence": 0.8618949949741364}]}, {"text": "The p-levels produced through two rounds of 1048576 shuffles are 0.0058 and 0.0057 respectively, both less than 0.01.", "labels": [], "entities": []}, {"text": "Thus the performance improvement imposed by the utilization of the analyzer is statistically significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The Toy dictionary", "labels": [], "entities": [{"text": "Toy dictionary", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.7998920977115631}]}, {"text": " Table 3: The information of the speech training data", "labels": [], "entities": []}, {"text": " Table 4: The Speech recognition results", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.9292376935482025}]}]}