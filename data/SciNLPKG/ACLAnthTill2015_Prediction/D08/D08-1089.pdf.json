{"title": [{"text": "A Simple and Effective Hierarchical Phrase Reordering Model", "labels": [], "entities": [{"text": "Phrase Reordering", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7594873309135437}]}], "abstractContent": [{"text": "While phrase-based statistical machine translation systems currently deliver state-of-the-art performance, they remain weak on word order changes.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 6, "end_pos": 50, "type": "TASK", "confidence": 0.6305879503488541}]}, {"text": "Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance re-orderings possible with syntax-based systems.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7456490099430084}]}, {"text": "In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency.", "labels": [], "entities": []}, {"text": "We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase.", "labels": [], "entities": []}, {"text": "We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).", "labels": [], "entities": [{"text": "BLEU point", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9752520024776459}, {"text": "MT05", "start_pos": 213, "end_pos": 217, "type": "DATASET", "confidence": 0.9205028414726257}, {"text": "MT08", "start_pos": 231, "end_pos": 235, "type": "DATASET", "confidence": 0.8891181945800781}, {"text": "MT05", "start_pos": 266, "end_pos": 270, "type": "DATASET", "confidence": 0.8662067651748657}]}], "introductionContent": [{"text": "Statistical phrase-based systems ( have consistently delivered state-of-the-art performance in recent machine translation evaluations, yet these systems remain weak at handling word order changes.", "labels": [], "entities": [{"text": "machine translation evaluations", "start_pos": 102, "end_pos": 133, "type": "TASK", "confidence": 0.8237349192301432}]}, {"text": "The reordering models used in the original phrase-based systems penalize phrase displacements proportionally to the amount of nonmonotonicity, with no consideration of the fact that some words are far more  likely to be displaced than others (e.g., in English-toJapanese translation, a verb should typically move to the end of the clause).", "labels": [], "entities": []}, {"text": "Recent efforts) have directly addressed this issue by introducing lexicalized reordering models into phrase-based systems, which condition reordering probabilities on the words of each phrase pair.", "labels": [], "entities": []}, {"text": "These models distinguish three orientations with respect to the previous phrase-monotone (M), swap (S), and discontinuous (D)-and as such are primarily designed to handle local re-orderings of neighboring phrases. is an example where such a model effectively swaps the prepositional phrase in Luxembourg with a verb phrase, and where the noun ministers remains in monotone order with respect to the previous phrase EU environment.", "labels": [], "entities": []}, {"text": "While these lexicalized re-ordering models have shown substantial improvements over unlexicalized phrase-based systems, these models only have a limited ability to capture sensible long distance reorderings, as can be seen in.", "labels": [], "entities": []}, {"text": "The phrase of the region should swap with the rest of the noun phrase, yet these previous approaches are unable to model this movement, and assume the orientation of this phrase is discontinuous.", "labels": [], "entities": []}, {"text": "Observe that, in a shortened version of the same sentence (without and progress), the phrase orientation would be different (S), even though the shortened version has essentially the same sentence structure.", "labels": [], "entities": []}, {"text": "Coming from the other direction, such observations about phrase reordering between different languages are precisely the kinds of facts that parsing approaches to machine translation are designed to handle and do successfully handle.", "labels": [], "entities": [{"text": "phrase reordering between different languages", "start_pos": 57, "end_pos": 102, "type": "TASK", "confidence": 0.8305903434753418}, {"text": "machine translation", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7142734378576279}]}, {"text": "In this paper, we introduce a novel orientation model for phrase-based systems that aims to better capture long distance dependencies, and that presents a solution to the problem illustrated in.", "labels": [], "entities": []}, {"text": "In this example, our reordering model effectively treats the adjacent phrases the development and and progress as one single phrase, and the displacement of of the region with respect to this phrase can be treated as a swap.", "labels": [], "entities": []}, {"text": "To be able identify that adjacent blocks (e.g., the development and and progress) can be merged into larger blocks, our model infers binary (non-linguistic) trees reminiscent of.", "labels": [], "entities": []}, {"text": "Crucially, our work distinguishes itself from previous hierarchical models in that it does not rely on any cubic-time parsing algorithms such as CKY (used in, e.g.,) or the Earley algorithm (used in).", "labels": [], "entities": []}, {"text": "Since our reordering model does not attempt to resolve natural language ambiguities, we can effectively rely on (linear-time) shiftreduce parsing, which is done jointly with left-toright phrase-based beam decoding and thus introduces no asymptotic change in running time.", "labels": [], "entities": []}, {"text": "As such, the hierarchical model presented in this paper maintains all the effectiveness and speed advantages of statistical phrase-based systems, while being able to capture some key linguistic phenomena (presented later in this paper) which have motivated the development of parsing-based approaches.", "labels": [], "entities": [{"text": "speed", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.96683269739151}]}, {"text": "We also illustrate this with results that are significantly better than previous approaches, in particular the lexical reordering models of Moses, a widely used phrase-based SMT system (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 174, "end_pos": 177, "type": "TASK", "confidence": 0.8202639222145081}]}, {"text": "This paper is organized as follows: the training of lexicalized re-ordering models is described in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we describe how to combine shift-reduce parsing with left-to-right beam search phrase-based decoding with the same asymptotic running time as the original phrase-based decoder.", "labels": [], "entities": []}, {"text": "We finally show in Section 6 that our approach yields results that are significantly better than previous approaches for two language pairs and different test sets.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Class distributions of the three orientation mod- els, estimated from 12M words of Chinese-English data  using the grow-diag alignment symmetrization heuristic  implemented in Moses, which is similar to the 'refined'  heuristic of (", "labels": [], "entities": []}, {"text": " Table 2: Monotone and swap probabilities for specific  phrases according to the three models (word, phrase, and  hierarchical). To ensure probabilities are representative,  we only selected phrase pairs that occur at least 100 times  in the training data.", "labels": [], "entities": []}, {"text": " Table 3: BLEU[%] scores (uncased) for Chinese-English  and the orientation categories {M, S, D}. Maximum dis- tortion is set to 6 words, which is the default in Moses.  The stars at the bottom of the tables indicate when a given  hierarchical model is significantly better than all local  models for a given development or test set (*: signifi- cance at the .05 level; **: significance at the .01 level).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991102814674377}, {"text": "Maximum dis- tortion", "start_pos": 98, "end_pos": 118, "type": "METRIC", "confidence": 0.8447824567556381}]}, {"text": " Table 4: BLEU[%] scores (uncased) for Chinese-English  and the orientation categories {M, S, D l , D r }. Since the  distinction between these four categories is not available  in Moses, hence we have no baseline results for this case.  Maximum distortion is set to 6 words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993708729743958}, {"text": "Maximum distortion", "start_pos": 238, "end_pos": 256, "type": "METRIC", "confidence": 0.736755758523941}]}, {"text": " Table 5: BLEU[%] scores (uncased) for Arabic-English  and the reordering categories {M, S, D}.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992501139640808}]}, {"text": " Table 6: BLEU[%] scores (uncased) for Arabic-English  and the reordering categories {M, S, D l , D r }.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992697834968567}]}]}