{"title": [{"text": "An Exploration of Document Impact on Graph-Based Multi-Document Summarization", "labels": [], "entities": [{"text": "Graph-Based Multi-Document Summarization", "start_pos": 37, "end_pos": 77, "type": "TASK", "confidence": 0.5734645028909048}]}], "abstractContent": [{"text": "The graph-based ranking algorithm has been recently exploited for multi-document sum-marization by making only use of the sentence to sentence relationships in the documents, under the assumption that all the sentences are indistinguishable.", "labels": [], "entities": []}, {"text": "However, given a document set to be summarized, different documents are usually not equally important , and moreover, different sentences in a specific document are usually differently important.", "labels": [], "entities": []}, {"text": "This paper aims to explore document impact on summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9910723567008972}]}, {"text": "We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process.", "labels": [], "entities": []}, {"text": "Various methods are employed to evaluate the two factors.", "labels": [], "entities": []}, {"text": "Experimental results on the DUC2001 and DUC2002 datasets demonstrate that the good effectiveness of the proposed model.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9733928442001343}, {"text": "DUC2002 datasets", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.9347464144229889}]}, {"text": "Moreover , the results show the robustness of the proposed model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-document summarization aims to produce a summary describing the main topic in a document set, without any prior knowledge.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7270618379116058}]}, {"text": "Multi-document summary can be used to facilitate users to quickly understand a document cluster.", "labels": [], "entities": []}, {"text": "For example, a number of news services (e.g. NewsInEssence ) have been developed to group news articles into news topics, and then produce a short summary for each news topic.", "labels": [], "entities": []}, {"text": "Users can easily understand the topic they have interest in by taking a look at the short summary, without looking into each individual article within the topic cluster.", "labels": [], "entities": []}, {"text": "1 http://lada.si.umich.edu:8080/clair/nie1/nie.cgi Automated multi-document summarization has drawn much attention in recent years.", "labels": [], "entities": [{"text": "Automated multi-document summarization", "start_pos": 51, "end_pos": 89, "type": "TASK", "confidence": 0.5468262135982513}]}, {"text": "In the communities of natural language processing and information retrieval, a series of workshops and conferences on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, COLING, and SIGIR have advanced the summarization techniques and produced a couple of experimental online systems.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6467895805835724}, {"text": "information retrieval", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7562772035598755}, {"text": "text summarization", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.6944979727268219}, {"text": "NTCIR", "start_pos": 153, "end_pos": 158, "type": "DATASET", "confidence": 0.8573998212814331}, {"text": "summarization", "start_pos": 233, "end_pos": 246, "type": "TASK", "confidence": 0.9785672426223755}]}, {"text": "A particular challenge for multi-document summarization is that a document set might contain diverse information, which is either related or unrelated to the main topic, and hence we need effective summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6405795812606812}, {"text": "summarization", "start_pos": 198, "end_pos": 211, "type": "TASK", "confidence": 0.9633824229240417}]}, {"text": "In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features.", "labels": [], "entities": []}, {"text": "Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the \"voting\" or \"recommendations\" between sentences in the documents ().", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.5857215225696564}]}, {"text": "The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences.", "labels": [], "entities": []}, {"text": "The sentences with large rank scores are chosen into the summary.", "labels": [], "entities": []}, {"text": "However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship.", "labels": [], "entities": []}, {"text": "Actually, given a document set, different documents are not equally important.", "labels": [], "entities": []}, {"text": "For example, the documents close to the main topics of the document set are usually more important than the documents faraway from the main topics of the document set.", "labels": [], "entities": []}, {"text": "This document-level information is deemed to have great impact on the sentence ranking process.", "labels": [], "entities": [{"text": "sentence ranking process", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8071183760960897}]}, {"text": "Moreover, the sentences in the same document cannot be treated uniformly, because some sentences in the document are more important than other sentences because of their different positions in the document or different distances to the document's centroid.", "labels": [], "entities": []}, {"text": "In brief, neither the document-level information nor the sentence-to-document relationship has been taken into account in the previous graph-based model.", "labels": [], "entities": []}, {"text": "In order to overcome the limitations of the previous graph-based model, this study proposes the document-based graph model to explore document impact on the graph-based summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process.", "labels": [], "entities": []}, {"text": "We develop various methods to evaluate the document-level information and the sentence-to-document relationship.", "labels": [], "entities": []}, {"text": "Experiments on the DUC2001 and DUC2002 datasets have been performed and the results demonstrate the good effectiveness of the proposed model, i.e., the incorporation of document impact can much improve the performance of the graph-based summarization.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.968309760093689}, {"text": "DUC2002 datasets", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9383643567562103}]}, {"text": "Moreover, the proposed model is robust with respect to most incorporation schemes.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first introduce the related work in Section 2.", "labels": [], "entities": []}, {"text": "The basic graph-based summarization model and the proposed document-based graph model are described in detail in Sections 3 and 4, respectively.", "labels": [], "entities": []}, {"text": "We show the experiments and results in Section 5 and finally we conclude this paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Generic multi-document summarization has been one of the fundamental tasks in DUC 2001 4 and DUC 2002 5 (i.e. task 2 in DUC 2001 and task 2 in DUC 2002), and we used the two tasks for evaluation.", "labels": [], "entities": [{"text": "Generic multi-document summarization", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.626332700252533}, {"text": "DUC 2001 4", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9415293137232462}, {"text": "DUC 2002 5", "start_pos": 93, "end_pos": 103, "type": "DATASET", "confidence": 0.8525985876719157}, {"text": "DUC 2001", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9184437394142151}]}, {"text": "DUC2001 provided 30 document sets and DUC 2002 provided 59 document sets (D088 is excluded from the original 60 document sets by NIST) and generic abstracts of each document set with lengths of approximately 100 words or less were required to be created.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9840054512023926}, {"text": "DUC 2002", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9456213116645813}, {"text": "NIST", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.929078996181488}]}, {"text": "The documents were news articles collected from TREC-9.", "labels": [], "entities": [{"text": "TREC-9", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.8881679773330688}]}, {"text": "The sentences in each article have been separated and the sentence information has been stored into files.", "labels": [], "entities": []}, {"text": "The summary of the two datasets are shown in Table 1.", "labels": [], "entities": []}, {"text": "We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e. ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.8761987686157227}, {"text": "DUC", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.8459091782569885}, {"text": "summarization evaluation", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.9225380718708038}]}, {"text": "It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary.", "labels": [], "entities": []}, {"text": "ROUGE-N was an n-gram recall measure computed as follows: where n stood for the length of the n-gram, and Count match (n-gram) was the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries.", "labels": [], "entities": [{"text": "ROUGE-N", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9751150012016296}, {"text": "recall measure", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.9684489965438843}, {"text": "Count match (n-gram)", "start_pos": 106, "end_pos": 126, "type": "METRIC", "confidence": 0.9367797255516053}]}, {"text": "Count(n-gram) was the number of n-grams in the reference summaries.", "labels": [], "entities": [{"text": "Count(n-gram)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.904727965593338}]}, {"text": "In the experiments, the combination weight \u03bb for the proposed summarization model is typically set to 0.5 without tuning, i.e. the two documents for two sentences have equal influence on the summarization process.", "labels": [], "entities": [{"text": "combination weight \u03bb", "start_pos": 24, "end_pos": 44, "type": "METRIC", "confidence": 0.8254051605860392}, {"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9727275967597961}, {"text": "summarization", "start_pos": 191, "end_pos": 204, "type": "TASK", "confidence": 0.9738933444023132}]}, {"text": "Note that after the saliency scores of sentences have been obtained, a greedy algorithm () is applied to remove redundancy and finally choose both informative and novel sentences into the summary.", "labels": [], "entities": []}, {"text": "The algorithm is actually a variant version of the MMR algorithm).", "labels": [], "entities": []}, {"text": "The proposed document-based graph model (denoted as DGM) with different settings is compared with the basic graph-based Model (denoted as GM), the top three performing systems and two baseline systems on DUC2001 and DUC2002, respectively.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 204, "end_pos": 211, "type": "DATASET", "confidence": 0.9751353859901428}, {"text": "DUC2002", "start_pos": 216, "end_pos": 223, "type": "DATASET", "confidence": 0.8738300204277039}]}, {"text": "The top three systems are the systems with highest ROUGE scores, chosen from the performing systems on each task respectively.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.9813170433044434}]}, {"text": "The lead baseline and coverage baseline are two baselines employed in the generic multi-document summarization tasks of DUC2001 and DUC2002.", "labels": [], "entities": [{"text": "coverage", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.971396803855896}, {"text": "DUC2001", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.9544757008552551}, {"text": "DUC2002", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.8866583108901978}]}, {"text": "The lead baseline takes the first sentences one by one in the last document in the collection, where documents are assumed to be ordered chronologically.", "labels": [], "entities": []}, {"text": "And the coverage baseline takes the first sentence one by one from the first document to the last document.", "labels": [], "entities": []}, {"text": "show the comparison results on DUC2001 and DUC2002, respectively.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9682884216308594}, {"text": "DUC2002", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9418192505836487}]}, {"text": "In, SystemN, SystemP and System T are the top three performing systems for DUC2001.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.9211925864219666}]}, {"text": "In, System19, System26, System28 are the top three performing systems for DUC2002.", "labels": [], "entities": [{"text": "DUC2002", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9104604125022888}]}, {"text": "The documentbased graph model is configured with different settings (i.e. \u03c0 1 -\u03c0 3 , \u03c9 1 -\u03c9 4 ).", "labels": [], "entities": []}, {"text": "For example, DGM(\u03c0 1 +\u03c9 1 ) refers to the DGM model with \u03c0 1 to evaluate the document importance and \u03c9 1 to evaluate the correlation between a sentence and its document.", "labels": [], "entities": []}, {"text": "Seen from the tables, the proposed documentbased graph model with different settings can outperform the basic graph-based model and other baselines over almost all three metrics on both DUC2001 and DUC2002 datasets.", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 186, "end_pos": 193, "type": "DATASET", "confidence": 0.9479328989982605}, {"text": "DUC2002 datasets", "start_pos": 198, "end_pos": 214, "type": "DATASET", "confidence": 0.828195720911026}]}, {"text": "The results demonstrate the good effectiveness of the proposed model, i.e. the incorporation of document impact does benefit the graph-based summarization model.", "labels": [], "entities": []}, {"text": "It is interesting that the three methods for computing document importance and the four methods for computing the sentence-document correlation are almost as effective as each other on the DUC2002 dataset.", "labels": [], "entities": [{"text": "DUC2002 dataset", "start_pos": 189, "end_pos": 204, "type": "DATASET", "confidence": 0.9838255047798157}]}, {"text": "However, \u03c0 1 does not perform as well as \u03c0 2 and \u03c0 3 , and \u03c9 1 and \u03c9 4 does not perform as well as \u03c9 2 and \u03c9 3 on the DUC2001 dataset.", "labels": [], "entities": [{"text": "DUC2001 dataset", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.989366352558136}]}], "tableCaptions": [{"text": " Table 1. Summary of datasets", "labels": [], "entities": []}, {"text": " Table 2. Comparison results on DUC2001", "labels": [], "entities": [{"text": "DUC2001", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9057638645172119}]}, {"text": " Table 3. Comparison results on DUC2002  ( * indicates that the improvement over the baseline GM  model is statistically significant at 95% confidence level)", "labels": [], "entities": [{"text": "DUC2002", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.8811027407646179}]}]}