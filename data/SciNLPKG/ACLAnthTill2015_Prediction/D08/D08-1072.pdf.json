{"title": [{"text": "Online Methods for Multi-Domain Learning and Adaptation", "labels": [], "entities": [{"text": "Multi-Domain Learning and Adaptation", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.6718215346336365}]}], "abstractContent": [{"text": "NLP tasks are often domain specific, yet systems can learn behaviors across multiple domains.", "labels": [], "entities": []}, {"text": "We develop anew multi-domain online learning framework based on parameter combination from multiple classifiers.", "labels": [], "entities": []}, {"text": "Our algorithms draw from multi-task learning and domain adaptation to adapt multiple source domain classifiers to anew target domain, learn across multiple similar domains, and learn across a large number of disparate domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7671427130699158}]}, {"text": "We evaluate our algorithms on two popular NLP domain adaptation tasks: sentiment classification and spam filtering.", "labels": [], "entities": [{"text": "NLP domain adaptation", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.6754561265309652}, {"text": "sentiment classification", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.9436178803443909}, {"text": "spam filtering", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.9161847531795502}]}], "introductionContent": [{"text": "Statistical classifiers routinely process millions of websites, emails, blogs and other text everyday.", "labels": [], "entities": []}, {"text": "Variability across different data sources means that training a single classifier obscures differences and separate classifiers ignore similarities.", "labels": [], "entities": []}, {"text": "Similarly, adding new domains to existing systems requires adapting existing classifiers.", "labels": [], "entities": []}, {"text": "We present new online algorithms for three multidomain learning scenarios: adapting existing classifiers to new domains, learning across multiple similar domains and scaling systems to many disparate domains.", "labels": [], "entities": []}, {"text": "Multi-domain learning combines characteristics of both multi-task learning and domain adaptation and drawing from both areas, we develop a multi-classifier parameter combination technique for confidence-weighted (CW) linear classifiers (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7585448920726776}]}, {"text": "We focus on online algorithms that scale to large amounts of data.", "labels": [], "entities": []}, {"text": "Next, we describe multi-domain learning and review the CW algorithm.", "labels": [], "entities": []}, {"text": "We then consider our three settings using multi-classifier parameter combination.", "labels": [], "entities": []}, {"text": "We conclude with related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation we selected two domain adaptation datasets: spam) and sentiment).", "labels": [], "entities": []}, {"text": "The spam data contains two tasks, one with three users (task A) and one with 15 (task B).", "labels": [], "entities": []}, {"text": "The goal is to classify an email (bag-ofwords) as either spam or ham (not-spam) and each user may have slightly different preferences and features.", "labels": [], "entities": []}, {"text": "We used 700 and 100 training messages for each user for task A and B respectively and 300 test emails for each user.", "labels": [], "entities": []}, {"text": "The sentiment data contains product reviews from Amazon for four product types: books, dvds, electronics and kitchen appliances and we extended this with three additional domains: apparel, music and videos.", "labels": [], "entities": []}, {"text": "We follow Blitzer et. al. for feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8165049254894257}]}, {"text": "We created different datasets by modifying the decision boundary using the ordinal rating of each instance (1-5 stars) and excluding boundary instances.", "labels": [], "entities": []}, {"text": "We use four versions of this data: \u2022 All -7 domains, one per product type \u2022 Books -3 domains of books with the binary decision boundary set to 2, 3 and 4 stars \u2022 DVDs -Same as Books but with DVD reviews \u2022 Books+DVDs -Combined Books and DVDs We combine the existing user-specific classifiers into a single new classifier fora new user.", "labels": [], "entities": []}, {"text": "Since nothing is known about the new user (their decision function), each source classifier maybe useful.", "labels": [], "entities": []}, {"text": "However, feature similarity -possibly measured using unlabeled data -could be used to weigh source domains.", "labels": [], "entities": []}, {"text": "Specifically, we combine the parameters of each classifier according to their confidence using the combination methods described above.", "labels": [], "entities": []}, {"text": "We evaluated the four combination strategies -L2 vs. KL, uniform vs. variance -on spam and sentiment data.", "labels": [], "entities": []}, {"text": "For each evaluation, a single domain was held out for testing while separate classifiers were trained on each source domain, i.e. no target training.", "labels": [], "entities": []}, {"text": "Source classifiers are then combined and the combined classifier is evaluated on the test data (400 instances) of the target domain.", "labels": [], "entities": []}, {"text": "Each classifier was trained for 5 iterations over the training data (to ensure convergence) and each experiment was repeated using 10-fold cross validation.", "labels": [], "entities": []}, {"text": "The CW parameter \u03c6 was tuned on a single randomized run for each experiment.", "labels": [], "entities": [{"text": "\u03c6", "start_pos": 17, "end_pos": 18, "type": "METRIC", "confidence": 0.8762738108634949}]}, {"text": "We include several baselines: training on target data to obtain an upper bound on performance (Target), training on all source domains together, a useful strategy if all source data is maintained (All Src), selecting (with omniscience) the best performing source classifier on target data (Best Src), and the expected real world performance of randomly selecting a source classifier (Avg Src).", "labels": [], "entities": []}, {"text": "While at least one source classifier achieved high performance on the target domain (Best Src), the correct source classifier cannot be selected without target data and selecting a random source classifier yields high error.", "labels": [], "entities": []}, {"text": "In contrast, a combined classifier almost always improved over the best source domain classifier (table 1).", "labels": [], "entities": []}, {"text": "That some of our results improve over the best training scenario is likely caused by increased training data from using multiple domains.", "labels": [], "entities": []}, {"text": "Increases overall available training data are very interesting and maybe due to a regularization effect of training separate models.", "labels": [], "entities": []}, {"text": "The L2 methods performed best and KL improved 7 out of 10 combinations.", "labels": [], "entities": [{"text": "KL", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.6329347491264343}]}, {"text": "Classifier parameter combination can clearly yield good classifiers without prior knowledge of the target domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test error for multi-source adaptation on sentiment and spam data. Combining classifiers improves over  selecting a single classifier a priori (Avg Src).", "labels": [], "entities": [{"text": "multi-source adaptation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6877783387899399}, {"text": "Avg Src)", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9007903337478638}]}, {"text": " Table 2: Online training error for learning across domains.", "labels": [], "entities": []}, {"text": " Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daum\u00e9 (2007).", "labels": [], "entities": []}]}