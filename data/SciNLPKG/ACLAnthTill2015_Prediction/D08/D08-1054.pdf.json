{"title": [{"text": "HTM: A Topic Model for Hypertexts", "labels": [], "entities": [{"text": "Hypertexts", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.5002278089523315}]}], "abstractContent": [{"text": "Previously topic models such as PLSI (Prob-abilistic Latent Semantic Indexing) and LDA (Latent Dirichlet Allocation) were developed for modeling the contents of plain texts.", "labels": [], "entities": [{"text": "LDA", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9342054128646851}]}, {"text": "Recently , topic models for processing hyper-texts such as web pages were also proposed.", "labels": [], "entities": []}, {"text": "The proposed hypertext models are generative models giving rise to both words and hyper-links.", "labels": [], "entities": []}, {"text": "This paper points out that to better represent the contents of hypertexts it is more essential to assume that the hyperlinks are fixed and to define the topic model as that of generating words only.", "labels": [], "entities": []}, {"text": "The paper then proposes anew topic model for hypertext processing, referred to as Hypertext Topic Model (HTM).", "labels": [], "entities": [{"text": "hypertext processing", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.8692536950111389}]}, {"text": "HTM defines the distribution of words in a document (i.e., the content of the document) as a mixture over latent topics in the document itself and latent topics in the documents which the document cites.", "labels": [], "entities": []}, {"text": "The topics are further characterized as distributions of words, as in the conventional topic models.", "labels": [], "entities": []}, {"text": "This paper further proposes a method for learning the HTM model.", "labels": [], "entities": []}, {"text": "Experimental results show that HTM outperforms the baselines on topic discovery and document classification in three datasets.", "labels": [], "entities": [{"text": "topic discovery", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.7478981614112854}, {"text": "document classification", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7054706811904907}]}], "introductionContent": [{"text": "Topic models are probabilistic and generative models representing contents of documents.", "labels": [], "entities": []}, {"text": "Examples of topic models include PLSI () and LDA (.", "labels": [], "entities": []}, {"text": "The key idea in topic modeling is to represent topics as distributions of words and define the distribution of words in document (i.e., the content of document) as a mixture over hidden topics.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.8207702338695526}]}, {"text": "Topic modeling technologies have been applied to natural language processing, text mining, and information retrieval, and their effectiveness have been verified.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6411149799823761}, {"text": "text mining", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.8547310829162598}, {"text": "information retrieval", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.8192330300807953}]}, {"text": "In this paper, we study the problem of topic modeling for hypertexts.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.7413712441921234}]}, {"text": "There is no doubt that this is an important research issue, given the fact that more and more documents are available as hypertexts currently (such as web pages).", "labels": [], "entities": []}, {"text": "Traditional work mainly focused on development of topic models for plain texts.", "labels": [], "entities": []}, {"text": "It is only recently several topic models for processing hypertexts were proposed, including Link-LDA and Link-PLSA-LDA (.", "labels": [], "entities": []}, {"text": "We point out that existing models for hypertexts may not be suitable for characterizing contents of hypertext documents.", "labels": [], "entities": [{"text": "characterizing contents of hypertext documents", "start_pos": 73, "end_pos": 119, "type": "TASK", "confidence": 0.8270018100738525}]}, {"text": "This is because all the models are assumed to generate both words and hyperlinks (outlinks) of documents.", "labels": [], "entities": []}, {"text": "The generation of the latter type of data, however, may not be necessary for the tasks related to contents of documents.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew topic model for hypertexts called HTM (Hypertext Topic Model), within the Bayesian learning approach (it is similar to LDA in that sense).", "labels": [], "entities": []}, {"text": "In HTM, the hyperlinks of hypertext documents are supposed to be given.", "labels": [], "entities": []}, {"text": "Each document is associated with one topic distribution.", "labels": [], "entities": []}, {"text": "The word distribution of a document is defined as a mixture of latent topics of the document itself and latent topics of documents which the document cites.", "labels": [], "entities": []}, {"text": "The topics are further defined as distributions of words.", "labels": [], "entities": []}, {"text": "That means the content (topic distributions for words) of a hypertext document is not only determined by the topics of itself but also the topics of documents it cites.", "labels": [], "entities": []}, {"text": "It is easy to see that HTM contains LDA as a special case.", "labels": [], "entities": []}, {"text": "Although the idea of HTM is simple and straightforward, it appears that this is the first work which studies the model.", "labels": [], "entities": [{"text": "HTM", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8499960899353027}]}, {"text": "We further provide methods for learning and inference of HTM.", "labels": [], "entities": []}, {"text": "Our experimental results on three web datasets show that HTM outperforms the baseline models of LDA, Link-LDA, and Link-PLSA-LDA, in the tasks of topic discovery and document classification.", "labels": [], "entities": [{"text": "topic discovery", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.790812611579895}, {"text": "document classification", "start_pos": 166, "end_pos": 189, "type": "TASK", "confidence": 0.7424474358558655}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the proposed HTM model and its learning and inference methods.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Conclusions are made in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared the performances of HTM and three baseline models: LDA, Link-LDA, and Link-PLSA-LDA in topic discovery and document classification.", "labels": [], "entities": [{"text": "topic discovery", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.7844049036502838}, {"text": "document classification", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.6792558431625366}]}, {"text": "Note that LDA does not consider the use of link information; we included it here for reference.", "labels": [], "entities": [{"text": "LDA", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9011594653129578}]}, {"text": "We made use of three datasets.", "labels": [], "entities": []}, {"text": "The documents in the datasets were processed by using the Lemur Took kit (http://www.lemurproject.org), and the low frequency words in the datasets were removed.", "labels": [], "entities": [{"text": "Lemur Took kit", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.879080335299174}]}, {"text": "The WebKB and Wikipedia are public datasets widely used in topic model studies.", "labels": [], "entities": [{"text": "WebKB", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.9667631983757019}, {"text": "Wikipedia", "start_pos": 14, "end_pos": 23, "type": "DATASET", "confidence": 0.905509352684021}]}, {"text": "ODP was collected by us in this work.", "labels": [], "entities": [{"text": "ODP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9492282271385193}]}], "tableCaptions": [{"text": " Table 2: Classification accuracies in 3-fold cross- validation.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.6926699280738831}]}, {"text": " Table 3: Sign-test results between HTM and the three  baseline models.", "labels": [], "entities": [{"text": "Sign-test", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9645597338676453}, {"text": "HTM", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9002187252044678}]}, {"text": " Table 4: Comparison of topics identified by the four mod- els for the example homepage. Only topics with proba- bilities > 0.1 and related to the subjects are shown.", "labels": [], "entities": []}, {"text": " Table 5: Word assignment in the example homepage.", "labels": [], "entities": [{"text": "Word assignment", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7290333062410355}]}, {"text": " Table 6: Most salient topics in cited pages.", "labels": [], "entities": []}]}