{"title": [{"text": "A Generative Model for Parsing Natural Language to Meaning Representations", "labels": [], "entities": [{"text": "Parsing Natural Language to Meaning Representations", "start_pos": 23, "end_pos": 74, "type": "TASK", "confidence": 0.8660376866658529}]}], "abstractContent": [{"text": "In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures.", "labels": [], "entities": [{"text": "generative model of natural language sentences", "start_pos": 54, "end_pos": 100, "type": "TASK", "confidence": 0.8364246090253195}]}, {"text": "The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning.", "labels": [], "entities": []}, {"text": "We introduce dynamic programming techniques for efficient training and decoding.", "labels": [], "entities": []}, {"text": "In experiments , we demonstrate that the model, when coupled with a discriminative reranking technique , achieves state-of-the-art performance when tested on two publicly available corpora.", "labels": [], "entities": []}, {"text": "The generative model degrades robustly when presented with instances that are different from those seen in training.", "labels": [], "entities": [{"text": "generative", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.964975118637085}]}, {"text": "This allows a notable improvement in recall compared to previous models.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9997343420982361}]}], "introductionContent": [{"text": "To enable computers to understand natural human language is one of the classic goals of research in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6608044505119324}]}, {"text": "Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning).", "labels": [], "entities": []}, {"text": "One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences.", "labels": [], "entities": []}, {"text": "This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions.", "labels": [], "entities": []}, {"text": "In practice, such a grammar may lack the rules required to correctly parse some of the new test examples.", "labels": [], "entities": []}, {"text": "In this paper, we develop an alternative approach that learns a model which does not make use of an explicit grammar but, instead, models the correspondence between sentences and their meanings with a generative process.", "labels": [], "entities": []}, {"text": "This model is defined over hybrid trees whose nodes include both natural language words and meaning representation tokens.", "labels": [], "entities": []}, {"text": "Inspired by the work of, the generative model builds trees by recursively creating nodes at each level according to a Markov process.", "labels": [], "entities": []}, {"text": "This implicit grammar representation leads to flexible learned models that generalize well.", "labels": [], "entities": []}, {"text": "In practice, we observe that it can correctly parse a wider range of test examples than previous approaches.", "labels": [], "entities": []}, {"text": "The generative model is learned from data that consists of sentences paired with their meaning representations.", "labels": [], "entities": []}, {"text": "However, there is no explicit labeling of the correspondence between words and meaning tokens that is necessary for building the hybrid trees.", "labels": [], "entities": []}, {"text": "This creates a challenging, hidden-variable learning problem that we address with the use of an insideoutside algorithm.", "labels": [], "entities": []}, {"text": "Specifically, we develop a dynamic programming parsing algorithm that leads to O(n 3 m) time complexity for inference, where n is the sentence length and m is the size of meaning structure.", "labels": [], "entities": [{"text": "dynamic programming parsing", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.642840564250946}, {"text": "O(n 3 m) time complexity", "start_pos": 79, "end_pos": 103, "type": "METRIC", "confidence": 0.7794422954320908}]}, {"text": "This approach allows for efficient training and decoding.", "labels": [], "entities": []}, {"text": "In practice, we observe that the learned generative models are able to assign a high score to the correct meaning for input sentences, but that this correct meaning is not always the highest scoring option.", "labels": [], "entities": []}, {"text": "To address this problem, we use a simple reranking approach to select a parse from a k-best list of parses.", "labels": [], "entities": []}, {"text": "This pipelined approach achieves state-ofthe-art performance on two publicly available corpora.", "labels": [], "entities": []}, {"text": "In particular, the flexible generative model leads to notable improvements in recall, the total percentage of sentences that are correctly parsed.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9994427561759949}]}], "datasetContent": [{"text": "Our evaluations were performed on two corpora, GEOQUERY and ROBOCUP.", "labels": [], "entities": [{"text": "GEOQUERY", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.4987148940563202}, {"text": "ROBOCUP", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9181729555130005}]}, {"text": "The GEOQUERY corpus contains MR defined by a Prolog-based language used in querying a database on U.S. geography.", "labels": [], "entities": [{"text": "GEOQUERY corpus", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9478068947792053}, {"text": "MR", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.8738430738449097}]}, {"text": "The ROBOCUP corpus contains MR defined by a coaching language used in a robot coaching competition.", "labels": [], "entities": [{"text": "ROBOCUP corpus", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.6871224492788315}]}, {"text": "There are in total 880 and 300 instances for the two corpora respectively.", "labels": [], "entities": []}, {"text": "Standard 10-fold cross validations were performed and the micro-averaged results are presented in this section.", "labels": [], "entities": []}, {"text": "To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of.", "labels": [], "entities": []}, {"text": "Following and other previous work, we report performance in terms of Precision (percentage of answered NL sentences that are correct), Recall (percentage of correctly answered NL sentences, out of all NL sentences) and F-score (harmonic mean of Precision and Recall).", "labels": [], "entities": [{"text": "Precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9474651217460632}, {"text": "Recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.997270405292511}, {"text": "F-score", "start_pos": 219, "end_pos": 226, "type": "METRIC", "confidence": 0.9985942244529724}]}, {"text": "Again following Wong, we define the correct output MR structure as follows.", "labels": [], "entities": []}, {"text": "For the GEO-QUERY corpus, an MR structure is considered correct if and only if it retrieves identical results as the reference MR structure when both are issued as queries to the underlying Prolog database.", "labels": [], "entities": [{"text": "GEO-QUERY corpus", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.9623973369598389}, {"text": "Prolog database", "start_pos": 190, "end_pos": 205, "type": "DATASET", "confidence": 0.9469894766807556}]}, {"text": "For the ROBOCUP corpus, an MR structure is considered correct if and only if it has the same string representation as the reference MR structure, up to reordering of children of MR productions whose function symbols are commutative, such as and, or, etc.", "labels": [], "entities": [{"text": "ROBOCUP corpus", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.7580829858779907}]}, {"text": "We evaluated the three models, with and without reranking.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "Comparing Model I and Model II, we noticed that for both corpora, Model I in general achieves better recall while Model II achieves better precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9991003274917603}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9980048537254333}]}, {"text": "This observation conforms to our earlier expectations.", "labels": [], "entities": []}, {"text": "Model III, as an interpolation of the above two models, achieves a much better F-measure on GEO-QUERY corpus.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9954699277877808}, {"text": "GEO-QUERY corpus", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9734006524085999}]}, {"text": "However, it is shown to be less effective on ROBOCUP corpus.", "labels": [], "entities": [{"text": "ROBOCUP corpus", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.7114161103963852}]}, {"text": "We noticed that compared to the GEOQUERY corpus, ROBOCUP corpus contains longer sentences, larger MR structures, and a significant amount of non-compositionality.", "labels": [], "entities": [{"text": "GEOQUERY corpus", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.9477526843547821}, {"text": "ROBOCUP corpus", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.709855705499649}]}, {"text": "These factors combine to present a challenging problem for parsing with the generative model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.9827377200126648}]}, {"text": "Interestingly, although Model III fails to produce better best predictions for this corpus, we found that its top-k list contains a relatively larger number of correct predictions than Model I or Model II.", "labels": [], "entities": []}, {"text": "This indicates the possibility of enhancing the performance with reranking.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance comparison over three models", "labels": [], "entities": []}, {"text": " Table 4: Performance comparison with other directly com- parable systems", "labels": [], "entities": []}, {"text": " Table 5: Performance on different natural languages for  GEOQUERY-250 corpus", "labels": [], "entities": [{"text": "GEOQUERY-250", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9155378341674805}]}]}