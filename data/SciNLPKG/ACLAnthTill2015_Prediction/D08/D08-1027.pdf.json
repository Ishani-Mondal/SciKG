{"title": [{"text": "Cheap and Fast -But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming.", "labels": [], "entities": [{"text": "Human linguistic annotation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6193422377109528}, {"text": "natural language processing tasks", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.7700890451669693}]}, {"text": "We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from abroad base of paid non-expert contributors over the Web.", "labels": [], "entities": []}, {"text": "We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.", "labels": [], "entities": [{"text": "affect recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7946323752403259}, {"text": "word similarity", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7664714455604553}, {"text": "recognizing textual entailment", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.8624854683876038}, {"text": "event temporal ordering", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.6212035318215688}, {"text": "word sense disambiguation", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.7342128753662109}]}, {"text": "For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert label-ers.", "labels": [], "entities": []}, {"text": "For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts.", "labels": [], "entities": [{"text": "affect recognition", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7995538115501404}]}, {"text": "We propose a technique for bias correction that significantly improves annotation quality on two tasks.", "labels": [], "entities": [{"text": "bias correction", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.6315217465162277}]}, {"text": "We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.", "labels": [], "entities": [{"text": "labeling tasks", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8988844156265259}]}], "introductionContent": [{"text": "Large scale annotation projects such as TreeBank (,), TimeBank (), FrameNet (),, and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8800404667854309}]}, {"text": "The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost.", "labels": [], "entities": []}, {"text": "Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them), one promising alternative for some tasks is the collection of non-expert annotations.", "labels": [], "entities": []}, {"text": "In this work we explore the use of Amazon Mechanical Turk 1 (AMT) to determine whether nonexpert labelers can provide reliable natural language annotations.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk 1 (AMT)", "start_pos": 35, "end_pos": 65, "type": "DATASET", "confidence": 0.7629258377211434}]}, {"text": "We chose five natural language understanding tasks that we felt would be sufficiently natural and learnable for non-experts, and for which we had gold standard labels from expert labelers, as well as (in some cases) expert labeler agreement information.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 14, "end_pos": 50, "type": "TASK", "confidence": 0.7586639821529388}]}, {"text": "The tasks are: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.", "labels": [], "entities": [{"text": "affect recognition", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8301402032375336}, {"text": "word similarity", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7768596410751343}, {"text": "recognizing textual entailment", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.8678348461786906}, {"text": "event temporal ordering", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.6291271845499674}, {"text": "word sense disambiguation", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.7289321223894755}]}, {"text": "For each task, we used AMT to annotate data and measured the quality of the annotations by comparing them with the gold standard (expert) labels on the same data.", "labels": [], "entities": []}, {"text": "Further, we compare machine learning classifiers trained on expert annotations vs. non-expert annotations.", "labels": [], "entities": []}, {"text": "In the next sections of the paper we introduce the five tasks and the evaluation metrics, and offer methodological insights, including a technique for bias correction that improves annotation quality.", "labels": [], "entities": []}, {"text": "1 http://mturk.com Please see http://blog.doloreslabs.com/?p=109 fora condensed version of this paper, follow-ups, and ongoing public discussion.", "labels": [], "entities": []}, {"text": "We encourage comments to be directed herein addition to email when appropriate.", "labels": [], "entities": []}, {"text": "Dolores Labs Blog, \"AMT is fast, cheap, and good for machine learning data,\" Brendan O'Connor, Sept. 9, 2008.", "labels": [], "entities": [{"text": "Dolores Labs Blog", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9560984174410502}]}, {"text": "More related work at http://blog.doloreslabs.com/topics/wisdom/.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the purpose of this experiment we create a simple bag-of-words unigram model for predicting affect and valence, similar to the SWAT system (, one of the top-performing systems on the SemEval Affective Text task.", "labels": [], "entities": [{"text": "predicting affect and valence", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.7697074711322784}, {"text": "SemEval Affective Text task", "start_pos": 187, "end_pos": 214, "type": "TASK", "confidence": 0.7883647680282593}]}, {"text": "For each token tin our training set, we assign ta weight for each emotion e equal to the average emotion score observed in each headline H that t participates in. i.e., if H t is the set of headlines containing the token t, then: Score(e, t) = H\u2208Ht Score(e, H) |H t | With these weights of the individual tokens we may then compute the score for an emotion e of anew headline H as the average score over the set of tokens t \u2208 H that we've observed in the training set (ignoring those tokens not in the training set), i.e.: Where |H| is simply the number of tokens in headline H, ignoring tokens not observed in the training set.", "labels": [], "entities": []}, {"text": "We use 100 headlines as a training set (examples 500-599 from the test set of SemEval Task 14), and we use the remaining 900 headlines as our test set.", "labels": [], "entities": [{"text": "SemEval Task 14", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.833786408106486}]}, {"text": "Since we are fortunate to have the six separate expert annotations in this task, we can perform an extended systematic comparison of the performance of the classifier trained with expert vs. non-expert data.: Performance of expert-trained and non-experttrained classifiers on test-set.", "labels": [], "entities": []}, {"text": "k is the minimum number of non-experts needed to beat an average expert.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average expert and non-expert ITA on test-set", "labels": [], "entities": []}, {"text": " Table 2: Average expert and averaged correlation over  10 non-experts on test-set. k is the minimum number of  non-experts needed to beat an average expert.", "labels": [], "entities": [{"text": "correlation", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.5404338240623474}]}, {"text": " Table 3: Summary of costs for non-expert labels", "labels": [], "entities": []}, {"text": " Table 4: Performance of expert-trained and non-expert- trained classifiers on test-set. k is the minimum number  of non-experts needed to beat an average expert.", "labels": [], "entities": []}]}