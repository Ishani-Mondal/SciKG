{"title": [], "abstractContent": [{"text": "We present an algorithmic framework for learning multiple related tasks.", "labels": [], "entities": []}, {"text": "Our framework exploits a form of prior knowledge that relates the output spaces of these tasks.", "labels": [], "entities": []}, {"text": "We present PAC learning results that analyze the conditions under which such learning is possible.", "labels": [], "entities": []}, {"text": "We present results on learning a shallow parser and named-entity recognition system that exploits our framework, showing consistent improvements over baseline methods.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7037186026573181}]}], "introductionContent": [{"text": "When two NLP systems are run on the same data, we expect certain constraints to hold between their outputs.", "labels": [], "entities": []}, {"text": "This is a form of prior knowledge.", "labels": [], "entities": []}, {"text": "We propose a self-training framework that uses such information to significantly boost the performance of one of the systems.", "labels": [], "entities": []}, {"text": "The key idea is to perform self-training only on outputs that obey the constraints.", "labels": [], "entities": []}, {"text": "Our motivating example in this paper is the task pair: named entity recognition (NER) and shallow parsing (aka syntactic chunking).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.7454745769500732}, {"text": "shallow parsing (aka syntactic chunking)", "start_pos": 90, "end_pos": 130, "type": "TASK", "confidence": 0.6756232466016497}]}, {"text": "Consider a hidden sentence with known POS and syntactic structure below.", "labels": [], "entities": [{"text": "POS", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9429370760917664}]}, {"text": "Further consider four potential NER sequences for this sentence.", "labels": [], "entities": []}, {"text": "Without ever seeing the actual sentence, can we guess which NER sequence is correct?", "labels": [], "entities": []}, {"text": "NER1 seems wrong because we feel like named entities should not be part of verb phrases.", "labels": [], "entities": [{"text": "NER1", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9372143149375916}]}, {"text": "NER2 seems wrong because there is an NNP 1 (proper noun) that is not part of a named entity (word 5).", "labels": [], "entities": [{"text": "NER2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9034414291381836}]}, {"text": "NER3 is amiss because we feel it is unlikely that a single name should span more than one NP (last two words).", "labels": [], "entities": [{"text": "NER3", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9632567167282104}]}, {"text": "NER4 has none of these problems and seems quite reasonable.", "labels": [], "entities": [{"text": "NER4", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9777247905731201}]}, {"text": "In fact, for the hidden sentence, NER4 is correct 2 . The remainder of this paper deals with the problem of formulating such prior knowledge into a workable system.", "labels": [], "entities": [{"text": "NER4", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8373677730560303}]}, {"text": "There are similarities between our proposed model and both self-training and cotraining; background is given in Section 2.", "labels": [], "entities": []}, {"text": "We present a formal model for our approach and perform a simple, yet informative, analysis.", "labels": [], "entities": []}, {"text": "This analysis allows us to define what good and bad constraints are.", "labels": [], "entities": []}, {"text": "Throughout, we use a running example of NER using hidden Markov models to show the efficacy of the method and the relationship between the theory and the implementation.", "labels": [], "entities": [{"text": "NER", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.921441912651062}]}, {"text": "Finally, we present full-blown results on seven different NER data sets (one from CoNLL, six from ACE), comparing our method to several competitive baselines (Section 4).", "labels": [], "entities": [{"text": "NER data sets", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.8449698090553284}, {"text": "CoNLL", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.9530937671661377}, {"text": "ACE", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8896520137786865}]}, {"text": "We see that for many of these data sets, less than one hundred labeled NER sentences are required to get state-of-the-art performance, using a discriminative sequence labeling algorithm).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our experimental results.", "labels": [], "entities": []}, {"text": "We have already discussed some of them in the context of the running example.", "labels": [], "entities": []}, {"text": "In Section 4.1, we briefly describe the data sets we use.", "labels": [], "entities": []}, {"text": "A full description of the HMM implementation and its results are in Section 4.2.", "labels": [], "entities": []}, {"text": "Finally, in Section 4.3, we present results based on a competitive, discriminativelylearned sequence labeling algorithm.", "labels": [], "entities": []}, {"text": "All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy.", "labels": [], "entities": [{"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9066146612167358}, {"text": "chunking", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.9536115527153015}, {"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9949830770492554}, {"text": "POS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7972390651702881}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9988228678703308}]}], "tableCaptions": [{"text": " Table 1: Comparison between hints, self-training and the  (best) baseline for varying amount of labeled data.", "labels": [], "entities": []}, {"text": " Table 2: Comparison between hints, self-training and the  (best) baseline for varying amount of unlabeled data.", "labels": [], "entities": []}, {"text": " Table 3: Results on two-sided learning with hints.", "labels": [], "entities": []}]}