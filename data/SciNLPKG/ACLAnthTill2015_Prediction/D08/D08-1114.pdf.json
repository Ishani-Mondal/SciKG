{"title": [], "abstractContent": [{"text": "We propose anew graph-based semi-supervised learning (SSL) algorithm and demonstrate its application to document categorization.", "labels": [], "entities": [{"text": "semi-supervised learning (SSL)", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.7251287937164307}]}, {"text": "Each document is represented by a vertex within a weighted undirected graph and our proposed framework minimizes the weighted Kullback-Leibler divergence between distributions that encode the class membership probabilities of each vertex.", "labels": [], "entities": []}, {"text": "The proposed objective is convex with guaranteed convergence using an alternating minimization procedure.", "labels": [], "entities": []}, {"text": "Further, it generalizes in a straightforward manner to multi-class problems.", "labels": [], "entities": []}, {"text": "We present results on two standard tasks, namely Reuters-21578 and WebKB, showing that the proposed algorithm significantly outperforms the state-of-the-art.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9618042707443237}, {"text": "WebKB", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.948127031326294}]}], "introductionContent": [{"text": "Semi-supervised learning (SSL) employs small amounts of labeled data with relatively large amounts of unlabeled data to train classifiers.", "labels": [], "entities": [{"text": "Semi-supervised learning (SSL)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7762206852436065}]}, {"text": "In many problems, such as speech recognition, document classification, and sentiment recognition, annotating training data is both time-consuming and tedious, while unlabeled data are easily obtained thus making these problems useful applications of SSL.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8400526344776154}, {"text": "document classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8007301390171051}, {"text": "sentiment recognition", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.9581057131290436}, {"text": "SSL", "start_pos": 250, "end_pos": 253, "type": "TASK", "confidence": 0.9542593955993652}]}, {"text": "Classic examples of SSL algorithms include self-training and co-training.", "labels": [], "entities": [{"text": "SSL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9736957550048828}]}, {"text": "Graphbased SSL algorithms are an important class of SSL techniques that have attracted much of attention of late;).", "labels": [], "entities": []}, {"text": "Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph.", "labels": [], "entities": []}, {"text": "In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices.", "labels": [], "entities": []}, {"text": "Most graph-based SSL algorithms fall under one of two categories -those that use the graph structure to spread labels from labeled to unlabeled samples;) and those that optimize a loss function based on smoothness constraints derived from the graph;).", "labels": [], "entities": [{"text": "SSL", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9209645390510559}]}, {"text": "Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (.", "labels": [], "entities": []}, {"text": "In general graph-based SSL algorithms are non-parametric and transductive.", "labels": [], "entities": [{"text": "SSL", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8965191841125488}]}, {"text": "1 A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training.", "labels": [], "entities": []}, {"text": "In practice, however, transductive learners can be modified to handle unseen data.", "labels": [], "entities": []}, {"text": "A common drawback of many graph-based SSL algorithms (e.g.) is that they assume binary classification tasks and thus require the use of sub-optimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees.", "labels": [], "entities": []}, {"text": "There are also issues related to degenerate solutions (all unlabeled samples classified as belonging to a single class)).", "labels": [], "entities": []}, {"text": "For more background on graph-based and general SSL and their applications, see.", "labels": [], "entities": [{"text": "SSL", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9529317617416382}]}, {"text": "In this paper we propose anew algorithm for graph-based SSL and use the task of text classification to demonstrate its benefits over the current stateof-the-art.", "labels": [], "entities": [{"text": "SSL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8502324819564819}, {"text": "text classification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7467963695526123}]}, {"text": "Text classification involves automatically assigning a given document to a fixed number of semantic categories.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8001487851142883}]}, {"text": "Each document may belong to one, many, or none of the categories.", "labels": [], "entities": []}, {"text": "In general, text classification is a multi-class problem (more than 2 categories).", "labels": [], "entities": [{"text": "text classification", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8182274699211121}]}, {"text": "Training fully-supervised text classifiers requires large amounts of labeled data whose annotation can be expensive (.", "labels": [], "entities": []}, {"text": "As a result there has been interest is using SSL techniques for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8661861419677734}]}, {"text": "However past work in semisupervised text classification has relied primarily on one vs. rest approaches to overcome the inherent multi-class nature of this problem.", "labels": [], "entities": [{"text": "semisupervised text classification", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.6013133823871613}]}, {"text": "We believe such an approach maybe sub-optimal because, disregarding data overlap, the different classifiers have training procedures that are independent of one other.", "labels": [], "entities": []}, {"text": "In order to address the above drawback we propose anew framework based on optimizing a loss function composed of Kullback-Leibler divergence (KL-divergence) terms between probability distributions defined for each graph vertex.", "labels": [], "entities": []}, {"text": "The use of probability distributions, rather than fixed integer labels, not only leads to a straightforward multi-class generalization, but also allows us to exploit other well-defined functions of distributions, such as entropy, to improve system performance and to allow for the measure of uncertainty.", "labels": [], "entities": []}, {"text": "For example, with a single integer, at most all we know is its assignment.", "labels": [], "entities": []}, {"text": "With a distribution, we can continuously move from knowing an assignment with certainty (i.e., an entropy of zero) to expressions of doubt or multiple valid possibilities (i.e., an entropy greater than zero).", "labels": [], "entities": []}, {"text": "This is particularly useful for document classification as we will see.", "labels": [], "entities": [{"text": "document classification", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7690088748931885}]}, {"text": "We also show how one can use the alternating minimization algorithm to optimize our objective leading to a relatively simple, fast, easy-to-implement, guaranteed to converge, iterative, and closed form update for each iteration.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: P/R Break Even Points (PRBEP) for the top  10 categories in the Reuters data set with l = 20 and  u = 3299. All results are averages over 20 randomly  generated transduction sets. The last row is the macro- average over all the categories. Note AM is the proposed  approach.", "labels": [], "entities": [{"text": "P/R Break Even Points (PRBEP)", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.7265157202879587}, {"text": "Reuters data set", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.987293004989624}, {"text": "AM", "start_pos": 255, "end_pos": 257, "type": "METRIC", "confidence": 0.868121325969696}]}]}