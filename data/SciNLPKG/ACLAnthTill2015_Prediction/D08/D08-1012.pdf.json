{"title": [{"text": "Coarse-to-Fine Syntactic Machine Translation using Language Projections", "labels": [], "entities": [{"text": "Coarse-to-Fine Syntactic Machine Translation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6812642812728882}]}], "abstractContent": [{"text": "The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding.", "labels": [], "entities": [{"text": "machine translation decoding", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.8240650296211243}]}, {"text": "We propose a multipass, coarse-to-fine approach in which the language model complexity is incremen-tally introduced.", "labels": [], "entities": []}, {"text": "In contrast to previous order-based bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language.", "labels": [], "entities": []}, {"text": "Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.9777109622955322}]}, {"text": "Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the absence of an n-gram language model, decoding asynchronous CFG translation model is very efficient, requiring only a variant of the CKY algorithm.", "labels": [], "entities": [{"text": "CFG translation", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7314476072788239}]}, {"text": "As in monolingual parsing, dynamic programming items are simply indexed by a source language span and a syntactic label.", "labels": [], "entities": []}, {"text": "Complexity arises when n-gram language model scoring is added, because items must now be distinguished by their initial and final few target language words for purposes of later combination.", "labels": [], "entities": []}, {"text": "This lexically exploded search space is a root cause of inefficiency in decoding, and several methods have been suggested to combat it.", "labels": [], "entities": []}, {"text": "The approach most relevant to the current work is, which begins with an initial bigram pass and uses the resulting chart to guide a final trigram pass.", "labels": [], "entities": []}, {"text": "Substantial speed-ups are obtained, but computation is still dominated by the initial bigram pass.", "labels": [], "entities": []}, {"text": "The key challenge is that unigram models are too poor to prune well, but bigram models are already huge.", "labels": [], "entities": []}, {"text": "In short, the problem is that there are too many words in the target language.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew, coarse-to-fine, multipass approach which allows much greater speedups by translating into abstracted languages.", "labels": [], "entities": []}, {"text": "That is, rather than beginning with a low-order model of a still-large language, we exploit language projections, hierarchical clusterings of the target language, to effectively reduce the size of the target language.", "labels": [], "entities": []}, {"text": "In this way, initial passes can be very quick, with complexity phased in gradually.", "labels": [], "entities": []}, {"text": "Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see).", "labels": [], "entities": [{"text": "coarse-to-fine language projection", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.7134523193041483}, {"text": "construction of sequences of word clusterings", "start_pos": 53, "end_pos": 98, "type": "TASK", "confidence": 0.5717308322588602}]}, {"text": "The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one.", "labels": [], "entities": []}, {"text": "There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass.", "labels": [], "entities": []}, {"text": "We demonstrate that likelihood-based hierarchical EM training () and cluster-based language modeling methods) are superior to both rank-based and random-projection methods.", "labels": [], "entities": []}, {"text": "In addition, we demonstrate that more than two passes are beneficial and show that our computation is equally distributed overall passes.", "labels": [], "entities": []}, {"text": "In our experiments, passes with less than 16-cluster language models are most advantageous, and even a single pass with just two word clusters can reduce decoding time greatly.", "labels": [], "entities": []}, {"text": "To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus, described in detail in Section 3, and using a trigram language model.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG) translation", "start_pos": 114, "end_pos": 162, "type": "TASK", "confidence": 0.6229813396930695}, {"text": "Europarl corpus", "start_pos": 195, "end_pos": 210, "type": "DATASET", "confidence": 0.9926791191101074}]}, {"text": "We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to single pass decoding.", "labels": [], "entities": []}, {"text": "In addition, coarse-to-fine decoding increases BLEU scores by up to 0.4 points.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9540652334690094}]}, {"text": "This increase is a mixture of improved search and subtly advantageous coarseto-fine effects which are further discussed below.", "labels": [], "entities": [{"text": "search", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9311429262161255}, {"text": "coarseto-fine", "start_pos": 70, "end_pos": 83, "type": "METRIC", "confidence": 0.9317410588264465}]}], "datasetContent": [{"text": "We ran our experiments on the Europarl corpus () and show results on Spanish, French and German to English translation.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9879324734210968}]}, {"text": "We used the setup and preprocessing steps detailed in the 2008 Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.7163844505945841}]}, {"text": "Our baseline decoder uses an ITG with an integrated trigram language model.", "labels": [], "entities": []}, {"text": "Phrase translation parameters are learned from parallel corpora with approximately 8.5 million words for each of the language pairs.", "labels": [], "entities": [{"text": "Phrase translation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9040803611278534}]}, {"text": "The English language model is trained on the entire corpus of English parliamentary proceedings provided with the Europarl distribution.", "labels": [], "entities": [{"text": "Europarl distribution", "start_pos": 114, "end_pos": 135, "type": "DATASET", "confidence": 0.9934360086917877}]}, {"text": "We report results on the 2000 development test set sentences of length up to 126 words (average length was 30 words).", "labels": [], "entities": []}, {"text": "Our ITG translation model is broadly competitive with state-of-the-art phrase-based-models trained on the same data.", "labels": [], "entities": [{"text": "ITG translation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7044692039489746}]}, {"text": "For example, on the Europarl development test set, we fall short of Moses (  The multipass coarse-to-fine architecture that we have introduced presents many choice points.", "labels": [], "entities": [{"text": "Europarl development test set", "start_pos": 20, "end_pos": 49, "type": "DATASET", "confidence": 0.9842921644449234}]}, {"text": "In the following, we investigate various axes individually.", "labels": [], "entities": []}, {"text": "We present our findings as BLEU-to-time plots, where the tradeoffs were generated by varying the complexity and the number of coarse passes, as well as the pruning thresholds and beam sizes.", "labels": [], "entities": [{"text": "BLEU-to-time", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.997218132019043}]}, {"text": "Unless otherwise noted, the experiments are on SpanishEnglish using trigram language models.", "labels": [], "entities": [{"text": "SpanishEnglish", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9270213842391968}]}, {"text": "When different decoder settings are applied to the same model, MERT weights from the unprojected single pass setup are used and are kept constant across runs.", "labels": [], "entities": [{"text": "MERT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.8817030191421509}]}, {"text": "In particular, the same MERT weights are used for all coarse passes; note that this slightly disadvantages the multipass runs, which use MERT weights optimized for the single pass decoder.", "labels": [], "entities": [{"text": "MERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9675896167755127}]}], "tableCaptions": []}