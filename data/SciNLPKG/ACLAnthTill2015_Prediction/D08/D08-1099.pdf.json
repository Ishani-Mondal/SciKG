{"title": [{"text": "Automatic Set Expansion for List Question Answering", "labels": [], "entities": [{"text": "List Question Answering", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6863576571146647}]}], "abstractContent": [{"text": "This paper explores the use of set expansion (SE) to improve question answering (QA) when the expected answer is a list of entities belonging to a certain class.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.8224372506141663}]}, {"text": "Given a small set of seeds, SE algorithms mine textual resources to produce an extended list including additional members of the class represented by the seeds.", "labels": [], "entities": [{"text": "SE", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9643999338150024}]}, {"text": "We explore the hypothesis that a noise-resistant SE algorithm can be used to extend candidate answers produced by a QA system and generate anew list of answers that is better than the original list produced by the QA system.", "labels": [], "entities": []}, {"text": "We further introduce a hybrid approach which combines the original answers from the QA system with the output from the SE algorithm.", "labels": [], "entities": [{"text": "SE", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.9467044472694397}]}, {"text": "Experimental results for several state-of-the-art QA systems show that the hybrid system performs better than the QA systems alone when tested on list question data from past TREC evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) systems are designed to retrieve precise answers to questions posed in natural language.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.911475682258606}]}, {"text": "A list question expects a list as its answer, e.g. Name the coffee-producing countries in South America.", "labels": [], "entities": []}, {"text": "The ability to answer list questions has been tested as part of the yearly TREC QA evaluation (.", "labels": [], "entities": [{"text": "TREC QA evaluation", "start_pos": 75, "end_pos": 93, "type": "DATASET", "confidence": 0.6700088580449423}]}, {"text": "This paper focuses on the use of set expansion to improve list question answering.", "labels": [], "entities": [{"text": "list question answering", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.719984620809555}]}, {"text": "A set expansion (SE) algorithm receives as input a few members of a class or set, and mines various textual resources (e.g. web pages) to produce an extended list including additional members of the class or set that are not in the input.", "labels": [], "entities": [{"text": "set expansion (SE)", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.8021944046020508}]}, {"text": "A well-known online SE system is Google Sets 1 . This system is publicly accessible, but since it is a proprietary system that might be changed at anytime, its results cannot be replicated reliably.", "labels": [], "entities": [{"text": "Google Sets 1", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8778833150863647}]}, {"text": "We explore the hypothesis that a SE algorithm, when carefully designed to handle noisy inputs, can be applied to the output from a QA system to produce an overall list of answers fora given question that is better than the answers produced by the QA system itself.", "labels": [], "entities": [{"text": "SE", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9120052456855774}]}, {"text": "We propose to exploit large, redundant sources of structured and/or semi-structured data and use linguistic analysis to seed a shallow analysis of these sources.", "labels": [], "entities": []}, {"text": "This is a hard problem since the linguistic evidence used as seeds is noisy.", "labels": [], "entities": []}, {"text": "More precisely, we combine the QA system Ephyra () with the SE system SEAL ( to create a hybrid approach that performs better than either system by itself when tested on data from the TREC 13-15 evaluations.", "labels": [], "entities": [{"text": "SE system SEAL", "start_pos": 60, "end_pos": 74, "type": "METRIC", "confidence": 0.8173249165217081}, {"text": "TREC 13-15 evaluations", "start_pos": 184, "end_pos": 206, "type": "DATASET", "confidence": 0.8899295528729757}]}, {"text": "In addition, we apply our SE algorithm to answers generated by the five QA systems that performed the best on the list questions in the TREC 15 evaluation and report improvements in F 1 scores for four of these systems.", "labels": [], "entities": [{"text": "SE", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.9957744479179382}, {"text": "TREC 15 evaluation", "start_pos": 136, "end_pos": 154, "type": "DATASET", "confidence": 0.8372940619786581}, {"text": "F 1 scores", "start_pos": 182, "end_pos": 192, "type": "METRIC", "confidence": 0.9887900948524475}]}, {"text": "Section 2 of the paper gives an overview of the QA and SE systems used for our experiments.", "labels": [], "entities": [{"text": "SE", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9823883175849915}]}, {"text": "Section 3 describes how the SE system was adapted to deal with noisy seeds produced by QA systems, and Section 4 presents the details of the experimental design.", "labels": [], "entities": []}, {"text": "Experimental results are discussed in Section 5, and the paper concludes in Section 6 with a discussion of planned future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments in two phases.", "labels": [], "entities": []}, {"text": "In the first phase, we evaluated the SE approach by applying SEAL to answers generated by Ephyra.", "labels": [], "entities": [{"text": "SE", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.626162052154541}, {"text": "SEAL", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9842877388000488}]}, {"text": "In the second phase, we evaluated the approach by applying SEAL to the output from QA systems that performed the best on the list questions in the TREC 15 evaluation.", "labels": [], "entities": [{"text": "SEAL", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9946957230567932}, {"text": "TREC 15 evaluation", "start_pos": 147, "end_pos": 165, "type": "DATASET", "confidence": 0.8251613974571228}]}, {"text": "In both phases, the answers found by SEAL were retrieved from the Web instead of the AQUAINT newswire corpus used in the TREC evaluations.", "labels": [], "entities": [{"text": "SEAL", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.8053237199783325}, {"text": "AQUAINT newswire corpus", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.8266740441322327}]}, {"text": "However, we rejected answers if they could only be found in the Web and not in the AQUAINT corpus to avoid an unfair advantage over the QA systems: TREC participants were allowed to extract candidates from the Web (or any other source), but they had to identify a supporting document in the AQUAINT corpus for each answer and thus could not return answers that were not covered by the corpus.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.8967290222644806}, {"text": "AQUAINT corpus", "start_pos": 291, "end_pos": 305, "type": "DATASET", "confidence": 0.8826844692230225}]}, {"text": "Preliminary experiments showed that we can obtain a good balance between the amount and quality of the documents fetched by using only rare question terms as hint words.", "labels": [], "entities": []}, {"text": "In particular, we select the three question words that occur least frequently in a sample of the AQUAINT corpus as hints.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9182904362678528}]}, {"text": "The candidate answers were evaluated by using the answer keys, composed of regular expression patterns, obtained from the TREC website.", "labels": [], "entities": [{"text": "TREC website", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.8669466078281403}]}, {"text": "We did not extend the patterns with additional correct answers found in our experiments.", "labels": [], "entities": []}, {"text": "These answer keys were not officially used in the TREC evaluation; thus the baseline scores we computed for Ephyra and other QA systems in our experiments are slightly different from those officially reported.", "labels": [], "entities": [{"text": "TREC", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.7157816290855408}]}], "tableCaptions": [{"text": " Table 3: Mean average precision of Ephyra, its top four answers, and various SEAL configurations, where LE is  Lenient Extractor, AF is Aggressive Fetcher, and HE is Hinted Expander.", "labels": [], "entities": [{"text": "Mean average precision", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8311495979626974}, {"text": "LE", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9493918418884277}, {"text": "HE", "start_pos": 161, "end_pos": 163, "type": "METRIC", "confidence": 0.9888562560081482}]}, {"text": " Table 4: Average F 1 of Ephyra, its top four answers, and various SEAL configurations when using an optimal threshold  for each question.", "labels": [], "entities": [{"text": "Average F 1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8243162830670675}, {"text": "Ephyra", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.8599895238876343}]}, {"text": " Table 5: Average F 1 of Ephyra, the best-configured SEAL, and the hybrid system, along with thresholds trained by  5-fold cross validation.", "labels": [], "entities": [{"text": "Average F 1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8330726226170858}, {"text": "Ephyra", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9069527983665466}]}, {"text": " Table 6: Average F 1 of the QA systems, their top four answers, Google Sets, the best-configured SEAL, the hybrid  system, and their relative improvements over the QA systems.", "labels": [], "entities": [{"text": "Average F 1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8499082128206888}]}]}