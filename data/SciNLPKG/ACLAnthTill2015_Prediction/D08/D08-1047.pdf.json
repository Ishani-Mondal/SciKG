{"title": [{"text": "A Discriminative Candidate Generator for String Transformations", "labels": [], "entities": [{"text": "String Transformations", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7950863242149353}]}], "abstractContent": [{"text": "String transformation, which maps a source string s into its desirable form t * , is related to various applications including stemming, lemmatization, and spelling correction.", "labels": [], "entities": [{"text": "String transformation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7678303718566895}, {"text": "spelling correction", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.9324200451374054}]}, {"text": "The essential and important step for string transformation is to generate candidates to which the given string sis likely to be transformed.", "labels": [], "entities": [{"text": "string transformation", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7419895827770233}]}, {"text": "This paper presents a discriminative approach for generating candidate strings.", "labels": [], "entities": []}, {"text": "We use sub-string substitution rules as features and score them using an L 1-regularized logistic regression model.", "labels": [], "entities": []}, {"text": "We also propose a procedure to generate negative instances that affect the decision boundary of the model.", "labels": [], "entities": []}, {"text": "The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model.", "labels": [], "entities": []}, {"text": "We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations.", "labels": [], "entities": [{"text": "normalizing inflected words and spelling variations", "start_pos": 68, "end_pos": 119, "type": "TASK", "confidence": 0.8185669183731079}]}], "introductionContent": [{"text": "String transformation maps a source string s into its destination string t * . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing.", "labels": [], "entities": [{"text": "String transformation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7606557607650757}, {"text": "string transformation", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7373705953359604}, {"text": "partof-speech tagging", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.8131571710109711}, {"text": "shallow parsing", "start_pos": 182, "end_pos": 197, "type": "TASK", "confidence": 0.6681029796600342}]}, {"text": "However, this study addresses string transformation in its narrow sense, in which apart of a source string is rewritten with a substring.", "labels": [], "entities": [{"text": "string transformation", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.729312390089035}]}, {"text": "Typical applications of this task include stemming, lemmatization, spelling correction, OCR error correction (), approximate string matching, and duplicate record detection (.", "labels": [], "entities": [{"text": "stemming", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.9756465554237366}, {"text": "spelling correction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.921943336725235}, {"text": "OCR error correction", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.6657189627488455}, {"text": "approximate string matching", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.740268329779307}, {"text": "duplicate record detection", "start_pos": 146, "end_pos": 172, "type": "TASK", "confidence": 0.6205923358599345}]}, {"text": "Recent studies have formalized the task in the discriminative framework, t\u2208gen(s) P (t|s).", "labels": [], "entities": []}, {"text": "(1) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s.", "labels": [], "entities": []}, {"text": "The scorer was modeled by a noisy-channel model) and maximum entropy framework ().", "labels": [], "entities": []}, {"text": "The candidate generator gen(s) also affects the accuracy of the string transformation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993298053741455}]}, {"text": "Previous studies of spelling correction mostly defined gen(s), gen(s) = {t | dist(s, t) < \u03b4}.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.9451943635940552}]}, {"text": "Here, the function dist(s, t) denotes the weighted Levenshtein distance between strings sand t.", "labels": [], "entities": [{"text": "dist", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9541305303573608}]}, {"text": "Furthermore, the threshold \u03b4 requires the distance between the source string sand a candidate string t to be less than \u03b4.", "labels": [], "entities": []}, {"text": "The choice of dist(s, t) and \u03b4 involves a tradeoff between the precision, recall, and training/tagging speed of the scorer.", "labels": [], "entities": [{"text": "dist", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9775652885437012}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9996869564056396}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9983382225036621}]}, {"text": "A less restrictive design of these factors broadens the search space, but it also increases the number of confusing candidates, amount of feature space, and computational cost for the scorer.", "labels": [], "entities": []}, {"text": "Moreover, the choice is highly dependent on the target task.", "labels": [], "entities": []}, {"text": "It might be sufficient fora spelling correction program to gather candidates from known words, but a stemmer must handle unseen words appropriately.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9400357306003571}]}, {"text": "The number of candidates can be huge when we consider transformations from and to unseen strings.", "labels": [], "entities": []}, {"text": "This paper addresses these challenges by exploring the discriminative training of candidate generators.", "labels": [], "entities": []}, {"text": "More specifically, we build a binary classifier that, when given a source string s, decides whether a candidate t should be included in the candidate set or not.", "labels": [], "entities": []}, {"text": "This approach appears straightforward, but it must resolve two practical issues.", "labels": [], "entities": []}, {"text": "First, the task of the classifier is not only to make a binary decision for the two strings sand t, but also to enumerate a set of positive strings for the string s, In other words, an efficient algorithm is necessary to find a set of strings with which the classifier predict(s, t) yields positive labels for the string s.", "labels": [], "entities": []}, {"text": "Another issue arises when we prepare a training set.", "labels": [], "entities": []}, {"text": "A discriminative model requires a training set in which each instance (pair of strings) is annotated with a positive or negative label.", "labels": [], "entities": []}, {"text": "Even though some existing resources (e.g., inflection table and query log) are available for positive instances, such resources rarely contain negative instances.", "labels": [], "entities": []}, {"text": "Therefore, we must generate negative instances that are effective for discriminative training.", "labels": [], "entities": []}, {"text": "To address the first issue, we design features that express transformations from a source string s to its destination string t.", "labels": [], "entities": []}, {"text": "Feature selection and weighting are performed using an L 1 -regularized logistic regression model, which can find a sparse solution to the classification model.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7803756892681122}]}, {"text": "We also present an algorithm that utilizes the feature weights to enumerate candidates of destination strings efficiently.", "labels": [], "entities": []}, {"text": "We deal with the second issue by generating negative instances from unlabeled instances.", "labels": [], "entities": []}, {"text": "We describe a procedure to choose negative instances that affect the decision boundary of the classifier.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 formalizes the task of the candidate generator as a binary classification modeled by logistic regression.", "labels": [], "entities": []}, {"text": "Features for the classifier are designed using the rules of substring substitution.", "labels": [], "entities": []}, {"text": "Therefore, we can obtain, efficiently, candidates of destination strings and negative instances for training.", "labels": [], "entities": []}, {"text": "Section 3 reports the remarkable performance of the proposed method in various applications including lemmatization, spelling normalization, and noun derivation.", "labels": [], "entities": [{"text": "spelling normalization", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.9406587779521942}, {"text": "noun derivation", "start_pos": 145, "end_pos": 160, "type": "TASK", "confidence": 0.9311098158359528}]}, {"text": "We briefly review previous work in Section 4, and conclude this paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the candidate generator using three different tasks: normalization of orthographic variants, noun derivation, and lemmatization.", "labels": [], "entities": [{"text": "noun derivation", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.8131346702575684}]}, {"text": "The datasets for these tasks were obtained from the UMLS SPECIALIST Lexicon 2 , a large lexicon that includes both commonly occurring English words and biomedical vocabulary.", "labels": [], "entities": [{"text": "UMLS SPECIALIST Lexicon 2", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.8696125745773315}]}, {"text": "displays the list of tables in the SPECIALIST Lexicon that were used in our experiments.", "labels": [], "entities": [{"text": "SPECIALIST Lexicon", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.7352969497442245}]}, {"text": "We prepared three datasets, Orthography, Derivation, and Inflection.", "labels": [], "entities": [{"text": "Orthography", "start_pos": 28, "end_pos": 39, "type": "METRIC", "confidence": 0.7080909013748169}]}, {"text": "The Orthography dataset includes spelling variants (e.g., color and colour) in the LRSPL table.", "labels": [], "entities": [{"text": "Orthography dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.854706346988678}, {"text": "LRSPL table", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.9169007539749146}]}, {"text": "We chose entries as positive instances in which spelling variants are caused by (case-insensitive) alphanumeric changes . The Derivation dataset was built directly from the LRNOM table, which includes noun derivations such as abandon \u2192 abandonment.", "labels": [], "entities": [{"text": "LRNOM table", "start_pos": 173, "end_pos": 184, "type": "DATASET", "confidence": 0.9282627105712891}]}, {"text": "The LRAGR table includes base forms and their inflectional variants of nouns (singular and plural forms), verbs (infinitive, third singular, past, past participle forms, etc), and adjectives/adverbs (positive, comparative, and superlative forms).", "labels": [], "entities": [{"text": "LRAGR table includes base forms and their inflectional variants of nouns (singular and plural forms), verbs (infinitive, third singular, past, past participle forms, etc), and adjectives/adverbs (positive, comparative, and superlative", "start_pos": 4, "end_pos": 238, "type": "Description", "confidence": 0.824546421898736}]}, {"text": "For the Inflection dataset, we extracted the entries in which inflectional forms differ from their base forms 4 , e.g., study \u2192 studies.", "labels": [], "entities": [{"text": "Inflection dataset", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.8689844310283661}]}, {"text": "For each dataset, we applied the algorithm described in Section 2.5 to generate substitution rules and negative instances.", "labels": [], "entities": []}, {"text": "shows the number of positive instances (# +), negative instances (# -), and substitution rules (# Rules).", "labels": [], "entities": []}, {"text": "We evaluated the performance of the proposed method in two different goals of the tasks: classification (Section 3.2) and normalization (Section 3.3).", "labels": [], "entities": [{"text": "normalization", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9369428753852844}]}, {"text": "In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels.", "labels": [], "entities": []}, {"text": "We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset . Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE), Longest Common Substring Ratio (LCSR), Longest Common Prefix Ratio (PREFIX)), Porter's stemmer,), and CST's lemmatiser (Dalianis and Jonge-3 LRSPL table includes trivial spelling variants that can be handled using simple character/string operations.", "labels": [], "entities": [{"text": "Levenshtein distance (LD)", "start_pos": 156, "end_pos": 181, "type": "METRIC", "confidence": 0.8657687187194825}, {"text": "normalized Levenshtein distance (NLD)", "start_pos": 183, "end_pos": 220, "type": "METRIC", "confidence": 0.7727499306201935}, {"text": "CST", "start_pos": 367, "end_pos": 370, "type": "DATASET", "confidence": 0.902515172958374}, {"text": "Dalianis and Jonge-3 LRSPL table", "start_pos": 385, "end_pos": 417, "type": "DATASET", "confidence": 0.8200098037719726}]}, {"text": "For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g.,.", "labels": [], "entities": []}, {"text": "LRAGR table also provides agreement information even when word forms do not change.", "labels": [], "entities": [{"text": "LRAGR", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5531460642814636}]}, {"text": "For example, the table contains an entry indicating that the first-singular present form of the verb study is study, which might be readily apparent to English speakers.", "labels": [], "entities": []}, {"text": "We determined the regularization parameter \u03c3 = 5 experimentally.", "labels": [], "entities": []}, {"text": "Refer to for the performance change.", "labels": [], "entities": []}, {"text": "jan, 2006) . The five systems LD, NLD, DICE, LCSR, and PREFIX employ corresponding metrics of string distance or similarity.", "labels": [], "entities": []}, {"text": "Each system assigns a positive label to a given pair of strings s, t if the distance/similarity of strings sand t is smaller/larger than the threshold \u03b4 (refer to equation 2 for distance metrics).", "labels": [], "entities": []}, {"text": "The threshold of each system was chosen so that the system achieves the best F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9856578409671783}]}, {"text": "The remaining three systems assign a positive label only if the system transforms the strings sand t into the identical string.", "labels": [], "entities": []}, {"text": "For example, a pair of two words studies and study is classified as positive by Porter's stemmer, which yields the identical stem studi for these words.", "labels": [], "entities": []}, {"text": "We trained CST's lemmatiser for each dataset to obtain flex patterns that are used for normalizing word inflections.", "labels": [], "entities": [{"text": "normalizing word inflections", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.7996597687403361}]}, {"text": "To examine the performance of the L 1 -regularized logistic regression as a discriminative model, we also built two classifiers based on the Support Vector Machine (SVM).", "labels": [], "entities": []}, {"text": "These SVM classifiers were implemented by the SVM perf 7 on a linear kernel 8 . An SVM classifier employs the same feature set (substitution rules) as the proposed method so that we can directly compare the L 1 -regularized logistic regression and the linear-kernel SVM.", "labels": [], "entities": []}, {"text": "Another SVM classifier incorporates the five string metrics; this system can be considered as our reproduction of the discriminative string similarity proposed by. reports the precision (P), recall (R), and F1 score (F1) based on the number of correct decisions for positive instances.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 176, "end_pos": 189, "type": "METRIC", "confidence": 0.960967019200325}, {"text": "recall (R)", "start_pos": 191, "end_pos": 201, "type": "METRIC", "confidence": 0.9520353078842163}, {"text": "F1 score (F1)", "start_pos": 207, "end_pos": 220, "type": "METRIC", "confidence": 0.9527862071990967}]}, {"text": "The proposed method outperformed the baseline systems, achieving 0.919, 0.888, and 0.984 of F1 scores, respectively.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9829595983028412}]}, {"text": "Porter's stemmer worked on the Inflection set, but not on the Orthography set, which is beyond the scope of the stemming algorithms.", "labels": [], "entities": []}, {"text": "CST's lemmatizer suffered from low recall on the Inflection set because it removed suffixes of base forms, e.g., (cloning, clone) \u2192 (clone, clo).", "labels": [], "entities": [{"text": "CST", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8887712359428406}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9989907145500183}, {"text": "Inflection set", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.7924387454986572}]}, {"text": "Morpha and CST's lemma- .084 .074 . .", "labels": [], "entities": [{"text": "CST's lemma", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8861325184504191}]}, {"text": "tizer were not designed for orthographic variants and noun derivations.", "labels": [], "entities": []}, {"text": "Levenshtein distance (\u03b4 = 1) did notwork for the Derivation set because noun derivations often append two or more letters (e.g., happy \u2192 happiness).", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.789605438709259}]}, {"text": "No string similarity/distance metrics yielded satisfactory results.", "labels": [], "entities": []}, {"text": "Some metrics obtained the best F1 scores with extreme thresholds only to classify every instance as positive.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9993544220924377}]}, {"text": "These results imply the difficulty of the string metrics for the tasks.", "labels": [], "entities": []}, {"text": "The L 1 -regularized logistic regression was comparable to the SVM with linear kernel in this experiment.", "labels": [], "entities": []}, {"text": "However, the presented model presents the advantage that it can reduce the number of active features (features with non-zero weights assigned); the L 1 regularization can remove 74%, 48%, and 82% of substitution rules in each dataset.", "labels": [], "entities": []}, {"text": "The performance improvements by incorporating string metrics as features were very subtle (less than 0.7%).", "labels": [], "entities": []}, {"text": "What is worse, the distance/similarity metrics do not specifically derive destination strings to which the classifier is likely to assign positive labels.", "labels": [], "entities": []}, {"text": "Therefore, we can no longer use the efficient algorithm as a candidate generator (in Section 2.4) with these features.", "labels": [], "entities": []}, {"text": "demonstrates the ability of our approach to obtain effective features; the table shows the top 10 features with high weights assigned for the Orthography data.", "labels": [], "entities": [{"text": "Orthography data", "start_pos": 142, "end_pos": 158, "type": "DATASET", "confidence": 0.868529736995697}]}, {"text": "An interesting aspect of the proposed method is that the process of the orthographic variants is interpretable through the feature weights.", "labels": [], "entities": []}, {"text": "shows plots of the F1 scores (y-axis) for the Inflection data when we change the number of active features (x-axis) by controlling the regularization parameter \u03c3 from 0.001 to 100.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9813863337039948}, {"text": "Inflection data", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.8950342535972595}]}, {"text": "The larger the value we set for \u03c3, the better the classifier performs, generally, with more active features.", "labels": [], "entities": []}, {"text": "In extreme cases, the number of active features drops to 97 with \u03c3 = 0.01; nonetheless, the classifier still achieves 0.961 of the F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9806530475616455}]}, {"text": "The result suggests that a small set of substitution rules can accommodate most cases of inflectional variations.", "labels": [], "entities": []}, {"text": "The second experiment examined the performance of the string normalization tasks formalized in equation 1.", "labels": [], "entities": [{"text": "string normalization tasks", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.7731759051481882}]}, {"text": "In this task, a system was given a string sand was required to yield either its transformed form t * (s = t * ) or the string s itself when the transformation is unnecessary for s.", "labels": [], "entities": []}, {"text": "The conditional probability distribution (scorer) in equation 1 was modeled   by the maximum entropy framework.", "labels": [], "entities": []}, {"text": "Features for the maximum entropy model consist of: substitution rules between strings sand t, letter bigrams and trigrams in s, and letter bigrams and trigrams int.", "labels": [], "entities": []}, {"text": "We prepared four datasets, Orthography, Derivation, Inflection, and XTAG morphology.", "labels": [], "entities": [{"text": "XTAG morphology", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.5590041279792786}]}, {"text": "Each dataset is a list of string pairs s, t that indicate the transformation of the string s into t.", "labels": [], "entities": []}, {"text": "A source string sis identical to its destination string t when string s should not be changed.", "labels": [], "entities": []}, {"text": "These instances correspond to the case where string s has already been lemmatized.", "labels": [], "entities": []}, {"text": "For each string pair (s, t) in LR-SPL 9 , LRNOM, and LRAGR tables, we generated two instances s, t and t, t.", "labels": [], "entities": [{"text": "LR-SPL 9", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.7911178767681122}, {"text": "LRNOM", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.6970710754394531}, {"text": "LRAGR", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9193475842475891}]}, {"text": "Consequently, a system is expected to leave the string t unchanged.", "labels": [], "entities": []}, {"text": "We also used XTAG morphology to perform a crossdomain evaluation of the lemmatizer trained on the Inflection dataset . The entries in XTAG morphol-9 We define that s precedes tin dictionary order.", "labels": [], "entities": [{"text": "Inflection dataset", "start_pos": 98, "end_pos": 116, "type": "DATASET", "confidence": 0.8851881623268127}]}, {"text": "10 XTAG morphology database 1.5: ftp://ftp.cis.upenn.edu/pub/xtag/morph-1. 5/morph-1.5.tar.gz We found that XTAG morphology contains numerous inogy that also appear in the Inflection dataset were 39,130 out of 317,322 (12.3 %).", "labels": [], "entities": [{"text": "Inflection dataset", "start_pos": 172, "end_pos": 190, "type": "DATASET", "confidence": 0.9618103802204132}]}, {"text": "We evaluated the proposed method and CST's lemmatizer by performing ten-fold cross validation.", "labels": [], "entities": []}, {"text": "reports the performance based on the number of correct transformations.", "labels": [], "entities": []}, {"text": "The proposed method again outperformed the baseline systems with a wide margin.", "labels": [], "entities": []}, {"text": "It is noteworthy that the proposed method can accommodate morphological inflections in the XTAG morphology corpus with no manual tuning or adaptation.", "labels": [], "entities": [{"text": "XTAG morphology corpus", "start_pos": 91, "end_pos": 113, "type": "DATASET", "confidence": 0.7673110167185465}]}, {"text": "Although we introduced no assumptions about target tasks (e.g. a known vocabulary), the average number of positive substitution rules relevant to source strings was as small as 23.9 (in XTAG morphology data).", "labels": [], "entities": [{"text": "XTAG morphology data", "start_pos": 186, "end_pos": 206, "type": "DATASET", "confidence": 0.7515191237131754}]}, {"text": "Therefore, the candidate generator performed 23.9 substitution operations fora given string.", "labels": [], "entities": []}, {"text": "It applied the decision rules (equation 7) 21.3 times, and generated 1.67 candidate strings per source string.", "labels": [], "entities": []}, {"text": "The experimental results described herein demonstrated that the candidate generator was modeled successfully by the discriminative framework.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table  Description  # Entries  LRSPL  Spelling variants  90,323  LRNOM Nominalizations (derivations)  14,029  LRAGR  Agreement and inflection  910,854  LRWD  Word index (vocabulary)  850,236", "labels": [], "entities": [{"text": "LRAGR  Agreement and inflection  910,854  LRWD  Word index", "start_pos": 111, "end_pos": 169, "type": "METRIC", "confidence": 0.7295484431087971}]}, {"text": " Table 2: Characteristics of datasets.", "labels": [], "entities": []}, {"text": " Table 3: Performance of candidate classification", "labels": [], "entities": []}, {"text": " Table 5: Performance of string transformation", "labels": [], "entities": [{"text": "string transformation", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7592789232730865}]}]}