{"title": [], "abstractContent": [{"text": "We present a generative model for unsuper-vised coreference resolution that views coref-erence as an EM clustering process.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.9138813316822052}, {"text": "EM clustering", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.812478631734848}]}, {"text": "For comparison purposes, we revisit Haghighi and Klein's (2007) fully-generative Bayesian model for unsupervised coreference resolution , discuss its potential weaknesses and consequently propose three modifications to their model.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.9039110541343689}]}, {"text": "Experimental results on the ACE data sets show that our model outperforms their original model by a large margin and compares favorably to the modified model.", "labels": [], "entities": [{"text": "ACE data sets", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9790756106376648}]}], "introductionContent": [{"text": "Coreference resolution is the problem of identifying which mentions (i.e., noun phrases) refer to which real-world entities.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9172150492668152}]}, {"text": "The availability of annotated coreference corpora produced as a result of the MUC conferences and the ACE evaluations has prompted the development of a variety of supervised machine learning approaches to coreference resolution in recent years.", "labels": [], "entities": [{"text": "MUC conferences", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9361077547073364}, {"text": "coreference resolution", "start_pos": 205, "end_pos": 227, "type": "TASK", "confidence": 0.9459679424762726}]}, {"text": "The focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether two mentions are co-referring (e.g.,,,) to the development of rich linguistic features (e.g.,,) and the exploitation of advanced techniques that involve joint learning (e.g.,) and joint inference (e.g.,) for coreference resolution and a related extraction task.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 339, "end_pos": 361, "type": "TASK", "confidence": 0.909972608089447}]}, {"text": "The rich features, coupled with the increased complexity of coreference models, have made these supervised approaches more dependent on labeled data and less applicable to languages for which little or no annotated data exists.", "labels": [], "entities": []}, {"text": "Given the growing importance of multi-lingual processing in the NLP community, however, the development of unsupervised and weakly supervised approaches for the automatic processing of resource-scarce languages has become more important than ever.", "labels": [], "entities": []}, {"text": "In fact, several popular weakly supervised learning algorithms such as self-training, co-training (, and EM have been applied to coreference resolution ( and the related task of pronoun resolution).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.9659037888050079}, {"text": "pronoun resolution", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.7588424682617188}]}, {"text": "Given a small number of coreference-annotated documents and a large number of unlabeled documents, these weakly supervised learners aim to incrementally augment the labeled data by iteratively training a classifier 1 on the labeled data and using it to label mention pairs randomly drawn from the unlabeled documents as COREFERENT or NOT COREFERENT.", "labels": [], "entities": []}, {"text": "However, classifying mention pairs using such iterative approaches is undesirable for coreference resolution: since the non-coreferent mention pairs significantly outnumber their coreferent counterparts, the resulting classifiers generally have an increasing tendency to (mis)label a pair as non-coreferent as bootstrapping progresses (see).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.9647172391414642}]}, {"text": "Motivated in part by these results, we present a generative, unsupervised model for probabilistically inducing coreference partitions on unlabeled documents, rather than classifying mention pairs, via EM clustering (Section 2).", "labels": [], "entities": []}, {"text": "In fact, our model combines the best of two worlds: it operates at the document level, while exploiting essential linguistic constraints on coreferent mentions (e.g., gender and number agreement) provided by traditional pairwise classification models.", "labels": [], "entities": []}, {"text": "For comparison purposes, we revisit a fullygenerative Bayesian model for unsupervised coreference resolution recently introduced by, discuss its potential weaknesses and consequently propose three modifications to their model (Section 3).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.9331957399845123}]}, {"text": "Experimental results on the ACE data sets show that our model outperforms their original model by a large margin and compares favorably to the modified model (Section 4).", "labels": [], "entities": [{"text": "ACE data sets", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9818611542383829}]}], "datasetContent": [{"text": "To evaluate our EM-based model and H&K's model, we use the ACE 2003 coreference corpus, which is composed of three sections: Broadcast News (BNEWS), Newswire (NWIRE), and Newspaper (NPAPER).", "labels": [], "entities": [{"text": "ACE 2003 coreference corpus", "start_pos": 59, "end_pos": 86, "type": "DATASET", "confidence": 0.9381949156522751}]}, {"text": "Each section is in turn composed of a training set and a test set.", "labels": [], "entities": []}, {"text": "Due to space limitations, we will present evaluation results only for the test sets of BNEWS and NWIRE, but verified that the same performance trends can be observed on NPA-PER as well.", "labels": [], "entities": [{"text": "BNEWS", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.8741358518600464}, {"text": "NWIRE", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.9055739641189575}, {"text": "NPA-PER", "start_pos": 169, "end_pos": 176, "type": "DATASET", "confidence": 0.9298911094665527}]}, {"text": "Unlike H&K, who report results using only true mentions (extracted from the answer keys), we show results for true mentions as well as system mentions that were extracted by an in-house noun phrase chunker.", "labels": [], "entities": []}, {"text": "The relevant statistics of the BNEWS and NWIRE test sets are shown in.", "labels": [], "entities": [{"text": "BNEWS", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9111183285713196}, {"text": "NWIRE test sets", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9474155902862549}]}, {"text": "To score the output of the coreference models, we employ the commonly-used MUC scoring program ( and the recently-developed CEAF scoring program).", "labels": [], "entities": [{"text": "MUC scoring program", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.6449929972489675}, {"text": "CEAF scoring program", "start_pos": 124, "end_pos": 144, "type": "DATASET", "confidence": 0.8201353549957275}]}, {"text": "In the MUC scorer, recall is computed as  the percentage of coreference links in the reference partition that appear in the system partition; precision is computed in the same fashion as recall, except that the roles of the reference partition and the system partition are reversed.", "labels": [], "entities": [{"text": "MUC scorer", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.727086752653122}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9992165565490723}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9993763566017151}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9956806898117065}]}, {"text": "As a link-based scoring program, the MUC scorer (1) does not reward successful identification of singleton entities and tends to under-penalize partitions that have too few entities.", "labels": [], "entities": [{"text": "MUC scorer", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.5473071932792664}]}, {"text": "The entity-based CEAF scorer was proposed in response to these two weaknesses.", "labels": [], "entities": [{"text": "CEAF scorer", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.8866719007492065}]}, {"text": "Specifically, it operates by computing the optimal alignment between the set of reference entities and the set of system entities.", "labels": [], "entities": []}, {"text": "CEAF precision and recall are both positively correlated with the score of this optimal alignment, which is computed by summing over each aligned entity pair the number of mentions that appear in both entities of that pair.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.621545672416687}, {"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.7125304937362671}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9995506405830383}]}, {"text": "As a consequence, a system that proposes too many entities or too few entities will have low precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9995306730270386}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9991790652275085}]}, {"text": "We use a small amount of labeled data for parameter initialization for the two models.", "labels": [], "entities": []}, {"text": "Specifically, for evaluations on the BNEWS test data, we use as labeled data one randomly-chosen document from the BNEWS training set, which has 58 true mentions and 102 system mentions.", "labels": [], "entities": [{"text": "BNEWS test data", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9579103390375773}, {"text": "BNEWS training set", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.948597272237142}]}, {"text": "Similarly for NWIRE, where the chosen document has 42 true mentions and 72 system mentions.", "labels": [], "entities": [{"text": "NWIRE", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.853073239326477}]}, {"text": "For our model, we use the labeled document to initialize the parameters.", "labels": [], "entities": []}, {"text": "Also, we set N (the number of most probable partitions) to 50 and \u03b4 (the start penalty used in the Bell tree) to 0.8, the latter being recommended by.", "labels": [], "entities": []}, {"text": "For H&K's model, we use the labeled data to tune the concentration parameter \u03b1.", "labels": [], "entities": []}, {"text": "While H&K set \u03b1 to 0.4 without much explanation, a moment's thought reveals that the choice of \u03b1 should reflect the fraction of mentions that appear in a singleton cluster.", "labels": [], "entities": []}, {"text": "We therefore estimate this value from the labeled document, yielding 0.4 for true mentions (which is consistent with H&K's choice) and 0.7 for system mentions.", "labels": [], "entities": []}, {"text": "The remaining parameters, the \u03bb's, are all set toe \u22124 , following H&K.", "labels": [], "entities": []}, {"text": "In addition, as is commonly done in Bayesian approaches, we do not sample entities directly from the conditional distribution P (Z|X); rather, we sample from this distribution raised to the power exp ci k\u22121 , where c=1.5, i is the current iteration number that starts at 0, and k (the number of sampling iterations) is set to 20.", "labels": [], "entities": []}, {"text": "Finally, due to sampling and the fact that the initial assignment of entity indices to mentions is random, all the reported results for H&K's model are averaged over five runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Posterior distribution of mention type given  salience (taken from Haghighi and Klein (2007))", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the BNEWS and NWIRE test sets", "labels": [], "entities": [{"text": "BNEWS", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.8857278227806091}, {"text": "NWIRE test sets", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9290947318077087}]}, {"text": " Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data sets", "labels": [], "entities": [{"text": "MUC", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.6086266040802002}, {"text": "Broadcast News and Newswire data sets", "start_pos": 65, "end_pos": 102, "type": "DATASET", "confidence": 0.9330542782942454}]}, {"text": " Table 5: Results obtained using the CEAF scoring program for the Broadcast News and Newswire data sets", "labels": [], "entities": [{"text": "CEAF", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.7444233298301697}, {"text": "Broadcast News and Newswire data sets", "start_pos": 66, "end_pos": 103, "type": "DATASET", "confidence": 0.9265100260575613}]}]}