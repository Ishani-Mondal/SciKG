{"title": [], "abstractContent": [{"text": "We explore a stacked framework for learning to predict dependency structures for natural language sentences.", "labels": [], "entities": []}, {"text": "A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.6860596686601639}]}, {"text": "Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another.", "labels": [], "entities": []}, {"text": "We show that this is an example of stacked learning, in which a second pre-dictor is trained to improve the performance of the first.", "labels": [], "entities": []}, {"text": "Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction.", "labels": [], "entities": []}, {"text": "Experiments on twelve languages show that stacking transition-based and graph-based parsers improves performance over existing state-of-the-art dependency parsers.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 65, "end_pos": 104, "type": "TASK", "confidence": 0.6391909345984459}]}, {"text": "This tradeoff is exemplified in dependency parsing, illustrated in, on which we focus in this paper: \u2022 Exact algorithms for dependency parsing are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as \"arc factorization\" for nonprojective dependency parsing).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8506923317909241}, {"text": "dependency parsing", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7990930080413818}]}, {"text": "\u2022 Feature-rich parsers must resort to search or greediness, (), so that parsing solutions are inexact and learned models maybe subject to certain kinds of bias ().", "labels": [], "entities": []}, {"text": "A solution that leverages the complementary strengths of these two approaches-described in detail by-was recently and successfully explored by.", "labels": [], "entities": []}, {"text": "Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers.", "labels": [], "entities": []}, {"text": "We give anew theoretical motivation for stacking parsers, in terms of extending a parsing model's feature space.", "labels": [], "entities": [{"text": "stacking parsers", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.9024356007575989}]}, {"text": "Specifically, we view stacked learning as away of approximating non-local features in a linear model, rather than making empirically dubious independence () or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations), solving a (generally NP-hard) integer linear program (), or adding latent variables.", "labels": [], "entities": [{"text": "stacked learning", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8658961951732635}]}, {"text": "Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser.", "labels": [], "entities": []}, {"text": "Related approaches are the belief propagation algorithm of, and the \"trading of structure for features\" explored by Liang et al.  those that assume each dependency decision is independent modulo the global structural constraint that dependency graphs must be trees.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7944996654987335}]}, {"text": "Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph).", "labels": [], "entities": []}, {"text": "Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time).", "labels": [], "entities": []}, {"text": "The primary problem in treating each dependency as independent is that it is not a realistic assumption.", "labels": [], "entities": []}, {"text": "Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies).", "labels": [], "entities": []}, {"text": "However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (.", "labels": [], "entities": []}, {"text": "The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting.", "labels": [], "entities": []}, {"text": "We start by investigating and extending the edge-factored model of.", "labels": [], "entities": []}, {"text": "In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating both the partition function and edge expectations overall possible dependency graphs fora given sentence.", "labels": [], "entities": []}, {"text": "To motivate these algorithms, we show that they can be used in many important learning and inference problems including min-risk decoding, training globally normalized log-linear models, syntactic language modeling, and unsupervised learning via the EM algorithm -none of which have previously been known to have exact non-projective implementations.", "labels": [], "entities": [{"text": "syntactic language modeling", "start_pos": 187, "end_pos": 214, "type": "TASK", "confidence": 0.6454921861489614}]}, {"text": "We then switch focus to models that account for non-local information, in particular arity and neighbouring parse decisions.", "labels": [], "entities": []}, {"text": "For systems that model arity constraints we give a reduction from the Hamiltonian graph problem suggesting that the parsing problem is intractable in this case.", "labels": [], "entities": []}, {"text": "For neighbouring parse decisions, we extend the work of  and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods.", "labels": [], "entities": []}, {"text": "A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following experiments we demonstrate the effectiveness of stacking parsers.", "labels": [], "entities": [{"text": "stacking parsers", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.9373547434806824}]}, {"text": "As noted in \u00a73.1, we make use of two component parsers, the graph-based MSTParser and the transition-based MaltParser.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.869105875492096}]}, {"text": "The publicly available version of MSTParser performs parsing and labeling jointly.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 34, "end_pos": 43, "type": "DATASET", "confidence": 0.9229455590248108}, {"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9680805802345276}]}, {"text": "We adapted this system to first perform unlabeled parsing, then label the arcs using a log-linear classifier with access to the full unlabeled parse ().", "labels": [], "entities": []}, {"text": "In stacking experiments, the arc labels from the level 0 parser are also used as a feature.", "labels": [], "entities": [{"text": "stacking", "start_pos": 3, "end_pos": 11, "type": "TASK", "confidence": 0.9656966328620911}]}, {"text": "In the following subsections, we refer to our modification of the MSTParser as MST 1O (the arcfactored version) and MST 2O (the second-order arc-pair-factored version).", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.9229462742805481}]}, {"text": "All our experiments use the non-projective version of this parser.", "labels": [], "entities": []}, {"text": "We refer to the MaltParser as Malt.", "labels": [], "entities": []}, {"text": "We report experiments on twelve languages from the CoNLL-X shared task ().", "labels": [], "entities": [{"text": "CoNLL-X shared task", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.6941725611686707}]}, {"text": "5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.", "labels": [], "entities": [{"text": "labeled attachment score (LAS)", "start_pos": 42, "end_pos": 72, "type": "METRIC", "confidence": 0.8794585267702738}]}, {"text": "Statistical significance is measured using Dan Bikel's randomized parsing evaluation comparator with 10,000 iterations.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.7053847312927246}]}, {"text": "The additional features used in the level 1 parser are enumerated in and their various subsets are depicted in Table 2.", "labels": [], "entities": []}, {"text": "The PredEdge features are exactly the six features used by in their MST Malt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4.", "labels": [], "entities": [{"text": "PredEdge", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7994291186332703}, {"text": "MST Malt parser", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.8911752700805664}]}, {"text": "In all our experiments, the number of partitions used to create\u02dcDcreate\u02dc create\u02dcD is L = 2.", "labels": [], "entities": []}, {"text": "Our first experiment stacks the highly accurate MST 2O parser with itself.", "labels": [], "entities": [{"text": "MST 2O parser", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.5783160527547201}]}, {"text": "At level 0, the parser uses only the standard features ( \u00a75.1), and at level 1, these are augmented by various subsets of features of x along with the output of the level 0 parser, g(x) (.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "While we see improvements over the single-parser baseline We made other modifications to MSTParser, implementing many of the successes described by ).", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.9282206892967224}]}, {"text": "Our version of the code is publicly available at http: //www.ark.cs.cmu.edu/MSTParserStacked.", "labels": [], "entities": [{"text": "MSTParserStacked", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.9572181701660156}]}, {"text": "The modifications included an approximation to lemmas for datasets without lemmas (three-character prefixes), and replacing morphology/word and morphology/lemma features with morphology/POS features.", "labels": [], "entities": []}, {"text": "The CoNLL-X shared task actually involves thirteen languages; our experiments do not include Czech (the largest dataset), due to time constraints.", "labels": [], "entities": []}, {"text": "Therefore, the average results plotted in the last rows of, and 5 are not directly comparable with previously published averages over thirteen languages.", "labels": [], "entities": []}, {"text": "for nine languages, the improvements are small (less than 0.5%).", "labels": [], "entities": []}, {"text": "One of the biggest concerns about this model is the fact that it stacks two predictors that are very similar in nature: both are graph-based and share the features f 1,a (x).", "labels": [], "entities": []}, {"text": "It has been pointed out by, among others, that the success of ensemble methods like stacked learning strongly depends on how uncorrelated the individual decisions made by each predictor are from the others' decisions.", "labels": [], "entities": []}, {"text": "8 This experiment provides further evidence for the claim.", "labels": [], "entities": []}, {"text": "We next use MaltParser at level 0 and the secondorder arc-pair-factored MST 2O at level 1.", "labels": [], "entities": []}, {"text": "This extends the experiments of, replicated in our feature subset A. enumerates the results.", "labels": [], "entities": []}, {"text": "Note that the best-performing stacked configuration for each and every language outperforms MST 2O , corroborating results reported by.", "labels": [], "entities": []}, {"text": "The best performing stacked configuration outperforms Malt as well, except for Japanese and Turkish.", "labels": [], "entities": [{"text": "Malt", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9358155131340027}]}, {"text": "Further, our non-arc-factored features largely outperform subset A, except on Bulgarian, Chinese, and Japanese.", "labels": [], "entities": []}, {"text": "On average, the best feature configuration is E, which is statistically significant over Malt and MST 2O with p < 0.0001, and over feature subset A with p < 0.01.", "labels": [], "entities": []}, {"text": "Finally, we consider stacking MaltParser with the first-order, arc-factored MSTParser.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.9269217848777771}]}, {"text": "We view this approach as perhaps the most promising, since it is an exact parsing method with the quadratic runtime complexity of MST 1O . enumerates the results.", "labels": [], "entities": []}, {"text": "For all twelve languages, some stacked configuration outperforms MST 1O and also, surprisingly, MST 2O , the second order model.", "labels": [], "entities": []}, {"text": "This provides empirical evidence that using rich features from MaltParser at level 0, a stacked level 1 first-order MSTParser can outperform the second-order MSTParser.", "labels": [], "entities": [{"text": "MaltParser", "start_pos": 63, "end_pos": 73, "type": "DATASET", "confidence": 0.9388883709907532}]}, {"text": "In only two cases (Japanese and Turkish), the MaltParser slightly outperforms the stacked parser.", "labels": [], "entities": []}, {"text": "On average, feature configuration D performs the best, and is statistically significant over Malt, MST , and MST 2O with p < 0.0001, and over feature subset A with p < 0.05.", "labels": [], "entities": [{"text": "MST", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.8707185387611389}]}, {"text": "Encouragingly, this configuration is barely outperformed by configura- tion A of Malt + MST 2O (see), the difference being statistically insignificant (p > 0.05).", "labels": [], "entities": [{"text": "Malt + MST 2O", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.7882409393787384}]}, {"text": "This shows that stacking M alt with the exact, arcfactored MST 1O bridges the difference between the individual MST 1O and MST 2O models, by approximating higher order features, but maintaining an O(n 2 ) runtime and finding the model-optimal parse.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of stacking MST 2O with itself at both level 0 and level 1. Column 2 enumerates LAS for MST 2O .  Columns 3-6 enumerate results for four different stacked feature subsets. Bold indicates best results for a particular  language.", "labels": [], "entities": [{"text": "stacking MST 2O", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7505170305569967}, {"text": "LAS", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9790007472038269}]}, {"text": " Table 5: Results of stacking Malt and MST 1O at level 0 and level 1, respectively. Columns 2-4 enumerate LAS for  Malt, MST 1O and MST 2O . Columns 5-9 enumerate results for five different stacked feature configurations. Bold  indicates the best result for a language.", "labels": [], "entities": [{"text": "stacking Malt", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7499547004699707}, {"text": "LAS", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.962445855140686}]}]}