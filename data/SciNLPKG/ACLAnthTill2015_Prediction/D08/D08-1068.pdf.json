{"title": [{"text": "Joint Unsupervised Coreference Resolution with Markov Logic", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.9340401589870453}]}], "abstractContent": [{"text": "Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9652083814144135}]}, {"text": "Some unsuper-vised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate.", "labels": [], "entities": []}, {"text": "In this paper, we present the first un-supervised approach that is competitive with supervised ones.", "labels": [], "entities": []}, {"text": "This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals.", "labels": [], "entities": []}, {"text": "On MUC and ACE datasets, our model outper-forms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.", "labels": [], "entities": [{"text": "ACE datasets", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.8971345722675323}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9984174966812134}]}], "introductionContent": [{"text": "The goal of coreference resolution is to identify mentions (typically noun phrases) that refer to the same entities.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.965069442987442}]}, {"text": "This is a key subtask in many NLP applications, including information extraction, question answering, machine translation, and others.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.8430080115795135}, {"text": "question answering", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.9142498672008514}, {"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8039068877696991}]}, {"text": "Supervised learning approaches treat the problem as one of classification: for each pair of mentions, predict whether they corefer or not (e.g.,).", "labels": [], "entities": []}, {"text": "While successful, these approaches require labeled training data, consisting of mention pairs and the correct decisions for them.", "labels": [], "entities": []}, {"text": "Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text.", "labels": [], "entities": []}, {"text": "However, unsupervised coreference resolution is much more difficult.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9453847408294678}]}, {"text": "model, the most sophisticated to date, still lags supervised ones by a substantial margin.", "labels": [], "entities": []}, {"text": "Extending it appears difficult, due to the limitations of its Dirichlet process-based representation.", "labels": [], "entities": []}, {"text": "The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the \"easy\" decisions to help make related \"hard\" ones.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.9044088423252106}]}, {"text": "Relations that have been exploited in supervised coreference resolution include transitivity) and anaphoricity.", "labels": [], "entities": [{"text": "supervised coreference resolution", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.6767714619636536}]}, {"text": "However, there is little work to date on joint inference for unsupervised resolution.", "labels": [], "entities": [{"text": "unsupervised resolution", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.650454580783844}]}, {"text": "We address this problem using Markov logic, a powerful and flexible language that combines probabilistic graphical models and first-order logic).", "labels": [], "entities": []}, {"text": "Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals.", "labels": [], "entities": []}, {"text": "By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 193, "end_pos": 215, "type": "TASK", "confidence": 0.8431163430213928}]}, {"text": "We test our approach on standard MUC and ACE datasets.", "labels": [], "entities": [{"text": "MUC", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7268750667572021}, {"text": "ACE datasets", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.8954368233680725}]}, {"text": "Our basic model, trained on a minimum of data, suffices to outperform one.", "labels": [], "entities": []}, {"text": "Our full model, using apposition and other relations for joint inference, is often as accurate as the best supervised models, or more.", "labels": [], "entities": []}, {"text": "We begin by reviewing the necessary background on Markov logic.", "labels": [], "entities": []}, {"text": "We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8587826192378998}]}, {"text": "Finally, we present our experiments and results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of coreference results in MUC  scores on the MUC-6 dataset.", "labels": [], "entities": [{"text": "coreference", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.8965683579444885}, {"text": "MUC-6 dataset", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9861045777797699}]}, {"text": " Table 2: Comparison of coreference results in MUC  scores on the ACE-2004 (English) datasets.", "labels": [], "entities": [{"text": "coreference", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9166142344474792}, {"text": "MUC", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.6539782881736755}, {"text": "ACE-2004 (English) datasets", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.9098482251167297}]}, {"text": " Table 3: Comparison of coreference results in MUC  scores on the ACE-2 datasets.", "labels": [], "entities": [{"text": "coreference", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9151492118835449}, {"text": "MUC", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.5846993327140808}, {"text": "ACE-2 datasets", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9893046021461487}]}, {"text": " Table 4: Comparison of coreference results in B 3 scores  on the ACE-2 datasets.", "labels": [], "entities": [{"text": "coreference", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9241070747375488}, {"text": "B 3 scores", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.965822180112203}, {"text": "ACE-2 datasets", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9879608154296875}]}, {"text": " Table 5: Our coreference results in precision, recall, and  F1 for pairwise resolution.", "labels": [], "entities": [{"text": "coreference", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9571549296379089}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9997658133506775}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9997046589851379}, {"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9998512268066406}]}, {"text": " Table 6: Average gold number of clusters per document  vs. the mean absolute error of our system.", "labels": [], "entities": [{"text": "Average gold number of clusters", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.8423035621643067}]}]}