{"title": [{"text": "Language and Translation Model Adaptation using Comparable Corpora", "labels": [], "entities": [{"text": "Language and Translation Model Adaptation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.693660968542099}]}], "abstractContent": [{"text": "Traditionally, statistical machine translation systems have relied on parallel bilingual data to train a translation model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 15, "end_pos": 46, "type": "TASK", "confidence": 0.658521980047226}]}, {"text": "While bilingual parallel data are expensive to generate, mono-lingual data are relatively common.", "labels": [], "entities": []}, {"text": "Yet mono-lingual data have been under-utilized, having been used primarily for training a language model in the target language.", "labels": [], "entities": []}, {"text": "This paper describes a novel method for utilizing monolin-gual target data to improve the performance of a statistical machine translation system on news stories.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 107, "end_pos": 138, "type": "TASK", "confidence": 0.6312115291754404}]}, {"text": "The method exploits the existence of comparable text-multiple texts in the target language that discuss the same or similar stories as found in the source language document.", "labels": [], "entities": []}, {"text": "For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to the source documents.", "labels": [], "entities": []}, {"text": "These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9219518899917603}]}, {"text": "Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "While the amount of parallel data available to train a statistical machine translation system is sharply limited, vast amounts of monolingual data are generally available, especially when translating to languages such as English.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6328226824601492}]}, {"text": "Yet monolingual data are generally only used to train the language model of the translation system.", "labels": [], "entities": []}, {"text": "Previous work) has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations.", "labels": [], "entities": []}, {"text": "More recently, and have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel.", "labels": [], "entities": []}, {"text": "This newly discovered bilingual data can then be used as additional training data for the translation system.", "labels": [], "entities": []}, {"text": "Such methods generally have a very low yield leaving vast amounts of data that is only used for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7125523537397385}]}, {"text": "These methods rely upon comparable corpora, that is, multiple corpora that are of the same general genre.", "labels": [], "entities": []}, {"text": "In addition to this, documents can be comparable-two documents that are both on the same event or topic.", "labels": [], "entities": []}, {"text": "Comparable documents occur because of the repetition of information across languages, and in the case of news data, on the fact that stories reported in one language are often reported in another language.", "labels": [], "entities": []}, {"text": "In cases where no direct translation can be found fora source document, it is often possible to find documents in the target language that are on the same story, or even on a related story, either in subject matter or historically.", "labels": [], "entities": []}, {"text": "Such documents can be classified as comparable to the original source document.", "labels": [], "entities": []}, {"text": "Phrases within this comparable document are likely to be translations of phrases in the source document, even if the documents themselves are not parallel.", "labels": [], "entities": []}, {"text": "shows an excerpt of the reference translation of an Arabic document, and figure 2 shows a Cameras are flashing and reporters are following up, for Hollywood star Angelina Jolie is finally talking to the public after a one-month stay in India, but not as a movie star.", "labels": [], "entities": []}, {"text": "In this case, the two new stories are not translations of each other and were not reported at the same time-the comparable passage being an older news story-but both discuss actress Angelina Jolie's visit to India.", "labels": [], "entities": []}, {"text": "Many phrases and words are shared between the two, including: the name of the movie, the name and relationship of the actress' character, the name and age of her son and many others.", "labels": [], "entities": []}, {"text": "Such a pairing is extremely comparable, although even less related document pairs could easily be considered comparable.", "labels": [], "entities": []}, {"text": "We seek to take advantage of these comparable documents to inform the translation of the source document.", "labels": [], "entities": [{"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9702606797218323}]}, {"text": "This can be done by augmenting the major components of the statistical translation system: the Language Model and the Translation Model.", "labels": [], "entities": []}, {"text": "This work is in the same tradition as,, and.", "labels": [], "entities": []}, {"text": "used large amounts of comparable data to adapt language models on a documentby-document basis, while used comparable data to perform sentence level adaptation of the language model.", "labels": [], "entities": [{"text": "sentence level adaptation", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.6587106287479401}]}, {"text": "These adapted language models were shown to improve performance  In addition to language model adaptation we also modify the translation model, adding additional translation rules that enable the translation of new words and phrases in both the source and target languages, as well as increasing the probability of existing translation rules.", "labels": [], "entities": []}, {"text": "Translation adaptation using the translation system's own output, known as Self-Training () has previously shown gains by augmenting the translation model with additional translation rules.", "labels": [], "entities": [{"text": "Translation adaptation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9842093288898468}]}, {"text": "In that approach however, the translation model was augmented using parallel data, rather than comparable data, by interpolating a translation model trained using the system output with the original translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.966586172580719}]}, {"text": "Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown by to yield significant gains over a baseline system.", "labels": [], "entities": [{"text": "Translation model adaptation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9221965471903483}]}, {"text": "The translation model was adapted by selecting comparable sentences from parallel corpora for each of the sentences to be translated.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9691005349159241}]}, {"text": "In addition to selecting outof-domain data to adapt the translation model, comparable data selection techniques have been used to select and weight portions of the existing training data for the translation model to improve translation performance (.", "labels": [], "entities": []}, {"text": "The research presented in this paper utilizes a different approach to translation model adaptation using comparable monolingual text rather than parallel text, exploiting data that would otherwise be unused for estimating the translation model.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.9437898596127828}]}, {"text": "In addition, this data also informs the translation system by interpolating the original language model with anew language model trained from the same comparable documents.", "labels": [], "entities": []}, {"text": "We discuss the selection of comparable text for model adaptation in section 2.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7597702741622925}]}, {"text": "In sections 3.1 and 3.2, we describe the model adaptation for the language model and translation model, respectively.", "labels": [], "entities": [{"text": "translation", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.969504177570343}]}, {"text": "Experimental results describing the application of model adaptation to a hierarchical Arabic-to-English MT system are presented in section 4.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7273365557193756}, {"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9264052510261536}]}, {"text": "Finally we draw conclusions in sections 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the performance of language and translation model adaptation with our translation system on two conditions, the details of which are presented in section 4.1.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.8666760921478271}]}, {"text": "One condition involved a small amount of parallel training, such as one might find when translating a less commonly taught language (LCTL).", "labels": [], "entities": []}, {"text": "The other condition involved the full amount of training available for Arabic-to-English translation.", "labels": [], "entities": [{"text": "Arabic-to-English translation", "start_pos": 71, "end_pos": 100, "type": "TASK", "confidence": 0.6715110540390015}]}, {"text": "In the case of LCTLs we expect our translation model to have the most deficiencies and be most in need of additional translation rules.", "labels": [], "entities": []}, {"text": "So, it is under such a condition we would expect the translation model adaptation to be the most beneficial.", "labels": [], "entities": [{"text": "translation model adaptation", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.9041224122047424}]}, {"text": "We evaluate the system's performance under this condition in section 4.2.", "labels": [], "entities": []}, {"text": "The effectiveness of this technique on state-of-the-art systems, and its efficiency when used with a well trained generic translation model is presented in section 4.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: LCTL Aligned Reference Adaptation Results", "labels": [], "entities": [{"text": "LCTL Aligned Reference Adaptation", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6851488649845123}]}, {"text": " Table 2: LCTL Unaligned Reference Adaptation Results", "labels": [], "entities": [{"text": "LCTL Unaligned Reference Adaptation", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.6363692209124565}]}, {"text": " Table 3: LCTL Fair Adaptation Results", "labels": [], "entities": [{"text": "LCTL Fair Adaptation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.654662032922109}]}, {"text": " Table 4: Full Training Adaptation Results", "labels": [], "entities": [{"text": "Full Training Adaptation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7111944258213043}]}]}