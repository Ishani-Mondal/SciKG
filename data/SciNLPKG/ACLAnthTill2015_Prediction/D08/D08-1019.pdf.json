{"title": [{"text": "Sentence Fusion via Dependency Graph Compression", "labels": [], "entities": [{"text": "Sentence Fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9604480266571045}]}], "abstractContent": [{"text": "We present a novel unsupervised sentence fusion method which we apply to a corpus of bi-ographies in German.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7363035529851913}]}, {"text": "Given a group of related sentences, we align their dependency trees and build a dependency graph.", "labels": [], "entities": []}, {"text": "Using integer linear programming we compress this graph to anew tree, which we then linearize.", "labels": [], "entities": []}, {"text": "We use GermaNet and Wikipedia for checking semantic compatibility of co-arguments.", "labels": [], "entities": []}, {"text": "In an evaluation with human judges our method out-performs the fusion approach of Barzilay & McKeown (2005) with respect to readability.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic text summarization is a rapidly developing field in computational linguistics.", "labels": [], "entities": [{"text": "Automatic text summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7793571352958679}, {"text": "computational linguistics", "start_pos": 62, "end_pos": 87, "type": "TASK", "confidence": 0.7480001151561737}]}, {"text": "Summarization systems can be classified as either extractive or abstractive ones).", "labels": [], "entities": []}, {"text": "To date, most systems are extractive: sentences are selected from one or several documents and then ordered.", "labels": [], "entities": []}, {"text": "This method exhibits problems, because input sentences very often overlap and complement each other at the same time.", "labels": [], "entities": []}, {"text": "As a result there is a trade-off between non-redundancy and completeness of the output.", "labels": [], "entities": []}, {"text": "Although the need for abstractive approaches has been recognized before (e.g.), so far almost all attempts to get closer to abstractive summarization using scalable, statistical techniques have been limited to sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 210, "end_pos": 230, "type": "TASK", "confidence": 0.7494608163833618}]}, {"text": "The main reason why there is little progress on abstractive summarization is that this task seems to require a conceptual representation of the text which is not yet available (see e.g. Hovy).", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.5048939138650894}]}, {"text": "Sentence fusion, where anew sentence is generated from a group of related sentences and where complete semantic and conceptual representation is not required, can be seen as a middle-ground between extractive and abstractive summarization.", "labels": [], "entities": [{"text": "Sentence fusion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9257630407810211}]}, {"text": "Our work regards a corpus of biographies in German where multiple documents about the same person should be merged into a single one.", "labels": [], "entities": []}, {"text": "An example of a fused sentence (3) with the source sentences (1,2) is given below: Having both (1) and (2) in a summary would make it redundant.", "labels": [], "entities": []}, {"text": "Selecting only one of them would not give all the information from the input.", "labels": [], "entities": []}, {"text": "(3), fused from both (1) and (2), conveys the necessary information without being redundant and is more appropriate fora summary.", "labels": [], "entities": []}, {"text": "To this end, we present a novel sentence fusion method based on dependency structure alignment and semantically and syntactically informed phrase aggregation and pruning.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7208717614412308}, {"text": "dependency structure alignment", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7349573771158854}]}, {"text": "We address the problem in an unsupervised manner and use integer linear programming (ILP) to find a globally optimal solution.", "labels": [], "entities": []}, {"text": "We argue that our method has three important advantages compared to existing methods.", "labels": [], "entities": []}, {"text": "First, we address the grammaticality issue empirically by means of knowledge obtained from an automatically parsed corpus.", "labels": [], "entities": []}, {"text": "We do not require such resources as subcategorization lexicons or hand-crafted rules, but decide to retain a dependency based on its syntactic importance score.", "labels": [], "entities": []}, {"text": "The second point concerns integrating semantics.", "labels": [], "entities": []}, {"text": "Being definitely important, \"this source of information remains relatively unused in work on aggregation 1 within NLG\".", "labels": [], "entities": []}, {"text": "To our knowledge, in the text-to-text generation field, we are the first to use semantic information not only for alignment but also for aggregation in that we check coarguments' compatibility.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7175140976905823}]}, {"text": "Apart from that, our method is not limited to sentence fusion and can be easily applied to sentence compression.", "labels": [], "entities": [{"text": "sentence fusion", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7646041214466095}, {"text": "sentence compression", "start_pos": 91, "end_pos": 111, "type": "TASK", "confidence": 0.7651925683021545}]}, {"text": "In we compress English sentences with the same approach and achieve state-of-the-art performance.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 gives an overview of related work and Section 3 presents our data.", "labels": [], "entities": []}, {"text": "Section 4 introduces our method and Section 5 describes the experiments and discusses the results of the evaluation.", "labels": [], "entities": []}, {"text": "The conclusions follow in the final section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We choose Barzilay & McKeown's system as a nontrivial baseline since, to our knowledge, there is no other system which outperforms theirs (Sec. 5.1).", "labels": [], "entities": []}, {"text": "It is important for us to evaluate the fusion part of our system, so the input and the linearization module of our method and the baseline are identical.", "labels": [], "entities": []}, {"text": "We are also interested in how many errors are due to the linearization module and thus define the readability upper bound (Sec. 5.2).", "labels": [], "entities": []}, {"text": "We further present and discuss the experiments (Sec. 5.3 and 5.5).", "labels": [], "entities": []}, {"text": "It is notoriously difficult to evaluate generation and summarization systems as there are many dimensions in which the quality of the output can be assessed.", "labels": [], "entities": [{"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9412841796875}]}, {"text": "The goal of our present evaluation is in the first place to check whether our method is able to produce sensible output.", "labels": [], "entities": []}, {"text": "We evaluated the three systems (GRAPH-COMPRESSION, BARZILAY & MCKEOWN and READABILITY UB) with 50 native German speakers on 120 fused sentences generated from 40 randomly drawn related sentences groups.", "labels": [], "entities": [{"text": "GRAPH-COMPRESSION", "start_pos": 32, "end_pos": 49, "type": "METRIC", "confidence": 0.9933179616928101}, {"text": "BARZILAY", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9976287484169006}, {"text": "READABILITY UB", "start_pos": 74, "end_pos": 88, "type": "METRIC", "confidence": 0.9302878677845001}]}, {"text": "In an online experiment, the participants were asked to read a fused sentence preceded by the input and to rate its readability (read) and informativity in respect to the input (inf ) on a five point scale.", "labels": [], "entities": []}, {"text": "The experiment was designed so that every participant rated 40 sentences in total.", "labels": [], "entities": []}, {"text": "No participant saw two sentences generated from the same input.", "labels": [], "entities": []}, {"text": "The results are presented in. len is an average length in words of the output.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. len is an average  length in words of the output.", "labels": [], "entities": []}, {"text": " Table 2: Average readability and informativity on a five  point scale, average length in words", "labels": [], "entities": []}]}