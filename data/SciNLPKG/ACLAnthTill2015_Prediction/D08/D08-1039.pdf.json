{"title": [{"text": "Triplet Lexicon Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.8403691450754801}]}], "abstractContent": [{"text": "This paper describes a lexical trigger model for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7281384666760763}]}, {"text": "We present various methods using triplets incorporating long-distance dependencies that can go beyond the local context of phrases or n-gram based language models.", "labels": [], "entities": []}, {"text": "We evaluate the presented methods on two translation tasks in a reranking framework and compare it to the related IBM model 1.", "labels": [], "entities": []}, {"text": "We show slightly improved translation quality in terms of BLEU and TER and address various constraints to speedup the training based on Expectation-Maximization and to lower the overall number of triplets without loss in translation performance .", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9536543488502502}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9992457628250122}, {"text": "TER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9977341890335083}, {"text": "Expectation-Maximization", "start_pos": 136, "end_pos": 160, "type": "METRIC", "confidence": 0.9885151982307434}]}], "introductionContent": [{"text": "Data-driven methods have been applied very successfully within the machine translation domain since the early 90s.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7674773633480072}]}, {"text": "Starting from single-wordbased translation approaches, significant improvements have been made through advances in modeling, availability of larger corpora and more powerful computers.", "labels": [], "entities": []}, {"text": "Thus, substantial progress made in the past enables today's MT systems to achieve acceptable results in terms of translation quality for specific language pairs such as Arabic-English.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9852098822593689}]}, {"text": "If sufficient amounts of parallel data are available, statistical MT systems can be trained on millions of * The work was carried out while the author was at the Human Language Technology and Pattern Recognition group at RWTH Aachen University and partly supported by the Valencian  sentence pairs and use an extended level of context based on bilingual groups of words which denote the building blocks of state-of-the-art phrase-based SMT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9438661932945251}, {"text": "Human Language Technology and Pattern Recognition", "start_pos": 162, "end_pos": 211, "type": "TASK", "confidence": 0.5776545107364655}, {"text": "SMT", "start_pos": 436, "end_pos": 439, "type": "TASK", "confidence": 0.859664797782898}]}, {"text": "Due to data sparseness, statistical models are often trained on local context only.", "labels": [], "entities": []}, {"text": "Language models are derived from n-grams with n \u2264 5 and bilingual phrase pairs are extracted with lengths up to 10 words on the target side.", "labels": [], "entities": []}, {"text": "This captures the local dependencies of the data in detail and is responsible for the success of data-driven phrase-based approaches.", "labels": [], "entities": []}, {"text": "In this work, we will introduce anew statistical model based on lexicalized triplets (f, e, e ) which we will also refer to as cross-lingual triggers of the form (e, e \u2192 f ).", "labels": [], "entities": []}, {"text": "This can be understood as two words in one language triggering one word in another language.", "labels": [], "entities": []}, {"text": "These triplets, modeled by p(f |e, e ), are closely related to lexical translation probabilities based on the IBM model 1, i.e. p(f |e).", "labels": [], "entities": []}, {"text": "Several constraints and setups will be described later on in more detail, but as an introduction one can think of the following interpretation which is depicted in: Using a phrase-based MT approach, a source word f is triggered by its translation e which is part of the phrase being considered, whereas another target word e outside this phrase serves as an additional trigger in order to allow for more fine-grained distinction of a specific word sense.", "labels": [], "entities": [{"text": "MT", "start_pos": 186, "end_pos": 188, "type": "TASK", "confidence": 0.866318941116333}]}, {"text": "Thus, this cross-lingual trigger model can be seen as a combination of a lexicon model (i.e. f and e) and a model similar to monolingual longrange (i.e. distant bigram) trigger models (i.e. e and e , although these dependencies are reflected indirectly via e \u2192 f ) which uses both local (in-phrase) and global (in-sentence) information for the scoring.", "labels": [], "entities": []}, {"text": "The motivation behind this approach is to get nonlocal information outside the current context (i.e. the currently considered bilingual phrase pair) into the translation process.", "labels": [], "entities": []}, {"text": "The triplets are trained via the EM algorithm, as will be shown later in more detail.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the system setup used in this work, including the translation tasks and the corresponding training corpora.", "labels": [], "entities": []}, {"text": "The experiments are based on an n-best list reranking framework.", "labels": [], "entities": []}, {"text": "One of the first questions that arises is how many EM iterations should be carried out during training of the triplet model.", "labels": [], "entities": []}, {"text": "Since the IWSLT task is small, we can quickly run the experiments on a full unconstrained triplet model without any cutoff or further constraints.", "labels": [], "entities": []}, {"text": "shows the rescoring performance for different numbers of EM iterations.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.960239052772522}]}, {"text": "The first 10 iterations significantly improve the triplet model performance for the IWSLT task.", "labels": [], "entities": [{"text": "IWSLT task", "start_pos": 84, "end_pos": 94, "type": "TASK", "confidence": 0.6339398324489594}]}, {"text": "After that, there are no big changes.", "labels": [], "entities": []}, {"text": "The performance even degrades a little bit after 30 iterations.", "labels": [], "entities": []}, {"text": "For the IWSLT task, we therefore set a fixed number of 20 EM iterations for the following experiments since it shows a good performance in terms of both BLEU and TER score.", "labels": [], "entities": [{"text": "IWSLT task", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.4906468838453293}, {"text": "BLEU", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9995861649513245}, {"text": "TER score", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9830765426158905}]}, {"text": "The oracle TER scores of the 10k-best lists are 14.18% for IWSLT'04, 11.36% for IWSLT'05 and 18.85% for IWSLT'07, respectively.", "labels": [], "entities": [{"text": "TER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.983196496963501}, {"text": "IWSLT'04", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9184100031852722}, {"text": "IWSLT'05", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.9039351940155029}, {"text": "IWSLT'07", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.9131017923355103}]}, {"text": "The next chain of experiments on the IWSLT task investigates the impact of changes to the setup of training an unconstrained triplet model, such as the addition of the empty word and the inclusion of singletons (i.e. triplets that were only seen once in the  training data).", "labels": [], "entities": [{"text": "IWSLT task", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.5464915335178375}]}, {"text": "This might show the importance of rare events in order to derive strategies when moving to larger tasks where it is not feasible to train all possible triplets, such as e.g. on the EPPS task (as shown later) or the Chinese-English NIST task.", "labels": [], "entities": []}, {"text": "The results for the unconstrained model are shown in Table 1, beginning with a full triplet model in reverse direction, pall (e|f, f ), that contains no singletons and no empty words for the triggering side.", "labels": [], "entities": []}, {"text": "In this setting, singletons seem to help on dev but there is no clear improvement on one of the test sets, whereas empty words do not make a significant difference but can be used since they do not harm either.", "labels": [], "entities": []}, {"text": "The baseline can be improved by +0.6% BLEU and around -0.5% in TER on the IWSLT'04 set.", "labels": [], "entities": [{"text": "baseline", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9721390008926392}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9997616410255432}, {"text": "TER", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9990907907485962}, {"text": "IWSLT'04 set", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9555802941322327}]}, {"text": "For the various setups, there are no big differences in the TER score which might bean effect of optimization on BLEU.", "labels": [], "entities": [{"text": "TER score", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9744325876235962}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9954830408096313}]}, {"text": "Therefore, for further experiments using the constraints from Section 3.2, we use both singletons and empty words as the default.", "labels": [], "entities": []}, {"text": "Adding the other direction p(f |e, e ) results in another increase, with a total of +0.8% BLEU and -0.8% TER, which shows that the combination of both directions helps overall translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9995700716972351}, {"text": "TER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9992650151252747}]}, {"text": "The results on the two test sets are shown in.", "labels": [], "entities": []}, {"text": "As can be seen, we arrive at similar improvements, namely +0.6% BLEU and -0.3% TER on IWSLT'05 and +0.8% BLEU and -0.4% TER on IWSLT'07, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9990743398666382}, {"text": "TER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9910026788711548}, {"text": "IWSLT'05", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.960669219493866}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.998650848865509}, {"text": "TER", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9808486700057983}, {"text": "IWSLT'07", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9600895643234253}]}, {"text": "The constrained models, i.e. the phrasebounded (p phr ) and path-aligned (p align ) triplets are outperformed by the full unconstrained case, although on IWSLT'07 both unconstrained and pathaligned models are close.", "labels": [], "entities": [{"text": "IWSLT'07", "start_pos": 154, "end_pos": 162, "type": "DATASET", "confidence": 0.9477869272232056}]}, {"text": "For a fair comparison, we added a classical IBM model 1 in the rescoring framework.", "labels": [], "entities": []}, {"text": "It can be seen that the presented triplet models slightly outperform  the simple IBM model 1.", "labels": [], "entities": []}, {"text": "Note that IBM model 1 is a special case of the triplet lexicon model if the second trigger is the empty word.", "labels": [], "entities": []}, {"text": "Since EPPS is a considerably harder task (larger vocabulary and longer sentences), the training of a full unconstrained triplet model cannot be done due to memory restrictions.", "labels": [], "entities": []}, {"text": "One possibility to reduce the number of extracted triplets is to apply a maximum distance constraint in the training procedure, i.e. only trigger pairs are considered where the distance between first and second trigger is below or equal to the specified maximum.", "labels": [], "entities": []}, {"text": "shows the effect of a maximum distance constraint for the Spanish-English direction.", "labels": [], "entities": []}, {"text": "Due to the large amount of triplets (we extract roughly two billion triplets 2 for the EPPS data), we drop all triplets that occur less than 3 times which results in 640 million triplets.", "labels": [], "entities": [{"text": "EPPS data", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.9852862656116486}]}, {"text": "Also, due to time restrictions 3 , we only train 4 iterations and compare it to 4 iterations of the same setting with the maximum distance set to 10.", "labels": [], "entities": []}, {"text": "The training with the maximum distance constraints ends with a total of 380 million triplets.", "labels": [], "entities": []}, {"text": "As can be seen, the performance is comparable while cutting down the computation time from 9.2 to 3.1 hours.", "labels": [], "entities": []}, {"text": "The experiments were carried out on a 2.2GHz Opteron machine with 16 GB of memory.", "labels": [], "entities": []}, {"text": "The overall gain is +0.4-0.6% BLEU and up to -0.4% in TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.999653697013855}, {"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9906009435653687}]}, {"text": "We even observe a slight increase in BLEU for the TC-Star'07 set which might be a random effect due to optimization on the development set where the behavior is the same as for TC-Star'06.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9996187686920166}, {"text": "TC-Star'07 set", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9425625801086426}, {"text": "TC-Star'06", "start_pos": 177, "end_pos": 187, "type": "DATASET", "confidence": 0.9202802181243896}]}, {"text": "Results on EPPS English-Spanish for the phrasebounded triplet model are presented in.", "labels": [], "entities": [{"text": "EPPS", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.934328019618988}]}, {"text": "Since the number of triplets is less than for the unconstrained model, we can lower the cutoff from 3 to 2 (denoted in the table by occ 3 and occ 2 , respectively).", "labels": [], "entities": [{"text": "occ 3", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.9542082250118256}]}, {"text": "There is a small additional gain on the TCStar'07 test set by this step, with a total of +0.7% BLEU for TC-Star'06 and +0.8% BLEU for TCStar'07.", "labels": [], "entities": [{"text": "TCStar'07 test", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9029816389083862}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9994837045669556}, {"text": "TC-Star'06", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.9307335019111633}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.999381422996521}, {"text": "TCStar'07", "start_pos": 134, "end_pos": 143, "type": "DATASET", "confidence": 0.954468309879303}]}, {"text": "shows results fora variation of the pathaligned triplet model p align that restricts the first trigger to the best aligned word as estimated in the IBM model 1, thus using a maximum-approximation of the given word alignment.", "labels": [], "entities": [{"text": "IBM model 1", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.9365211327870687}]}, {"text": "The model was trained on two word alignments, firstly the one contained in the forced alignments on the training data, and secondly on an IBM-4 word alignment generated using GIZA++.", "labels": [], "entities": []}, {"text": "For this second model we also demonstrate the improvement obtained when increasing the triplet lexicon size by using less trimming.", "labels": [], "entities": []}, {"text": "Another experiment was carried out to investigate the effect of immediate neighboring words used as triggers within the p align setting.", "labels": [], "entities": []}, {"text": "This is equivalent to using a \"maximum distance of 1\" constraint.", "labels": [], "entities": []}, {"text": "We obtained worse results, namely a 0.2-0.3% drop in BLEU and a 0.3-0.4% raise in TER (cf.  eration using less than 2 GB of memory.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9992411136627197}, {"text": "TER", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9982300400733948}]}, {"text": "However, this shows that triggers outside the immediate context help overall translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.9682198166847229}]}, {"text": "Additionally, it supports the claim that the presented methods area complementary alternative to the WSD approaches mentioned in Section 2 which only consider the immediate context of a single word.", "labels": [], "entities": []}, {"text": "Finally, we compare the constrained models to an unconstrained setting and, again, to a standard IBM model 1.", "labels": [], "entities": []}, {"text": "shows that the p align model constrained on using the IBM-4 word alignments yields +0.7% in BLEU on TC-Star'06 which is +0.2% more than with a standard IBM model 1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9987080097198486}, {"text": "TC-Star'06", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.9096667170524597}]}, {"text": "TER decreases by -0.3% when compared to model 1.", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.99782794713974}]}, {"text": "For the TC-Star'07 set, the observations are similar.", "labels": [], "entities": [{"text": "TC-Star'07 set", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8612923324108124}]}, {"text": "The oracle TER scores of the development n-best list are 25.16% for English-Spanish and 27.0% for Spanish-English, respectively.", "labels": [], "entities": [{"text": "TER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9853357076644897}]}], "tableCaptions": [{"text": " Table 1: Different setups showing the effect of singletons  and empty words for IWSLT CE IWSLT'04 (dev) and  IWSLT'05 (test) sets, p all triplets, 20 EM iterations.", "labels": [], "entities": [{"text": "IWSLT CE IWSLT'04", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.7280117074648539}, {"text": "IWSLT'05 (test) sets", "start_pos": 110, "end_pos": 130, "type": "DATASET", "confidence": 0.8062052488327026}]}, {"text": " Table 2: Comparison of triplet variants on IWSLT CE test  sets, 20 EM iterations, with singletons and empty words.", "labels": [], "entities": [{"text": "IWSLT CE test  sets", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.9713004380464554}]}, {"text": " Table 3: Effect of using maximum distance constraint for  p all on EPPS Spanish-English test sets, occ 3 , 4 EM iter- ations due to time constraints.", "labels": [], "entities": [{"text": "EPPS Spanish-English test sets", "start_pos": 68, "end_pos": 98, "type": "DATASET", "confidence": 0.9484748542308807}, {"text": "EM iter- ations", "start_pos": 110, "end_pos": 125, "type": "METRIC", "confidence": 0.950906291604042}]}, {"text": " Table 4: Results on EPPS, English-Spanish, p phr com- bined, occ 3 , 10 EM iterations.", "labels": [], "entities": [{"text": "EPPS", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.9008758068084717}]}, {"text": " Table 5: Results on EPPS, English-Spanish, maximum  approximation, p align combined, occ 3 , 10 EM iterations.", "labels": [], "entities": [{"text": "EPPS", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8085846304893494}]}, {"text": " Table 6: Final results on EPPS English-Spanish, con- strained triplet models, 10 EM iterations, compared to  standard IBM model 1.", "labels": [], "entities": [{"text": "EPPS", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8772338032722473}]}, {"text": " Table 7: Example of triplets and related IBM model 1  lexical probabilities. The triggers \"taxpayer\" and \"bill\"  have a new effect (\"pagar\"), previously not seen in the  top ranks of the lexicon.", "labels": [], "entities": []}]}