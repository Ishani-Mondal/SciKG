{"title": [{"text": "Scaling Textual Inference to the Web", "labels": [], "entities": [{"text": "Scaling Textual Inference to the Web", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8770816624164581}]}], "abstractContent": [{"text": "Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question.", "labels": [], "entities": []}, {"text": "These systems are helpless if the answer has to be inferred from multiple sentences , possibly on different pages.", "labels": [], "entities": []}, {"text": "To solve this problem, we introduce the HOLMES system , which utilizes textual inference (TI) over tuples extracted from text.", "labels": [], "entities": []}, {"text": "Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, HOLMES utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages.", "labels": [], "entities": [{"text": "textual entailment)", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7930674354235331}]}, {"text": "Given only a few minutes, HOLMES doubles recall for example queries in three disparate domains (geography, business, and nutrition).", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9987064599990845}]}, {"text": "Importantly, HOLMES's runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus-they are \"approximately\" functional in a well-defined sense.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We tested HOLMES on 183 million distinct ground assertions extracted from the Web by the TEX-TRUNNER system (, coupled with 159 thousand ground assertions from WordNet (, and a compact set of handcoded inference rules.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.953329861164093}]}, {"text": "Given a total of 55 to 145 seconds, HOLMES was able to produce high-quality inferences that doubled the number of answers to example queries in three disparate domains: geography, business, and nutrition.", "labels": [], "entities": []}, {"text": "We also evaluated how the speed of HOLMES scaled with the size of its input corpus.", "labels": [], "entities": []}, {"text": "In the general case, logical inference over a Horn theory (needed in order to produce the probabilistic network) is polynomial in the number of ground assertions, and hence in the size of the textual corpus.", "labels": [], "entities": []}, {"text": "1 Unfortunately, this is prohibitive, since even loworder polynomial growth is fatal on a 117 millionpage corpus, let alone the full Web.", "labels": [], "entities": []}, {"text": "This section reports on measurements that confirm that linear scaling with |A| occurs in practice, and that HOLMES's inference is not only scalable but also improves precision/recall on sample queries in a diverse set of domains.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9981020092964172}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.971601128578186}]}, {"text": "After describing the experimental domains and queries, Section 4.2 reports on the boost to the area under the precision/recall curve fora set of example queries in three domains: geography, business, and nutrition.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9985494017601013}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.7870686054229736}]}, {"text": "Section 4.3 then shows that APF relations are very common in the Web corpus, and finally Section 4.4 demonstrates empirically that HOLMES's inference time scales linearly with the number of pages in the corpus.", "labels": [], "entities": []}, {"text": "HOLMES utilized two knowledge bases in these experiments: TEXTRUNNER and WordNet.", "labels": [], "entities": [{"text": "TEXTRUNNER", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.6704127788543701}, {"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9710383415222168}]}, {"text": "TEX-TRUNNER contains approximately 183 million distinct ground assertions extracted from over 117 million web pages, and WordNet contains 159 thousand manually created IS-A, Part-Of, and Synonym assertions.", "labels": [], "entities": [{"text": "TEX-TRUNNER", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.659010112285614}, {"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9465631246566772}]}, {"text": "In all queries, HOLMES utilizes the domainindependent inference rules described in Section 2.3.", "labels": [], "entities": []}, {"text": "HOLMES additionally makes use of two domain-specific inference rules in the Nutrition domain, to demonstrate the benefits of including domain-specific information.", "labels": [], "entities": [{"text": "HOLMES", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8708304762840271}]}, {"text": "Estimating the precision and relative recall of HOLMES requires extensive and careful manual tagging of HOLMES output.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9991693496704102}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9796226620674133}]}, {"text": "To make this feasible, we restricted ourselves to a set of twenty queries in three domains, but made the domains diverse to illustrate the broad scope of the system.", "labels": [], "entities": []}, {"text": "We now describe each domain briefly.", "labels": [], "entities": []}, {"text": "Geography: the query issued is: \"Who was born in one of the following countries?\"", "labels": [], "entities": []}, {"text": "More formally, Q(X) :-BornIn(X,{country}) where {country} is bound to each of the following nine countries in turn {France, Germany, China, Thailand, Kenya, Morocco, Peru, Columbia, Guatemala}, yielding a total of nine queries.", "labels": [], "entities": []}, {"text": "Because Web text often refers to a person's birth city rather than birth country, this query illustrates how combining an ground assertion (e.g., BornIn(Alberto Fujimori, Lima)) with background knowledge (e.g., LocatedIn(Lima, Peru)) enables the system to draw new conclusions (e.g., BornIn(Alberto Fujimori, Peru)).", "labels": [], "entities": [{"text": "BornIn", "start_pos": 284, "end_pos": 290, "type": "DATASET", "confidence": 0.9583641886711121}]}, {"text": "Business: we issued the following two queries.", "labels": [], "entities": []}, {"text": "1) Which companies are acquiring software companies?", "labels": [], "entities": []}, {"text": "Formally, Q(X) :-Acquired(X, Y) \u2227 Develops(Y, 'software') This query tests HOLMES's ability to scalably join a large number of assertions from multiple pages.", "labels": [], "entities": []}, {"text": "2) Which companies are headquartered in the USA?", "labels": [], "entities": []}, {"text": "Answering this query comprehensively requires HOLMES to combine a join (over the relations HeadquarteredIn and IS-A) with transitive inference on PartOf (e.g., Seattle is PartOf Washington which is PartOf the USA) and on IS-A (e.g., Microsoft IS-A software company which IS-A company).", "labels": [], "entities": [{"text": "HeadquarteredIn", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.9351359605789185}]}, {"text": "The IS-A assertions came from both TEXTRUNNER (using patterns from) and WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9785023927688599}]}], "tableCaptions": [{"text": " Table 1: Improvement in the AuC of HOLMES over the  BASELINE and total inference time taken by HOLMES.  Results are summed over all queries in the geography,  business, and nutrition domains. Inference time mea- sured on unoptimized prototype.", "labels": [], "entities": [{"text": "AuC", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9728096723556519}, {"text": "BASELINE", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9643967151641846}]}]}