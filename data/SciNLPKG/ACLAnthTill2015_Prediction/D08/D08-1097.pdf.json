{"title": [{"text": "Question Classification using Head Words and their Hypernyms", "labels": [], "entities": [{"text": "Question Classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7188234180212021}]}], "abstractContent": [{"text": "Question classification plays an important role in question answering.", "labels": [], "entities": [{"text": "Question classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7867636680603027}, {"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9166936874389648}]}, {"text": "Features are the key to obtain an accurate question classifier.", "labels": [], "entities": []}, {"text": "In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set.", "labels": [], "entities": []}, {"text": "In particular, we propose headword feature and present two approaches to augment semantic features of such head words using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 124, "end_pos": 131, "type": "DATASET", "confidence": 0.9746568202972412}]}, {"text": "In addition, Lesk's word sense disambigua-tion (WSD) algorithm is adapted and the depth of hypernym feature is optimized.", "labels": [], "entities": []}, {"text": "With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.", "labels": [], "entities": [{"text": "Maximum Entropy (ME)", "start_pos": 85, "end_pos": 105, "type": "METRIC", "confidence": 0.8810901999473572}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9993481040000916}, {"text": "accuracy", "start_pos": 245, "end_pos": 253, "type": "METRIC", "confidence": 0.9855723977088928}]}], "introductionContent": [{"text": "An important step in question answering (QA) and other dialog systems is to classify the question to the anticipated type of the answer.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8866866588592529}]}, {"text": "For example, the question of Who discovered x-rays should be classified into the type of human (individual).", "labels": [], "entities": []}, {"text": "This information would narrow down the search space to identify the correct answer string.", "labels": [], "entities": []}, {"text": "In addition, this information can suggest different strategies to search and verify a candidate answer.", "labels": [], "entities": []}, {"text": "For instance, the classification of question What is autism to a definition type question would trigger the search strategy specific for definition type (e.g., using predefined templates like: Autism is ... or Autism is defined as...).", "labels": [], "entities": []}, {"text": "In fact, the combination of QA and the named entity recognition is a key approach in modern question answering systems.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7295014063517252}, {"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.9015607833862305}]}, {"text": "The question classification is by no means trivial: Simply using question wh-words cannot achieve satisfactory results.", "labels": [], "entities": [{"text": "question classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7707362174987793}]}, {"text": "The difficulty lies in classifying the what and which type questions.", "labels": [], "entities": []}, {"text": "Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type.", "labels": [], "entities": []}, {"text": "Considering also examples () What tourist attractions are therein Reims, What are the names of the tourist attractions in Reims, What do most tourists visit in Reims, What attracts tourists to Reims, and What is worth seeing in Reims, all these reformulations are of the same answer type of location.", "labels": [], "entities": []}, {"text": "Different wording and syntactic structures make it difficult for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.9651346802711487}]}, {"text": "Many QA systems used manually constructed sets of rules to map a question to a type, which is not efficient in maintain and upgrading.", "labels": [], "entities": []}, {"text": "With the increasing popularity of statistical approaches, machine learning plays a more and more important role in this task.", "labels": [], "entities": []}, {"text": "A salient advantage of machine learning approach is that one can focus on designing insightful features, and rely on learning process to efficiently and effectively cope with the features.", "labels": [], "entities": []}, {"text": "In addition, a learned classifier is more flexible to reconstruct than a manually constructed system because it can be trained on anew taxonomy in a very short time.", "labels": [], "entities": []}, {"text": "Earlier question classification work includes and, in which language model and Rappier rule learning were employed respectively.", "labels": [], "entities": [{"text": "question classification", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.8568700551986694}]}, {"text": "More recently, have developed a machine learning approach which uses the SNoW learning architecture ().", "labels": [], "entities": []}, {"text": "They have compiled the UIUC question classification dataset which consists of 5500 training and 500 test questions.", "labels": [], "entities": [{"text": "UIUC question classification dataset", "start_pos": 23, "end_pos": 59, "type": "DATASET", "confidence": 0.8178246170282364}]}, {"text": "The questions in this dataset are collected from four sources: 4,500 English questions published by USC (), about 500 manually constructed questions fora few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serve as the test dataset.", "labels": [], "entities": [{"text": "USC", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.7084915637969971}]}, {"text": "All questions in the dataset have been manually labeled by them according to the coarse and fine grained categories as shown in, with coarse classes (in bold) followed by their fine class refinements.", "labels": [], "entities": []}, {"text": "In addition, the table shows the distribution of the 500 test questions over such categories.", "labels": [], "entities": []}, {"text": "have made use of lexical words, part of speech tags, chunks (non-overlapping phrases), head chunks (the first noun chunk in a question) and named entities.", "labels": [], "entities": []}, {"text": "They achieved 78.8% accuracy for 50 fine grained classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9995005130767822}]}, {"text": "With a handbuilt dictionary of semantically related words, their system is able to reach 84.2%.", "labels": [], "entities": []}, {"text": "The UIUC dataset has laid a platform for the follow-up research.", "labels": [], "entities": [{"text": "UIUC dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9906635582447052}]}, {"text": "used linear support vector machines with question word bigrams and error-correcting output to obtain accuracy of 80.2% to 82.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9995552897453308}]}, {"text": "used linear SVMs with all possible question word grams, and obtained accuracy of 79.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9997205138206482}]}, {"text": "Later used more semantic information sources including named entities, WordNet senses, class-specific related words, and distributional similarity based categories in question classification task.", "labels": [], "entities": [{"text": "question classification", "start_pos": 167, "end_pos": 190, "type": "TASK", "confidence": 0.7862657308578491}]}, {"text": "With all these semantic features plus the syntactic ones, their model was trained on 21'500 questions and was able to achieve the best accuracy of 89.3% on a test set of 1000 questions (taken from TREC 10 and TREC 11) for 50 fine classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9992932081222534}, {"text": "TREC", "start_pos": 197, "end_pos": 201, "type": "DATASET", "confidence": 0.8903858065605164}, {"text": "TREC", "start_pos": 209, "end_pos": 213, "type": "DATASET", "confidence": 0.7276358008384705}]}, {"text": "Most recently, used a short (typically one to three words) subsequence of question tokens as features for question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.8027876317501068}]}, {"text": "Their model can reach the accuracy of 86.2% using UIUC dataset over fine grained question categories, which is the highest reported accuracy on UIUC dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.99953293800354}, {"text": "UIUC dataset", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9867068231105804}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9599153399467468}, {"text": "UIUC dataset", "start_pos": 144, "end_pos": 156, "type": "DATASET", "confidence": 0.9880608916282654}]}, {"text": "In contrast to's approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set.", "labels": [], "entities": []}, {"text": "In particular, we propose headword feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 182, "end_pos": 189, "type": "DATASET", "confidence": 0.9693191647529602}]}, {"text": "In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.7123323827981949}]}, {"text": "With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9996078610420227}, {"text": "ME", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9289504289627075}]}], "datasetContent": [{"text": "We designed two experiments to test the accuracy of our classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9988327622413635}]}, {"text": "The first experiment evaluates the individual contribution of different feature types to question classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9326373934745789}]}, {"text": "In particular, the SVM and ME are trained from the UIUC 5500 training data using the following feature sets: 1) whword + headword, 2) wh-word + headword + direct hypernym, 3) wh-wod + headword + indirect hypernym, 4) unigram, 5) bigram, 6) trigram, and 7) word shape.", "labels": [], "entities": [{"text": "UIUC 5500 training data", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.9721122831106186}]}, {"text": "We setup the tests of 1), 2) and 3) due to the fact that wh-word and headword can be treated as a unit, and hypernym depends on headword.", "labels": [], "entities": []}, {"text": "In the second experiment, feature sets are incrementally feeded to the SVM and ME.", "labels": [], "entities": [{"text": "SVM", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8305652737617493}, {"text": "ME", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.7333804965019226}]}, {"text": "The parameters for both SVM and ME classifiers (e.g., the C in the SVM) are all with the default values.", "labels": [], "entities": []}, {"text": "In order to facilitate the comparison with previously reported results, the question classification performance is measured by accuracy, i.e., the proportion of the correctly classified questions among all test questions.", "labels": [], "entities": [{"text": "question classification", "start_pos": 76, "end_pos": 99, "type": "TASK", "confidence": 0.7119923532009125}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9989830851554871}]}, {"text": "shows the question classification accuracy of SVM and ME using individual feature sets for 6 coarse and 50 fine classes.", "labels": [], "entities": [{"text": "question classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7496304214000702}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9650169014930725}, {"text": "ME", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.7511549592018127}]}, {"text": "Among all feature sets, wh-word + headword proves to be very informative for question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7771022617816925}]}, {"text": "Our first WordNet semantic feature augment, the inclusion of direct hypernym, can further boost the accuracy in the fine classes for both SVM and ME, up to four per- cent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9993627667427063}]}, {"text": "This phenomena conforms to that WordNet hypernym benefits mainly on the 50 fine classes classification.", "labels": [], "entities": [{"text": "WordNet hypernym", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.9421132206916809}]}, {"text": "made use of semantic features including named entities, WordNet senses, class-specific related words, and distributional similarity based categories.", "labels": [], "entities": []}, {"text": "Their system managed to improve around 4 percent with the help of those semantic features.", "labels": [], "entities": []}, {"text": "They reported that WordNet didn't contribute much to the system, while our results show that the WordNet significantly boosts the accuracy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9480059742927551}, {"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.944366991519928}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9989755153656006}]}, {"text": "The reason maybe that their system expanded the hypernyms for each word in the question, while ours only expanded the headword.", "labels": [], "entities": []}, {"text": "In doing so, the augmentation does not introduce much noisy information.", "labels": [], "entities": []}, {"text": "Notice that the inclusion of various depth of hypernyms results in different accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.999471127986908}]}, {"text": "The depth of six brings the highest accuracy of 85.4% and 85.6% for SVM and ME under 50 classes, which is very competitive to the previously reported best accuracy of 86.2% ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995784163475037}, {"text": "SVM and ME under 50", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.5903849482536316}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9971531629562378}]}], "tableCaptions": [{"text": " Table 1: Question classification accuracy of SVM and  ME using individual feature sets for 6 and 50 classes over  UIUC dataset", "labels": [], "entities": [{"text": "Question classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8288586735725403}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9522994756698608}, {"text": "ME", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.7770907878875732}, {"text": "UIUC dataset", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.9762794673442841}]}, {"text": " Table 3: Precision and recall for fine grained question  categories", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9892948269844055}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9988443851470947}]}, {"text": " Table 2: Question classification accuracy of SVM and ME using incremental feature sets for 6 and 50 classes", "labels": [], "entities": [{"text": "Question classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8214430809020996}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.970888078212738}, {"text": "ME", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.8696801066398621}]}, {"text": " Table 4: Classification accuracy of all models which were  applied to UIUC dataset", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.5305368900299072}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.5530961751937866}, {"text": "UIUC dataset", "start_pos": 71, "end_pos": 83, "type": "DATASET", "confidence": 0.9272243082523346}]}]}