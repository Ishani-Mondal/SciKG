{"title": [{"text": "Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems", "labels": [], "entities": [{"text": "Indirect-HMM-based Hypothesis Alignment", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5696286459763845}, {"text": "Machine Translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.694982185959816}]}], "abstractContent": [{"text": "This paper presents anew hypothesis alignment method for combining outputs of multiple machine translation (MT) systems.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.8276115536689759}]}, {"text": "An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment.", "labels": [], "entities": [{"text": "synonym matching", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.7988680899143219}, {"text": "word ordering", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.7037641555070877}, {"text": "hypothesis alignment", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.676646888256073}]}, {"text": "Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.", "labels": [], "entities": []}, {"text": "The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets.", "labels": [], "entities": [{"text": "TER-based alignment", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.6938049793243408}, {"text": "NIST benchmark datasets", "start_pos": 117, "end_pos": 140, "type": "DATASET", "confidence": 0.9189313650131226}]}, {"text": "Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9922365546226501}, {"text": "Chinese-to-English translation", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.5667200833559036}, {"text": "NIST Open MT Evaluation", "start_pos": 152, "end_pos": 175, "type": "DATASET", "confidence": 0.7320948094129562}]}], "introductionContent": [], "datasetContent": [{"text": "In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation.", "labels": [], "entities": [{"text": "IHMM-based hypothesis alignment", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.7203480203946432}, {"text": "Chinese-toEnglish (C2E) test", "start_pos": 79, "end_pos": 107, "type": "DATASET", "confidence": 0.7614975810050965}, {"text": "NIST Open MT Evaluation", "start_pos": 154, "end_pos": 177, "type": "DATASET", "confidence": 0.723386749625206}]}, {"text": "We compare to the TER-based method used by.", "labels": [], "entities": [{"text": "TER-based", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9601123332977295}]}, {"text": "In the following experiments, the NIST BLEU score is used as the evaluation metric (), which is reported as a percentage in the following sections.", "labels": [], "entities": [{"text": "NIST", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.5762908458709717}, {"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9581358730792999}]}, {"text": "In our main experiments, outputs from a total of eight single MT systems were combined.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9608601331710815}]}, {"text": "As listed in, Sys-1 is a tree-to-string system proposed by; Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by; Sys-4 is a syntax-based preordering system proposed by; Sys-5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by; Sys-7 is a twopass phrase-based system with adapted LM proposed by; and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by.", "labels": [], "entities": []}, {"text": "All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation.", "labels": [], "entities": [{"text": "NIST MT08 evaluation", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.8488382895787557}]}, {"text": "These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data.", "labels": [], "entities": [{"text": "NIST MT test data", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.9001282751560211}]}, {"text": "The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by, which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training condition of MT08.", "labels": [], "entities": [{"text": "MT08", "start_pos": 258, "end_pos": 262, "type": "DATASET", "confidence": 0.9429438710212708}]}], "tableCaptions": [{"text": " Table 1. Results of single and combined systems  on the dev set and the MT08 test set  System  Dev  ciBLEU%", "labels": [], "entities": [{"text": "MT08 test set  System  Dev  ciBLEU", "start_pos": 73, "end_pos": 107, "type": "DATASET", "confidence": 0.9451092878977457}]}, {"text": " Table 3. It shows that the IHMM- based method is still about 1 BLEU point better  than the TER-based method. Moreover, combining  15 single systems gives an output that has a NIST  BLEU score of 34.82%, which is 3.9 points better  than the best submission to the NIST MT08  constrained training track", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9986447691917419}, {"text": "TER-based", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.889830470085144}, {"text": "NIST", "start_pos": 176, "end_pos": 180, "type": "DATASET", "confidence": 0.6027082800865173}, {"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9647974371910095}, {"text": "NIST MT08  constrained training track", "start_pos": 264, "end_pos": 301, "type": "DATASET", "confidence": 0.8590654253959655}]}, {"text": " Table 2. Results of seven additional single systems  on the NIST MT08 test set  System  MT08  BLEU%  System 9  29.59  System 10  29.57  System 11  29.64  System 12  29.85  System 13  25.53  System 14  26.04  System 15  29.70", "labels": [], "entities": [{"text": "NIST MT08 test set", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.8768123686313629}, {"text": "MT08", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.6036443114280701}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9759334921836853}]}, {"text": " Table 3. Results of the 15-way system combination  on the NIST MT08 C2E test set  Sys. Comb. MT08  BLEU%  TER  33.81  IHMM  34.82", "labels": [], "entities": [{"text": "NIST MT08 C2E test set  Sys. Comb. MT08", "start_pos": 59, "end_pos": 98, "type": "DATASET", "confidence": 0.9167262554168701}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9564104080200195}, {"text": "TER  33.81  IHMM  34.82", "start_pos": 107, "end_pos": 130, "type": "METRIC", "confidence": 0.6154185459017754}]}, {"text": " Table 4. In one extreme  case, \u03b1 = 1, the overall similarity model is based  only on semantic similarity. This gives a case  insensitive BLEU score of 41.70% and a case  sensitive BLEU score of 28.92% on the dev and  test set, respectively. The accuracy is significantly  improved to 43.62% on the dev set and 30.89% on  test set when \u03b1 = 0.3. In another extreme case, \u03b1 = 0, in which only the surface similarity model is  used for the overall similarity model, the  performance degrades by about 0.2 point.  Therefore, the surface similarity information seems  more important for monolingual hypothesis  alignment, but both sub-models are useful.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 138, "end_pos": 148, "type": "METRIC", "confidence": 0.9803646504878998}, {"text": "BLEU", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9664384126663208}, {"text": "accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.9994192123413086}, {"text": "monolingual hypothesis  alignment", "start_pos": 582, "end_pos": 615, "type": "TASK", "confidence": 0.6920555432637533}]}, {"text": " Table 4. Effect of the similarity model  Dev  ciBLEU%", "labels": [], "entities": []}, {"text": " Table 5. Effect of the distortion model  Dev  ciBLEU%", "labels": [], "entities": []}]}