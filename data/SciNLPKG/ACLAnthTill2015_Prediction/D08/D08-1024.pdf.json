{"title": [{"text": "Online Large-Margin Training of Syntactic and Structural Translation Features", "labels": [], "entities": [{"text": "Syntactic and Structural Translation", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.6118211522698402}]}], "abstractContent": [{"text": "Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize.", "labels": [], "entities": [{"text": "Minimum-error-rate training (MERT)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7574097514152527}, {"text": "statistical machine translation", "start_pos": 78, "end_pos": 109, "type": "TASK", "confidence": 0.6949976682662964}]}, {"text": "Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9301930069923401}, {"text": "MERT", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.45403245091438293}]}, {"text": "We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.8951550126075745}, {"text": "MERT", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.8414520621299744}]}, {"text": "We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model.", "labels": [], "entities": []}, {"text": "In both cases we obtain significant improvements in translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9728416204452515}]}, {"text": "Optimizing them in combination, fora total of 56 feature weights, we improve performance by 2.6 B\uf76c\uf765\uf775 on a subset of the NIST 2006 Arabic-English evaluation data.", "labels": [], "entities": [{"text": "NIST 2006 Arabic-English evaluation data", "start_pos": 120, "end_pos": 160, "type": "DATASET", "confidence": 0.9645594835281373}]}], "introductionContent": [{"text": "Since its introduction by, minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems.", "labels": [], "entities": [{"text": "minimum error rate training (MERT", "start_pos": 27, "end_pos": 60, "type": "METRIC", "confidence": 0.8075483540693918}, {"text": "statistical machine translation (MT)", "start_pos": 99, "end_pos": 135, "type": "TASK", "confidence": 0.7755634784698486}]}, {"text": "However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features.", "labels": [], "entities": [{"text": "MERT", "start_pos": 9, "end_pos": 13, "type": "TASK", "confidence": 0.5919933319091797}]}, {"text": "One recent example of this limitation is a series of experiments by, in which they added syntactic features to Hiero, which ordinarily uses no linguistically motivated syntactic information.", "labels": [], "entities": []}, {"text": "Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence.", "labels": [], "entities": []}, {"text": "They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently.", "labels": [], "entities": []}, {"text": "Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features.", "labels": [], "entities": [{"text": "MERT", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7324100732803345}]}, {"text": "But it would have been preferable to use a training method that can optimize the features all at once.", "labels": [], "entities": []}, {"text": "There has been much work on improving MERT's performance, or on replacing MERT wholesale (.", "labels": [], "entities": [{"text": "MERT", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.807441771030426}]}, {"text": "This paper continues a line of research on online discriminative training (, extending that of, who use the Margin Infused Relaxed Algorithm (MIRA) due to).", "labels": [], "entities": [{"text": "Margin Infused Relaxed Algorithm (MIRA)", "start_pos": 108, "end_pos": 147, "type": "TASK", "confidence": 0.5259781181812286}]}, {"text": "Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.731003999710083}]}, {"text": "Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.812009334564209}]}, {"text": "First, we generalize Marton and soft syntactic constraints by training all of them simultaneously; and, second, we introduce a novel structural distortion model.", "labels": [], "entities": []}, {"text": "We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined.", "labels": [], "entities": []}, {"text": "The paper proceeds as follows.", "labels": [], "entities": []}, {"text": "We describe our training algorithm in section 2; our generalization of Marton and Resnik's soft syntactic constraints in section 3; our novel structural distortion features in section 4; and experimental results in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now describe our experiments to test MIRA and our features, the soft-syntactic constraints and the structural distortion features, on an Arabic-English translation task.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.4514327347278595}]}, {"text": "It is worth noting that this experimentation is on a larger scale than, and considerably larger than.", "labels": [], "entities": []}, {"text": "The baseline model was Hiero with the following baseline features): \u2022 two language models \u2022 phrase translation probabilities p( f | e) and p(e | f ) \u2022 lexical weighting in both directions ( \u2022 word penalty \u2022 penalties for: -automatically extracted rules -identity rules (translating a word into itself) -two classes of number/name translation rules -glue rules The probability features are base-100 logprobabilities.", "labels": [], "entities": [{"text": "phrase translation probabilities", "start_pos": 92, "end_pos": 124, "type": "TASK", "confidence": 0.7677850325902303}, {"text": "number/name translation", "start_pos": 318, "end_pos": 341, "type": "TASK", "confidence": 0.6805898994207382}]}, {"text": "The rules were extracted from all the allowable parallel text from the NIST 2008 evaluation (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions).", "labels": [], "entities": [{"text": "NIST 2008 evaluation", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9537540475527445}]}, {"text": "Hierarchical rules were extracted from the most in-domain corpora (4.2+5.4 million words) and phrases were extracted from the remainder.", "labels": [], "entities": []}, {"text": "We trained the coarse-grained distortion model on 10,000 sentences of the training data.", "labels": [], "entities": []}, {"text": "Two language models were trained, one on data similar to the English side of the parallel text and one on 2 billion words of English.", "labels": [], "entities": []}, {"text": "Both were 5-gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants (2008) but using minimal perfect hashing ().", "labels": [], "entities": []}, {"text": "We partitioned the documents of the NIST 2004 (newswire) and 2005 Arabic-English evaluation data into a tuning set (1178 sentences) and a development set (1298 sentences).", "labels": [], "entities": [{"text": "NIST 2004 (newswire) and 2005 Arabic-English evaluation data", "start_pos": 36, "end_pos": 96, "type": "DATASET", "confidence": 0.9231678724288941}]}, {"text": "The test data was the NIST 2006 Arabic-English evaluation data (NIST part, newswire and newsgroups, 1529 sentences).", "labels": [], "entities": [{"text": "NIST 2006 Arabic-English evaluation data", "start_pos": 22, "end_pos": 62, "type": "DATASET", "confidence": 0.9482161164283752}, {"text": "NIST part", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.9223745465278625}]}, {"text": "To obtain syntactic parses for this data, we tokenized it according to the Arabic Treebank standard using AMIRA (), parsed it with the Stanford parser (, and then forced the trees back into the MT system's tokenization.", "labels": [], "entities": [{"text": "Arabic Treebank standard", "start_pos": 75, "end_pos": 99, "type": "DATASET", "confidence": 0.903176744778951}, {"text": "AMIRA", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.8998311758041382}]}, {"text": "We ran both MERT and MIRA on the tuning set using 20 parallel processors.", "labels": [], "entities": [{"text": "MERT", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9551717638969421}, {"text": "MIRA", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9929906129837036}]}, {"text": "We stopped MERT when the score on the tuning set stopped increasing, as is common practice, and for MIRA, we used the development set to decide when to stop training.", "labels": [], "entities": [{"text": "MERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9722796082496643}, {"text": "MIRA", "start_pos": 100, "end_pos": 104, "type": "TASK", "confidence": 0.5357171297073364}]}, {"text": "In our runs, MERT took an average of 9 passes through the tuning set and MIRA took an average of 8 passes.", "labels": [], "entities": [{"text": "MERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9290082454681396}, {"text": "MIRA", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.975172221660614}]}, {"text": "(For comparison, Watanabe et al. report decoding their tuning data of 663 sentences 80 times.) shows the results of our experiments with the training methods and features described above.", "labels": [], "entities": []}, {"text": "All significance testing was performed against the first line (MERT baseline) using paired bootstrap resampling.", "labels": [], "entities": [{"text": "MERT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.825648307800293}]}], "tableCaptions": []}