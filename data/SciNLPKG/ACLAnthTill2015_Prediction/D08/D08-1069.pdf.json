{"title": [{"text": "Specialized models and ranking for coreference resolution", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.978520005941391}]}], "abstractContent": [{"text": "This paper investigates two strategies for improving coreference resolution: (1) training separate models that specialize in particular types of mentions (e.g., pronouns versus proper nouns) and (2) using a ranking loss function rather than a classification function.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.9675150811672211}]}, {"text": "In addition to being conceptually simple, these modifications of the standard single-model, classification-based approach also deliver significant performance improvements.", "labels": [], "entities": []}, {"text": "Specifically , we show that on the ACE corpus both strategies produce f-score gains of more than 3% across the three coreference evaluation metrics (MUC, B 3 , and CEAF).", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9036853313446045}, {"text": "MUC", "start_pos": 149, "end_pos": 152, "type": "DATASET", "confidence": 0.746344804763794}, {"text": "B 3", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.8138728141784668}]}], "introductionContent": [{"text": "Coreference resolution is the task of partitioning a set of entity mentions in a text, where each partition corresponds to some entity in an underlying discourse model.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9270623624324799}]}, {"text": "While early machine learning approaches for the task relied on local, discriminative classifiers (), more recent approaches use joint and/or global models (.", "labels": [], "entities": []}, {"text": "This shift improves performance, but the systems are considerably more complex and often less efficient.", "labels": [], "entities": []}, {"text": "Here, we explore two simple modifications of the first type of approach that yield performance gains which are comparable, and sometimes better, to those obtained with these more complex systems.", "labels": [], "entities": []}, {"text": "These modifications involve: (i) the use of rankers instead of classifiers, and (ii) the use of linguistically motivated, specialized models for different types of mentions.", "labels": [], "entities": []}, {"text": "Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classificationbased approaches.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7814630270004272}]}, {"text": "In essence, ranking models directly capture during training the competition among potential antecedent candidates, instead of considering them independently.", "labels": [], "entities": []}, {"text": "This gives the ranker additional discriminative power and in turn better antecedent selection accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9900637269020081}]}, {"text": "Here, we show that ranking is also effective for the wider task of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.974177747964859}]}, {"text": "Coreference resolution involves several different types of anaphoric expressions: third-person pronouns, speech pronouns (i.e., first and second person pronouns), proper names, definite descriptions and other types of nominals (e.g., anaphoric uses of indefinite, quantified, and bare noun phrases).", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9259060323238373}]}, {"text": "Different anaphoric expressions exhibit different patterns of resolution and are sensitive to different factors), yet most machine learning approaches have ignored these differences and handle these different phenomena with a single, monolithic model.", "labels": [], "entities": []}, {"text": "A few exceptions are worth noting. and propose different classifiers models for different NPs for coreference resolution and pronoun resolution, respectively.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.9551815688610077}, {"text": "pronoun resolution", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.7018734663724899}]}, {"text": "Other partially capture the differential preferences between different anaphors via different sample selection strategies during training ().", "labels": [], "entities": []}, {"text": "More recently, use the distinction between pronouns, nominals and proper nouns in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.9753628373146057}, {"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9901484251022339}]}, {"text": "Here, we show that using specialized models for different types of referential expressions improves performance for supervised models (both classifiers and rankers).", "labels": [], "entities": []}, {"text": "Both these strategies lead to improvements for all three standard coreference metrics:, B 3 (Bagga and, and CEAF ().", "labels": [], "entities": [{"text": "B 3", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9436377584934235}, {"text": "CEAF", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.7770520448684692}]}, {"text": "In particular, our specialized ranker system provides absolute f -score improvements against an otherwise identical standard classifier system by 3.2%, 3.1%, and 3.6% for MUC, B 3 , and CEAF, respectively.", "labels": [], "entities": [{"text": "absolute f -score", "start_pos": 54, "end_pos": 71, "type": "METRIC", "confidence": 0.8167653754353523}, {"text": "MUC", "start_pos": 171, "end_pos": 174, "type": "DATASET", "confidence": 0.8026757836341858}, {"text": "CEAF", "start_pos": 186, "end_pos": 190, "type": "DATASET", "confidence": 0.8878601789474487}]}], "datasetContent": [{"text": "We use the ACE corpus (Phase 2).", "labels": [], "entities": [{"text": "ACE corpus", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.9727688729763031}]}, {"text": "The corpus has three parts, each corresponding to a different genre: newspaper texts (NPAPER), newswire texts (NWIRE), and broadcast news (BNEWS).", "labels": [], "entities": []}, {"text": "Each set is split into a train part and a devtest part.", "labels": [], "entities": []}, {"text": "In our experiments, we consider only true ACE mentions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of the different anaphors in ACE", "labels": [], "entities": [{"text": "ACE", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.5678814053535461}]}, {"text": " Table 6: Recall (R), Precision (P), and f -score (F) results on the entire ACE corpus using the MUC, B 3 , and CEAF  metrics. Note that R=P=F for CEAF when using true mentions, as we do here.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9341267347335815}, {"text": "Precision (P)", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9465974420309067}, {"text": "f -score (F)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9808884163697561}, {"text": "ACE corpus", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.935968667268753}, {"text": "MUC", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.9178242683410645}, {"text": "CEAF", "start_pos": 147, "end_pos": 151, "type": "DATASET", "confidence": 0.8778053522109985}]}, {"text": " Table 7: Recall (R), Precision (P), and f -score (F) results for RANK+DS-ORACLE+SP and LINK-ORACLE on the  entire ACE corpus.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9445502012968063}, {"text": "Precision (P)", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9571568518877029}, {"text": "f -score (F)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9749289155006409}, {"text": "LINK-ORACLE", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.9634648561477661}, {"text": "ACE corpus", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.9188222289085388}]}]}