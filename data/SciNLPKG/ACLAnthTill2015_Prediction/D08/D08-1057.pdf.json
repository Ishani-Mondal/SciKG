{"title": [{"text": "Seed and Grow: Augmenting Statistically Generated Summary Sentences using Schematic Word Patterns", "labels": [], "entities": [{"text": "Augmenting Statistically Generated Summary Sentences", "start_pos": 15, "end_pos": 67, "type": "TASK", "confidence": 0.8282329201698303}]}], "abstractContent": [{"text": "We examine the problem of content selection in statistical novel sentence generation.", "labels": [], "entities": [{"text": "content selection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7242128849029541}, {"text": "statistical novel sentence generation", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.5949445143342018}]}, {"text": "Our approach models the processes performed by professional editors when incorporating material from additional sentences to support some initially chosen key summary sentence, a process we refer to as Sentence Augmentation.", "labels": [], "entities": [{"text": "Sentence Augmentation", "start_pos": 202, "end_pos": 223, "type": "TASK", "confidence": 0.9318756461143494}]}, {"text": "We propose and evaluate a method called \"Seed and Grow\" for selecting such auxiliary information.", "labels": [], "entities": []}, {"text": "Additionally, we argue that this can be performed using schemata, as represented by word-pair co-occurrences, and demonstrate its use in statistical summary sentence generation.", "labels": [], "entities": [{"text": "statistical summary sentence generation", "start_pos": 137, "end_pos": 176, "type": "TASK", "confidence": 0.5991420522332191}]}, {"text": "Evaluation results are supportive , indicating that a schemata model significantly improves over the baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the context of automatic text summarisation, we examine the problem of statistical novel sentence generation, with the aim of moving from the current state-of-the-art of sentence extraction to abstractlike summaries.", "labels": [], "entities": [{"text": "automatic text summarisation", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.5830634037653605}, {"text": "statistical novel sentence generation", "start_pos": 74, "end_pos": 111, "type": "TASK", "confidence": 0.5763757079839706}, {"text": "sentence extraction", "start_pos": 173, "end_pos": 192, "type": "TASK", "confidence": 0.7363361865282059}]}, {"text": "In particular, we focus on the task of selecting content to include within a generated sentence.", "labels": [], "entities": []}, {"text": "Our approach to novel sentence generation is to model the processes underlying summarisation as performed by professional editors and abstractors.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6927052289247513}, {"text": "summarisation", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9680355787277222}]}, {"text": "An example of the target output of this kind of generation is presented in.", "labels": [], "entities": []}, {"text": "In this example, the human authored summary sentence was taken verbatim from the executive summary of a United Nations proposal for the provision of aid addressing a particular humanitarian crisis.", "labels": [], "entities": []}, {"text": "Such documents typically exceed a hundred pages.", "labels": [], "entities": []}, {"text": "To write such summaries, we assume that the human abstractor begins by choosing key sentences from the full document.", "labels": [], "entities": []}, {"text": "Then, for each key sentence, a set of auxiliary material is identified.", "labels": [], "entities": []}, {"text": "The key sentence is revised incorporating these auxiliary sentences to produce the eventual summary sentence.", "labels": [], "entities": []}, {"text": "To study this phenomenon, a corpus of UN documents was collected and analysed.", "labels": [], "entities": []}, {"text": "Each document was divided into two parts comprising its executive summary, and the remainder, referred to here as the source.", "labels": [], "entities": []}, {"text": "We manually aligned each executive summary sentence with one or more sentences from the source, by choosing a key sentence that provided evidence for the content of the summary sentence along with additional sentences that provided supporting material.", "labels": [], "entities": []}, {"text": "We refer to the resulting corpus as the UN Consolidated Appeals Process (UN CAP) corpus.", "labels": [], "entities": [{"text": "UN Consolidated Appeals Process (UN CAP) corpus", "start_pos": 40, "end_pos": 87, "type": "DATASET", "confidence": 0.7907648351457384}]}, {"text": "It is a collection of sentence alignments, each referred to as an aligned sentence tuple, which consists of: 1.", "labels": [], "entities": []}, {"text": "A human authored summary sentence from the executive summary; 2.", "labels": [], "entities": []}, {"text": "A key sentence from the source; 3.", "labels": [], "entities": []}, {"text": "Zero or more auxiliary sentences from the source.", "labels": [], "entities": []}, {"text": "The key and any auxiliary sentences are referred to collectively as the aligned source sentences.", "labels": [], "entities": []}, {"text": "We argue that some process that combines information from multiple sentences is required if we are to generate summary sentences similar to that portrayed in.", "labels": [], "entities": []}, {"text": "This is supported by our analysis of the UN CAP corpus.", "labels": [], "entities": [{"text": "UN CAP corpus", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9095175862312317}]}, {"text": "Of the 580 aligned sentence tuples, the majority, 61% of cases, appear to be examples of such a process.", "labels": [], "entities": []}, {"text": "Furthermore, the auxiliary sentences are clearly necessary.", "labels": [], "entities": []}, {"text": "We found that only 30% of the open-class words in the summary are found in the key sentence.", "labels": [], "entities": []}, {"text": "If one selects all the open-class words from aligned source sentences, recall increases to an upper limit of 45% without yet accounting for stemming.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9996253252029419}]}, {"text": "This upper bound is consistent with the upper limit of 50% found by which takes into account stemming differences.", "labels": [], "entities": []}, {"text": "This demonstrates that the auxiliary material is a valuable source of content which should be integrated into the summary sentence, allowing an improvement in recall of up to 15% prior to accounting for morphological, synonym and paraphrase differences.", "labels": [], "entities": [{"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9987336993217468}]}, {"text": "Of course, the trick is to improve recall without hurting precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9984903335571289}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9984564781188965}]}, {"text": "A naive addition of all words in the aligned source sentences incurs a drop in precision from 30% to 23%.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9995561242103577}]}, {"text": "The problem thus is one of selecting the relevant auxiliary content words without introducing unimportant content.", "labels": [], "entities": []}, {"text": "We refer to this problem of incorporating material from auxiliary sentences to supplement a key sentence as Sentence Augmentation.", "labels": [], "entities": [{"text": "Sentence Augmentation", "start_pos": 108, "end_pos": 129, "type": "TASK", "confidence": 0.930728554725647}]}, {"text": "In this paper, sentence augmentation is modelled as a noisy channel process and has two facets: content selection and language modelling.", "labels": [], "entities": [{"text": "sentence augmentation", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7571641802787781}, {"text": "content selection", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.7501727938652039}, {"text": "language modelling", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.755121260881424}]}, {"text": "This paper focuses on the former, in which the system must rank text segments-in this case, words-for inclusion in the generated sentence.", "labels": [], "entities": []}, {"text": "Given a ranked selection of words, a language model would then order them appropriately, as described in work on sentence regeneration (for example, see;).", "labels": [], "entities": [{"text": "sentence regeneration", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7332414835691452}]}, {"text": "Provided with an aligned sentence tuple, the problem lies in effectively selecting words from the auxiliary sentences to bolster those taken from the key sentence.", "labels": [], "entities": []}, {"text": "Given that there are on average 2.7 auxiliary sentences per aligned sentence tuple, this additional influx of words poses a considerable challenge.", "labels": [], "entities": []}, {"text": "We begin with the premise that, for documents of a homogeneous type (in this case, the genre is a funding proposal, and the domain is humanitarian aid), it maybe possible to identify patterns in the organisation of information in summaries.", "labels": [], "entities": []}, {"text": "For example, presents three summary sentences from our corpus that share the same patterned juxtaposition of two concepts DisplacedPersons and HostingCommunities.", "labels": [], "entities": []}, {"text": "Documents may exhibit common patterns since they have a similar goal: namely, to convince donors to give financial support.", "labels": [], "entities": []}, {"text": "In the above example, the juxtaposition highlights the fact that those in need are not just those people from the 'epicenter' of the crisis but also those that look after them.", "labels": [], "entities": []}, {"text": "We propose and evaluate a method called \"Seed and Grow\" for selecting content from auxiliary sentences.", "labels": [], "entities": []}, {"text": "That is, we first select the core meaning of the summary, given hereby the key sentence, and then we find those pieces of additional information that are conventionally juxtaposed with it.", "labels": [], "entities": []}, {"text": "Such patterns are reminiscent of Schemata, the organisations of propositional content introduced by.", "labels": [], "entities": []}, {"text": "Schemata typically involve a symbolic representation of each proposition's semantics.", "labels": [], "entities": []}, {"text": "However, in our case, a text-to-text generation scenario, we are without such representations and so must find other means to encode these patterns.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7164664268493652}]}, {"text": "To alleviate the situation, we turn to word-pair cooccurrences to approximate schematic patterns.", "labels": [], "entities": []}, {"text": "ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them.", "labels": [], "entities": []}, {"text": "In this particular example, this is realised lexically in the co-occurrences of the words displaced and host.", "labels": [], "entities": []}, {"text": "Corpus-based methods inspired by the notion of schemata have been explored in the past by and for ordering sentences extracted in a multi-document summarisation application.", "labels": [], "entities": []}, {"text": "However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored.", "labels": [], "entities": []}, {"text": "This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences.", "labels": [], "entities": []}, {"text": "In particular, we propose the \"Seed and Grow\" approach for this task.", "labels": [], "entities": []}, {"text": "The results show that even simple modelling approaches are able to model this schematic information.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we contrast our approach to related text-to-text research in Section 2.", "labels": [], "entities": []}, {"text": "The Content Selection model is presented in Section 3.", "labels": [], "entities": [{"text": "Content Selection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7333815693855286}]}, {"text": "Section 4 describes how a binary classification model is used in a statistical text generation system.", "labels": [], "entities": [{"text": "statistical text generation", "start_pos": 67, "end_pos": 94, "type": "TASK", "confidence": 0.6851972341537476}]}, {"text": "Section 5 describes our evaluation of the model fora summary generation task.", "labels": [], "entities": [{"text": "summary generation task", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.775872806708018}]}, {"text": "We conclude, in Section 6, that domain-specific schematic patterns can be acquired and applied to content selection for statistical sentence generation.", "labels": [], "entities": [{"text": "statistical sentence generation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6834329664707184}]}], "datasetContent": [{"text": "In this evaluation, the task is to select n words from the aligned source sentences for inclusion in a summary.", "labels": [], "entities": []}, {"text": "As a gold-standard for comparison, we simply examine what words were actually chosen in the summary sentence of the aligned sentence tuple.", "labels": [], "entities": []}, {"text": "We are specifically interested in open-class words, and so a stopword list of closed-class words is used to filter the sentences in each test case.", "labels": [], "entities": []}, {"text": "We evaluate against the set of open-class words in the human-authored summary sentence using recall and precision metrics.", "labels": [], "entities": [{"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9984623193740845}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9781621694564819}]}, {"text": "Recall is the size of the intersection of the selected and gold-standard sets, normalised by the length of the gold-standard sentence (in words).", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.985975444316864}]}, {"text": "This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric ( used in the Document Understanding Conferences 2 (DUC).", "labels": [], "entities": [{"text": "recall", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.9962879419326782}, {"text": "Document Understanding Conferences 2 (DUC)", "start_pos": 107, "end_pos": 149, "type": "TASK", "confidence": 0.8225781151226589}]}, {"text": "Precision is the size of the intersection normalised by the number of words selected.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9899333119392395}]}, {"text": "We also report the F-measure, which is the harmonic mean of the recall and precision scores.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9981951117515564}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9990379810333252}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9812774062156677}]}, {"text": "Recall, precision and F-measure are measured at various values of n ranging from 1 to the number of open-class words in the gold-standard summary sentence fora particular test case.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.99962317943573}, {"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9992214441299438}]}, {"text": "For the purposes of evaluation, differences in tokens due to morphology were explored crudely via the use of Porter's stemming algorithm.", "labels": [], "entities": []}, {"text": "However, the results from stemming are not that different from exact token matches when examining performance on the entire data set and so, for simplicity, these are omitted in this discussion.", "labels": [], "entities": [{"text": "stemming", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.9665660262107849}]}], "tableCaptions": [{"text": " Table 1: Statistics for the UN CAP training set", "labels": [], "entities": [{"text": "UN CAP training set", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.8725827634334564}]}]}