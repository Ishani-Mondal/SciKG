{"title": [{"text": "Dependency-based Semantic Role Labeling of PropBank", "labels": [], "entities": [{"text": "Dependency-based Semantic Role Labeling", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6375460550189018}, {"text": "PropBank", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.6527904272079468}]}], "abstractContent": [{"text": "We present a PropBank semantic role labeling system for English that is integrated with a dependency parser.", "labels": [], "entities": [{"text": "PropBank semantic role labeling", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.7103432416915894}]}, {"text": "To tackle the problem of joint syntactic-semantic analysis, the system relies on a syntactic and a semantic sub-component.", "labels": [], "entities": [{"text": "joint syntactic-semantic analysis", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6982120871543884}]}, {"text": "The syntactic model is a projec-tive parser using pseudo-projective transformations , and the semantic model uses global inference mechanisms on top of a pipeline of classifiers.", "labels": [], "entities": []}, {"text": "The complete syntactic-semantic output is selected from a candidate pool generated by the subsystems.", "labels": [], "entities": []}, {"text": "We evaluate the system on the CoNLL-2005 test sets using segment-based and dependency-based metrics.", "labels": [], "entities": [{"text": "CoNLL-2005 test sets", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9809539715449015}]}, {"text": "Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently.", "labels": [], "entities": [{"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9994334578514099}, {"text": "WSJ+Brown test set", "start_pos": 113, "end_pos": 131, "type": "DATASET", "confidence": 0.9552864789962768}]}, {"text": "Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008.", "labels": [], "entities": [{"text": "F1", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9993374943733215}, {"text": "CoNLL-2008", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.6010929346084595}]}, {"text": "Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance.", "labels": [], "entities": [{"text": "dependency-based semantic role labeler", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.5873191878199577}, {"text": "PropBank", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9433799386024475}]}], "introductionContent": [{"text": "Automatic semantic role labeling (SRL), the task of determining who does what to whom, is a useful intermediate step in NLP applications performing semantic analysis.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7699773361285528}, {"text": "semantic analysis", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.8928499817848206}]}, {"text": "It has obvious applications for template-filling tasks such as information extraction and question answering (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.834983229637146}, {"text": "question answering", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.8869671821594238}]}, {"text": "It has also been used in prototypes of NLP systems that carryout complex reasoning, such as entailment recognition systems ().", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7603974640369415}]}, {"text": "In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization.", "labels": [], "entities": []}, {"text": "The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out.", "labels": [], "entities": [{"text": "role-semantic analysis automatically", "start_pos": 111, "end_pos": 147, "type": "TASK", "confidence": 0.816972017288208}]}, {"text": "Following the seminal work of, there have been many extensions in machine learning models, feature engineering (), and inference procedures (.", "labels": [], "entities": []}, {"text": "With very few exceptions (e.g., published SRL methods have used some sort of syntactic structure as input ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9553837180137634}]}, {"text": "Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (, although there has also been much research on the use of shallow syntax) in SRL.", "labels": [], "entities": [{"text": "role-semantic analysis", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7427815794944763}, {"text": "Penn Treebank", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.994054764509201}, {"text": "SRL", "start_pos": 182, "end_pos": 185, "type": "TASK", "confidence": 0.9395654797554016}]}, {"text": "In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate-argument relations.", "labels": [], "entities": [{"text": "SRL task", "start_pos": 82, "end_pos": 90, "type": "TASK", "confidence": 0.9467321038246155}]}, {"text": "Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent-based counterparts.", "labels": [], "entities": []}, {"text": "For instance, reported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9975023865699768}, {"text": "WSJ section 23", "start_pos": 200, "end_pos": 214, "type": "DATASET", "confidence": 0.9606473048528036}]}, {"text": "Ina similar vein, reported that parse tree path features extracted from a rule-based dependency parser are much less reliable than those from a modern constituent parser.", "labels": [], "entities": []}, {"text": "In contrast, we recently carried out a detailed comparison) between constituent-based and dependency-based SRL systems for FrameNet, in which the results of the two types of systems where almost equivalent when using modern statistical dependency parsers.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.8537313342094421}]}, {"text": "We suggested that the previous lack of progress in dependency-based SRL was due to low parsing accuracy.", "labels": [], "entities": [{"text": "SRL", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.7586644887924194}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9535383582115173}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9255088567733765}]}, {"text": "The experiments showed that the grammatical function information available in dependency representations results in a steeper learning curve when training semantic role classifiers, and it also seemed that the dependency-based role classifiers were more resilient to lexical problems caused by change of domain.", "labels": [], "entities": []}, {"text": "The recent was an attempt to show that SRL can be accurately carried out using only dependency syntax.", "labels": [], "entities": [{"text": "SRL", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9937580823898315}]}, {"text": "However, these results are not easy to compare to previously published results since the task definitions and evaluation metrics were different.", "labels": [], "entities": []}, {"text": "This paper compares the best-performing system in the) with previously published constituent-based SRL systems.", "labels": [], "entities": []}, {"text": "The system carries out joint dependency-syntactic and semantic analysis.", "labels": [], "entities": []}, {"text": "We first describe its implementation in Section 2, and then compare the system with the best system in the CoNLL-2005 Shared Task in Section 3.", "labels": [], "entities": [{"text": "CoNLL-2005 Shared Task", "start_pos": 107, "end_pos": 129, "type": "DATASET", "confidence": 0.826749304930369}]}, {"text": "Since the outputs of the two systems are different, we carryout two types of evaluations: first by using the traditional segment-based metric used in the CoNLL-2005 Shared Task, and then by using the dependency-based metric from the CoNLL-2008 Shared Task.", "labels": [], "entities": [{"text": "CoNLL-2005 Shared Task", "start_pos": 154, "end_pos": 176, "type": "DATASET", "confidence": 0.905014177163442}, {"text": "CoNLL-2008 Shared Task", "start_pos": 233, "end_pos": 255, "type": "DATASET", "confidence": 0.9428920745849609}]}, {"text": "Both evaluations require a transformation of the output of one system: For the segmentbased metric, we have to convert the dependencybased output to segments; and for the dependencybased metric, a head-finding procedure is needed to select heads in segments.", "labels": [], "entities": []}, {"text": "For the first time fora system using only dependency syntax, we report results for PropBank-based semantic role labeling of English that are close to the state of the art, and for some measures even superior.", "labels": [], "entities": [{"text": "PropBank-based semantic role labeling", "start_pos": 83, "end_pos": 120, "type": "TASK", "confidence": 0.7241201251745224}]}], "datasetContent": [{"text": "To be able to score the output of a dependency-based SRL system using the segment scorer, a conversion step is needed.", "labels": [], "entities": [{"text": "SRL", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8726537823677063}]}, {"text": "Algorithm 2 shows how a set of segments is constructed from an argument dependency node.", "labels": [], "entities": []}, {"text": "For each argument node, the algorithm computes the yield Y of the argument node, i.e. the set of dependency nodes to include in the bracketing.", "labels": [], "entities": []}, {"text": "This set is then partitioned into contiguous parts, from which the segments are computed.", "labels": [], "entities": []}, {"text": "In most cases, the yield is just the subtree dominated by the argument node.", "labels": [], "entities": []}, {"text": "However, if the argument dominates the predicate, then the branch containing the predicate is removed.", "labels": [], "entities": []}, {"text": "shows the performance figures of our system on the WSJ and Brown corpora: precision, recall, F 1 -measure, and complete proposition accuracy (PP).", "labels": [], "entities": [{"text": "WSJ and Brown corpora", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.886798545718193}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9995338916778564}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9988862872123718}, {"text": "F 1 -measure", "start_pos": 93, "end_pos": 105, "type": "METRIC", "confidence": 0.9728277921676636}, {"text": "complete proposition accuracy (PP)", "start_pos": 111, "end_pos": 145, "type": "METRIC", "confidence": 0.7511475682258606}]}, {"text": "These figures are compared to the best-performing system in the, referred to as Punyakanok in the table, and the best result currently published (, referred to as Surdeanu.", "labels": [], "entities": []}, {"text": "To validate the sanity of the segment creation algorithm, the table also shows the result of applying segment creation to gold-standard syntactic-  semantic trees.", "labels": [], "entities": [{"text": "segment creation", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.8458531498908997}, {"text": "segment creation", "start_pos": 102, "end_pos": 118, "type": "TASK", "confidence": 0.7421291172504425}]}, {"text": "We see that the two conversion procedures involved (constituent-to-dependency conversion by the CoNLL-2008 Shared Task organizers, and our dependency-to-segment conversion) work satisfactorily although the process is not completely lossless.", "labels": [], "entities": [{"text": "CoNLL-2008 Shared Task organizers", "start_pos": 96, "end_pos": 129, "type": "DATASET", "confidence": 0.9071771800518036}, {"text": "dependency-to-segment conversion", "start_pos": 139, "end_pos": 171, "type": "TASK", "confidence": 0.6786979138851166}]}, {"text": "During inspection of the output, we noted that many errors arise from inconsistent punctuation attachment in PropBank/Treebank.", "labels": [], "entities": [{"text": "PropBank/Treebank", "start_pos": 109, "end_pos": 126, "type": "DATASET", "confidence": 0.9208795229593912}]}, {"text": "We therefore normalized the segments to exclude punctuation at the beginning or end of a segment.", "labels": [], "entities": []}, {"text": "The results of this evaluation is shown in  The results on the WSJ test set clearly show that dependency-based SRL systems can rival constituent-based systems in terms of performance -it clearly outperforms the Punyakanok system, and has a higher recall and complete proposition accuracy than the Surdeanu system.", "labels": [], "entities": [{"text": "WSJ test set", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9661462306976318}, {"text": "recall", "start_pos": 247, "end_pos": 253, "type": "METRIC", "confidence": 0.9985888600349426}, {"text": "accuracy", "start_pos": 279, "end_pos": 287, "type": "METRIC", "confidence": 0.8971932530403137}]}, {"text": "We interpret the high recall as a result of the dependency syntactic representation, which makes the parse tree paths simpler and thus the arguments easier to find.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9985622763633728}]}, {"text": "For the Brown test set, on the other hand, the dependency-based system suffers from a low precision compared to the constituent-based systems.", "labels": [], "entities": [{"text": "Brown test set", "start_pos": 8, "end_pos": 22, "type": "DATASET", "confidence": 0.8578376571337382}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9975712895393372}]}, {"text": "Our error analysis indicates that the domain change caused problems with prepositional attachment for the dependency parser -it is well-known that prepositional attachment is a highly lexicalized problem, and thus sensitive to domain changes.", "labels": [], "entities": []}, {"text": "We believe that the reason why the constituent-based systems are more robust in this respect is that they utilize a combination strategy, using inputs from two different full constituent parsers, a clause bracketer, and a chunker.", "labels": [], "entities": []}, {"text": "However, caution is needed when drawing conclusions from results on the Brown test set, which is only 7,585 words, compared to the 59,100 words in the WSJ test set.", "labels": [], "entities": [{"text": "Brown test set", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.8432880242665609}, {"text": "WSJ test set", "start_pos": 151, "end_pos": 163, "type": "DATASET", "confidence": 0.984533687432607}]}, {"text": "It has previously been noted () that a segment-based evaluation maybe unfavorable to a dependency-based system, and that an evaluation that scores argument heads maybe more indicative of its true performance.", "labels": [], "entities": []}, {"text": "We thus carried out an evaluation using the evaluation script of the In this evaluation method, an argument is counted as correctly identified if its head and label are correct.", "labels": [], "entities": []}, {"text": "Note that this is not equivalent to the segment-based metric: Ina perfectly identified segment, we may still pick out the wrong head, and if the head is correct, we may infer an incorrect segment.", "labels": [], "entities": []}, {"text": "The evaluation script also scores predicate disambiguation performance; we did not include this score since the 2005 systems did not output predicate sense identifiers.", "labels": [], "entities": [{"text": "predicate disambiguation", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.8559476435184479}]}, {"text": "Since CoNLL-2005-style segments have no internal tree structure, it is nontrivial to extract ahead.", "labels": [], "entities": []}, {"text": "It is conceivable that the output of the parsers used by the Punyakanok system could be used to extract heads, but this is not recommendable because the Punyakanok system is an ensemble system and a segment does not always exactly match a constituent in a parse tree.", "labels": [], "entities": []}, {"text": "Furthermore, the CoNLL-2008 constituent-to-dependency conversion method uses a richer structure than just the raw constituents: empty categories, grammatical functions, and named entities.", "labels": [], "entities": [{"text": "CoNLL-2008 constituent-to-dependency conversion", "start_pos": 17, "end_pos": 64, "type": "TASK", "confidence": 0.73858775695165}]}, {"text": "To recreate this additional information, we would have to apply automatic systems and end up with unreliable results.", "labels": [], "entities": []}, {"text": "Instead, we thus chose to find an upper bound on the performance of the segment-based system.", "labels": [], "entities": []}, {"text": "We applied a simple head-finding procedure (Algorithm 3) to find a set of head nodes for each segment.", "labels": [], "entities": []}, {"text": "Since the CoNLL-2005 output does not include dependency information, the algorithm uses gold-standard dependencies and intersects segments with the gold-standard segments.", "labels": [], "entities": [{"text": "CoNLL-2005 output", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.9000462591648102}]}, {"text": "This will give us an upper bound, since if the segment contains the correct head, it will always be counted as correct.", "labels": [], "entities": []}, {"text": "The algorithm looks for dependencies leaving the segment, and if multiple outgoing edges are found, a couple of simple heuristics are applied.", "labels": [], "entities": []}, {"text": "We found that the best performance is achieved when selecting only one outgoing edge.", "labels": [], "entities": []}, {"text": "\"Small clauses,\" which are split into an object and a predicative complement in the dependency framework, are the only cases where we select two heads.", "labels": [], "entities": []}, {"text": "shows the results of the dependencybased evaluation.", "labels": [], "entities": []}, {"text": "In the table, the output of the Algorithm 3 Finding head nodes in a segment.", "labels": [], "entities": []}, {"text": "input Argument segment a if a overlaps with a segment in the gold standard a \u2190 intersection of a and gold standard F \u2190 {n : governor of n outside a} if |F | = 1 return F remove punctuation nodes from F if |F | = 1 return F if F = {n 1 , n 2 , . .", "labels": [], "entities": []}, {"text": ".} where n 1 is an object and n 2 is the predicative part of a small clause return {n 1 , n 2 } if F contains anode n that is a subject or an object return {n} else return {n}, where n is the leftmost node in F dependency-based system is compared to the semantic dependency links automatically extracted from the segments of the Punyakanok system.", "labels": [], "entities": [{"text": "Punyakanok system", "start_pos": 329, "end_pos": 346, "type": "DATASET", "confidence": 0.9454131722450256}]}, {"text": "In this evaluation, the dependency-based system has a higher F1-measure than the Punyakanok system on both test sets.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9995081424713135}]}, {"text": "This suggests that the main advantage of using a dependency-based semantic role labeler is that it is better at finding the heads of semantic arguments, rather than finding segments.", "labels": [], "entities": []}, {"text": "The results are also interesting in comparison to the multi-view system described by, which has a reported head F1 measure of 85.2 on the WSJ test set.", "labels": [], "entities": [{"text": "head F1 measure", "start_pos": 107, "end_pos": 122, "type": "METRIC", "confidence": 0.838147501150767}, {"text": "WSJ test set", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9820744792620341}]}, {"text": "The figure is not exactly compatible with ours, however, since that system used a different head extraction mechanism.", "labels": [], "entities": [{"text": "head extraction", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.726815789937973}]}], "tableCaptions": [{"text": " Table 2: Evaluation with unnormalized segments.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation with normalized segments.", "labels": [], "entities": []}]}