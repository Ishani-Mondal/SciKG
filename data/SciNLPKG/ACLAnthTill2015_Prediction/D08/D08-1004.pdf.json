{"title": [{"text": "Modeling Annotators: A Generative Approach to Learning from Annotator Rationales *", "labels": [], "entities": []}], "abstractContent": [{"text": "A human annotator can provide hints to a machine learner by highlighting contextual \"rationales\" for each of his or her annotations (Zaidan et al., 2007).", "labels": [], "entities": []}, {"text": "How can one exploit this side information to better learn the desired parameters \u03b8?", "labels": [], "entities": []}, {"text": "We present a generative model of how a given annotator, knowing the true \u03b8, stochastically chooses rationales.", "labels": [], "entities": []}, {"text": "Thus, observing the rationales helps us infer the true \u03b8.", "labels": [], "entities": []}, {"text": "We collect substring rationales fora sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.9189125100771586}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9969660639762878}]}, {"text": "Our new generative approach exploits the rationales more effectively than our previous \"masking SVM\" approach.", "labels": [], "entities": []}, {"text": "It is also more principled, and could be adapted to help learn other kinds of probabilistic classi-fiers for quite different tasks.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In, we introduced the \"Movie Review Polarity Dataset Enriched with Annotator Rationales.\"", "labels": [], "entities": [{"text": "Movie Review Polarity Dataset", "start_pos": 23, "end_pos": 52, "type": "DATASET", "confidence": 0.8051739186048508}]}, {"text": "8 It is based on the dataset of, which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F 0 -F 9 ).", "labels": [], "entities": [{"text": "F 0 -F 9", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9303262948989868}]}, {"text": "All our experiments use F 9 as their final blind test set.", "labels": [], "entities": [{"text": "F 9", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9074264168739319}]}, {"text": "The enriched dataset adds rationale annotations produced by an annotator A0, who annotated folds F 0 -F 8 of the movie review set with rationales (in the form of textual substrings) that supported the goldstandard classifications.", "labels": [], "entities": [{"text": "movie review set", "start_pos": 113, "end_pos": 129, "type": "DATASET", "confidence": 0.7070948878924052}]}, {"text": "We will use A0's data to determine the improvement of our method over a (log-linear) baseline model without rationales.", "labels": [], "entities": []}, {"text": "We also use A0 to compare against the \"masking SVM\" method and SVM baseline of.", "labels": [], "entities": []}, {"text": "Since \u03c6 can be tuned to a particular annotator, we would also like to know how well this works with data from annotators other than A0.", "labels": [], "entities": []}, {"text": "We randomly selected 100 reviews (50 positive and 50 negative) and collected both class and rationale annotation data from each of six new annotators A3-A8, 10 following the same procedures as (.", "labels": [], "entities": []}, {"text": "We report results using only data from A3-A5, since we used the data from A6-A8 as development data in the early stages of our work.", "labels": [], "entities": []}, {"text": "We use this new rationale-enriched dataset 8 to determine if our method works well across annotators.", "labels": [], "entities": []}, {"text": "We will only be able to carryout that comparison at small training set sizes, due to limited data from A3-A8.", "labels": [], "entities": []}, {"text": "The larger A0 dataset will still allow us to evaluate our method on a range of training set sizes.", "labels": [], "entities": [{"text": "A0 dataset", "start_pos": 11, "end_pos": 21, "type": "DATASET", "confidence": 0.8877551555633545}]}, {"text": "We report on two sets of experiments.", "labels": [], "entities": []}, {"text": "In the first set, we use the annotation data that A3-A5 provided for the small set of 100 documents (as well as the data from A0 on those same 100 documents).", "labels": [], "entities": []}, {"text": "In the second set, we used A0's abundant annotation data to evaluate our method with training set sizes up to 1600 documents, and compare it with three other methods: log-linear baseline, SVM baseline, and the SVM masking method of ().", "labels": [], "entities": [{"text": "SVM masking", "start_pos": 210, "end_pos": 221, "type": "TASK", "confidence": 0.7939120531082153}]}, {"text": "8.1 The added benefit of rationales shows learning curves for four methods.", "labels": [], "entities": []}, {"text": "A log-linear model shows large and significant improvements, at all training sizes, when we incorporate rationales into its training via equation (4).", "labels": [], "entities": []}, {"text": "Moreover, the resulting classifier consistently outperforms 17 prior work, the masking SVM, which starts with a slightly better baseline classifier (an SVM) but incorporates the rationales more crudely.: Accuracy rates using each annotator's data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 204, "end_pos": 212, "type": "METRIC", "confidence": 0.9854324460029602}]}, {"text": "Ina given column, a value in italics is not significantly different from the highest value in that column, which is boldfaced.", "labels": [], "entities": []}, {"text": "The size=20 results average over 5 experiments.", "labels": [], "entities": [{"text": "size", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9638444185256958}]}, {"text": "To confirm that we could successfully model annotators other than A0, we performed the same comparison for annotators A3-A5; each had provided class and rationale annotations on a small 100-document training set.", "labels": [], "entities": []}, {"text": "We trained a separate \u03c6 for each annotator.", "labels": [], "entities": []}, {"text": "shows improvements over baseline, usually significant, at 2 training set sizes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy rate for an annotator's \u03b8 (rows) ob- tained when using some other annotator's \u03c6 (columns).  Notice that the diagonal entries and the baseline column  are taken from rows of", "labels": [], "entities": [{"text": "Accuracy rate", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9884596168994904}]}, {"text": " Table 3: Cross-entropy per tag of rationale annotations  r for each annotator (rows), when predicted from that  annotator's x and  \u03b8 via a possibly different annotator's  \u03c6 (columns). For comparison, the trivial model is a bi- gram model of r, which is trained on the target annotator  but ignores x and  \u03b8. 5-fold cross-validation on the 100- document set was used to prevent testing on training data.", "labels": [], "entities": []}]}