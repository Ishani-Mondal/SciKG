{"title": [{"text": "Graph-based Analysis of Semantic Drift in Espresso-like Bootstrapping Algorithms", "labels": [], "entities": []}], "abstractContent": [{"text": "Bootstrapping has a tendency, called semantic drift, to select instances unrelated to the seed instances as the iteration proceeds.", "labels": [], "entities": []}, {"text": "We demonstrate the semantic drift of bootstrap-ping has the same root as the topic drift of Kleinberg's HITS, using a simplified graph-based reformulation of bootstrapping.", "labels": [], "entities": []}, {"text": "We confirm that two graph-based algorithms, the von Neumann kernels and the regularized Laplacian, can reduce semantic drift in the task of word sense disambiguation (WSD) on Senseval-3 English Lexical Sample Task.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.7845442593097687}, {"text": "Senseval-3 English Lexical Sample Task", "start_pos": 175, "end_pos": 213, "type": "TASK", "confidence": 0.4942510187625885}]}, {"text": "Proposed algorithms achieve superior performance to Espresso and previous graph-based WSD methods, even though the proposed algorithms have less parameters and are easy to calibrate.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years machine learning techniques become widely used in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.8027055164178213}]}, {"text": "These techniques offer various ways to exploit large corpora and are known to perform well in many tasks.", "labels": [], "entities": []}, {"text": "However, these techniques often require tagged corpora, which are not readily available to many languages.", "labels": [], "entities": []}, {"text": "So far, reducing the cost of human annotation is one of the important problems for building NLP systems.", "labels": [], "entities": []}, {"text": "To mitigate the problem of hand-tagging resources, semi(or minimally)-supervised and unsupervised techniques have been actively studied.", "labels": [], "entities": []}, {"text": "first presented a bootstrapping method which requires only a small amount of instances (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction.", "labels": [], "entities": [{"text": "pattern induction", "start_pos": 256, "end_pos": 273, "type": "TASK", "confidence": 0.7982960045337677}, {"text": "pattern ranking/selection", "start_pos": 275, "end_pos": 300, "type": "TASK", "confidence": 0.830456867814064}, {"text": "instance extraction", "start_pos": 306, "end_pos": 325, "type": "TASK", "confidence": 0.7452200353145599}]}, {"text": "Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation, named entity recognition) and relation extraction ().", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.6919879118601481}, {"text": "named entity recognition", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6231110890706381}, {"text": "relation extraction", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8858893811702728}]}, {"text": "However, it is known that bootstrapping often acquires instances not related to seed instances.", "labels": [], "entities": []}, {"text": "For example, consider the task of collecting the names of common tourist sites from web corpora.", "labels": [], "entities": [{"text": "collecting the names of common tourist sites", "start_pos": 34, "end_pos": 78, "type": "TASK", "confidence": 0.7585449389048985}]}, {"text": "Given words like \"Geneva\" and \"Bali\" as seed instances, bootstrapping would eventually learn generic patterns such as \"pictures\" and \"photos,\" which also co-occur with many other unrelated instances.", "labels": [], "entities": [{"text": "Geneva", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.9585879445075989}]}, {"text": "The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as \"Britney Spears.\"", "labels": [], "entities": []}, {"text": "This phenomenon is called semantic drift.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8798880279064178}]}, {"text": "A straightforward approach to avoid semantic drift is to terminate iterations before hitting generic patterns, but the optimal number of iterations is task dependent and is hard to come by.", "labels": [], "entities": []}, {"text": "The recently proposed Espresso () algorithm incorporates sophisticated scoring functions to cope with generic patterns, but as pointed out, Espresso still shows semantic drift unless iterations are terminated appropriately.", "labels": [], "entities": []}, {"text": "Another deficiency in bootstrapping is its sensitivity to many parameters such as the number of seed instances, the stopping criterion of iteration, the number of instances and patterns selected on each iteration, and so forth.", "labels": [], "entities": []}, {"text": "These parameters also need to be calibrated for each task.", "labels": [], "entities": []}, {"text": "In this paper, we present a graph-theoretic analysis of Espresso-like bootstrapping algorithms.", "labels": [], "entities": []}, {"text": "We argue that semantic drift is inherent in these algorithms, and propose to use two graph-based algorithms that are theoretically less prone to semantic drift, as an alternative to bootstrapping.", "labels": [], "entities": []}, {"text": "After a brief review of related work in Section 2, we analyze in Section 3 a bootstrapping algorithm (Simplified Espresso) which can bethought of as a degenerate version of Espresso.", "labels": [], "entities": []}, {"text": "Simplified Espresso is simple enough to allow an algebraic treatment, and its equivalence to) is shown.", "labels": [], "entities": []}, {"text": "An implication of this equivalence is that semantic drift in this bootstrapping algorithm is essentially the same phenomenon as topic drift observed in link analysis.", "labels": [], "entities": [{"text": "link analysis", "start_pos": 152, "end_pos": 165, "type": "TASK", "confidence": 0.6965768486261368}]}, {"text": "Another implication is that semantic drift is inevitable in Simplified Espresso as it converges to the same score vector regardless of seed instances.", "labels": [], "entities": []}, {"text": "The original Espresso also suffers from the same problem as its simplified version does.", "labels": [], "entities": [{"text": "Espresso", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8335841298103333}]}, {"text": "It incorporates heuristics not present in Simplified Espresso to reduce semantic drift, but these heuristics have limited effect as we demonstrate in Section 3.3.", "labels": [], "entities": []}, {"text": "In Section 4, we propose two graph-based algorithms to reduce semantic drift.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.8326248228549957}]}, {"text": "These algorithms are used in link analysis community to reduce the effect of topic drift.", "labels": [], "entities": [{"text": "link analysis", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.8512776792049408}]}, {"text": "In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed reduce semantic drift.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.7769622405370077}]}, {"text": "Finally, we conclude our work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the von Neumann kernels and the regularized Laplacian on the same task as we used in Section 3.3; i.e., word sense disambiguation of word \"bank.\"", "labels": [], "entities": []}, {"text": "During the training phase, a pattern-instance matrix M was constructed using the training and testing data from Senseval-3 Lexical Sample (S3LS) Task.", "labels": [], "entities": []}, {"text": "The (i, j) element of M of both kernels is set to pointwise mutual information of a pattern i and an instance j, just the same as in Espresso.", "labels": [], "entities": []}, {"text": "Recall is used in evaluation.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8071706891059875}]}, {"text": "The diffusion parameter \u03b2 is set to 10 \u22125 and 10 \u22122 for the von Neumann kernels and the regularized Laplacian, respectively.", "labels": [], "entities": []}, {"text": "illustrates how well the proposed methods reduce semantic drift, just the same as the experiment of in Section 3.3.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.8248938620090485}]}, {"text": "We evaluate the recall on predicting the most frequent sense (MFS) and the recall on predicting other less frequent senses (others).", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9990792274475098}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.998222291469574}]}, {"text": "For Filtered Espresso, two results are shown: the result on the seventh iteration, which maximizes the performance (Filtered Espresso (optimal stopping)), and the one after convergence.", "labels": [], "entities": []}, {"text": "As in Section 3.3, if semantic drift occurs, recall of prediction on the most frequent sense increases while recall of prediction on other senses declines.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.998624324798584}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9971311092376709}]}, {"text": "Even Filtered Espresso was affected by semantic drift, which is again a consequence of the inherent graphical nature of Espresso-like bootstrapping algorithms.", "labels": [], "entities": []}, {"text": "On the other hand, both proposed methods succeeded to balance the most frequent sense and other senses.", "labels": [], "entities": []}, {"text": "Filtered Espresso at the optimal number of iterations achieved the best performance.", "labels": [], "entities": []}, {"text": "Nevertheless, the number of iterations has to be estimated separately.", "labels": [], "entities": []}, {"text": "We conducted experiments on the task of word sense disambiguation of S3LS data, this time not just on the word \"bank\" but on all target nouns in the data, following).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7534145514170328}]}, {"text": "We used two types of patterns.", "labels": [], "entities": []}, {"text": "Unordered single words (bag-of-words) We used all single words (unigrams) in the provided context from S3LS data sets.", "labels": [], "entities": [{"text": "S3LS data sets", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.7958953976631165}]}, {"text": "Each word in the context constructs one pattern.", "labels": [], "entities": []}, {"text": "The pattern corresponding to a word w is set to 1 if it appears in the context of instance i.", "labels": [], "entities": []}, {"text": "Words were lowercased and preprocessed with the Porter Stemmer 6 . Local collocations A local collocation refers to the ordered sequence of tokens in the local, narrow context of the target word.", "labels": [], "entities": [{"text": "Porter Stemmer 6", "start_pos": 48, "end_pos": 64, "type": "DATASET", "confidence": 0.8751428723335266}]}, {"text": "We allowed a pattern to have wildcard expressions like \"sale of * interest in * *\" for the target word interest.", "labels": [], "entities": []}, {"text": "We set the window size to \u00b13 by a preliminary experiment.", "labels": [], "entities": []}, {"text": "We report the results of Filtered Espresso both after convergence, and with its optimal number of iterations to show the upper bound of its performance.", "labels": [], "entities": []}, {"text": "compares proposed methods with Espresso with various configurations.", "labels": [], "entities": []}, {"text": "The proposed methods outperform by a large margin the most frequent sense baseline and both Simplified-and Filtered Espresso.", "labels": [], "entities": []}, {"text": "This means that the proposed methods effectively prevent semantic drift.", "labels": [], "entities": [{"text": "semantic drift", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.8527844846248627}]}, {"text": "Also, Filtered Espresso without early stopping shows more or less identical performance to Simplified Espresso.", "labels": [], "entities": []}, {"text": "It is implied that the heuristics of filtering and early stopping is a crucial step not to select generic patterns in Espresso, and the result is consistent with the experiment of convergence process of Espresso in Section 3.3.", "labels": [], "entities": []}, {"text": "Filtered Espresso halted after the seventh iteration (Filtered Espresso (optimal stopping)) is comparable to the proposed methods.", "labels": [], "entities": [{"text": "Filtered Espresso (optimal stopping))", "start_pos": 54, "end_pos": 91, "type": "METRIC", "confidence": 0.7107991824547449}]}, {"text": "However, in bootstrapping, not only the number of iterations but also a large number of parameters must be adjusted for each task and domain.", "labels": [], "entities": []}, {"text": "This shortcoming makes it hard to adapt bootstrapping in practical cases.", "labels": [], "entities": []}, {"text": "One of the main advantages of the proposed methods is that they have only one parameter \u03b2 and are much easier to tune.", "labels": [], "entities": []}, {"text": "It: Recall of the von Neumann kernels with a different diffusion factor \u03b2 on S3LS WSD task converge to the principal eigenvector of A, though the result does not seem to support this claim (both Simplified-and Filtered Espresso are 10 points lower than the most frequent sense baseline).", "labels": [], "entities": []}, {"text": "The reason seems to be because Espresso and the von Neumann kernels use pointwise mutual information as a weighting factor so that the principal eigenvector of A may not always represent the most frequent sense.", "labels": [], "entities": []}, {"text": "We also show the results of previous graph-based methods (), based on HyperLex and).", "labels": [], "entities": []}, {"text": "The experimental set-up is the same as ours in that they do not use the sense tags of training corpus to construct a co-occurrence graph, and they use the sense tags of all the S3LS training corpus for mapping senses to clusters.", "labels": [], "entities": [{"text": "S3LS training corpus", "start_pos": 177, "end_pos": 197, "type": "DATASET", "confidence": 0.6034761766592661}]}, {"text": "However, these methods have seven parameters to tune in order to achieve the best performance, and hence are difficult to optimize.", "labels": [], "entities": []}, {"text": "Diffusion Factor shows the performance of the von Neumann kernels with a diffusion factor \u03b2.", "labels": [], "entities": []}, {"text": "As expected, smaller \u03b2 leads to relatedness to seed instances, and larger \u03b2 asymptotically converges to the HITS authority ranking (or equivalently, Simplified One of the disadvantages of the von Neumann kernels over the regularized Laplacian is their sensitivity to parameter \u03b2. illustrates the performance of the regularized Laplacian with a diffusion factor \u03b2.", "labels": [], "entities": [{"text": "HITS authority ranking", "start_pos": 108, "end_pos": 130, "type": "DATASET", "confidence": 0.5790606737136841}]}, {"text": "The regularized Laplacian is stable for various values of \u03b2, while the von Neumann kernels change their behavior drastically depending on the value of \u03b2.", "labels": [], "entities": []}, {"text": "However, \u03b2 in the von Neumann kernels is upper-bounded by the reciprocal 1/\u03bb of the principal eigenvalue of A, and the derivatives of kernel matrices with respect to \u03b2 can be used to guide systematic calibration of \u03b2 (see () for detail).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Recall of predicted labels of bank", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9848360419273376}]}, {"text": " Table 2: Comparison of WSD algorithms", "labels": [], "entities": [{"text": "WSD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.6294184327125549}]}]}