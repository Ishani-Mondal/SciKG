{"title": [{"text": "Arabic Named Entity Recognition using Optimized Feature Sets", "labels": [], "entities": [{"text": "Arabic Named Entity Recognition", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5819900259375572}]}], "abstractContent": [{"text": "The Named Entity Recognition (NER) task has been garnering significant attention in NLP as it helps improve the performance of many natural language processing applications.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER) task", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.8143140503338405}]}, {"text": "In this paper, we investigate the impact of using different sets of features in two discriminative machine learning frameworks, namely, Support Vector Machines and Conditional Random Fields using Arabic data.", "labels": [], "entities": []}, {"text": "We explore lexical, contextual and morphological features on eight standardized data-sets of different genres.", "labels": [], "entities": []}, {"text": "We measure the impact of the different features in isolation, rank them according to their impact for each named entity class and incrementally combine them in order to infer the optimal machine learning approach and feature set.", "labels": [], "entities": []}, {"text": "Our system yields a performance of F \u03b2=1-measure=83.5 on ACE 2003 Broadcast News data.", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9841930866241455}, {"text": "ACE 2003 Broadcast News data", "start_pos": 57, "end_pos": 85, "type": "DATASET", "confidence": 0.9637283205986023}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is the process by which named entities are identified and classified in an open-domain text.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7984250982602438}]}, {"text": "NER is one of the most important sub-tasks in Information Extraction.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9089195132255554}, {"text": "Information Extraction", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.817317545413971}]}, {"text": "Thanks to standard evaluation test beds such as the Automatic Content Extraction (ACE) 1 , the task of NER has garnered significant attention within the natural language processing (NLP) community.", "labels": [], "entities": [{"text": "NER", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9661915302276611}]}, {"text": "ACE has facilitated evaluation for different languages creating standardized test sets and evaluation metrics.", "labels": [], "entities": [{"text": "ACE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8821600079536438}]}, {"text": "NER systems are typically enabling sub-tasks within large NLP systems.", "labels": [], "entities": []}, {"text": "The quality of the NER system has a direct impact on the quality of the overall NLP system.", "labels": [], "entities": []}, {"text": "Evidence abound in the literature in areas such as Question Answering, Machine Translation, and Information Retrieval ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8912388384342194}, {"text": "Machine Translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8763908743858337}, {"text": "Information Retrieval", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.7928968369960785}]}, {"text": "The most prominent NER systems approach the problem as a classification task: identifying the named entities (NE) in the text and then classifying them according to a set of designed features into one of a predefined set of classes (.", "labels": [], "entities": []}, {"text": "The number of classes differ depending on the data set.", "labels": [], "entities": []}, {"text": "To our knowledge, to date, the approach is always to model the problem with a single set of features for all the classes simultaneously.", "labels": [], "entities": []}, {"text": "This research, diverges from this view.", "labels": [], "entities": []}, {"text": "We recognize that different classes are sensitive to differing features.", "labels": [], "entities": []}, {"text": "Hence, in this study, we aspire to discover the optimum feature set per NE class.", "labels": [], "entities": []}, {"text": "We approach the NER task from a multi-classification perspective.", "labels": [], "entities": [{"text": "NER task", "start_pos": 16, "end_pos": 24, "type": "TASK", "confidence": 0.9313702583312988}]}, {"text": "We create a classifier for each NE class independently based on an optimal feature set, then combine the different classifiers fora global NER system.", "labels": [], "entities": []}, {"text": "For creating the different classifiers per class, we adopt two discriminative approaches: Support Vector Machines (SVM), and Conditional Random Fields (CRF)().", "labels": [], "entities": []}, {"text": "We comprehensively investigate many sets of features for each class of NEs: contextual, lexical, morphological and shallow syntactic features.", "labels": [], "entities": []}, {"text": "We explore the feature sets in isolation first.", "labels": [], "entities": []}, {"text": "Then, we employ the Fuzzy Borda Voting Scheme (FBVS)) in order to rank the features according to their perfor-mance per class.", "labels": [], "entities": [{"text": "Fuzzy Borda Voting Scheme (FBVS))", "start_pos": 20, "end_pos": 53, "type": "METRIC", "confidence": 0.4192255990845816}]}, {"text": "The incremental approach to feature selection leads to an interpretable system where we have a better understanding of the resulting errors.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7593265175819397}]}, {"text": "The paper is structured as follows: Section 2 gives a general overview of the state-of-the-art NER approaches with a particular emphasis on Arabic NER; Section 3 describes relevant characteristics of the Arabic language illustrating the challenges posed to NER; in Section 4.1 we describe the Support Vector Machines and Conditional Random Fields Modeling approaches.", "labels": [], "entities": [{"text": "NER", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9595813155174255}, {"text": "NER", "start_pos": 257, "end_pos": 260, "type": "TASK", "confidence": 0.8449651002883911}, {"text": "Conditional Random Fields Modeling", "start_pos": 321, "end_pos": 355, "type": "TASK", "confidence": 0.5862148851156235}]}, {"text": "We discuss details about our feature-set in 4.2 and describe the Fuzzy Borda Voting Scheme in Section 4.3.", "labels": [], "entities": [{"text": "Fuzzy Borda Voting", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.5032526652018229}]}, {"text": "Section 5 describes the experiments and shows the results obtained; Withing Section 5, Section 5.1 gives details about the data-sets which we use; finally, we discuss the results and some of our insights in Section 6 and draw some conclusions in 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our objective is to find the optimum set of features per NE class and then combine the outcome in a global NER system for Arabic.", "labels": [], "entities": []}, {"text": "We set the context window to be of size \u22121/+1 for all the experiments, as it empirically yields the best performance.", "labels": [], "entities": []}, {"text": "We use the CoNLL evaluation metrics of precision, recall, and F \u03b2=1 measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9997618794441223}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9997027516365051}, {"text": "F \u03b2=1 measures", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.973249876499176}]}, {"text": "The CoNLL metrics are geared to the chunk level yielding results as they pertain to the entire NE.", "labels": [], "entities": []}, {"text": "Our experiments are presented as follows: 1.", "labels": [], "entities": []}, {"text": "Training per individual NE class: We train for an individual class by turning off the other annotations for the other classes in the training set.", "labels": [], "entities": []}, {"text": "We experimented with two settings: 1.", "labels": [], "entities": []}, {"text": "Setting all the other NE classes to O, similar to non-NE words, thereby yielding a 3-way classification, namely, B-NE and I-NE for the class of interest, and O for the rest including the rest of the NEs and other words and punctuation; 2.", "labels": [], "entities": []}, {"text": "The second setting discriminated between the other NE classes that are not of interest and the rest of the words.", "labels": [], "entities": []}, {"text": "The intuition in this case is that NE class words will naturally behave differently than the rest of the words in the data.", "labels": [], "entities": []}, {"text": "Thereby, this setting yields a 4-way classification: B-NE and I-NE for class of interest, NE for the other NE classes, and O for the other words and punctuation in the data.", "labels": [], "entities": [{"text": "NE", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9453819990158081}]}, {"text": "In order to contrast the 3-way vs the 4-way classification, we run experiments and evaluate using the ACE 2003 data set with no features apart from 'CXT' and 'current word' using SVM.", "labels": [], "entities": [{"text": "ACE 2003 data set", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.9841290712356567}]}, {"text": "10 of these conflicts are between GPE and PER, and 6 of them are between GPE and ORG.", "labels": [], "entities": [{"text": "GPE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.5427804589271545}, {"text": "PER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9431751370429993}, {"text": "GPE", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.6778857707977295}, {"text": "ORG", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9074182510375977}]}, {"text": "4 classes: 10 conflicts (3 conflicts in BN and 7 in NW).", "labels": [], "entities": [{"text": "BN", "start_pos": 40, "end_pos": 42, "type": "DATASET", "confidence": 0.8825168013572693}]}, {"text": "9 of these conflicts are between GPE and ORG, and only one of them is between GPE and FAC.", "labels": [], "entities": [{"text": "GPE", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7622166872024536}, {"text": "ORG", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.831295907497406}, {"text": "GPE", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8388571739196777}, {"text": "FAC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.5248745083808899}]}, {"text": "An example of a conflict observed using the 3-way classification that disappeared when we apply the 4-way classification is in the following sentence: n$rt SHyfp WA$nTn tAyms tqryrA, which is translated as 'The Washington Times newspaper published a report'.", "labels": [], "entities": [{"text": "Washington Times newspaper published a report", "start_pos": 211, "end_pos": 256, "type": "DATASET", "confidence": 0.9124303162097931}]}, {"text": "When trained using a 3-way classifier, 'Washington' is assigned the tag GPE by the GPE classifier system and as an ORG by the ORG classifier system.", "labels": [], "entities": [{"text": "GPE", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.9310030341148376}, {"text": "ORG", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.9610413312911987}]}, {"text": "However, when trained using the 4-way classifier, this conflict is resolved as an ORG in the ORG classifier system and an NE in the GPE classifier system.", "labels": [], "entities": [{"text": "ORG", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.991601288318634}, {"text": "NE", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9331369400024414}, {"text": "GPE", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.9403237104415894}]}, {"text": "Thereby confirming our intuition that a 4-way classification is better suited for the individual NE classification systems.", "labels": [], "entities": [{"text": "NE classification", "start_pos": 97, "end_pos": 114, "type": "TASK", "confidence": 0.8221578598022461}]}, {"text": "Accordingly, for the rest of the experiments in this paper reporting on individual NE classifiers systems, we use a 4-way classification approach.", "labels": [], "entities": []}, {"text": "4. Feature set/class generalization : Finally, we pick the first n features that yield the best converging performance (after which additional features do not impact performance or cause it to deteriorate).", "labels": [], "entities": []}, {"text": "We use the top n features to tag the test data and compare the results against the system when it is trained on the whole feature set.", "labels": [], "entities": []}, {"text": "After running experiments using each feature individually, each result is considered an expert (the obtained F-measure is the weight in this framework).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9925566911697388}]}, {"text": "Our goal is to find a general ranking of the features for each ML approach and each class.", "labels": [], "entities": [{"text": "ML", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9550944566726685}]}, {"text": "shows the obtained rankings of the features for each class using SVM.", "labels": [], "entities": []}, {"text": "It is worth noting that the obtained CRF rankings are very similar to those yielded by using SVM.", "labels": [], "entities": []}, {"text": "We note that there are no specific features that have proven to be useless for all classes and ML approaches.", "labels": [], "entities": [{"text": "ML", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9585539102554321}]}, {"text": "We combine the features per NE class incrementally.", "labels": [], "entities": []}, {"text": "Since the total number of features is 16, each ML classifier is trained and evaluated on the tuning data 16 times for each genre.", "labels": [], "entities": [{"text": "ML classifier", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.8799309730529785}]}, {"text": "A best number of features per class per genre per ML technique is determined based on the highest yielded F \u03b2=1 . Finally, the last step is combining the outputs of the different classifiers for all the classes.", "labels": [], "entities": []}, {"text": "In case of conflicts, where the same token is tagged as two different NE classes, we use a simple heuristic based on the classifier precision for that specific tag, favoring the tag with the highest precision.", "labels": [], "entities": []}, {"text": "For each data set and each genre it shows the F-measure obtained using the best feature set and ML approach.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.996841311454773}]}, {"text": "We show results for both the dev and test data using the optimal number of features Best Feat-Set/ML contrasted against the system when using all 16 features per class All Feats/ML.", "labels": [], "entities": []}, {"text": "The table also illustrates three baseline results on the test data only.", "labels": [], "entities": []}, {"text": "FreqBaseline: For this baseline, we assign a test token the most frequent tag observed for it in the training data, if a test token is not observed in the training data, it is assigned the most frequent tag which is the O tag.", "labels": [], "entities": [{"text": "FreqBaseline", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8981834650039673}]}, {"text": "MLBaseline: In this baseline setting, we train an NER system with the full 16 features for all the NE classes at once.", "labels": [], "entities": [{"text": "MLBaseline", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.958861231803894}]}, {"text": "We use the two different ML approaches yielding two baselines: MLBaseline SV M and MLBaseline CRF . It is important to note the difference between the All Feats/ML setting and the MLBaseline setting.", "labels": [], "entities": [{"text": "MLBaseline SV M", "start_pos": 63, "end_pos": 78, "type": "METRIC", "confidence": 0.6103269259134928}, {"text": "MLBaseline CRF", "start_pos": 83, "end_pos": 97, "type": "DATASET", "confidence": 0.8203908205032349}]}, {"text": "In the former, All Feats/ML, all 16 features are used per class in a 4-way classifier system and then the classifications are combined and the conflicts are resolved using our simple heuristic while in the latter case of MLBaseline the classes are trained together with all 16 features for all classes in one system.", "labels": [], "entities": [{"text": "MLBaseline", "start_pos": 221, "end_pos": 231, "type": "DATASET", "confidence": 0.9248945116996765}]}, {"text": "Since different feature-sets and different ML approaches are used and combined for each experiment, it is not possible to present the number of features used in each experiment in.", "labels": [], "entities": []}, {"text": "However, shows the number of features and the ML approach used for each genre and NE class.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: F \u03b2=1 Results using 3-way vs. 4-way class anno- tations using SVM", "labels": [], "entities": [{"text": "F \u03b2", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9341748356819153}, {"text": "SVM", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.6568101644515991}]}, {"text": " Table 3: Ranked features according to FBVS using SVM  for each NE class", "labels": [], "entities": [{"text": "FBVS", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7817925810813904}]}, {"text": " Table 4: Final Results obtained with selected features contrasted against all features combined", "labels": [], "entities": []}]}