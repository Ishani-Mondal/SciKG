{"title": [{"text": "Generalizing Local and Non-Local Word-Reordering Patterns for Syntax-Based Machine Translation", "labels": [], "entities": [{"text": "Syntax-Based Machine Translation", "start_pos": 62, "end_pos": 94, "type": "TASK", "confidence": 0.6890734136104584}]}], "abstractContent": [{"text": "Syntactic word reordering is essential for translations across different grammar structures between syntactically distant language-pairs.", "labels": [], "entities": [{"text": "Syntactic word reordering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7006464600563049}]}, {"text": "In this paper, we propose to embed local and non-local word reordering decisions in asynchronous context free grammar , and leverages the grammar in a chart-based decoder.", "labels": [], "entities": []}, {"text": "Local word-reordering is effectively encoded in Hiero-like rules; whereas non-local word-reordering, which allows for long-range movements of syntactic chunks, is represented in tree-based reordering rules, which contain variables correspond to source-side syntactic constituents.", "labels": [], "entities": []}, {"text": "We demonstrate how these rules are learned from parallel corpora.", "labels": [], "entities": []}, {"text": "Our proposed shallow Tree-to-String rules show significant improvements in translation quality across different test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the main issues that a translator (human or machine) must address during the translation process is how to match the different word orders between the source language and the target language.", "labels": [], "entities": []}, {"text": "Different language-pairs require different levels of word reordering.", "labels": [], "entities": []}, {"text": "For example, when we translate between English and Spanish (or other Romance languages), most of the word reordering needed is local because of the shared syntactical features (e.g., Spanish noun modifier constructs are written in English as modifier noun).", "labels": [], "entities": []}, {"text": "However, for syntactically distant language-pairs such as ChineseEnglish, long-range reordering is required where whole phrases are moved across the sentence.", "labels": [], "entities": []}, {"text": "The idea of \"syntactic cohesion\") is characterized by its simplicity, which has attracted researchers for years.", "labels": [], "entities": []}, {"text": "Previous works include several approaches of incorporating syntactic information to preprocess the source sentences to make them more like the target language in structure.) described approaches applied to languagepairs such as French-English and German-English.", "labels": [], "entities": []}, {"text": "Later, presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English.", "labels": [], "entities": []}, {"text": "Overall, these works are similar, in that they design a few language-specific and linguistically motivated reordering rules, which are generally simple.", "labels": [], "entities": []}, {"text": "The eleven rules described in are appealing, as they have rather simple structure, modeling only NP, VP and LCP via one-level sub-tree structure with two children, in the source parse-tree (a special case of ITG ().", "labels": [], "entities": []}, {"text": "It effectively enhances the quality of the phrase-based translation of Chinese-to-English.", "labels": [], "entities": [{"text": "phrase-based translation of Chinese-to-English", "start_pos": 43, "end_pos": 89, "type": "TASK", "confidence": 0.7905613034963608}]}, {"text": "One major weakness is that the reordering decisions were done in the preprocessing step, therefore rendering the decoding process unable to recover the reordering errors from the rules if incorrectly applied to.", "labels": [], "entities": []}, {"text": "Also the reordering decisions are made without the benefits of additional models (e.g., the language models) that are typically used during decoding.", "labels": [], "entities": []}, {"text": "Another method to address the re-ordering problem in translation is the Hiero model proposed by, in which a probabilistic synchronous context free grammar (PSCFG) was applied to guide the decoding.", "labels": [], "entities": []}, {"text": "Hiero rules generalize phrase-pairs by introducing a single generic nonterminal (i.e., a variable).", "labels": [], "entities": []}, {"text": "The combination of variables and lexicalized words in a Hiero rule nicely captures local word and phrase reordering (modeling an implicit reordering window of max-phrase length).", "labels": [], "entities": []}, {"text": "These rules are then applied in a CYK-style decoder.", "labels": [], "entities": []}, {"text": "In Hiero rules, any nested phrase-pair can be generalized as variables.", "labels": [], "entities": []}, {"text": "This usually leads to too many redundant translations, which worsens the spurious ambiguities) problems for both decoding and optimization (i.e., parameter tuning).", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.6903094202280045}]}, {"text": "We found thatvariables (nonterminal) in Hiero rules offer a generalization too coarse to improve the effectiveness of hierarchical models' performance.", "labels": [], "entities": []}, {"text": "We propose to enrich the variables in Hiero rules with additional source syntactic reordering information, in the form of shallow Tree-to-String syntactic structures.", "labels": [], "entities": []}, {"text": "The syntactic information is represented by flat one-level sub-tree structures, with Hiero-like nonterminal variables at the leaf nodes.", "labels": [], "entities": []}, {"text": "The syntactic rules, proposed in this paper, are composed of (possibly lexicalized) source treelets and target surface strings, with one or more variables that help capture local-reordering similar to the Hiero rules.", "labels": [], "entities": []}, {"text": "Variables in a given rule are derived not only from the embedded aligned blocks (phrase-pairs), but also from the aligned source syntactic constituents.", "labels": [], "entities": []}, {"text": "The aligned constituents, as in our empirical observations for Chinese-English, tend to move together in translations.", "labels": [], "entities": []}, {"text": "The decoder is guided by these rules to reduce spurious derivations; the rules also constrain the exploration of the search space toward better translation quality and sometime improved speed by breaking long sentences into pieces.", "labels": [], "entities": [{"text": "speed", "start_pos": 186, "end_pos": 191, "type": "METRIC", "confidence": 0.981097936630249}]}, {"text": "Overall, what we want is to enable the long-range reordering decisions to be local in a chart-based decoder.", "labels": [], "entities": []}, {"text": "To be more specific, we think the simple shallow syntactic structure is powerful enough for capturing the major structure-reordering patterns, such as NP, VP and LCP structures.", "labels": [], "entities": []}, {"text": "We also use simple frequency-based feature functions, similar to the blocks used in phrase-based decoder, to further improve the rules' representation power.", "labels": [], "entities": []}, {"text": "Overall, this enables us to avoid either a complex decoding process to generate the source parse tree, or difficult combinatorial optimizations for the feature functions associated with rules.", "labels": [], "entities": []}, {"text": "In, hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training.", "labels": [], "entities": [{"text": "MER", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.8898123502731323}]}, {"text": "The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult.", "labels": [], "entities": []}, {"text": "Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work.", "labels": [], "entities": []}, {"text": "Word reordering can also be addressed via distortion models.", "labels": [], "entities": [{"text": "Word reordering", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.665222704410553}]}, {"text": "Work in) modeled the limited information available at phrase-boundaries.", "labels": [], "entities": []}, {"text": "Syntax-based approaches such as) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs.", "labels": [], "entities": []}, {"text": "Their algorithms are also subject to parsers' performances to a larger extent, and have high complexity and less scalability in reality.", "labels": [], "entities": []}, {"text": "In, multi-level tree-structured rules were designed, which made the decoding process very complex, and auxiliary rules have to be designed and incorporated to shrink multiple source nonterminals into one target nonterminal.", "labels": [], "entities": []}, {"text": "From our empirical observations, most of the time, however, the multi-level tree-structure is broken in the translation process, and POS tags are frequently distorted.", "labels": [], "entities": []}, {"text": "Indeed, strictly following the source parse tree is usually not necessary, and maybe too expensive for the translation process.", "labels": [], "entities": [{"text": "translation", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.9703274369239807}]}, {"text": "The remainder of this paper is structured as follows: in section \u00a7 2, we define the notations in our synchronous context free grammar, in section \u00a7 3, the rule extractions are illustrated in details, in section \u00a7 4, the decoding process of applying these rules is described.", "labels": [], "entities": [{"text": "rule extractions", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.7369507849216461}]}, {"text": "Experiments in \u00a7 5 were carried out using GALE Dev07 datasets.", "labels": [], "entities": [{"text": "GALE Dev07 datasets", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.951186458269755}]}, {"text": "Improved translation qualities were obtained by applying the proposed Tree-to-String rules.", "labels": [], "entities": []}, {"text": "Conclusions and discussions are given in \u00a7 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our training data consists of two corpora: the GALE Chinese-English parallel corpus and the LDC handaligned corpus . The Chinese side of these two corpora were parsed using a constituency parser.", "labels": [], "entities": [{"text": "GALE Chinese-English parallel corpus", "start_pos": 47, "end_pos": 83, "type": "DATASET", "confidence": 0.8875612765550613}, {"text": "LDC handaligned corpus", "start_pos": 92, "end_pos": 114, "type": "DATASET", "confidence": 0.684766044219335}]}, {"text": "The average labeled F-measure of the parser is 81.4%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.8179879784584045}]}, {"text": "Parallel sentences were first word-aligned using a MaxEnt aligner).", "labels": [], "entities": []}, {"text": "Then, phrase-pairs that overlap with our development and test set were extracted from the word alignments (from both hand alignments and automatically aligned GALE corpora) based on the projection principle.", "labels": [], "entities": []}, {"text": "Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora.", "labels": [], "entities": []}, {"text": "The detailed statistics are shown in.", "labels": [], "entities": []}, {"text": "Our re-implementation of Hiero system is the baseline.", "labels": [], "entities": [{"text": "Hiero system", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9548395276069641}]}, {"text": "We integrated the eleven reordering rules described in (, in our chart-based decoder.", "labels": [], "entities": []}, {"text": "In addition, we report the results of using the Tree-to-String rules extracted from the hand-aligned training data and the automatically aligned training data.", "labels": [], "entities": []}, {"text": "We also report the result of our translation quality in terms of both BLEU) and TER () against four human reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9992941617965698}, {"text": "TER", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9949700236320496}]}, {"text": "shows the statistics of our training, development and test data.", "labels": [], "entities": []}, {"text": "As our word aligner) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set \"CE16K\", which consists of 16K sentencepairs, to get relatively clean rules, free from alignment errors.", "labels": [], "entities": []}, {"text": "A much larger GALE data set, which consists of 10 million sentence-pairs, is used to investigate the scalability of our proposed approach.", "labels": [], "entities": [{"text": "GALE data set", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.7956108450889587}]}, {"text": "The NIST 2003 MT Evaluation (MT03) is used as our development data set to tune the decoder's parameters toward better BLEU score.", "labels": [], "entities": [{"text": "NIST 2003 MT Evaluation (MT03)", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.9035715886524746}, {"text": "BLEU score", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9811510443687439}]}, {"text": "The text part of GALE 2007 Chinese-to-English Development set (GALE DEV07) is used as our test set.", "labels": [], "entities": [{"text": "GALE 2007 Chinese-to-English Development set (GALE DEV07)", "start_pos": 17, "end_pos": 74, "type": "DATASET", "confidence": 0.9158642623159621}]}, {"text": "MT03 consists of 919 sentences, whereas GALE DEV07 consists of 2303 sentences under two genres: NewsWire and WebLog.", "labels": [], "entities": [{"text": "MT03", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9677261114120483}, {"text": "GALE DEV07", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8364747166633606}]}, {"text": "Both have four human reference translations.", "labels": [], "entities": []}, {"text": "We tuned the decoding parameters using the MT03 data set, and applied the updated parameters to the GALE evaluation set.", "labels": [], "entities": [{"text": "MT03 data set", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9842561284701029}, {"text": "GALE evaluation set", "start_pos": 100, "end_pos": 119, "type": "DATASET", "confidence": 0.7519728342692057}]}, {"text": "The eleven rules of VP, NP, and LCP (tree-based) improved the Hiero baseline 3 from 32.43 to 33.02 on BLEU.", "labels": [], "entities": [{"text": "Hiero baseline 3", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8325490156809489}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.6172676682472229}]}, {"text": "The reason, the treereordering does not gain much over Hiero baseline, is probably that the reordering patterns covered by tree-reordering rules, are potentially handled in the standard Hiero grammar.", "labels": [], "entities": []}, {"text": "A small but noticeable further improvement over tree-based rules, from 33.02 to 33.26, was obtained on applying Tree-to-String rules extracted from hand-aligned dataset.", "labels": [], "entities": []}, {"text": "We think that the Treebased rules covers major reordering patterns for Chinese-English, and our hand-aligned dataset is also too small to capture representative statistics and more reordering patterns.", "labels": [], "entities": []}, {"text": "A close check at the rules we learned from the hand-aligned data shows that the tree-based rules are simply the subset of the rules extracted.", "labels": [], "entities": []}, {"text": "The Tree-to-String grammar improved the Hiero baseline from 32.43 to 33.26 on BLEU; considering the effects from the tree-based rules only, the additional information improved the BLEU scores from 33.02 to 33.26.", "labels": [], "entities": [{"text": "Hiero baseline", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.7483580410480499}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9801629781723022}, {"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.9977468848228455}]}, {"text": "Similar pictures of improvements were observed for the two unseen tests of newswire and weblog in GALE data.", "labels": [], "entities": [{"text": "GALE data", "start_pos": 98, "end_pos": 107, "type": "DATASET", "confidence": 0.8833164274692535}]}, {"text": "When applying the rules extracted from the much Hiero results are from our own re-implementation.", "labels": [], "entities": []}, {"text": "larger GALE training set with about ten million sentence-pairs, we achieved significant improvements from both genres (newswire and web data).", "labels": [], "entities": [{"text": "GALE training set", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.6516329944133759}]}, {"text": "The improvements are significant in both BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9956645369529724}, {"text": "TER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9269254803657532}]}, {"text": "BLEU improved from 32.44 to 33.51 on newswire, and from 25.88 to 27.91 on web data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.98602294921875}]}, {"text": "Similar improvements were found in TER as shown in the table.", "labels": [], "entities": [{"text": "TER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.589891791343689}]}, {"text": "The gain came mostly from the richer extracted rule set, which not only presents robust statistics for reordering patterns, but also offers more target spontaneous words generated from the syntactic structures.", "labels": [], "entities": []}, {"text": "Since the top-frequent rules extracted are NP, VP, and IP as shown in, our proposed rules will be able to win the correct word order with reliable statistics, as long as the parser shows acceptable performances on these structures.", "labels": [], "entities": []}, {"text": "This is especially important for weblog data, where the parser's overall accuracy potentially might not be very good.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9987295269966125}]}, {"text": "shows the translations from different grammars for the same source sentence.", "labels": [], "entities": []}, {"text": "Both Treebased and Tree-to-String methods get the correct reordering, while the latter can suggest insertions of target spontaneous words like \"a\" to allow the translation to run more fluently.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training and Test Data", "labels": [], "entities": []}, {"text": " Table 2: Rules extracted from hand-aligned data", "labels": [], "entities": []}, {"text": " Table 3: Distributions of the NP, VP, QP, LCP rules", "labels": [], "entities": []}, {"text": " Table 4: Distribution of the reordering rules for subtrees  rooted at VP: [X,VP]; [X,PP] [X,VP]; statistics are col- lected from GALE training data", "labels": [], "entities": [{"text": "GALE training data", "start_pos": 130, "end_pos": 148, "type": "DATASET", "confidence": 0.8105960289637247}]}, {"text": " Table 6: Hiero, Tree-Based (NP, VP, LCP), and Tree-to-String rules extracted from hand-aligned data (H) or from  GALE training data (G)", "labels": [], "entities": [{"text": "GALE training data", "start_pos": 114, "end_pos": 132, "type": "DATASET", "confidence": 0.7529510458310446}]}]}