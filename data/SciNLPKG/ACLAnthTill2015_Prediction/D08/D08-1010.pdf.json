{"title": [{"text": "Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Rule Selection", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7640623450279236}, {"text": "Syntax-based Statistical Machine Translation", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.7264348492026329}]}], "abstractContent": [{"text": "This paper proposes a novel maximum en-tropy based rule selection (MERS) model for syntax-based statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.8043580452601115}]}, {"text": "The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules.", "labels": [], "entities": []}, {"text": "Therefore, our model allows the de-coder to perform context-dependent rule selection during decoding.", "labels": [], "entities": []}, {"text": "We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the tree-to-string alignment template model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.8956542611122131}]}, {"text": "Experiments show that our approach achieves significant improvements over the baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax-based statistical machine translation (SMT) models () capture long distance reorderings by using rules with structural and linguistical information as translation knowledge.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.8114285469055176}]}, {"text": "Typically, a translation rule consists of a source-side and a target-side.", "labels": [], "entities": []}, {"text": "However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules.", "labels": [], "entities": []}, {"text": "Therefore, during decoding, the decoder should select a correct target-side fora source-side.", "labels": [], "entities": []}, {"text": "We call this rule selection.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7488609254360199}]}, {"text": "Rule selection is of great importance to syntaxbased SMT systems.", "labels": [], "entities": [{"text": "Rule selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8232865631580353}, {"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8351485729217529}]}, {"text": "Comparing with word selection in word-based SMT and phrase selection in phrase-based SMT, rule selection is more generic and important.", "labels": [], "entities": [{"text": "word selection", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7617213428020477}, {"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.7910388112068176}, {"text": "phrase selection in phrase-based SMT", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.5874352991580963}, {"text": "rule selection", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.7583974897861481}]}, {"text": "This is because that a rule not only contains terminals (words or phrases), but also con- tains nonterminals and structural information.", "labels": [], "entities": []}, {"text": "Terminals indicate lexical translations, while nonterminals and structural information can capture short or long distance reorderings.", "labels": [], "entities": []}, {"text": "See rules in for illustration.", "labels": [], "entities": []}, {"text": "These two rules share the same syntactic tree on the source side.", "labels": [], "entities": []}, {"text": "However, on the target side, either the translations for terminals or the phrase reorderings for nonterminals are quite different.", "labels": [], "entities": []}, {"text": "During decoding, when a rule is selected and applied to a source text, both lexical translations (for terminals) and reorderings (for nonterminals) are determined.", "labels": [], "entities": []}, {"text": "Therefore, rule selection affects both lexical translation and phrase reordering.", "labels": [], "entities": [{"text": "lexical translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7137443274259567}, {"text": "phrase reordering", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7474468052387238}]}, {"text": "However, most of the current syntax-based systems ignore contextual information when they selecting rules during decoding, especially the information of sub-trees covered by nonterminals.", "labels": [], "entities": []}, {"text": "For example, the information of X 1 and X 2 is not recorded when the rules in extracted from the training examples in.", "labels": [], "entities": []}, {"text": "This makes the decoder hardly distinguish the two rules.", "labels": [], "entities": []}, {"text": "Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed to be helpful for rule selection.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 136, "end_pos": 150, "type": "TASK", "confidence": 0.8566480576992035}]}, {"text": "Recent research showed that contextual information can help perform word or phrase selection.", "labels": [], "entities": [{"text": "word or phrase selection", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6101828217506409}]}, {"text": "Carpuat and Wu (2007b) and showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively.", "labels": [], "entities": [{"text": "SMT", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.8005149960517883}]}, {"text": "Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8305758237838745}]}, {"text": "They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.8578643798828125}]}, {"text": "In this paper, we propose a novel solution for rule selection for syntax-based SMT.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7383138537406921}, {"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.8613005876541138}]}, {"text": "We use the maximum entropy approach to combine rich contextual information around a rule and the information of sub-trees covered by nonterminals in a rule.", "labels": [], "entities": []}, {"text": "For each ambiguous source-side of translation rules, a maximum entropy based rule selection (MERS) model is built.", "labels": [], "entities": []}, {"text": "Thus the MERS models can help the decoder to perform a context-dependent rule selection.", "labels": [], "entities": []}, {"text": "Comparing with WSD (or PSD), there are some advantages of our approach: \u2022 Our approach resolves ambiguity for rules with multi-level syntactic structure, while WSD resolves ambiguity for strings that have no structures; \u2022 Our approach can help the decoder perform both lexical selection and phrase reorderings, while WSD can help the decoder only perform lexical selection; \u2022 Our method takes WSD as a special case, since a rule may only consists of terminals.", "labels": [], "entities": [{"text": "phrase reorderings", "start_pos": 291, "end_pos": 309, "type": "TASK", "confidence": 0.7243480384349823}]}, {"text": "In our previous work, we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9116553664207458}]}, {"text": "In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.8063543438911438}]}, {"text": "The basic differences are: \u2022 The MERS model here combines rich information of source syntactic tree as features since the translation model is linguistically syntaxbased.", "labels": [], "entities": []}, {"text": "did not use this information.", "labels": [], "entities": []}, {"text": "\u2022 In this paper, we build MERS models for all ambiguous source-sides, including lexicalized (source-side which only contains terminals), partially lexicalized (source-side which contains both terminals and nonterminals), and unlexicalized (source-side which only contains nonterminals).", "labels": [], "entities": []}, {"text": "only built MERS models for partially lexicalized sourcesides.", "labels": [], "entities": []}, {"text": "In the TAT model, a TAT can be considered as a translation rule which describes correspondence between source syntactic tree and target string.", "labels": [], "entities": []}, {"text": "TAT can capture linguistically motivated reorderings at short or long distance.", "labels": [], "entities": [{"text": "TAT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8793891072273254}]}, {"text": "Experiments show that by incorporating MERS model, the baseline system achieves statistically significant improvement.", "labels": [], "entities": [{"text": "MERS", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.5918475985527039}]}, {"text": "This paper is organized as follows: Section 2 reviews the TAT model; Section 3 introduces the MERS model and describes feature definitions; Section 4 demonstrates a method to incorporate the MERS model into the translation model; Section 5 reports and analyzes experimental results; Section 6 gives conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005.", "labels": [], "entities": [{"text": "TATs", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.9078145027160645}, {"text": "NIST MT 2003", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.9182695746421814}]}, {"text": " Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used  by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for  WP and AP indicate a reward.", "labels": [], "entities": [{"text": "Lynx", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.9087682366371155}, {"text": "TP", "start_pos": 130, "end_pos": 132, "type": "METRIC", "confidence": 0.974047839641571}, {"text": "TAT penalty", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.928115725517273}, {"text": "AP=ambiguous TAT penalty", "start_pos": 163, "end_pos": 187, "type": "METRIC", "confidence": 0.7971685647964477}, {"text": "AP", "start_pos": 240, "end_pos": 242, "type": "METRIC", "confidence": 0.9747558236122131}]}, {"text": " Table 3: BLEU-4 scores (case-insensitive) on the test sets.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9990528225898743}]}, {"text": " Table 4: BLEU-4 scores on different feature sets.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9981018900871277}]}]}