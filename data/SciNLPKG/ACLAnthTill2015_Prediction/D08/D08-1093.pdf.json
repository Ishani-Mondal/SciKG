{"title": [], "abstractContent": [{"text": "Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications.", "labels": [], "entities": [{"text": "Statistical parsers", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7721350789070129}]}, {"text": "However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees.", "labels": [], "entities": [{"text": "estimating parsing", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8432559370994568}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.871209979057312}]}, {"text": "In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains.", "labels": [], "entities": []}, {"text": "As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical natural language parsers have recently become more accurate and more widely available.", "labels": [], "entities": [{"text": "natural language parsers", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.6685899694760641}]}, {"text": "As a result, they are being used in a variety of applications, such as question answering), speech recognition (), language modeling, language generation) and, most notably, machine translation (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8682349324226379}, {"text": "speech recognition", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.8074758946895599}, {"text": "language modeling", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7404988706111908}, {"text": "language generation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7437124103307724}, {"text": "machine translation", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.8485709428787231}]}, {"text": "These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute.", "labels": [], "entities": []}, {"text": "Ideally, one would want to have available a recipe for precisely answering this question: \"given a parser and a particular domain of interest, how accurate are the parse trees produced?\"", "labels": [], "entities": []}, {"text": "The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees fora sample set from the domain of interest, and consequently use them to compute a PARSEVAL ( score that is indicative of the intrinsic performance of the parser.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.9917112588882446}]}, {"text": "Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive.", "labels": [], "entities": []}, {"text": "The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (, and assume that the accuracy measure will carryover to the domains of interest.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.9188926815986633}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9974316954612732}]}, {"text": "This recipe, albeit cheap, cannot provide any guarantee regarding the performance of a parser on anew domain, and, as experiments in this paper show, can give wrong indications regarding important decisions for the design of NLP systems that use a syntactic parser as an important component.", "labels": [], "entities": []}, {"text": "This paper proposes another method for measuring the performance of a parser on a given domain that is both cheap and effective.", "labels": [], "entities": []}, {"text": "It is a fully automated procedure (no expensive annotation involved) that uses properties of both the domain of interest and the domain on which the parser was trained in order to measure the performance of the parser on the domain of interest.", "labels": [], "entities": []}, {"text": "It is, in essence, a solution to the following prediction problem: Input: (1) a statistical parser and its training data, (2) some chunk of text from anew domain or genre Output: an estimate of the accuracy of the parse trees produced for that chunk of text Accurate estimations for this prediction problem will allow a system designer to make the right decisions for the given domain of interest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9902967810630798}]}, {"text": "Such decisions include, but are not restricted to, the choice of the parser, the choice of the training data, the choice of how to implement various components such as the treatment of unknown words, etc.", "labels": [], "entities": [{"text": "treatment of unknown words", "start_pos": 172, "end_pos": 198, "type": "TASK", "confidence": 0.8229948133230209}]}, {"text": "Altogether, a correct estimation of the impact of such decisions on the resulting parse trees can guide a system designer in a hill-climbing scenario for which an extrinsic metric (such as the impact on the overall quality of the system) is usually too expensive to be employed often enough.", "labels": [], "entities": []}, {"text": "To provide an example, a machine translation engine that requires parse trees as training data in order to learn syntax-based translation rules () needs to employ a syntactic parser as soon as the training process starts, but it can take up to hundreds and even thousands of CPU hours (for large training data sets) to train the engine before translations can be produced and measured.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7060565948486328}]}, {"text": "Although areal estimate of the impact of a parser design decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision.", "labels": [], "entities": []}, {"text": "On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly training procedure.", "labels": [], "entities": []}], "datasetContent": [{"text": "There have been previous studies which explored the problem of automatically predicting the task difficulty for various NLP applications.) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations.) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 223, "end_pos": 242, "type": "TASK", "confidence": 0.7169092446565628}]}, {"text": "There have been a few studies of English parser accuracy in domains/genres other than WSJ), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non-WSJ domain of interest.", "labels": [], "entities": [{"text": "English parser", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.4702583998441696}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.7714515328407288}, {"text": "WSJ", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8330584764480591}]}, {"text": "Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9426168203353882}]}, {"text": "He looked at sentences with 40 words or less.", "labels": [], "entities": []}, {"text": "() carried out a similar experiment on sentences of all lengths, and () report additional results.", "labels": [], "entities": []}, {"text": "The table below shows results from our own measurements of Charniak parser 1) accuracy (F-measure on sentences of all lengths), which are consistent with these studies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.7494101524353027}, {"text": "F-measure", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9916946291923523}]}, {"text": "For the Brown corpus, the test set was formed from every tenth sentence in the corpus Here we investigate algorithms for predicting the accuracy of a parser P on sentences, chunks of sentences, and whole corpora.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9200466871261597}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9935450553894043}]}, {"text": "We also investigate and contrast several scenarios for prediction: (1) the predictor looks at the input text only, (2) the predictor looks at the input text and the output parse trees of P, and (3) the predictor looks at the input text, the output parse trees of P, and the outputs of other programs, such as the output parse trees of a different parser P ref used as a reference.", "labels": [], "entities": []}, {"text": "Under none of these scenarios is the predictor allowed to look at goldstandard parses in the new domain/genre.", "labels": [], "entities": []}, {"text": "The intuition behind what we are trying to achieve here can be compared to an analogous task-trying to assess the performance of a median student from a math class on a given test, without having access to the answer sheet.", "labels": [], "entities": []}, {"text": "Looking at the test only, we could probably tell whether the test looks hard or not, and therefore whether the student will do well or not.", "labels": [], "entities": []}, {"text": "Looking at the student's answers will likely give us an even better idea of the performance.", "labels": [], "entities": []}, {"text": "Finally, the answers of a second student with similar proficiency will provide even better clues: if the students agree on every answer, then they probably both did well, but if they disagree frequently, then they (and hence our student) probably did not do as well.", "labels": [], "entities": []}, {"text": "Our first experiments are concerned with validating the idea itself: can a predictor be trained such that it predicts the same F-scores as the ones obtained using gold-trees?", "labels": [], "entities": [{"text": "F-scores", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9829905033111572}]}, {"text": "We first validate this using the WSJ corpus itself, by dividing the WSJ treebank into several sections: 1.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9591869711875916}, {"text": "WSJ treebank", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9824493229389191}]}, {"text": "Training (WSJ section 02-21).", "labels": [], "entities": [{"text": "WSJ section 02-21)", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.7808484137058258}]}, {"text": "The parser P is trained on this data.", "labels": [], "entities": []}, {"text": "2. Development (WSJ section 24).", "labels": [], "entities": [{"text": "Development", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9676547646522522}, {"text": "WSJ section 24)", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.8165033906698227}]}, {"text": "We use this data for training our predictor.", "labels": [], "entities": [{"text": "predictor", "start_pos": 34, "end_pos": 43, "type": "TASK", "confidence": 0.9553496241569519}]}, {"text": "3. Test (WSJ section 23).", "labels": [], "entities": [{"text": "WSJ section 23)", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.8429999053478241}]}, {"text": "We use this data for measuring our predictions.", "labels": [], "entities": []}, {"text": "For each test sentence, we compute (1) the PARSEVAL F-measure score using the test gold standard, and (2) our predicted F-measure.", "labels": [], "entities": [{"text": "PARSEVAL F-measure score", "start_pos": 43, "end_pos": 67, "type": "METRIC", "confidence": 0.7833000719547272}, {"text": "F-measure", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9647093415260315}]}, {"text": "We report the correlation coefficient (r) between the actual F-scores and our predicted Fscores.", "labels": [], "entities": [{"text": "correlation coefficient (r)", "start_pos": 14, "end_pos": 41, "type": "METRIC", "confidence": 0.9547593593597412}, {"text": "F-scores", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9780454635620117}, {"text": "Fscores", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9902960658073425}]}, {"text": "We will also use a root-mean-square error (rms error) metric to compare actual and predicted F-scores.", "labels": [], "entities": [{"text": "root-mean-square error (rms error) metric", "start_pos": 19, "end_pos": 60, "type": "METRIC", "confidence": 0.889808884688786}, {"text": "F-scores", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9225718379020691}]}, {"text": "Section 3 describes the features used by our predictor.", "labels": [], "entities": []}, {"text": "Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9655649065971375}]}, {"text": "To this end, we use SVM-Regression 2) with an RBF kernel, to learn the feature weights and build our predictor system.", "labels": [], "entities": []}, {"text": "We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown corpus (Section 5).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9992201328277588}, {"text": "WSJ", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9287186861038208}, {"text": "Brown corpus", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.952413946390152}]}], "tableCaptions": [{"text": " Table 1: Comparison of correlation (r) obtained using MCT versus  SVM-Regression on development corpus.", "labels": [], "entities": [{"text": "correlation (r)", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.90570929646492}]}, {"text": " Table 2: Performance of predictor on n-sentence chunks from WSJ-test  (Correlation and rms error between actual/predicted accuracies).", "labels": [], "entities": [{"text": "WSJ-test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.885063886642456}]}, {"text": " Table 3: Comparing Charniak parser accuracy (from different systems)  on entire WSJ-test corpus", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8207309246063232}, {"text": "WSJ-test", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.8036377429962158}]}, {"text": " Table 4: Performance of predictor on n-sentence chunks from WSJ-test  and Brown-test (rms error between actual/predicted accuracies).", "labels": [], "entities": [{"text": "WSJ-test", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.933997631072998}]}, {"text": " Table 5: Charniak parser accuracy on entire Brown-test corpus", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9844180345535278}]}, {"text": " Table 6: Performance of predictor on the Xinhua News domain, com- pared with actual F-scores.", "labels": [], "entities": [{"text": "Xinhua News domain", "start_pos": 42, "end_pos": 60, "type": "DATASET", "confidence": 0.9642565449078878}, {"text": "F-scores", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9876732230186462}]}]}