{"title": [{"text": "Word Sense Disambiguation Using OntoNotes: An Empirical Study", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.703943928082784}]}], "abstractContent": [{"text": "The accuracy of current word sense disam-biguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as alack of training examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999151349067688}, {"text": "word sense disam-biguation (WSD)", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.757894809047381}, {"text": "WordNet", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9372432231903076}]}, {"text": "Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory.", "labels": [], "entities": [{"text": "WSD evaluation", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.8722032010555267}]}, {"text": "We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop inaccuracy when applied to a different domain.", "labels": [], "entities": [{"text": "WSD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9610078930854797}, {"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9988492727279663}]}, {"text": "To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7384966611862183}]}, {"text": "Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to anew domain.", "labels": [], "entities": [{"text": "WSD", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9366466403007507}]}, {"text": "Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types.", "labels": [], "entities": [{"text": "WSD", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.8622882962226868}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9285220503807068}]}], "introductionContent": [{"text": "In language, many words have multiple meanings.", "labels": [], "entities": []}, {"text": "The process of identifying the correct meaning, or sense of a word in context, is known as word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.7794471085071564}]}, {"text": "WSD is one of the fundamental problems in natural language processing and is important for applications such as machine translation (MT) (, information retrieval (IR), etc.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8670648336410522}, {"text": "natural language processing", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.6557010511557261}, {"text": "machine translation (MT)", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.8371109008789063}, {"text": "information retrieval (IR)", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.8531892478466034}]}, {"text": "WSD is typically viewed as a classification problem where each ambiguous word is assigned a sense label (from a pre-defined sense inventory) during the disambiguation process.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9659389853477478}]}, {"text": "In current WSD research, WordNet is usually used as the sense inventory.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9678421020507812}, {"text": "WordNet", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.942828893661499}]}, {"text": "WordNet, however, adopts a very fine level of sense granularity, thus restricting the accuracy of WSD systems.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9719007015228271}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9986605644226074}, {"text": "WSD", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.8626397848129272}]}, {"text": "Also, current state-of-the-art WSD systems are based on supervised learning and face a general lack of training data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9648884534835815}]}, {"text": "To provide a standardized test-bed for evaluation of WSD systems, a series of evaluation exercises called SENSEVAL were held.", "labels": [], "entities": [{"text": "WSD", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9781761765480042}, {"text": "SENSEVAL", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.8109771609306335}]}, {"text": "In the English all-words task of SENSEVAL-2 and SENSEVAL-3 (, no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses.", "labels": [], "entities": []}, {"text": "In SENSEVAL-2, the best performing system) in the English all-words task achieved an accuracy of 69.0%, while in SENSEVAL-3, the best performing system) achieved an accuracy of 65.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.999075174331665}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9990085959434509}]}, {"text": "In SemEval-2007, which was the most recent SENSEVAL evaluation, a similar English all-words task was held, where systems had to provide the correct WordNet sense tag for all the verbs and head words of their arguments in running English texts.", "labels": [], "entities": []}, {"text": "For this task, the best performing system ( achieved an accuracy of 59.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9997072815895081}]}, {"text": "Results of these evaluations showed that state-of-the-art English all-words WSD systems performed with an accuracy of 60%-70%, using the fine-grained sense inventory of WordNet.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9991549253463745}, {"text": "WordNet", "start_pos": 169, "end_pos": 176, "type": "DATASET", "confidence": 0.9631015658378601}]}, {"text": "The low level of performance by these state-ofthe-art WSD systems is a cause for concern, since WSD is supposed to bean enabling technology to be incorporated as a module into applications such as MT and IR.", "labels": [], "entities": [{"text": "MT", "start_pos": 197, "end_pos": 199, "type": "TASK", "confidence": 0.9818341135978699}]}, {"text": "As mentioned earlier, one of the major reasons for the low performance is that these evaluation exercises adopted WordNet as the reference sense inventory, which is often too finegrained.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9616482853889465}]}, {"text": "As an indication of this, inter-annotator agreement (ITA) reported for manual sense-tagging on these SENSEVAL English all-words datasets is typically in the mid-70s.", "labels": [], "entities": [{"text": "inter-annotator agreement (ITA)", "start_pos": 26, "end_pos": 57, "type": "METRIC", "confidence": 0.7873436748981476}, {"text": "SENSEVAL English all-words datasets", "start_pos": 101, "end_pos": 136, "type": "DATASET", "confidence": 0.7635825723409653}]}, {"text": "To address this issue, a coarse-grained English all-words task () was conducted during SemEval-2007.", "labels": [], "entities": []}, {"text": "This task used a coarse-grained version of WordNet and reported an ITA of around 90%.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.96681809425354}, {"text": "ITA", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9952412843704224}]}, {"text": "We note that the best performing system ( of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9992730021476746}]}, {"text": "Another issue faced by current WSD systems is the lack of training data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9637407660484314}]}, {"text": "We note that the top performing systems mentioned in the previous paragraphs are all based on supervised learning.", "labels": [], "entities": []}, {"text": "With this approach, however, one would need to obtain a corpus where each ambiguous word occurrence is manually annotated with the correct sense, to serve as training data.", "labels": [], "entities": []}, {"text": "Since it is time consuming to perform sense annotation of word occurrences, only a handful of sense-tagged corpora are publicly available.", "labels": [], "entities": []}, {"text": "Among the existing sense-tagged corpora, the SEMCOR corpus) is one of the most widely used.", "labels": [], "entities": [{"text": "SEMCOR corpus", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.871550589799881}]}, {"text": "In SEMCOR, content words have been manually tagged with WordNet senses.", "labels": [], "entities": [{"text": "WordNet senses", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.918871134519577}]}, {"text": "Current supervised WSD systems (which include all the top-performing systems in the English all-words task) usually rely on this relatively small manually annotated corpus for training examples, and this has inevitably affected the accuracy and scalability of current WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9462295770645142}, {"text": "accuracy", "start_pos": 232, "end_pos": 240, "type": "METRIC", "confidence": 0.9984917640686035}]}, {"text": "Related to the problem of alack of training data for WSD, there is also alack of test data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.8853088021278381}]}, {"text": "Having a large amount of test data for evaluation is important to ensure the robustness and scalability of WSD systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9600028991699219}]}, {"text": "Due to the expensive process of manual sense-tagging, the SENSEVAL English all-words task evaluations were conducted on relatively small sets of evaluation data.", "labels": [], "entities": [{"text": "SENSEVAL English all-words task evaluations", "start_pos": 58, "end_pos": 101, "type": "TASK", "confidence": 0.7970116138458252}]}, {"text": "For instance, the evaluation data of SENSEVAL-2 and SENSEVAL-3 English all-words task consists of 2,473 and 2,041 test examples respectively.", "labels": [], "entities": []}, {"text": "In SemEval-2007, the fine-grained English all-words task consists of only 465 test examples, while the SemEval-2007 coarse-grained English all-words task consists of 2,269 test examples.", "labels": [], "entities": [{"text": "SemEval-2007 coarse-grained English all-words task", "start_pos": 103, "end_pos": 153, "type": "TASK", "confidence": 0.608634889125824}]}, {"text": "Hence, it is necessary to address the issues of sense granularity, and the lack of both training and test data.", "labels": [], "entities": []}, {"text": "To this end, a recent large-scale annotation effort called the OntoNotes project) was started.", "labels": [], "entities": []}, {"text": "Building on the annotations from the Wall Street Journal (WSJ) portion of the Penn Treebank (, the project added several new layers of semantic annotations, such as coreference information, word senses, etc.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 37, "end_pos": 91, "type": "DATASET", "confidence": 0.9298324964263223}]}, {"text": "In its first release (LDC2007T21) through the Linguistic Data Consortium (LDC), the project manually sense-tagged more than 40,000 examples belonging to hundreds of noun and verb types with an ITA of 90%, based on a coarse-grained sense inventory, where each word has an average of only 3.2 senses.", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 46, "end_pos": 78, "type": "DATASET", "confidence": 0.8191232283910116}, {"text": "ITA", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9839372038841248}]}, {"text": "Thus, besides providing WSD examples that were sense-tagged with a high ITA, the project also addressed the previously discussed issues of alack of training and test data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9425710439682007}]}, {"text": "In this paper, we use the sense-tagged data provided by the OntoNotes project to investigate the accuracy achievable by current WSD systems when adopting a coarse-grained sense inventory.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9984421133995056}]}, {"text": "Through our experiments, we then highlight that domain adaptation for WSD is an important issue as it substantially affects the performance of a state-of-theart WSD system which is trained on SEMCOR but evaluated on sense-tagged examples in OntoNotes.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7056633979082108}, {"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.950712263584137}, {"text": "SEMCOR", "start_pos": 192, "end_pos": 198, "type": "DATASET", "confidence": 0.9175857901573181}, {"text": "OntoNotes", "start_pos": 241, "end_pos": 250, "type": "DATASET", "confidence": 0.9517644047737122}]}, {"text": "To address this issue, we then show that by combining a domain adaptation technique using feature augmentation with active learning, one only needs to annotate a small amount of in-domain examples to obtain a substantial improvement in the accuracy of the WSD system which is previously trained on out-of-domain examples.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7357365190982819}, {"text": "accuracy", "start_pos": 240, "end_pos": 248, "type": "METRIC", "confidence": 0.9988576173782349}]}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first large-scale WSD evaluation conducted that involves hundreds of word types and tens of thousands of sense-tagged examples, and that is based on a coarse-grained sense inventory.", "labels": [], "entities": [{"text": "WSD evaluation", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.8999542891979218}]}, {"text": "The present study also highlights the practical significance of domain adaptation in word sense disambiguation in the context of a large-scale empirical evaluation, and proposes an effective method to address the domain adaptation problem.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.6725138525168101}, {"text": "domain adaptation", "start_pos": 213, "end_pos": 230, "type": "TASK", "confidence": 0.7245433926582336}]}, {"text": "In the next section, we give a brief description of our WSD system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9696085453033447}]}, {"text": "In Section 3, we describe experiments where we conduct both training and evaluation using data from OntoNotes.", "labels": [], "entities": [{"text": "OntoNotes", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.9188165068626404}]}, {"text": "In Section 4, we investigate the WSD performance when we train our system on examples that are gathered from a different domain as compared to the OntoNotes evaluation data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9777481555938721}, {"text": "OntoNotes evaluation data", "start_pos": 147, "end_pos": 172, "type": "DATASET", "confidence": 0.8259905974070231}]}, {"text": "In Section 5, we perform domain adaptation experiments using a recently introduced feature augmentation technique.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.816576361656189}]}, {"text": "In Section 6, we investigate the use of active learning to reduce the annotation effort required to adapt our WSD system to the domain of the OntoNotes data, before concluding in Section 7.", "labels": [], "entities": [{"text": "OntoNotes data", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.9238214194774628}]}], "datasetContent": [{"text": "As mentioned earlier, we use the examples in OntoNotes sections 02-21 as our adaptation exam-ples during active learning.", "labels": [], "entities": []}, {"text": "Hence, we perform active learning experiments on all the word types that have sense-tagged examples from OntoNotes sections 02-21, and show the evaluation results on OntoNotes section 23 as the topmost \"all\" curve in.", "labels": [], "entities": [{"text": "OntoNotes sections 02-21", "start_pos": 105, "end_pos": 129, "type": "DATASET", "confidence": 0.9166280229886373}, {"text": "OntoNotes section 23", "start_pos": 166, "end_pos": 186, "type": "DATASET", "confidence": 0.9215670625368754}]}, {"text": "Since our aim is to reduce the human annotation effort required in adapting a WSD system to anew domain, we may not want to perform active learning on all the word types in practice.", "labels": [], "entities": [{"text": "WSD", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9340925812721252}]}, {"text": "Instead, we can maximize the benefits by performing active learning only on the more frequently occurring word types.", "labels": [], "entities": []}, {"text": "Hence, in, we also show via various curves the results of applying active learning only to various sets of word types, according to their frequency, or number of sense-tagged examples in OntoNotes sections 02-21.", "labels": [], "entities": []}, {"text": "Note that the various accuracy curves in are plotted in terms of evaluation accuracies overall the test examples in OntoNotes section 23, hence they are directly comparable to the results reported thus far in this paper.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9992138147354126}, {"text": "OntoNotes section", "start_pos": 116, "end_pos": 133, "type": "DATASET", "confidence": 0.836461216211319}]}, {"text": "Also, since the accuracies for the various curves stabilize after 35 active learning iterations, we only show the results of the first 35 iterations.", "labels": [], "entities": []}, {"text": "From, we note that by performing active learning on the set of 150 most frequently occurring word types, we are able to achieve a WSD accuracy of 82.6% after 10 active learning iterations.", "labels": [], "entities": [{"text": "WSD accuracy", "start_pos": 130, "end_pos": 142, "type": "METRIC", "confidence": 0.6224385201931}]}, {"text": "Note that in Section 4, we mentioned that training only on the out-of-domain SEMCOR examples gave an accuracy of 76.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9996172189712524}]}, {"text": "Hence, we have gained an accuracy improvement of 6.4% (82.6% \u2212 76.2%) by just using 1,500 in-domain OntoNotes examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995099306106567}]}, {"text": "Compared with the 12.9% (89.1% \u2212 76.2%) improvement inaccuracy achieved by using all 31,114 OntoNotes sections 02-21 examples, we have obtained half of this maximum increase inaccuracy, by requiring only about 5% (1,500/31,114) of the total number of sense-tagged examples.", "labels": [], "entities": []}, {"text": "Based on these results, we propose that when there is a need to apply a previously trained WSD system to a different domain, one can apply the AUGMENT technique with active learning on the most frequent word types, to greatly reduce the annotation effort required while obtaining a substantial improvement inaccuracy.", "labels": [], "entities": [{"text": "AUGMENT", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.8435885906219482}]}], "tableCaptions": [{"text": " Table 1: Size of the sense-tagged data in the various WSJ  sections.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.7264551520347595}]}]}