{"title": [], "abstractContent": [{"text": "Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-based systems that need parse trees from either or both sides of the bi-text.", "labels": [], "entities": [{"text": "Translation rule extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8806358377138773}, {"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7660521864891052}]}, {"text": "The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.", "labels": [], "entities": [{"text": "parsing", "start_pos": 106, "end_pos": 113, "type": "TASK", "confidence": 0.9704602360725403}]}, {"text": "So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.", "labels": [], "entities": []}, {"text": "Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30-best parses.", "labels": [], "entities": [{"text": "translation", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9562485218048096}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9989413619041443}]}, {"text": "When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the base-line, and even outperforms the hierarchical system of Hiero by 0.7 points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9997153878211975}, {"text": "Hiero", "start_pos": 173, "end_pos": 178, "type": "DATASET", "confidence": 0.913604199886322}]}], "introductionContent": [{"text": "Automatic extraction of translation rules is a fundamental problem in statistical machine translation, especially for many syntax-based models where translation rules directly encode linguistic knowledge.", "labels": [], "entities": [{"text": "Automatic extraction of translation rules", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.856142771244049}, {"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6512214044729868}]}, {"text": "Typically, these models extract rules using parse trees from both or either side(s) of the bitext.", "labels": [], "entities": []}, {"text": "The former case, with trees on both sides, is often called tree-to-tree models; while the latter case, with trees on either source or target side, include both treeto-string and string-to-tree models (see).", "labels": [], "entities": []}, {"text": "Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in source target examples (partial) tree-to-tree tree-to-string; string-to-tree string-to-string: A classification of syntax-based MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 279, "end_pos": 281, "type": "TASK", "confidence": 0.8640263676643372}]}, {"text": "The first three use linguistic syntax, while the last one only formal syntax.", "labels": [], "entities": []}, {"text": "Our experiments cover the second type using a packed forest in place of the tree for rule-extraction.", "labels": [], "entities": []}, {"text": "handling non-local reorderings, and have achieved promising translation results.", "labels": [], "entities": [{"text": "translation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9662306904792786}]}, {"text": "However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors.", "labels": [], "entities": [{"text": "rule extractor", "start_pos": 64, "end_pos": 78, "type": "TASK", "confidence": 0.7426836490631104}]}, {"text": "To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9792801737785339}]}, {"text": "By contrast, formally syntax-based models () do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts.", "labels": [], "entities": []}, {"text": "To alleviate this problem, an obvious idea is to extract rules from k-best parses instead.", "labels": [], "entities": []}, {"text": "However, a k-best list, with its limited scope, has too few variations and too many redundancies.", "labels": [], "entities": []}, {"text": "This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space.", "labels": [], "entities": []}, {"text": "In addition, many subtrees are repeated across different parses, so it is IP NP x 1 :NPB CC y\u02c7uy\u02c7u x 2 :NPB x 3 :VPB \u2192 x 1 x 3 with x 2 Figure 1: Example translation ruler 1 . The Chinese conjunction y\u02c7uy\u02c7u \"and\" is translated into English prep.", "labels": [], "entities": []}, {"text": "also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k 2 similar tree-pairs in tree-to-tree models).", "labels": [], "entities": []}, {"text": "We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists.", "labels": [], "entities": []}, {"text": "Experiments (Section 5) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system (, which is also 0.5 points better than (and twice as fast as) extracting on 30-best parses.", "labels": [], "entities": [{"text": "forestbased extraction", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.6584562063217163}, {"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9730599820613861}]}, {"text": "When combined with our previous orthogonal work on forest-based decoding, the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9995342493057251}, {"text": "Hiero", "start_pos": 207, "end_pos": 212, "type": "DATASET", "confidence": 0.9171369671821594}]}, {"text": "Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models () where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: BLEU score results trained on large data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9603975117206573}]}]}