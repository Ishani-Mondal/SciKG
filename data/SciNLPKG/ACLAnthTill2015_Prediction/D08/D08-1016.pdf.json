{"title": [{"text": "Dependency Parsing by Belief Propagation *", "labels": [], "entities": [{"text": "Dependency Parsing by Belief Propagation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8285871386528015}]}], "abstractContent": [{"text": "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7185802310705185}]}, {"text": "We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference.", "labels": [], "entities": [{"text": "loopy belief propagation (BP)", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7907994290192922}]}, {"text": "As a parsing algorithm, BP is both asymptotically and empirically efficient.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9666869640350342}, {"text": "BP", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.7808986902236938}]}, {"text": "Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n 3) time with a small constant factor.", "labels": [], "entities": [{"text": "BP", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.5721383690834045}]}, {"text": "Furthermore, such features significantly improve parse accuracy over exact first-order methods.", "labels": [], "entities": [{"text": "parse", "start_pos": 49, "end_pos": 54, "type": "TASK", "confidence": 0.9612339735031128}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8739559650421143}]}, {"text": "Incorporating additional features would increase the runtime additively rather than multiplicatively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational linguists worry constantly about runtime.", "labels": [], "entities": []}, {"text": "Sometimes we oversimplify our models, trading linguistic nuance for fast dynamic programming.", "labels": [], "entities": []}, {"text": "Alternatively, we write down a better but intractable model and then use approximations.", "labels": [], "entities": []}, {"text": "The CL community has often approximated using heavy pruning or reranking, but is beginning to adopt other methods from the machine learning community, such as Gibbs sampling, rejection sampling, and certain variational approximations.", "labels": [], "entities": []}, {"text": "We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation.", "labels": [], "entities": [{"text": "loopy belief propagation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.6494182844956716}]}, {"text": "In this paper, we show that BP can be used to train and decode complex parsing models.", "labels": [], "entities": [{"text": "BP", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.8759337067604065}]}, {"text": "Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.", "labels": [], "entities": [{"text": "parsing problem", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.9123274683952332}]}], "datasetContent": [{"text": "We asked: (1) For projective parsing, where higherorder factors have traditionally been incorporated into slow but exact dynamic programming (DP), what are the comparative speed and quality of the BP approximation?", "labels": [], "entities": [{"text": "projective parsing", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6605578362941742}]}, {"text": "(2) How helpful are such higherorder factors-particularly for non-projective parsing, where BP is needed to make them tractable?", "labels": [], "entities": [{"text": "BP", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9977009892463684}]}, {"text": "(3) Do our global constraints (e.g., TREE) contribute to the goodness of BP's approximation?", "labels": [], "entities": [{"text": "TREE", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9970184564590454}]}, {"text": "One could use coarser estimates at earlier stages of training, by running fewer iterations of BP.", "labels": [], "entities": [{"text": "BP", "start_pos": 94, "end_pos": 96, "type": "DATASET", "confidence": 0.5438268184661865}]}, {"text": "The BP framework makes it tempting to extend an MRF model with various sorts of latent variables, whose values are not specified in training data.", "labels": [], "entities": [{"text": "BP framework", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8444425165653229}, {"text": "MRF", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8919911980628967}]}, {"text": "It is straightforward to train under these conditions.", "labels": [], "entities": []}, {"text": "When counting which features fire on a training parse or (for error-driven training) on an current erroneous parse, we can find expected counts if these parses are not fully observed, by using BP to sum over latent variables.", "labels": [], "entities": [{"text": "BP", "start_pos": 193, "end_pos": 195, "type": "METRIC", "confidence": 0.9523681402206421}]}, {"text": "We trained all models using stochastic gradient descent ( \u00a77).", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6232206324736277}]}, {"text": "SGD initialized \u03b8 = 0 and ran for 10 consecutive passes over the data; we picked the stopping point that performed best on held-out data.", "labels": [], "entities": []}, {"text": "When comparing runtimes for projective parsers, we took care to produce comparable implementations.", "labels": [], "entities": []}, {"text": "All beliefs and dynamic programming items were stored and indexed using the high-level Dyna language, while all inference and propagation was written in C++.", "labels": [], "entities": []}, {"text": "The BP parser averaged 1.8 seconds per sentence for non-projective parsing and 1.5 seconds per sentence for projective parsing (1.2 and 0.9 seconds/sentence for \u2264 40 words), using our standard setup, which included five iterations of BP and the final MBR tree decoding pass.", "labels": [], "entities": [{"text": "MBR tree decoding pass", "start_pos": 251, "end_pos": 273, "type": "DATASET", "confidence": 0.8686252236366272}]}, {"text": "In our tables, we boldface the best result in each column along with any results that are not significantly worse (paired permutation test, p < .05).", "labels": [], "entities": [{"text": "paired permutation test", "start_pos": 115, "end_pos": 138, "type": "METRIC", "confidence": 0.9127279122670492}]}], "tableCaptions": [{"text": " Table 2: (a) Percent unlabeled dependency accuracy for  various non-projective BP parsers (5 iterations only),  showing the cumulative contribution of different features.  (b) Accuracy for an projective DP parser with all features.", "labels": [], "entities": [{"text": "Percent unlabeled dependency accuracy", "start_pos": 14, "end_pos": 51, "type": "METRIC", "confidence": 0.8375288993120193}, {"text": "Accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9933358430862427}]}, {"text": " Table 3: After training a non-projective first-order model  with TREE, decoding it with weaker constraints is asymp- totically faster (except for NOT2) but usually harm- ful. (Parenthetical numbers show that the harm is com- pounded if the weaker constraints are used in training  as well; even though this matches training to test con- ditions, it may suffer more from BP's approximate gradi- ents.) Decoding the TREE model with the even stronger  PTREE constraint can actually be helpful for a more pro- jective language. All results use 5 iterations of BP.", "labels": [], "entities": [{"text": "PTREE", "start_pos": 450, "end_pos": 455, "type": "METRIC", "confidence": 0.9712315201759338}]}]}