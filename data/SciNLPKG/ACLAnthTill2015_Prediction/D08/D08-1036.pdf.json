{"title": [{"text": "A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers", "labels": [], "entities": []}], "abstractContent": [{"text": "There is growing interest in applying Bayesian techniques to NLP problems.", "labels": [], "entities": []}, {"text": "There area number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.", "labels": [], "entities": []}, {"text": "This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.", "labels": [], "entities": [{"text": "Hidden Markov Model POS taggers", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6753641843795777}]}, {"text": "Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM.", "labels": [], "entities": [{"text": "HMM POS tagging", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7431355317433676}]}, {"text": "We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study.", "labels": [], "entities": [{"text": "HMMs", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.9222296476364136}]}, {"text": "We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.", "labels": [], "entities": []}, {"text": "In terms of times of convergence , we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic models now play a central role in computational linguistics.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.7752943336963654}]}, {"text": "These models define a probability distribution P(x) over structures or analyses x.", "labels": [], "entities": []}, {"text": "For example, in the part-of-speech (POS) tagging application described in this paper, which involves predicting the part-of-speech tag ti of each word w i in the sentence w = (w 1 , . .", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6371797144412994}]}, {"text": ", w n ), the structure x = (w, t) consists of the words win a sentence together with their corresponding parts-ofspeech t = (t 1 , . .", "labels": [], "entities": []}, {"text": "In general the probabilistic models used in computational linguistics have adjustable parameters \u03b8 which determine the distribution P(x | \u03b8).", "labels": [], "entities": []}, {"text": "In this paper we focus on bitag Hidden Markov Models (HMMs).", "labels": [], "entities": []}, {"text": "Since our goal here is to compare algorithms rather than achieve the best performance, we keep the models simple by ignoring morphology and capitalization (two very strong cues in English) and treat each word as anatomic entity.", "labels": [], "entities": []}, {"text": "This means that the model parameters \u03b8 consist of the HMM stateto-state transition probabilities and the state-to-word emission probabilities.", "labels": [], "entities": []}, {"text": "In virtually all statistical approaches the parameters \u03b8 are chosen or estimated on the basis of training data d.", "labels": [], "entities": []}, {"text": "This paper studies unsupervised estimation, sod = w = (w 1 , . .", "labels": [], "entities": []}, {"text": ", w n ) consists of a sequence of words w i containing all of the words of training corpus appended into a single string, as explained below.", "labels": [], "entities": []}, {"text": "Maximum Likelihood (ML) is the most common estimation method in computational linguistics.", "labels": [], "entities": []}, {"text": "A Maximum Likelihood estimator sets the parameters to the valu\u00ea \u03b8 that makes the likelihood L d of the data d as large as possible: In this paper we use the Inside-Outside algorithm, which is a specialized form of Expectation-Maximization, to find HMM parameters which (at least locally) maximize the likelihood function L d . Recently there is increasing interest in Bayesian methods in computational linguistics, and the primary goal of this paper is to compare the performance of various Bayesian estimators with each other and with EM.", "labels": [], "entities": []}, {"text": "A Bayesian approach uses Bayes theorem to factorize the posterior distribution P(\u03b8 | d) into the likelihood P(d | \u03b8) and the prior P(\u03b8).", "labels": [], "entities": []}, {"text": "Priors can be useful because they can express preferences for certain types of models.", "labels": [], "entities": []}, {"text": "To take an example from our POS-tagging application, most words belong to relatively few parts-of-speech (e.g., most words belong to a single POS, and while there are some words which are both nouns and verbs, very few are prepositions and adjectives as well).", "labels": [], "entities": []}, {"text": "One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words.", "labels": [], "entities": []}, {"text": "An appropriate Dirichlet prior can express this preference.", "labels": [], "entities": []}, {"text": "While it is possible to use Bayesian inference to find a single model, such as the Maximum A Posteriori or MAP value of \u03b8 which maximizes the posterior P(\u03b8 | d), this is not necessarily the best approach.", "labels": [], "entities": [{"text": "Maximum A Posteriori or MAP value", "start_pos": 83, "end_pos": 116, "type": "METRIC", "confidence": 0.65848242243131}]}, {"text": "Instead, rather than commiting to a single value for the parameters \u03b8 many Bayesians often prefer to work with the full posterior distribution P(\u03b8 | d), as this naturally reflects the uncertainty in \u03b8's value.", "labels": [], "entities": []}, {"text": "In all but the simplest models there is no known closed form for the posterior distribution.", "labels": [], "entities": []}, {"text": "However, the Bayesian literature describes a number of methods for approximating the posterior P(\u03b8 | d).", "labels": [], "entities": []}, {"text": "Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers.", "labels": [], "entities": [{"text": "HMM POS taggers", "start_pos": 159, "end_pos": 174, "type": "TASK", "confidence": 0.6311657230059305}]}, {"text": "These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t | w) of POS tags (i.e., HMM hidden states) t given words w.", "labels": [], "entities": []}, {"text": "This recent literature reports contradictory results about these Bayesian inference methods.", "labels": [], "entities": []}, {"text": "Johnson (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM.", "labels": [], "entities": []}, {"text": "On the other hand,  reported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.8094571828842163}]}, {"text": "One of the primary motivations for this paper was to understand and resolve the difference in these results.", "labels": [], "entities": []}, {"text": "We replicate the results of both papers and show that the difference in their results stems from differences in the sizes of the training data and numbers of states in their models.", "labels": [], "entities": []}, {"text": "It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs.", "labels": [], "entities": [{"text": "HMMs", "start_pos": 101, "end_pos": 105, "type": "TASK", "confidence": 0.8948732614517212}]}, {"text": "This paper compares the performance of four different kinds of Gibbs samplers, Variational Bayes and Expectation Maximization on unsupervised POS tagging problems of various sizes.", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.8740942478179932}, {"text": "POS tagging", "start_pos": 142, "end_pos": 153, "type": "TASK", "confidence": 0.7621040046215057}]}, {"text": "Our goal here is to try to learn how the performance of these different estimators varies as we change the number of hidden states in the HMMs and the size of the training data.", "labels": [], "entities": []}, {"text": "In theory, the Gibbs samplers produce streams of samples that eventually converge on the true posterior distribution, while the Variational Bayes (VB) estimator only produces an approximation to the posterior.", "labels": [], "entities": [{"text": "Variational Bayes (VB) estimator", "start_pos": 128, "end_pos": 160, "type": "METRIC", "confidence": 0.910027821858724}]}, {"text": "However, as the size of the training data distribution increases the likelihood function and therefore the posterior distribution becomes increasingly peaked, so one would expect this variational approximation to become increasingly accurate.", "labels": [], "entities": []}, {"text": "Further the Gibbs samplers used in this paper should exhibit reduced mobility as the size of training data increases, so as the size of the training data increases eventually the Variational Bayes estimator should prove to be superior.", "labels": [], "entities": []}, {"text": "However the two point-wise Gibbs samplers investigated here, which resample the label of each word conditioned on the labels of its neighbours (amongst other things) only require O(m) steps per sample (where m is the number of HMM states), while EM, VB and the sentence-blocked Gibbs samplers require O(m 2 ) steps per sample.", "labels": [], "entities": [{"text": "O", "start_pos": 179, "end_pos": 180, "type": "METRIC", "confidence": 0.9880362749099731}]}, {"text": "Thus for HMMs with many states it is possible to perform one or two orders of magnitude more iterations of the point-wise Gibbs samplers in the same run-time as the other samplers, so it is plausible that they would yield better results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The previous section described six different unsupervised estimators for HMMs.", "labels": [], "entities": []}, {"text": "In this section we compare their performance for English part-ofspeech tagging.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7512563169002533}]}, {"text": "One of the difficulties in evaluating unsupervised taggers such as these is mapping the system's states to the gold-standard partsof-speech.", "labels": [], "entities": []}, {"text": "proposed an information-theoretic measure known as the Variation of Information (VI) described by Meil\u02c7a as an evaluation of an unsupervised tagging.", "labels": [], "entities": [{"text": "Variation of Information (VI)", "start_pos": 55, "end_pos": 84, "type": "METRIC", "confidence": 0.6983890483776728}]}, {"text": "However as Goldwater (p.c.) points out, this may not bean ideal evaluation measure; e.g., a tagger which assigns all words the same single part-of-speech tag does disturbingly well under Variation of Information, suggesting that a poor tagger may score well under VI.", "labels": [], "entities": []}, {"text": "In order to avoid this problem we focus hereon evaluation measures that construct an explicit mapping between the gold-standard part-of-speech tags and the HMM's states.", "labels": [], "entities": []}, {"text": "Perhaps the most straightforward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state sequence t to a sequence of part-of-speech tags.", "labels": [], "entities": []}, {"text": "But as observes, this approach has several defects.", "labels": [], "entities": []}, {"text": "If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state.", "labels": [], "entities": []}, {"text": "We can partially address this by cross-validation.", "labels": [], "entities": []}, {"text": "We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech.", "labels": [], "entities": []}, {"text": "We call the accuracy of the resulting tagging the crossvalidation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993053674697876}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.937455952167511}]}, {"text": "Finally, following and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag.", "labels": [], "entities": []}, {"text": "Following these authors, we used a greedy algorithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1  The column heading indicates the size of the corpus and the number of HMM states.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9979460835456848}]}, {"text": "In the Gibbs sampler (GS) results the subscript \"e\" indicates that the parameters \u03b8 and \u03c6 were explicitly sampled while the subscript \"c\" indicates that they were integrated out, and the subscript \"p\" indicates pointwise sampling, while \"b\" indicates sentence-blocked sampling.", "labels": [], "entities": []}, {"text": "Entries tagged with a star indicate that the estimator had not converged after weeks of run-time, but was still slowly improving.", "labels": [], "entities": []}], "tableCaptions": []}