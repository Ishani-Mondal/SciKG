{"title": [{"text": "N -gram Weighting: Reducing Training Data Mismatch in Cross-Domain Language Model Estimation", "labels": [], "entities": [{"text": "Cross-Domain Language Model Estimation", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.6166859343647957}]}], "abstractContent": [{"text": "In domains with insufficient matched training data, language models are often constructed by interpolating component models trained from partially matched corpora.", "labels": [], "entities": []}, {"text": "Since the n-grams from such corpora may not be of equal relevance to the target domain, we propose an n-gram weighting technique to adjust the component n-gram probabilities based on features derived from readily available segmen-tation and metadata information for each corpus.", "labels": [], "entities": []}, {"text": "Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task.", "labels": [], "entities": [{"text": "absolute word error rate reduction", "start_pos": 91, "end_pos": 125, "type": "METRIC", "confidence": 0.8399177432060242}, {"text": "lecture transcription task", "start_pos": 184, "end_pos": 210, "type": "TASK", "confidence": 0.7782773772875468}]}], "introductionContent": [{"text": "Many application domains in machine learning suffer from a dearth of matched training data.", "labels": [], "entities": []}, {"text": "However, partially matched data sets are often available in abundance.", "labels": [], "entities": []}, {"text": "Past attempts to utilize the mismatched data for training often result in models that exhibit biases not observed in the target domain.", "labels": [], "entities": []}, {"text": "In this work, we will investigate the use of the often readily available data segmentation and metadata attributes associated with each corpus to reduce the effect of such bias.", "labels": [], "entities": []}, {"text": "We will examine this approach in the context of language modeling for lecture transcription.", "labels": [], "entities": [{"text": "lecture transcription", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7031212747097015}]}, {"text": "Compared with other types of audio data, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with special terminologies ().", "labels": [], "entities": []}, {"text": "While we may have existing transcripts from general lectures or written text on the precise topic, training data that matches both the style and topic of the target lecture is often scarce.", "labels": [], "entities": []}, {"text": "Thus, past research has investigated various adaptation and interpolation techniques to make use of partially matched corpora).", "labels": [], "entities": []}, {"text": "Training corpora are often segmented into documents with associated metadata, such as title, date, and speaker.", "labels": [], "entities": []}, {"text": "For lectures, if the data contains even a few lectures on linear algebra, conventional language modeling methods that lump the documents together will tend to assign disproportionately high probability to frequent terms like vector and matrix.", "labels": [], "entities": []}, {"text": "Can we utilize the segmentation and metadata information to reduce the biases resulting from training data mismatch?", "labels": [], "entities": []}, {"text": "In this work, we present such a technique where we weight each n-gram count in a standard n-gram language model (LM) estimation procedure by a relevance factor computed via a log-linear combination of n-gram features.", "labels": [], "entities": []}, {"text": "Utilizing features that correlate with the specificity of n-grams to subsets of the training documents, we effectively de-emphasize out-of-domain n-grams.", "labels": [], "entities": []}, {"text": "By interpolating models, such as general lectures and course textbook, that match the target domain in complementary ways, and optimizing the weighting and interpolation parameters jointly, we allow each n-gram probability to be modeled by the most relevant interpolation component.", "labels": [], "entities": []}, {"text": "Using a combination of features derived from multiple partitions of the training documents, the resulting weighted n-gram model achieves up to a 1.2% absolute word error rate (WER) reduction over a linearly interpolated baseline on a lecture transcription task.", "labels": [], "entities": [{"text": "absolute word error rate (WER) reduction", "start_pos": 150, "end_pos": 190, "type": "METRIC", "confidence": 0.8955211266875267}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: A list of n-gram weighting features. f : n-gram  frequency, h: normalized entropy, t: topic probability.", "labels": [], "entities": []}, {"text": " Table 2: Summary of evaluation corpora.", "labels": [], "entities": []}, {"text": " Table 3: Performance of n-gram weighting with a variety  of Kneser-Ney settings. FixKN(d): Kneser-Ney with d  fixed discount parameters. KN(d): FixKN(d) with tuned  values. W(feat): n-gram weighting with feat feature.", "labels": [], "entities": []}, {"text": " Table 4: N -gram weighting with linear interpolation.", "labels": [], "entities": []}, {"text": " Table 5: N -gram weighting with various features.", "labels": [], "entities": []}, {"text": " Table 6: N -gram weighting with feature combinations.", "labels": [], "entities": []}, {"text": " Table 7: Effect of interpolation technique. L: Lectures, T:", "labels": [], "entities": []}, {"text": " Table 9: Test set WER with various training corpus com- binations. L: Lectures, T: Textbook, S: Switchboard, W:  n-gram weighting.", "labels": [], "entities": [{"text": "WER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9790646433830261}]}]}