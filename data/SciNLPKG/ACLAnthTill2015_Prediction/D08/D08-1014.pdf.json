{"title": [{"text": "Multilingual Subjectivity Analysis Using Machine Translation", "labels": [], "entities": [{"text": "Multilingual Subjectivity Analysis", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7477130095163981}, {"text": "Machine Translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7270871102809906}]}], "abstractContent": [{"text": "Although research in other languages is increasing , much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language.", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7124413996934891}]}, {"text": "In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages.", "labels": [], "entities": []}, {"text": "Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7309373021125793}, {"text": "subjectivity analysis", "start_pos": 138, "end_pos": 159, "type": "TASK", "confidence": 0.6932497620582581}]}, {"text": "Through comparative evaluations on two different languages (Romanian and Span-ish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in anew target language.", "labels": [], "entities": [{"text": "automatic translation", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.675285592675209}]}], "introductionContent": [{"text": "We have seen a surge in interest towards the application of automatic tools and techniques for the extraction of opinions, emotions, and sentiments in text (subjectivity).", "labels": [], "entities": [{"text": "extraction of opinions, emotions, and sentiments in text", "start_pos": 99, "end_pos": 155, "type": "TASK", "confidence": 0.735044252872467}]}, {"text": "A large number of text processing applications have already employed techniques for automatic subjectivity analysis, including automatic expressive text-to-speech synthesis), text semantic analysis;), tracking sentiment timelines in on-line forums and news), mining opinions from product reviews (, and question answering (.", "labels": [], "entities": [{"text": "automatic subjectivity analysis", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.6961946686108907}, {"text": "text semantic analysis", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.8184243241945902}, {"text": "tracking sentiment timelines in on-line forums and news", "start_pos": 201, "end_pos": 256, "type": "TASK", "confidence": 0.7313135676085949}, {"text": "question answering", "start_pos": 303, "end_pos": 321, "type": "TASK", "confidence": 0.8715309500694275}]}, {"text": "A significant fraction of the research work to date in subjectivity analysis has been applied to English, which led to several resources and tools available for this language.", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7362463474273682}]}, {"text": "In this paper, we explore multiple paths that employ machine translation while leveraging on the resources and tools available for English, to automatically generate resources for subjectivity analysis fora new target language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7649710476398468}]}, {"text": "Through experiments carried outwith automatic translation and cross-lingual projections of subjectivity annotations, we try to answer the following questions.", "labels": [], "entities": []}, {"text": "First, assuming an English corpus manually annotated for subjectivity, can we use machine translation to generate a subjectivity-annotated corpus in the target language?", "labels": [], "entities": []}, {"text": "Second, assuming the availability of a tool for automatic subjectivity analysis in English, can we generate a corpus annotated for subjectivity in the target language by using automatic subjectivity annotations of English text and machine translation?", "labels": [], "entities": []}, {"text": "Finally, third, can these automatically generated resources be used to effectively train tools for subjectivity analysis in the target language?", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7450697422027588}]}, {"text": "Since our methods are particularly useful for languages with only a few electronic tools and resources, we chose to conduct our initial experiments on Romanian, a language with limited text processing resources developed to date.", "labels": [], "entities": []}, {"text": "Furthermore, to validate our results, we carried a second set of experiments on Spanish.", "labels": [], "entities": []}, {"text": "Note however that our methods do not make use of any target language specific knowledge, and thus they are applicable to any other language as long as a machine translation engine exists between the selected language and English.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we use a corpus in the source language manually annotated for subjectivity.", "labels": [], "entities": []}, {"text": "The corpus is automatically translated into the target language, followed by a projection of the subjectivity labels from the source to the target language.", "labels": [], "entities": []}, {"text": "The experiment is illustrated in.", "labels": [], "entities": []}, {"text": "We use the MPQA corpus ( ), which is a collection of 535 English-language news articles from a variety of news sources manually annotated for subjectivity.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9709052741527557}]}, {"text": "Although the corpus was originally annotated at clause and phrase level, we use the sentence-level annotations associated with the dataset (Wiebe and).", "labels": [], "entities": []}, {"text": "From the total of 9,700 sentences in this corpus, 55% of the sentences are labeled as subjective while the rest are objective.", "labels": [], "entities": []}, {"text": "After the automatic translation of the cor- pus and the projection of the annotations, we obtain a large corpus of 9,700 subjectivity-annotated sentences in the target language, which can be used to train a subjectivity classifier.", "labels": [], "entities": []}, {"text": "In the second experiment, we assume that the only resources available area tool for subjectivity annotation in the source language and a collection of raw texts, also in the source language.", "labels": [], "entities": []}, {"text": "The source language text is automatically annotated for subjectivity and then translated into the target language.", "labels": [], "entities": []}, {"text": "In this way, we produce a subjectivity annotated corpus that we can use to train a subjectivity annotation tool for the target language.", "labels": [], "entities": []}, {"text": "In order to generate automatic subjectivity annotations, we use the OpinionFinder tool developed by).", "labels": [], "entities": []}, {"text": "The first one is a rule-based highprecision classifier that labels sentences based on the presence of subjective clues obtained from a large lexicon.", "labels": [], "entities": []}, {"text": "The second one is a high-coverage classifier that starts with an initial corpus annotated using the high-precision classifier, followed by several bootstrapping steps that increase the size of the lexicon and the coverage of the classifier.", "labels": [], "entities": []}, {"text": "For most of our experiments we use the high-coverage classifier.", "labels": [], "entities": []}, {"text": "shows the performance of the two OpinionFinder classifiers as measured on the MPQA corpus (Wiebe and As a raw corpus, we use a subset of the SemCor corpus (, consisting of 107 documents with roughly 11,000 sentences.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.9763466119766235}, {"text": "SemCor corpus", "start_pos": 141, "end_pos": 154, "type": "DATASET", "confidence": 0.81270632147789}]}, {"text": "This is a balanced corpus covering a number of topics in sports, politics, fashion, education, and others.", "labels": [], "entities": []}, {"text": "The reason for working with this collection is the fact that we also have a manual translation of the SemCor documents from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).", "labels": [], "entities": []}, {"text": "Note that in this experiment the annotation of subjectivity is carried out on the original source language text, and thus expected to be more accurate than if it were applied on automatically translated text.", "labels": [], "entities": [{"text": "accurate", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9773657321929932}]}, {"text": "However, the training data in the target language is produced by automatic translation, and thus likely to contain errors.", "labels": [], "entities": []}, {"text": "The third experiment is similar to the second one, except that we reverse the direction of the translation.", "labels": [], "entities": []}, {"text": "We translate raw text that is available in the target language into the source language, and then use a subjectivity annotation tool to label the automatically translated source language text.", "labels": [], "entities": []}, {"text": "After the annotation, the labels are projected back into the target language, and the resulting annotated corpus is used to train a subjectivity classifier.", "labels": [], "entities": []}, {"text": "As before, we use the high-coverage classifier available in OpinionFinder, and the SemCor corpus.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.8650778830051422}]}, {"text": "We use a manual translation of this corpus available in the target language.", "labels": [], "entities": []}, {"text": "In this experiment, the subjectivity annotations are carried out on automatically generated source text, and thus expected to be less accurate.", "labels": [], "entities": []}, {"text": "However, since the training data was originally written in the target language, it is free of translation errors, and thus training carried out on this data should be more robust.", "labels": [], "entities": []}, {"text": "Our initial evaluations are carried out on Romanian.", "labels": [], "entities": [{"text": "Romanian", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9857608675956726}]}, {"text": "The performance of each of the three methods is evaluated using a dataset manually annotated for subjectivity.", "labels": [], "entities": []}, {"text": "To evaluate our methods, we generate a Romanian training corpus annotated for subjectivity on which we train a subjectivity classifier, which is then used to label the test data.", "labels": [], "entities": [{"text": "Romanian training corpus annotated", "start_pos": 39, "end_pos": 73, "type": "DATASET", "confidence": 0.7342226430773735}]}, {"text": "We evaluate the results against a gold-standard corpus consisting of 504 Romanian sentences manually annotated for subjectivity.", "labels": [], "entities": []}, {"text": "These sentences represent the manual translation into Romanian of a small subset of the SemCor corpus, which was removed from the training corpora used in experiments two and three.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.7556909918785095}]}, {"text": "This is the same evaluation dataset as used in (.", "labels": [], "entities": []}, {"text": "Two Romanian native speakers annotated the sentences individually, and the differences were adjudicated through discussions.", "labels": [], "entities": []}, {"text": "The agreement of the two annotators is 0.83% (\u03ba = 0.67); when the uncertain annotations are removed, the agreement rises to 0.89 (\u03ba = 0.77).", "labels": [], "entities": [{"text": "agreement", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9882376790046692}]}, {"text": "The two annotators reached consensus on all sentences for which they disagreed, resulting in a gold standard dataset with 272 (54%) subjective sentences and 232 (46%) objective sentences.", "labels": [], "entities": []}, {"text": "More details about this dataset are available in (.", "labels": [], "entities": []}, {"text": "In order to learn from our annotated data, we experiment with two different classifiers, Na\u00a8\u0131veNa\u00a8\u0131ve Bayes and support vector machines (SVM), selected for their performance and diversity of learning methodology.", "labels": [], "entities": []}, {"text": "For Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, we use the multinomial model) with a threshold of 0.3.", "labels": [], "entities": []}, {"text": "For SVM, we use the LibSVM implementation) with a linear kernel.", "labels": [], "entities": []}, {"text": "The automatic translation of the MPQA and of the SemCor corpus was performed using Language Weaver, 1 a commercial statistical machine translation software.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.7591968774795532}, {"text": "SemCor corpus", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8670019209384918}, {"text": "statistical machine translation", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.6611691017945608}]}, {"text": "The resulting text was post-processed by removing diacritics, stopwords and numbers.", "labels": [], "entities": []}, {"text": "For training, we experimented with a series of weighting schemes, yet we only report the results obtained for binary weighting, as it had the most consistent behavior.", "labels": [], "entities": []}, {"text": "The results obtained by running the three experiments on Romanian are shown in.", "labels": [], "entities": [{"text": "Romanian", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9686963558197021}]}, {"text": "The baseline on this data set is 54.16%, represented by the percentage of sentences in the corpus that are subjective, and the upper bound (UB) is 71.83%, which is the accuracy obtained under the scenario where the test data is translated into the source language and then annotated using the high-coverage OpinionFinder tool.", "labels": [], "entities": [{"text": "UB)", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9924505949020386}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9993304014205933}]}, {"text": "Perhaps not surprisingly, the SVM classifier outperforms Na\u00a8\u0131veNa\u00a8\u0131ve Bayes by 2% to 6%, implying that SVM maybe better fitted to lessen the amount of noise embedded in the dataset and provide more accurate classifications.", "labels": [], "entities": []}, {"text": "The first experiment, involving the automatic translation of the MPQA corpus enhanced with manual annotations for subjectivity at sentence level, does not seem to perform well when compared to the experiments in which automatic subjectivity classi-: Precision (P), Recall (R) and F-measure (F) for Romanian experiments fication is used.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.7308080196380615}, {"text": "Precision (P)", "start_pos": 250, "end_pos": 263, "type": "METRIC", "confidence": 0.9196493178606033}, {"text": "Recall (R)", "start_pos": 265, "end_pos": 275, "type": "METRIC", "confidence": 0.9401589632034302}, {"text": "F-measure (F)", "start_pos": 280, "end_pos": 293, "type": "METRIC", "confidence": 0.9545275270938873}]}, {"text": "This could imply that a classifier cannot be so easily trained on the cues that humans use to express subjectivity, especially when they are not overtly expressed in the sentence and thus can be lost in the translation.", "labels": [], "entities": []}, {"text": "Instead, the automatic annotations produced with a rule-based tool (OpinionFinder), relying on overt mentions of words in a subjectivity lexicon, seems to be more robust to translation, further resulting in better classification results.", "labels": [], "entities": [{"text": "translation", "start_pos": 173, "end_pos": 184, "type": "TASK", "confidence": 0.9667838215827942}]}, {"text": "To exemplify, consider the following subjective sentence from the MPQA corpus, which does not include overt clues of subjectivity, but was annotated as subjective by the human judges because of the structure of the sentence: It is the Palestinians that are calling for the implementation of the agreements, understandings, and recommendations pertaining to the Palestinian-Israeli conflict.", "labels": [], "entities": [{"text": "MPQA corpus", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.9738472700119019}]}, {"text": "We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SemCor subjectivityannotated corpus.", "labels": [], "entities": [{"text": "SemCor subjectivityannotated corpus", "start_pos": 123, "end_pos": 158, "type": "DATASET", "confidence": 0.6447164018948873}]}, {"text": "In (, we used the manual translation of the SemCor corpus into Romanian to form an English-Romanian parallel data set.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.7666578590869904}]}, {"text": "The English side was annotated using the Opinion Finder tool, and the subjectivity labels were projected on the Romanian text.", "labels": [], "entities": [{"text": "Opinion Finder tool", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.789327065149943}]}, {"text": "A Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier was then trained on the subjectivity annotated Romanian corpus and tested on the same gold standard as used in our experiments.", "labels": [], "entities": [{"text": "Romanian corpus", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.7606064975261688}]}, {"text": "shows the results obtained in those experiments by using the high-coverage OpinionFinder classifier.", "labels": [], "entities": []}, {"text": "Among our experiments, experiments two and three are closest to those proposed in (: Precision (P), Recall (R) and F-measure (F) for subjectivity analysis in Romanian obtained by using an English-Romanian parallel corpus English into Romanian (experiment two) or Romanian into English (experiment three), and annotating this dataset with the high-coverage OpinionFinder classifier, we obtain an F-measure of 63.69%, and 65.87% respectively, using Na\u00a8\u0131veNa\u00a8\u0131ve Bayes (the same machine learning classifier as used in).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 85, "end_pos": 98, "type": "METRIC", "confidence": 0.9205027967691422}, {"text": "Recall (R)", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9370936155319214}, {"text": "F-measure (F)", "start_pos": 115, "end_pos": 128, "type": "METRIC", "confidence": 0.9489332437515259}, {"text": "F-measure", "start_pos": 395, "end_pos": 404, "type": "METRIC", "confidence": 0.9890170097351074}]}, {"text": "This implies that at most 4% in Fmeasure can be gained by using a parallel corpus as compared to an automatically translated corpus, further suggesting that machine translation is a viable alternative to devising subjectivity classification in a target language leveraged on the tools existent in a source language.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.7867680191993713}, {"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.763511061668396}]}, {"text": "As English is a language with fewer inflections when compared to Romanian, which accommodates for gender and case as a suffix to the base form of a word, the automatic translation into English is closer to a human translation (experiment three).", "labels": [], "entities": []}, {"text": "Therefore labeling this data using the OpinionFinder tool and projecting the labels onto a fully inflected humangenerated Romanian text provides more accurate classification results, as compared to a setup where the training is carried out on machine-translated Romanian text (experiment two).", "labels": [], "entities": []}, {"text": "We re-ran experiments two and three with 20% corpus size increments at a time ().", "labels": [], "entities": []}, {"text": "It is interesting to note that a corpus of approximately 6000 sentences is able to achieve a high enough Fmeasure (around 66% for both experiments) to be considered viable for training a subjectivity classifier.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.997517466545105}]}, {"text": "Also, at a corpus size over 10,000 sentences, the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier performs worse than SVM, which displays a directly proportional trend between the number of sentences in the data set and the observed F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 215, "end_pos": 224, "type": "METRIC", "confidence": 0.9864808320999146}]}, {"text": "This trend could be explained by the fact that the SVM classifier is more robust with regard to noisy data, when compared to Na\u00a8\u0131veNa\u00a8\u0131ve Bayes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision (P), Recall (R) and F-measure (F) for  the two OpinionFinder classifiers, as measured on the  MPQA corpus", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9429612308740616}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9570674151182175}, {"text": "F-measure (F)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9667560905218124}, {"text": "MPQA", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.9733683466911316}]}, {"text": " Table 2: Precision (P), Recall (R) and F-measure (F) for  Romanian experiments", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9359038770198822}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9666928946971893}, {"text": "F-measure (F)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.969882145524025}]}, {"text": " Table 3: Precision (P), Recall (R) and F-measure (F) for  subjectivity analysis in Romanian obtained by using an  English-Romanian parallel corpus", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9392673075199127}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9609811753034592}, {"text": "F-measure (F)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.971334844827652}, {"text": "subjectivity analysis", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.7525979280471802}]}, {"text": " Table 4: Precision (P), Recall (R) and F-measure (F) for  Spanish experiments", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9380159378051758}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9650775492191315}, {"text": "F-measure (F)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9667999595403671}]}, {"text": " Table 5: Precision (P), Recall (R) and F-measure (F) for  identifying language specific information", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9413274973630905}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9601653069257736}, {"text": "F-measure (F)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9705639779567719}]}]}