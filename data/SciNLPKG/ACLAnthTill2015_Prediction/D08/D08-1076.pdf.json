{"title": [{"text": "Lattice-based Minimum Error Rate Training for Statistical Machine Translation", "labels": [], "entities": [{"text": "Minimum Error Rate", "start_pos": 14, "end_pos": 32, "type": "METRIC", "confidence": 0.6932469407717387}, {"text": "Statistical Machine Translation", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.8621588746706644}]}], "abstractContent": [{"text": "Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.", "labels": [], "entities": [{"text": "Minimum Error Rate Training (MERT", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.7342173606157303}]}, {"text": "To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.", "labels": [], "entities": []}, {"text": "The feature function weights are then adjusted by traversing the error surface combined overall sentences and picking those values for which the resulting error count reaches a minimum.", "labels": [], "entities": []}, {"text": "Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder.", "labels": [], "entities": [{"text": "MERT", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.8581298589706421}]}, {"text": "In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice.", "labels": [], "entities": []}, {"text": "Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.", "labels": [], "entities": [{"text": "MERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.5642203688621521}]}, {"text": "The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 71, "end_pos": 115, "type": "TASK", "confidence": 0.5706159174442291}]}, {"text": "Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.", "labels": [], "entities": [{"text": "NIST 2008 translation tasks", "start_pos": 29, "end_pos": 56, "type": "DATASET", "confidence": 0.7732033133506775}, {"text": "BLEU score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9779277443885803}, {"text": "MERT", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.7555034160614014}]}], "introductionContent": [{"text": "Many statistical methods in natural language processing aim at minimizing the probability of sentence errors.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.6655718485514323}]}, {"text": "In practice, however, system quality is often measured based on error metrics that assign non-uniform costs to classification errors and thus go far beyond counting the number of wrong decisions.", "labels": [], "entities": []}, {"text": "Examples are the mean average precision for ranked retrieval, the F-measure for parsing, and the BLEU score for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "mean average precision", "start_pos": 17, "end_pos": 39, "type": "METRIC", "confidence": 0.851087768872579}, {"text": "F-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9863276481628418}, {"text": "parsing", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9707421660423279}, {"text": "BLEU score", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9813053011894226}, {"text": "statistical machine translation (SMT)", "start_pos": 112, "end_pos": 149, "type": "TASK", "confidence": 0.7890755931536356}]}, {"text": "A class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as Minimum Error Rate Training (MERT) and has been suggested for SMT in.", "labels": [], "entities": [{"text": "Minimum Error Rate Training (MERT)", "start_pos": 129, "end_pos": 163, "type": "METRIC", "confidence": 0.8358594094003949}, {"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.9979871511459351}]}, {"text": "MERT aims at estimating the model parameters such that the decision under the zero-one loss function maximizes some end-to-end performance measure on a development corpus.", "labels": [], "entities": []}, {"text": "In combination with log-linear models, the training procedure allows fora direct optimization of the unsmoothed error count.", "labels": [], "entities": []}, {"text": "The criterion can be derived from Bayes' decision rule as follows: Let f \ud97b\udf59 f 1 , ..., f J denote a source sentence ('French') which is to be translated into a target sentence ('English') e \ud97b\udf59 e 1 , ..., e I . Under the zero-one loss function, the translation which maximizes the a posteriori probability is chosen: The feature function weights are the parameters of the model, and the objective of the MERT criterion is to find a parameter set \u03bb M 1 that minimizes the error count on a representative set of training sentences.", "labels": [], "entities": []}, {"text": "More precisely, let f S 1 denote the source sentences of a training corpus with given reference translations r S 1 , and let C s \ud97b\udf59 \u00d8e s,1 , ..., e s,K \u00d9 denote a set of K candidate translations.", "labels": [], "entities": []}, {"text": "Assuming that the corpusbased error count for some translations e S 1 is additively decomposable into the error counts of the individual sentences, i.e.,  In, it was shown that linear models can effectively be trained under the MERT criterion using a special line optimization algorithm.", "labels": [], "entities": []}, {"text": "This line optimization determines for each feature function h m and sentence f s the exact error surface on a set of candidate translations C s . The feature function weights are then adjusted by traversing the error surface combined overall sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum.", "labels": [], "entities": []}, {"text": "Candidate translations in MERT are typically represented as N -best lists which contain the N most probable translation hypotheses.", "labels": [], "entities": [{"text": "MERT", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.5109183192253113}]}, {"text": "A downside of this approach is, however, that N -best lists can only capture a very small fraction of the search space.", "labels": [], "entities": []}, {"text": "As a consequence, the line optimization algorithm needs to repeatedly translate the development corpus and enlarge the candidate repositories with newly found hypotheses in order to avoid overfitting on C sand preventing the optimization procedure from stopping in a poor local optimum.", "labels": [], "entities": [{"text": "line optimization", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7247177809476852}]}, {"text": "In this paper, we present a novel algorithm that allows for efficiently constructing and representing the unsmoothed error surface for all translations that are encoded in a phrase lattice.", "labels": [], "entities": []}, {"text": "The number of candidate translations thus taken into account increases by several orders of magnitudes compared to N -best MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.6415309309959412}]}, {"text": "Lattice MERT is shown to yield significantly faster convergence rates while it explores a much larger space of candidate translations which is exponential in the lattice size.", "labels": [], "entities": []}, {"text": "Despite this vast search space, we show that the suggested algorithm is always efficient in both running time and memory.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly reviews N -best MERT and introduces some basic concepts that are used in order to develop the line optimization algorithm for phrase lattices in Section 3.", "labels": [], "entities": [{"text": "line optimization", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7079804539680481}]}, {"text": "Section 4 presents an upper bound on the complexity of the unsmoothed error surface for the translation hypotheses represented in a phrase lattice.", "labels": [], "entities": []}, {"text": "This upper bound is used to prove the space and runtime efficiency of the suggested algorithm.", "labels": [], "entities": []}, {"text": "Section 5 lists some best practices for MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.8737285137176514}]}, {"text": "Section 6 discusses related work.", "labels": [], "entities": []}, {"text": "Section 7 reports on experiments conducted on the NIST 2008 translation tasks.", "labels": [], "entities": [{"text": "NIST 2008 translation tasks", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6880785524845123}]}, {"text": "The paper concludes with a summary in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were conducted on the NIST 2008 translation tasks under the conditions of the constrained data track for the language pairs Arabicto-English (aren), English-to-Chinese (enzh), and Chinese-to-English (zhen).", "labels": [], "entities": [{"text": "NIST 2008 translation tasks", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.8736549913883209}]}, {"text": "The development corpora were compiled from test data used in the 2002 and 2004 NIST evaluations.", "labels": [], "entities": [{"text": "NIST", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.868928849697113}]}, {"text": "Each corpus set provides 4 reference translations per source sentence.", "labels": [], "entities": []}, {"text": "Translation results were evaluated using the mixedcase BLEU score metric in the implementation as suggested by).", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9252204298973083}, {"text": "BLEU score metric", "start_pos": 55, "end_pos": 72, "type": "METRIC", "confidence": 0.9706313411394755}]}, {"text": "Translation results were produced with a state-ofthe-art phrase-based SMT system which uses EMtrained word alignment models (IBM1, HMM) and a 5-gram language model built from the Web-1T collection 2 . Translation hypotheses produced on the blind test data were reranked using the MinimumBayes Risk (MBR) decision rule (.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9476081132888794}, {"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.80448979139328}]}, {"text": "Each system uses a loglinear combination of 20 to 30 feature functions.", "labels": [], "entities": []}, {"text": "Ina first experiment, we investigated the convergence speed of lattice MERT and N -best MERT.: BLEU scores on the zhen-dev1 corpus for lattice MERT with additional directions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9995923638343811}]}, {"text": "shows the evolution of the BLEU score in the course of the iteration index on the zhendev1 corpus for either method.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9723387360572815}, {"text": "zhendev1 corpus", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.6987834572792053}]}, {"text": "In each iteration, the training procedure translates the development corpus using the most recent weights set and merges the top ranked candidate translations (either represented as phrase lattices or N -best lists) into the candidate repositories before the line optimization is performed.", "labels": [], "entities": []}, {"text": "For N -best MERT, we used N \ud97b\udf59 50 which yielded the best results.", "labels": [], "entities": [{"text": "N -best", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.8782813350359598}, {"text": "MERT", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.6349716782569885}]}, {"text": "In contrast to lattice MERT, N -best MERT optimizes all dimensions in each iteration and, in addition, it also explores a large number of random starting points before it re-decodes and expands the hypothesis set.", "labels": [], "entities": []}, {"text": "As is typical for N -best MERT, the first iteration causes a dramatic performance loss caused by overadapting the candidate repositories, which amounts to more than 27.3 BLEU points.", "labels": [], "entities": [{"text": "N -best MERT", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.35594991594552994}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9988596439361572}]}, {"text": "Although this performance loss is recouped after the 5th iteration, the initial decline makes the line optimization under N -best MERT more fragile since the optimum found at the end of the training procedure is affected by the initial performance drop rather than by the choice of the initial start weights.", "labels": [], "entities": [{"text": "N -best MERT", "start_pos": 122, "end_pos": 134, "type": "METRIC", "confidence": 0.6961616426706314}]}, {"text": "Lattice MERT on the other hand results in a significantly faster convergence speed and reaches its optimum already in the 5th iteration.", "labels": [], "entities": [{"text": "MERT", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9677707552909851}, {"text": "convergence speed", "start_pos": 65, "end_pos": 82, "type": "METRIC", "confidence": 0.9174885749816895}]}, {"text": "For lattice MERT, we used a graph density of 40 arcs per phrase which corresponds to an N -best size of more than two octillion \u00d42 \u00a4 10 27 \u00d5 entries.", "labels": [], "entities": [{"text": "MERT", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.8285928964614868}]}, {"text": "This huge number of alternative candidate translations makes updating the weights under lattice MERT more reliable and robust and, compared to N -best MERT, it becomes less likely that the same feature weight needs to be picked again and adjusted in subsequent iterations.", "labels": [], "entities": []}, {"text": "shows the evolution of the BLEU score on the zhen-dev1 corpus using lattice MERT with 5 weights updates per iteration.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9694982469081879}, {"text": "MERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9626140594482422}]}, {"text": "The performance drop in iteration 1 is also attributed to overfitting the candidate repository.", "labels": [], "entities": []}, {"text": "The decline of less than 0.5% in terms of BLEU is, however, almost negligible compared to the performance drop of more than 27% in case of N -best MERT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9996278285980225}, {"text": "N -best MERT", "start_pos": 139, "end_pos": 151, "type": "METRIC", "confidence": 0.6412335336208344}]}, {"text": "The vast number of alternative translation hypotheses represented in a lattice also increases the number of phase transitions in the error surface, and thus prevents MERT from selecting a low performing feature weights set at early stages in the optimization procedure.", "labels": [], "entities": []}, {"text": "This is illustrated in, where lattice MERT and N -best MERT find different optima for the weight of the phrase penalty feature function after the first iteration.", "labels": [], "entities": []}, {"text": "shows the BLEU score results on the NIST 2008 blind test using the combined dev1+dev2 corpus as training data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9667672514915466}, {"text": "NIST 2008 blind test", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.9619121849536896}]}, {"text": "While only the aren task shows improvements on the development data, lattice MERT provides consistent gains over N -best MERT on all three blind test sets.", "labels": [], "entities": [{"text": "aren", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9137649536132812}]}, {"text": "The reduced performance for N -best MERT is a consequence of the performance drop in the first iteration which causes the final weights to be far off from the initial parameter set.", "labels": [], "entities": []}, {"text": "This can impair the ability of N -best MERT to generalize to unseen data if the initial weights are already capable of producing a decent baseline.", "labels": [], "entities": []}, {"text": "Lattice MERT on the other hand can produce weights sets which are closer to the initial weights and thus more likely to retain the ability to generalize to unseen data.", "labels": [], "entities": [{"text": "Lattice", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6847074031829834}, {"text": "MERT", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.5249887704849243}]}, {"text": "It could therefore be worthwhile to investigate whether a more elaborated version of an initial-weights prior allows for alleviating this effect in case of Nbest MERT.", "labels": [], "entities": [{"text": "Nbest MERT", "start_pos": 156, "end_pos": 166, "type": "DATASET", "confidence": 0.6915507018566132}]}, {"text": "shows the effect of optimizing the feature function weights along some randomly chosen directions in addition to the coordinate axes.", "labels": [], "entities": []}, {"text": "The different local optima found on the development set by using random directions result in additional gains on the blind test sets and range from 0.1% to 0.6% absolute in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.9982715845108032}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics for three text translation sets:  Arabic-to-English (aren), Chinese-to-English (zhen),  and English-to-Chinese (enzh). Development and test  data are compiled from evaluation data used in past  NIST Machine Translation Evaluations.", "labels": [], "entities": [{"text": "NIST Machine Translation Evaluations", "start_pos": 222, "end_pos": 258, "type": "TASK", "confidence": 0.7233638763427734}]}, {"text": " Table 2: BLEU score results on the NIST-08 test set  obtained after 25 iterations using N -best MERT or 5  iterations using lattice MERT, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990752935409546}, {"text": "NIST-08 test set", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.9803169965744019}]}, {"text": " Table 3: BLEU score results on the NIST-08 tests set  obtained after 5 iterations using lattice MERT with  different numbers of random directions in addition to the  optimization along the coordinate axes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991621971130371}, {"text": "NIST-08 tests set", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.9764145215352377}]}]}