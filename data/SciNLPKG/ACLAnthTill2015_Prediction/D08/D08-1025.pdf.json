{"title": [{"text": "A noisy-channel model of rational human sentence comprehension under uncertain input", "labels": [], "entities": []}], "abstractContent": [{"text": "Language comprehension, as with all other cases of the extraction of meaningful structure from perceptual input, takes places under noisy conditions.", "labels": [], "entities": [{"text": "Language comprehension", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7264353036880493}]}, {"text": "If human language comprehension is a rational process in the sense of making use of all available information sources, then we might expect uncertainty at the level of word-level input to affect sentence-level comprehension.", "labels": [], "entities": []}, {"text": "However, nearly all contemporary models of sentence comprehension assume clean input-that is, that the input to the sentence-level comprehension mechanism is a perfectly-formed, completely certain sequence of input tokens (words).", "labels": [], "entities": []}, {"text": "This article presents a simple model of rational human sentence comprehension under noisy input, and uses the model to investigate some outstanding problems in the psycholinguistic literature for theories of rational human sentence comprehension.", "labels": [], "entities": []}, {"text": "We argue that by explicitly accounting for input-level noise in sentence processing, our model provides solutions for these outstanding problems and broadens the scope of theories of human sentence comprehension as rational prob-abilistic inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Considering the adversity of the conditions under which linguistic communication takes place in everyday life-ambiguity of the signal, environmental competition for our attention, speaker error, and so forth-it is perhaps remarkable that we are as successful at it as we are.", "labels": [], "entities": []}, {"text": "Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (.", "labels": [], "entities": []}, {"text": "Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension.", "labels": [], "entities": []}, {"text": "This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (.", "labels": [], "entities": []}, {"text": "However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific \"winner-takes-all\" sequence of words, which is in turn the input to an evidential sentencecomprehension process.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 169, "end_pos": 185, "type": "TASK", "confidence": 0.7337868809700012}]}, {"text": "It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity.", "labels": [], "entities": []}, {"text": "On the other hand, it is also possible that this partition has been a theoretical convenience but that, in fact, evidence at the sub-word level plays an important role in sentence processing, and that sentence-level information can in turn affect word recognition.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.7596374750137329}, {"text": "word recognition", "start_pos": 247, "end_pos": 263, "type": "TASK", "confidence": 0.759892463684082}]}, {"text": "If the latter is the case, then the question arises of how we might model this type of information flow, and what consequences it might have for our understanding of human language comprehension.", "labels": [], "entities": []}, {"text": "This article employs the well-understood formalisms of probabilistic context-free grammars (PCFGs) and weighted finite-state automata (wFSAs) to propose a novel yet simple noisy-channel probabilistic model of sentence comprehension under circumstances where there is uncertainty about word-level representations.", "labels": [], "entities": []}, {"text": "Section 2 introduces this model.", "labels": [], "entities": []}, {"text": "We use this new model to investigate two outstanding problems for the theory of rational sentence comprehension: one involving global inference-the beliefs that a human comprehender arrives at regarding the meaning of a sentence after reading it in its entirety (Section 3)-and one involving incremental inference-the beliefs that a comprehender forms and updates moment by moment while reading each part of it (Section 4).", "labels": [], "entities": [{"text": "global inference-the beliefs that a human comprehender arrives at regarding the meaning of a sentence after reading it in its entirety", "start_pos": 127, "end_pos": 261, "type": "TASK", "confidence": 0.6501324687685285}]}, {"text": "The common challenge posed by each of these problems is an apparent failure on the part of the comprehender to use information made available in one part of a sentence to rule out an interpretation of another part of the sentence that is inconsistent with this information.", "labels": [], "entities": []}, {"text": "In each case, we will see that the introduction of uncertainty into the input representation, coupled with noisy-channel inference, provides a unified solution within a theory of rational comprehension.", "labels": [], "entities": []}], "datasetContent": [{"text": "As in Section 3, we use a small probabilistic grammar covering the relevant structures in the problem domain to represent the comprehender's knowledge, and a wFSA based on the Levenshtein-distance kernel to represent noisy input.", "labels": [], "entities": []}, {"text": "We are interested in comparing the EIS at the word tossed in versus the EIS at the word thrown in (4a).", "labels": [], "entities": [{"text": "EIS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.7532194256782532}, {"text": "EIS", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.6734259724617004}]}, {"text": "In this case, the interval w [0,j) contains all the material that could possibly have come before the word tossed/thrown, but does not contain material at or after the position introduced by the word itself.", "labels": [], "entities": []}, {"text": "Loops in the probabilistic grammar and the Levenshtein-distance kernel pose a challenge, however, to evaluating the EIS, because the normalization constant of the resulting grammar/input intersection is essential to evaluating Equation (XIII).", "labels": [], "entities": [{"text": "Equation (XIII)", "start_pos": 227, "end_pos": 242, "type": "METRIC", "confidence": 0.9344236105680466}]}, {"text": "To circumvent this problem, we eliminate loops from the kernel by allowing only one insertion per inter-word space.", "labels": [], "entities": []}, {"text": "8 (See Section 5 fora possible alternative).", "labels": [], "entities": []}], "tableCaptions": []}