{"title": [{"text": "Seeded Discovery of Base Relations in Large Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Relationship discovery is the task of identifying salient relationships between named entities in text.", "labels": [], "entities": [{"text": "Relationship discovery", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8460148870944977}]}, {"text": "We propose novel approaches for two sub-tasks of the problem: identifying the entities of interest, and partitioning and describing the relations based on their semantics.", "labels": [], "entities": []}, {"text": "In particular, we show that term frequency patterns can be used effectively instead of supervised NER, and that the p-median clustering objective function naturally uncovers relation exemplars appropriate for describing the partitioning.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce a novel application of relationship discovery: the unsupervised identification of protein-protein interaction phrases.", "labels": [], "entities": [{"text": "relationship discovery", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7424132525920868}, {"text": "identification of protein-protein interaction phrases", "start_pos": 90, "end_pos": 143, "type": "TASK", "confidence": 0.735464334487915}]}], "introductionContent": [{"text": "Relationship extraction (RE) is the task of extracting named relationships between entities in text given some information about the relationships of interest.", "labels": [], "entities": [{"text": "Relationship extraction (RE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8905961751937866}]}, {"text": "Relationship discovery (RD), on the other hand, is the task of finding which relations exist in a corpus without any prior knowledge.", "labels": [], "entities": [{"text": "Relationship discovery (RD)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.925419020652771}]}, {"text": "The discovered relationships can then be used to bootstrap RE, which is why RD has also been called unsupervised relation extraction).", "labels": [], "entities": [{"text": "bootstrap RE", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.4882858544588089}, {"text": "relation extraction", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7352298945188522}]}, {"text": "RD generally involves three sub-tasks: entities of interest are either supplied or recognized in the corpus; second, of all phrases in which entities co-occur, those which express a relation are picked out; finally, these relationship phrases are partitioned based on their semantics and described.", "labels": [], "entities": [{"text": "RD", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9806170463562012}]}, {"text": "This work considers only binary relations (those between exactly two entities).", "labels": [], "entities": []}, {"text": "Finding entities of interest has involved either named entity recognition (NER) or general noun * This work was conducted while author was at Virginia Tech.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.7667708198229471}]}, {"text": "phrase (NP) chunking, to create the initial pool of candidate entities.", "labels": [], "entities": [{"text": "phrase (NP) chunking", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5857916355133057}]}, {"text": "In Section 2, we describe a corpus statistics approach, previously applied for web mining), which we extend for relation discovery.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.9280532598495483}]}, {"text": "Unlike supervised machine learning methods, this algorithm does not need training, is computationally efficient, and requires as input only the raw corpus and a small set of seed entities (as few as two).", "labels": [], "entities": []}, {"text": "The result is a set of entities likely to be related to the seeds.", "labels": [], "entities": []}, {"text": "An assumption commonly held in RD work is that frequently co-occurring entity tuples are likely to stand in some fixed relation.", "labels": [], "entities": [{"text": "RD work", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9257105588912964}]}, {"text": "Tuples which share similar contexts (the exact definition of context varies) are then grouped together in clusters of relations using variants of hierarchical agglomerate clustering (HAC).", "labels": [], "entities": []}, {"text": "However, to our knowledge, no prior work has satisfactorily addressed the problem of describing the resulting clusters.", "labels": [], "entities": []}, {"text": "In Section 3, we propose an approach which incorporates this requirement directly into the clustering objective: to find relation clusters which are well-described by a single exemplar.", "labels": [], "entities": []}, {"text": "In Section 4, we apply RD to recognize proteinprotein interaction (PPI) sentences, using proteins as seeds for the entity discovery phase.", "labels": [], "entities": [{"text": "entity discovery phase", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7657154599825541}]}, {"text": "We compare our results against special-purpose methods in terms of precision and recall on standard data sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9995032548904419}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9995030164718628}]}, {"text": "The remainder of this paper is outlined below: Section 2 describes how a small number of input words (the entities of interest) are used as seeds for unsupervised entity discovery.", "labels": [], "entities": [{"text": "unsupervised entity discovery", "start_pos": 150, "end_pos": 179, "type": "TASK", "confidence": 0.7064464092254639}]}, {"text": "Section 3 describes how discovered entities are used to discover relationships.", "labels": [], "entities": []}, {"text": "Section 4 describes evaluation methodology and results.", "labels": [], "entities": []}, {"text": "Section 5 describes related work.", "labels": [], "entities": []}, {"text": "Section 6 concludes and discusses directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "RD systems are usually evaluated based on their results fora particular task such as RE (), or by a manual inspection of their results (), but we are not aware of any which examines the effects of parameters on performance exhaustively.", "labels": [], "entities": [{"text": "RD", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9391487240791321}, {"text": "RE", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.6032702326774597}]}, {"text": "In this section we test several hypotheses of RD using data sets which are already labeled for sentences which contain entities of a particular type and in a fixed relation of some kind.", "labels": [], "entities": [{"text": "RD", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9841310381889343}]}, {"text": "In particular, we adapt the output of the discovery phase to identify phrases which express PPIs.", "labels": [], "entities": []}, {"text": "While this task is traditionally performed using supervised algorithms such as support vector machines (, we show that RD is capable of achieving similar levels of precision without any manually annotated training data.", "labels": [], "entities": [{"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9977535605430603}]}, {"text": "Method: To evaluate the performance of our system, we measure how well the relationships discovered compare with manually selected PPI sentences.", "labels": [], "entities": []}, {"text": "To do so, we follow the same procedure and data sets used to evaluate semi-supervised classification of PPI sentences ().", "labels": [], "entities": []}, {"text": "The two data sets are AIMED and CB, which have been marked for protein entities and interaction phrases 3 . For each sentence in which n proteins appear, we build n 2 phrases.", "labels": [], "entities": [{"text": "AIMED", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.5614080429077148}]}, {"text": "Each phrase consists of the words between each entity combination, and is labeled as positive if it describes a PPI, or negative otherwise.", "labels": [], "entities": []}, {"text": "This results in 4026 phrases for the AIMED data set (951 positive, 3075 negative), and 4056 phrases for the CB data set (2202 positive, 1854 negative).", "labels": [], "entities": [{"text": "AIMED data set", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.9522897601127625}, {"text": "CB data set", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.9522781570752462}]}, {"text": "The output of the discovery phase is a clustering of RPs.", "labels": [], "entities": []}, {"text": "For purpose of this experiment, we ignore the partition and treat the phrases in aggregate.", "labels": [], "entities": []}, {"text": "A phrase in the evaluation data set is classified as positive (describing a PPI) if any substring of the phrase matches an RP in our output.", "labels": [], "entities": []}, {"text": "For example, if the phrase is: A significantly inhibited B and the string 'inhibited' appears as a relation in our output, then this phrase is marked positive.", "labels": [], "entities": []}, {"text": "Otherwise, the phrase is marked negative.", "labels": [], "entities": []}, {"text": "Performance is evaluated using standard metrics of precision (P ), recall (R), and F-measure (F 1 ), defined as: where T P is the number of phrases correctly identified as describing a PPI, F P is the number of phrases incorrectly classified as describing a relation, and F N is the number of interaction phrases (positives) marked negative.", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 51, "end_pos": 65, "type": "METRIC", "confidence": 0.9224525094032288}, {"text": "recall (R)", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9412208199501038}, {"text": "F-measure (F 1 )", "start_pos": 83, "end_pos": 99, "type": "METRIC", "confidence": 0.9447009563446045}]}, {"text": "F 1 is defined as: We calculate P , R, and F 1 for three parameters affecting which phrases are identified as expressing a relation: \u2022 the minimum co-occurrence threshold that controls which entity tuples are kept as likely to stand in some fixed relation \u2022 the minimum cluster size that controls which groups of relations are discarded \u2022 the minimum RP length that controls the smallest number of words appearing in relations The threshold on the length of the relations can bethought of as controlling the amount of contextual information expressed.", "labels": [], "entities": [{"text": "F 1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9261477887630463}, {"text": "F 1", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.974832147359848}]}, {"text": "A single term relation will be very general, while longer RPs express a relation very specific to the context in which they are written.", "labels": [], "entities": []}, {"text": "The results are reported in.", "labels": [], "entities": []}, {"text": "Odd numbered figures use the AIMED corpus; even numbered figures the CB corpus.", "labels": [], "entities": [{"text": "AIMED corpus", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.8331815004348755}, {"text": "CB corpus", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.932894378900528}]}, {"text": "Results: Discarding clusters below a certain size had no significant effect on precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9984062314033508}]}, {"text": "However, this step is still necessary for bootstrapping RE, since machine learning approaches require a sufficient number of positive examples to train the extractor.", "labels": [], "entities": [{"text": "bootstrapping RE", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.712262362241745}]}, {"text": "On the other hand, our results confirm the observation that frequently co-occurring pairs of entities are likely to stand in a fixed relation.", "labels": [], "entities": []}, {"text": "On the CB corpus, precision ranges from 0.63 to 0.86 for phrases between entities co-occurring at least 50 times.", "labels": [], "entities": [{"text": "CB corpus", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.7601417601108551}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9994348883628845}]}, {"text": "On the AIMED corpus, precision ranges from 0.29 to 0.55 in the same threshold range.", "labels": [], "entities": [{"text": "AIMED corpus", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.7483790814876556}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.999367892742157}]}, {"text": "The minimum phrase length had the most impact on performance, which was particularly evident in the CB corpus: this corpus reached perfect precision discarding all RPs of fewer than 3 words.", "labels": [], "entities": [{"text": "CB corpus", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.793046236038208}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.993362307548523}]}, {"text": "Lower thresholds result in significantly more relations, at the cost of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9985349178314209}]}, {"text": "The generally lower performance on the AIMED corpus suggests that our training data (retrieved from the seed proteins) provided less coverage for those interactions than for the those in the CB corpus.", "labels": [], "entities": [{"text": "AIMED corpus", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8266586363315582}]}, {"text": "compare our results at fixed parameter settings with supervised approaches.", "labels": [], "entities": []}, {"text": "RD-F 1 reports parameters which give highest recall and RD-P highest precision.", "labels": [], "entities": [{"text": "RD-F 1", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5355142951011658}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9996870756149292}, {"text": "RD-P", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9043735265731812}, {"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9789621233940125}]}, {"text": "Specifically, both RD-F 1 and RD-P use a minimum RP length of 1, RD-F 1 uses a co-occurrence threshold of 10, and RD-P uses a co-occurrence threshold of 50.", "labels": [], "entities": [{"text": "RP length", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9134493470191956}]}, {"text": "As expected, RD alone does not match combined precision and recall of state-of-the-art supervised systems.", "labels": [], "entities": [{"text": "RD", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.8207978010177612}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9993010759353638}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9985859394073486}]}, {"text": "However, we show better performance than expected.", "labels": [], "entities": []}, {"text": "RD-F 1 outperforms the best results of ().", "labels": [], "entities": [{"text": "RD-F 1", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9740957617759705}]}, {"text": "RD-P settings outperform or match the precision of top-performing systems on both datasets.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9968000650405884}]}, {"text": "Method: We evaluate the appropriateness of the p-median clustering as follows.", "labels": [], "entities": []}, {"text": "For each cluster, we take the cluster exemplar as defining the base relation.", "labels": [], "entities": []}, {"text": "If the base relation does not express something meaningful, then we mark each member of the cluster incorrect.", "labels": [], "entities": []}, {"text": "Otherwise, we label each member of the cluster either as semantically similar to the exemplar (correct) or different than the exemplar (incorrect).", "labels": [], "entities": []}, {"text": "Thus, clusters with inappropriate exemplars are heavily penalized.", "labels": [], "entities": []}, {"text": "These results are reported in.", "labels": [], "entities": []}, {"text": "For purpose of this experiment, we use the same parameters as for RD-P , and evaluate the 20 largest clusters.", "labels": [], "entities": []}, {"text": "Results: In the 20 largest clusters, each cluster exemplar expressed something meaningful.", "labels": [], "entities": []}, {"text": "3 of the cluster exemplars were not representative of their other members.", "labels": [], "entities": []}, {"text": "We found that most error was due to stopwords not being considered in our similarity calculations.", "labels": [], "entities": [{"text": "error", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9862365126609802}]}, {"text": "For example, 'detected by' and 'detected in' express the same relationship in our similarity calculations; however, they are clearly quite different.", "labels": [], "entities": []}, {"text": "Another source of error evident in are mistakes in the pattern and entropy based chunking.", "labels": [], "entities": []}, {"text": "The exemplar 'mrna expression in' includes the token 'mrna', which belongs with the left protein NP in the relation chosen as an exemplar.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison with supervised methods-AIMED  corpus", "labels": [], "entities": []}, {"text": " Table 3: Comparison with supervised methods-CB  corpus", "labels": [], "entities": []}, {"text": " Table 4: Base relations identified using RP-P parameters", "labels": [], "entities": []}]}