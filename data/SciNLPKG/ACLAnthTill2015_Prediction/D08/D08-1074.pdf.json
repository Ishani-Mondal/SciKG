{"title": [{"text": "Automatic inference of the temporal location of situations in Chinese text", "labels": [], "entities": []}], "abstractContent": [{"text": "Chinese is a language that does not have morphological tense markers that provide explicit grammaticalization of the temporal location of situations (events or states).", "labels": [], "entities": []}, {"text": "However, in many NLP applications such as Machine Translation , Information Extraction and Question Answering , it is desirable to make the temporal location of the situations explicit.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8505402505397797}, {"text": "Information Extraction", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8217790126800537}, {"text": "Question Answering", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8239302039146423}]}, {"text": "We describe a machine learning framework where different sources of information can be combined to predict the temporal location of situations in Chinese text.", "labels": [], "entities": []}, {"text": "Our experiments show that this approach significantly outperforms the most frequent tense baseline.", "labels": [], "entities": []}, {"text": "More importantly, the high training accuracy shows promise that this challenging problem is solvable to a level where it can be used in practical NLP applications with more training data, better modeling techniques and more informative and general-izable features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.954778790473938}]}], "introductionContent": [{"text": "Ina language like English, tense is an explicit (and maybe imperfect) grammaticalization of the temporal location of situations, and such temporal location is either directly or indirectly defined in relation to the moment of speech.", "labels": [], "entities": []}, {"text": "Chinese does not have grammaticalized tense in the sense that Chinese verbs are not morphologically marked for tense.", "labels": [], "entities": []}, {"text": "This is not to say, however, that Chinese speakers do not attempt to convey the temporal location of situations when they speak or write, or that they cannot interpret the temporal location when they read Chinese text, or even that they have a different way of representing the temporal location of situations.", "labels": [], "entities": []}, {"text": "In fact, there is evidence that the temporal location is represented in Chinese in exactly the same way as it is represented in English and most world languages: in relation to the moment of speech.", "labels": [], "entities": []}, {"text": "One piece of evidence to support this claim is that Chinese temporal expressions like (\"today\"), (\"tomorrow\") and (\"yesterday\") all assume a temporal deixis that is the moment of speech in relation to which all temporal locations are defined.", "labels": [], "entities": []}, {"text": "Such temporal expressions, where they are present, give us a clear indication of the temporal location of the situations they are associated with.", "labels": [], "entities": []}, {"text": "However, not all Chinese sentences have such temporal expressions associated with them.", "labels": [], "entities": []}, {"text": "In fact, they occur only infrequently in Chinese text.", "labels": [], "entities": []}, {"text": "It is thus theoretically interesting to ask, in the absence of grammatical tense and explicit temporal expressions, how do readers of a particular piece of text interpret the temporal location of situations?", "labels": [], "entities": []}, {"text": "There area few linguistic devices in Chinese that provide obvious clues to the temporal location of situations, and one such linguistic device is aspect markers.", "labels": [], "entities": []}, {"text": "Although Chinese does not have grammatical tense, it does have grammaticalized aspect in the form of aspect markers.", "labels": [], "entities": []}, {"text": "These aspect markers often give some indication of the temporal location of an event.", "labels": [], "entities": []}, {"text": "For example, Chinese has the perfective aspect marker and , and they are often associated with the past.", "labels": [], "entities": []}, {"text": "Progressive aspect marker , on the other hand, is often associated with the present.", "labels": [], "entities": [{"text": "Progressive aspect marker", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5271829068660736}]}, {"text": "In addition to aspect, certain adverbs also provide clues to the temporal location of the situations they are as-sociated with.", "labels": [], "entities": []}, {"text": "For example, or (\"already\"), often indicates that the situation they are associated with has already occurred and is thus in the past.", "labels": [], "entities": []}, {"text": ", another adverbial modifier, often indicates that the situation it modifies is in the present.", "labels": [], "entities": []}, {"text": "However, such linguistic associations are imperfect, and they can only be viewed as tendencies rather than rules that one can use to deterministically infer the temporal location of a situation.", "labels": [], "entities": []}, {"text": "For example, while indeed indicates that the situation described in (1) is in the past, when it modifies a stative verb as it does in (1b), the situation is still in the present.", "labels": [], "entities": []}, {"text": "( \"China already has the foundation to produce world-class software.\"", "labels": [], "entities": []}, {"text": "More importantly, only a small proportion of verb instances in any given text have such explicit temporal indicators and therefore they cannot be the whole story in the temporal interpretation of Chinese text.", "labels": [], "entities": []}, {"text": "It is thus theoretically interesting to go beyond the obvious and investigate what additional information is relevant in determining the temporal location of a situation in Chinese.", "labels": [], "entities": []}, {"text": "Being able to infer the temporal location of a situation has many practical applications as well.", "labels": [], "entities": []}, {"text": "For example, this information would be highly valuable to Machine Translation.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.8641883730888367}]}, {"text": "To translate a language like Chinese into a language like English in which tense is grammatically marked with inflectional morphemes, an MT system will have to infer the necessary temporal information to determine the correct tense for verbs.", "labels": [], "entities": [{"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.9655032753944397}]}, {"text": "Statistical MT systems, the currently dominant research paradigm, typically do not address this issue directly.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.7424033880233765}]}, {"text": "As a result, when evaluated for tense, current MT systems often perform miserably.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9898423552513123}]}, {"text": "For example, when a simple sentence like \"/he /tomorrow /return /Shanghai\" is given to Google's state-of-the-art Machine Translation system 1 , it produces the output \"He returned to Shanghai tomorrow\", instead of the correct \"he will return to Shanghai tomorrow\".", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.6927042603492737}]}, {"text": "The past tense on the verb \"returned\" contradicts the temporal expression \"tomorrow\".", "labels": [], "entities": []}, {"text": "Determining the temporal location is also important for an Information Extraction task that extracts events so that the extracted events are put in a temporal context.", "labels": [], "entities": [{"text": "Information Extraction task", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.8223110636075338}]}, {"text": "Similarly, for Question Answering tasks, it is also important to know whether a situation has already happened or it is going to happen, for example.", "labels": [], "entities": [{"text": "Question Answering tasks", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.8854843378067017}]}, {"text": "In this paper, we are interested in investigating the kind of information that is relevant in inferring the temporal location of situations in Chinese text.", "labels": [], "entities": []}, {"text": "We approach this problem by manually annotating each verb in a Chinese document with a \"tense\" tag that indicates the temporal location of the verb 2 . We then formulate the tense determination problem as a classification task where standard machine learning techniques can be applied.", "labels": [], "entities": [{"text": "tense determination", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.7230139821767807}]}, {"text": "Figuring out what linguistic information contributes to the determination of the temporal location of a situation becomes a feature engineering problem of selecting features that help with the automatic classification.", "labels": [], "entities": [{"text": "determination of the temporal location of a situation", "start_pos": 60, "end_pos": 113, "type": "TASK", "confidence": 0.7614847868680954}]}, {"text": "In Section 2, we present a linguistic annotation framework that annotates the temporal location of situations in Chinese text.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our setup for an automatic tense classification experiment and present our experimental results.", "labels": [], "entities": [{"text": "tense classification experiment", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.8266496857007345}]}, {"text": "In Section 4 we focus in on the features we have used in our experiment and attempt to provide a quantitative as well as intuitive explanation of the contribution of the individual features and speculate on what additional features could be useful.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss related work and Section 6 concludes the paper and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The tense determination task is then a simple fiveway classification task.", "labels": [], "entities": [{"text": "tense determination task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8818694949150085}, {"text": "fiveway classification task", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7172115544478098}]}, {"text": "Theoretically any standard machine learning algorithm can be applied to the task.", "labels": [], "entities": []}, {"text": "For our purposes we used the Maximum Entropy algorithm implemented as part of the Mallet machine learning package) for its competitive training time and performance tradeoff.", "labels": [], "entities": [{"text": "Mallet machine learning package", "start_pos": 82, "end_pos": 113, "type": "DATASET", "confidence": 0.9285058230161667}]}, {"text": "There might be algorithms that could achieve higher classification accuracy, but our goal in this paper is not to pursue the absolute high performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9209885597229004}]}, {"text": "Rather, our purpose is to investigate what information when used as features is relevant to determining the temporal location of a situation in Chinese, so that these features can be used to design high performance practical systems in the future.", "labels": [], "entities": []}, {"text": "The annotation of 42 articles yielded 5709 verb instances, each of which is annotated with one of the five tense tags.", "labels": [], "entities": []}, {"text": "For our automatic classification experiments, we randomly divided the data into a training set and a test set based on a 3-to-1 ratio, so that the training data has 4,250 instances while the test set has 1459 instances.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.6660754084587097}]}, {"text": "As expected, the past tense is the most frequent tense in both the training and test data, although they vary quite a bit in the proportions of verbs that are labeled with the past tense.", "labels": [], "entities": []}, {"text": "In the training data, 2145, or 50.5% of the verb instances are labeled with the past tense while in the test data, 911 or 62.4% of the verb instances are labeled with the past tense.", "labels": [], "entities": []}, {"text": "The 62.4% can thus be used as a baseline when evaluating the automatic classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9458708763122559}]}, {"text": "This is a very high baseline given that the much smaller proportion of verbs that are assigned the past tense in the training data.", "labels": [], "entities": []}, {"text": "Instead of raw text, the input to the classification algorithm is parsed sentences from the Chinese Treebank that has the syntactic structure information as well as the part-of-speech tags.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9639027714729309}]}, {"text": "As we will show in the next section, information extracted from the parse tree as well as the part-of-speech tags prove to be very important in determining the temporal location of a situation.", "labels": [], "entities": []}, {"text": "The reason for using \"correct\" parse trees in the Chinese Treebank is to factor out noises that are inevitable in the output of an automatic parser and evaluate the contribution of syntactic information in the \"ideal\" scenario.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9768559336662292}]}, {"text": "Ina realistic setting, one of course has to use an automatic parser.", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "The overall accuracy is 67.1%, exceeding the baseline of choosing the most frequent tense in the test, which is 62.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996620416641235}]}, {"text": "It is worth noting that the training accuracy is fairly high, 93%, and there is a steep drop-off from the training accuracy to the test accuracy although this is hardly unexpected given the relatively small training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9628880023956299}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.5850688815116882}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.832764208316803}]}, {"text": "The high training accuracy nevertheless attests the relevance of the features we have chosen for the classification, which we will look at in greater detail in the next section.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9158632159233093}, {"text": "classification", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.9736594557762146}]}], "tableCaptions": [{"text": " Table 1. The overall  accuracy is 67.1%, exceeding the baseline of choos- ing the most frequent tense in the test, which is  62.4%. It is worth noting that the training accu- racy is fairly high, 93%, and there is a steep drop-off  from the training accuracy to the test accuracy al- though this is hardly unexpected given the relatively  small training set. The high training accuracy never- theless attests the relevance of the features we have  chosen for the classification, which we will look at  in greater detail in the next section.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9990353584289551}, {"text": "training accu- racy", "start_pos": 161, "end_pos": 180, "type": "METRIC", "confidence": 0.8042770028114319}, {"text": "accuracy", "start_pos": 378, "end_pos": 386, "type": "METRIC", "confidence": 0.6456953287124634}]}]}