{"title": [{"text": "Complexity of finding the BLEU-optimal hypothesis in a confusion network", "labels": [], "entities": [{"text": "BLEU-optimal", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9946843981742859}]}], "abstractContent": [{"text": "Confusion networks area simple representation of multiple speech recognition or translation hypotheses in a machine translation system.", "labels": [], "entities": [{"text": "representation of multiple speech recognition or translation hypotheses", "start_pos": 31, "end_pos": 102, "type": "TASK", "confidence": 0.7552765347063541}]}, {"text": "A typical operation on a confusion network is to find the path which minimizes or maximizes a certain evaluation metric.", "labels": [], "entities": []}, {"text": "In this article, we show that this problem is generally NP-hard for the popular BLEU metric, as well as for smaller variants of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9232748746871948}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.6501787900924683}]}, {"text": "This also holds for more complex representations like generic word graphs.", "labels": [], "entities": []}, {"text": "In addition, we give an efficient polynomial-time algorithm to calculate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9502171277999878}]}, {"text": "Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9656659960746765}]}], "introductionContent": [{"text": "In machine translation (MT), confusion networks (CNs) are commonly used to represent alternative versions of sentences.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.8606736779212951}]}, {"text": "Typical applications include translation of different speech recognition hypotheses ( or system combination).", "labels": [], "entities": [{"text": "translation of different speech recognition hypotheses", "start_pos": 29, "end_pos": 83, "type": "TASK", "confidence": 0.8538330296675364}]}, {"text": "A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric.", "labels": [], "entities": []}, {"text": "This operation can be used in applications like Minimum Error Rate Training, or optimizing system combination as described by.", "labels": [], "entities": [{"text": "Minimum Error Rate Training", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.708405464887619}]}, {"text": "Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by, current research in MT uses more sophisticated measures, like the BLEU score ().", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 62, "end_pos": 83, "type": "METRIC", "confidence": 0.8610508392254511}, {"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.9820355772972107}, {"text": "BLEU score", "start_pos": 167, "end_pos": 177, "type": "METRIC", "confidence": 0.9824132919311523}]}, {"text": "Zens and Ney (2005) first described this task on general word graphs, and sketched a complete algorithm for calculating the maximum BLEU score in a word graph.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9830273985862732}]}, {"text": "While they do not give an estimate on the complexity of their algorithm, they note that already a simpler algorithm for calculating the Position independent Error Rate (PER) has an exponential worst-case complexity.", "labels": [], "entities": [{"text": "Position independent Error Rate (PER)", "start_pos": 136, "end_pos": 173, "type": "METRIC", "confidence": 0.8797202110290527}]}, {"text": "The same can be expected for their BLEU algorithm.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9930549263954163}]}, {"text": "examined a special class of word graphs, namely those that denote constrained reorderings of single sentences.", "labels": [], "entities": []}, {"text": "These word graphs have some properties which simplify the calculation; for example, no edge is labeled with the empty word, and all paths have the same length and end in the same node.", "labels": [], "entities": []}, {"text": "Even then, their decoder does not optimize the true BLEU score, but an approximate version which uses a language-model-like unmodified precision.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9786345064640045}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9432435035705566}]}, {"text": "We give a very short introduction to CNs and the BLEU score in Section 2.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9626371264457703}]}, {"text": "In Section 3 we show that finding the best BLEU score is an NP-hard problem, even fora simplified variant of BLEU which only scores unigrams and bigrams.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9662114679813385}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.7365227341651917}]}, {"text": "The main reason for this problem to become NP-hard is that by looking at bigrams, we allow for one decision to also influence the following decision, which itself can influence the decisions after that.", "labels": [], "entities": []}, {"text": "We also show that this also holds for unigram BLEU and the position independent error rate (PER) on a slightly augmented variant of CNs which allows for edges to carry multiple symbols.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9742975234985352}, {"text": "position independent error rate (PER)", "start_pos": 59, "end_pos": 96, "type": "METRIC", "confidence": 0.8869390487670898}]}, {"text": "The concatenation of symbols corresponds to the interdependency of decisions in the case of bigram matches above.", "labels": [], "entities": []}, {"text": "NP-hard problems are quite common in machine translation; for example, has shown that even fora simple form of statistical MT models, the decoding problem is NP-complete.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7407988905906677}, {"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.8876393437385559}]}, {"text": "More recently, have proven the NP-completeness of the phrase alignment problem.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7894563376903534}]}, {"text": "But even a simple, common procedure as BLEU scoring, which can be performed in linear time on single sentences, becomes a potentially intractable problem as soon as it has to be performed on a slightly more powerful representation, such as confusion networks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9917992353439331}]}, {"text": "This rather surprising result is the motivation of this paper.", "labels": [], "entities": []}, {"text": "The problem of finding the best unigram BLEU score in an unaugmented variant of CNs is not NPcomplete, as we show in Section 4.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9615976512432098}]}, {"text": "We present an algorithm that finds such a unigram BLEU-best path in polynomial time.", "labels": [], "entities": [{"text": "BLEU-best", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.985057532787323}]}, {"text": "An important corollary of this work is that calculating the BLEU-best path on general word graphs is also NP-complete, as CNs area true subclass of word graphs.", "labels": [], "entities": [{"text": "BLEU-best", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9962437152862549}]}, {"text": "It is still desirable to calculate a \"good\" path in terms of the BLEU score in a CN, even if calculating the best path is infeasible.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9801629483699799}]}, {"text": "In Section 5, we present an algorithm which can calculate \"good\" solutions for CNs in polynomial time.", "labels": [], "entities": []}, {"text": "This algorithm can easily be extended to handle arbitrary word graphs.", "labels": [], "entities": []}, {"text": "We assess the algorithm experimentally on real-world MT data in Section 6, and draw some conclusions from the results in this article in Section 7.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9632129669189453}]}], "datasetContent": [{"text": "The question arises how many path hypotheses we need to retain in each step to obtain optimal paths.", "labels": [], "entities": []}, {"text": "To examine this, we created confusion networks out of the translations of the four best MT systems of q q q q q q q q q q q q q q q the Chinese-English evaluation campaigns, as available from the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9664677977561951}, {"text": "Linguistic Data Consortium (LDC)", "start_pos": 196, "end_pos": 228, "type": "DATASET", "confidence": 0.8570821185906728}]}, {"text": "The hypotheses of the best single system served as skeleton, those of the three remaining systems were reordered and aligned to the skeleton hypothesis.", "labels": [], "entities": []}, {"text": "This corpus is described in Table 1.", "labels": [], "entities": []}, {"text": "show the measured BLEU scores in three different definitions, versus the maximum number of path hypotheses that are kept in each node of the search graph.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9988799691200256}]}, {"text": "Shown are the average sentence-wise BLEUS score, which is what the algorithm actually optimizes, for comparison the average sentence-wise BLEU score, and the total document-wise BLEU score.", "labels": [], "entities": [{"text": "BLEUS score", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.962832123041153}, {"text": "BLEU score", "start_pos": 138, "end_pos": 148, "type": "METRIC", "confidence": 0.948472797870636}, {"text": "BLEU", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.8633188009262085}]}, {"text": "All scores increase with increasing number of retained hypotheses, but stabilize around a total of 15 hypotheses per node.", "labels": [], "entities": []}, {"text": "The difference over a greedy approach, which corresponds to a maximum of one hypothesis per node if we leave out the separation bypath length, is quite significant.", "labels": [], "entities": []}, {"text": "No further improvements can be expected fora higher number of hypotheses, as experiments up to 100 hypotheses show.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the (Chinese-)English MT corpora  used for the experiments", "labels": [], "entities": []}]}