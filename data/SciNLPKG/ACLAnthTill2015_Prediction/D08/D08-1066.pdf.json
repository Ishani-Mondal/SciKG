{"title": [{"text": "Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective", "labels": [], "entities": [{"text": "Phrase Translation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9156310260295868}, {"text": "ITG", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.7295123934745789}, {"text": "Priors", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.4395449757575989}]}], "abstractContent": [{"text": "The conditional phrase translation probabilities constitute the principal components of phrase-based machine translation systems.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.7210637032985687}, {"text": "phrase-based machine translation", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.624321460723877}]}, {"text": "These probabilities are estimated using a heuristic method that does not seem to optimize any reasonable objective function of the word-aligned, parallel training corpus.", "labels": [], "entities": []}, {"text": "Earlier efforts on devising a better understood estimator either do not scale to reasonably sized training data, or lead to deteriorating performance.", "labels": [], "entities": []}, {"text": "In this paper we explore anew approach based on three ingredients (1) A generative model with a prior over latent segmentations derived from Inversion Trans-duction Grammar (ITG), (2) A phrase table containing all phrase pairs without length limit, and (3) Smoothing as learning objective using a novel Maximum-A-Posteriori version of Deleted Estimation working with Expectation-Maximization.", "labels": [], "entities": []}, {"text": "Where others conclude that latent segmentations lead to overfitting and deteriorating performance, we show here that these three ingredients give performance equivalent to the heuristic method on reasonably sized training data.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Decoding and Baseline Model: In this work we employ an existing decoder,, which defines a log-linear model interpolating feature functions, with interpolation scores \u03bb f e * = arg max e The \u03bb fare optimized by Minimum-Error Training (MERT).", "labels": [], "entities": []}, {"text": "The set \u03a6 consists of the following feature functions (see): a 5-gram target language model, the standard reordering scores, the word and phrase penalty scores, the conditional lexical estimates obtained from the word-alignment in both directions, and the conditional phrase translation estimates in both directions P (f | e) and P (e | f ).", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 268, "end_pos": 286, "type": "TASK", "confidence": 0.7250106036663055}]}, {"text": "Keeping the other five feature functions fixed, we compare our estimates of P (f | e) and P (e | f ) (and the phrase penalty) to the commonly used heuristic estimates.", "labels": [], "entities": []}, {"text": "Because our model employs a latent segmentation variable, this variable should be marginalized out during decoding to allow selecting the highest probability translation given the input.", "labels": [], "entities": []}, {"text": "This turns out crucial for improved results (cf. ().", "labels": [], "entities": []}, {"text": "However, such a marginalization can be NPComplete, in analogy to a similar problem in DataOriented Parsing . We do not have a decoder yet that can approximate this marginalization efficiently and we employ the standard Moses decoder for this work.", "labels": [], "entities": []}, {"text": "Experimental Setup: The training, development and test data all come from the French-English translation shared task of the ACL 2007 Second A reduction of simple instances of the first problem to instances of the latter problem should be possible.", "labels": [], "entities": [{"text": "ACL 2007", "start_pos": 124, "end_pos": 132, "type": "DATASET", "confidence": 0.9311961531639099}]}, {"text": "For both the baseline system and our method, we produce word-level alignments for the parallel training corpus using GIZA++.", "labels": [], "entities": []}, {"text": "We use 5 iterations of each IBM Model 1 and HMM alignment models, followed by 3 iterations of each Model 3 and Model 4.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.5656816214323044}]}, {"text": "From this aligned training corpus, we extract the phrase pairs according to the heuristics in ().", "labels": [], "entities": []}, {"text": "The baseline system extracts all phrase-pairs upto a certain maximum length on both sides and employs the heuristic estimator.", "labels": [], "entities": []}, {"text": "The language model used in all systems is a 5-gram language model trained on the English side of the parallel corpus.", "labels": [], "entities": []}, {"text": "Minimum-Error Rate Training (MERT) is applied on the development set to obtain optimal log-linear interpolation weights for all systems.", "labels": [], "entities": [{"text": "Minimum-Error Rate Training (MERT)", "start_pos": 0, "end_pos": 34, "type": "METRIC", "confidence": 0.9050442079703013}]}, {"text": "Performance is measured by computing the BLEU scores () of the system's translations, when compared against a single reference translation per sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.999046266078949}]}], "tableCaptions": [{"text": " Table 1: Results: data from ACL07 2 nd Wkshp on SMT", "labels": [], "entities": [{"text": "ACL07 2 nd Wkshp", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.8325341641902924}, {"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.7153745889663696}]}]}