{"title": [{"text": "Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks", "labels": [], "entities": [{"text": "Penn-II and Penn-III Treebanks", "start_pos": 67, "end_pos": 97, "type": "DATASET", "confidence": 0.7805320024490356}]}], "abstractContent": [{"text": "We present a methodology for extracting subcategorization frames based on an automatic lexical-functional grammar (LFG) f-structure annotation algorithm for the Penn-II and Penn-III Treebanks.", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 161, "end_pos": 168, "type": "DATASET", "confidence": 0.9803542494773865}, {"text": "Penn-III Treebanks", "start_pos": 173, "end_pos": 191, "type": "DATASET", "confidence": 0.8194801211357117}]}, {"text": "We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based subcategorization frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.", "labels": [], "entities": []}, {"text": "Our approach associates probabilities with frames conditional on the lemma, distinguishes between active and passive frames, and fully reflects the effects of long-distance dependencies in the source data structures.", "labels": [], "entities": []}, {"text": "In contrast to many other approaches, ours does not predefine the subcategorization frame types extracted, learning them instead from the source data.", "labels": [], "entities": []}, {"text": "Including particles and prepositions, we extract 21,005 lemma frame types for 4,362 verb lemmas, with a total of 577 frame types and an average of 4.8 frame types per verb.", "labels": [], "entities": []}, {"text": "We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource.", "labels": [], "entities": [{"text": "COMLEX resource", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9569789171218872}]}, {"text": "To our knowledge, this is the largest and most complete evaluation of subcategorization frames acquired automatically for English.", "labels": [], "entities": []}], "introductionContent": [{"text": "In modern syntactic theories (e.g., lexical-functional grammar [, head-driven phrase structure grammar [, tree-adjoining grammar [, and combinatory categorial grammar [), the lexicon is the central repository for much morphological, syntactic, and semantic information.", "labels": [], "entities": []}, {"text": "Extensive lexical resources, therefore, are crucial in the construction of wide-coverage computational systems based on such theories.", "labels": [], "entities": []}, {"text": "One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction).", "labels": [], "entities": []}, {"text": "Lexicons, including subcategorization details, were traditionally produced by hand.", "labels": [], "entities": []}, {"text": "However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component.", "labels": [], "entities": []}, {"text": "In addition, subcategorization requirements may vary across linguistic domain or genre.", "labels": [], "entities": []}, {"text": "argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature.", "labels": [], "entities": []}, {"text": "Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.", "labels": [], "entities": [{"text": "automating acquisition of dictionaries for lexically based NLP systems", "start_pos": 31, "end_pos": 101, "type": "TASK", "confidence": 0.7831791970464919}]}, {"text": "Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (.", "labels": [], "entities": []}, {"text": "In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7360914051532745}]}, {"text": "However, our approach also generalizes to CFG category-based approaches.", "labels": [], "entities": []}, {"text": "In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate.", "labels": [], "entities": []}, {"text": "Our approach is based on earlier work on LFG semantic form extraction) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures ().", "labels": [], "entities": [{"text": "LFG semantic form extraction", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.5529497936367989}, {"text": "Penn-II and Penn-III Treebanks", "start_pos": 123, "end_pos": 153, "type": "DATASET", "confidence": 0.779068723320961}]}, {"text": "Our technique requires a treebank annotated with LFG functional schemata.", "labels": [], "entities": []}, {"text": "In the early approach of van, this was provided by manually annotating the rules extracted from the publicly available subset of the AP Treebank to automatically produce corresponding f-structures.", "labels": [], "entities": [{"text": "AP Treebank", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.9465702772140503}]}, {"text": "If the f-structures are of high quality, reliable LFG semantic forms can be generated quite simply by recursively reading off the subcategorizable grammatical functions for each local PRED value at each level of embedding in the f-structures.", "labels": [], "entities": []}, {"text": "The work reported in van Genabith, was small scale (100 trees) and proof of concept and required considerable manual annotation work.", "labels": [], "entities": []}, {"text": "It did not associate frames with probabilities, discriminate between frames for active and passive constructions, properly reflect the effects of long-distance dependencies (LDDs), or include CFG category information.", "labels": [], "entities": []}, {"text": "In this article we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II Treebank, with about one million words in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm described in and.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section of the Penn-II Treebank", "start_pos": 81, "end_pos": 138, "type": "DATASET", "confidence": 0.9446121129122648}]}, {"text": "More recently we have extended the extraction approach to the larger, domain-diverse Penn-III Treebank.", "labels": [], "entities": [{"text": "Penn-III Treebank", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.9856239855289459}]}, {"text": "Aside from the parsed WSJ section, this version of the treebank contains parses fora subsection of the Brown corpus (almost 385,000 words in 24,000 trees) taken from a variety of text genres.", "labels": [], "entities": [{"text": "WSJ section", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.8473435342311859}, {"text": "Brown corpus", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.949012964963913}]}, {"text": "In addition to extracting grammatical-function-based subcategorization frames, we also include the syntactic categories of the predicate and its subcategorized arguments, as well as additional details such as the prepositions required by obliques and particles accompanying particle verbs.", "labels": [], "entities": []}, {"text": "Our method discriminates between active and passive frames, properly reflects LDDs in the source data structures, assigns conditional probabilities to the semantic forms associated with each predicate, and does not predefine the subcategorization frames extracted.", "labels": [], "entities": []}, {"text": "In Section 2 of this article, we briefly outline LFG, presenting typical lexical entries and the encoding of subcategorization information.", "labels": [], "entities": []}, {"text": "Section 3 reviews related work in the area of automatic subcategorization frame extraction.", "labels": [], "entities": [{"text": "subcategorization frame extraction", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.6615411440531412}]}, {"text": "Our methodology and its implementation are presented in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5 we present results from the extraction process.", "labels": [], "entities": []}, {"text": "We evaluate the complete induced lexicon against the COMLEX resource  and present the results in Section 6.", "labels": [], "entities": [{"text": "COMLEX resource", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9709103107452393}]}, {"text": "To our knowledge, this is by far the largest and most complete evaluation of subcategorization frames automatically acquired for English.", "labels": [], "entities": []}, {"text": "In Section 7, we examine the coverage of our lexicon in regard to unseen data and the rate at which new lexical entries are learned.", "labels": [], "entities": []}, {"text": "Finally, in Section 8 we conclude and give suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most of the previous approaches discussed in Section 3 have been evaluated to different degrees.", "labels": [], "entities": []}, {"text": "In general, a small number of frequently occurring verbs is selected, and the subcategorization frames extracted for these verbs (from some quantity of unseen test data) are compared to a gold standard.", "labels": [], "entities": []}, {"text": "The gold standard is either manually custom-made based on the test data or adapted from an existing external resource such as the OALD (Hornby 1980) or COMLEX . There are advantages and disadvantages to both types of gold standard.", "labels": [], "entities": [{"text": "OALD (Hornby 1980)", "start_pos": 130, "end_pos": 148, "type": "DATASET", "confidence": 0.8453101873397827}, {"text": "COMLEX", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.6610360145568848}]}, {"text": "While it is time-consuming to manually construct a custom-made standard, the resulting standard has the advantage of containing only the subcategorization frames exhibited in the test data.", "labels": [], "entities": []}, {"text": "Using an existing externally produced resource is quicker, but the gold standard may contain many more frames than those which occur in the data from which the test lexicon is induced or, indeed, may omit relevant correct frames contained in the data.", "labels": [], "entities": []}, {"text": "As a result, systems generally score better against custom-made, manually established gold standards.", "labels": [], "entities": []}, {"text": "achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.", "labels": [], "entities": [{"text": "F-score", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.999733030796051}, {"text": "OALD", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.5606558322906494}]}, {"text": "Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions.", "labels": [], "entities": []}, {"text": "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.", "labels": [], "entities": []}, {"text": "Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9811268448829651}]}, {"text": "However, their evaluation does not examine the extracted subcategorization frames but rather the argument-adjunct distinctions posited by their system.", "labels": [], "entities": []}, {"text": "The largest lexical evaluation we know of is that of Schulte im for German.", "labels": [], "entities": []}, {"text": "She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).", "labels": [], "entities": [{"text": "Duden", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9281132817268372}, {"text": "Dudenredaktion 2001)", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.9123475948969523}]}, {"text": "We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.", "labels": [], "entities": []}, {"text": "We carried out a large-scale evaluation of our automatically induced lexicon (2,993 active verb lemmas for Penn-II and 3,529 for Penn-III, as well as 1,422 passive verb lemmas from Penn-II) against the COMLEX resource.", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 107, "end_pos": 114, "type": "DATASET", "confidence": 0.9852146506309509}, {"text": "Penn-III", "start_pos": 129, "end_pos": 137, "type": "DATASET", "confidence": 0.9584131836891174}, {"text": "Penn-II", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.9568060040473938}, {"text": "COMLEX resource", "start_pos": 202, "end_pos": 217, "type": "DATASET", "confidence": 0.9723853170871735}]}, {"text": "To our knowledge this is the most extensive evaluation ever carried out for English lexical extraction.", "labels": [], "entities": [{"text": "English lexical extraction", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.6660039822260538}]}, {"text": "We conducted a number of experiments on the subcategorization frames extracted from Penn-II and Penn-III which are described and discussed in Sections 6.2, 6.3, and 6.4.", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9885621070861816}, {"text": "Penn-III", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.8766655921936035}]}, {"text": "Finding a common format for the gold standard and induced lexical entries is a nontrivial task.", "labels": [], "entities": []}, {"text": "To ensure that we did not bias the evaluation in favor of either resource, we carried out two different mappings for the frames from Penn-II and Penn-III: COMLEX-LFG Mapping I and COMLEX-LFG Mapping II.", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.989521861076355}, {"text": "Penn-III", "start_pos": 145, "end_pos": 153, "type": "DATASET", "confidence": 0.9448410868644714}]}, {"text": "For each mapping we carried out six basic experiments (and two additional ones for COMLEX-LFG Mapping II) for the active subcategorization frames extracted.", "labels": [], "entities": [{"text": "COMLEX-LFG Mapping", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.6003343611955643}]}, {"text": "Within each experiment, the following factors were varied: level of prepositional phrase detail, level of particle detail, relative threshold (1% or 5%), and incorporation of an expanded set of directional prepositions.", "labels": [], "entities": [{"text": "level of particle detail", "start_pos": 97, "end_pos": 121, "type": "METRIC", "confidence": 0.5905293971300125}]}, {"text": "Using the second mapping we also evaluated the automatically extracted passive frames and experimented with absolute thresholds.", "labels": [], "entities": []}, {"text": "Direct comparison of subcategorization frame acquisition systems is difficult because of variations in the number of frames extracted, the number of test verbs, the gold standards used, the size of the test data, and the level of detail in the subcategorization frames (e.g., whether they are parameterized for specific prepositions).", "labels": [], "entities": [{"text": "subcategorization frame acquisition", "start_pos": 21, "end_pos": 56, "type": "TASK", "confidence": 0.6354219814141592}]}, {"text": "Therefore, in order to establish a baseline against which to compare our results, following Schulte in Walde (2002b), we assigned the two most frequent frame types (transitive and intransitive) by default to each verb and compared this \"artificial\" lexicon to the gold standard.", "labels": [], "entities": []}, {"text": "The section concludes with a full discussion of the reported results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Results of f-structure evaluation.", "labels": [], "entities": []}, {"text": " Table 3  Precision and recall on automatically generated f-structures by feature against the DCU 105.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9579305052757263}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9992102384567261}, {"text": "DCU 105", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9745209813117981}]}, {"text": " Table 5  Semantic forms for the verb accept.", "labels": [], "entities": []}, {"text": " Table 6  Semantic forms for the verb accept marked with p for passive use.", "labels": [], "entities": []}, {"text": " Table 7  Semantic forms for the verb accept including syntactic category for each grammatical function.", "labels": [], "entities": []}, {"text": " Table 8  Number of semantic form types for Penn-III.", "labels": [], "entities": [{"text": "Penn-III", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9829450249671936}]}, {"text": " Table 9  Number of frame types for verbs for Penn-II.", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9761264324188232}]}, {"text": " Table 10  Number of frame types for verbs for Penn-III.", "labels": [], "entities": [{"text": "Penn-III", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9813925623893738}]}, {"text": " Table 12  Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.7984434366226196}, {"text": "COMLEX", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.5391013026237488}]}, {"text": " Table 13  Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.7894343733787537}, {"text": "COMLEX", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.48068419098854065}]}, {"text": " Table 15  Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8801568150520325}, {"text": "COMLEX", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8948002457618713}]}, {"text": " Table 17  Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.7903847098350525}, {"text": "COMLEX", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.4905345141887665}]}, {"text": " Table 18  Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.7939046025276184}, {"text": "COMLEX", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.4892914891242981}]}, {"text": " Table 19  Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8731037378311157}, {"text": "COMLEX", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8927736878395081}]}, {"text": " Table 20  Results of Penn-II evaluation of passive frames (relative threshold of 1%).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8416376709938049}]}, {"text": " Table 21  Penn-II evaluation of active frames against COMLEX using absolute thresholds (Experiment 2).", "labels": [], "entities": [{"text": "Penn-II", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.8905283808708191}, {"text": "COMLEX", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8259012699127197}]}, {"text": " Table 22  Results of Penn-III active frames (Brown Corpus only) COMLEX comparison (relative threshold  of 1%).", "labels": [], "entities": [{"text": "Penn-III active frames", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.888344387213389}, {"text": "Brown Corpus", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8949726223945618}]}, {"text": " Table 23  Results of Penn-III active frames (Brown corpus only) COMLEX comparison (relative threshold  of 5%).", "labels": [], "entities": [{"text": "Penn-III active frames", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.8855880896250407}]}, {"text": " Table 24  Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of  1%).", "labels": [], "entities": [{"text": "Penn-III active frames (Brown and WSJ) COMLEX", "start_pos": 22, "end_pos": 67, "type": "DATASET", "confidence": 0.6543464097711775}]}, {"text": " Table 25  Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of  5%).", "labels": [], "entities": [{"text": "Penn-III active frames (Brown and WSJ) COMLEX", "start_pos": 22, "end_pos": 67, "type": "DATASET", "confidence": 0.665613419479794}]}, {"text": " Table 27  Coverage of induced lexicon (WSJ 02-21) on unseen data (WSJ 23) (verbs only).", "labels": [], "entities": [{"text": "WSJ 02-21", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.8874346613883972}, {"text": "WSJ", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8974804282188416}]}]}