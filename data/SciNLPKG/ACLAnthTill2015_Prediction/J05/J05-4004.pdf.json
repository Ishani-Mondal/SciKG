{"title": [{"text": "Induction of Word and Phrase Alignments for Automatic Document Summarization", "labels": [], "entities": [{"text": "Induction of Word and Phrase Alignments", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.873269667228063}, {"text": "Automatic Document Summarization", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.7310136953989664}]}], "abstractContent": [{"text": "Current research in automatic single-document summarization is dominated by two effective, yet na\u00a8\u0131vena\u00a8\u0131ve approaches: summarization by sentence extraction and headline generation via bag-of-words models.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.499620646238327}, {"text": "summarization", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.9810734987258911}, {"text": "sentence extraction", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.708784818649292}, {"text": "headline generation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7626218497753143}]}, {"text": "While successful in some tasks, neither of these models is able to adequately capture the large set of linguistic devices utilized by humans when they produce summaries.", "labels": [], "entities": []}, {"text": "One possible explanation for the widespread use of these models is that good techniques have been developed to extract appropriate training data for them from existing document/abstract and document/ headline corpora.", "labels": [], "entities": []}, {"text": "We believe that future progress in automatic summarization will be driven both by the development of more sophisticated, linguistically informed models, as well as a more effective leveraging of document/abstract corpora.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.8356295824050903}]}, {"text": "In order to open the doors to simultaneously achieving both of these goals, we have developed techniques for automatically producing word-to-word and phrase-to-phrase alignments between documents and their human-written abstracts.", "labels": [], "entities": []}, {"text": "These alignments make explicit the correspondences that exist in such docu-ment/abstract pairs and create a potentially rich data source from which complex summarization algorithms may learn.", "labels": [], "entities": []}, {"text": "This paper describes experiments we have carried out to analyze the ability of humans to perform such alignments, and based on these analyses, we describe experiments for creating them automatically.", "labels": [], "entities": []}, {"text": "Our model for the alignment task is based on an extension of the standard hidden Markov model and learns to create alignments in a completely unsupervised fashion.", "labels": [], "entities": [{"text": "alignment task", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.914403885602951}]}, {"text": "We describe our model in detail and present experimental results that show that our model is able to learn to reliably identify word-and phrase-level alignments in a corpus of \ud97b\udf59document, abstract\ud97b\udf59 pairs.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The experiments we perform are on the same Ziff-Davis corpus described in the introduction.", "labels": [], "entities": []}, {"text": "In order to judge the quality of the alignments produced, we compare them against the gold-standard references annotated by the humans.", "labels": [], "entities": []}, {"text": "The standard precision and recall metrics used in information retrieval are modified slightly to deal with the sure and possible alignments created during the annotation process.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9987611770629883}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9939450621604919}, {"text": "information retrieval", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7953752875328064}]}, {"text": "Given the set S of sure alignments, the set S \u2286 P of possible alignments, and a set A of hypothesized alignments, we compute the precision as |A \u2229 P|/|A| and the recall as |A \u2229 S|/|S|.", "labels": [], "entities": [{"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9993237257003784}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9993593096733093}]}, {"text": "One problem with these definitions is that phrase-based models are fond of making phrases.", "labels": [], "entities": []}, {"text": "That is, when given an abstract containing the man and a document also containing the man, a human will align the to the and man to man.", "labels": [], "entities": []}, {"text": "However, a phrasebased model will almost always prefer to align the entire phrase the man to the man.", "labels": [], "entities": []}, {"text": "This is because it results in fewer probabilities being multiplied together.", "labels": [], "entities": []}, {"text": "To compensate for this, we define soft precision (SoftP in the tables) by counting alignments where ab is aligned to ab the same as ones in which a is aligned to a and b is aligned to b.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.7228095531463623}]}, {"text": "Note, however, that this is not the same as a aligned to ab and b aligned to b.", "labels": [], "entities": []}, {"text": "This latter alignment will, of course, incur a precision error.", "labels": [], "entities": [{"text": "precision error", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9783512651920319}]}, {"text": "The soft precision metric induces anew, soft F-Score, labeled SoftF.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.960041880607605}]}, {"text": "Often, even humans find it difficult to align function words and punctuation.", "labels": [], "entities": []}, {"text": "A list of 58 function words and punctuation marks that appeared in the corpus (henceforth called the ignore-list) was assembled.", "labels": [], "entities": []}, {"text": "We computed precision and recall scores both on all words and on all words that do not appear in the ignore-list.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994674324989319}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9992905855178833}]}, {"text": "The results, in terms of precision, recall, and F-score, are shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9997803568840027}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9997488856315613}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9994229078292847}]}, {"text": "The first three columns are when these three statistics are computed overall words.", "labels": [], "entities": []}, {"text": "The next three columns are when these statistics are only computed over words that do not appear in our ignore list of 58 stop words.", "labels": [], "entities": []}, {"text": "Under the methodology for combining the two human annotations by taking the union, either of the human scores would achieve a precision and recall of 1.0.", "labels": [], "entities": [{"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9996575117111206}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.998970627784729}]}, {"text": "To give a sense of how well humans actually perform on this task, we compare each human against the other.", "labels": [], "entities": []}, {"text": "As we can see from, none of the machine translation models is well suited to this task, achieving, at best, an F-score of 0.298.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7386420667171478}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9994970560073853}]}, {"text": "The flipped models, in which the document sentences are the source language and the abstract sentences are the target language perform significantly better (comparatively).", "labels": [], "entities": []}, {"text": "Since the MT models are not symmetric, going the bad way requires that many document words have zero fertility, which is difficult for these models to cope with.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9099244475364685}]}, {"text": "The Cut and Paste method performs significantly better, which is to be expected, since it is designed specifically for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 119, "end_pos": 132, "type": "TASK", "confidence": 0.9872235059738159}]}, {"text": "As one would expect, this method achieves higher precision than recall, though not by very much.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9991874098777771}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9989664554595947}]}, {"text": "The fact that the Cut and Paste model performs so well, compared to the MT models, which are able to learn non-identity correspondences, suggests that any successful model should be able to take advantage of both, as ours does.", "labels": [], "entities": []}, {"text": "Our methods significantly outperform both the IBM models and the Cut and Paste method, achieving a precision of 0.522 and a recall of 0.712, yielding an overall F-score of 0.606 when stop words are not considered.", "labels": [], "entities": [{"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9989192485809326}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9995261430740356}, {"text": "F-score", "start_pos": 161, "end_pos": 168, "type": "METRIC", "confidence": 0.9991875290870667}]}, {"text": "This is still below the human-against-human F-score of 0.775 (especially considering that the true human-against-human scores are 1.0), but significantly better than any of the other models.", "labels": [], "entities": [{"text": "F-score", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9807927012443542}]}, {"text": "Among the three settings of our jump table, the syntax-based model performs best, followed by the relative jump model, with the Gaussian model coming in worst (though still better than any other approach).", "labels": [], "entities": []}, {"text": "Inspecting, the fact that the Gaussian model does not perform well is not surprising; the data shown there is very non-Gaussian.", "labels": [], "entities": []}, {"text": "A double-exponential model might be a better fit, but it is unlikely that such a model will outperform the syntax based model, so we did not perform this experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Ziff-Davis corpus statistics.", "labels": [], "entities": []}, {"text": " Table 2. By these calculations,  regardless of document phrase lengths, transitioning forward between two consecutive  segments will result in jump rel (1). When transitioning from the start state p to state  r i,i , the value we use is a jump length of i. Thus, if we begin at the first word in the  document, we incur a transition probability of j1. There are no transitions into p. We  additionally remember a specific transition jump rel (\u2205) for the probability of transition- ing to a null state. It is straightforward to estimate these parameters based on the  estimations from the forward-backward algorithm. In particular, jump rel (i) is simply  the relative frequency of length i jumps, and jump rel (\u2205) is simply the count of jumps that  end in a null state to the total number of jumps. The null state remembers the position  we ended in before we jumped there, and so to jump out of a null state, we make a jump  based on this previous position. 6", "labels": [], "entities": []}, {"text": " Table 3  Ziff-Davis extract corpus statistics.", "labels": [], "entities": []}, {"text": " Table 4  Results on the Ziff-Davis corpus.", "labels": [], "entities": [{"text": "the Ziff-Davis corpus", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.6961001455783844}]}]}