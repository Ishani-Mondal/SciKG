{"title": [{"text": "ACL Lifetime Achievement Award Some Points in a Time 1", "labels": [], "entities": [{"text": "ACL Lifetime Achievement Award Some Points in a Time 1", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.5668352395296097}]}], "abstractContent": [{"text": "This article offers a personal perspective on the development of language and information processing over the last half century, focusing on the use of statistical methods.", "labels": [], "entities": []}, {"text": "Introduced, with computers, in the 1950s, these have not always been highly regarded, but were revived in the 1990s.", "labels": [], "entities": []}, {"text": "They have proved effective in more ways than might have been expected, and encourage new thinking about what language and information processing involve.", "labels": [], "entities": []}, {"text": "First, to say how much I appreciate the completely unexpected honour of this award, and to thank the ACL for it.", "labels": [], "entities": [{"text": "honour", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9436627626419067}]}, {"text": "I want to look atone line, or thread, in natural language processing (NLP) research: how it began, what happened to it, what it suggests we need to investigate now.", "labels": [], "entities": [{"text": "natural language processing (NLP) research", "start_pos": 41, "end_pos": 83, "type": "TASK", "confidence": 0.7357778974941799}]}, {"text": "I shall take some papers of my own as pegs to hang the story on, but without making any claims for particular merit in these papers.", "labels": [], "entities": []}, {"text": "My first paper, ''The analogy between MT and IR,'' with Margaret Masterman and Roger Needham, was fora conference in 1958.", "labels": [], "entities": [{"text": "MT and IR", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.7536019881566366}]}, {"text": "The analogy it proclaimed was in the need fora thesaurus, that is, a semantic classification.", "labels": [], "entities": []}, {"text": "Machine translation (MT) and information retrieval (IR) are different tasks in their granularity and in the role of syntax.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8848211765289307}, {"text": "information retrieval (IR)", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.8680690467357636}]}, {"text": "But we argued that both need a means of finding common concepts behind surface words, so we at once identify text content and word senses.", "labels": [], "entities": []}, {"text": "We took Roget's Thesaurus as such a tool in our MT experiments (with punched cards), where we exploited the redundancy that text always has to select class labels, that indicate senses, for words.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9651321172714233}]}, {"text": "The essential idea is illustrated, in extremely simple form (as with all my examples) in Figure 1.", "labels": [], "entities": []}, {"text": "If we have to translate The farmer cultivates the field, where field has a range of senses including LAND and SUBJECT that may well have different target language equivalents, the fact that the general concept AGRICULTURE underlies each of farmer, cultivates, and field selects the sense LAND for field.", "labels": [], "entities": [{"text": "LAND", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9608274698257446}, {"text": "SUBJECT", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.9582120776176453}, {"text": "AGRICULTURE", "start_pos": 210, "end_pos": 221, "type": "METRIC", "confidence": 0.9768772125244141}]}, {"text": "But we found in our research that existing thesauruses, like Roget's, were not wholly satisfactory, for example through missing senses; and we wanted to build a better one, ideally automatically.", "labels": [], "entities": []}, {"text": "The natural way to do this, the obverse of the way the thesaurus would be applied once it was constructed, was by using text distribution data for words and applying statistical classification methods to these data.", "labels": [], "entities": []}, {"text": "There were of course no corpora available then, so in my thesis work I finessed getting the input data for classification by taking dictionary definitions, which often consist of close synonym sets, as showing the most primitive and minimal shared", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}