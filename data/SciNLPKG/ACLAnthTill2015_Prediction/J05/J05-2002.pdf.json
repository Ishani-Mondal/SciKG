{"title": [{"text": "A General Technique to Train Language Models on Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We show that under certain conditions, a language model can be trained on the basis of a second language model.", "labels": [], "entities": []}, {"text": "The main instance of the technique trains a finite automaton on the basis of a probabilistic context-free grammar, such that the Kullback-Leibler distance between grammar and trained automaton is provably minimal.", "labels": [], "entities": []}, {"text": "This is a substantial generalization of an existing algorithm to train an n-gram model on the basis of a probabilistic context-free grammar.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this article, the term language model is used to refer to any description that assigns probabilities to strings over a certain alphabet.", "labels": [], "entities": []}, {"text": "Language models have important applications in natural language processing, and in particular, in speech recognition systems).", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6700637141863505}, {"text": "speech recognition", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7448708117008209}]}, {"text": "Language models often consist of a symbolic description of a language, such as a finite automaton (FA) or a context-free grammar (CFG), extended by a probability assignment to, for example, the transitions of the FA or the rules of the CFG, by which we obtain a probabilistic finite automaton (PFA) or probabilistic context-free grammar (PCFG), respectively.", "labels": [], "entities": []}, {"text": "For certain applications, one may first determine the symbolic part of the automaton or grammar and in a second phase try to find reliable probability estimates for the transitions or rules.", "labels": [], "entities": []}, {"text": "The current article is involved with the second problem, that of extending FAs or CFGs to become PFAs or PCFGs.", "labels": [], "entities": []}, {"text": "We refer to this process as training.", "labels": [], "entities": []}, {"text": "Training is often done on the basis of a corpus of actual language use in a certain domain.", "labels": [], "entities": []}, {"text": "If each sentence in this corpus is annotated by a list of transitions of an FA recognizing the sentence or a parse tree fora CFG generating the sentence, then training may consist simply in relative frequency estimation.", "labels": [], "entities": []}, {"text": "This means that we estimate probabilities of transitions or rules by counting their frequencies in the corpus, relative to the frequencies of the start states of transitions or to the frequencies of the left-hand side nonterminals of rules, respectively.", "labels": [], "entities": []}, {"text": "By this estimation, the likelihood of the corpus is maximized.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9866763353347778}]}, {"text": "The technique we introduce in this article is different in that training is done on the basis not of a finite corpus, but of an input language model.", "labels": [], "entities": []}, {"text": "Our goal is to find estimations for the probabilities of transitions or rules of the input FA or CFG such that the resulting PFA or PCFG approximates the input language model as well as possible, or more specifically, such that the Kullback-Leibler (KL) distance (or relative entropy) between the input model and the trained model is minimized.", "labels": [], "entities": []}, {"text": "The input FA or CFG to be trained maybe structurally unrelated to the input language model.", "labels": [], "entities": [{"text": "CFG", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.8769013285636902}]}, {"text": "This technique has several applications.", "labels": [], "entities": []}, {"text": "One is an extension with probabilities of existing work on approximation of CFGs by means of FAs (Nederhof 2000).", "labels": [], "entities": [{"text": "FAs", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9659973382949829}]}, {"text": "The motivation for this work was that application of FAs is generally less costly than application of CFGs, which is an important benefit when the input is very large, as is often the casein, for example, speech recognition systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.71998330950737}]}, {"text": "The practical relevance of this work was limited, however, by the fact that in practice one is more interested in the probabilities of sentences than in a purely Boolean distinction between grammatical and ungrammatical sentences.", "labels": [], "entities": []}, {"text": "Several approaches were discussed by to extend this work to approximation of PCFGs by means of PFAs.", "labels": [], "entities": []}, {"text": "A first approach is to directly map rules with attached probabilities to transitions with attached probabilities.", "labels": [], "entities": []}, {"text": "Although this is computationally the easiest approach, the resulting PFA maybe a very inaccurate approximation of the probability distribution described by the input PCFG.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 166, "end_pos": 170, "type": "DATASET", "confidence": 0.9252241849899292}]}, {"text": "In particular, there maybe assignments of probabilities to the transitions of the same FA that lead to more accurate approximating language models.", "labels": [], "entities": []}, {"text": "A second approach is to train the approximating FA by means of a corpus.", "labels": [], "entities": [{"text": "approximating FA", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6527429819107056}]}, {"text": "If the input PCFG was itself obtained by training on a corpus, then we already possess training material.", "labels": [], "entities": []}, {"text": "However, this may not always be the case, and no training material maybe available.", "labels": [], "entities": []}, {"text": "Furthermore, as a determinized approximating FA maybe much larger than the input PCFG, the sparse-data problem maybe more severe for the automaton than it was for the grammar.", "labels": [], "entities": []}, {"text": "1 Hence, even if sufficient material was available to train the CFG, it may not be sufficient to accurately train the FA.", "labels": [], "entities": [{"text": "CFG", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.9775621294975281}, {"text": "FA", "start_pos": 118, "end_pos": 120, "type": "DATASET", "confidence": 0.48839250206947327}]}, {"text": "A third approach is to construct a training corpus from the PCFG by means of a (pseudo)random generator of sentences, such that sentences that are more likely according to the PCFG are generated with greater likelihood.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.933726966381073}]}, {"text": "This has been proposed by, for the special case of bigrams, extending a nonprobabilistic technique by.", "labels": [], "entities": []}, {"text": "It is not clear, however, whether this idea is feasible for training of finite-state models that are larger than bigrams.", "labels": [], "entities": []}, {"text": "The reason is that very large corpora would have to be generated in order to obtain accurate probability estimates for the PFA.", "labels": [], "entities": []}, {"text": "Note that the number of parameters of a bigram model is bounded by the square of the size of the lexicon; such abound does not exist for general PFAs.", "labels": [], "entities": []}, {"text": "The current article discusses a fourth approach.", "labels": [], "entities": []}, {"text": "In the limit, it is equivalent to the third approach above, as if an infinite corpus were constructed on which the PFA is trained, but we have found away to avoid considering sentences individually.", "labels": [], "entities": []}, {"text": "The key idea that allows us to handle an infinite set of strings generated by the PCFG is that we construct anew grammar that represents the intersection of the languages described by the input PCFG and the FA.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.9440503716468811}]}, {"text": "Within this new grammar, we can compute the expected frequencies of transitions of the FA, using a fairly standard analysis of PCFGs.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.9329007267951965}]}, {"text": "These expected frequencies then allow us to determine the assignment of probabilities to transitions of the FA that minimizes the KL distance between the PCFG and the resulting PFA.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.9235191345214844}]}, {"text": "The only requirement is that the FA to be trained be unambiguous, by which we mean that each input string can be recognized by at most one computation of the FA.", "labels": [], "entities": [{"text": "FA", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9328264594078064}]}, {"text": "The special case of n-grams has already been formulated by, realizing an idea previously envisioned by.", "labels": [], "entities": []}, {"text": "An n-gram model is here seen as a (P)FA that contains exactly one state for each possible history of then \u2212 1 previously read symbols.", "labels": [], "entities": []}, {"text": "It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by, although the way that the two techniques are formulated is rather different.", "labels": [], "entities": []}, {"text": "Also note that the FA underlying an n-gram model accepts any input string over the alphabet, which does not hold for general (unambiguous) FAs.", "labels": [], "entities": []}, {"text": "Another application of our work involves determinization and minimization of PFAs.", "labels": [], "entities": []}, {"text": "As shown by, PFAs cannot always be determinized, and no practical algorithms are known to minimize arbitrary nondeterministic (P)FAs.", "labels": [], "entities": []}, {"text": "This can be a problem when deterministic or small PFAs are required.", "labels": [], "entities": []}, {"text": "We can, however, always compute a minimal deterministic FA equivalent to an input FA.", "labels": [], "entities": []}, {"text": "The new results in this article offer away to extend this determinized FA to a PFA such that it approximates the probability distribution described by the input PFA as well as possible, in terms of the KL distance.", "labels": [], "entities": []}, {"text": "Although the proposed technique has some limitations, in particular, that the model to be trained is unambiguous, it is by no means restricted to language models based on finite automata or context-free grammars, as several other probabilistic grammatical formalisms can be treated in a similar manner.", "labels": [], "entities": []}, {"text": "The structure of this article is as follows.", "labels": [], "entities": []}, {"text": "We provide some preliminary definitions in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 discusses how the expected frequency of a rule in a PCFG can be computed.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.8308753967285156}]}, {"text": "This is an auxiliary step in the algorithms to be discussed below.", "labels": [], "entities": []}, {"text": "Section 4 defines away to combine a PFA and a PCFG into anew PCFG that extends a well-known representation of the intersection of a regular and a context-free language.", "labels": [], "entities": []}, {"text": "Thereby we merge the input model and the model to be trained into a single structure.", "labels": [], "entities": []}, {"text": "This structure is the foundation fora number of algorithms, presented in section 5, which allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1), training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an unambiguous FA on the basis of a PFA (section 5.3).", "labels": [], "entities": [{"text": "PCFG", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.9426525831222534}]}], "datasetContent": [], "tableCaptions": []}