{"title": [{"text": "Squibs and Discussions Evaluating Discourse and Dialogue Coding Schemes", "labels": [], "entities": [{"text": "Squibs and Discussions Evaluating Discourse and Dialogue Coding Schemes", "start_pos": 0, "end_pos": 71, "type": "TASK", "confidence": 0.8780326644579569}]}], "abstractContent": [{"text": "Agreement statistics play an important role in the evaluation of coding schemes for discourse and dialogue.", "labels": [], "entities": []}, {"text": "Unfortunately there is alack of understanding regarding appropriate agreement measures and how their results should be interpreted.", "labels": [], "entities": []}, {"text": "In this article we describe the role of agreement measures and argue that only chance-corrected measures that assume a common distribution of labels for all coders are suitable for measuring agreement in reliability studies.", "labels": [], "entities": []}, {"text": "We then provide recommendations for how reliability should be inferred from the results of agreement statistics.", "labels": [], "entities": [{"text": "reliability", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.9904008507728577}]}, {"text": "Since Jean Carletta (1996) exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes, there has been a general acceptance of their use within the field.", "labels": [], "entities": []}, {"text": "However, there are prevailing misunderstandings concerning agreement statistics and the meaning of reliability.", "labels": [], "entities": [{"text": "reliability", "start_pos": 99, "end_pos": 110, "type": "METRIC", "confidence": 0.9364810585975647}]}, {"text": "Investigation of new dialogue types and genres has been shown to reveal new phenomena in dialogue that are ill suited to annotation by current methods and also new annotation schemes that are qualitatively different from those commonly used in dialogue analysis.", "labels": [], "entities": [{"text": "dialogue analysis", "start_pos": 244, "end_pos": 261, "type": "TASK", "confidence": 0.8069108724594116}]}, {"text": "Previously prescribed practices for evaluating coding schemes become less applicable as annotation schemes become more sophisticated.", "labels": [], "entities": []}, {"text": "To compensate , we need a greater understanding of reliability statistics and how they should be interpreted.", "labels": [], "entities": []}, {"text": "In this article we discuss the purpose of reliability testing, address certain misunderstandings, and make recommendations regarding the way in which coding schemes should be evaluated.", "labels": [], "entities": []}, {"text": "1. Agreement, Reliability, and Coding Schemes After developing schemes for annotating discourse or dialogue, it is necessary to assess their suitability for the purpose for which they are designed.", "labels": [], "entities": []}, {"text": "Although no statistical test can determine whether any form of annotation is worthwhile or how applications will benefit from it, we at least need to show that coders are capable of performing the annotation.", "labels": [], "entities": []}, {"text": "This often means assessing reliability based on agreement between annotators applying the scheme.", "labels": [], "entities": [{"text": "reliability", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9735227227210999}]}, {"text": "Agreement measures are discussed in detail in section 2.", "labels": [], "entities": []}, {"text": "Much of the confusion regarding which agreement measures to apply and how their results should be interpreted stems from alack of understanding of what it means to", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Prevalence in coding.", "labels": [], "entities": [{"text": "Prevalence", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9336005449295044}]}]}