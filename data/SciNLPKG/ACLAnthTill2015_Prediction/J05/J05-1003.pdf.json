{"title": [{"text": "Articles Discriminative Reranking for Natural Language Parsing", "labels": [], "entities": [{"text": "Discriminative Reranking", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.7556578814983368}, {"text": "Natural Language Parsing", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.672028919061025}]}], "abstractContent": [{"text": "This article considers approaches which rerank the output of an existing probabilistic parser.", "labels": [], "entities": []}, {"text": "The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.", "labels": [], "entities": []}, {"text": "A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.", "labels": [], "entities": []}, {"text": "The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.", "labels": [], "entities": []}, {"text": "We introduce anew method for the reranking task, based on the boosting approach to ranking problems described in Freund et al.", "labels": [], "entities": []}, {"text": "We apply the boosting method to parsing the Wall Street Journal treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9799955487251282}, {"text": "Wall Street Journal treebank", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.899973914027214}]}, {"text": "The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model.", "labels": [], "entities": []}, {"text": "The new model achieved 89.75% F-measure, a 13% relative decrease in F-measure error over the baseline model's score of 88.2%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9980261921882629}, {"text": "F-measure error", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.9337197542190552}]}, {"text": "The article also introduces anew algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data.", "labels": [], "entities": []}, {"text": "Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.", "labels": [], "entities": []}, {"text": "We argue that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models.", "labels": [], "entities": []}, {"text": "Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.", "labels": [], "entities": [{"text": "natural language parsing (NLP)", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.813849687576294}, {"text": "speech recognition", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.7795505523681641}, {"text": "machine translation", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.7873551547527313}, {"text": "natural language generation", "start_pos": 243, "end_pos": 270, "type": "TASK", "confidence": 0.6373702088991801}]}], "introductionContent": [{"text": "Machine-learning approaches to natural language parsing have recently shown some success in complex domains such as news wire text.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6588850418726603}]}, {"text": "Many of these methods fall into the general category of history-based models, in which a parse tree is represented as a derivation (sequence of decisions) and the probability of the tree is then calculated as a product of decision probabilities.", "labels": [], "entities": []}, {"text": "While these approaches have many advantages, it can be awkward to encode some constraints within this framework.", "labels": [], "entities": []}, {"text": "In the ideal case, the designer of a statistical parser would be able to easily add features to the model that are believed to be useful in discriminating among candidate trees fora sentence.", "labels": [], "entities": []}, {"text": "In practice, however, adding new features to a generative or history-based model can be awkward: The derivation in the model must be altered to take the new features into account, and this can bean intricate task.", "labels": [], "entities": []}, {"text": "This article considers approaches which rerank the output of an existing probabilistic parser.", "labels": [], "entities": []}, {"text": "The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.", "labels": [], "entities": []}, {"text": "A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.", "labels": [], "entities": []}, {"text": "The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation which takes these features into account.", "labels": [], "entities": []}, {"text": "We introduce anew method for the reranking task, based on the boosting approach to ranking problems described in.", "labels": [], "entities": []}, {"text": "The algorithm can be viewed as a feature selection method, optimizing a particular loss function (the exponential loss function) that has been studied in the boosting literature.", "labels": [], "entities": []}, {"text": "We applied the boosting method to parsing the Wall Street Journal (WSJ) treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.97905033826828}, {"text": "Wall Street Journal (WSJ) treebank", "start_pos": 46, "end_pos": 80, "type": "DATASET", "confidence": 0.7094357865197318}]}, {"text": "The method combines the log-likelihood under a baseline model (that of Collins) with evidence from an additional 500,000 features over parse trees that were not included in the original model.", "labels": [], "entities": [{"text": "Collins", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9238826632499695}]}, {"text": "The baseline model achieved 88.2% F-measure on this task.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9994832277297974}]}, {"text": "The new model achieves 89.75% Fmeasure, a 13% relative decrease in F-measure error.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9979714751243591}, {"text": "F-measure error", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.9563741385936737}]}, {"text": "Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6653174161911011}, {"text": "speech recognition", "start_pos": 223, "end_pos": 241, "type": "TASK", "confidence": 0.7804312407970428}, {"text": "machine translation", "start_pos": 243, "end_pos": 262, "type": "TASK", "confidence": 0.790806770324707}, {"text": "natural language generation", "start_pos": 267, "end_pos": 294, "type": "TASK", "confidence": 0.6371763944625854}]}, {"text": "See for an application of the boosting approach to named entity recognition, and for the application of boosting techniques for ranking in the context of natural language generation.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6179265181223551}, {"text": "natural language generation", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.718724270661672}]}, {"text": "The article also introduces anew, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data.", "labels": [], "entities": []}, {"text": "Other NLP tasks are likely to have similar characteristics in terms of sparsity.", "labels": [], "entities": []}, {"text": "Experiments show an efficiency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting approach.", "labels": [], "entities": []}, {"text": "Efficiency issues are important, because the parsing task is a fairly large problem, involving around one million parse trees and over 500,000 features.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9736349582672119}, {"text": "parsing task", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.9149549305438995}]}, {"text": "The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds.", "labels": [], "entities": []}, {"text": "The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the ''naive'' implementation).", "labels": [], "entities": []}, {"text": "The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before.", "labels": [], "entities": []}, {"text": "In particular, previous work has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 155, "end_pos": 162, "type": "TASK", "confidence": 0.9691601991653442}]}, {"text": "(Log-linear models are often referred to as maximum-entropy models in the NLP literature.)", "labels": [], "entities": []}, {"text": "Similar methods have also been proposed for machine translation and language understanding in dialogue systems).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8142220079898834}, {"text": "language understanding", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7472346425056458}]}, {"text": "Previous work has drawn connections between log-linear models and boosting for classification problems.", "labels": [], "entities": []}, {"text": "One contribution of our research is to draw similar connections between the two approaches to ranking problems.", "labels": [], "entities": [{"text": "ranking problems", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.9299947321414948}]}, {"text": "We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the literature on maximum-entropy models.", "labels": [], "entities": []}, {"text": "The earlier methods for maximum-entropy feature selection methods) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.", "labels": [], "entities": []}, {"text": "In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of;; and.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.8472208380699158}]}, {"text": "The remainder of this article is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article.", "labels": [], "entities": []}, {"text": "Section 3 describes previous work) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods.", "labels": [], "entities": []}, {"text": "Section 4 describes how these approaches can be generalized to ranking problems.", "labels": [], "entities": []}, {"text": "We introduce loss functions for boosting and MRF approaches and discuss optimization methods.", "labels": [], "entities": [{"text": "MRF", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8607411980628967}]}, {"text": "We also derive the efficient algorithm for boosting in this section.", "labels": [], "entities": []}, {"text": "Section 5 gives experimental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.985566258430481}]}, {"text": "Section 6 discusses related work in more detail.", "labels": [], "entities": []}, {"text": "Finally, section 7 gives conclusions.", "labels": [], "entities": []}, {"text": "The reranking models in this article were originally introduced in.", "labels": [], "entities": []}, {"text": "In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.633995403846105}]}], "datasetContent": [{"text": "This section describes further experiments investigating various aspects of the boosting algorithm: the effect of the & and N parameters, learning curves, the choice of the S i,j weights, and efficiency issues.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Peak performance achieved for various values of &. ''Best N'' refers to the number of  rounds at which peak development set accuracy was reached. ''Best score'' indicates the  relative performance, compared to the baseline method, at the optimal value for N.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9615809321403503}]}, {"text": " Table 3  Peak performance achieved for various values of & for S i, j \u00bc Score\u00f0x i,1 \u00de \u00c0 Score\u00f0x i, j \u00de (column  labeled ''weighted'') and S i,j \u00bc 1 (column labeled ''unweighted'').", "labels": [], "entities": [{"text": "Score\u00f0x i,1 \u00de \u00c0 Score\u00f0x", "start_pos": 73, "end_pos": 96, "type": "METRIC", "confidence": 0.7189435958862305}]}]}