{"title": [{"text": "Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity", "labels": [], "entities": [{"text": "Co-occurrence Retrieval", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7200657278299332}, {"text": "Lexical Distributional Similarity", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.6871292392412821}]}], "abstractContent": [{"text": "Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing.", "labels": [], "entities": []}, {"text": "For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7115981131792068}]}, {"text": "In other applications, distribu-tional similarity is taken to bean approximation to semantic similarity.", "labels": [], "entities": []}, {"text": "However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted.", "labels": [], "entities": []}, {"text": "In this work, a flexible, parameterized framework for calculating distributional similarity is proposed.", "labels": [], "entities": []}, {"text": "Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9986968636512756}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9940531849861145}]}, {"text": "As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework.", "labels": [], "entities": []}, {"text": "In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity.", "labels": [], "entities": []}, {"text": "First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship?", "labels": [], "entities": []}, {"text": "Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity?", "labels": [], "entities": []}, {"text": "Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type?", "labels": [], "entities": []}, {"text": "Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation.", "labels": [], "entities": [{"text": "automatic thesaurus generation", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.6045774718125662}]}, {"text": "It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks.", "labels": [], "entities": []}, {"text": "This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over recent years, approaches to abroad range of natural language processing (NLP) applications have been proposed that require knowledge about the similarity of words.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.8001242280006409}]}, {"text": "The application areas in which these approaches have been proposed range from speech recognition and parse selection to information retrieval (IR) and natural language generation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7642820477485657}, {"text": "parse selection", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.8681600987911224}, {"text": "information retrieval (IR)", "start_pos": 120, "end_pos": 146, "type": "TASK", "confidence": 0.8370225548744201}, {"text": "natural language generation", "start_pos": 151, "end_pos": 178, "type": "TASK", "confidence": 0.65150714914004}]}, {"text": "For example, language models that incorporate substantial lexical knowledge play a key role in many statistical NLP techniques (e.g., in speech recognition and probabilistic parse selection).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7510270476341248}, {"text": "probabilistic parse selection", "start_pos": 160, "end_pos": 189, "type": "TASK", "confidence": 0.6397552986939748}]}, {"text": "However, they are difficult to acquire, since many plausible combinations of events are not seen in corpus data.", "labels": [], "entities": []}, {"text": "report that one can expect 14.7% of the word triples in any new English text to be unseen in a training corpus of 366 million English words.", "labels": [], "entities": []}, {"text": "In our own experiments with grammatical relation data extracted by a Robust Accurate Statistical Parser (RASP) from the British National Corpus (BNC), we found that 14% of noun-verb direct-object co-occurrence tokens and 49% of noun-verb direct-object co-occurrence types in one half of the data set were not seen in the other half.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 120, "end_pos": 149, "type": "DATASET", "confidence": 0.9552908738454183}]}, {"text": "A statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible.", "labels": [], "entities": []}, {"text": "Similarity-based smoothing) provides an intuitively appealing approach to language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7288808524608612}]}, {"text": "In order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined.", "labels": [], "entities": []}, {"text": "For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is \"similar\" to words that do occur as the subject of growl (e.g., dog and tiger), whereas cap is not.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7916983564694723}]}, {"text": "However, what is meant when we say that cat is \"similar\" to dog?", "labels": [], "entities": []}, {"text": "Are we referring to their semantic similarity, e.g., the components of meaning they share by virtue of both being carnivorous four-legged mammals?", "labels": [], "entities": []}, {"text": "Or are we referring to their distributional similarity, e.g., in keeping with the Firthian tradition, 1 the fact that these words tend to occur as the arguments of the same verbs (e.g., eat, feed, sleep) and tend to be modified by the same adjectives (e.g., hungry and playful).", "labels": [], "entities": [{"text": "Firthian tradition", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.953005313873291}]}, {"text": "In some applications, the knowledge required is clearly semantic.", "labels": [], "entities": []}, {"text": "In IR, documents might be usefully retrieved that use synonymous terms or terms subsuming those specified in a user's query (.", "labels": [], "entities": [{"text": "IR", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.971648633480072}]}, {"text": "In natural language generation (including text simplification), possible words fora concept should be similar in meaning rather than just in syntactic or distributional behavior.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.7015482981999716}, {"text": "text simplification", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7024578005075455}]}, {"text": "In these application areas, distributional similarity can betaken to bean approximation to semantic similarity.", "labels": [], "entities": []}, {"text": "The underlying idea is based largely on the central claim of the distributional hypothesis, that is: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.", "labels": [], "entities": []}, {"text": "This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation.", "labels": [], "entities": [{"text": "automatic thesaurus generation", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.6752444704373678}]}, {"text": "There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist (see and Weeds for more discussion of this).", "labels": [], "entities": [{"text": "automatic thesaurus extraction", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7383002638816833}, {"text": "Weeds", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.9231757521629333}]}, {"text": "A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy (see and Lin et al. for work on this).", "labels": [], "entities": [{"text": "automatic thesaurus generation", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.667643258968989}]}, {"text": "Thus, one may question the benefit of automatically generating a thesaurus if one has access to large-scale manually constructed thesauri (e.g., WordNet], Roget's], the Macquarie] and Moby 2 ).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.9483150243759155}, {"text": "Macquarie", "start_pos": 169, "end_pos": 178, "type": "DATASET", "confidence": 0.89797443151474}]}, {"text": "Automatic techniques give us the opportunity to model language changeover time or across domains and genres.", "labels": [], "entities": []}, {"text": "investigate using distributional similarity methods to find predominant word senses within a corpus, making it possible to tailor an existing resource (WordNet) to specific domains.", "labels": [], "entities": []}, {"text": "For example, in the computing domain, the word worm is more likely to be used in its 'malicious computer program' sense than in its 'earthworm' sense.", "labels": [], "entities": []}, {"text": "This domain knowledge will be reflected in a thesaurus automatically generated from a computing-specific corpus, which will show increased similarity between worm and virus and reduced similarity between worm and caterpillar.", "labels": [], "entities": []}, {"text": "In other application areas, however, the requirement for \"similar\" words to be semantically related as well as distributionally related is less clear.", "labels": [], "entities": []}, {"text": "For example, in prepositional phrase attachment ambiguity resolution, it is necessary to decide whether the prepositional phrase attaches to the verb or the noun as in the examples (1) and (2).", "labels": [], "entities": [{"text": "prepositional phrase attachment ambiguity resolution", "start_pos": 16, "end_pos": 68, "type": "TASK", "confidence": 0.8129566431045532}]}], "datasetContent": [{"text": "The rest of this paper is concerned with evaluation of the proposed framework; first, by comparing it to existing distributional similarity measures, and second, by evaluating performance on two tasks.", "labels": [], "entities": []}, {"text": "Throughout our empirical work, we use one data-set and one neighbor set comparison technique, which we now discuss in advance of presenting any of our actual experiments.", "labels": [], "entities": []}, {"text": "As discussed by, evaluation is a major problem in this area of research.", "labels": [], "entities": []}, {"text": "In some areas of natural language research, evaluation can be performed against a gold standard or against human plausibility judgments.", "labels": [], "entities": []}, {"text": "The first of these approaches is taken by, who evaluate a number of different distributional similarity measures and weight functions against a gold standard thesaurus compiled from Roget's, the Macquarie thesaurus, and the Moby thesaurus.", "labels": [], "entities": [{"text": "Macquarie thesaurus", "start_pos": 195, "end_pos": 214, "type": "DATASET", "confidence": 0.9658786356449127}, {"text": "Moby thesaurus", "start_pos": 224, "end_pos": 238, "type": "DATASET", "confidence": 0.9786875545978546}]}, {"text": "However, we argue that this approach can only be considered when distributional similarity is required as an approximation to semantic similarity and that, in any case, it is not ideal since it is not clear that there is a single \"right answer\" as to which words are most distributionally similar.", "labels": [], "entities": []}, {"text": "The best measure of distributional similarity will be the one that returns the most useful neighbors in the context of a particular application and thus leads to the best performance in that application.", "labels": [], "entities": []}, {"text": "This section investigates whether the desirable characteristics of a lexical distributional similarity measure in an automatic thesaurus generation task (WordNet prediction) are the same as those in a language modeling task (pseudo-disambiguation).", "labels": [], "entities": [{"text": "automatic thesaurus generation task (WordNet prediction)", "start_pos": 117, "end_pos": 173, "type": "TASK", "confidence": 0.6917801834642887}]}, {"text": "There area number of ways to measure the distance between two nouns in the WordNet noun hierarchy (see Budanitsky fora review).", "labels": [], "entities": [{"text": "WordNet noun hierarchy", "start_pos": 75, "end_pos": 97, "type": "DATASET", "confidence": 0.8601898550987244}]}, {"text": "In previous work (Weeds and Weir 2003b), we used the WordNet-based similarity measure first proposed in and used in Lin (1998a): 2 log P(c) log (P(c 1 )) + log (P(c 2 )) where S(w) is the set of senses of the word win WordNet, sup(c) is the set of possibly indirect super-classes of concept c in WordNet, and P(c) is the probability that a randomly selected word refers to an instance of concept c (estimated over some corpus such as SemCor).", "labels": [], "entities": [{"text": "WordNet-based similarity measure", "start_pos": 53, "end_pos": 85, "type": "METRIC", "confidence": 0.6769904593626658}, {"text": "WordNet", "start_pos": 218, "end_pos": 225, "type": "DATASET", "confidence": 0.923992395401001}, {"text": "WordNet", "start_pos": 296, "end_pos": 303, "type": "DATASET", "confidence": 0.939643144607544}]}, {"text": "However, in other research (, it has been shown that the distance measure of Jiang and Conrath (1997) (referred to herein as the \"JC measure\") is a superior WordNet-based semantic similarity measure: 2 log(c) \u2212 log P(c 1 ) \u2212 log P(c 2 ) (50) In our work, we make an empirical comparison of neighbors derived using a WordNet-based measure and each of the distributional similarity measures using the technique discussed in Section 3.", "labels": [], "entities": []}, {"text": "We have carried out the same experiments using both the Lin measure and the JC measure.", "labels": [], "entities": [{"text": "JC measure", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.8506386876106262}]}, {"text": "Correlation between distributional similarity measures and the WordNet measure tends to be slightly higher when using the JC measure  (percentage increase in similarity of approximately 10%), but the relative differences between distributional similarity measures remain approximately the same.", "labels": [], "entities": [{"text": "WordNet measure", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9336154162883759}, {"text": "JC measure", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.8117115497589111}, {"text": "percentage increase in similarity", "start_pos": 135, "end_pos": 168, "type": "METRIC", "confidence": 0.8111753314733505}]}, {"text": "Here, for brevity, we present results just using the JC measure.", "labels": [], "entities": [{"text": "JC measure", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.741127222776413}]}], "tableCaptions": [{"text": " Table 3  Table of special values of \u03b2 and \u03b3.", "labels": [], "entities": []}, {"text": " Table 4  Optimized similarities between CRMs and dist \u03b1 and corresponding parameter settings.", "labels": [], "entities": []}, {"text": " Table 6  Mean optimal error rates using five-fold cross-validation (when optimizing k, \u03b3 and \u03b2).", "labels": [], "entities": []}, {"text": " Table 7  Summary of results on pseudo-disambiguation task when optimizing \u03b2, \u03b3 and k.", "labels": [], "entities": []}]}