{"title": [{"text": "The Proposition Bank: An Annotated Corpus of Semantic Roles", "labels": [], "entities": []}], "abstractContent": [{"text": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.8388053476810455}, {"text": "Penn Treebank", "start_pos": 194, "end_pos": 207, "type": "DATASET", "confidence": 0.9941386878490448}]}, {"text": "The resulting resource can bethought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.", "labels": [], "entities": []}, {"text": "We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.", "labels": [], "entities": []}, {"text": "We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.", "labels": [], "entities": [{"text": "semantic role tagging", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.6415600876013438}]}], "introductionContent": [{"text": "Robust syntactic parsers, made possible by new statistical techniques and by the availability of large, hand-annotated training corpora), have had a major impact on the field of natural language processing in recent years.", "labels": [], "entities": [{"text": "Robust syntactic parsers", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.643223355213801}, {"text": "natural language processing", "start_pos": 178, "end_pos": 205, "type": "TASK", "confidence": 0.6358312567075094}]}, {"text": "However, the syntactic analyses produced by these parsers area long way from representing the full meaning of the sentences that are parsed.", "labels": [], "entities": []}, {"text": "As a simple example, in the sentences John broke the window.", "labels": [], "entities": []}, {"text": "(2) The window broke.", "labels": [], "entities": []}, {"text": "a syntactic analysis will represent the window as the verb's direct object in the first sentence and its subject in the second but does not indicate that it plays the same underlying semantic role in both cases.", "labels": [], "entities": []}, {"text": "Note that both sentences are in the active voice and that this alternation in subject between transitive and intransitive uses of the verb does not always occur; for example, in the sentences The sergeant played taps.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Agreement  on role identification is very high (.99 under both treatments of ArgM), given the large  number of obviously irrelevant nodes. Reassuringly, kappas for the more difficult  role classification task are also high: .93 including all types of ArgM and .96 con- sidering only numbered arguments. Kappas on the combined identification and  classication decision, calculated over all nodes in the tree, are .91 including all sub- types of ArgM and .93 over numbered arguments only. Interannotator agreement  among nodes that either annotator identified as an argument was .84, including ArgMs  and .87, excluding ArgMs.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9948726296424866}, {"text": "role identification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8479191064834595}, {"text": "role classification task", "start_pos": 194, "end_pos": 218, "type": "TASK", "confidence": 0.8143203457196554}]}, {"text": " Table 3  Confusion matrix for argument labels, with ArgM labels collapsed into one category. Entries are  a fraction of total annotations; true zeros are omitted, while other entries are rounded to zero.", "labels": [], "entities": []}, {"text": " Table 4  Confusion matrix among subtypes of ArgM, defined in Table 1. Entries are fraction of all ArgM  labels. Entries are a fraction of all ArgM labels; true zeros are omitted, while other entries are  rounded to zero.", "labels": [], "entities": []}, {"text": " Table 6  Most frequent semantic roles for each syntactic position.", "labels": [], "entities": []}, {"text": " Table 7  Most frequent syntactic positions for each semantic role.", "labels": [], "entities": []}, {"text": " Table 8  Semantic roles of verbs' subjects, for the verb classes of", "labels": [], "entities": []}, {"text": " Table 9  Semantic roles for different frame sets of kick.", "labels": [], "entities": []}, {"text": " Table 12  Common values (in percentages) for parse tree path in PropBank data, using gold-standard  parses.", "labels": [], "entities": [{"text": "PropBank data", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9846588671207428}]}, {"text": " Table 11  Accuracy of semantic-role prediction (in percentages) for unknown boundaries (the system must  identify the correct constituents as arguments and give them the correct roles).", "labels": [], "entities": [{"text": "semantic-role prediction", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.7433244585990906}]}, {"text": " Table 14  Summary of results for unknown-boundary condition.", "labels": [], "entities": [{"text": "Summary", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.918789803981781}]}]}