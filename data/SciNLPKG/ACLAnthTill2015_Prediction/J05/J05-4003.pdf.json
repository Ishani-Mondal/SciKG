{"title": [{"text": "Improving Machine Translation Performance by Exploiting Non-Parallel Corpora", "labels": [], "entities": [{"text": "Improving Machine Translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.887370785077413}]}], "abstractContent": [{"text": "We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.", "labels": [], "entities": []}, {"text": "We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.", "labels": [], "entities": []}, {"text": "Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.", "labels": [], "entities": []}, {"text": "We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 112, "end_pos": 143, "type": "TASK", "confidence": 0.6554818252722422}]}, {"text": "We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9927090406417847}]}, {"text": "Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel texts-texts that are translations of each other-are an important resource in many NLP applications.", "labels": [], "entities": []}, {"text": "They provide indispensable training data for statistical machine translation and have been found useful in research on automatic lexical acquisition (, crosslanguage information retrieval (, and annotation projection.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.7563477754592896}, {"text": "automatic lexical acquisition", "start_pos": 119, "end_pos": 148, "type": "TASK", "confidence": 0.5982770919799805}, {"text": "crosslanguage information retrieval", "start_pos": 152, "end_pos": 187, "type": "TASK", "confidence": 0.6973125040531158}, {"text": "annotation projection", "start_pos": 195, "end_pos": 216, "type": "TASK", "confidence": 0.7212700545787811}]}, {"text": "Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register.", "labels": [], "entities": []}, {"text": "There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations).", "labels": [], "entities": []}, {"text": "This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.812876229484876}]}, {"text": "One way to alleviate this lack of parallel data is to exploit a much more available and diverse resource: comparable non-parallel corpora.", "labels": [], "entities": []}, {"text": "Comparable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information.", "labels": [], "entities": []}, {"text": "Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc.", "labels": [], "entities": []}, {"text": "Such texts are widely available on the Web for many language pairs and domains.", "labels": [], "entities": []}, {"text": "They often contain many sentence pairs that are fairly good translations of each other.", "labels": [], "entities": []}, {"text": "The ability to reliably identify these pairs would enable the automatic creation of large and diverse parallel corpora.", "labels": [], "entities": []}, {"text": "However, identifying good translations in comparable corpora is hard.", "labels": [], "entities": []}, {"text": "Even texts that convey the same information will exhibit great differences at the sentence level.", "labels": [], "entities": []}, {"text": "Consider the two newspaper articles in.", "labels": [], "entities": []}, {"text": "They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang.", "labels": [], "entities": [{"text": "Agence France Presse", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.9635651310284933}]}, {"text": "The lines in the figure connect sentence pairs that are approximate translations of each other.", "labels": [], "entities": []}, {"text": "Discovering these links automatically is clearly non-trivial.", "labels": [], "entities": []}, {"text": "Traditional sentence alignment algorithms () are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7427648305892944}]}, {"text": "Thus, they perform poorly on comparable, non-parallel texts.", "labels": [], "entities": []}, {"text": "What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context.", "labels": [], "entities": []}, {"text": "This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.6727957526842753}]}, {"text": "We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.", "labels": [], "entities": []}, {"text": "Using this classifier, we extract parallel sentences from very large comparable corpora of newspaper articles.", "labels": [], "entities": []}, {"text": "We demonstrate the quality of our extracted sentences by showing that adding them to the training data of an SMT system improves the system's performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.9872910380363464}]}, {"text": "We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data.", "labels": [], "entities": [{"text": "MT", "start_pos": 291, "end_pos": 293, "type": "TASK", "confidence": 0.9610052108764648}]}, {"text": "Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains.", "labels": [], "entities": [{"text": "MT training", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.8778967559337616}]}, {"text": "In such a situation, the test data is in-domain, and the training data is out-of-domain.", "labels": [], "entities": []}, {"text": "The problem is that in such conditions, translation performance is quite poor; the out-of-domain data doesn't really help the system to produce good translations.", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9668394923210144}]}, {"text": "What is needed is additional in-domain training data.", "labels": [], "entities": []}, {"text": "Our goal is to get such data from a large in-domain comparable corpus and use it to improve the performance of an out-of-domain MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9810419082641602}]}, {"text": "We work in the context of Arabic-English and Chinese-English statistical machine translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.6679949561754862}]}, {"text": "Our out-of-domain data comes from translated United Nations proceedings, and our indomain data consists of news articles.", "labels": [], "entities": []}, {"text": "In this experimental framework we have access to a variety of resources, all of which are available from the Linguistic Data Consortium: 1 r large amounts of out-of-domain parallel data; r smaller amounts of in-domain parallel data; r in-domain MT test corpora with four reference translations; and r in-domain comparable corpora: large collections of Arabic, Chinese, and English news articles from various news agencies.", "labels": [], "entities": [{"text": "Linguistic Data Consortium", "start_pos": 109, "end_pos": 135, "type": "DATASET", "confidence": 0.75327401359876}]}, {"text": "In summary, we call in-domain the domain of the test data that we wish to translate; in this article, that in-domain data consists of news articles.", "labels": [], "entities": []}, {"text": "Out-of-domain data is data that belongs to any other domain; in this article, the out-of-domain data is drawn from United Nations (UN) parliamentary proceedings.", "labels": [], "entities": []}, {"text": "We are interested in the situation that exists when we need to translate news data but only have UN data available for training.", "labels": [], "entities": []}, {"text": "The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set.", "labels": [], "entities": []}, {"text": "The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution.", "labels": [], "entities": []}, {"text": "We can train baseline systems on UN parallel data (using the data from the first bulletin the previous paragraph), extract additional news data from the large comparable corpora (the fourth bullet), accurately measure translation performance on news data against four reference translations (the third bullet), and compare the impact of the automatically extracted news data with that of similar amounts of human-translated news data (the second bullet).", "labels": [], "entities": []}, {"text": "In the next section, we give a high-level overview of our parallel sentence extraction system.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.6274466812610626}]}, {"text": "In Section 3, we describe in detail the core of the system, the parallel sen- tence classifier.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss several data extraction experiments.", "labels": [], "entities": [{"text": "data extraction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7747018337249756}]}, {"text": "In Section 5, we evaluate the extracted data by showing that adding it to out-of-domain parallel data improves the in-domain performance of an out-of-domain MT system, and in Section 6, we show that in certain cases, even larger improvements can be obtained by using bootstrapping.", "labels": [], "entities": [{"text": "MT", "start_pos": 157, "end_pos": 159, "type": "TASK", "confidence": 0.9680628180503845}]}, {"text": "In Section 7, we present examples of sentence pairs extracted by our method and discuss some of its weaknesses.", "labels": [], "entities": []}, {"text": "Before concluding, we discuss related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are two factors that influence a classifier's performance: dictionary coverage and similarity between the domains of the training and test instances.", "labels": [], "entities": [{"text": "dictionary coverage", "start_pos": 65, "end_pos": 84, "type": "METRIC", "confidence": 0.8143937587738037}, {"text": "similarity", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9854119420051575}]}, {"text": "We performed evaluation experiments to account for both these factors.", "labels": [], "entities": []}, {"text": "All our dictionaries are automatically learned from parallel data; thus, we can create dictionaries of various coverage by learning them from parallel corpora of different sizes.", "labels": [], "entities": []}, {"text": "We use five dictionaries, learned from five initial out-of-domain parallel corpora, whose sizes are 100k, 1M, 10M, 50M, and 95M tokens, as measured on the English side.", "labels": [], "entities": []}, {"text": "Since we want to use the classifier to extract sentence pairs from our in-domain comparable corpus, we test it on instances generated from an in-domain parallel corpus.", "labels": [], "entities": []}, {"text": "In order to measure the effect of the domain difference, we use two training sets: one generated from an in-domain parallel corpus and another one from an out-ofdomain parallel corpus.", "labels": [], "entities": []}, {"text": "In summary, for each language pair, we use the following corpora: r five initial out-of-domain corpora of various sizes, used for learning dictionaries; r one out-of-domain classifier training corpus; r one in-domain classifier training corpus; and r one in-domain classifier test corpus.", "labels": [], "entities": []}, {"text": "From each initial, out-of-domain corpus, we learn a dictionary.", "labels": [], "entities": []}, {"text": "We then take the classifier training and test corpora and, using the method described in the previous section, create two sets of training instances and one set of test instances.", "labels": [], "entities": []}, {"text": "We train two classifiers (one on each training set) and evaluate both of them on the test set.", "labels": [], "entities": []}, {"text": "The parallel corpora used for generating training and test instances have around 5k sentence pairs each (approximately 150k English tokens), and generate around 10k training instances (for each training set) and 8k test instances.", "labels": [], "entities": []}, {"text": "show the recall and precision of our classifiers, for both ArabicEnglish and Chinese-English.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9995039701461792}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9988044500350952}]}, {"text": "The results show that the precision of our classification process is robust with respect to dictionary coverage and training domain.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.999128520488739}]}, {"text": "Even when starting from a very small initial parallel corpus, we can build a high-precision classifier.", "labels": [], "entities": []}, {"text": "Having a good dictionary and training data from the right domain does help though, mainly with respect to recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9926654696464539}]}, {"text": "The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards \"goodlooking\" alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9974109530448914}]}, {"text": "The low recall results are partly due to the word-overlap filter (the first stage of the classification process), which discards many parallel pairs.", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9983705878257751}]}, {"text": "If we don't apply the filter before the classifier, the recall results increase by about 20% (with no loss in precision).", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9995861649513245}, {"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9992326498031616}]}, {"text": "However, the filter plays a very important role in keeping the extraction pipeline robust and efficient (as shown in, the filter discards 99% of the candidate pairs), so this loss of recall is a price worth paying.", "labels": [], "entities": [{"text": "recall", "start_pos": 183, "end_pos": 189, "type": "METRIC", "confidence": 0.9991488456726074}]}, {"text": "Classifier evaluations using different subsets of features show that most of the classifier performance comes from the general features together with the alignment features concerning the percentage and number of words that have no connection.", "labels": [], "entities": []}, {"text": "However, we expect that in real data, the differences between parallel and non-parallel pairs are less clear than in our test data (see the discussion in Section 7) and can no   We first evaluate the extracted corpora presented in Section 4.1.", "labels": [], "entities": []}, {"text": "The extraction system used to obtain each of those corpora made use of a certain initial out-of-domain parallel corpus.", "labels": [], "entities": []}, {"text": "We train a Baseline MT system on that initial corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.7922874689102173}]}, {"text": "We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus.", "labels": [], "entities": []}, {"text": "In order to compare the quality of our extracted data with that of human-translated data from the same domain, we also train an UpperBound MT system, using the initial corpus plus a corpus of in-domain, human-translated data.", "labels": [], "entities": []}, {"text": "For each initial corpus, we use the same amount of human-translated data as there is extracted data (see).", "labels": [], "entities": []}, {"text": "Thus, for each language pair and each initial parallel corpus, we compare 3 MT systems: Baseline, PlusExtracted, and UpperBound.", "labels": [], "entities": []}, {"text": "All our MT systems were trained using a variant of the alignment template model described in . Each system used two language models: a very large one, trained on 800 million English tokens, which is the same for all the systems; and a smaller one, trained only on the English side of the parallel training data for that particular system.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9822383522987366}]}, {"text": "This ensured that any differences in performance are caused only by differences in the training data.", "labels": [], "entities": []}, {"text": "The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.", "labels": [], "entities": [{"text": "news test corpus", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.7917320728302002}, {"text": "NIST 2003 MT evaluation", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.8789108246564865}]}, {"text": "Translation performance was measured using the automatic BLEU evaluation metric ( on four reference translations.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9596914052963257}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9619402885437012}]}, {"text": "show the BLEU scores obtained by our MT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9990930557250977}, {"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9415088891983032}]}, {"text": "The 95% confidence intervals of the scores computed by bootstrap resampling) are marked on the graphs; the delta value is around 1.2 for Arabic-English and 1 for Chinese-English.", "labels": [], "entities": []}, {"text": "As the results show, the automatically extracted additional training data yields significant improvements in performance over most initial training corpora for both language pairs.", "labels": [], "entities": []}, {"text": "At least for Chinese-English, the improvements are quite comparable to those produced by the human-translated data.", "labels": [], "entities": []}, {"text": "And, as can be expected, the impact of the extracted data decreases as the size of the initial corpus increases.", "labels": [], "entities": []}, {"text": "In order to check that the classifier really does something important, we performed a few experiments without it.", "labels": [], "entities": []}, {"text": "After the article selection step, we simply paired each foreign document with the best-matching English one, assumed they are parallel, sentence-aligned them with a generic sentence alignment method, and added the resulting data to the training corpus.", "labels": [], "entities": [{"text": "article selection", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8298002481460571}]}, {"text": "The resulting BLEU scores were practically the same as the baseline; thus, our classifier does indeed help to discover higher-quality parallel data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9981973767280579}]}, {"text": "The extraction experiments from the previous section are controlled experiments in which we only use limited amounts of parallel data for our extraction system.", "labels": [], "entities": []}, {"text": "In this section, we describe experiments in which the goal is to assess the applicability of our method to data that we mined from the Web.", "labels": [], "entities": []}, {"text": "We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently.", "labels": [], "entities": []}, {"text": "In order to get as many articles as possible, we used the web site's search engine to get lists of articles and their URLs, and then crawled those lists.", "labels": [], "entities": []}, {"text": "We used the AgentBuilder tool for crawling.", "labels": [], "entities": []}, {"text": "The tool can be programmed to automatically initiate searches with different parameters and to identify and extract the desired article URLs (as well as other information such as dates and titles) from the result pages.", "labels": [], "entities": []}, {"text": "shows the sources, time periods, and size of the datasets that we downloaded.", "labels": [], "entities": []}, {"text": "For the extraction experiments, we used dictionaries of high coverage, learned from all our available parallel training data.", "labels": [], "entities": [{"text": "extraction", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9631404876708984}]}, {"text": "The sizes of these training corpora, measured in number of English tokens, are as follows: r Arabic-English: 100M tokens out-of-domain data and 4.5M tokens in-domain data r Chinese-English: 150M tokens out-of-domain data and 40M tokens in-domain data We applied our extraction method on both the LDC-released Gigaword corpora and the Web-downloaded comparable corpora.", "labels": [], "entities": [{"text": "LDC-released Gigaword corpora", "start_pos": 296, "end_pos": 325, "type": "DATASET", "confidence": 0.847334643205007}]}, {"text": "For each language pair, we used the highest precision classifier from those presented in Section 3.4.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9722374677658081}]}, {"text": "In order to obtain data of higher quality, we didn't use all the sentences classified as parallel, but only those for which the probability computed by our classifier was higher than 0.70.", "labels": [], "entities": []}, {"text": "shows the amounts of extracted data, measured in number of English tokens.", "labels": [], "entities": []}, {"text": "For ArabicEnglish, we were able to extract from the Gigaword corpora much more data than in our previous experiments (see), clearly due to the better dictionary.", "labels": [], "entities": [{"text": "ArabicEnglish", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9188840985298157}]}, {"text": "For ChineseEnglish, there was no increase in the size of extracted data (although the amount from is smaller than that from, it counts only sentence pairs extracted with confidence higher than 0.70).", "labels": [], "entities": [{"text": "ChineseEnglish", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9348410367965698}]}, {"text": "In the previous section, we measured, for our training corpora, their coverage of the test set.", "labels": [], "entities": []}, {"text": "We repeated the measurements for the training data from and obtained very similar results: using the additional extracted data improves coverage, especially for longer n-grams.", "labels": [], "entities": [{"text": "coverage", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9972884654998779}]}, {"text": "To give the reader an idea of the amount of data that is funneled through our system, we show in the sizes of the data processed by each of the system's  components during extraction from the Gigaword and Web-based Chinese-English comparable corpora.", "labels": [], "entities": []}, {"text": "We use a dictionary learned from a parallel corpus on 190M English tokens and a classifier trained on instances generated from a parallel corpus of 220k English tokens.", "labels": [], "entities": []}, {"text": "We start with a comparable corpus consisting of 500k Chinese articles and 600k English articles.", "labels": [], "entities": []}, {"text": "The article selection step (Section 2.1) outputs 7.5M similar article pairs; from each article pair we generate all possible sentence pairs and obtain 2,400M pairs.", "labels": [], "entities": []}, {"text": "Of these, less than 1% (17M) pass the candidate selection stage (Section 2.2) and are presented to the ME classifier.", "labels": [], "entities": []}, {"text": "The system outputs 430k sentence pairs (9.5M English tokens) that have been classified as parallel (with probability greater than 0.7).", "labels": [], "entities": []}, {"text": "The figure also presents, in the lower part, the parameters that control the filtering at each stage.", "labels": [], "entities": []}, {"text": "r best K results: in the article selection stage (Section 2.1), for each foreign article we only consider the top K most similar English ones.", "labels": [], "entities": []}, {"text": "In our experiments, K is set to 20.", "labels": [], "entities": [{"text": "K", "start_pos": 20, "end_pos": 21, "type": "METRIC", "confidence": 0.9624482989311218}]}, {"text": "r date window: when looking for possible article pairs, we only consider English articles whose publication dates fall within a window of 5 days around the publication date of the foreign one.", "labels": [], "entities": [{"text": "r date window", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.799623429775238}]}, {"text": "r word overlap: the word-overlap filter (Section 2.2) will discard sentence pairs that have less than a certain proportion of words in common (according to the bilingual dictionary).", "labels": [], "entities": []}, {"text": "The value we use (expressed as a percentage of sentence length) is 50.", "labels": [], "entities": []}, {"text": "r length ratio: similarly, the word-overlap filter will discard pairs whose length ratio is greater than this value, which we set to 2.", "labels": [], "entities": [{"text": "r length ratio", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.68260657787323}]}, {"text": "r decision threshold: The ME classifier associates a probability with each of its predictions.", "labels": [], "entities": []}, {"text": "Values above 0.5 indicate that the classifier considers the particular sentence pair to be parallel; the higher the value, the higher the classifier's confidence.", "labels": [], "entities": []}, {"text": "Thus, in order to obtain higher precision, we can choose to define as parallel only those pairs for which the classifier probability is above a certain threshold.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9986854195594788}]}, {"text": "In the experiments from Section 4.1, we use the (default) threshold of 0.5, while in Section 4.2 we use 0.7.", "labels": [], "entities": []}, {"text": "We also measured the MT performance impact of the extracted corpora described in Section 4.2.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.932615339756012}]}, {"text": "We trained a Baseline MT system on all our available (in-domain and out-of-domain) parallel data, and a PlusExtracted system on the parallel data plus the extracted in-domain data.", "labels": [], "entities": []}, {"text": "Clearly, we have access to no UpperBound system in this case.", "labels": [], "entities": [{"text": "UpperBound", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.9399132132530212}]}, {"text": "The results are presented in the first two rows of.", "labels": [], "entities": []}, {"text": "Adding the extracted corpus lowers the score for the Arabic-English system and improves the score for the Chinese-English one; however, none of the differences are statistically significant.", "labels": [], "entities": []}, {"text": "Since the baseline systems are trained on such large amounts of data (see Section 4.2), it is not surprising that our extracted corpora have no significant impact.", "labels": [], "entities": []}, {"text": "In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data.", "labels": [], "entities": [{"text": "MT training", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.8640326261520386}]}, {"text": "The BLEU scores obtained by the systems we trained on them are presented in the third row of.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.998684823513031}]}, {"text": "For comparison purposes, the last line of the table shows the scores of systems trained on 10M English tokens of outof-domain data.", "labels": [], "entities": []}, {"text": "As can be seen, our automatically extracted corpora obtain better MT performance than out-of-domain parallel corpora of similar size.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9912837743759155}]}, {"text": "It's true that this is not a fair comparison, since the extracted corpora were obtained using all our available parallel data.", "labels": [], "entities": []}, {"text": "The numbers do show, however, that the extracted data, although it was obtained automatically, is of good value for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.756202220916748}]}], "tableCaptions": [{"text": " Table 1  The Gigaword comparable corpora.", "labels": [], "entities": [{"text": "Gigaword comparable corpora", "start_pos": 14, "end_pos": 41, "type": "DATASET", "confidence": 0.8150917291641235}]}, {"text": " Table 3  Coverage of the extracted corpora for Arabic-English.", "labels": [], "entities": []}, {"text": " Table 4  Coverage of the extracted corpora for Chinese-English.", "labels": [], "entities": []}, {"text": " Table 5  Comparable corpora downloaded from the Web.", "labels": [], "entities": []}, {"text": " Table 7  BLEU scores of the systems obtained using all available parallel data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990832805633545}]}]}