{"title": [{"text": "Comparing Knowledge Sources for Nominal Anaphora Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora and definite noun phrase coreference.", "labels": [], "entities": [{"text": "definite noun phrase coreference", "start_pos": 98, "end_pos": 130, "type": "TASK", "confidence": 0.6027029752731323}]}, {"text": "Specifically, we compare an algorithm that relies on links encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora by means of shallow lexico-semantic patterns.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9362438321113586}]}, {"text": "As corpora we use the British National Corpus (BNC), as well as the Web, which has not been previously used for this task.", "labels": [], "entities": [{"text": "British National Corpus (BNC)", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.9690937499205271}]}, {"text": "Our results show that (a) the knowledge encoded in WordNet is often insufficient, especially for anaphor-antecedent relations that exploit subjective or context-dependent knowledge; (b) for other-anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite NP coreference, the Web-based method yields results comparable to those obtained using WordNet over the whole data set and outperforms the WordNet-based method on subsets of the data set; (d) in both case studies, the BNC-based method is worse than the other methods because of data sparseness.", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 282, "end_pos": 296, "type": "TASK", "confidence": 0.685839518904686}]}, {"text": "Thus, in our studies, the Web-based method alleviated the lexical knowledge gap often encountered in anaphora resolution and handled examples with context-dependent relations between anaphor and antecedent.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8030486404895782}]}, {"text": "Because it is inexpensive and needs no hand-modeling of lexical knowledge, it is a promising knowledge source to integrate into anaphora resolution systems.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7322546392679214}]}], "introductionContent": [{"text": "Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.,, for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8094742000102997}, {"text": "accuracy.", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9987223744392395}, {"text": "accuracies", "start_pos": 122, "end_pos": 132, "type": "METRIC", "confidence": 0.9975224137306213}, {"text": "F-measure", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9979286193847656}]}, {"text": "Less attention has been paid to nominal anaphors with full lexical heads, which cover a variety of phenomena, such as coreference (Example (1)), bridging), and comparative anaphora (Examples (3-4)).", "labels": [], "entities": []}, {"text": "(1) The death of Maxwell, the British publishing magnate whose empire collapsed in ruins of fraud, and who was the magazine's publisher, gave the periodical a brief international fame.", "labels": [], "entities": []}, {"text": "(BNC) (2) [.", "labels": [], "entities": [{"text": "BNC) (2)", "start_pos": 1, "end_pos": 9, "type": "DATASET", "confidence": 0.9349724411964416}]}, {"text": "] you don't have to undo the jacket to get to the map-particularly important when it's blowing a hooley.", "labels": [], "entities": []}, {"text": "There are elasticated adjustable drawcords on the hem, waist and on the hood.", "labels": [], "entities": []}, {"text": "(BNC) (3) In addition to increasing costs as a result of greater financial exposure for members, these measures could have other, far-reaching repercussions.", "labels": [], "entities": [{"text": "BNC)", "start_pos": 1, "end_pos": 5, "type": "DATASET", "confidence": 0.5862571150064468}]}, {"text": "(4) The ordinance, in Moon Township, prohibits locating a group home for the handicapped within a mile of another such facility.", "labels": [], "entities": [{"text": "Moon Township", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.8055018782615662}]}, {"text": "(WSJ) In Example (1), the definite noun phrase (NP) the periodical corefers with the magazine.", "labels": [], "entities": []}, {"text": "In Example (2), the definite NP the hood can be felicitously used because a related entity has already been introduced by the NP the jacket, and a part-of relation between the two entities can be established.", "labels": [], "entities": []}, {"text": "Examples (3)-(4) are instances of other-anaphora.", "labels": [], "entities": []}, {"text": "Other-anaphora area subclass of comparative anaphora) in which the anaphoric NP is introduced by a lexical modifier (such as other, such, and comparative adjectives) that specifies the relationship (such as set-complement, similarity and comparison) between the entities invoked by anaphor and antecedent.", "labels": [], "entities": []}, {"text": "For other-anaphora, the modifiers other or another provide a set-complement to an entity already evoked in the discourse model.", "labels": [], "entities": []}, {"text": "In Example (3), the NP other, far-reaching repercussions refers to a set of repercussions excluding increasing costs and can be paraphrased as other (far-reaching) repercussions than (increasing) costs.", "labels": [], "entities": []}, {"text": "Similarly, in Example (4), the NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before.", "labels": [], "entities": []}, {"text": "A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads.", "labels": [], "entities": []}, {"text": "For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities.", "labels": [], "entities": []}, {"text": "Therefore, many resolution systems that handle these phenomena, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy).", "labels": [], "entities": [{"text": "WordNet lexical hierarchy", "start_pos": 151, "end_pos": 176, "type": "DATASET", "confidence": 0.9104887247085571}]}, {"text": "In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora.", "labels": [], "entities": []}, {"text": "Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and mining ontologies; domain-specific and context-dependent knowledge; different ways of encoding information; and sense ambiguity.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss an alternative to the manual construction of knowledge bases, which we call the corpus-based approach.", "labels": [], "entities": []}, {"text": "A number of researchers have suggested that knowledge bases be enhanced via (semi)automatic knowledge extraction from corpora, and such enhanced knowledge bases have also been used for anaphora resolution, specifically for bridging (.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.7199350744485855}]}, {"text": "Building on our previous work , we extend this corpus-based approach in two ways.", "labels": [], "entities": []}, {"text": "First, we suggest using the Web for anaphora resolution instead of the smallersize, but less noisy and more balanced, corpora used previously, making available a huge additional source of knowledge.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7135796248912811}]}, {"text": "Second, we do not induce a fixed lexical knowledge base from the Web but use shallow lexicosyntactic patterns and their Web frequencies for anaphora resolution on the fly.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7592875957489014}]}, {"text": "This allows us to circumvent some of the above-mentioned methodological problems that occur with any fixed ontology, whether constructed manually or automatically.", "labels": [], "entities": []}, {"text": "The core of this article consists of an empirical comparison of these different sources of lexical knowledge for the task of antecedent selection or antecedent ranking in anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.7688166201114655}]}, {"text": "We focus on two types of full NP anaphora: other-anaphora (Section 4) and definite NP coreference (Section 5).", "labels": [], "entities": [{"text": "NP coreference", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.6432443112134933}]}, {"text": "In both case studies, we compare an algorithm that relies mainly on the frequencies of lexico-syntactic patterns in corpora (both the Web and the BNC) with an algorithm that relies mainly on a fixed ontology (WordNet 1.7.1).", "labels": [], "entities": [{"text": "WordNet 1.7.1", "start_pos": 209, "end_pos": 222, "type": "DATASET", "confidence": 0.919872373342514}]}, {"text": "We specifically address the following questions: 1.", "labels": [], "entities": []}, {"text": "Can the shortcomings of using a fixed ontology that have been stipulated by previous research on definite NPs be confirmed in our coreference study?", "labels": [], "entities": []}, {"text": "Do they also hold for other-anaphora, a phenomenon less studied so far?", "labels": [], "entities": []}, {"text": "2. How does corpus-based knowledge acquisition compare to using manually constructed lexical hierarchies in antecedent selection?", "labels": [], "entities": [{"text": "corpus-based knowledge acquisition", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.6382154623667399}]}, {"text": "And is the use of the Web an improvement over using smaller, but manually controlled, corpora?", "labels": [], "entities": []}, {"text": "3. To what extent is the answer to the previous question dependent on the anaphoric phenomenon addressed?", "labels": [], "entities": []}, {"text": "In Section 6 we discuss several aspects of our findings that still need elaboration in future work.", "labels": [], "entities": []}, {"text": "Specifically, our work is purely comparative and regards the different lexical knowledge sources in isolation.", "labels": [], "entities": []}, {"text": "It remains to be seen how the results carry forward when the knowledge sources interact with other features (for example, grammatical preferences).", "labels": [], "entities": []}, {"text": "A similar issue concerns the integration of the methods into anaphoricity determination in addition to antecedent selection.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7540221810340881}]}, {"text": "Additionally, future work should explore the contribution of different knowledge sources for yet other anaphora types.", "labels": [], "entities": []}, {"text": "There is a growing body of research that uses the Web for NLP.", "labels": [], "entities": []}, {"text": "As we concentrate on anaphora resolution in this article, we refer the reader to and, as well as the December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other NLP tasks.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8193827867507935}]}, {"text": "5 As described above, in other-anaphora the entities invoked by the anaphor area set complement to the entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and antecedent are identical.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each anaphor, each algorithm selects at most one antecedent as the correct one.", "labels": [], "entities": []}, {"text": "If this antecedent provides the appropriate set complement to the anaphor (i.e., is marked in the gold standard as corrector lenient), the assignment is evaluated as correct.", "labels": [], "entities": []}, {"text": "Otherwise, it is evaluated as wrong.", "labels": [], "entities": []}, {"text": "We use the following evaluation measures: Precision is the number of correct assignments divided by the number of assignments, recall is the number of correct assignments divided by the number of anaphors, and F-measure is based on equal weighting of precision and recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.996882438659668}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9993181228637695}, {"text": "F-measure", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.9987510442733765}, {"text": "precision", "start_pos": 251, "end_pos": 260, "type": "METRIC", "confidence": 0.9989109039306641}, {"text": "recall", "start_pos": 265, "end_pos": 271, "type": "METRIC", "confidence": 0.9967114925384521}]}, {"text": "In addition, we also give the coverage of each algorithm as the number of assignments divided by the number of anaphors.", "labels": [], "entities": []}, {"text": "This last measure is included to indicate how often the algorithm has any knowledge to goon, whether corrector false.", "labels": [], "entities": []}, {"text": "For algorithms in which the coverage is 100%, precision, recall, and F-measure all coincide.", "labels": [], "entities": [{"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.983902633190155}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9997101426124573}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9994611144065857}, {"text": "F-measure", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9992426633834839}]}, {"text": "We developed two simple rule-based baseline algorithms.", "labels": [], "entities": []}, {"text": "The first, a recency-based baseline (baselineREC), always selects the antecedent candidate closest to the anaphor.", "labels": [], "entities": []}, {"text": "The second (baselineSTR) takes into account that the lemmatized head of an otheranaphor is sometimes the same as that of its antecedent, as in the pilot's claim . .", "labels": [], "entities": []}, {"text": "For each anaphor, baselineSTR string-compares its last (lemmatized) word with the last (lemmatized) word of each of its potential antecedents.", "labels": [], "entities": []}, {"text": "If the strings match, the corresponding antecedent is chosen as the correct one.", "labels": [], "entities": []}, {"text": "If several antecedents produce a match, the baseline chooses the most recent one among them.", "labels": [], "entities": []}, {"text": "If no antecedent produces a match, no antecedent is assigned.", "labels": [], "entities": []}, {"text": "We tested two variations of this baseline.", "labels": [], "entities": []}, {"text": "The algorithm baselineSTR v1 uses only the original antecedents for string matching, disregarding named-entity resolution.", "labels": [], "entities": [{"text": "string matching", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7328766286373138}]}, {"text": "If string-comparison returns no match, a back-off version (baselineSTR * v1 ) chooses the antecedent closest to the anaphor among all antecedent candidates, thereby yielding a 100% coverage.", "labels": [], "entities": []}, {"text": "The second variation (baselineSTR v2 ) uses the replacements for named entities for string matching; again a back-off version (baselineSTR * v2 ) uses a recency back-off.", "labels": [], "entities": [{"text": "string matching", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7709707617759705}]}, {"text": "This baseline performs slightly better, as now cases such as that in Example (8) (South Carolina . .", "labels": [], "entities": []}, {"text": "another state, in which South Carolina is resolved to STATE) can also be resolved.", "labels": [], "entities": [{"text": "STATE", "start_pos": 54, "end_pos": 59, "type": "TASK", "confidence": 0.5454654693603516}]}, {"text": "The results of all baselines are summarized in.", "labels": [], "entities": []}, {"text": "Results of the 100% coverage backoff algorithms are indicated by Precision * in all tables.", "labels": [], "entities": [{"text": "Precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9930665493011475}]}, {"text": "The sets of anaphors covered by the string-matching baselines baselineSTR v1 and baselineSTR v2 . For our WordNet and corpus-based algorithms we additionally deleted pronouns from the antecedent sets, since they are lexically not very informative and are also not encoded in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 106, "end_pos": 113, "type": "DATASET", "confidence": 0.9568431973457336}, {"text": "WordNet", "start_pos": 275, "end_pos": 282, "type": "DATASET", "confidence": 0.9782060384750366}]}, {"text": "This removes 49 (10.5%) of the 468 correct antecedents (see); however, we can still resolve some of the anaphors with pronoun antecedents if they also have a lenient non-pronominal antecedent, as in Example (6).", "labels": [], "entities": []}, {"text": "After pronoun deletion, the total number of antecedents in our data set is 3,875 for 408 anaphors, of which 419 are correct antecedents, 160 are lenient, and 3,296 are distractors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Distribution of antecedent NP types in the other-anaphora data set.", "labels": [], "entities": []}, {"text": " Table 2  Overview of the results for all baselines for other-anaphora.", "labels": [], "entities": []}, {"text": " Table 5  Descriptive statistics for Web scores and BNC scores for other-anaphora.", "labels": [], "entities": [{"text": "BNC scores", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9177669882774353}]}, {"text": " Table 7  Web results for other-anaphora.", "labels": [], "entities": []}, {"text": " Table 8  BNC results for other-anaphora.", "labels": [], "entities": [{"text": "BNC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.728233277797699}]}, {"text": " Table 9  Overview of the results for the best algorithms for other-anaphora.", "labels": [], "entities": []}, {"text": " Table 10  Occurrences of error types for the best other-anaphora algorithm algoWeb v4 .", "labels": [], "entities": []}, {"text": " Table 11  Distribution of antecedent NP types for definite NP anaphora.", "labels": [], "entities": []}, {"text": " Table 12  Overview of the results for all baselines for coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9502750635147095}]}, {"text": " Table 14  Overview of the results for all WordNet algorithms for coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9564507007598877}]}, {"text": " Table 15  Overview of the results for all Web algorithms for coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9648007750511169}]}, {"text": " Table 16  Overview of the results for all BNC algorithms for coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9597840905189514}]}, {"text": " Table 17  Occurrences of error types for the best coreference algorithm algoWeb v4n .", "labels": [], "entities": []}]}