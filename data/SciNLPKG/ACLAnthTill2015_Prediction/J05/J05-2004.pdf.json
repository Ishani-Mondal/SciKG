{"title": [{"text": "A Mathematical Model of Historical Semantics and the Grouping of Word Meanings into Concepts", "labels": [], "entities": [{"text": "Grouping of Word Meanings into Concepts", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.9128946165243784}]}], "abstractContent": [{"text": "A statistical analysis of polysemy in sixteen English and French dictionaries has revealed that, in each dictionary, the number of senses per word has a near-exponential distribution.", "labels": [], "entities": []}, {"text": "A probabilistic model of historical semantics is presented which explains this distribution.", "labels": [], "entities": []}, {"text": "This mathematical model also provides a means of estimating the average number of distinct concepts per word, which was found to be considerably less than the average number of senses listed per word.", "labels": [], "entities": []}, {"text": "The grouping of word senses into concepts is based on whether they could inspire the same new senses (by metaphor, metonymy, etc.), that is, their potential future rather than their history.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ambiguity is ubiquitous in natural language.", "labels": [], "entities": []}, {"text": "It is most dramatic when it concerns the parsing of a sentence in examples such as The High Court judges rape and murder suspects.", "labels": [], "entities": [{"text": "parsing of a sentence", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8601271212100983}]}, {"text": "I heard a giant swallow after seeing a horsefly.", "labels": [], "entities": []}, {"text": "La petite brise la glace.", "labels": [], "entities": []}, {"text": "('The girl breaks the mirror.'/'The little breeze chills her.')", "labels": [], "entities": []}, {"text": "(from Fuchs 1996) However, the most common form of ambiguity concerns the meanings of individual words, as in the following examples: The minister decided to leave the party.", "labels": [], "entities": []}, {"text": "The last example involves homographs (different words which happen to be spelled the same).", "labels": [], "entities": []}, {"text": "However, it should be noted that only a small percentage of word sense ambiguity is due to homography.", "labels": [], "entities": [{"text": "word sense ambiguity", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.6961880822976431}]}, {"text": "(We obtained an estimate of approximately 2% by random sampling of English and French dictionaries.)", "labels": [], "entities": []}, {"text": "Many words have gained multiple senses by metonymy or by figurative or metaphorical uses.", "labels": [], "entities": []}, {"text": "The resulting senses are sufficiently different to be considered by lexicographers as distinct concepts (e.g., political party/drinks party).", "labels": [], "entities": []}, {"text": "In information retrieval systems with natural language interfaces or in models of human language processing via networks of semantic links, a fundamental question is what should correspond to a basic semantic concept.", "labels": [], "entities": []}, {"text": "Is it a word, a word sense, or a group of word senses?", "labels": [], "entities": []}, {"text": "This article presents a stochastic model of the evolution of language which allows us to answer this question.", "labels": [], "entities": []}, {"text": "Applying the model to statistics obtained from a large number of monolingual and bilingual dictionaries provides convincing evidence that neither words nor individual word senses (as identified by lexicographers) correspond to concepts, but rather groups of word senses.", "labels": [], "entities": []}, {"text": "Our model demonstrates that each word represents, on average, about 1.3 distinct concepts.", "labels": [], "entities": []}, {"text": "This can be compared with the average 2.0 distinct senses per word listed in the dictionaries.", "labels": [], "entities": []}, {"text": "This model also allows us to propose a novel and formal definition of the word concept.", "labels": [], "entities": []}, {"text": "There are clear applications in artificial intelligence, cognitive science, lexicography, and historical linguistics).", "labels": [], "entities": []}], "datasetContent": [{"text": "We make the no-obsolescence assumption throughout this section, that is, that t = 0.", "labels": [], "entities": []}, {"text": "Knowing that u = 1/m allows us to estimate that, in French, approximately 60% of new word senses correspond to the creation of anew word and approximately 40% to the introduction of anew sense for an existing word.", "labels": [], "entities": []}, {"text": "In English the split is approximately 50-50.", "labels": [], "entities": [{"text": "split", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9479056000709534}]}, {"text": "There are, however, quite large variations (between 55% and 65% in French) depending on the dictionary consulted.", "labels": [], "entities": []}, {"text": "Variations are inevitable, since different lexicographers have different interpretations of what constitutes distinct senses of a word.", "labels": [], "entities": []}, {"text": "We conjecture that similar percentages exist for all natural languages, although there will be variations among languages depending, among other things, on the ease with which new words can be created.", "labels": [], "entities": []}, {"text": "The curves in are approximately straight lines, but all have a slight positive curvature.", "labels": [], "entities": []}, {"text": "This curvature can be explained by the fact that \u03b1 > 0.", "labels": [], "entities": []}, {"text": "Note that, under the assumption t = 0, the concept creation factor \u03b1 is simply the probability that anew sense for an existing word is sufficiently different from previous senses for it to correspond to anew concept (capable of inspiring associations different from those that could be inspired by the existing senses).", "labels": [], "entities": []}, {"text": "When \u03b1 = t = 0, it follows from the results proved in the previous section that P(s) is an exponential function.", "labels": [], "entities": []}, {"text": "For \u03b1 > 0, however, the plot of log N s against s does indeed have a positive curvature.", "labels": [], "entities": []}, {"text": "In order to evaluate visually the influence of the value of \u03b1 on the predicted values of N s , we generated the values of N s using equation for various values of \u03b1.", "labels": [], "entities": []}, {"text": "The results are plotted in (with the average number m of meanings per word set to be the same as that for the LDCE in order to provide a concrete comparison).", "labels": [], "entities": []}, {"text": "The observed values of N s (for the LDCE) coincide so closely with those predicted by our model with \u03b1 = 0.31 that the curves of observed and predicted values would be barely distinguishable if drawn in the same For each dictionary we studied, we calculated the value of \u03b1 which provided the best fit, in a least squares sense, between the observed values of N sand those calculated from the values of P(s) given by equation (6).", "labels": [], "entities": []}, {"text": "These best-fit values of \u03b1 are given in the second column of for each dictionary we examined.", "labels": [], "entities": []}, {"text": "The values of \u03b1 vary between 0.22 and 0.41 for the English dictionaries and between 0.28 and 0.47 for the French dictionaries.", "labels": [], "entities": [{"text": "\u03b1", "start_pos": 14, "end_pos": 15, "type": "METRIC", "confidence": 0.9732396006584167}]}, {"text": "Our conclusion is that, although nearly half of the words in a dictionary are ambiguous in the sense that they require more than one definition, only approximately one-third of this ambiguity corresponds to ambiguity in the underlying concept (as defined in section 4).", "labels": [], "entities": []}, {"text": "The value of the concept creation factor \u03b1 found for different dictionaries depends on the number of divisions into different senses the lexicographer chooses to list for each word.", "labels": [], "entities": []}, {"text": "We can nevertheless calculate the average number of concepts per word in a dictionary.", "labels": [], "entities": []}, {"text": "This number should be more independent of lexicographic choices.", "labels": [], "entities": []}, {"text": "also lists c, the average number of concepts per word, which is given by c = 1 + \u03b1(m \u2212 1), for each of the dictionaries studied.", "labels": [], "entities": []}, {"text": "The average number of concepts per word is not the same, even for dictionaries of the same language.", "labels": [], "entities": []}, {"text": "Variations are to be expected as a result of different lexicographical choices of which words and senses to include in the dictionary.", "labels": [], "entities": []}, {"text": "We can note, in particular, that technical terms do not have the same distribution of number of senses per word as everyday words.", "labels": [], "entities": []}, {"text": "Furthermore, many derived words do not have their own entries but are simply listed at the end of the entry for the root word.", "labels": [], "entities": []}, {"text": "For example, in the LDCE, solidly and solidness have no senses listed and were hence ignored in our study, even though solid has 15 senses in the same dictionary.", "labels": [], "entities": [{"text": "LDCE", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8867514729499817}]}, {"text": "2.26 0.22 1.28 New Shorter Oxford English Dictionary (English) 2.26 0.24 1.30 Longman Dictionary of Contemporary English (English) 2.04 0.31 1.32 Oxford Illustrated (English) 2.46 0.22 1.32 Le Robert Junior (French) 1.34 0.47 1.16 Acad\u00e9mie Fran\u00e7aise (French) 1.64 0.29 1.19 Hachette (French) 1.53 0.38 1.20 Le Petit Robert (French) 1.83 0.28 1.23 Le Grand Robert (French) 1.79 0.46 1.36 Despite these interdictionary variations, we can nevertheless conclude that the average number of concepts per word (as defined in section 4) is approximately 1.3 for English dictionaries and a little less for French dictionaries.", "labels": [], "entities": [{"text": "New Shorter Oxford English Dictionary (English) 2.26 0.24 1.30 Longman Dictionary of Contemporary English (English) 2.04 0.31 1.32 Oxford Illustrated (English) 2.46 0.22 1.32 Le Robert Junior (French) 1.34 0.47 1.16 Acad\u00e9mie Fran\u00e7aise (French) 1.64 0.29 1.19 Hachette (French) 1.53 0.38 1.20 Le Petit Robert (French) 1.83 0.28 1.23 Le Grand Robert (French)", "start_pos": 15, "end_pos": 371, "type": "DATASET", "confidence": 0.9115680983101112}]}, {"text": "As with any scientific theory, if our theory is correct, we should be able to put it to the test by means of experiment.", "labels": [], "entities": []}, {"text": "Playing the devil's advocate, we invented several experiments which, if unsuccessful, would demonstrate the invalidity of our mathematical model.", "labels": [], "entities": []}, {"text": "First, we performed a chi-square test to compare the observed values of N sand the values of N s predicted by our model (as calculated from equation).", "labels": [], "entities": []}, {"text": "For nine out of the ten dictionaries tested, the \u03c7 2 value was less than \u03c7 2 0.10 (the value which should be exceeded in only 10% of random trials).", "labels": [], "entities": []}, {"text": "In the one remaining case, \u03c7 2 was only marginally greater than \u03c7 2 0.10 . These results are consistent with the hypothesis that the difference between the observed and predicted values of N sis due to random sampling and that E s = N obs s \u2212 N pred s (for s = 1, 2, . . .) is an independent normally distributed random variable with mean zero.", "labels": [], "entities": []}, {"text": "It is interesting to note that the difference between the observed values of N sand those predicted by our model with \u03b1 = 0 (corresponding to the hypothesis that associations are with words) or \u03b1 = 1 (corresponding to the hypothesis that associations are with senses) are both statistically highly significant (at levels of 15 and 28 standard deviations, respectively, in the case of the LCDE).", "labels": [], "entities": [{"text": "LCDE", "start_pos": 388, "end_pos": 392, "type": "DATASET", "confidence": 0.9334922432899475}]}, {"text": "In order to test the validity of the stationary-state hypothesis, we simulated the generation of a dictionary using the stochastic process model described in section 5.", "labels": [], "entities": []}, {"text": "We used a random number generator to decide whether the next step should be the creation of anew word or the creation of anew sense for an existing word. is a graphical summary of one such simulation, for the particular values m = 0.6, t = 0, and \u03b1 = 0.3.", "labels": [], "entities": []}, {"text": "The values of P(1), P(2), P(3), P(4), and P(5) are plotted against the number of words generated.", "labels": [], "entities": []}, {"text": "After the generation of only 1,000 senses (which corresponds to less Values of P(1), P(2), P(3), P(4), and P(5) against the number of words generated in the simulation of the evolution of a dictionary. than 600 words), the values of P(1), P(2), P(3), P(4), and P(5) are practically constant.", "labels": [], "entities": []}, {"text": "We can deduce that a steady state has been attained long before the simulation generates a dictionary of size comparable to those studied (several tens of thousands of senses).", "labels": [], "entities": []}, {"text": "We conclude that the stationary-state hypothesis is, in fact, for dictionaries of any reasonable size, simply a mathematical consequence of our other assumptions.", "labels": [], "entities": []}, {"text": "To check the validity of our assumption that the average number of concepts corresponding to a word with s senses is 1 + \u03b1(s \u2212 1), we tested a more general linear model b + \u03b1(s \u2212 1) fora constant b.", "labels": [], "entities": [{"text": "validity", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9526234865188599}]}, {"text": "The best-fit values of b for each dictionary were all found to be between 0.98 and 1.08, thus confirming our assumption b = 1.", "labels": [], "entities": []}, {"text": "Our conclusion that there is only a negligible loss of word senses from dictionaries through obsolescence contrasts with the fact that 22% of the words in the Oxford English Dictionary (OED) are marked as obsolete.", "labels": [], "entities": [{"text": "Oxford English Dictionary (OED)", "start_pos": 159, "end_pos": 190, "type": "DATASET", "confidence": 0.8922067681948344}]}, {"text": "Nevalainen (1999) points out that many of these obsolete words were abortive attempts by pre-17th-century writers to introduce new words which simply never caught on. attributes 1,700 neologisms to Shakespeare alone.", "labels": [], "entities": []}, {"text": "Before the publication of the first monolingual English dictionaries in the early 17th century, both vocabulary and spelling were more a matter of personal taste than convention.", "labels": [], "entities": []}, {"text": "Standardization occurred only after the publication of Samuel Johnson's dictionary in the 18th century.", "labels": [], "entities": [{"text": "Standardization", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7179874777793884}]}, {"text": "We should mention in passing that the very exhaustiveness of the OED makes it completely unsuitable (in the present context) as an accurate representation of the English language, since 90% of the senses listed are unknown to the majority of educated native English speakers).", "labels": [], "entities": [{"text": "OED", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.7755157947540283}]}, {"text": "Thus our model cannot be expected to provide a faithful prediction of the evolution of the OED, since we assume that the set of word senses in a dictionary is an approximation of those available to people who create new senses for existing words.", "labels": [], "entities": []}, {"text": "Instead of attempting to list all English words ever used, most dictionaries aim simply to list a set of words that an educated person might reasonably encounter during his or her lifetime, which is more in keeping with the assumptions of our model.", "labels": [], "entities": []}, {"text": "Not surprisingly, therefore, fitting our model to values of N s obtained from the OED gave incoherent values of the parameters (\u03b1 = 1.51 when, by assumption, we should have 0 \u2264 \u03b1 \u2264 1).", "labels": [], "entities": [{"text": "OED", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8974310159683228}]}, {"text": "We obtained a similar anomalous best-fit value \u03b1 = 1.16 for Webster's Third International Dictionary, no doubt because this dictionary is again so exhaustive.", "labels": [], "entities": [{"text": "Webster's Third International Dictionary", "start_pos": 60, "end_pos": 100, "type": "DATASET", "confidence": 0.8456390142440796}]}, {"text": "It is worth going back to the counts of the number of senses per entry in specialized dictionaries, plotted in, to explain why these do not fit our model.", "labels": [], "entities": []}, {"text": "The number of translations of a French word win English slang is related to the number of synonyms of w [10], since they both concern the onomasiological question of the different ways the same concept can be expressed in a language.", "labels": [], "entities": []}, {"text": "This is the converse of the semasiological question of the development of different meanings of a given word, which is the problem our model addresses.", "labels": [], "entities": []}, {"text": "The number of meanings of abbreviations and acronyms is closely related to the question of the distribution of homographs in a language, since abbreviations and acronyms almost invariably obtain new meanings by coincidence rather than by association with existing meanings.", "labels": [], "entities": []}, {"text": "For example, the 'temperature' and 'temporary' meanings of the abbreviation temp were clearly not derived by some direct semantic association between the notions of temperature and temporary (as would be required by our model).", "labels": [], "entities": []}, {"text": "The distribution of the number of meanings of scientific and technical terms can, on the other hand, be partly explained by our model.", "labels": [], "entities": []}, {"text": "The reason that the distribution of these types of terms is so far from satisfying the near-exponential rule is simply that 75% of the terms listed in scientific and technical dictionaries are composed of at least two words.", "labels": [], "entities": []}, {"text": "When we count only single-word entries (as we did for all dictionaries in), we obtain a distribution which can be explained by our model.", "labels": [], "entities": []}, {"text": "We found that, although the average number of senses listed per word for the scientific and technical dictionary we examined was much less than for English dictionaries of everyday language (1.35 compared to 2.0), the number of concepts per word was approximately the same at 1.32.", "labels": [], "entities": []}, {"text": "In order to test the universality of the near-exponential rule, we also studied three monolingual Basque dictionaries.", "labels": [], "entities": []}, {"text": "Basque is a well-known language isolate.", "labels": [], "entities": []}, {"text": "The curves of log N s against s were again nearly straight lines with a slight positive curvature, and the values of N s predicted by our model provided a very good fit to the observed values of N s . The corresponding values of m, \u03b1, and care given in.", "labels": [], "entities": []}, {"text": "The number of concepts per word was approximately 1.2 for all three dictionaries.", "labels": [], "entities": []}, {"text": "Our model assumes that no ambiguity arises in deciding what constitutes a word.", "labels": [], "entities": []}, {"text": "However, such ambiguity is clearly present in fusional languages.", "labels": [], "entities": []}, {"text": "In this article, we have chosen the pragmatically simple definition that the words of a language can be approximated by those sequences of characters without spaces whose meanings are listed in a given dictionary.", "labels": [], "entities": []}, {"text": "Applying this definition to a German monolingual dictionary, we observed the usual near-exponential distribution in N s . The best-fit values of the parameters of our model were m = 1.20, \u03b1 = 0.80, and c = 1.16.", "labels": [], "entities": []}, {"text": "The average number of meanings per word m and the average number of concepts per word care low, no doubt because many specialized terms which are expressed by a sequence of words in other languages count, according to our definition, as a single word in German.", "labels": [], "entities": []}, {"text": "Further research is required to test our model on other languages with complex morphology.", "labels": [], "entities": []}, {"text": "Finally, we were surprised that the number of concepts per word was almost identical for the five English dictionaries tested (see).", "labels": [], "entities": []}, {"text": "However, we found that this was not always the case, since further trials on six other English dictionaries gave a larger range of values, shown in, varying from 1.23 to 1.55.", "labels": [], "entities": []}, {"text": "1.74 0.39 1.29 Nelson 1.72 0.43 1.31 Collins School 1.64 0.56 1.36 Johnson 1.55 1.00 1.55", "labels": [], "entities": [{"text": "Nelson 1.72 0.43 1.31", "start_pos": 15, "end_pos": 36, "type": "METRIC", "confidence": 0.7266542166471481}, {"text": "Collins School 1.64 0.56 1.36 Johnson 1.55 1.00 1.55", "start_pos": 37, "end_pos": 89, "type": "DATASET", "confidence": 0.9323046406110128}]}], "tableCaptions": [{"text": " Table 1  Number N s of words with s senses in samples from the 1933 and the 1993 edition of the Shorter  Oxford English Dictionary.", "labels": [], "entities": [{"text": "Shorter  Oxford English Dictionary", "start_pos": 97, "end_pos": 131, "type": "DATASET", "confidence": 0.9169656336307526}]}, {"text": " Table 2  Average number m of meanings listed per word, concept-creation factor \u03b1, and average number  c of concepts per word for various dictionaries.", "labels": [], "entities": []}, {"text": " Table 3  Average number m of meanings per word, concept-creation factor \u03b1, and average number c of  concepts per word for three monolingual Basque dictionaries.", "labels": [], "entities": []}, {"text": " Table 4  Average number m of meanings per word, concept-creation factor \u03b1, and average number c of  concepts per word for six English dictionaries.", "labels": [], "entities": []}]}