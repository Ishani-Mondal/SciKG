{"title": [{"text": "Domain Specific Speech Acts for Spoken Language Translation", "labels": [], "entities": [{"text": "Spoken Language Translation", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.872548500696818}]}], "abstractContent": [{"text": "We describe a coding scheme for machine translation of spoken task-oriented dialogue.", "labels": [], "entities": [{"text": "machine translation of spoken task-oriented dialogue", "start_pos": 32, "end_pos": 84, "type": "TASK", "confidence": 0.8399189164241155}]}, {"text": "The coding scheme covers two levels of speaker intention \u2212 domain independent speech acts and domain dependent domain actions.", "labels": [], "entities": []}, {"text": "Our database contains over 14,000 tagged sentences in English, Italian, and Ger-man.", "labels": [], "entities": []}, {"text": "We argue that domain actions, and not speech acts, are the relevant discourse unit for improving translation quality.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.8762171566486359}]}, {"text": "We also show that, although domain actions are domain specific, the approach scales up to large domains without an explosion of domain actions and can be coded with high inter-coder reliability across research sites.", "labels": [], "entities": []}, {"text": "Furthermore , although the number of domain actions is on the order often times the number of speech acts, sparseness is not a problem for the training of classi-fiers for identifying the domain action.", "labels": [], "entities": []}, {"text": "We describe our work on developing high accuracy speech act and domain action classifiers, which is the core of the source language analysis module of our NESPOLE machine translation system .", "labels": [], "entities": [{"text": "NESPOLE machine translation", "start_pos": 155, "end_pos": 182, "type": "TASK", "confidence": 0.661990096171697}]}], "introductionContent": [{"text": "The NESPOLE and C-STAR machine translation projects use an interlingua representation based on speaker intention rather than literal meaning.", "labels": [], "entities": [{"text": "C-STAR machine translation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.5112700561682383}]}, {"text": "The speaker's intention is represented as a domain independent speech act followed by domain dependent concepts.", "labels": [], "entities": []}, {"text": "We use the term domain action to refer to the combination of a speech act with domain specific concepts.", "labels": [], "entities": []}, {"text": "Examples of domain actions and speech acts are shown in. c:give-information+party \"I will be traveling with my husband and our two children ages two and eleven\" c:request-information+existence+facility \"Do they have parking available?\"", "labels": [], "entities": []}, {"text": "\"Is there someplace to go ice skating?\" c:give-information+view+information-object \"I seethe bus icon\"", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments to assess the performance of several machine-learning approaches on the DA classification tasks.", "labels": [], "entities": [{"text": "DA classification tasks", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.9459033211072286}]}, {"text": "We evaluated all of the classifiers on English and German input in the NESPOLE travel domain.", "labels": [], "entities": [{"text": "NESPOLE travel domain", "start_pos": 71, "end_pos": 92, "type": "DATASET", "confidence": 0.8551299373308817}]}, {"text": "In our first experiment, we compared the performance of the four machine learning approaches.", "labels": [], "entities": []}, {"text": "Each SDU was parsed using the argument and pseudo-argument grammars described above.", "labels": [], "entities": []}, {"text": "The feature set for the DA and SA classifiers consisted of binary features indicating the presence or absence of labels from the grammars in the parse forest for the SDU.", "labels": [], "entities": []}, {"text": "The feature set included 212 features for English and 259 features for German.", "labels": [], "entities": []}, {"text": "The concept sequence classifiers used the same feature set with the addition of the speech act.", "labels": [], "entities": []}, {"text": "In the SA classification experiment, the TiMBL classifier used the IB1 (k-NN) algorithm with 1 neighbor and gain ratio feature weighting.", "labels": [], "entities": [{"text": "SA classification", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.9340740442276001}, {"text": "IB1", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7094253897666931}]}, {"text": "The C4.5 classifier required at least one instance per branch and used node post-pruning.", "labels": [], "entities": []}, {"text": "Both the TiMBL and C4.5 classifiers used the binary features described above and produced the single best class as output.", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.8676663637161255}]}, {"text": "The SNNS classifier used a simple feed-forward network with 1 input unit for each binary feature, 1 hidden layer containing 15 units, and 1 output unit for each speech act.", "labels": [], "entities": [{"text": "SNNS classifier", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9122669100761414}]}, {"text": "The network was trained using backpropagation.", "labels": [], "entities": []}, {"text": "The order of presentation of the training examples was randomized in each epoch, and the weights were updated after each training example presentation.", "labels": [], "entities": []}, {"text": "In order to simulate the binary features used by the other classifiers as closely as possible, the Rainbow classifier used a simple unigram model whose vocabulary was the set of labels included in the binary feature set.", "labels": [], "entities": []}, {"text": "The setup for the DA classification experiment was identical except that the neural network had 50 hidden units.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.959867388010025}]}, {"text": "The setup of the classifiers for the concept sequence classification experiment was very similar.", "labels": [], "entities": [{"text": "concept sequence classification experiment", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.6982842460274696}]}, {"text": "The TiMBL and C4.5 classifiers were setup exactly as in the DA and SA experiments with one extra feature whose value was the speech act.", "labels": [], "entities": []}, {"text": "The SNNS concept sequence classifier used a similar network with 50 hidden units.", "labels": [], "entities": [{"text": "SNNS concept sequence classifier", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7983955591917038}]}, {"text": "The SA feature was represented as a set of binary input units.", "labels": [], "entities": []}, {"text": "The Rainbow classifier was setup exactly as in the DA and SA experiments.", "labels": [], "entities": [{"text": "DA and SA experiments", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.8076304197311401}]}, {"text": "The SA feature was not included.", "labels": [], "entities": [{"text": "SA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.8578482270240784}]}, {"text": "As mentioned above, both experiments used a 20-fold cross-validation setup.", "labels": [], "entities": []}, {"text": "In each fold, the TiMBL, C4.5, and Rainbow classifiers were simply trained on 19 subsets of the data and tested on the remaining set.", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.7943492531776428}]}, {"text": "The SNNS classifiers required a more complex setup to determine the number of epochs to train the neural network for each test set.", "labels": [], "entities": [{"text": "SNNS classifiers", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9121317565441132}]}, {"text": "Within each fold, a cross-validation setup was used to determine the number of training epochs.", "labels": [], "entities": []}, {"text": "Each of the 19 training subsets fora fold was used as a validation set.", "labels": [], "entities": []}, {"text": "The network was trained on the remaining 18 subsets until the accuracy on the validation set did not improve for 50 consecutive epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9995556473731995}]}, {"text": "The network was then trained on all 19 training subsets for the average number of epochs from the validation sets.", "labels": [], "entities": []}, {"text": "This process was used for all 20-folds in the SA classification experiment.", "labels": [], "entities": [{"text": "SA classification", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.8089490532875061}]}, {"text": "For the DA and concept sequence experiments, this process ran for approximately 1.5 days for each fold.", "labels": [], "entities": [{"text": "DA", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9029616117477417}]}, {"text": "Thus, this process was run for the first two folds, and the average number of epochs from those folds was used for training.", "labels": [], "entities": []}, {"text": "show the average accuracy of each learning approach on the 20-fold cross validation experiments for domain action, speech act, and concept classification respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9994402527809143}, {"text": "concept classification", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7142098844051361}]}, {"text": "For DA classification, there were no significant differences between the TiMBL, C4.5, and SNNS classifiers for English or German.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9644028842449188}, {"text": "TiMBL", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.7906324863433838}]}, {"text": "In the SA experiment, the difference between the TiMBL and C4.5 classifiers for English was not significant.", "labels": [], "entities": []}, {"text": "The SNNS classifier was significantly better than both TiMBL and C4.5 (at least p=0.0001).", "labels": [], "entities": [{"text": "SNNS classifier", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7783650457859039}, {"text": "TiMBL", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.863023579120636}]}, {"text": "For German SA classification, there were no significant differences between the TiMBL, C4.5, and SNNS classifiers.", "labels": [], "entities": [{"text": "SA classification", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.6365111023187637}, {"text": "TiMBL", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.7521586418151855}]}, {"text": "For concept sequence classification, SNNS was significantly better than TiMBL and C4.5 (at least p=0.0001) for both English and German.", "labels": [], "entities": [{"text": "concept sequence classification", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7796781261761984}, {"text": "TiMBL", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.619916558265686}]}, {"text": "For English only, TiMBL was significantly better than C4.5 (p=0.005).", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9825876355171204}]}], "tableCaptions": [{"text": " Table 1: Tagged SDUs in the Interlingua Data- base.", "labels": [], "entities": [{"text": "Interlingua Data- base", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9540837109088898}]}, {"text": " Table 2: DA component counts in NESPOLE  data.", "labels": [], "entities": [{"text": "DA component counts", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7762599984804789}, {"text": "NESPOLE  data", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8870449960231781}]}, {"text": " Table 3: DA Overlap (All languages).", "labels": [], "entities": [{"text": "DA", "start_pos": 10, "end_pos": 12, "type": "DATASET", "confidence": 0.45943063497543335}, {"text": "Overlap", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.4458782970905304}]}, {"text": " Table 5: Most frequent DAs, SAs, and CSs.", "labels": [], "entities": []}, {"text": " Table 6: Domain Action classifier accuracy.", "labels": [], "entities": [{"text": "Domain Action classifier", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7103269100189209}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8902579545974731}]}, {"text": " Table 7: Speech Act classifier accuracy.", "labels": [], "entities": [{"text": "Speech Act classifier", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5903659462928772}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8706066608428955}]}, {"text": " Table 8: Concept Sequence classifier accuracy.", "labels": [], "entities": [{"text": "Concept Sequence classifier", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6244941353797913}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9326474666595459}]}]}