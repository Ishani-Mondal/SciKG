{"title": [{"text": "A Statistical Approach to the Semantics of Verb-Particles", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off).", "labels": [], "entities": []}, {"text": "We report first on a framework for implementing and evaluating such models.", "labels": [], "entities": []}, {"text": "We then goon to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The semantic representation of multiword expressions (MWEs) has recently become the target of renewed attention, notably in the area of hand-written grammar development ().", "labels": [], "entities": [{"text": "semantic representation of multiword expressions (MWEs)", "start_pos": 4, "end_pos": 59, "type": "TASK", "confidence": 0.7941836193203926}, {"text": "hand-written grammar development", "start_pos": 136, "end_pos": 168, "type": "TASK", "confidence": 0.6508485376834869}]}, {"text": "Such items cause considerable problems for any semantically-grounded NLP application (including applications where semantic information is implicit, such as information retrieval) because their meaning is often not simply a function of the meaning of the constituent parts.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.7124208956956863}]}, {"text": "However, corpus-based or empirical NLP has shown limited interest in the problem.", "labels": [], "entities": []}, {"text": "While there has been some work on statistical approaches to the semantics of compositional compound nominals (e.g.,,), the more idiosyncratic items have been largely ignored beyond attempts at identification.", "labels": [], "entities": []}, {"text": "And yet the identification of noncompositional phrases, while valuable in itself, would by no means be the end of the matter.", "labels": [], "entities": [{"text": "identification of noncompositional phrases", "start_pos": 12, "end_pos": 54, "type": "TASK", "confidence": 0.8802780359983444}]}, {"text": "The unique challenge posed by MWEs for empirical NLP is precisely that they do not fall cleanly into the binary classes of compositional and non-compositional expressions, but populate a continuum between the two extremes.", "labels": [], "entities": []}, {"text": "Part of the reason for the lack of interest by computational linguists in the semantics of MWEs is that there is no established gold standard data from which to constructor evaluate models.", "labels": [], "entities": []}, {"text": "Evaluation to date has tended to be fairly ad hoc.", "labels": [], "entities": []}, {"text": "Another key problem is the lack of any firm empirical foundations for the notion of compositionality.", "labels": [], "entities": []}, {"text": "Given this background, this paper has two aims.", "labels": [], "entities": []}, {"text": "The first is to put the treatment of non-compositionality in corpus-based NLP on a firm empirical footing.", "labels": [], "entities": []}, {"text": "As such it describes the development of a resource for implementing and evaluating statistical models of MWE meaning, based on nonexpert human judgements.", "labels": [], "entities": [{"text": "MWE meaning", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.9318229556083679}]}, {"text": "The second is to demonstrate the usefulness of such approaches by implementing and evaluating a handful of approaches.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We outline the linguistic foundations of this research in Section 2 before describing the process of resource building in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 summarises previous work on the subject and Section 5 details our proposed models of compositionality.", "labels": [], "entities": []}, {"text": "Section 6 lays out the evaluation of those models over the gold standard data, and we conclude the paper in Section 7.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.8409416278203329}]}], "datasetContent": [{"text": "In an attempt to normalise the annotators' entailment judgements, we decided upon an experimental setup where the subject is, for each VPC type, presented with a fixed selection of sentential contexts for that VPC.", "labels": [], "entities": []}, {"text": "So as to avoid introducing any bias into the experiment through artificially-generated sentences, we chose to extract the sentences from naturallyoccurring text, namely the written component of the British National Corpus).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 198, "end_pos": 221, "type": "DATASET", "confidence": 0.9407129486401876}]}, {"text": "Extraction of the VPCs was based on the method of.", "labels": [], "entities": []}, {"text": "First, we used a POS tagger and chunker (both built using fnTBL 1.0 () to (re)tag the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9265802502632141}]}, {"text": "This allowed us to extract VPC tokens through use of: (a) the particle POS in the POS tagged output, for each instance of which we simply then look for the rightmost verb within a fixed window to the left of the particle, and (b) the particle chunk tag in the chunker output, where we similarly locate the rightmost verb associated with each particle chunk occurrence.", "labels": [], "entities": []}, {"text": "Finally, we ran a stochastic chunk-based grammar over the chunker output to extend extraction coverage to include mistagged particles and also more reliably determine the valence of the VPC.", "labels": [], "entities": []}, {"text": "The token output of these three methods was amalgamated by weighted voting.", "labels": [], "entities": []}, {"text": "The above method extracted 461 distinct VPC types occurring at least 50 times, attested in a total of 110,199 sentences.", "labels": [], "entities": []}, {"text": "After partitioning the sentence data by type, we randomly selected 5 sentences for each VPC type.", "labels": [], "entities": []}, {"text": "We then randomly selected 40 VPC types (with 5 sentences each) to use in the entailment experiment.", "labels": [], "entities": []}, {"text": "That is, all results described in this paper are over 40 VPC types.", "labels": [], "entities": []}, {"text": "Each participant was presented with 40 sets of 5 sentences, where each of the five sentences contained a particular VPC.", "labels": [], "entities": []}, {"text": "The VPC in question was indicated at the top of the screen, and they were asked two questions: (1) whether the VPC implies the verb, and (2) whether the VPC implies the particle.", "labels": [], "entities": []}, {"text": "If the VPC was roundup, e.g., the subject would be asked \"Does roundup imply round?\" and \"Does roundup imply up?\", respectively.", "labels": [], "entities": []}, {"text": "They were given the option of three responses: \"Yes\", \"No\" or \"Don't Know\".", "labels": [], "entities": []}, {"text": "Once they had indicated their answer and pressed next, they advanced to the next VPC and set of 5 sentences.", "labels": [], "entities": []}, {"text": "They were unable to move on until a choice had been indicated.", "labels": [], "entities": []}, {"text": "As with any corpus-based approach to lexical semantics, our study of VPCs is hampered by polysemy, e.g. carryout TRANS in the execute and transport out (from a location) senses.", "labels": [], "entities": []}, {"text": "Rather than intervene to customise example sentences to a prescribed sense, we accepted whatever composition of senses random sampling produced.", "labels": [], "entities": []}, {"text": "Participants were advised that if they felt more that one meaning was present in the set of five sentences, they should base their decision on the sense that had the greatest number of occurrences in the set.", "labels": [], "entities": []}, {"text": "The effects of polysemy were compounded by not having any reliable method for determining valence.", "labels": [], "entities": []}, {"text": "We consider that simply partitioning VPC items into intransitive and transitive usages would reduce polysemy significantly.", "labels": [], "entities": []}, {"text": "The experiment was conducted remotely over the Web, using the experimental software package WebExp ().", "labels": [], "entities": []}, {"text": "Experimental sessions lasted approximately 20 minutes and were self-paced.", "labels": [], "entities": []}, {"text": "The order in which the forty sets of sentences were presented was randomised by the software.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Participant entailment judgements", "labels": [], "entities": [{"text": "Participant entailment judgements", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8678752382596334}]}, {"text": " Table 2: Summary of judgements for all VPCs", "labels": [], "entities": [{"text": "VPCs", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.8040093779563904}]}, {"text": " Table 3: Logistic regression for Method 4", "labels": [], "entities": [{"text": "Logistic regression", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8617538809776306}]}, {"text": " Table 4: Results for the four methods over the different compositionality classification tasks", "labels": [], "entities": [{"text": "compositionality classification", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.9470840096473694}]}]}