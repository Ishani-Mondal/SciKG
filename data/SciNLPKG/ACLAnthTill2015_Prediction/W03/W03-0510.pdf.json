{"title": [{"text": "The Potential and Limitations of Automatic Sentence Extraction for Summarization", "labels": [], "entities": [{"text": "Sentence Extraction", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8335734009742737}, {"text": "Summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9310526251792908}]}], "abstractContent": [{"text": "In this paper we present an empirical study of the potential and limitation of sentence extraction in text summarization.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7422230690717697}, {"text": "text summarization", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.723797619342804}]}, {"text": "Our results show that the single document generic summariza-tion task as defined in DUC 2001 needs to be carefully refocused as reflected in the low inter human agreement at 100-word 1 (0.40 score) and high upper bound at full text 2 (0.88) summaries.", "labels": [], "entities": [{"text": "DUC 2001", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9628816545009613}]}, {"text": "For 100-word summaries, the performance upper bound, 0.65, achieved oracle extracts 3.", "labels": [], "entities": []}, {"text": "Such oracle extracts show the promise of sentence extraction algorithms; however, we first need to raise inter-human agreement to be able to achieve this performance level.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7736722528934479}]}, {"text": "We show that compression is a promising direction and that the compression ratio of summaries affects average human and system performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most automatic text summarization systems existing today are extraction systems that extract parts of original documents and output the results as summaries.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.6546269059181213}]}, {"text": "Among them, sentence extraction is by far the most We compute unigram co-occurrence score of a pair of manual summaries, one as candidate summary and the other as reference.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.8107834458351135}]}, {"text": "We compute unigram co-occurrence scores of a full text and its manual summaries of 100 words.", "labels": [], "entities": []}, {"text": "These scores are the best achievable using the unigram co-occurrence scoring metric since all possible words are contained in the full text.", "labels": [], "entities": []}, {"text": "Three manual summaries are used.", "labels": [], "entities": [{"text": "summaries", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.9422537088394165}]}, {"text": "Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100\u00b15 words. popular.", "labels": [], "entities": []}, {"text": "The majority of systems participating in the past Document Understanding Conference, a large scale summarization evaluation effort sponsored by the US government, are extraction based.", "labels": [], "entities": [{"text": "Document Understanding Conference", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.6489955385526022}]}, {"text": "Although systems based on information extraction and discourse analysis) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress inmost of the automatic text summarization systems and evaluation setup.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7314480841159821}, {"text": "discourse analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7116831541061401}, {"text": "sentence extraction", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7179063409566879}, {"text": "text summarization", "start_pos": 245, "end_pos": 263, "type": "TASK", "confidence": 0.6509119123220444}]}, {"text": "The evaluation results of the single document summarization task in indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much.", "labels": [], "entities": []}, {"text": "This leads to the belief that lead-based summaries are as good as we can get for single document summarization in the news genre, implying that the research community should invest future efforts in other areas.", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 81, "end_pos": 110, "type": "TASK", "confidence": 0.7274765968322754}]}, {"text": "In fact, a very short summary of about 10 words (headline-like) task has replaced the single document 100-word summary task in DUC 2003.", "labels": [], "entities": [{"text": "DUC 2003", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.9660288095474243}]}, {"text": "The goal of this study is to renew interest in sentence extraction-based summarization and its evaluation by estimating the performance upper bound using oracle extracts, and to highlight the importance of taking into account the compression ratio when we evaluate extracts or summaries.", "labels": [], "entities": [{"text": "sentence extraction-based summarization", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.8146099050839742}]}, {"text": "Section 2 gives an overview of DUC relevant to this study.", "labels": [], "entities": [{"text": "DUC", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.49454689025878906}]}, {"text": "Section 3 introduces a recall-based unigram cooccurrence automatic evaluation metric.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9968470931053162}]}, {"text": "Section 4 presents the experimental design.", "labels": [], "entities": []}, {"text": "Section 5 shows the empirical results.", "labels": [], "entities": []}, {"text": "Section 6 concludes this paper and discusses future directions.", "labels": [], "entities": []}, {"text": "Fully automatic single-document summarization was one of two main tasks in the 2001 Document Understanding Conference.", "labels": [], "entities": [{"text": "single-document summarization", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.4784955233335495}, {"text": "Document Understanding Conference", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.7441712220509847}]}, {"text": "Participants were required to create a generic 100-word summary.", "labels": [], "entities": []}, {"text": "There were 30 test sets in DUC 2001 and each test set contained about 10 documents.", "labels": [], "entities": [{"text": "DUC 2001", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.9555196464061737}]}, {"text": "For each document, one summary was created manually as the \u00ebideal\u00ed model summary at approximately 100 words.", "labels": [], "entities": [{"text": "\u00ebideal\u00ed model summary", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.8816453019777933}]}, {"text": "We will refer to this manual summary as H1.", "labels": [], "entities": []}, {"text": "Two other manual summaries were also created at about that length.", "labels": [], "entities": [{"text": "summaries", "start_pos": 17, "end_pos": 26, "type": "TASK", "confidence": 0.9197988510131836}]}, {"text": "We will refer to these two additional human summaries as H2 and H3.", "labels": [], "entities": []}, {"text": "In addition, baseline summaries were created automatically by taking the first n sentences up to 100 words.", "labels": [], "entities": []}, {"text": "We will refer this baseline extract as B1.", "labels": [], "entities": [{"text": "B1", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9590740203857422}]}], "datasetContent": [{"text": "As stated in the introduction, we aim to find the performance upper bound of a sentence extraction system and the effect of compression ratio on its performance.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7492245137691498}]}, {"text": "We present our experimental designs to address these questions in the following sections.", "labels": [], "entities": []}], "tableCaptions": []}