{"title": [{"text": "Bio-Medical Entity Extraction using Support Vector Machines", "labels": [], "entities": [{"text": "Bio-Medical Entity Extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8060948650042216}]}], "abstractContent": [{"text": "Support Vector Machines have achieved state of the art performance in several classification tasks.", "labels": [], "entities": []}, {"text": "In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology.", "labels": [], "entities": []}, {"text": "This illustrates the extensibility of the traditional named entity task to special domains with extensive terminologies such as those in medicine and related disciplines.", "labels": [], "entities": []}, {"text": "We illustrate SVM's capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MED-LINE.", "labels": [], "entities": [{"text": "SVM", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9320141077041626}]}, {"text": "Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests.", "labels": [], "entities": [{"text": "F-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9994663596153259}]}, {"text": "A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the rapid growth in the number of published papers in the scientific fields such as medicine there has been growing interest in the application of Information Extraction (IE), ()), to help solve some of the problems that are associated with information overload.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 152, "end_pos": 179, "type": "TASK", "confidence": 0.7925072550773621}]}, {"text": "IE can benefit the medical sciences by enabling the automatic extraction of facts related to prototypical events such as those contained in patient records or research articles regarding molecular processes and their affect on human health.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9425532221794128}, {"text": "extraction of facts related to prototypical events such as those contained in patient records or research articles regarding molecular processes and their affect on human health", "start_pos": 62, "end_pos": 239, "type": "Description", "confidence": 0.6772350783531482}]}, {"text": "These facts can then be used to populate databases, aid in searching or document summarization and a variety of tasks which require the computer to have an intelligent understanding of the contents inside a document.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.6347019374370575}]}, {"text": "Our aim here is to show a state of the art method for identifying and classifying technical terminology.", "labels": [], "entities": [{"text": "identifying and classifying technical terminology", "start_pos": 54, "end_pos": 103, "type": "TASK", "confidence": 0.6990665137767792}]}, {"text": "This task is an extension of the named entity task defined by the DARPA-sponsored Message Understanding Conferences (MUCs) and is aimed at acquiring the shallow semantic building blocks that contribute to a high level understanding of the text.", "labels": [], "entities": [{"text": "DARPA-sponsored Message Understanding Conferences (MUCs)", "start_pos": 66, "end_pos": 122, "type": "TASK", "confidence": 0.6121339116777692}]}, {"text": "Although our study here looks at shallow semantics that can be captured using IE our basic goal is to join this with deep semantic representations so that computers can obtain a full understanding of the facts in a text using logical inference and reasoning.", "labels": [], "entities": []}, {"text": "The scenario is that human experts will create taxonomies and axioms (ontologies) and by providing a small set of annotated examples, machine learning can takeover the role of instance capturing though information extraction technology.", "labels": [], "entities": [{"text": "instance capturing", "start_pos": 176, "end_pos": 194, "type": "TASK", "confidence": 0.7463518381118774}, {"text": "information extraction", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.7105018198490143}]}, {"text": "Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (, and decision trees (, and maximum entropy) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as).", "labels": [], "entities": []}, {"text": "The method we use is based on support vector machines (SVMs), a state of the art model that has achieved new levels of performance in many classification tasks.", "labels": [], "entities": []}, {"text": "In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citations omitted).", "labels": [], "entities": []}, {"text": "This paper explores the underlying SVM model and shows through detailed empirical analysis the key features and parameter settings.", "labels": [], "entities": []}, {"text": "To show the application of SVMs to term extraction in unstructured texts related to the medical sciences we are using a collection of abstracts from PubMed's MEDLINE).", "labels": [], "entities": [{"text": "term extraction", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7241013795137405}, {"text": "MEDLINE", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.48187655210494995}]}, {"text": "The MEDLINE database is an online collection of abstracts for published journal articles in biology and medicine and contains more than nine million articles.", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8860993683338165}]}, {"text": "The collection we use in our tests is a controlled subset of MEDLINE obtained using three search keywords in the domain of molecular biology.", "labels": [], "entities": []}, {"text": "From the retrieved abstracts 100 were randomly chosen for annotation by a human expert according to classes in a small top-level ontology.", "labels": [], "entities": []}, {"text": "In the remainder of this paper in Section (2) we outline the background to the task and the data set we are using; in Section (3) we described the basic advantages of SVMs and the formal model we are using as well as implementation specific issues such as the choice of feature set and report experimental results.", "labels": [], "entities": []}, {"text": "In Section (4) we provide extensive results and a discussion of four sets of experiments we conducted that show the best feature sets and parameter settings in our sample domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "Results are given as F-scores) using the CoNLL evaluation script and are defined as F = (2P R)/(P +R).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9381764531135559}, {"text": "CoNLL evaluation script", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.886743168036143}, {"text": "F = (2P R)/(P +R)", "start_pos": 84, "end_pos": 101, "type": "METRIC", "confidence": 0.8143323957920074}]}, {"text": "where P denotes Precision and R Recall.", "labels": [], "entities": [{"text": "Precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9960373640060425}, {"text": "R", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9418537020683289}, {"text": "Recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9038441181182861}]}, {"text": "P is the ratio of the number of correctly found NE chunks to the number of found NE chunks, and R is the ratio of the number of correctly found NE chunks to the number of true NE chunks.", "labels": [], "entities": [{"text": "R", "start_pos": 96, "end_pos": 97, "type": "METRIC", "confidence": 0.9664008617401123}]}, {"text": "All results are calculated using 10-fold cross validation.", "labels": [], "entities": []}, {"text": "The effect of context window size is shown along the top column of.", "labels": [], "entities": []}, {"text": "It can be seen that without exception more training data results in higher overall F-scores except at 10 percent.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9985053539276123}]}, {"text": "where the result seems to be biased by the small sample, perhaps because one abstract is partly included in the training and testing sets.", "labels": [], "entities": []}, {"text": "As we would expect larger training sets reduce the effects of data sparseness and allow more accurate models to be induced.", "labels": [], "entities": []}, {"text": "The rate of increase in improvement however is not uniform according to the feature sets that are used.", "labels": [], "entities": []}, {"text": "For surface word features and head noun features the improvement in performance is consistently increasing whereas the improvement for using orthographic and part of speech features is quite erratic.", "labels": [], "entities": []}, {"text": "This maybe an effect of the small sample of training data that we used and we could not find any consistent explanation why this occurred.", "labels": [], "entities": []}, {"text": "As we observed before, the best overall result comes from using Or hd, i.e. surface words, orthographic and head features.", "labels": [], "entities": []}, {"text": "However the total score hides the fact that three classes, i.e. SOURCE.mo, SOURCE.mu and SOURCE.ti actually perform worse when using anything but surface word forms (shown in).", "labels": [], "entities": []}, {"text": "One possible explanation for this is that all of these classes have very small numbers of samples and the effect of adding features maybe to blur the distinction between these and other more numerous classes in the model.", "labels": [], "entities": []}, {"text": "However it is interesting to note that this does not happen with the RNA class which is also very small.", "labels": [], "entities": []}, {"text": "The effects of feature sets is of major importance in modelling named entity.", "labels": [], "entities": []}, {"text": "In general we would like to identify only the necessary features that are required and to remove those that do not contribute to an increase in performance.", "labels": [], "entities": []}, {"text": "This also saves time in training and testing.", "labels": [], "entities": []}, {"text": "The results from at 100 percent.", "labels": [], "entities": []}, {"text": "training data are summarized in and clearly illustrate the value of surface word level features combined with orthographic and head noun features.", "labels": [], "entities": []}, {"text": "Orthographic features allow us to capture many generalities that are not obvious at the surface word level such as IkappaB alpha and IkappaB beta both being PROTEINs and IL-10 and IL-2 both being PROTEINs.", "labels": [], "entities": []}, {"text": "The orthographic-head noun feature combination (Or hd) gives the best combined-class performance of 74.23 at 100 percent.", "labels": [], "entities": [{"text": "orthographic-head noun feature combination (Or hd)", "start_pos": 4, "end_pos": 54, "type": "METRIC", "confidence": 0.6621690690517426}]}, {"text": "training data on a -2+2 window.", "labels": [], "entities": []}, {"text": "Overall orthographic features combined with surface word features gave an improvement of between 4.9 and 22.0 percent.", "labels": [], "entities": []}, {"text": "data depending on window size over surface words alone.", "labels": [], "entities": []}, {"text": "This was the biggest contribution by any feature except the surface words.", "labels": [], "entities": []}, {"text": "Head information for example allowed us to correctly capture the fact that in the phrase NF-kappaB consensus site the whole of it is a DNA, whereas using orthographic information alone the SVM could only say that NF-kappaB was a PROTEIN and ignoring consensus site.", "labels": [], "entities": []}, {"text": "We see a similar casein the phrase primary NK cells which is correctly classified as SOURCE.ct using head noun and orthographic features but only NK cells are found using orthographic features.", "labels": [], "entities": []}, {"text": "This mistake is a natural consequence of a limited contextual view which the head noun feature helped to rectify.", "labels": [], "entities": []}, {"text": "Part of speech (POS) when combined with surface word features gave an improvement of between 7.9 and 11.7 percent.", "labels": [], "entities": []}, {"text": "at 100 percent. data.", "labels": [], "entities": []}, {"text": "The influence of POS though does not appear to be sustained when combined with other features and we found that it actually degraded performance slightly in many cases.", "labels": [], "entities": []}, {"text": "This may possibly be due to either overlapping knowledge or more likely subtle inconsistencies between POS features and say, orthographic features.", "labels": [], "entities": []}, {"text": "This could have occurred during training when the POS tagger was trained on an out of domain (news) text collection.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.7516127228736877}]}, {"text": "It is possible that if the POS tagger was trained on in-domain texts it would make a greater and more consistent contribution.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.6612691730260849}]}, {"text": "An example where orthographic features allowed correct classification but adding POS features resulted in failure is p50 in the phrase consisting of 50 (p50) -and 65 (p65) -kDa proteins.", "labels": [], "entities": []}, {"text": "Also in the phrase c-Jun transactivation domain where only c-Jun should be tagged as a protein, by using orthographic features and POS the model tags the whole phrase as a PROTEIN.", "labels": [], "entities": []}, {"text": "This is probably because POS tagging gives a NN feature value (common noun) to each word.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7334282994270325}]}, {"text": "This is very general and does not allow the model to discriminate between them.", "labels": [], "entities": []}, {"text": "The fourth feature we investigated is related to syntactic rather than lexical knowledge.", "labels": [], "entities": []}, {"text": "We felt though that there should exist a strong semantic relation between a word in a term and the head noun of that term.", "labels": [], "entities": []}, {"text": "The results in show that while the overall contribution of the Head feature is quite small, it is consistent for almost all classes.", "labels": [], "entities": [{"text": "Head", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.7403600215911865}]}], "tableCaptions": [{"text": " Table 3: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.  Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speech  features.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.977837860584259}]}, {"text": " Table 4: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.  Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speech  features.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9797332286834717}]}]}