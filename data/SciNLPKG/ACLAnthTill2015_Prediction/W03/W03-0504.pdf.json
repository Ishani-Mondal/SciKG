{"title": [{"text": "Summarization of Noisy Documents: A Pilot Study", "labels": [], "entities": [{"text": "Summarization of Noisy Documents", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9026237279176712}]}], "abstractContent": [{"text": "We investigate the problem of summarizing text documents that contain errors as a result of optical character recognition.", "labels": [], "entities": [{"text": "summarizing text documents", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.9059252142906189}, {"text": "optical character recognition", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.5710394382476807}]}, {"text": "Each stage in the process is tested, the error effects analyzed, and possible solutions suggested.", "labels": [], "entities": []}, {"text": "Our experimental results show that current approaches, which are developed to deal with clean text, suffer significant degradation even with slight increases in the noise level of a document.", "labels": [], "entities": []}, {"text": "We conclude by proposing possible ways of improving the performance of noisy document summarization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Previous work in text summarization has focused predominately on clean, well-formatted documents, i.e., documents that contain relatively few spelling and grammatical errors, such as news articles or published technical material.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.6980934143066406}]}, {"text": "In this paper, we present a pilot study of noisy document summarization, motivated primarily by the impact of various kinds of physical degradation that pages may endure before they are scanned and processed using optical character recognition (OCR) software.", "labels": [], "entities": [{"text": "noisy document summarization", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6457749009132385}, {"text": "optical character recognition (OCR)", "start_pos": 214, "end_pos": 249, "type": "TASK", "confidence": 0.7908986111481985}]}, {"text": "As more and more documents are now scanned in by OCR, an understanding of the impact of OCR on summarization is crucial and timely.", "labels": [], "entities": [{"text": "OCR", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8130955100059509}, {"text": "summarization", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.9847286343574524}]}, {"text": "The Million Book Project is one of the projects that uses OCR technology for digitizing books.", "labels": [], "entities": [{"text": "Million Book Project", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.806038479010264}]}, {"text": "Pioneered by researchers at Carnegie Mellon University, it aims to digitize a million books by 2005, by scanning the books and indexing their full text with OCR technology (http://www.archive.org/texts/millionbooks.php).", "labels": [], "entities": []}, {"text": "Understandably, summarizing documents that contain many errors is an extremely difficult task.", "labels": [], "entities": [{"text": "summarizing documents", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.9259640276432037}]}, {"text": "In our study, we focus on analyzing how the quality of summaries is affected by the level of noise in the input document, and how each stage in summarization is impacted by the noise.", "labels": [], "entities": [{"text": "summarization", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.9656298160552979}]}, {"text": "Based on our analysis, we suggest possible ways of improving the performance of automatic summarization systems for noisy documents.", "labels": [], "entities": [{"text": "summarization", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.8604166507720947}]}, {"text": "We hope to use what we have learned from this initial investigation to shed light on future directions.", "labels": [], "entities": []}, {"text": "What we ascertain from studying the problem of noisy document summarization can be useful in a number of other applications as well.", "labels": [], "entities": [{"text": "noisy document summarization", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6421447992324829}]}, {"text": "Noisy documents constitute a significant percentage of documents we encounter in everyday life.", "labels": [], "entities": []}, {"text": "The output from OCR and speech recognition (ASR) systems typically contain various degrees of errors, and even purely electronic media, such as email, are not error-free.", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.83865327835083}]}, {"text": "To summarize such documents, we need to develop techniques to deal with noise, in addition to working on the core algorithms.", "labels": [], "entities": []}, {"text": "Whether we can successfully handle noise will greatly influence the final quality of summaries of such documents.", "labels": [], "entities": []}, {"text": "Some researchers have studied problems relating to information extraction from noisy sources.", "labels": [], "entities": [{"text": "information extraction from noisy sources", "start_pos": 51, "end_pos": 92, "type": "TASK", "confidence": 0.8386012852191925}]}, {"text": "To date, this work has focused predominately on errors that arise during speech recognition, and on problems somewhat different from summarization.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.8035551011562347}, {"text": "summarization", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.9812827110290527}]}, {"text": "For example, Gotoh and Renals propose a finite state modeling approach to extract sentence boundary information from text and audio sources, using both n-gram and pause duration information).", "labels": [], "entities": []}, {"text": "They found that precision and recall of over 70% could be achieved by combining both kinds of features.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9994984865188599}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9994524121284485}]}, {"text": "Palmer and Ostendorf describe an approach for improving named entity extraction by explicitly modeling speech recognition errors through the use of statistics annotated with confidence scores).", "labels": [], "entities": [{"text": "named entity extraction", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.694915642340978}]}, {"text": "Hori and Furui summarize broadcast news speech by extracting words from automatic transcripts using a word significance measure, a confidence score, linguistic likelihood, and a word concatenation probability).", "labels": [], "entities": []}, {"text": "There has been much less work, however, in the case of noise induced by optical character recognition.", "labels": [], "entities": [{"text": "optical character recognition", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.6448241372903188}]}, {"text": "Early papers by show that moderate error rates have little impact on the effectiveness of traditional information retrieval measures (), but this conclusion does not seem to apply to the task of summarization.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.7381060123443604}, {"text": "summarization", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.9896601438522339}]}, {"text": "Miller, et al. study the performance of named entity extraction under a variety of scenarios involving both ASR and OCR output (), although speech is their primary interest.", "labels": [], "entities": [{"text": "named entity extraction", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.6336820522944132}, {"text": "ASR", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.8584544062614441}]}, {"text": "They found that by training their system on both clean and noisy input material, performance degraded linearly as a function of word error rates.", "labels": [], "entities": []}, {"text": "They also note in their paper: \"To our knowledge, no other information extraction technology has been applied to OCR material\" (pg. 322).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7838231921195984}]}, {"text": "An intriguing alternative to text-based summarization is Chen and Bloomberg's approach to creating summaries without the need for optical character recognition).", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.7873733043670654}, {"text": "optical character recognition", "start_pos": 130, "end_pos": 159, "type": "TASK", "confidence": 0.6917034387588501}]}, {"text": "Instead, they extract indicative summary sentences using purely image-based techniques and common document layout conventions.", "labels": [], "entities": []}, {"text": "While this is effective when the final summary is to be viewed onscreen by the user, the issue of optical character recognition must ultimately be faced inmost applications of interest (e.g., keyword-driven information retrieval).", "labels": [], "entities": [{"text": "optical character recognition", "start_pos": 98, "end_pos": 127, "type": "TASK", "confidence": 0.6285955309867859}, {"text": "keyword-driven information retrieval", "start_pos": 192, "end_pos": 228, "type": "TASK", "confidence": 0.6054969827334086}]}, {"text": "For the work we present in this paper, we performed a small pilot study in which we selected a set of documents and created noisy versions of them.", "labels": [], "entities": []}, {"text": "These were generated both by scanning real pages via OCR and by using a filter we have developed that injects various levels of noise into an original source document.", "labels": [], "entities": []}, {"text": "The clean and noisy documents were then piped through a summarization system.", "labels": [], "entities": []}, {"text": "We tested different modules that are often included in such systems, including sentence boundary detection, part-of-speech tagging, syntactic parsing, extraction, and editing of extracted sentences.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.7220526536305746}, {"text": "part-of-speech tagging", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.708794966340065}, {"text": "syntactic parsing", "start_pos": 132, "end_pos": 149, "type": "TASK", "confidence": 0.7067187428474426}]}, {"text": "The experimental results show that these modules suffer significant degradation as the noise level in the document increases.", "labels": [], "entities": []}, {"text": "We discuss the errors made at each stage and how they affect the quality of final summaries.", "labels": [], "entities": []}, {"text": "In Section 2, we describe our experiment, including the data creation process and various tests we performed.", "labels": [], "entities": [{"text": "data creation", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.7197359800338745}]}, {"text": "In Section 3, we analyze the results of the experiment and correlate the quality of summaries with noise levels in the input document and the errors made at different stages of the summarization process.", "labels": [], "entities": [{"text": "summarization", "start_pos": 181, "end_pos": 194, "type": "TASK", "confidence": 0.9695059061050415}]}, {"text": "We then discuss some of the challenges in summarizing noisy documents and suggest possible methods for improving the performance of noisy document summarization.", "labels": [], "entities": [{"text": "summarizing noisy documents", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.9210622310638428}]}, {"text": "We conclude with future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: OCR performance relative to ground-truth (av- erage precision and recall).", "labels": [], "entities": [{"text": "OCR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5787029266357422}, {"text": "av- erage precision", "start_pos": 52, "end_pos": 71, "type": "METRIC", "confidence": 0.7480379343032837}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9950475692749023}]}, {"text": " Table 2: Sentence boundary detection results: total num- ber of sentences detected and average words per sentence  for two tokenizers. Tokenizer 1 is decision tree based,  and tokenizer 2 is rule based.", "labels": [], "entities": [{"text": "Sentence boundary detection", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.878274401028951}]}, {"text": " Table 3: Percentage of sentences with incomplete parse  trees. Sentence boundaries were first detected using Tok- enizer 1 and Tokenizer 2.", "labels": [], "entities": []}, {"text": " Table 4: Unigram overlap, bigram overlap, and simple  cosine between extracts and human-created summaries  (the numbers in parentheses are the corresponding values  between the documents and the original text).", "labels": [], "entities": []}, {"text": " Table 5: Average trigram scores.", "labels": [], "entities": [{"text": "Average trigram scores", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8956433335940043}]}]}