{"title": [{"text": "Reuse and Challenges in Evaluating Language Generation Systems: Position Paper", "labels": [], "entities": [{"text": "Evaluating Language Generation", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.793768564860026}]}], "abstractContent": [{"text": "Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are still many NLG-specific open issues that hinder effective comparative and quantitative evaluation in this field.", "labels": [], "entities": [{"text": "evaluating Natural Language Generation (NLG)", "start_pos": 46, "end_pos": 90, "type": "TASK", "confidence": 0.7632484521184649}]}, {"text": "The paper starts off by describing a task-based, i.e., black-box evaluation of a hyper-text NLG system.", "labels": [], "entities": []}, {"text": "Then we examine the problem of glass-box, i.e., module specific , evaluation in language generation, with focus on evaluating machine learning methods for text planning.", "labels": [], "entities": [{"text": "language generation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.6989774256944656}, {"text": "text planning", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.7789480090141296}]}], "introductionContent": [{"text": "Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are still many NLG-specific open issues that hinder effective comparative and quantitative evaluation in this field.", "labels": [], "entities": [{"text": "evaluating Natural Language Generation (NLG)", "start_pos": 46, "end_pos": 90, "type": "TASK", "confidence": 0.7632484521184649}]}, {"text": "As discussed in), because of the differences between language understanding and generation, most NLU evaluation techniques 1 cannot be applied to generation.", "labels": [], "entities": []}, {"text": "The main problems come from the lack of well-defined input and output for NLG systems (see also).", "labels": [], "entities": []}, {"text": "Different systems assume different kinds of input, depending on their domains, tasks and target media, which makes comparative evaluation particularly difficult.", "labels": [], "entities": [{"text": "comparative evaluation", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.9195065200328827}]}, {"text": "It is also very hard to obtain a quantitative, objective, measure of the quality of output texts, especially across different domains and genres.", "labels": [], "entities": []}, {"text": "Therefore, NLG systems are normally evaluated with respect to their usefulness fora particular (set of) task(s), which is established by measuring user performance on these tasks, i.e., extrinsic evaluation.", "labels": [], "entities": []}, {"text": "This is often also referred to as black-box evaluation, because it does not focus on any specific module, but evaluates the system's performance as a whole.", "labels": [], "entities": []}, {"text": "This paper presents one such evaluation experiment with focus on the issue of reusing resources such as questionnaires, and task and experiment designs.", "labels": [], "entities": []}, {"text": "It then examines the problem of glass-box, i.e., module specific, evaluation in language generation, with focus on the problem of evaluating machine learning methods for text planning.", "labels": [], "entities": [{"text": "language generation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.6911680102348328}, {"text": "text planning", "start_pos": 170, "end_pos": 183, "type": "TASK", "confidence": 0.7703241109848022}]}], "datasetContent": [{"text": "Due to the fact that HYLITE+ generates hypertext which content and links are adapted to the user, it can be evaluated following strategies from two fields: NLG and adaptive hypertext.", "labels": [], "entities": []}, {"text": "After reviewing the approaches, used for evaluation of the NLG and adaptive hypertext systems most similar to ours,e.g.,,,, we discovered that they were all evaluated extrinsically by measuring human performance on a set of tasks, given different versions of the system.", "labels": [], "entities": []}, {"text": "The experiments were typically followed by an informal interview and/or questionnaire, used to gather some qualitative data, e.g., on the quality of the generated text.", "labels": [], "entities": []}, {"text": "Setting up and conducting such task-based experiments is costly and time-consuming, therefore we looked at opportunities for reusing materials and methodologies from previous evaluation experiments of similar systems from the two fields.", "labels": [], "entities": []}, {"text": "This resulted in a substantial reduction of the time and effort needed to prepare the experiments.", "labels": [], "entities": []}, {"text": "We also used the findings of some of these experiments in order to improve the design of our own evaluation.", "labels": [], "entities": []}, {"text": "For example, () used pre-generated static pages as a baseline and the study reported that the difference in the two systems' response times might have influenced some of the results.", "labels": [], "entities": []}, {"text": "Therefore, we chose instead to have both the baseline non-adaptive and the adaptive systems to generate the pages in real time, which eliminated the possible influence of the different response times.", "labels": [], "entities": []}, {"text": "The first issue that needs to be addressed when designing the extrinsic, or black-box, evaluation is to determine what are the goals of the experiment.", "labels": [], "entities": []}, {"text": "Hypermedia applications are evaluated along three aspects: interface look and feel, representation of the information structure, and application-specific information ().", "labels": [], "entities": []}, {"text": "The information structure is concerned with the hypertext network (nodes and links) and navigation aids (e.g., site maps, links to related material, index).", "labels": [], "entities": []}, {"text": "The application-specific information concerns the hypermedia content -text, images, audio and video.", "labels": [], "entities": []}, {"text": "For our system there is no need to evaluate the interface, since HYLITE+ uses simple HTML and existing Web browsers (e.g. Netscape, Internet Explorer) as rendering tools.", "labels": [], "entities": []}, {"text": "Therefore, the evaluation efforts were concentrated on the information content and navigational structure of the generated hypertext.", "labels": [], "entities": []}, {"text": "Information content was measured on the basis of: average time to complete each task; average number of pages visited per task; average number of distinct pages visited per task; percent of correctly answered questions per task; questionnaire results about content and comprehension of the generated pages; user preference for any of the systems.", "labels": [], "entities": []}, {"text": "The navigational structure was measured by the following metrics: average time per page visited; average number of pages visited; total number of pages visited; number of links followed; usage of the browser Back button; usage of the system's topic list to find information; observation and subjective opinion on orientation; subjective opinion on navigation and ease of finding information.", "labels": [], "entities": [{"text": "ease", "start_pos": 363, "end_pos": 367, "type": "METRIC", "confidence": 0.9779896140098572}]}, {"text": "Content planning, also called deep language generation, is the stage where the system needs to decide what to say, i.e., select some predicates encoding the semantics of the text to be generated, and then decide when to say them, i.e., choose an ordering of these predicates that will result in the generation of coherent discourse.", "labels": [], "entities": [{"text": "Content planning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7916336059570312}, {"text": "deep language generation", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6342761019865671}]}, {"text": "Typically content plans are created manually by NLG experts in collaboration with domain specialists, using a corpus of target texts.", "labels": [], "entities": []}, {"text": "However, this is a time consuming process, so recently researchers have started experimenting with using machine learning for content planning.", "labels": [], "entities": [{"text": "content planning", "start_pos": 126, "end_pos": 142, "type": "TASK", "confidence": 0.7555692791938782}]}, {"text": "This is the research area which we will investigate as part of building an NLG system for the e-science Grid project MIAKT 4 . The surface realisation module will be reused from HYLITE+, while the HYLITE+ content planner will be used as a baseline.", "labels": [], "entities": [{"text": "HYLITE", "start_pos": 178, "end_pos": 184, "type": "DATASET", "confidence": 0.9123004674911499}, {"text": "HYLITE+ content planner", "start_pos": 197, "end_pos": 220, "type": "DATASET", "confidence": 0.899235337972641}]}, {"text": "An integral part of the development of machine learning approaches to NLP tasks is the ability to perform automatic quantitative evaluation in order to measure differences between different configurations of the module and also allow comparative evaluation with other approaches.", "labels": [], "entities": []}, {"text": "For example, the MUC corpora and the associated scoring tool are frequently used by researchers working on machine learning for Information Extraction both as part of the development process and also as means for comparison of the performance of dif-ferent systems (see e.g.,).", "labels": [], "entities": [{"text": "MUC corpora", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.9158643782138824}, {"text": "Information Extraction", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7551924586296082}]}, {"text": "Similarly, automatic quantitative evaluation of content planners needs: an annotated corpus; an evaluation metric and a scoring tool, implementing this metric.", "labels": [], "entities": []}, {"text": "Below we will discuss each of these components and highlight the outstanding problems and challenges.", "labels": [], "entities": []}, {"text": "Research on content planning comes from two fields: document summarisation which uses some NLG techniques to generate the summaries; and natural language generation where the systems generate from some semantic representation, e.g., a domain knowledge base or numeric weather data.", "labels": [], "entities": [{"text": "content planning", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7110446393489838}, {"text": "document summarisation", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.6534985899925232}, {"text": "natural language generation", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.6687145332495371}]}, {"text": "Here we review some work from these fields that has addressed the issue of evaluation corpora.", "labels": [], "entities": []}, {"text": "Previous work on learning order constraints has used human subjects for evaluation.", "labels": [], "entities": []}, {"text": "For example, () asked humans to grade the summaries, while) manually analysed the derived constraints by comparing them to an existing text planner.", "labels": [], "entities": []}, {"text": "However, this is not sufficient if different planners or versions of the same planner are to be compared in a quantitative fashion.", "labels": [], "entities": []}, {"text": "In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed ( and they have been shown to correlate well with human judgement for quality and understandability.", "labels": [], "entities": []}, {"text": "These metrics are two kinds: using string edit distance and using tree-based metrics.", "labels": [], "entities": []}, {"text": "The string edit distance ones measure the insertion, deletion, and substitution errors between the reference sentences in the corpus and the generated ones.", "labels": [], "entities": []}, {"text": "Two different measures were evaluated and the one that treats deletions in one place and insertion in the other as a single movement error was found to be more appropriate.", "labels": [], "entities": [{"text": "insertion", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9732060432434082}]}, {"text": "In the context of content planning we intend use the string edit distance metrics by comparing the proposition sequence generated by the planner against the \"ideal\" proposition sequence from the corpus.", "labels": [], "entities": []}, {"text": "The tree-based metrics were developed to reflect the intuition that not all moves are equally bad in surface realisation.", "labels": [], "entities": []}, {"text": "Therefore these metrics use the dependency tree as a basis of calculating the string edit distances.", "labels": [], "entities": []}, {"text": "However, it is not very clear whether this type of metrics will be applicable to the content planning problem given that we do not intend to use a planner that produces a tree-like structure of the text (as do for example RST-based planners, e.g.,).", "labels": [], "entities": []}, {"text": "If the reuse experiments in MIAKT are successful, we will make our evaluation tool publically available, together with the annotated corpus and the knowledge base of predicates, which we hope will encourage other researchers to use them for development and/or comparative evaluation of content planners.", "labels": [], "entities": []}], "tableCaptions": []}