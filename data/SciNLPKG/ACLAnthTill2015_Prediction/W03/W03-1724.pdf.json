{"title": [{"text": "Integrating Ngram Model and Case-based Learning For Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 52, "end_pos": 77, "type": "TASK", "confidence": 0.5424875020980835}]}], "abstractContent": [{"text": "This paper presents our recent work for participation in the First International Chinese Word Segmentation Bake-off (ICWSB-1).", "labels": [], "entities": [{"text": "First International Chinese Word Segmentation Bake-off (ICWSB-1)", "start_pos": 61, "end_pos": 125, "type": "TASK", "confidence": 0.7344410419464111}]}, {"text": "It is based on a general-purpose ngram model for word segmen-tation and a case-based learning approach to disambiguation.", "labels": [], "entities": []}, {"text": "This system excels in identifying in-vocabulary (IV) words, achieving a recall of around 96-98%.", "labels": [], "entities": [{"text": "identifying in-vocabulary (IV) words", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.7025954077641169}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9904682040214539}]}, {"text": "Here we present our strategies for language model training and disambiguation rule learning, analyze the system's performance , and discuss areas for further improvement , e.g., out-of-vocabulary (OOV) word discovery.", "labels": [], "entities": [{"text": "disambiguation rule learning", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.665540874004364}, {"text": "out-of-vocabulary (OOV) word discovery", "start_pos": 178, "end_pos": 216, "type": "TASK", "confidence": 0.6324192186196645}]}], "introductionContent": [{"text": "After about two decades of studies of Chinese word segmentation, ICWSB-1 (henceforth, the bakeoff) is the first effort to put different approaches and systems to the test and comparison on common datasets.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6362350185712179}]}, {"text": "We participated in the bakeoff with a segmentation system that is designed to integrate a general-purpose ngram model for probabilistic segmentation and a case-or example-based learning approach () for disambiguation.", "labels": [], "entities": []}, {"text": "The ngram model, with words extracted from training corpora, is trained with the EM algorithm) using unsegmented training corpora.", "labels": [], "entities": []}, {"text": "Originally it was developed to enhance word segmentation accuracy so as to facilitate Chinese-English word alignment for our ongoing EBMT project, where only unsegmented texts are available for training.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7554898262023926}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.7456875443458557}, {"text": "Chinese-English word alignment", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.6226115226745605}]}, {"text": "It is expected to be robust enough to handle novel texts, independent of any segmented texts for training.", "labels": [], "entities": []}, {"text": "To simplify the EM training, we used the uni-gram model for the bakeoff and relied on the Viterbi algorithm for the most probable segmentation, instead of attempting to exhaust all possible segmentations of each sentence fora complicated full version of EM training.", "labels": [], "entities": [{"text": "EM training", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.9134010076522827}]}, {"text": "The case-based learning works in a straightforward way.", "labels": [], "entities": []}, {"text": "It first extracts case-based knowledge, as a set of context-dependent transformation rules, from the segmented training corpus, and then applies them to ambiguous strings in a test corpus in terms of the similarity of their contexts.", "labels": [], "entities": []}, {"text": "The similarity is empirically computed in terms of the length of relevant common affixes of context strings.", "labels": [], "entities": []}, {"text": "The effectiveness of this integrated approach is verified by its outstanding performance on IV word identification.", "labels": [], "entities": [{"text": "IV word identification", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.8493925730387369}]}, {"text": "Its IV recall rate, ranging from 96% to 98%, stands at the top or the next to the top in all closed tests in which we have participated.", "labels": [], "entities": [{"text": "IV", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9562089443206787}, {"text": "recall rate", "start_pos": 7, "end_pos": 18, "type": "METRIC", "confidence": 0.9442453682422638}]}, {"text": "Unfortunately, its overall performance is not sustainable at the same level, due to the lack of a module for OOV word detection.", "labels": [], "entities": [{"text": "OOV word detection", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7507999142011007}]}, {"text": "This paper is intended to present the implementation of the system and analyze its performance and problems, aiming at exploration of directions for further improvement.", "labels": [], "entities": []}, {"text": "The remaining sections are organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the ngram model and its training with the EM algorithm, and Section 3 presents the case-based learning for disambiguation.", "labels": [], "entities": []}, {"text": "The overall architecture of our system is given in Section 4, and its performance and problems are analyzed in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper and previews future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: System performance, in percentages (%)", "labels": [], "entities": []}]}