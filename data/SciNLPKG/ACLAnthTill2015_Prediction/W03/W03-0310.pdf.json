{"title": [], "abstractContent": [{"text": "We present two methods for the automatic creation of parallel corpora.", "labels": [], "entities": []}, {"text": "Whereas previous work into the automatic construction of parallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs.", "labels": [], "entities": []}, {"text": "First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts.", "labels": [], "entities": []}, {"text": "Retraining translation models yields modest improvements.", "labels": [], "entities": []}, {"text": "Second , we simulate the creation of training data fora language pair for which a parallel corpus is not available.", "labels": [], "entities": []}, {"text": "Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages.", "labels": [], "entities": [{"text": "German to English translation", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.6832186430692673}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9990383386611938}]}, {"text": "This suggests the method maybe useful in the creation of parallel corpora for languages with scarce resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical translation models (such as those formulated in) are trained from bilingual sentencealigned texts.", "labels": [], "entities": [{"text": "Statistical translation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6579158008098602}]}, {"text": "The bilingual data used for constructing translation models is often gathered from government documents produced in multiple languages.", "labels": [], "entities": []}, {"text": "For example, the Candide system () was trained on ten years' worth of Canadian Parliament proceedings, which consists of 2.87 million parallel sentences in French and English.", "labels": [], "entities": []}, {"text": "While the Candide system was widely regarded as successful, its success is not indicative of the potential for statistical translation between arbitrary language pairs.", "labels": [], "entities": [{"text": "statistical translation between arbitrary language pairs", "start_pos": 111, "end_pos": 167, "type": "TASK", "confidence": 0.8082267393668493}]}, {"text": "The reason for this is that collections of parallel texts as large as the Canadian Hansards are rare.", "labels": [], "entities": [{"text": "Canadian Hansards", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9181439280509949}]}, {"text": "Al- explains in simple terms the reasons that using large amounts of training data ensures translation quality: if a program sees a particular word or phrase one thousand times during training, it is more likely to learn a correct translation than if sees it ten times, or once, or never.", "labels": [], "entities": []}, {"text": "Increasing the amount of training material therefore leads to improved quality.", "labels": [], "entities": []}, {"text": "This is illustrated in, which plots translation accuracy (measured as 100 minus word error rate) for French\u21d2English, German\u21d2English, and Spanish\u21d2English translation models trained on incrementally larger parallel corpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9370495080947876}, {"text": "word error rate", "start_pos": 80, "end_pos": 95, "type": "METRIC", "confidence": 0.7981815536816915}]}, {"text": "The quality of the translations produced by each system increases over the 100,000 training items, and the graph suggests the the trend would continue if more data were added.", "labels": [], "entities": []}, {"text": "Notice that the rate of improvement is slow: after 90,000 manually provided training sentences pairs, we only see a 4-6% change in performance.", "labels": [], "entities": []}, {"text": "Sufficient performance for statistical models may therefore only come when we have access to many millions of aligned sentences.", "labels": [], "entities": []}, {"text": "One approach that has been proposed to address the problem of limited training data is to harvest the web for bilingual texts.", "labels": [], "entities": []}, {"text": "The STRAND method automatically gathers web pages that are potential translations of each other by looking for documents in one language which have links whose text contains the name of another language.", "labels": [], "entities": [{"text": "STRAND", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.5741841793060303}]}, {"text": "For example, if an English web page had a link with the text \"Espa\u00f1ol\" or \"en Espa\u00f1ol\" then the page linked to is treated as a candidate translation of the English page.", "labels": [], "entities": []}, {"text": "Further checks verify the plausibility of its being a translation).", "labels": [], "entities": []}, {"text": "Instead of attempting to gather new translations from the web, we describe an alternate method for automatically creating parallel corpora.", "labels": [], "entities": []}, {"text": "Specifically, we examine the use of existing translations as a resource to bootstrap more training data, and to create data for new language pairs.", "labels": [], "entities": []}, {"text": "We generate translation models from existing data and use them to produce translations of new sen- tences.", "labels": [], "entities": []}, {"text": "Incorporating this machine-created parallel data to the original set, and retraining the translation models improves the translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9076794385910034}]}, {"text": "To perform the retraining we use co-training) which is a weakly supervised learning technique that relies on having distinct views of the items being classified.", "labels": [], "entities": []}, {"text": "The views that we employ for co-training are multiple source documents.", "labels": [], "entities": []}, {"text": "Section 2 motivates the use of weakly supervised learning, and introduces co-training for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8235273361206055}]}, {"text": "Section 3 reports our experimental results.", "labels": [], "entities": []}, {"text": "One experiment shows that co-training can modestly benefit translation systems trained from similarly sized corpora.", "labels": [], "entities": []}, {"text": "A second experiment shows that co-training can have a dramatic benefit when the size of initial training corpora are mismatched.", "labels": [], "entities": []}, {"text": "This suggests that co-training for statistical machine translation is especially useful for languages with impoverished training corpora.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.7282148599624634}]}, {"text": "Section 4 discusses the implications of our experiments, and discusses ways which our methods might be used more practically.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to conduct co-training experiments we first needed to assemble appropriate corpora.", "labels": [], "entities": []}, {"text": "The corpus used in our experiments was assembled from the data used in the) multiple source translation paper.", "labels": [], "entities": []}, {"text": "The data was gathered from the Bulletin of the European Union which is published on the Internet in the eleven official languages of the European Union.", "labels": [], "entities": []}, {"text": "We used a subset of the data to create a multi-lingual corpus, aligning sentences between French, Spanish, German, Italian and Portuguese).", "labels": [], "entities": []}, {"text": "Additionally we created bilingual corpora between English and each of the five languages using sentences that were not included in the multi-lingual corpus.", "labels": [], "entities": []}, {"text": "used the data to find a translation that was most probable given multiple source strings.", "labels": [], "entities": []}, {"text": "Och and Ney found that multi-source translations using two source languages reduced word error rate when compared to using source strings from a single language.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 84, "end_pos": 99, "type": "METRIC", "confidence": 0.6424242357412974}]}, {"text": "For multi-source translations using source strings in six languages a greater reduction in word error rate was achieved.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 91, "end_pos": 106, "type": "METRIC", "confidence": 0.7270291447639465}]}, {"text": "Our work is similar in spirit, although instead of using multi-source translation at the time of translation, we integrate it into the training stage.", "labels": [], "entities": []}, {"text": "Whereas Och and Ney use multiple source strings to improve the quality of one translation only, our co-training method attempts to improve the accuracy of all translation models by bootstrapping more training data from multiple source documents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9971945285797119}]}, {"text": "The performance of translation models was evaluated using a held-out set of 1,000 sentences in each language, with reference translations into English.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9627358317375183}]}, {"text": "Each translation model was used to produce translation of these sentences and the machine translations were compared to the reference human translations using word error rate (WER).", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 159, "end_pos": 180, "type": "METRIC", "confidence": 0.8889243503411611}]}, {"text": "The results are reported in terms of increasing accuracy, rather than decreasing error.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9992732405662537}, {"text": "error", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.98261559009552}]}, {"text": "We define accuracy as 100 minus WER.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997238516807556}, {"text": "WER", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9987840056419373}]}, {"text": "Other evaluation metrics such as position independent WER or the Bleu method () could have been used.", "labels": [], "entities": [{"text": "WER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.6086301207542419}, {"text": "Bleu", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9623770713806152}]}, {"text": "While WER may not be the best measure of translation quality, it is sufficient to track performance improvements in the following experiments.", "labels": [], "entities": [{"text": "WER", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.973517656326294}]}, {"text": "gives the result of co-training using the most accurate translation from the candidate translations produced by five translation models.", "labels": [], "entities": []}, {"text": "Each translation model was initially trained on bilingual corpora consisting of around 20,000 human translated sentences.", "labels": [], "entities": []}, {"text": "These translation models were used to translate 63,000 sentences, of which the top 10,000 were selected for the first round.", "labels": [], "entities": []}, {"text": "At the next round 53,000 sentences were translated and the top 10,000 sentences were selected for the second round.", "labels": [], "entities": []}, {"text": "The final candidate pool contained 43,000 translations and again the top 10,000 were selected.", "labels": [], "entities": []}, {"text": "The table indicates that gains maybe had from co-training.", "labels": [], "entities": []}, {"text": "Each of the translation models improves over its initial training size at some point in the co-training.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9602466225624084}]}, {"text": "The German to English translation model improves the most -exhibiting a 2.5% improvement inaccuracy.", "labels": [], "entities": [{"text": "German to English translation", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.5817344561219215}]}], "tableCaptions": [{"text": " Table 1: Co-training results over three rounds", "labels": [], "entities": []}]}