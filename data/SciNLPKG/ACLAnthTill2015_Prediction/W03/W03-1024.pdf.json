{"title": [{"text": "Japanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning", "labels": [], "entities": [{"text": "Japanese Zero Pronoun Resolution", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7158298045396805}]}], "abstractContent": [{"text": "Anaphora resolution is one of the most important research topics in Natural Language Processing.", "labels": [], "entities": [{"text": "Anaphora resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7955662608146667}, {"text": "Natural Language Processing", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6375796993573507}]}, {"text": "In English, overt pronouns such as she and definite noun phrases such as the company are anaphors that refer to preceding entities (an-tecedents).", "labels": [], "entities": []}, {"text": "In Japanese, anaphors are often omitted, and these omissions are called zero pronouns.", "labels": [], "entities": []}, {"text": "There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6922110120455424}]}, {"text": "Since we have to take various factors into consideration, it is difficult to find a good combination of heuris-tic rules.", "labels": [], "entities": []}, {"text": "Therefore, the machine learning approach is attractive, but it requires a large amount of training data.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.758359432220459}]}, {"text": "In this paper, we propose a method that combines ranking rules and machine learning.", "labels": [], "entities": []}, {"text": "The ranking rules are simple and effective, while machine learning can take more factors into account.", "labels": [], "entities": []}, {"text": "From the results of our experiments, this combination gives better performance than either of the two previous approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Anaphora resolution is an important research topic in Natural Language Processing.", "labels": [], "entities": [{"text": "Anaphora resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7944081127643585}, {"text": "Natural Language Processing", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.639932801326116}]}, {"text": "For instance, machine translation systems should identify antecedents of anaphors (such as he or she) in the source language to achieve better translation quality in the target language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7903693914413452}]}, {"text": "We are now studying open-domain question answering systems , and we expect QA systems to benefit from anaphora resolution.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7674536406993866}, {"text": "anaphora resolution", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7018131762742996}]}, {"text": "Typical QA systems try to answer a user's question by finding relevant phrases from large corpora.", "labels": [], "entities": []}, {"text": "When a correct answer phrase is far from the keywords given in the question, the systems will not succeed in finding the answer.", "labels": [], "entities": []}, {"text": "If the system can correctly resolve anaphors, it will find keywords or answers represented by anaphors, and the chances of finding the answer will increase.", "labels": [], "entities": []}, {"text": "From this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles.", "labels": [], "entities": [{"text": "resolve anaphors in full-text newspaper articles", "start_pos": 73, "end_pos": 121, "type": "TASK", "confidence": 0.8942284882068634}]}, {"text": "In Japanese, anaphors are often omitted and these omissions are called zero pronouns.", "labels": [], "entities": []}, {"text": "Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult.", "labels": [], "entities": [{"text": "automatic zero pronoun resolution", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.5774365141987801}]}, {"text": "In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to 'zero.'", "labels": [], "entities": []}, {"text": "Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles.", "labels": [], "entities": [{"text": "Japanese zero pronoun resolution", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.6124423742294312}]}, {"text": "They have discussed simple sentenses), dialogues (), stereotypical lead sentences of newspaper articles (, intrasentential resolution ( or organization names in newspaper articles.", "labels": [], "entities": [{"text": "intrasentential resolution", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.785985141992569}]}, {"text": "There are two approaches to the problem: the heuristic approach and the machine learning ap-proach.", "labels": [], "entities": []}, {"text": "The Centering Theory () is important in the heuristic approach.", "labels": [], "entities": []}, {"text": "proposed forward center ranking for Japanese.", "labels": [], "entities": []}, {"text": "emphasized the importance of a property-sharing constraint.", "labels": [], "entities": []}, {"text": "experimented on the roles of conjunctive postpositions in complex sentences.", "labels": [], "entities": []}, {"text": "However, these improvements are not sufficient for resolving zeros accurately.", "labels": [], "entities": []}, {"text": "proposed complicated heuristic rules that take various features of antecedents and anaphors into account.", "labels": [], "entities": []}, {"text": "We have to take even more factors into account, but it is difficult to maintain such heuristic rules.", "labels": [], "entities": []}, {"text": "Therefore, recent studies employ machine learning approaches.", "labels": [], "entities": []}, {"text": "However, it is also difficult to prepare a sufficient number of annotated corpora.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method that combines these two approaches.", "labels": [], "entities": []}, {"text": "Heuristic ranking rules give a general preference, while a machine learning method excludes inappropriate antecedent candidates.", "labels": [], "entities": []}, {"text": "From the results of our experiments, the proposed method shows better performance than either of the two approaches alone.", "labels": [], "entities": []}, {"text": "Before giving a description of our methodology, we briefly introduce the grammar of the Japanese language here.", "labels": [], "entities": []}, {"text": "A Japanese sentence is a sequence of bunsetsus: A bunsetsu is a sequence of content words (e.g., nouns, adjectives, and verbs) followed by zero or more functional words (e.g., particles and auxiliary verbs): A bunsetsu modifies one of the following bunsetsus.", "labels": [], "entities": []}, {"text": "A particle (joshi) marks the grammatical case of the noun phrase immediately before it.", "labels": [], "entities": []}, {"text": "For example, ga is nominative (subject), wo is accusative (object), ni is dative (object2), and wa marks a topic.", "labels": [], "entities": []}, {"text": "indicates that there are four bunsetsus in this sentence and that the first bunsetsu modifies the fourth bunsetsu and soon.", "labels": [], "entities": []}, {"text": "The last bunsetsu modifies no bunsetsu, which is indicated by 3 # . It takes along time to construct high-quality annotated data, and we want to compare our results with conventional methods.", "labels": [], "entities": []}, {"text": "Therefore, we obtained Seki's data (, which are based on the Kyoto University Corpus 2 2.0.", "labels": [], "entities": [{"text": "Seki's data", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.6712876160939535}, {"text": "Kyoto University Corpus 2 2.0", "start_pos": 61, "end_pos": 90, "type": "DATASET", "confidence": 0.9786289453506469}]}, {"text": "These data are divided into two groups: general and editorial.", "labels": [], "entities": []}, {"text": "General contains 30 general news articles, and editorial contains 30 editorial articles.", "labels": [], "entities": []}, {"text": "According to his experiments, editorial is harder than general.", "labels": [], "entities": []}, {"text": "Perhaps this is caused by the difference in rhetorical styles and the lengths of articles.", "labels": [], "entities": []}, {"text": "The average number of sentences in an editorial article is 28.7, while that in a general article is 13.9.", "labels": [], "entities": []}, {"text": "However, we found problems in his data.", "labels": [], "entities": []}, {"text": "For instance, the data contained ambiguous antecedents like dou-shi (the same person) or dou-sha (the same company) as correct antecedents.", "labels": [], "entities": []}, {"text": "We replaced these 'correct answers' with their explicit names.", "labels": [], "entities": []}, {"text": "We also removed zeros in quoted sentences because they are quite different from other sentences.", "labels": [], "entities": []}, {"text": "In addition, we decided to use the output of ChaSen 2.2.9 3 and CaboCha 0.34 4 instead of the morphological information and the dependency information provided by the Kyoto Corpus since classification of the joshi (particles) in the Corpus was not satisfactory for our purpose.", "labels": [], "entities": [{"text": "Kyoto Corpus", "start_pos": 167, "end_pos": 179, "type": "DATASET", "confidence": 0.9857634902000427}]}, {"text": "Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha's dependency output is very similar to that of the Corpus.", "labels": [], "entities": [{"text": "Kyoto Corpus 3.0", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9721092780431112}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Percentage of correctly resolved zeros", "labels": [], "entities": [{"text": "Percentage of correctly resolved zeros", "start_pos": 10, "end_pos": 48, "type": "METRIC", "confidence": 0.7537924170494079}]}]}