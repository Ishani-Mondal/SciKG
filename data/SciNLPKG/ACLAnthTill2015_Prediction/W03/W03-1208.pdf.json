{"title": [], "abstractContent": [{"text": "This paper proposes a machine learning based question classification method using a kernel function, Hierarchical Directed Acyclic Graph (HDAG) Kernel.", "labels": [], "entities": [{"text": "question classification", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7957858741283417}]}, {"text": "The HDAG Kernel directly accepts struc-tured natural language data, such as several levels of chunks and their relations, and computes the value of the kernel function at a practical cost and time while reflecting all of these structures.", "labels": [], "entities": [{"text": "HDAG Kernel", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8797470033168793}]}, {"text": "We examine the proposed method in a question classification experiment using 5011 Japanese questions that are labeled by 150 question types.", "labels": [], "entities": [{"text": "question classification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7701914310455322}]}, {"text": "The results demonstrate that our proposed method improves the performance of question classification over that by conventional methods such as bag-of-words and their combinations.", "labels": [], "entities": [{"text": "question classification", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7936793863773346}]}], "introductionContent": [{"text": "Open-domain Question Answering (ODQA) involves the extraction of correct answer(s) to a given free-form factual question from a large collection of texts.", "labels": [], "entities": [{"text": "Open-domain Question Answering (ODQA)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7749030788739523}, {"text": "extraction of correct answer(s) to a given free-form factual question from a large collection of texts", "start_pos": 51, "end_pos": 153, "type": "TASK", "confidence": 0.7645775738515352}]}, {"text": "ODQA has been actively studied allover the world since the start of the Question Answering Track at TREC-8 in 1999.", "labels": [], "entities": [{"text": "ODQA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7070464491844177}, {"text": "Question Answering Track at TREC-8 in 1999", "start_pos": 72, "end_pos": 114, "type": "TASK", "confidence": 0.7791416474751064}]}, {"text": "The definition of ODQA tasks at the TREC QATrack has been revised and extended year after year.", "labels": [], "entities": [{"text": "TREC QATrack", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.8740437030792236}]}, {"text": "At first, ODQA followed the Passage Retrieval method as used at TREC-8.", "labels": [], "entities": [{"text": "Passage Retrieval", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8538317382335663}, {"text": "TREC-8", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.842894971370697}]}, {"text": "That is, the ODQA task was to answer a question in the form of strings of 50 bytes or 250 bytes excerpted from a large set of news wires.", "labels": [], "entities": []}, {"text": "Recently, however, the ODQA task is considered to be a task of extracting exact answers to a question.", "labels": [], "entities": []}, {"text": "For instance, if a QA system is given the question \"When was Queen Victoria born?\", it should answer \"1832\".", "labels": [], "entities": []}, {"text": "Typically, QA systems have the following components for achieving ODQA: Question analysis analyzes a given question and determines the question type and keywords.", "labels": [], "entities": [{"text": "Question analysis analyzes a given question", "start_pos": 72, "end_pos": 115, "type": "TASK", "confidence": 0.8179170191287994}]}, {"text": "Text retrieval finds the top \u00a2 paragraphs or documents that match the result of the question analysis component.", "labels": [], "entities": [{"text": "Text retrieval", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7664541602134705}]}, {"text": "Answer candidate extraction extracts answer candidates of the given question from the documents retrieved by the text retrieval component, based on the results of the question types.", "labels": [], "entities": [{"text": "Answer candidate extraction extracts answer candidates of the given question", "start_pos": 0, "end_pos": 76, "type": "TASK", "confidence": 0.8290981829166413}]}, {"text": "Answer selection selects the most plausible answer(s) to the given question from among the answer candidates extracted by the answer candidate extraction component.", "labels": [], "entities": [{"text": "Answer selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.920669287443161}]}, {"text": "One of the most important processes of those listed above is identifying the target of intention in a given question to determine the type of sought-after answer.", "labels": [], "entities": []}, {"text": "This process of determining the question type fora given question is usually called question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 84, "end_pos": 107, "type": "TASK", "confidence": 0.7830522656440735}]}, {"text": "Without a question type, that is, the result of question classification, it would be much more difficult or even nearly infeasible to select correct answers from among the possible answer candidates, which would necessarily be all of the noun phrases or named entities in the texts.", "labels": [], "entities": [{"text": "question classification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7344080209732056}]}, {"text": "Question classification provides the benefit of a powerful restriction that reduces to a practical number of the answer candidates that should be evaluated in the answer selection process.", "labels": [], "entities": [{"text": "Question classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8074891567230225}, {"text": "answer selection process", "start_pos": 163, "end_pos": 187, "type": "TASK", "confidence": 0.8862658143043518}]}, {"text": "This work develops a machine learning approach to question classification ().", "labels": [], "entities": [{"text": "question classification", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.9272092878818512}]}, {"text": "We use the Hierarchical Directed Acyclic Graph (HDAG) Kernel (, which is suited to handle structured natural language data.", "labels": [], "entities": []}, {"text": "It can handle structures within texts as the features of texts without converting the structures to the explicit representation of numerical feature vectors.", "labels": [], "entities": []}, {"text": "This framework is useful for question classification because the works of ( showed that richer information, such as structural and semantical information inside a given question, improves the question classification performance over using the information of just simple key terms.", "labels": [], "entities": [{"text": "question classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.89931720495224}, {"text": "question classification", "start_pos": 192, "end_pos": 215, "type": "TASK", "confidence": 0.717691570520401}]}, {"text": "In Section 2, we present the question classification problem.", "labels": [], "entities": [{"text": "question classification", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.889540821313858}]}, {"text": "In Section 3, we explain our proposed method for question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.8884178102016449}]}, {"text": "Finally, in Section 4, we describe our experiment and results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the 5011 questions by using fivefold cross-validation and used the following two approaches to evaluate the performance.", "labels": [], "entities": []}, {"text": "In Qacc, classifying with a correct question type implies that all of the one-vs-rest models from the top of the hierarchy of the question taxonomy to the given question type must classify correctly.", "labels": [], "entities": []}, {"text": "shows the results of our question classification experiment, which is evaluated by five-fold cross-validation.", "labels": [], "entities": [{"text": "question classification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8225988447666168}]}], "tableCaptions": [{"text": " Table 2: Distribution of 5011 questions over question type hierarchy", "labels": [], "entities": [{"text": "Distribution of 5011 questions", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.865898996591568}]}, {"text": " Table 3: Results of question classification experi- ment by five-fold cross-validation", "labels": [], "entities": [{"text": "question classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.799767941236496}]}, {"text": " Table 4: Accuracy of each question (Qacc) evalu- ated at different depths of hierarchy in question tax- onomy", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.991563081741333}]}]}