{"title": [{"text": "Question answering via Bayesian inference on lexical relations", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8361549973487854}]}], "abstractContent": [{"text": "Many researchers have used lexical networks and ontologies to mitigate synonymy and polysemy problems in Question Answering (QA), systems coupled with taggers, query classifiers, and answer extractors in complex and ad-hoc ways.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.8505876898765564}, {"text": "answer extractors", "start_pos": 183, "end_pos": 200, "type": "TASK", "confidence": 0.6269696801900864}]}, {"text": "We seek to make QA systems reproducible with shared and modest human effort, carefully separating knowledge from algorithms.", "labels": [], "entities": []}, {"text": "To this end, we propose an aesthetically \"clean\" Bayesian inference scheme for exploiting lexical relations for passage-scoring for QA.", "labels": [], "entities": []}, {"text": "The factors which contribute to the efficacy of Bayesian Inferencing on lexical relations are soft word sense disambiguation, parameter smoothing which ameliorates the data sparsity problem and estimation of joint probability over words which overcomes the deficiency of naive-bayes-like approaches.", "labels": [], "entities": [{"text": "soft word sense disambiguation", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.6199726387858391}]}, {"text": "Our system is superior to vector-space ranking techniques from IR, and its accuracy approaches that of the top contenders at the TREC QA tasks in recent years.", "labels": [], "entities": [{"text": "IR", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.8658280372619629}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9996147155761719}, {"text": "TREC QA tasks", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.5688642164071401}]}], "introductionContent": [{"text": "This paper describes an approach to probabilistic inference using lexical relations, such as expressed by a WordNet, an ontology, or a combination, with applications to passage-scoring for open-domain question answering (QA).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9326304793357849}, {"text": "open-domain question answering (QA)", "start_pos": 189, "end_pos": 224, "type": "TASK", "confidence": 0.7853267192840576}]}, {"text": "The use of lexical resources in Information Retrieval (IR) is not new; for almost a decade, the IR community has considered the use of natural language processing techniques ( to circumvent synonymy, polysemy, and other barriers to purely string-matching search engines.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.8858860611915589}]}, {"text": "In particular, a number of researchers have attempted to use the English WordNet to \"bridge the gap\" between query and response.", "labels": [], "entities": [{"text": "English WordNet", "start_pos": 65, "end_pos": 80, "type": "DATASET", "confidence": 0.838017076253891}]}, {"text": "Interestingly, the results have mostly been inconclusive or negative.", "labels": [], "entities": []}, {"text": "A number of explanations have been offered for this lack of success, some of which are \u00a2 presence of unnecessary links and absence of necessary links in the WordNet, \u00a2 hurdle of Word Sense Disambiguation (WSD) \u00a2 ad-hocness in the distance and scoring functions ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 157, "end_pos": 164, "type": "DATASET", "confidence": 0.9544683694839478}, {"text": "hurdle", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9880684018135071}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 178, "end_pos": 209, "type": "TASK", "confidence": 0.6237895240386327}]}], "datasetContent": [{"text": "We perform extensive experiments to evaluate our system, using the TREC http://trec.nist.", "labels": [], "entities": [{"text": "TREC", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.661260187625885}]}, {"text": "gov/data/qa.html QA benchmark.", "labels": [], "entities": []}, {"text": "We find that our algorithm is a substantial improvement beyond a baseline IR approach to passage ranking.", "labels": [], "entities": [{"text": "passage ranking", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.959342747926712}]}, {"text": "Based on published numbers, it also appears to be in the same league as the top performers at recent TREC QA events.", "labels": [], "entities": [{"text": "TREC QA events", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.5478279888629913}]}, {"text": "We also note that training our system improves the quality of our ranking, even though WSD accuracy does not increase, which affirms the belief that passage scoring need not depend on perfect WSD, given we use a robust, 'soft WSD'.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9416278004646301}, {"text": "passage scoring", "start_pos": 149, "end_pos": 164, "type": "TASK", "confidence": 0.8289069533348083}]}, {"text": "See section \u00a5 3 .3.", "labels": [], "entities": []}, {"text": "We use the Text REtrieval Conference (TREC)) corpus and question/answers from its QA track.", "labels": [], "entities": [{"text": "Text REtrieval Conference (TREC))", "start_pos": 11, "end_pos": 44, "type": "TASK", "confidence": 0.7240819533665975}]}, {"text": "The corpus is 2 GB of newspaper articles.", "labels": [], "entities": []}, {"text": "There is a set z of about 690 factual questions.", "labels": [], "entities": []}, {"text": "For each question, we retrieve the top { | d ocuments using a standard TFIDF-based IR engine such as SMART.", "labels": [], "entities": [{"text": "SMART", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9081849455833435}]}, {"text": "We used the question set and corresponding top 50 document collection from TREC 2001 for our experiments.", "labels": [], "entities": [{"text": "question set and corresponding top 50 document collection from TREC 2001", "start_pos": 12, "end_pos": 84, "type": "DATASET", "confidence": 0.8004819561134685}]}, {"text": "We used MXPOST, a maximum entropy based POS tagger.", "labels": [], "entities": [{"text": "MXPOST", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.6787109971046448}]}, {"text": "The part of speech tag is used while mapping document and question terms to their corresponding nodes in the BBN.", "labels": [], "entities": [{"text": "BBN", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.965526819229126}]}, {"text": "The passage length we chose was \u00a4 ~ } | w ords.", "labels": [], "entities": []}, {"text": "Unless otherwise stated explicitly, the maximum height upto which the BBN was used for inferencing for each Q-passage pair can be assumed to be \u007f .  TREC QA evaluation has two runs based on the length of system response to a question.", "labels": [], "entities": [{"text": "BBN", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8691756725311279}, {"text": "TREC QA evaluation", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.6553624073664347}]}, {"text": "In the first the response is a passage up to 250 bytes in size.", "labels": [], "entities": []}, {"text": "The second, more ambitious run asks for shorter responses of up to 50 bytes.", "labels": [], "entities": []}, {"text": "(More recently, TREC has updated its requirements to demand exact, extracted answers.)", "labels": [], "entities": [{"text": "TREC", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.4111036956310272}]}, {"text": "To determine if the response is actually an answer to the question, TREC provides a set of regular expressions for each question.", "labels": [], "entities": [{"text": "TREC", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.47364580631256104}]}, {"text": "The presence of any of these in the response indicates that it is a valid answer.", "labels": [], "entities": []}, {"text": "For evaluation the system is required to submit its top five responses for each question.", "labels": [], "entities": []}, {"text": "This is used to calculate the performance measure mean reciprocal rank (MRR) for the system, defined as The IR baseline MRR is only about 0.3, which is far short of Falcon, which has an MRR of almost 0.7.", "labels": [], "entities": [{"text": "performance measure mean reciprocal rank (MRR)", "start_pos": 30, "end_pos": 76, "type": "METRIC", "confidence": 0.8721657395362854}, {"text": "IR baseline MRR", "start_pos": 108, "end_pos": 123, "type": "METRIC", "confidence": 0.5769404570261637}, {"text": "MRR", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.9800829291343689}]}, {"text": "The baseline MRR is low for the obvious reasons: the IR engine cannot bridge the lexical gap.", "labels": [], "entities": [{"text": "MRR", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6306040287017822}]}], "tableCaptions": [{"text": " Table 1: MRRs for baseline, untrained and trained BBNs", "labels": [], "entities": [{"text": "MRRs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.5981277823448181}, {"text": "BBNs", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.34809911251068115}]}, {"text": " Table 2: MRRs for best performing systems in TREC9", "labels": [], "entities": [{"text": "MRRs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7952464818954468}, {"text": "TREC9", "start_pos": 46, "end_pos": 51, "type": "TASK", "confidence": 0.4620160460472107}]}, {"text": " Table 3: MRRs for BBNs truncated at different heights", "labels": [], "entities": [{"text": "MRRs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6134109497070312}]}, {"text": " Table 4: MRRs for BBNs restricted to diff parts of WordNet", "labels": [], "entities": [{"text": "MRRs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6817148923873901}, {"text": "WordNet", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.8975964784622192}]}]}