{"title": [], "abstractContent": [{"text": "We introduce a method for using images for word sense disambiguation, either alone, or in conjunction with traditional text based methods.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7274180849393209}]}, {"text": "The approach is based in recent work on a method for predicting words for images which can be learned from image datasets with associated text.", "labels": [], "entities": []}, {"text": "When word prediction is constrained to a narrow set of choices such as possible senses, it can be quite reliable, and we use these predictions either by themselves or to reinforce standard methods.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.7702386677265167}]}, {"text": "We provide preliminary results on a subset of the Corel image database which has three to five keywords per image.", "labels": [], "entities": [{"text": "Corel image database", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.9135258396466573}]}, {"text": "The subset was automatically selected to have a greater portion of keywords with sense ambiguity and the word senses were hand labeled to provide ground truth for testing.", "labels": [], "entities": []}, {"text": "Results on this data strongly suggest that images can help with word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7979980309804281}]}, {"text": "piggy bank coins currency money water grass trees banks bank buildings trees city bank machine money currency bills snow banks hills winter Figure 1.", "labels": [], "entities": []}, {"text": "Word sense ambiguity in the Corel dataset.", "labels": [], "entities": [{"text": "Corel dataset", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9695554673671722}]}], "introductionContent": [{"text": "In this paper we investigate using words and pictures to disambiguate each other.", "labels": [], "entities": []}, {"text": "Word sense disambiguation has long been studied as an important problem in natural language processing.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6774572332700094}, {"text": "natural language processing", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.6437772214412689}]}, {"text": "It is illustrated in with the arguably overused \"bank\" example.", "labels": [], "entities": []}, {"text": "A priori, the word \"bank\" has a number of meanings including financial institution and a step or edge as in \"snow bank\" or \"river bank\".", "labels": [], "entities": []}, {"text": "Words which are spelt the same but have different meanings are very common, and clearly can confuse attempts to automatically deduce meaning from language.", "labels": [], "entities": []}, {"text": "Furthermore, they cannot simply be identified as ambiguous and then ignored, as there are too many such words and they do constrain the possible meanings of a body of text.", "labels": [], "entities": []}, {"text": "Since the words are spelt the same, resolving what they mean requires considering context.", "labels": [], "entities": []}, {"text": "A purely natural language based approach considers words near the one in question.", "labels": [], "entities": []}, {"text": "Thus in the bank example, words like \"financial\" or \"money\" are strong hints that the financial institution sense is meant.", "labels": [], "entities": []}, {"text": "Interestingly, despite much work, and a number of innovative ideas, doing significantly better than choosing the most commonsense is difficult.", "labels": [], "entities": []}, {"text": "In this work we present preliminary work on whether an associated images can help in word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.7982974251111349}]}, {"text": "In the simplest application, text and images might be analyzed in conjunction; for example, a news photograph with a caption, or a larger document with illustrations.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our initial test, we studied word sense disambiguation on the Corel image dataset which we have used extensively for studying word prediction from images.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.7788963913917542}, {"text": "Corel image dataset", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.9508859316507975}, {"text": "word prediction from images", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.8218714594841003}]}, {"text": "Each Corel image has 3-5 keywords associated with it.", "labels": [], "entities": []}, {"text": "Unfortunately, these keywords are unusual in that they do not have much sense conflict over the data set.", "labels": [], "entities": []}, {"text": "Put differently, although a keyword like \"head\" has many senses, one sense predominates in this data set.", "labels": [], "entities": []}, {"text": "To use the data despite this problem, we computed all the senses from WordNet () of all the words for an initial set of 16,000 images, together with the score for each sense using the method described above.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9779402613639832}]}, {"text": "We then applied some heuristics to create a subset of 1,800 images which had candidate sense problems.", "labels": [], "entities": []}, {"text": "Each word was then hand labeled with the correct sense.", "labels": [], "entities": []}, {"text": "The resulting dataset had only a handful of words with ambiguous senses present in sufficient quantity, but fortunately these were common enough such that about 1/6 of the documents had true word sense problems.", "labels": [], "entities": []}, {"text": "The results reported below were restricted to documents that had at least one word sense ambiguity.", "labels": [], "entities": []}, {"text": "The data set prepared as above thus consists of a vocabulary of word-sense combinations, together with a human labeling of whether the sense was valid or not.", "labels": [], "entities": []}, {"text": "We restrict the vocabulary to word-sense pairs which occur in at least 5 images.", "labels": [], "entities": []}, {"text": "We represent the observed senses fora word occurring in a document as a vector over the senses for that word from the vocabulary.", "labels": [], "entities": []}, {"text": "We give all relevant senses of the word a score of one, and incorrect senses a score of zero.", "labels": [], "entities": []}, {"text": "We normalize this vector so that its sum is one.", "labels": [], "entities": []}, {"text": "Although potentially a word could be ambiguous to a human examiner, typically the word sense vector would simply contain a single value of one, with the other values being zero.", "labels": [], "entities": []}, {"text": "We evaluate word-sense disambiguation strategies by comparing the vector of observed word-senses described above (the truth) to the vector containing estimates of the relevance of each word-sense pair corresponding to each occurring word.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.7336702942848206}]}, {"text": "For example, suppose an image has the word \"bank\", which maps to bank_1 with hand labeling, and suppose that bank_1 and bank_3 are in the vocabulary, but no other senses of bank.", "labels": [], "entities": []}, {"text": "Then the vector {\u2026,bank_1(0.7),bank_3(0.3),\u2026} should be ranked better than {\u2026,bank_1(0.3),bank_3(0.7),\u2026} when compared to the observed vector {\u2026,bank_1(1.0),bank_3(0.0),\u2026} (hand-labeled) To compare the vectors we simply normalize them and take the dot-product.", "labels": [], "entities": []}, {"text": "For the experiments we divided the data into training data (75%), and test data (25%).", "labels": [], "entities": []}, {"text": "We averaged results for 10 separate runs using different samples for the test and training sets.", "labels": [], "entities": []}, {"text": "We restricted the computation of results to those documents where there was clear sense ambiguity.", "labels": [], "entities": []}, {"text": "Because each such document typically had only one sense problem amid 3 or 4 words without sense problems, the baseline score using the measure above is greater than 0.80 because any strategy will get about 3 out or 4 correct for free.", "labels": [], "entities": []}, {"text": "To clarify this further, we include the results of randomly chosen among senses when there is more than one available.", "labels": [], "entities": []}, {"text": "The results are shown in strongly suggest that images can help disambiguate senses.", "labels": [], "entities": []}, {"text": "The na\u00efve method of text based disambiguation is comparable to chance, whereas adding image information substantially increased the performance.", "labels": [], "entities": [{"text": "text based disambiguation", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6500157415866852}]}], "tableCaptions": []}