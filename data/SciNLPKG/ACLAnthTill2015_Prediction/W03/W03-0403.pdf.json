{"title": [{"text": "Active learning for HPSG parse selection", "labels": [], "entities": [{"text": "HPSG parse selection", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8153367439905802}]}], "abstractContent": [{"text": "We describe new features and algorithms for HPSG parse selection models and address the task of creating annotated material to train them.", "labels": [], "entities": [{"text": "HPSG parse selection", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7734103997548422}]}, {"text": "We evaluate the ability of several sample selection methods to reduce the number of annotated sentences necessary to achieve a given level of performance.", "labels": [], "entities": []}, {"text": "Our best method achieves a 60% reduction in the amount of training material without any loss inaccuracy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Even with significant resources such as the Penn Treebank, a major bottleneck for improving statistical parsers has been the lack of sufficient annotated material from which to estimate their parameters.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9948185980319977}]}, {"text": "Most statistical parsing research, such as, has centered on training probabilistic context-free grammars using the Penn Treebank.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7684518694877625}, {"text": "Penn Treebank", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9955949783325195}]}, {"text": "For richer linguistic frameworks, such as Head-Driven Phrase Structure Grammar (HPSG), there is even less annotated material available for training stochastic parsing models.", "labels": [], "entities": [{"text": "Head-Driven Phrase Structure Grammar (HPSG)", "start_pos": 42, "end_pos": 85, "type": "TASK", "confidence": 0.7422645134585244}]}, {"text": "There is thus a pressing need to create significant volumes of annotated material in a logistically efficient manner.", "labels": [], "entities": []}, {"text": "Even if it were possible to bootstrap from the Penn Treebank, it is still unlikely that there would be sufficient quantities of high quality material.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9966555535793304}]}, {"text": "There has been a strong focus in recent years on using the active learning technique of selective sampling to reduce the amount of human-annotated training material needed to train models for various natural language processing tasks.", "labels": [], "entities": []}, {"text": "The aim of selective sampling is to identify the most informative examples, according to some selection method, from a large pool of unlabelled material.", "labels": [], "entities": []}, {"text": "Such selected examples are then manually labelled.", "labels": [], "entities": []}, {"text": "Selective sampling has typically been applied to classification tasks, but has also been shown to reduce the number of examples needed for inducing Lexicalized Tree Insertion Grammars from the Penn Treebank).", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8982443809509277}, {"text": "Lexicalized Tree Insertion Grammars", "start_pos": 148, "end_pos": 183, "type": "TASK", "confidence": 0.6471302211284637}, {"text": "Penn Treebank", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.9745110273361206}]}, {"text": "The suitability of active learning for HPSG-type grammars has as yet not been explored.", "labels": [], "entities": []}, {"text": "This paper addresses the problem of minimizing the human effort expended in creating annotated training material for HPSG parse selection by using selective sampling.", "labels": [], "entities": [{"text": "HPSG parse selection", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.9350359241167704}]}, {"text": "We do so in the context of), a treebank that contains HPSG analyses for sentences from the Verbmobil appointment scheduling and travel planning domains.", "labels": [], "entities": [{"text": "Verbmobil appointment scheduling", "start_pos": 91, "end_pos": 123, "type": "TASK", "confidence": 0.7319625417391459}]}, {"text": "We show that sample selection metrics based on tree entropy and disagreement between two different parse selection models significantly reduce the number of annotated sentences necessary to match a given level of performance according to random selection.", "labels": [], "entities": []}, {"text": "Furthermore, by combining these two methods as an ensemble selection method, we require even fewer examplesachieving a 60% reduction in the amount of annotated training material needed to outperform a model trained on randomly selected material.", "labels": [], "entities": []}, {"text": "These results suggest that significant reductions inhuman effort can be realized through selective sampling when creating annotated material for linguistically rich grammar formalisms.", "labels": [], "entities": []}, {"text": "As the basis of our active learning approach, we create both log-linear and perceptron models, the latter of which has not previously been used for feature-based grammars.", "labels": [], "entities": []}, {"text": "We show that the different biases of the two types of models is sufficient to create diverse members fora committee, even when they use exactly the same features.", "labels": [], "entities": []}, {"text": "With respect to the features used to train models, we demonstrate that a very simple feature selection strategy that ignores the proper structure of trees is competitive with one that fully respects tree configurations.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "In sections 2 and 3, we briefly introduce active learning and the Redwoods treebank.", "labels": [], "entities": [{"text": "Redwoods treebank", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9871791005134583}]}, {"text": "Section 4 discusses the parse selection models that we use in the experiments.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.8986281752586365}]}, {"text": "In sections 5 and 6, we explain the different selection methods that we use for active learning and explicate the setup in which the experiments were conducted.", "labels": [], "entities": []}, {"text": "Finally, the results of the experiments are presented and discussed in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The pseudo-code for committee-based active learning with two members is given in.", "labels": [], "entities": []}, {"text": "Starting with a small amount of initial annotated training material, the learners on the committee are used to select examples, based on the method being used.", "labels": [], "entities": []}, {"text": "These examples are subsequently manually annotated and added to the set of labelled training material and the learners are retrained on the extended set.", "labels": [], "entities": []}, {"text": "This loop continues until all available unannotated examples are exhausted, or until some other pre-determined condition is met.", "labels": [], "entities": []}, {"text": "As is standard for active learning experiments, we quantify the effect of different selection techniques by using them to select subsets of the material already annotated in Redwoods-3.", "labels": [], "entities": []}, {"text": "For the experiments, we used tenfold cross-validation by moving a fixed window of 500 sentences through Redwoods-3 for the test set and selecting samples from the remaining 4802 sentences.: Pseudo-code for committee-based active learning. are selected for annotation from a randomly chosen subset according to the operative selection method until the total amount of annotated training material made available to the learners reaches 3000.", "labels": [], "entities": []}, {"text": "We select 25 examples at time until the training set contains 1000 examples, then 50 at a time until it has 2000, and finally 100 at a time until it has 3000.", "labels": [], "entities": []}, {"text": "The results for each selection method are averaged over four tenfold cross-validation runs.", "labels": [], "entities": []}, {"text": "Whereas Hwa) evaluated the effectiveness of selective sampling according to the number of brackets which were needed to create the parse trees for selected sentences, we compare selection methods based on the absolute number of sentences they select.", "labels": [], "entities": []}, {"text": "This is realistic in the Redwoods setting since the derivation trees are created automatically from the ERG, and the task of the human annotator is to select the best from all licensed parses.", "labels": [], "entities": []}, {"text": "Annotation in Redwoods uses an interface that presents local discriminants which disambiguate large portions of the parse forest, so options are narrowed down quickly even for sentences with a large number of parses.", "labels": [], "entities": []}, {"text": "shows the performance of the LL-CONFIG model as more examples are chosen according to the different selection methods.", "labels": [], "entities": []}, {"text": "As can be seen, both tree entropy and disagreement are equally effective and significantly improve on random selection.", "labels": [], "entities": []}, {"text": "Selection by sentence length is worse than random until 2100 examples have been annotated.", "labels": [], "entities": []}, {"text": "Selecting more ambiguous sentences does eventually perform significantly better than random, but its accuracy does not rise nearly as steeply as tree entropy and disagreement selection.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9993797540664673}]}, {"text": "shows the precise values for all methods using different amounts of annotated sentences.", "labels": [], "entities": []}, {"text": "The accuracies for entropy and disagreement are statistically significant improvements over random selection.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.992382824420929}]}, {"text": "Using a pair-wise t-test, the values for 500, 1000, and 2000 are significant at 99% confidence, and those for 3000 are significant at 95% confidence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parse selection models.", "labels": [], "entities": [{"text": "Parse selection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8405323326587677}]}, {"text": " Table 3: Parse selection accuracy.", "labels": [], "entities": [{"text": "Parse selection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7006848454475403}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9875879883766174}]}, {"text": " Table 4: Accuracy for different selection methods with  different amounts of training data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976397752761841}]}, {"text": " Table 5: Number of examples needed for different selec- tion methods to outperform random selection with 3000  examples. The final column gives the percentage reduc- tion in the number of examples used.", "labels": [], "entities": []}, {"text": " Table 6: Accuracy for random, tree entropy and com- bined selection selection with different amounts of train- ing data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9926693439483643}]}]}