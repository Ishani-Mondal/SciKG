{"title": [{"text": "Statistical Machine Translation Using Coercive Two-Level Syntactic Transduction", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7622923254966736}, {"text": "Syntactic Transduction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6761548221111298}]}], "abstractContent": [{"text": "We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.6942807336648306}, {"text": "part-of-speech tagging", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7128051519393921}, {"text": "phrase chunking", "start_pos": 158, "end_pos": 173, "type": "TASK", "confidence": 0.7070801705121994}]}, {"text": "It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language.", "labels": [], "entities": []}, {"text": "We also examine aspects of lexical transfer, suggesting and exploring a concept of translation coercion across parts of speech, as well as a transfer model based on lemma-to-lemma translation probabilities , which holds promise for improving machine translation of low-density languages.", "labels": [], "entities": [{"text": "lexical transfer", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7271854281425476}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.7693832218647003}]}, {"text": "Experiments are performed in both Arabic-to-English and French-to-English translation demonstrating the efficacy of the proposed techniques.", "labels": [], "entities": []}, {"text": "Performance is automatically evaluated via the Bleu score metric.", "labels": [], "entities": [{"text": "Bleu score metric", "start_pos": 47, "end_pos": 64, "type": "METRIC", "confidence": 0.9445827603340149}]}], "introductionContent": [{"text": "In this work we define, implement and evaluate a novel model for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 65, "end_pos": 102, "type": "TASK", "confidence": 0.8238460024197897}]}, {"text": "Our goal was to produce a SMT system for translating foreign languages into English which utilizes some syntactic information in both the foreign language and English without, however, requiring a full parse in either language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9948111772537231}]}, {"text": "Some advantages of not relying on full parses include that (1) there is alack of availability of parsers for many languages of interest; (2) parsing time complexity represents a potential bottleneck for both model training and testing.", "labels": [], "entities": []}, {"text": "Intuitively, the explicit modeling of syntactic phenomena should be of benefit in the machine translation task; the ability to handle long-distance motion in an intelligently constrained way is a salient example of such a benefit.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.80189248919487}]}, {"text": "Allowing unconstrained translation reorderings at the word level generates a very large set of permutations that pose a difficult search problem at decoding time.", "labels": [], "entities": [{"text": "translation reorderings", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8550858795642853}]}, {"text": "We propose a model that makes use of shallow parses (text chunking) to support long-distance motion of phrases without requiring deeper analysis of syntax.", "labels": [], "entities": []}, {"text": "The resources required to train this system on anew language are minimal, and we gain the ability to model long-distance movement and some interesting properties of lexical translation across parts of speech.", "labels": [], "entities": []}, {"text": "One of the source languages we examine in this paper, Arabic, has a canonical sentence-level order of Verb-SubjectObject, which means that translation into English (with a standard ordering of Subject-Verb-Object) commonly requires motion of entire phrasal constituents, which is not true of French-to-English translation, to cite one language pair whose characteristics have wielded great influence in the history of work on statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 426, "end_pos": 457, "type": "TASK", "confidence": 0.6481536626815796}]}, {"text": "A key motivation for and objective of this work was to build a translation model and feature space to handle the above-described phenomenon effectively.", "labels": [], "entities": [{"text": "translation", "start_pos": 63, "end_pos": 74, "type": "TASK", "confidence": 0.9673187136650085}]}], "datasetContent": [{"text": "Results Tables A and B below list evaluation results for translation on the Arabic and French test sets respectively.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9763760566711426}, {"text": "French test sets", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.7692201435565948}]}, {"text": "In each case, results fora comparison systemthe Giza++ IBM Model 4 implementation) with the ReWrite decoder () -are included as a benchmark.", "labels": [], "entities": [{"text": "ReWrite decoder", "start_pos": 92, "end_pos": 107, "type": "DATASET", "confidence": 0.9058412909507751}]}, {"text": "Results were generated for training corpora of varying sizes.", "labels": [], "entities": []}, {"text": "For Arabic, we ran our system on two large subsets of the UN corpus and evaluated on a 200-sentence held-out set (refer to Results Table A below).", "labels": [], "entities": [{"text": "UN corpus", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8896831572055817}]}, {"text": "For the 150K sentence Arabic training set, Giza++ and the shallow syntax model achieved very similar performance.", "labels": [], "entities": []}, {"text": "We were unable to obtain evaluation numbers for Giza++/ReWrite on the large Arabic training set, however, since its language model component has a vocabulary size limit which was exceeded in the larger corpus.", "labels": [], "entities": []}, {"text": "In French we observed the systems to perform similarly on the small training sets we used (Results).", "labels": [], "entities": []}, {"text": "We performed some experiments in classifier combination using the two compatible (150K-training-sentence) Arabic systems, wherein a small devtest set was used to identify simple system combination parameters based on model confidence and sentence length.", "labels": [], "entities": [{"text": "classifier combination", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.905413031578064}]}, {"text": "In situations where our system was confident we used its output, and used Giza++ output otherwise.", "labels": [], "entities": []}, {"text": "We achieved a 3% boost in Bleu score over Giza++ performance on the evaluation set with these very simple classifier combination techniques, and anticipate that research in this direction -classifier combination of diversely trained SMT systems -could yield significant performance improvements.: Direct generation (word-to-word translation probabilities at the various levels of backoff) is contrasted with lemma generation.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9828575551509857}, {"text": "SMT", "start_pos": 233, "end_pos": 236, "type": "TASK", "confidence": 0.9693670868873596}, {"text": "Direct generation", "start_pos": 297, "end_pos": 314, "type": "TASK", "confidence": 0.6632806211709976}, {"text": "lemma generation", "start_pos": 408, "end_pos": 424, "type": "TASK", "confidence": 0.732163280248642}]}, {"text": "Manger (\"to eat\") is a relatively rare word in the Hansards.", "labels": [], "entities": [{"text": "Manger", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7690438628196716}, {"text": "Hansards", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8402741551399231}]}, {"text": "Note that due to low counts, the desired verb POS (target of generation) for \"eat\" may not have been observed as a translation in training data.", "labels": [], "entities": [{"text": "POS (target of generation)", "start_pos": 46, "end_pos": 72, "type": "METRIC", "confidence": 0.8787634074687958}]}, {"text": "In addition, in this situation, noisy word alignments may cause an incorrect translation to have similar estimated translation probability.", "labels": [], "entities": []}, {"text": "This problem is addressed by the lemma model; note the much sharper probability distribution for verb lemmas given manger.", "labels": [], "entities": []}, {"text": "Generation of English inflections given lemma and target POS is algorithmic (and irregular exceptions are handled via a lookup: Examples of word translation coercions.", "labels": [], "entities": [{"text": "word translation coercions", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.7882553736368815}]}, {"text": "Coercions of the French verb accepter \"to accept\" and the French noun droit \"right\" (there is parallel polysemy between the two languages for this word, but the predominant sense in our corpus is the philosophical/judicial sense, as opposed to the direction).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An Arabic translation from the test set. We revisit portions of this example throughout the text. All Arabic  strings in this paper are rendered in the reversible Buckwalter transliteration. In addition, all words or symbols referring  to Arabic and French in this paper are italicized.", "labels": [], "entities": []}, {"text": " Table 3: Top learned sentence-level reorderings for Ara- bic, for canonical Arabic simple sentence structure VP  (verb) NP (subject) NP (object). Subscripts in English  phrase sequence are alignments to positions in the corre- sponding Arabic phrase sequence.", "labels": [], "entities": []}]}