{"title": [{"text": "Cross-Lingual Lexical Triggers in Statistical Language Modeling", "labels": [], "entities": [{"text": "Cross-Lingual Lexical Triggers", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6413348019123077}, {"text": "Statistical Language Modeling", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.7513615687688192}]}], "abstractContent": [{"text": "We propose new methods to take advantage of text in resource-rich languages to sharpen statistical language models in resource-deficient languages.", "labels": [], "entities": []}, {"text": "We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihood-based adaptation scheme for combining a trigger model with an \u00a1-gram model.", "labels": [], "entities": []}, {"text": "We describe the application of such language models for automatic speech recognition.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.5946490466594696}]}, {"text": "By exploiting a side-corpus of con-temporaneous English news articles for adapting a static Chinese language model to transcribe Mandarin news stories, we demonstrate significant reductions in both perplexity and recognition errors.", "labels": [], "entities": []}, {"text": "We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via cross-lingual information retrieval and machine translation, proposed elsewhere.", "labels": [], "entities": [{"text": "monolingual language model adaptation", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.7003314793109894}, {"text": "machine translation", "start_pos": 200, "end_pos": 219, "type": "TASK", "confidence": 0.7292554527521133}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word-Perplexity and ASR WER of LMs  based on single English document and global  \u0093  .", "labels": [], "entities": [{"text": "ASR", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9271508455276489}, {"text": "WER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9273654222488403}]}, {"text": " Table 2: Perplexity and ASR Performance with a Likelihood-Based Story-Specific Selection of the Number  of English Documents  \u00a3", "labels": [], "entities": [{"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9829748272895813}]}]}