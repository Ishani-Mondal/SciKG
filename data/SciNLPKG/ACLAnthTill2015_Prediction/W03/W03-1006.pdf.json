{"title": [{"text": "Use of Deep Linguistic Features for the Recognition and Labeling of Semantic Arguments", "labels": [], "entities": [{"text": "Recognition and Labeling of Semantic Arguments", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.8509716093540192}]}], "abstractContent": [{"text": "We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.", "labels": [], "entities": []}, {"text": "We also show that predicting labels from a \"lightweight\" parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.", "labels": [], "entities": [{"text": "predicting labels", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.8946122527122498}]}], "introductionContent": [{"text": "Syntax mediates between surface word order and meaning.", "labels": [], "entities": []}, {"text": "The goal of parsing (syntactic analysis) is ultimately to provide the first step towards giving a semantic interpretation of a string of words.", "labels": [], "entities": [{"text": "syntactic analysis)", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7555974622567495}]}, {"text": "So far, attention has focused on parsing, because the semantically annotated corpora required for learning semantic interpretation have not been available.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9688947200775146}, {"text": "learning semantic interpretation", "start_pos": 98, "end_pos": 130, "type": "TASK", "confidence": 0.6421384712060293}]}, {"text": "The completion of the first phase of the PropBank () represents an important step.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.8854238390922546}]}, {"text": "The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.9656974673271179}]}, {"text": "The arc labels chosen for the arguments are specific to the predicate, not universal.", "labels": [], "entities": []}, {"text": "In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed).", "labels": [], "entities": []}, {"text": "Specifically, we show that the syntactic dependency structure that results from the extraction of a Tree Adjoining Grammar (TAG) from the PTB, and the features that accompany this structure, form a better basis for determining semantic role labels.", "labels": [], "entities": []}, {"text": "Crucially, the same structure is also produced when parsing with TAG.", "labels": [], "entities": []}, {"text": "We suggest that the syntactic representation chosen in the PTB is less well suited for semantic processing than the other, deeper syntactic representations.", "labels": [], "entities": [{"text": "PTB", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.845344066619873}, {"text": "semantic processing", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8459339737892151}]}, {"text": "In fact, this deeper representation expresses syntactic notions that have achieved a wide acceptance across linguistic frameworks, unlike the very particular surface-syntactic choices made by the linguists who created the PTB syntactic annotation rules.", "labels": [], "entities": []}, {"text": "The outline of this paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we introduce the PropBank and describe the problem of predicting semantic tags.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.8616034388542175}, {"text": "predicting semantic tags", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.902922789255778}]}, {"text": "Section 3 presents an overview of our work and distinguishes it from previous work.", "labels": [], "entities": []}, {"text": "Section 4 describes the method used to produce the TAGs that are the basis of our experiments.", "labels": [], "entities": []}, {"text": "Section 5 specifies how training and test data that are used in our experiments are derived from the PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 101, "end_pos": 109, "type": "DATASET", "confidence": 0.9624998569488525}]}, {"text": "Next, we give results on two sets of experiments.", "labels": [], "entities": []}, {"text": "Those that predict semantic tags given gold-standard linguistic information are described in Section 6.", "labels": [], "entities": []}, {"text": "Those that do prediction from raw text are described in Section 7.", "labels": [], "entities": []}, {"text": "Finally, in Section 8 we present concluding remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Error rates of models which label semantic  roles on gold-standard parses. Each model is based  on its own feature sets, with features coming from a  particular kind of extracted grammar.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of dependency parsing using  LDA on supertagged input for different kinds of ex- tracted grammar.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7344365119934082}]}, {"text": " Table 4: Evaluation of semantic argument recogni- tion on SEM-TAG corpus via supertag and LDA.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation of semantic argument recogni- tion on SYNT-TAG corpus via supertag and LDA.", "labels": [], "entities": []}]}