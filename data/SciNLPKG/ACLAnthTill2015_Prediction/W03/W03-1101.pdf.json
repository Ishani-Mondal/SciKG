{"title": [{"text": "Improving Summarization Performance by Sentence Compression - A Pilot Study", "labels": [], "entities": [{"text": "Improving Summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8695113956928253}]}], "abstractContent": [{"text": "In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summari-zation system.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7075890153646469}]}, {"text": "Our results show that pure syntactic-based compression does not improve system performance.", "labels": [], "entities": [{"text": "syntactic-based compression", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.7913292050361633}]}, {"text": "Topic signature based reranking of compressed sentences does not help much either.", "labels": [], "entities": []}, {"text": "However reranking using an oracle showed a significant improvement remains possible.", "labels": [], "entities": []}], "introductionContent": [{"text": "The majority of systems participating in the past Document Understanding Conference) (a large scale summarization evaluation effort sponsored by the United States government), and the Text Summarization Challenge) (sponsored by Japanese government) are extraction based.", "labels": [], "entities": [{"text": "Document Understanding Conference", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.49169806639353436}, {"text": "Text Summarization Challenge)", "start_pos": 184, "end_pos": 213, "type": "TASK", "confidence": 0.8349306732416153}]}, {"text": "Extraction-based automatic text summarization systems extract parts of original documents and output the results as summaries (.", "labels": [], "entities": [{"text": "Extraction-based automatic text summarization", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.6391182169318199}]}, {"text": "Other systems based on information extraction) and discourse analysis) also exist but they are not yet usable for general-domain summarization.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.736396998167038}, {"text": "discourse analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7418158799409866}, {"text": "general-domain summarization", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.5994574725627899}]}, {"text": "Our study focuses on the effectiveness of applying sentence compression techniques to improve the performance of extraction-based automatic text summarization systems.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7169368863105774}, {"text": "extraction-based automatic text summarization", "start_pos": 113, "end_pos": 158, "type": "TASK", "confidence": 0.6030506566166878}]}, {"text": "Sentence compression aims to retain the most salient information of a sentence, rewritten in a short form).", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9277441799640656}]}, {"text": "It can be used to deliver compressed content to portable devices () or as a reading aid for aphasic readers) or the blind.", "labels": [], "entities": []}, {"text": "Earlier research in sentence compression focused on compressing single sentences, and were evaluated on a sentence by sentence basis.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7210438102483749}]}, {"text": "For example, Jing (2000) trained her system on a set of 500 sentences from the Benton Foundation (http://www.benton.org) and their reduced forms written by humans.", "labels": [], "entities": [{"text": "Benton Foundation", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.8628437221050262}]}, {"text": "The results were evaluated at the parse tree level against the reduced trees; while Knight and Marcu (2000) trained their system on a set of 1,067 sentences from Ziff-Davis magazine articles and evaluated their results on grammaticality and importance rated by humans.", "labels": [], "entities": []}, {"text": "Both reported success in their evaluation criteria.", "labels": [], "entities": []}, {"text": "However, neither of them reported their techniques' effectiveness in improving the overall performance of automatic text summarization systems.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 106, "end_pos": 134, "type": "TASK", "confidence": 0.6200745602448782}]}, {"text": "The goal of this pilot study is set to answer this question and provide a guideline for future research.", "labels": [], "entities": []}, {"text": "Section 2 gives an overview of Knight and Marcu's sentence compression algorithm that we used to compressed summary sentences.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6873422563076019}]}, {"text": "Section 3 describes the multi-document summarization system, NeATS, which was used as our testbed.", "labels": [], "entities": [{"text": "NeATS", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8194966912269592}]}, {"text": "Section 4 introduces a recall-based unigram co-occurrence automatic evaluation metric.", "labels": [], "entities": [{"text": "recall-based", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.996442973613739}]}, {"text": "Section 5 presents the experimental design.", "labels": [], "entities": []}, {"text": "Section 6 shows the empirical results.", "labels": [], "entities": []}, {"text": "Section 7 concludes this paper and discusses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "As stated in the introduction, we aim to investigate the effectiveness of sentence compression on overall system performance.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.7351154685020447}]}, {"text": "If we can have a lossless compression function that compresses a given sentence to a minimal length and still retains the most important content of the sentence then we would be able to pack more information content into a fixed size summary.", "labels": [], "entities": []}, {"text": "illustrates this effect <multi size=\"225\" docset=\"d19d\" org-size=\"227\" comp-size=\"227\"> Lawmakers clashed on 06/23/1988 over the question of counting illegal aliens in the 1990 Census, debating whether following the letter of the Constitution results in a system that is unfair to citizens.", "labels": [], "entities": []}, {"text": "The forum was a Census subcommittee hearing on bills which would require the Census Bureau to figure out whether people are in the country legally and, if not, to delete them from the counts used in reapportioning seats in the House of Representatives.", "labels": [], "entities": []}, {"text": "Simply put, the question was who should be counted as a person and who, if anybody, should not.", "labels": [], "entities": []}, {"text": "The point at issue in Senate debate on anew immigration bill was whether illegal aliens should be counted in the process that will reallocate House seats among states after the 1990 census.", "labels": [], "entities": []}, {"text": "The national headcount will betaken.", "labels": [], "entities": [{"text": "national headcount", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9231142103672028}]}, {"text": "Ina blow to California and other states with large immigrant populations, the Senate voted on 09/ to bar the Census Bureau from counting illegal aliens in the 1990 population count.", "labels": [], "entities": []}, {"text": "At stake are the number of seats in Congress for California, Florida, New York, Illinois, Pennsylvania and other states that will be reapportioned on the basis of next year's census.", "labels": [], "entities": []}, {"text": "Federal aid to states also is frequently based on population counts, so millions of dollars in grants and other funds made available on a per capita basis would be affected.", "labels": [], "entities": []}, {"text": "227-word summary for topic D19 (\"Aliens\").", "labels": [], "entities": []}, {"text": "<multi size=\"225\" docset=\"d19d\" org-size=\"227\" comp-size=\"98\"> Lawmakers clashed over question of counting illegal aliens Census debating whether results.", "labels": [], "entities": []}, {"text": "Forum was a Census hearing, to delete them from the counts.", "labels": [], "entities": [{"text": "Forum", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9650242924690247}, {"text": "Census hearing", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.8806857466697693}]}, {"text": "Simply put question was who should be counted and who, if anybody, should not.", "labels": [], "entities": []}, {"text": "Point at issue in debate on an immigration bill was whether illegal aliens should be counted.", "labels": [], "entities": []}, {"text": "Senate voted to bar Census Bureau from counting illegal aliens.", "labels": [], "entities": []}, {"text": "At stake are number of seats for California New York.", "labels": [], "entities": []}, {"text": "Aid to states is frequently based on population counts, so millions would be affected.", "labels": [], "entities": []}, {"text": "Compressed summary for topic D19 (\"Aliens\"), 98 words.", "labels": [], "entities": [{"text": "Compressed", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9328906536102295}]}, {"text": "<DOC> <TEXT> <S SNTNO=\"1\">Elizabeth Taylor battled pneumonia at her hospital, assisted by a ventilator, doctors say.</S> <S SNTNO=\"2\">Hospital officials described her condition late Monday as stabilizing after a lung biopsy to determine the cause of the pneumonia.</S> <S SNTNO=\"3\">Analysis of the tissue sample was expected to be complete by Thursday.</S> <S SNTNO=\"4\">Ms.", "labels": [], "entities": [{"text": "TEXT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9555990695953369}]}, {"text": "A manual summary for document AP900424-0035. graphically.", "labels": [], "entities": [{"text": "AP900424-0035", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.5458853840827942}]}, {"text": "For document AP900424-0035, which consists of 23 sentences or 417 words, we generate the full permutation set of sentence extracts, i.e., all possible 100\u00b15, 150\u00b15, and 200\u00b15 words extracts.", "labels": [], "entities": [{"text": "AP900424-0035", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.7909756302833557}]}, {"text": "The 100\u00b15 words extract at average compression ratio of 0.76 has most of its unigram co-occurrence score instances falling within the interval between 0.40 and 0.50, i.e., the expected performance of an extraction-based system would be between 0.40 and 0.50.", "labels": [], "entities": []}, {"text": "The 150\u00b15 words extract at lower compression ratio of 0.64 has most of its instances between 0.50 and 0.60 (115,240/377,933 \u2248 30%) and the 200\u00b15 words extract at compression ratio of 0.52 has most of its instances between 0.70 and 0.80 (212,116/731,819 \u2248 29%).", "labels": [], "entities": []}, {"text": "If we can compress 150 or 200-word summaries into 100 words and retain their important content, we would be to achieve an average 30% to 50% increase in performance.", "labels": [], "entities": []}, {"text": "The question is: can an off-the-shelf sentence compression algorithm such as K&M's noisychannel model achieve this?", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.763435572385788}]}, {"text": "If the answer is yes, then how much performance gain can be achieved?", "labels": [], "entities": []}, {"text": "If not, are there other ways to use sentence compression to improve system performance?", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7208996564149857}]}, {"text": "To answer these questions, we conduct the following experiments over 30 DUC 2001 topic sets: (1) Run NeATS through the 30 DUC 2001 topic sets and generate summaries of size: Run K&M's sentence compression algorithm overall summary sentences (run KM).", "labels": [], "entities": [{"text": "DUC 2001 topic sets", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.9686915725469589}, {"text": "DUC 2001 topic sets", "start_pos": 122, "end_pos": 141, "type": "DATASET", "confidence": 0.9672770947217941}, {"text": "sentence compression", "start_pos": 184, "end_pos": 204, "type": "TASK", "confidence": 0.6558142751455307}]}, {"text": "For each summary sentence, we have a set of candidate compressions.", "labels": [], "entities": []}, {"text": "Rerank each candidate compression set using different scoring methods: a.", "labels": [], "entities": []}, {"text": "Rerank each candidate compression set using topic signatures (run SIG). b. Rerank each candidate compression set using combination of KM and SIG scores using linear interpolation of topic signature score (SIG) and K&M's log-probability score (KM).", "labels": [], "entities": [{"text": "K&M's log-probability score (KM)", "start_pos": 214, "end_pos": 246, "type": "METRIC", "confidence": 0.7346688508987427}]}, {"text": "We use the following formula in this experiment:  Select the best compression combination.", "labels": [], "entities": []}, {"text": "For a given length constraint, for example 100 words, we produce the final result by selecting a compressed summary across different summary sizes for each topic that fits the length limit (<= 100\u00b15 words), and output them as the final summary.", "labels": [], "entities": []}, {"text": "For example, we found that a 227-word summary for topic D19 could be compressed to 98 words using the topic signature reranking method.", "labels": [], "entities": []}, {"text": "The compressed summary would then be selected as the final summary for topic D19.", "labels": [], "entities": []}, {"text": "shows the original 227-word summary and shows its compressed version.", "labels": [], "entities": []}, {"text": "There were 30 test topics in DUC 2001 and each topic contained about 10 documents.", "labels": [], "entities": [{"text": "DUC 2001", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9446689784526825}]}, {"text": "For each topic, four summaries of approximately 50, 100, 200, and 400 words were created manually as the 'ideal' model summaries.", "labels": [], "entities": []}, {"text": "We used the set of 100-word manual summaries as our references in our experiments.", "labels": [], "entities": []}, {"text": "An example manual summary is shown in.", "labels": [], "entities": []}, {"text": "We report results of these experiments in the next section.", "labels": [], "entities": []}, {"text": "Analyzing all runs according to these two tables, we made the following observations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Result table for six runs. Avg: mean unigram co-occurrence scores of  30 topics, Var: variance, Std: standard deviation, AvgCR: mean compression  ratio, VarCR: variance of compression ratio, and StdCR: standard deviation of  compression ratio.", "labels": [], "entities": [{"text": "Avg", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.993894636631012}, {"text": "Var: variance", "start_pos": 91, "end_pos": 104, "type": "METRIC", "confidence": 0.9233556588490804}, {"text": "AvgCR: mean compression  ratio", "start_pos": 131, "end_pos": 161, "type": "METRIC", "confidence": 0.9012394666671752}, {"text": "VarCR: variance of compression ratio", "start_pos": 163, "end_pos": 199, "type": "METRIC", "confidence": 0.8631919225056967}]}, {"text": " Table 2. Pairwise Z-test for six runs shown in Table 1 (\u03b1 = 5%). Light gray  (green) indicates runs on the column that are significantly better than runs on  the row; dark gray indicates significantly worse.", "labels": [], "entities": [{"text": "Pairwise Z-test", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.822809249162674}]}]}