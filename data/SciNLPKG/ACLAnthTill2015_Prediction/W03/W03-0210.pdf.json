{"title": [{"text": "A Hybrid Text Classification Approach for Analysis of Student Essays", "labels": [], "entities": [{"text": "Hybrid Text Classification Approach", "start_pos": 2, "end_pos": 37, "type": "TASK", "confidence": 0.6794014126062393}, {"text": "Analysis of Student Essays", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.8941637873649597}]}], "abstractContent": [{"text": "We present CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Ros\u00e9 et al., 2002a).", "labels": [], "entities": [{"text": "analyzing essay answers to qualitative physics questions", "start_pos": 69, "end_pos": 125, "type": "TASK", "confidence": 0.7075624891689846}]}, {"text": "CarmelTC learns to classify units of text based on features extracted from a syntactic analysis of that text as well as on a Naive Bayes classification of that text.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9122002124786377}]}, {"text": "We explore the trade-offs between symbolic and \"bag of words\" approaches.", "labels": [], "entities": []}, {"text": "Our goal has been to combine the strengths of both of these approaches while avoiding some of the weaknesses.", "labels": [], "entities": []}, {"text": "Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two \"bag of words\" approaches , namely LSA and a Naive Bayes, as well as a purely symbolic approach.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.570500373840332}]}], "introductionContent": [{"text": "In this paper we describe CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.6481994986534119}, {"text": "analyzing essay answers to qualitative physics questions", "start_pos": 84, "end_pos": 140, "type": "TASK", "confidence": 0.6687219398362296}]}, {"text": "In our evaluation we demonstrate that the novel hybrid CarmelTC approach outperforms both Latent Semantic Analysis (LSA) and Rainbow, which is a Naive Bayes approach, as well as a purely symbolic approach similar to ().", "labels": [], "entities": []}, {"text": "Whereas LSA and Rainbow are pure \"bag of words\" approaches, CarmelTC is a rule learning approach where rules for classifying units of text rely on features extracted from a syntactic analysis of that text as well as on a \"bag of words\" classification of that text.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8150840401649475}]}, {"text": "Thus, our evaluation demonstrates the advantage of combining predictions from symbolic and \"bag of words\" approaches for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.812829852104187}]}, {"text": "Similar to (), neither CarmelTC nor the purely symbolic approach require any domain specific knowledge engineering or text annotation beyond providing a training corpus of texts matched with appropriate classifications, which is also necessary for Rainbow, and to a much lesser extent for LSA.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8375656008720398}, {"text": "Rainbow", "start_pos": 248, "end_pos": 255, "type": "DATASET", "confidence": 0.8682835698127747}, {"text": "LSA", "start_pos": 289, "end_pos": 292, "type": "DATASET", "confidence": 0.7337284684181213}]}, {"text": "CarmelTC was developed for use inside of the Why2-Atlas conceptual physics tutoring system () for the purpose of grading short essays written in response to questions such as \"Suppose you are running in a straight line at constant speed.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9176205396652222}, {"text": "Why2-Atlas conceptual physics tutoring system", "start_pos": 45, "end_pos": 90, "type": "DATASET", "confidence": 0.8212886810302734}]}, {"text": "You throw a pumpkin straight up.", "labels": [], "entities": []}, {"text": "This is an appropriate task domain for pursuing questions about the benefits of tutorial dialogue for learning because questions like this one are known to elicit robust, persistent misconceptions from students, such as \"heavier objects exert more force.\".", "labels": [], "entities": []}, {"text": "In Why2-Atlas, a student first types an essay answering a qualitative physics problem.", "labels": [], "entities": []}, {"text": "A computer tutor then engages the student in a natural language dialogue to provide feedback, correct misconceptions, and to elicit more complete explanations.", "labels": [], "entities": []}, {"text": "The first version of Why2-Atlas was deployed and evaluated with undergraduate students in the spring of 2002; the system is continuing to be actively developed ().", "labels": [], "entities": [{"text": "Why2-Atlas", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8949922919273376}]}, {"text": "In contrast to many previous approaches to automated essay grading (, our goal is not to assign a letter grade to student essays.", "labels": [], "entities": []}, {"text": "Instead, our purpose is to tally which set of \"correct answer aspects\" are present in student essays.", "labels": [], "entities": []}, {"text": "For example, we expect satisfactory answers to the example question above to include a detailed explanation of how Newton's first law applies to this scenario.", "labels": [], "entities": []}, {"text": "From Newton's first law, the student should infer that the pumpkin and the man will continue at the same constant horizontal velocity that they both had before the release.", "labels": [], "entities": []}, {"text": "Thus, they will always have the same displacement from the point of release.", "labels": [], "entities": []}, {"text": "Therefore, after the pumpkin rises and falls, it will land back in the man's hands.", "labels": [], "entities": []}, {"text": "Our goal is to coach students through the process of constructing good physics explanations.", "labels": [], "entities": []}, {"text": "Thus, our focus is on the physics content and not the quality of the student's writing, in contrast to ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted an evaluation to compare the effectiveness of CarmelTC at analyzing student essays in comparison to LSA, Rainbow, and a purely symbolic approach similar to (), which we refer to here as CarmelTCsymb.", "labels": [], "entities": [{"text": "Rainbow", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.8840581774711609}]}, {"text": "CarmelTCsymb is identical to CarmelTC except that it does not include in its feature set the prediction from Rainbow.", "labels": [], "entities": [{"text": "CarmelTCsymb", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9102460145950317}, {"text": "CarmelTC", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9557341933250427}]}, {"text": "Thus, by comparing CarmelTC with Rainbow and LSA, we can demonstrate the superiority of our hybrid approach to purely \"bag of words\" approaches.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9508348107337952}, {"text": "Rainbow", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9187012910842896}]}, {"text": "And by comparing with CarmelTCsymb, we can demonstrate the superiority of our hybrid approach to an otherwise equivalent purely symbolic approach.", "labels": [], "entities": [{"text": "CarmelTCsymb", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.956351637840271}]}, {"text": "We conducted our evaluation over a corpus of 126 previously unseen student essays in response to the Pumpkin Problem described above, with a total of 500 text segments, and just under 6000 words altogether.", "labels": [], "entities": []}, {"text": "We first tested to see if the text segments could be reliably tagged by humans with the six possible Classes associated with the problem.", "labels": [], "entities": []}, {"text": "Note that this includes \"nothing\" as a class, i.e., Class 6.", "labels": [], "entities": []}, {"text": "Three human coders hand classified text segments for 20 essays.", "labels": [], "entities": []}, {"text": "We computed a pairwise Kappa coefficient to measure the agreement between coders, which was always greater than .75, thus demonstrating good agreement according to the Krippendorf scale.", "labels": [], "entities": [{"text": "agreement", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9652978181838989}]}, {"text": "We then selected two coders to individually classify the remaining sentences in the corpus.", "labels": [], "entities": []}, {"text": "They then met to come to a consensus on the tagging.", "labels": [], "entities": [{"text": "tagging", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9573621153831482}]}, {"text": "The resulting consensus tagged corpus was used as a gold standard for this evaluation.", "labels": [], "entities": []}, {"text": "Using this gold standard, we conducted a comparison of the four approaches on the problem of tallying the set of \"correct answer aspects\" present in each student essay.", "labels": [], "entities": []}, {"text": "The LSA space used for this evaluation was trained over three first year physics text books.", "labels": [], "entities": [{"text": "LSA space", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8495454490184784}]}, {"text": "The other three approaches are trained over a corpus of tagged examples using a 50 fold random sampling evaluation, similar to a cross-validation methodology.", "labels": [], "entities": []}, {"text": "On each iteration, we randomly selected a subset of essays such that the number of text segments included in the test set were greater than 10 but less than 15.", "labels": [], "entities": []}, {"text": "The randomly selected essays were then used as a test set for that iteration, and the remainder of the essays were used for training in addition to a corpus of 248 hand tagged example sentences extracted from a corpus of human-human tutoring transcripts in our domain.", "labels": [], "entities": []}, {"text": "The training of the three approaches differed only in terms of how the training data was partitioned.", "labels": [], "entities": []}, {"text": "Rainbow and CarmelTCsymb were trained using all of the example sentences in the corpus as a single training set.", "labels": [], "entities": [{"text": "CarmelTCsymb", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.8821887969970703}]}, {"text": "CarmelTC, on the other hand, required partitioning the training data into two subsets, one for training the Rainbow model used for generating the value of its Rainbow feature, and one subset for training the decision trees.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9550881385803223}]}, {"text": "This is because for CarmelTC, the data for training Rainbow must be separate from that used to train the decision trees so the decision trees are trained from a realistic distribution of assigned Rainbow classes based on its performance on unseen data rather than on Rainbow's training data.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.8085384964942932}]}, {"text": "In setting up our evaluation, we made it our goal to present our competing approaches in the best possible light in order to provide CarmelTC with the strongest competitors as possible.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.8739381432533264}]}, {"text": "Note that LSA works by using its trained LSA space to construct a vector representation for any text based on the set of words included therein.", "labels": [], "entities": []}, {"text": "It can thus be used for text classification by comparing the vector obtained fora set of exemplar texts for each class with that obtained from the text to be classified.", "labels": [], "entities": [{"text": "text classification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8031516671180725}]}, {"text": "We tested LSA using as exemplars the same set of examples used  as Rainbow training data, but it always performed better when using a small set of hand picked exemplars.", "labels": [], "entities": [{"text": "LSA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6710413098335266}, {"text": "Rainbow training data", "start_pos": 67, "end_pos": 88, "type": "DATASET", "confidence": 0.7571083903312683}]}, {"text": "Thus, we present results here using only those hand picked exemplars.", "labels": [], "entities": []}, {"text": "For every approach except LSA, we first segmented the essays at sentence boundaries and classified each sentence separately.", "labels": [], "entities": []}, {"text": "However, for LSA, rather than classify each segment separately, we compared the LSA vector for the entire essay to the exemplars for each class (other than \"nothing\"), since LSA's performance is better with longer texts.", "labels": [], "entities": []}, {"text": "We verified that LSA also performed better specifically on our task under these circumstances.", "labels": [], "entities": [{"text": "LSA", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.4473467171192169}]}, {"text": "Thus, we compared each essay to each exemplar, and we counted LSA as identifying the corresponding \"correct answer aspect\" if the cosine value obtained by comparing the two vectors was above a threshold.", "labels": [], "entities": [{"text": "LSA", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9883698225021362}]}, {"text": "We tested LSA with threshold values between .1 and .9 at increments of .1 as well as testing a threshold of .53 as is used in the AUTO-TUTOR system).", "labels": [], "entities": [{"text": "LSA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8730891346931458}, {"text": "AUTO-TUTOR", "start_pos": 130, "end_pos": 140, "type": "DATASET", "confidence": 0.7309187650680542}]}, {"text": "As expected, as the threshold increases from .1 to .9, recall and false alarm rate both decrease together as precision increases.", "labels": [], "entities": [{"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9996302127838135}, {"text": "false alarm rate", "start_pos": 66, "end_pos": 82, "type": "METRIC", "confidence": 0.8560607433319092}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9994638562202454}]}, {"text": "We determined based on computing f-scores 2 for each threshold level that .53 achieves the best trade off between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9993937015533447}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9948351383209229}]}, {"text": "Thus, we used a threshold of .53, to determine whether LSA identified the corresponding key point in the student essay or not for the evaluation presented here.", "labels": [], "entities": []}, {"text": "We evaluated the four approaches in terms of precision, recall, false alarm rate, and f-score, which were computed for each approach for each test essay, and then averaged over the whole set of test essays.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9996975660324097}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9991400241851807}, {"text": "false alarm rate", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.9625627199808756}, {"text": "f-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9847261309623718}]}, {"text": "We computed precision by dividing the number of \"correct answer aspects\" (CAAs) correctly identified by the total number of CAAs identified We computed recall by dividing the number of CAAs correctly identified over the number of CAAs actually present in the essay 4 False alarm rate was computed by dividing the number of CAAs incorrectly identified by the total number of CAAs that could potentially be incor-rectly identified 5 . F-scores were computed using 1 as the beta value in order to treat precision and recall as equally important.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9980930685997009}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9976375102996826}, {"text": "False alarm rate", "start_pos": 267, "end_pos": 283, "type": "METRIC", "confidence": 0.9596108396848043}, {"text": "F-scores", "start_pos": 433, "end_pos": 441, "type": "METRIC", "confidence": 0.9969877600669861}, {"text": "precision", "start_pos": 500, "end_pos": 509, "type": "METRIC", "confidence": 0.9983488321304321}, {"text": "recall", "start_pos": 514, "end_pos": 520, "type": "METRIC", "confidence": 0.9956510663032532}]}, {"text": "The results presented in clearly demonstrate that CarmelTC outperforms the other approaches.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.6042389869689941}]}, {"text": "In particular, CarmelTC achieves the highest f-score, which combines the precision and recall scores into a single measure.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.45271700620651245}, {"text": "f-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9818241000175476}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9993487000465393}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9909929633140564}]}, {"text": "In comparison with CarmelTCsymb, CarmelTC achieves a higher recall as well as a slightly higher precision.", "labels": [], "entities": [{"text": "CarmelTCsymb", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9170802235603333}, {"text": "CarmelTC", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.8471982479095459}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9997033476829529}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9986373782157898}]}, {"text": "While LSA achieves a slightly higher precision, its recall is much lower.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9994000196456909}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9998205304145813}]}, {"text": "Thus, the difference between the two approaches is clearly shown in the fscore value, which strongly favors CarmelTC.", "labels": [], "entities": [{"text": "fscore", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.932104229927063}, {"text": "CarmelTC", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.8695787787437439}]}, {"text": "Rainbow achieves a lower score than CarmelTC in terms of precision, recall, false alarm rate, and f-score.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.659446120262146}, {"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9997542500495911}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9996849298477173}, {"text": "false alarm rate", "start_pos": 76, "end_pos": 92, "type": "METRIC", "confidence": 0.9390464623769125}, {"text": "f-score", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9939016103744507}]}], "tableCaptions": []}