{"title": [], "abstractContent": [{"text": "In this paper we attempt to apply the IBM algorithm, BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9945259690284729}]}, {"text": "The objective of this experiment is to explore whether a metric, originally developed for the evaluation of machine translation output, could be used for assessing another type of output reliably.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.705343077580134}]}, {"text": "Changing the type of text to be evaluated by BLEU into automatically generated extracts and setting the conditions and parameters of the evaluation experiment according to the idiosyncrasies of the task, we put the feasibility of porting BLEU in different Natural Language Processing research areas under test.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9483340382575989}, {"text": "porting BLEU", "start_pos": 230, "end_pos": 242, "type": "TASK", "confidence": 0.6641412824392319}]}, {"text": "Furthermore, some important conclusions relevant to the resources needed for evaluating summaries have come up as a side-effect of running the whole experiment .", "labels": [], "entities": [{"text": "summaries", "start_pos": 88, "end_pos": 97, "type": "TASK", "confidence": 0.9172008633613586}]}], "introductionContent": [{"text": "Machine Translation and Automatic Summarization are two very different Natural Language Processing (NLP) tasks with -among others-different implementation needs and goals.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.862956702709198}, {"text": "Automatic Summarization", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7161356508731842}]}, {"text": "They both aim at generating text; however, the properties and characteristics of these target texts vary considerably.", "labels": [], "entities": []}, {"text": "Simply put, in Machine Translation, the generated document should bean accurate and fluent translation of the original document, in the target language.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7910797297954559}]}, {"text": "In Summarization, the generated text should bean informative, reduced version of the original document (single-document summary), or sets of documents (multi-document summary) in the form of an abstract, or an extract.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.9780083298683167}]}, {"text": "Abstracts present an overview of the main points expressed in the original document, while extracts consist of a number of informative sentences taken directly from the source document.", "labels": [], "entities": [{"text": "Abstracts", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8636349439620972}]}, {"text": "The fact that, by their very nature, automatically generated extracts carry the single sentence qualities of the source documents 1 , may lead one to the conclusion that evaluating this type of text is trivial, as compared to the evaluation of abstracts or even machine translation, since in the latter, one needs to be able to evaluate the content of the generated translation in terms of grammaticality, semantic equivalence to the source document and other quality characteristics ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 262, "end_pos": 281, "type": "TASK", "confidence": 0.7578073143959045}]}, {"text": "The nature of this disagreement on the adequacy of the extracts is such that -by definition -cannot manifest itself in Machine Translation; this is because it refers to the adequacy of the contents chosen to form the extract, rather than what constitutes an adequate way of expressing all the contents of the source document in a target language.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7594133615493774}]}, {"text": "The difference on the parameters to betaken into consideration when performing evaluation within these two NLP tasks presents a challenge for porting evaluation metrics from the one research area to the other.", "labels": [], "entities": []}, {"text": "Given the relatively recent success in achieving high correlations with human judgement for Machine Translation evaluation, using the IBM content-based evaluation metric, BLEU (), we attempt to run this same metric on system generated extracts; this way we explore whether BLEU can be used reliably in this research area and if so, which testing parameters need to betaken into consideration.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.8981701135635376}, {"text": "BLEU", "start_pos": 171, "end_pos": 175, "type": "METRIC", "confidence": 0.9969581365585327}, {"text": "BLEU", "start_pos": 273, "end_pos": 277, "type": "METRIC", "confidence": 0.9796929359436035}]}, {"text": "First, we refer briefly to BLEU and its use across different NLP areas, then we locate our experiments relatively to this related work and we describe the resources we used, the tools we developed and the parameters we set for running the experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9943497776985168}]}, {"text": "The description of these experiments and the interpretation of the results follows.", "labels": [], "entities": []}, {"text": "The paper concludes with some preliminary observations we make as a result of this restricted, first experimentation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will present a description of the experiments themselves, along with the results obtained and their analysis, preceded by information on the corpus we used for our experiments and the tools we developed for setting their parameters and running them automatically.", "labels": [], "entities": []}, {"text": "We have developed a number of software components to facilitate the evaluation and we make use of the GATE development environment for testing and processing.", "labels": [], "entities": [{"text": "GATE development environment", "start_pos": 102, "end_pos": 130, "type": "DATASET", "confidence": 0.7475884358088175}]}, {"text": "The evaluation package allows the user to specify different reference extracts (judgebased summarizers) and summarization systems to be compared.", "labels": [], "entities": [{"text": "summarization", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9579347968101501}]}, {"text": "Co-selection comparison (i.e., precision and recall) is being done with modules obtained from the GATE library (AnnotationDiff components).", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9992596507072449}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9984143972396851}, {"text": "GATE library", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.918942391872406}]}, {"text": "Content-based comparison by the Bleu algorithm was implemented as a Java class.", "labels": [], "entities": [{"text": "Content-based comparison", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5767207741737366}]}, {"text": "The exact formula provided by the developers of BLEU has been implemented following the baseline configurations i.e use of 4-grams and uniform weights summing to 1: where Sand R are the system and reference sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.8312988877296448}]}, {"text": "Si and R i are the \"bags\" of i-grams for system and reference.", "labels": [], "entities": []}, {"text": "n is a parameter of our implementation, but for the purpose of our experiments we have set n to 4.", "labels": [], "entities": []}, {"text": "In our experiments we have treated compression rates and clusters as variables each one being a condition for the other and both dependent to a third variable, the gold standard summary.", "labels": [], "entities": []}, {"text": "We ran BLEU in all different combinations in order to seethe main effects of each combination and the interactions among them.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9967789053916931}]}, {"text": "In particular, we have used three different text clusters, consisting of texts that refer to the same topic: cluster 1197 on \"Museum exhibits and hours\", cluster 125 which deals with \"Narcotics and rehabilitation\" and cluster 241 which refers to \"Fire safety and building management\".", "labels": [], "entities": [{"text": "Narcotics and rehabilitation", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.8992307384808859}, {"text": "Fire safety and building management", "start_pos": 247, "end_pos": 282, "type": "TASK", "confidence": 0.6224788427352905}]}, {"text": "For the texts of each cluster we have three different reference summaries (created according to the utility judgement score assigned by human evaluators cf. 3.1 and 3.2).", "labels": [], "entities": []}, {"text": "We will refer to these as Reference1, Reference2 and Reference3.", "labels": [], "entities": []}, {"text": "The judges behind these references are all the same for the three text clusters with one exception: Reference1 in cluster 241 has not been created by the same human evaluator as the Reference 1 summaries for the other two clusters.", "labels": [], "entities": []}, {"text": "Last, we ran the experiments at five different compression rates 4 : 10%, 20%, 30%, 40% and 50%.", "labels": [], "entities": [{"text": "compression", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9714816212654114}]}, {"text": "We first ran BLEU on the reference summaries in order to check whether BLEU is consistent in the data it produces concerning the agreement among human evaluators.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9962223768234253}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.996488094329834}]}, {"text": "We tried all possible combinations for comparing the reference summaries; using at first Reference 1 as the gold standard, we ran BLEU over References 2 and 3 and we did this for two clusters (since the third's -241-Reference 1 set of summaries had been created by another judge -a fourth one).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9982498288154602}]}, {"text": "We did this for all five compression rates separately.", "labels": [], "entities": []}, {"text": "We repeated the experiment changing the gold standard and the references to be scored accordingly (i.e Reference 1 and 3 against 2, Reference 1 and 2 against 3).", "labels": [], "entities": []}, {"text": "The results we got were consistent neither across clusters, nor within clusters across compression rates; however the latter, did show a general tendency for consistency which allows for some observations to be made.", "labels": [], "entities": [{"text": "consistency", "start_pos": 158, "end_pos": 169, "type": "METRIC", "confidence": 0.9775042533874512}]}, {"text": "In cluster 1197, References 1 and 2 are generally in higher agreement than with 3, a fact verified regardless the reference chosen as a gold standard.", "labels": [], "entities": []}, {"text": "The fact that References 1 and 2 are very close was also evident when both compared against Reference 3; though the latter is generally closer to Reference 2, the scores assigned to Reference 1 and 2 are extremely close.", "labels": [], "entities": []}, {"text": "In cluster 125, Reference 1 is consistently closer to 3, while 2 is closer to 1 at some compression rates and closer to 3 at others.", "labels": [], "entities": []}, {"text": "These very close scores indicate that all three references are similarly \"distant\" one from another, and no groupings of agreement can actually be made.", "labels": [], "entities": []}, {"text": "Agreement between reference summaries augments as the compression rate also increases, with the higher similarity scores always found at the 50% compression rate and the lower ones consistently found at 10%.", "labels": [], "entities": [{"text": "compression rate", "start_pos": 54, "end_pos": 70, "type": "METRIC", "confidence": 0.9749676585197449}, {"text": "compression rate", "start_pos": 145, "end_pos": 161, "type": "METRIC", "confidence": 0.9535960853099823}]}, {"text": "shows a consistent ranking across compression rates in cluster 1197 and an inconsistent one in cluster 125, using in both cases Reference 2 as the gold standard.", "labels": [], "entities": []}, {"text": "From this first experiment, the rankings of In our experiments compression is always performed at the sentence level the reference summaries seem to depend on the different values of the variables used.", "labels": [], "entities": []}, {"text": "If that is the case, then one should use BLEU in summarization only when determining specific values for the evaluation experiment, that will guarantee reliable results; but how could one determine which value(s) should be chosen?", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9989221096038818}]}, {"text": "To explore things further we decided to proceed with a second experiment setup in a similar way.", "labels": [], "entities": []}, {"text": "In our second experiment we try to compare the system generated extracts (and therefore the performance of the four summarizers) against the different human references.", "labels": [], "entities": []}, {"text": "Again, the different rounds of the experiment involve multiple parameters; the generated extracts of all three text clusters are compared against each reference summary, against all reference summaries (integrated summary) and at all five compression rates.", "labels": [], "entities": []}, {"text": "Going through the different stages of this experiment we observe that: \u2022 For Reference X within Cluster Y across Compressions, the ranking of the systems is not consistent One does not get the same system ranking at different compression rates.", "labels": [], "entities": []}, {"text": "The similarity of a generated extract to a specific reference summary is the same at some compression rates, similar at others (e.g the order of two of the systems swaps) and totally different at other rates.", "labels": [], "entities": []}, {"text": "No patterns arise in the way that rankings are similar at specific compression rates; for example, in table 2, there seems to be a prevailing ranking common in four compression rates; however, the ranking provided at 10% is totally different, and no apparent reason seems to justify this deviation (e.g. very close scores).", "labels": [], "entities": []}, {"text": "Furthermore, this agreement among the four highest compression rates does not form a pattern i.e it does not appear as such across clusters or references.", "labels": [], "entities": []}, {"text": "\u2022 For Reference X at Compression Y across Clusters, the ranking of the systems is not consistent In our experiments we were able to observe 15 different realisations of these testing configurations and hardly did a case of consistency at a compression rate across clusters appeared.", "labels": [], "entities": []}, {"text": "presents the scores and corresponding system rankings for two different clusters and at the five different compression rates.", "labels": [], "entities": []}, {"text": "The prevailing system ranking is, which is what we would intuitively expect according to the features of the summarizers we compare.", "labels": [], "entities": []}, {"text": "Some deviations from this ranking are due to very small differences in the similarity scores assigned to the systems 5 , which indicates the need for using a larger testing corpus for the experiments.", "labels": [], "entities": [{"text": "similarity", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9626665711402893}]}, {"text": "So, the need for multiple references is evident; BLEU is a consistent, reliable metric, but when used in summarization, one has to apply it to multiple references in order to get reliable results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.998824417591095}, {"text": "summarization", "start_pos": 105, "end_pos": 118, "type": "TASK", "confidence": 0.9811718463897705}]}, {"text": "This is not just away to improve correlation with human judgement (); it is a crucial evaluation parameter that affects the quality of the automatic evaluation results.", "labels": [], "entities": []}, {"text": "In our case we had a balanced set of reference summaries to work with, i.e none of them was too similar to another.", "labels": [], "entities": []}, {"text": "The more reference summaries one has and the larger one's testing corpus, the safer the conclusions drawn will be.", "labels": [], "entities": []}, {"text": "However, what happens when there is lack of such resources and especially", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: System scores and rankings for cluster 241, against Reference 3, at different compression rates", "labels": [], "entities": []}, {"text": " Table 3: Systems' similarity scores and rankings using Reference All as gold standard", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 19, "end_pos": 36, "type": "METRIC", "confidence": 0.9512131214141846}, {"text": "Reference All", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.8353296518325806}]}, {"text": " Table 4: Systems' average rankings resulting from ranks at multiple compression rates in clusters 125 and  1197. (Systems assumed to be listed in alphabetical order: Query-based, Simple1, Simple2, Simple3)", "labels": [], "entities": []}]}