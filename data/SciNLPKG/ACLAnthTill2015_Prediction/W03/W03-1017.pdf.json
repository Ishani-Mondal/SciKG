{"title": [{"text": "Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences", "labels": [], "entities": [{"text": "Answering Opinion Questions", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.901649554570516}, {"text": "Identifying the Polarity of Opinion Sentences", "start_pos": 72, "end_pos": 117, "type": "TASK", "confidence": 0.6596659123897552}]}], "abstractContent": [{"text": "Opinion question answering is a challenging task for natural language processing.", "labels": [], "entities": [{"text": "Opinion question answering", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8560425043106079}, {"text": "natural language processing", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6511058608690897}]}, {"text": "In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.", "labels": [], "entities": [{"text": "opinion question answering", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.6507510642210642}]}, {"text": "We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.", "labels": [], "entities": []}, {"text": "We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion.", "labels": [], "entities": []}, {"text": "Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).", "labels": [], "entities": [{"text": "document classification", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.689462885260582}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9983507394790649}, {"text": "recall", "start_pos": 208, "end_pos": 214, "type": "METRIC", "confidence": 0.9964455962181091}, {"text": "accuracy", "start_pos": 355, "end_pos": 363, "type": "METRIC", "confidence": 0.9947815537452698}]}], "introductionContent": [{"text": "Newswire articles include those that mainly present opinions or ideas, such as editorials and letters to the editor, and those that mainly report facts such as daily news articles.", "labels": [], "entities": [{"text": "Newswire", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9134219884872437}]}, {"text": "Text materials from many other sources also contain mixed facts and opinions.", "labels": [], "entities": []}, {"text": "For many natural language processing applications, the ability to detect and classify factual and opinion sentences offers distinct advantages in deciding what information to extract and how to organize and present this information.", "labels": [], "entities": []}, {"text": "For example, information extraction applications may target factual statements rather than subjective opinions, and summarization systems may list separately factual information and aggregate opinions according to distinct perspectives.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7681323289871216}, {"text": "summarization", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.9684407711029053}]}, {"text": "At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g., filtering or retrieving only editorials in favor of a particular policy decision).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7512404918670654}]}, {"text": "Our motivation for building the opinion detection and classification system described in this paper is the need for organizing information in the context of question answering for complex questions.", "labels": [], "entities": [{"text": "opinion detection and classification", "start_pos": 32, "end_pos": 68, "type": "TASK", "confidence": 0.7240899875760078}, {"text": "question answering", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7641406357288361}]}, {"text": "Unlike questions like \"Who was the first man on the moon?\" which can be answered with a simple phrase, more intricate questions such as \"What are the reasons for the US-Iraq war?\" require long answers that must be constructed from multiple sources.", "labels": [], "entities": []}, {"text": "In such a context, it is imperative that the question answering system can discriminate between opinions and facts, and either use the appropriate type depending on the question or combine them in a meaningful presentation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8086211681365967}]}, {"text": "Perspective information can also help highlight contrasts and contradictions between different sources-there will be significant disparity in the material collected for the question mentioned above between Fox News and the Independent, for example.", "labels": [], "entities": [{"text": "Fox News and the Independent", "start_pos": 206, "end_pos": 234, "type": "DATASET", "confidence": 0.8830587148666382}]}, {"text": "Fully analyzing and classifying opinions involves tasks that relate to some fairly deep semantic and syntactic analysis of the text.", "labels": [], "entities": []}, {"text": "These include not only recognizing that the text is subjective, but also determining who the holder of the opinion is, what the opinion is about, and which of many possible positions the holder of the opinion expresses regarding that subject.", "labels": [], "entities": []}, {"text": "In this paper, we are presenting three of the components of our opinion detection and organization subsystem, which have already been integrated into our larger question-answering system.", "labels": [], "entities": [{"text": "opinion detection", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7539175152778625}]}, {"text": "These components deal with the initial tasks of classifying articles as mostly subjective or objective, finding opinion sentences in both kinds of articles, and determining, in general terms and without reference to a specific subject, if the opinions are positive or negative.", "labels": [], "entities": []}, {"text": "The three modules of the system discussed here provide the basis for ongoing work for further classification of opinions according to subject and opinion holder and for refining the original positive/negative attitude determination.", "labels": [], "entities": []}, {"text": "We review related work in Section 2, and then present our document-level classifier for opinion or factual articles (Section 3), three implemented techniques for detecting opinions at the sentence level (Section 4), and our approach for rating an opinion as positive or negative (Section 5).", "labels": [], "entities": []}, {"text": "We have evaluated these methods using a large collection of news articles without additional annotation (Section 6) and an evaluation corpus of 400 sentences annotated for opinion classifications (Section 7).", "labels": [], "entities": []}, {"text": "The results, presented in Section 8, indicate that we achieve very high performance (more than 97%) at document-level classification and respectable performance (86-91%) at detecting opinion sentences and classifying them according to orientation.", "labels": [], "entities": [{"text": "document-level classification", "start_pos": 103, "end_pos": 132, "type": "TASK", "confidence": 0.7314595580101013}]}], "datasetContent": [{"text": "For classification tasks (i.e., classifying between facts and opinions and identifying the semantic orientation of sentences), we measured our system's performance by standard recall and precision.", "labels": [], "entities": [{"text": "classifying between facts and opinions", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.7395739674568176}, {"text": "identifying the semantic orientation of sentences)", "start_pos": 75, "end_pos": 125, "type": "TASK", "confidence": 0.5729805103370121}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9993200302124023}, {"text": "precision", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9971202611923218}]}, {"text": "We evaluated the quality of semantically oriented words by mapping the extracted words and labels to an external gold standard.", "labels": [], "entities": []}, {"text": "We took the subset of our output containing words that appear in the standard, and measured the accuracy of our output as the portion of that subset that was assigned the correct label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9992191791534424}]}, {"text": "A gold standard for document-level classification is readily available, since each article in our Wall Street Journal collection comes with an article type label (see Section 6).", "labels": [], "entities": [{"text": "document-level classification", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.7190067768096924}, {"text": "Wall Street Journal collection", "start_pos": 98, "end_pos": 128, "type": "DATASET", "confidence": 0.966385155916214}]}, {"text": "We mapped article types News and Business to facts, and article types Editorial and Letter to the Editor to opinions.", "labels": [], "entities": []}, {"text": "We cannot automatically select a sentence-level gold standard discriminating between facts and opinions, or between positive and negative opinions.", "labels": [], "entities": []}, {"text": "We therefore asked human evaluators to classify a set of sentences between facts and opinions as well as determine the type of opinions.", "labels": [], "entities": []}, {"text": "Since we have implemented our methods in an opinion question answering system, we selected four different topics (gun control, illegal aliens, social security, and welfare reform).", "labels": [], "entities": [{"text": "opinion question answering", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7218381961186727}, {"text": "gun control", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.7676303386688232}]}, {"text": "For each topic, we randomly selected 25 articles from the entire combined TREC corpus (not just the WSJ portion); these were articles matching the corresponding topical phrase given above as determined by the Lucene search engine.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.8372039496898651}, {"text": "WSJ", "start_pos": 100, "end_pos": 103, "type": "DATASET", "confidence": 0.8088846802711487}, {"text": "Lucene search engine", "start_pos": 209, "end_pos": 229, "type": "DATASET", "confidence": 0.9183723529179891}]}, {"text": "From each of these documents we randomly selected four sentences.", "labels": [], "entities": []}, {"text": "If a document happened to have less than four sentences, additional sentences were then interleaved so that successive sentences came from different topics and documents and divided into ten 50-sentence blocks.", "labels": [], "entities": []}, {"text": "Each block shares ten sentences with the preceding and following block (the last block is considered to precede the first one), so that 100 of the 400 sentences appear in two blocks.", "labels": [], "entities": []}, {"text": "Each often human evaluators (all with graduate training in computational linguistics) was presented with one block and asked to select a label for each sentence among the following: \"fact\", \"positive opinion\", \"negative opinion\", \"neutral opinion\", \"sentence contains both positive and negative opinions\", \"opinion but cannot determine orientation\", and \"uncertain\".", "labels": [], "entities": []}, {"text": "Since we have one judgment for 300 sentences and two judgments for 100 sentences, we created two gold standards for sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 116, "end_pos": 139, "type": "TASK", "confidence": 0.7145448178052902}]}, {"text": "The first (Standard A) includes the 300 sentences with one judgment and a single judgment for the remaining 100 sentences.", "labels": [], "entities": []}, {"text": "The second standard (Standard B) contains the subset of the 100 sentences for which we obtained identical labels.", "labels": [], "entities": []}, {"text": "Statistics of these two standards are given in.", "labels": [], "entities": []}, {"text": "We measured the pairwise agreement among the 100 sentences that were judged by two evaluators, as the ratio of sentences that receive a label  the 100 sentences for all seven choices was 55%; if we group together the five subtypes of opinion sentences, the overall agreement rises to 82%.", "labels": [], "entities": []}, {"text": "The low agreement for some labels was not surprising because there is much ambiguity between facts and opinions.", "labels": [], "entities": [{"text": "agreement", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.963976263999939}]}, {"text": "An example of an arguable sentence is \"A lethal guerrilla war between poachers and wardens now rages in central and eastern Africa\", which one rater classified as \"fact\" and another rater classified as \"opinion\".", "labels": [], "entities": []}, {"text": "Finally, for evaluating the quality of extracted words with semantic orientation labels, we used two distinct manually labeled collections as gold standards.", "labels": [], "entities": []}, {"text": "One set consists of the previously described 657 positive and 679 negative adjectives).", "labels": [], "entities": []}, {"text": "We also used the ANEW list which was constructed during psycholinguistic experiments) and contains 1,031 words of all four open classes.", "labels": [], "entities": [{"text": "ANEW list", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.7912267446517944}]}, {"text": "As described in), humans assigned valence scores to each score according to dimensions such as pleasure, arousal, and dominance; following heuristics proposed in psycholinguistics we obtained 284 positive and 272 negative words from the valence scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of gold standards A and B.", "labels": [], "entities": []}, {"text": " Table 2: Document-level fact/opinion classification  by Naive Bayes algorithm.", "labels": [], "entities": [{"text": "Document-level fact/opinion classification", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.7895070791244507}]}, {"text": " Table 5: Accuracy of sentence polarity tagging on  gold standards A and B for different sets of parts-of- speech.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.977275550365448}, {"text": "sentence polarity tagging", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.7673852642377218}]}]}