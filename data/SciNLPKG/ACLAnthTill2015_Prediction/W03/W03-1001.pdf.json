{"title": [{"text": "A Projection Extension Algorithm for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.8621041576067606}]}], "abstractContent": [{"text": "In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.7209392786026001}]}, {"text": "The units of translation are blocks-pairs of phrases.", "labels": [], "entities": []}, {"text": "During decoding , we use a block unigram model and a word-based trigram language model.", "labels": [], "entities": []}, {"text": "During training, the blocks are learned from source interval projections using an underlying high-precision word alignment.", "labels": [], "entities": []}, {"text": "The system performance is significantly increased by applying a novel block extension algorithm using an additional high-recall word alignment.", "labels": [], "entities": []}, {"text": "The blocks are further filtered using unigram-count selection criteria.", "labels": [], "entities": []}, {"text": "The system has been successfully test on a Chinese-English and an Arabic-English translation task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Various papers use phrase-based translation systems () that have shown to improve translation quality over single-word based translation systems introduced in (.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.685592770576477}]}, {"text": "In this paper, we present a similar system with a much simpler set of model parameters.", "labels": [], "entities": []}, {"text": "Specifically, we compute the probability of a block sequence \u00a2 \u00a1 \u00a3 . A block is a pair consisting of a contiguous source and a contiguous target phrase.", "labels": [], "entities": []}, {"text": "The block sequence is decomposed into conditional probabilities using the chain rule: . The model proposed is a joint model as in (), since target and source phrases are generated jointly.", "labels": [], "entities": []}, {"text": "The approach is illustrated in.", "labels": [], "entities": []}, {"text": "The source phrases are given on the P -axis and the target phrases are given on the Q -axis.", "labels": [], "entities": [{"text": "P -axis", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9073677460352579}, {"text": "Q -axis", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9406839410463969}]}, {"text": "During block decoding a bijection between source and target phrases is : We compute unigram probabilities for the blocks.", "labels": [], "entities": []}, {"text": "The blocks are simpler than the alignment templates () in that they do not have an internal structure.", "labels": [], "entities": []}, {"text": "R cannot be computed for all blocks in the training data: we would obtain hundreds of millions of blocks.", "labels": [], "entities": [{"text": "R", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9583748579025269}]}, {"text": "The blocks are restricted by an underlying word alignment.", "labels": [], "entities": []}, {"text": "In this paper, we present a block generation algorithm similar to the one in () in full detail: source intervals are projected into target intervals under a restriction derived from a high-precision word alignment.", "labels": [], "entities": [{"text": "block generation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7451553046703339}]}, {"text": "The projection yields a set of high-precision block links.", "labels": [], "entities": []}, {"text": "These block links are further extended using a high-recall word alignment.", "labels": [], "entities": []}, {"text": "The block extension algorithm is shown to improve translation performance significantly.", "labels": [], "entities": [{"text": "block extension", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7589595317840576}, {"text": "translation", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.9707971811294556}]}, {"text": "The system is tested on a Chinese-English (CE) and an Arabic-English (AE) translation task.", "labels": [], "entities": [{"text": "Arabic-English (AE) translation", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.6285130739212036}]}, {"text": "The paper is structured as follows: in Section 2, we present the baseline block generation algorithm.", "labels": [], "entities": [{"text": "block generation", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.7671038508415222}]}, {"text": "The block extension approach is described in Section 2.1.", "labels": [], "entities": []}, {"text": "Section 3 describes a DP-based decoder using blocks.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "The translation system is tested on a Chinese-to-  restriction: for the translation direction Chinese-to-English, selecting blocks with longer English phrases seems to be important for good translation performance.", "labels": [], "entities": []}, {"text": "It is interesting to note, that the unigram translation model is symmetric: the translation direction can be switched to English-to-Chinese without re-training the modeljust anew Chinese language model is needed.", "labels": [], "entities": [{"text": "unigram translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.662779688835144}]}, {"text": "Our experiments, though, show that there is an unbalance with respect to the projection direction that has a significant influence on the translation results.", "labels": [], "entities": []}, {"text": "Finally, we carried out an experiment where we used the \u00a9 \u009b \u00aa \u0098 \u00ab \u00aa block set as a baseline.", "labels": [], "entities": [{"text": "\u00a9 \u009b \u00aa", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9075552821159363}]}, {"text": "The extension algorithm was applied only to blocks of target and source length \u00a7 producing one-to-many translations, e.g. the blocks I \u00af and \u00b0 in.", "labels": [], "entities": []}, {"text": "The BLEU score improved to with a block set of \u00a4 g \u00a5 \u00a7 w \u00ac million blocks.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9699492156505585}]}, {"text": "It seems to be important to carryout the block extension also for larger blocks.", "labels": [], "entities": []}, {"text": "We also ran the N2 system on the June 2002 DARPA TIDES Large Data evaluation test set.", "labels": [], "entities": [{"text": "DARPA TIDES Large Data evaluation test set", "start_pos": 43, "end_pos": 85, "type": "DATASET", "confidence": 0.8386133483478001}]}, {"text": "Six research sites and four commercial off-the-shelf systems were evaluated in Large Data track.", "labels": [], "entities": [{"text": "Large Data track", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.7330475250879923}]}, {"text": "A majority of the systems were phrase-based translation systems.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7964253127574921}]}, {"text": "For comparison with other sites, we quote the We cannot compute the block set resulting from all word link quadruples in \u00b3 , which is much bigger, due to CPU and memory restrictions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Effect of the unigram threshold on the  BLEU score. The maximum phrase length is", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.9754509329795837}]}]}