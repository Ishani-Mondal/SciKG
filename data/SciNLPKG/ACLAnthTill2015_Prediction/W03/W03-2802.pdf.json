{"title": [{"text": "The PEACE SLDS understanding evaluation paradigm of the French MEDIA campaign", "labels": [], "entities": [{"text": "PEACE SLDS understanding evaluation", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.6486499831080437}, {"text": "French MEDIA", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.8676166236400604}]}], "abstractContent": [{"text": "This paper presents a paradigm for evaluating the context-sensitive under", "labels": [], "entities": []}], "introductionContent": [{"text": "Generally speaking common reference tasks) and methods to compare and diagnose spoken language dialog systems (SLDS) and spoken dialog techniques are lacking despite previous efforts futher discussed in the next section such as EAGLES, DISC, AUPELF ARCB2 or the ongoing American project DARPA COMMUNICATOR.", "labels": [], "entities": [{"text": "compare and diagnose spoken language dialog systems (SLDS)", "start_pos": 58, "end_pos": 116, "type": "TASK", "confidence": 0.7086079627275467}, {"text": "AUPELF ARCB2", "start_pos": 242, "end_pos": 254, "type": "DATASET", "confidence": 0.7910025715827942}]}, {"text": "Without an objective assessment of dialog systems, it is difficult to reuse previous work and to advance theories.", "labels": [], "entities": []}, {"text": "The assessment of a dialog system is complex in part to the high integration factor and tight coupling between the various modules present in any SLDS, for which unfortunately today, no common accepted reference architecture exists.", "labels": [], "entities": []}, {"text": "Nevertheless, a major problem remains the dynamic nature of dialog.", "labels": [], "entities": []}, {"text": "Consequently to these shortcomings, researchers are often unable to provide principled design and system capabilities for technology transfer.", "labels": [], "entities": []}, {"text": "In other research areas, such as speech recognition and information retrieval, common reference tasks have been highly effective in sharing research costs and efforts.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8845653235912323}, {"text": "information retrieval", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7985990345478058}]}, {"text": "A similar development is highly needed in the dialog community.", "labels": [], "entities": []}, {"text": "In this contribution which addresses only apart of the SLDS evaluation problem, a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system is proposed.", "labels": [], "entities": [{"text": "SLDS evaluation", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.8822585344314575}]}, {"text": "PEACE) described in section 3, is based on test sets extracted from real corpora, and has three main aspects: it is generic, contextual and it offers diagnostic capabilities.", "labels": [], "entities": [{"text": "PEACE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5775216221809387}]}, {"text": "Here genericity is envisaged in a context of information dialogs access.", "labels": [], "entities": []}, {"text": "The diagnostic aspect is important in order to determine the different qualities of the systems under test.", "labels": [], "entities": []}, {"text": "The contextual aspect of evaluation is a crucial point since dialog is dynamic by nature.", "labels": [], "entities": []}, {"text": "We propose to simulate/synthesize the contextual information.", "labels": [], "entities": []}, {"text": "The PEACE paradigm will be tested in the French Technolangue MEDIA project and will serve as basis in the comparison and diagnostic evaluation of systems presented by various academic and industrial sites (section 4).", "labels": [], "entities": [{"text": "French Technolangue MEDIA project", "start_pos": 41, "end_pos": 74, "type": "DATASET", "confidence": 0.8858670592308044}]}, {"text": "ELRA/ELDA is the coordinator of the larger scope evaluation campaign EVALDA, which includes the MEDIA campaign that began in January 2003.", "labels": [], "entities": [{"text": "ELRA/ELDA", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7588738799095154}, {"text": "EVALDA", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.8290672898292542}]}], "datasetContent": [{"text": "Without an attempt to be exhaustive, we overview some recent efforts for evaluation of SLDS.", "labels": [], "entities": [{"text": "SLDS", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.9384657144546509}]}, {"text": "The objective of the European DISC project was to write the best-practice guidelines for SLDS development and evaluation of its time.", "labels": [], "entities": [{"text": "European DISC project", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.8611361583073934}, {"text": "SLDS development", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.9093447029590607}]}, {"text": "DISC has collected a systematic list of bottom-up evaluation criteria, each corresponding to a partially ordered list of properties likely to be encountered in any SLDS.", "labels": [], "entities": [{"text": "DISC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9782664179801941}]}, {"text": "This properties are positioned on a grid defining an SLDS abstract architecture and relate to various phases of the generic DISC SLDS development life-cycle ().", "labels": [], "entities": [{"text": "SLDS abstract", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.881748765707016}]}, {"text": "They are complemented by a standard evaluation pattern made of 10 generic questions (e.g. \"Which symptoms need to be observed?\"", "labels": [], "entities": []}, {"text": ") which has been instantiated for all the evaluation criteria.", "labels": [], "entities": []}, {"text": "If the DISC results are quite extensive and presented in an homogeneous way, they do not provide a direct answer to the question of SLDS evaluation.", "labels": [], "entities": [{"text": "SLDS evaluation", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.9199069738388062}]}, {"text": "Its contribution lies more at the specification level.", "labels": [], "entities": []}, {"text": "Although the approach and the goals of the European EAGLES project were different, one could forward the same remark about the results of the speech evaluation work group).", "labels": [], "entities": [{"text": "European EAGLES project", "start_pos": 43, "end_pos": 66, "type": "DATASET", "confidence": 0.8874300320943197}]}, {"text": "In, one find a set of evaluation criteria for voice oriented products and services, organized in four broad categories.: 1) voice command, 2) document generation, 3) phone services 4) other.", "labels": [], "entities": [{"text": "document generation", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7219056040048599}]}, {"text": "To the best of our knowledge, the MADCOW (Multi Site Data COllection Working group) coordination group setup in the USA by ARPA in the context of the ATIS (Air Travel Information Services) task to collect corpora, was the first to propose a common infrastructure for SLDS automatic evaluation, which also addressed the problem of language understanding evaluation, based on system answer comparison.", "labels": [], "entities": [{"text": "SLDS automatic evaluation", "start_pos": 267, "end_pos": 292, "type": "TASK", "confidence": 0.9184476137161255}, {"text": "language understanding evaluation", "start_pos": 330, "end_pos": 363, "type": "TASK", "confidence": 0.8222135106722513}]}, {"text": "Unfortunately no direct diagnostic information can be produced, since understanding is appreciated by gauging the distance from the answer to a pair of minimal and a maximal reference answers.", "labels": [], "entities": []}, {"text": "In ATIS, the protocol was only been applied to context free sentences.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.8149479627609253}]}, {"text": "Up to now it has been one of the most used by the community since it is relatively objective and generic because it relies on counts of explicit information and allows fora certain variation in the answers.", "labels": [], "entities": []}, {"text": "On the other hand, the method displays a bias toward silence and does not give the means to appreciate error severity.", "labels": [], "entities": []}, {"text": "In ARISE (Automatic Railway Information Systems for Europe), a corpus of roughly 10,000 calls has been used in conjunction with user debriefing questionnaire analysis to diagnose different versions of a phone information server.", "labels": [], "entities": [{"text": "ARISE", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.7677291631698608}]}, {"text": "The hand-tagging objective measures of the corpus include understanding error counts (glass box methodology).", "labels": [], "entities": [{"text": "understanding error counts", "start_pos": 58, "end_pos": 84, "type": "METRIC", "confidence": 0.8542353510856628}]}, {"text": "Although it provides fine grained diagnostic information, this procedure cannot be easily generalized since it requires handannotated corpus and access to the internal representation of the system.", "labels": [], "entities": []}, {"text": "Two metrics have been developped at MIT (): the Query Density (QD) and the Concept Efficiency (CE), which measure respectively over the course of a dialogue: the mean number of new concepts introduced per user query, and the number of turns necessary for each concept to be understood by the system.", "labels": [], "entities": [{"text": "Concept Efficiency (CE)", "start_pos": 75, "end_pos": 98, "type": "METRIC", "confidence": 0.715504115819931}]}, {"text": "Concepts are generated automatically for each utterance with a parsable orthographic transcription as a series of keyword-value pairs.", "labels": [], "entities": []}, {"text": "The higher the QD, the more effectively a user is able to communicate information to the system.", "labels": [], "entities": [{"text": "QD", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.6938304901123047}]}, {"text": "The CE is an indicator of recognition or understanding errors; the higher it is, the fewer times a user needs to repeat himself.", "labels": [], "entities": [{"text": "CE", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9938289523124695}, {"text": "recognition or understanding errors", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.7337295711040497}]}, {"text": "These metrics were evaluated on single systems (JUPITER and and MERCURY); to compare different systems of the same type, one would need a common ontology.", "labels": [], "entities": [{"text": "JUPITER", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.5709279179573059}, {"text": "MERCURY", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9554604291915894}]}, {"text": "In (), the authors believe that CE should be related to user frustation, but to show it they would need to use the PARADISE framework.", "labels": [], "entities": [{"text": "PARADISE framework", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.7359497249126434}]}, {"text": "PARADISE ( can be seen as a sort of meta-paradigm which correlates objective and subjective measurements.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9146119952201843}]}, {"text": "Its grounding hypothesis states that the goal of any SLDS is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs.", "labels": [], "entities": [{"text": "SLDS", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9493199586868286}]}, {"text": "With the help of the kappa coefficient proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation.", "labels": [], "entities": [{"text": "task generic comparative evaluation", "start_pos": 154, "end_pos": 189, "type": "TASK", "confidence": 0.5661413818597794}]}, {"text": "PARADISE has been tested in the COMMUNICATOR project () with 9 systems working on the same task over different databases.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.6514548063278198}, {"text": "COMMUNICATOR project", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.8170508444309235}]}, {"text": "With four basic measures (e.g. task completion) the protocol has been able to predict 37% of user satisfaction variation, and 42% with the help of a few extra measurements on dialog acts and subtasks.", "labels": [], "entities": []}, {"text": "One critic, one can make about PARADISE concern its cost (real user tests are costly) and the use of subjective assessment.", "labels": [], "entities": []}, {"text": "The adaption of the DQR text understanding evaluation methodology () to speech resulted in a generic and qualitative procedure.", "labels": [], "entities": [{"text": "DQR text understanding evaluation", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.7763078212738037}]}, {"text": "Each element of its test set holds three parts, the Declaration to define the context, a Question which bears on point present in the context and the Response.", "labels": [], "entities": []}, {"text": "The test set is organized through seven levels of test, from basic explicit understanding to semantic interpretation and reply pertinence assessment.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.7394425719976425}, {"text": "reply pertinence assessment", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.7786860466003418}]}, {"text": "This protocol is task and system generic but test set construction is not straightforward and the bias introduced by the wording of the question is difficult to assess.", "labels": [], "entities": []}, {"text": "Recently the GDR-13 work group of CNRS on spoken dialog understanding, has proposed an evaluation methodology for literal understanding.", "labels": [], "entities": [{"text": "GDR-13 work", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.9074324071407318}, {"text": "spoken dialog understanding", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.5751783152421316}, {"text": "literal understanding", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.8981474936008453}]}, {"text": "According to), DEFI tries to remedy two important weaknesses of the MAD-COW methodology, namely the lack of genericity and the lack of diagnostic information, by crafting system specific test sets from a primary set of enunciations representative of the task (provided by the developers).", "labels": [], "entities": []}, {"text": "Secondary enunciations are then derived from the primary ones in order to exhibit particular language phenomena.", "labels": [], "entities": []}, {"text": "Afterwards, the systems are evaluated by their developers using specific test set and their own metrics.", "labels": [], "entities": []}, {"text": "The various results can be mapped over a generic abstract architecture for comparison (although this mapping is still unspecified at the time of writing).", "labels": [], "entities": []}, {"text": "DEFI has already been used in one evaluation campaign, with 5 systems presented by 4 laboratories.", "labels": [], "entities": [{"text": "DEFI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8601090312004089}]}, {"text": "() has reported the following weaknesses of the protocol: how to control the bias introduced by the derivation of enunciations, how to guaranty that derived enunciation will remain in the task scope (this prevented some system from being evaluated over the complete test set) and finally how to restrict and organize the language phenomena used in the test set.", "labels": [], "entities": []}, {"text": "Common evaluation metrics are essential for analyzing the system capabilities.", "labels": [], "entities": []}, {"text": "The scoring tool for AVR comparison is able to compare between two AVR frame representation sets.", "labels": [], "entities": [{"text": "AVR comparison", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7222425937652588}]}, {"text": "For evaluation, system outputs translated in AVR format composed one set, the other one contains the AVR references which are manually annotated.", "labels": [], "entities": []}, {"text": "Both frame sets have the form of a list of AVRs (fixed length records).", "labels": [], "entities": [{"text": "AVRs", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.7597537040710449}]}, {"text": "Each record is composed of three or four fields (mode, attribute, value, reference).", "labels": [], "entities": []}, {"text": "The comparison consists in applying a set of predefined operators each assigned with a cost value.", "labels": [], "entities": []}, {"text": "The comparison process looks for operator lists to be applied to the test frame in order to obtain the reference frame that minimizes the final cost value.", "labels": [], "entities": []}, {"text": "For a global evaluation, the classical operators from speech evaluation (DELetion, INSertion and SUBstitution) maybe used (as used for first two values of Accuracy percentage in).", "labels": [], "entities": [{"text": "DELetion", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9359160661697388}, {"text": "INSertion", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9937186241149902}, {"text": "Accuracy percentage", "start_pos": 155, "end_pos": 174, "type": "METRIC", "confidence": 0.9778318405151367}]}, {"text": "With our scoring tool the definition of new operators is quite easy.", "labels": [], "entities": []}, {"text": "It is then also possible to distinguish between different types of errors by defining specific operators (as used to estimate Topic identification in, or by using different cost values (for example a substitution is often considered more costly for dialog management).", "labels": [], "entities": [{"text": "Topic identification", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.7990324199199677}, {"text": "dialog management", "start_pos": 249, "end_pos": 266, "type": "TASK", "confidence": 0.88852658867836}]}], "tableCaptions": [{"text": " Table 1: Literal understanding (LU) accuracy on both exact", "labels": [], "entities": [{"text": "Literal understanding (LU)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8176829814910889}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.684192955493927}]}]}