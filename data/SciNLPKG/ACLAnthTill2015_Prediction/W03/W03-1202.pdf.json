{"title": [{"text": "Using Thematic Information in Statistical Headline Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore the problem of single sentence summarisation.", "labels": [], "entities": [{"text": "single sentence summarisation", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6010912954807281}]}, {"text": "In the news domain, such a summary might resemble a headline.", "labels": [], "entities": []}, {"text": "The headline generation system we present uses Singular Value Decomposition (SVD) to guide the generation of a headline towards the theme that best represents the document to be summarised.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7995617687702179}]}, {"text": "In doing so, the intuition is that the generated summary will more accurately reflect the content of the source document.", "labels": [], "entities": []}, {"text": "This paper presents SVD as an alternative method to determine if a word is a suitable candidate for inclusion in the headline.", "labels": [], "entities": [{"text": "SVD", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9149527549743652}]}, {"text": "The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9888370037078857}, {"text": "word selection", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.8181386590003967}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9745256900787354}]}], "introductionContent": [{"text": "Ours is an age where many documents are archived electronically and are available whenever needed.", "labels": [], "entities": []}, {"text": "In the midst of this plethora of information, the successful completion of a research task is affected by the ease with which users can quickly identify the relevant electronic documents that satisfy their information needs.", "labels": [], "entities": []}, {"text": "To do so, a researcher often relies on generated summaries that reflect the contents of the original document.", "labels": [], "entities": []}, {"text": "We explore the problem of single sentence summarisation, the primary focus of this paper.", "labels": [], "entities": [{"text": "single sentence summarisation", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.5978182256221771}]}, {"text": "Instead of identifying and extracting the most important sentence, we generate anew sentence from scratch.", "labels": [], "entities": []}, {"text": "The resulting sentence summary may not occur verbatim in the source document but may instead be a paraphrase combining key words and phrases from the text.", "labels": [], "entities": []}, {"text": "As a precursor to single sentence summarisation, we first explore the particular case of headline generation in the news domain, specifically English news.", "labels": [], "entities": [{"text": "single sentence summarisation", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.5803371071815491}, {"text": "headline generation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7848953008651733}]}, {"text": "Although headlines are often constructed to be sensationalist, we regard headline generation as an approximation to single sentence summarisation, given that a corpus of single sentence summaries does not exist.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8021650016307831}, {"text": "single sentence summarisation", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.6687854329744974}]}, {"text": "Our system re-uses words from the news article to generate a single sentence summary that resembles a headline.", "labels": [], "entities": []}, {"text": "This is done by selecting and then appending words from the source article.", "labels": [], "entities": []}, {"text": "This approach has been explored by a number of researchers (eg. see) and we will describe their work further in the next section.", "labels": [], "entities": []}, {"text": "In existing approaches, a word is selected on the basis of two criteria: how well it acts as a summary word, and how grammatical it will be given the preceding summary words that have already been chosen.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to present work which investigates the use of Singular Value Decomposition (SVD) as a means of determining if a word is a good candidate for inclusion in the headline.", "labels": [], "entities": [{"text": "Singular Value Decomposition (SVD)", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.6277764489253362}]}, {"text": "To introduce the notion of using SVD for single sentence summarisation in this paper, we examine the simplest summarisation scenario.", "labels": [], "entities": [{"text": "single sentence summarisation", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.6353772282600403}, {"text": "summarisation", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.9795048832893372}]}, {"text": "Thus, presently we are only concerned with single document summarisation.", "labels": [], "entities": [{"text": "single document summarisation", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.5395970145861307}]}, {"text": "In addition, we limit the focus of our discussion to the generation of generic summaries.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we describe our motivation for using SVD by describing difficulties in generating headlines in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, as motivation for our approach, we illustrate how words can be used out of context, resulting in factually incorrect statements.", "labels": [], "entities": []}, {"text": "Section 4 provides an overview of related work.", "labels": [], "entities": []}, {"text": "In Section 5, we give a detailed description of how we generate the sentence summary statistically and how we use SVD to guide the generation process.", "labels": [], "entities": []}, {"text": "In Section 6, we present our experimental design in which we evaluated our approach, along with the results and corresponding discussion.", "labels": [], "entities": []}, {"text": "Finally, in Section 7, we present our conclusions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted an evaluation experiment to compare the performance of the three Content Selection strategies that we identified in Section 5: the Conditional probability, the SVD probability, and the Combined probability.", "labels": [], "entities": [{"text": "Combined probability", "start_pos": 198, "end_pos": 218, "type": "METRIC", "confidence": 0.9612878561019897}]}, {"text": "We measure performance in terms of recall, i.e. how many of the words in the actual headline match words in the generated headline.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.998985230922699}]}, {"text": "The recall metric is normalised to form a percentage by dividing the word overlap by the number of words in the actual headline.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.998120129108429}]}, {"text": "For each test article, we generated headlines using each of the three strategies.", "labels": [], "entities": []}, {"text": "For each strategy, we generated headlines of varying lengths, ranging from length 1 to 13, where the latter is the length of the longest headline found in the test set.", "labels": [], "entities": []}, {"text": "We then compared the different strategies for generated headlines of equal length.", "labels": [], "entities": []}, {"text": "To determine if differences in recall scores were significant, we used the Wilcoxon Matched Pairs Signed Ranks (WMPSR) test).", "labels": [], "entities": [{"text": "recall scores", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9769208133220673}, {"text": "Wilcoxon Matched Pairs Signed Ranks (WMPSR) test", "start_pos": 75, "end_pos": 123, "type": "METRIC", "confidence": 0.5880939596229129}]}, {"text": "In our case, fora particular pair of Content Selection strategies, the alternate hypothesis was that the choice of Content Selection strategy affects recall performance.", "labels": [], "entities": [{"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9600737690925598}]}, {"text": "The null hypothesis held that there was no difference between the two content selection strategies.", "labels": [], "entities": []}, {"text": "Our use of the non-parametric test was motivated by the observation that recall scores were not normally distributed.", "labels": [], "entities": [{"text": "recall scores", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9772293567657471}]}, {"text": "In fact, our results showed a positive skew for recall scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9993487000465393}]}, {"text": "To begin with, we compared the recall scores of the SVD strategy and the Conditional strategy in one evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9994118213653564}]}, {"text": "The strategy that was found to perform better was then compared with the Combined strategy.", "labels": [], "entities": [{"text": "Combined", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.5286902785301208}]}, {"text": "In addition to the recall tests, we conducted an analysis to determine the extent to which the SVD strategy and the Conditional probability strategy were in agreement about which words to select for inclusion in the generated headline.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9988034963607788}]}, {"text": "For this analysis, we ignored the bigram probability of the Realisation component and just measured the agreement between the top n ranking words selected by each content selection strategy.", "labels": [], "entities": [{"text": "Realisation", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.9579060077667236}]}, {"text": "Over the test set, we counted how many words were selected by both strategies, just one strategy, and no strategies.", "labels": [], "entities": []}, {"text": "By normalising scores by the number of test cases, we determine the average agreement across the test set.", "labels": [], "entities": [{"text": "agreement", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.927941620349884}]}, {"text": "We ran this experiment fora range of different values of N, ranging from 1 to 13, the length of the longest headline in the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. A comparison of recall scores for the  SVD strategy and the Conditional strategy.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9990718364715576}]}, {"text": " Table 2. A comparison of recall scores for the  Conditional strategy and the Combined strategy.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9992033839225769}, {"text": "Combined", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9264647960662842}]}]}