{"title": [{"text": "Training Connectionist Models for the Structured Language Model", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the performance of the Structured Language Model (SLM) in terms of perplexity (PPL) when its components are modeled by connectionist models.", "labels": [], "entities": []}, {"text": "The connectionist models use a distributed representation of the items in the history and make much better use of contexts than currently used interpolated or back-off models, not only because of the inherent capability of the connection-ist model in fighting the data sparseness problem, but also because of the sub-linear growth in the model size when the context length is increased.", "labels": [], "entities": []}, {"text": "The connec-tionist models can be further trained by an EM procedure, similar to the previously used procedure for training the SLM.", "labels": [], "entities": []}, {"text": "Our experiments show that the connectionist models can significantly improve the PPL over the interpolated and back-off models on the UPENN Treebank corpora, after interpolating with a baseline trigram language model.", "labels": [], "entities": [{"text": "UPENN Treebank corpora", "start_pos": 134, "end_pos": 156, "type": "DATASET", "confidence": 0.9722898205121359}]}, {"text": "The EM training procedure can improve the connectionist models further , by using hidden events obtained by the SLM parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many systems dealing with natural speech or language such as Automatic Speech Recognition and \u00a3 This work was supported by the National Science Foundation under grants No.IIS-9982329 and No.IIS-0085940.", "labels": [], "entities": [{"text": "Automatic Speech Recognition", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.5846799810727438}, {"text": "\u00a3", "start_pos": 97, "end_pos": 98, "type": "DATASET", "confidence": 0.8675848841667175}, {"text": "National Science Foundation", "start_pos": 130, "end_pos": 157, "type": "DATASET", "confidence": 0.9308531483014425}]}, {"text": "Statistical Machine Translation, a language model is a crucial component for searching in the often prohibitively large hypothesis space.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7236152291297913}]}, {"text": "Most of the state-of-the-art systems use n-gram language models, which are simple and effective most of the time.", "labels": [], "entities": []}, {"text": "Many smoothing techniques that improve language model probability estimation have been proposed and studied in the n-gram literature).", "labels": [], "entities": [{"text": "language model probability estimation", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.674263060092926}]}, {"text": "Recent efforts have studied various ways of using information from a longer context span than that usually captured by normal n-gram language models, as well as ways of using syntactical information that is not available to the word-based n-gram models).", "labels": [], "entities": []}, {"text": "All these language models are based on stochastic parsing techniques that buildup parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees.", "labels": [], "entities": []}, {"text": "Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks.", "labels": [], "entities": []}, {"text": "Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (), a severe data sparseness problem was observed in () when the number of conditioning features was increased.", "labels": [], "entities": []}, {"text": "There has been recent promising work in using distributional representation of words and neural networks for language modeling () and parsing.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7211576849222183}, {"text": "parsing", "start_pos": 134, "end_pos": 141, "type": "TASK", "confidence": 0.9630654454231262}]}, {"text": "One great advantage of this approach is its ability to fight data sparseness.", "labels": [], "entities": []}, {"text": "The model size grows only sub-linearly with the number of predicting features used.", "labels": [], "entities": []}, {"text": "It has been shown that this method improves significantly on regular n-gram models in perplexity ().", "labels": [], "entities": []}, {"text": "The ability of the method to accommodate longer contexts is most appealing, since experiments have shown consistent improvements in PPL when the context of one of the components of the SLM is increased in length (.", "labels": [], "entities": []}, {"text": "Moreover, because the SLM provides an EM training procedure for its components, the connectionist models can also be improved by the EM training.", "labels": [], "entities": []}, {"text": "In this paper, we will study the impact of neural network modeling on the SLM, when all of its three components are modeled with this approach.", "labels": [], "entities": [{"text": "SLM", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9700419902801514}]}, {"text": "An EM training procedure will be outlined and applied to further training of the neural network models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have used the UPenn Treebank portion of the WSJ corpus to carryout our experiments.", "labels": [], "entities": [{"text": "UPenn Treebank portion of the WSJ corpus", "start_pos": 17, "end_pos": 57, "type": "DATASET", "confidence": 0.9544556396348136}]}, {"text": "The UPenn Treebank contains 24 sections of handparsed sentences.", "labels": [], "entities": [{"text": "UPenn Treebank", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9842690527439117}]}, {"text": "We used section 00-20 for training our models, section 21-22 for tuning some parameters (i.e., estimating discount constant for smoothing, and/or making sure overtraining does not occur) and section 23-24 to test our models.", "labels": [], "entities": [{"text": "estimating discount constant", "start_pos": 95, "end_pos": 123, "type": "METRIC", "confidence": 0.7880311806996664}]}, {"text": "Before carrying out our experiments, we normalized the text in the following ways: numbers in Arabic form are replaced by a single token \"N\", punctuations are removed, all words are mapped to lowercase, extra information in the parse (such like traces) are ignored.", "labels": [], "entities": []}, {"text": "The word vocabulary contains 10k words including a special token for unknown words.", "labels": [], "entities": []}, {"text": "There are 40 items in the part-of-speech set and 54 items in the non-terminal set, respectively.", "labels": [], "entities": []}, {"text": "All of the experimental results in this section are based on this corpus and split, unless otherwise stated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison between KN and DI smoothing", "labels": [], "entities": [{"text": "smoothing", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.6794203519821167}]}, {"text": " Table 2: Comparison between KN and NN (E0)", "labels": [], "entities": []}, {"text": " Table 3: EM training results", "labels": [], "entities": [{"text": "EM training", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8206814229488373}]}]}