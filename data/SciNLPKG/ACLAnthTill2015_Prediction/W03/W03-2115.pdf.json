{"title": [], "abstractContent": [{"text": "In this paper we present a contextual extension to ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology.", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.8141965866088867}]}, {"text": "We apply the contextually enhanced system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence.", "labels": [], "entities": [{"text": "scoring alternative speech recognition hypotheses (SRH)", "start_pos": 57, "end_pos": 112, "type": "TASK", "confidence": 0.7591599561274052}]}, {"text": "We conducted several annotation experiments and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses (both with and without discourse context).", "labels": [], "entities": [{"text": "speech recognition hypotheses", "start_pos": 149, "end_pos": 178, "type": "TASK", "confidence": 0.7866918047269186}]}, {"text": "We also showed, that annotators can reliably identify the overall best hypothesis from a given n-best list.", "labels": [], "entities": []}, {"text": "While the original ONTOSCORE system correctly assigns the highest score to 84.06% of the corpus, the inclusion of the conceptual context increases the number of correct classifications to yield 86.76%, given a baseline of 63.91% in both cases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Following, we can distinguish between controlled and conversational dialogue systems.", "labels": [], "entities": []}, {"text": "Since controlled and restricted interactions between the user and the system increase recognition and understanding accuracy, such systems are reliable enough to be deployed in various real world applications, e.g. public transportation or cinema information systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.979496419429779}]}, {"text": "The more conversational a dialogue system becomes, the less predictable are the users' utterances.", "labels": [], "entities": []}, {"text": "Recognition and processing become increasingly difficult and unreliable.", "labels": [], "entities": [{"text": "Recognition and processing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6286544303099314}]}, {"text": "Today's dialogue systems employ domain-and discourse-specific knowledge bases, so-called ontologies, to represent the individual discourse entities as concepts as well as their relations to each other.", "labels": [], "entities": []}, {"text": "In this paper we employ an algorithm for measuring the semantic coherence of sets of concepts using such an ontology and show how its performance can be improved by means of an inclusion of the conceptual context.", "labels": [], "entities": []}, {"text": "Thereby creating a method for scoring the contextual coherence of individual sets of concepts.", "labels": [], "entities": []}, {"text": "In the following, we will show how the contextual coherence measurement can be applied to estimate how well a given speech recognition hypothesis (SRH) fits with respect to the existing knowledge representation and the given conceptual context, thereby providing a mechanism that increases the robustness and reliability of dialogue systems.", "labels": [], "entities": [{"text": "speech recognition hypothesis (SRH)", "start_pos": 116, "end_pos": 151, "type": "TASK", "confidence": 0.7897330224514008}]}, {"text": "We can, therefore, show how the algorithm can be successfully employed by a spoken dialogue system to enhance the interface between automatic speech recognition (ASR) and natural language understanding (NLU).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 132, "end_pos": 166, "type": "TASK", "confidence": 0.8139944672584534}, {"text": "natural language understanding (NLU)", "start_pos": 171, "end_pos": 207, "type": "TASK", "confidence": 0.7969977557659149}]}, {"text": "In Section 2 we discuss the problem of scoring and classifying SRHs in terms of their semantic coherence followed by a description of our annotation experiments and the corresponding results in Section 3.", "labels": [], "entities": [{"text": "classifying SRHs", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.6154465079307556}]}, {"text": "Section 4 contains a description of the kind of knowledge representations and the algorithm employed by ONTOSCORE.", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.7837073802947998}]}, {"text": "In Section 5 we present the contextually enhanced system.", "labels": [], "entities": []}, {"text": "Evaluations of the corresponding system for scoring SRHs are given in Section 6.", "labels": [], "entities": [{"text": "scoring SRHs", "start_pos": 44, "end_pos": 56, "type": "TASK", "confidence": 0.42723895609378815}]}, {"text": "A conclusion and additional applications are given in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments reported here are based on the data collected in hidden-operator tests where subjects were prompted to say certain inputs.", "labels": [], "entities": []}, {"text": "We obtained 232 dialogues, which were divided into 1479 audio files with single user utterances.", "labels": [], "entities": []}, {"text": "Each utterance corresponded to a single intention, e.g. a route-or a sight information request.", "labels": [], "entities": []}, {"text": "Firstly, all utterances were also transcribed.", "labels": [], "entities": []}, {"text": "Then the audio files were sent to the speech recognizer.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7226281613111496}]}, {"text": "We logged the speech recognition output, i.e. n-best lists of SRHs for all utterances.", "labels": [], "entities": []}, {"text": "A subset of the corpus was used to log also the scores of the recognizer, parser and that of OntoScore -including context-independent and context-dependent semantic coherence scores.", "labels": [], "entities": []}, {"text": "This trial resulted in a sub-corpus of 552 utterances corresponding to 1,375 SRHs along with the respective confidence scores.", "labels": [], "entities": [{"text": "SRHs", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.6335357427597046}, {"text": "confidence", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9579148888587952}]}, {"text": "We, then, conducted several annotation experiments with a two-fold motivation.", "labels": [], "entities": []}, {"text": "In the first place, it was necessary to produce a hand-annotated corpus to be used as a gold standard for the evaluation of the contextual coherence scores.", "labels": [], "entities": []}, {"text": "Furthermore, we wanted to test whether human subjects were able to annotate the data reliably according to our annotation schemata.", "labels": [], "entities": []}, {"text": "We had two annotators specially trained for each of these particular annotation tasks.", "labels": [], "entities": []}, {"text": "In an earlier annotation experiment reported in, the task of annotators was to classify a subset of the corpus of SRHs as either coherent or incoherent.", "labels": [], "entities": []}, {"text": "Here we randomly mixed SRHs in order to avoid contextual priming.", "labels": [], "entities": []}, {"text": "In the first new experiment, a sub-corpus of 552 utterances was annotated within the discourse context, i.e. the SRHs were presented in their original dialogue order.", "labels": [], "entities": []}, {"text": "For each SRH, a decision again had to be made whether it is semantically coherent or incoherent with respect to the best SRH representing the previous user utterance.", "labels": [], "entities": []}, {"text": "Given a total of 1,375 markables, the annotators reached an agreement of 79.71%, i.e. 1,096 markables.", "labels": [], "entities": []}, {"text": "In the second new annotation experiment, the annotators saw the SRHs together with the transcribed user utterances.", "labels": [], "entities": []}, {"text": "The task of annotators was to determine the best SRH from the n-best list of SRHs corresponding to a single user utterance.", "labels": [], "entities": []}, {"text": "The decision had to be made on the basis of several criteria.", "labels": [], "entities": []}, {"text": "The most important criteria was how well the SRH captures the intentional content of the user's utterance.", "labels": [], "entities": []}, {"text": "If none of the SRHs captured the user's intention adequately, the decision had to be made by looking at the actual word error rate.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 115, "end_pos": 130, "type": "METRIC", "confidence": 0.689870019753774}]}, {"text": "In this experiment the inter-annotator agreement was 90.69%, i.e. 1,247 markables out of 1,375.", "labels": [], "entities": []}, {"text": "3 Each corpus was then tranformed into an evaluation gold standard by means of the annotators agreeing on a single solution for the cases of disagreement.", "labels": [], "entities": []}, {"text": "The aim of the work presented here, then, was to provide a knowledge-based score, that can be employed by any NLU system to select the best hypothesis from a given n-best list.", "labels": [], "entities": []}, {"text": "The corresponding ON-TOSCORE system will be described below, followed by its evaluation against the human gold standards.", "labels": [], "entities": [{"text": "ON-TOSCORE", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9220795631408691}]}, {"text": "The ONTOSCORE software runs as a module in the SMARTKOM multi-modal and multi-domain spoken dialogue system ().", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.5014280676841736}, {"text": "SMARTKOM multi-modal and multi-domain spoken dialogue", "start_pos": 47, "end_pos": 100, "type": "TASK", "confidence": 0.45814448098341626}]}, {"text": "The system features the combination of speech and gesture as its input and output modalities.", "labels": [], "entities": []}, {"text": "The domains of the system include cinema and TV program information, home electronic device control as well as mobile services for tourists, e.g. tour planning and sights information.", "labels": [], "entities": [{"text": "tour planning", "start_pos": 146, "end_pos": 159, "type": "TASK", "confidence": 0.7523634135723114}]}, {"text": "ONTOSCORE operates on n-best lists of SRHs produced by the language interpretation module out of the ASR word graphs.", "labels": [], "entities": [{"text": "language interpretation", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.6988058388233185}]}, {"text": "It computes a numerical ranking of alternative SRHs and thus provides an important aid to the spoken language understanding component.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.6598440210024515}]}, {"text": "More precisely, the task of ON-TOSCORE in the system is to identify the best SRH suitable for further processing and evaluate it in terms of its contextual coherence against the domain and discourse knowledge.", "labels": [], "entities": []}, {"text": "The ONTOSCORE module currently employs two knowledge sources, an ontology (about 730 concepts and 200 relations) and a lexicon (ca.", "labels": [], "entities": []}, {"text": "3.600 words) with word to concept mappings, covering the respective domains of the system.", "labels": [], "entities": []}, {"text": "The evaluation of ONTOSCORE was carried out on a set of 95 dialogues.", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.41840025782585144}]}, {"text": "The resulting dataset contained 552 utterances resulting in 1,375 SRHs, corresponding to an average of 2.49 SRHs per user utterance.", "labels": [], "entities": []}, {"text": "The corpus had been annotated by humans subjects according to two separate annotation schemata.", "labels": [], "entities": []}, {"text": "The results of annotation experiments are reported in Section 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The scores for the SRHs of Example (1).", "labels": [], "entities": [{"text": "SRHs", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.5666653513908386}]}, {"text": " Table 3: The scores for the SRHs of Example 2.", "labels": [], "entities": [{"text": "SRHs", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.5948867201805115}]}]}