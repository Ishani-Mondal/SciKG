{"title": [{"text": "Towards a Framework for Learning Structured Shape Models from Text-Annotated Images", "labels": [], "entities": []}], "abstractContent": [{"text": "We present ongoing work on the topic of learning translation models between image data and text (English) captions.", "labels": [], "entities": [{"text": "learning translation models between image data and text (English) captions", "start_pos": 40, "end_pos": 114, "type": "TASK", "confidence": 0.7822137723366419}]}, {"text": "Most approaches to this problem assume a one-to-one or a flat, one-to-many mapping between a segmented image region and a word.", "labels": [], "entities": []}, {"text": "However, this assumption is very restrictive from the computer vision standpoint, and fails to account for two important properties of image segmentation: 1) objects often consist of multiple parts, each captured by an individual region; and 2) individual regions are often over-segmented into multiple subregions.", "labels": [], "entities": [{"text": "image segmentation", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7434821128845215}]}, {"text": "Moreover, this assumption also fails to capture the structural relations among words, e.g., part/whole relations.", "labels": [], "entities": []}, {"text": "We outline a general framework that accommodates a many-to-many mapping between image regions and words, allowing for struc-tured descriptions on both sides.", "labels": [], "entities": []}, {"text": "In this paper, we describe our extensions to the probabilis-tic translation model of Brown et al.", "labels": [], "entities": []}, {"text": "(1993) (as in Duygulu et al. (2002)) that enable the creation of structured models of image objects.", "labels": [], "entities": []}, {"text": "We demonstrate our work in progress, in which a set of annotated images is used to derive a set of labeled, structured descriptions in the presence of oversegmentation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Researchers in computer vision and computational linguistics have similar goals in their desire to automatically associate semantic information with the visual or linguistic representations they extract from an image or text.", "labels": [], "entities": []}, {"text": "Given paired image and text data, one approach is to use the visual and linguistic representations as implicit semantics for each other-that is, using the words as names for the visual features, and using the image objects as referents for the words in the text (cf.).", "labels": [], "entities": []}, {"text": "The goal of our work is to automatically acquire structured object models from image data associated with text, at the same time learning an assignment of text labels for objects as well as for their subparts (and, in the long run, also for collections of objects).", "labels": [], "entities": []}, {"text": "Multimodal datasets that contain both images and text are ubiquitous, including annotated medical images and the Corel dataset, not to mention the World Wide Web, allowing the possibility of associating textual and visual information in this way.", "labels": [], "entities": [{"text": "Corel dataset", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9635666310787201}]}, {"text": "For example, if a web crawler encountered many images containing a particular shape, and also found that the word chair was contained in the captions of those images, it might associate the shape with the word chair, simultaneously indicating a name for the shape and a visual \"definition\" for the word.", "labels": [], "entities": []}, {"text": "Such a framework could then learn the class names fora set of shape classes, effectively yielding a translation model between image shapes (or more generally, features) and words ().", "labels": [], "entities": []}, {"text": "This translation model could then be used to answer many types of queries, including labeling anew image in terms of its visible objects, or generating a visual prototype fora given class name.", "labels": [], "entities": []}, {"text": "Furthermore, since figure captions (or, in general, image annotations) may contain words for entire objects, as well as words for their component parts, a natural semantic hierarchy may emerge from the words.", "labels": [], "entities": []}, {"text": "For example, just as tables in the image maybe composed of \"leg\" image parts, the word leg can be associated with the word table in a part-whole relation.", "labels": [], "entities": []}, {"text": "Others have explored the problem of learning associations between image regions (or features) and text, including,,, and.", "labels": [], "entities": []}, {"text": "As impressive as the results are, these approaches make limiting assumptions that prevent them from being appropriate to our goals of a structured object model.", "labels": [], "entities": []}, {"text": "On the vision side, each segmented region is mapped one-to-one or one-to-many to words.", "labels": [], "entities": []}, {"text": "Conceptually, associating a word with only one region prevents an appropriate treatment of objects with parts, since such objects may consistently be region-segmented into a collection of regions corresponding to those components.", "labels": [], "entities": []}, {"text": "Practically, even putting aside the goal of part-whole processing, any given region maybe (incorrectly) oversegmented into a set of subregions (that are not component parts) in real images.", "labels": [], "entities": []}, {"text": "propose a ranking scheme for potential merges of regions based on a model of word-region association, but do not address the creation of a structured object model from sequences of merges.", "labels": [], "entities": []}, {"text": "To address these issues, we propose a more elaborate translation/association model in which we use the text of the image captions to guide us in structuring the regions.", "labels": [], "entities": [{"text": "translation/association", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.8618833621342977}]}, {"text": "On the language side of this task, words have typically been treated individually with no semantic structure among them (though see, which induces syntactic structure among the words).", "labels": [], "entities": []}, {"text": "Multiple words maybe assigned as the label to a region, but there's no knowledge of the relations among the words (and in fact they maybe treated as interchangeable labels,).", "labels": [], "entities": []}, {"text": "The more restrictive goal of image labeling has put the focus on the image as the (structured) object.", "labels": [], "entities": [{"text": "image labeling", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7307114452123642}]}, {"text": "But we take an approach in principle of building a structured hierarchy for both the image objects and their text labels.", "labels": [], "entities": []}, {"text": "In this way, we aim not only to use the words to help guide us in how to interpret image regions, but also to use the image structure to help us induce a part/whole hierarchy among the words.", "labels": [], "entities": []}, {"text": "For example, assume we find consistently associated leg and top regions together referred to as a table.", "labels": [], "entities": []}, {"text": "Then instead of treating leg and table, e.g., as two labels for the same object, we could capture the image part-whole structure as word relations in our lexicon.", "labels": [], "entities": []}, {"text": "Our goal of inducing associated structured hierarchies of visual and linguistic descriptions is a long-term one, and this paper reports on our work thus far.", "labels": [], "entities": []}, {"text": "We start with the probabilistic translation model of (as in), and extend it to structured shape descriptions of visual data.", "labels": [], "entities": []}, {"text": "As alluded to earlier, we distinguish between two types of structured shape descriptions: collections of regions that should be merged due to oversegmentation versus collections of regions that represent components of an object.", "labels": [], "entities": []}, {"text": "To handle both types, we incorporate into our algorithm several region merge operations that iteratively evaluate potential merges in terms of their improvement to the translation model.", "labels": [], "entities": []}, {"text": "These operations can exploit probabilities over region adjacency, thus constraining the potential combinatorial explosion of possible region merges.", "labels": [], "entities": []}, {"text": "We also permit a many-to-many mapping between regions and words, in support of our goal of inducing structured text as well, although here we report only on the structured image model, assuming similar mechanisms will be useful on the text side.", "labels": [], "entities": []}, {"text": "We are currently developing a system to demonstrate our proposal.", "labels": [], "entities": []}, {"text": "The input to the system is a set of images segmented into regions organized into a region adjacency graph.", "labels": [], "entities": []}, {"text": "Nodes in the graph encode the qualitative shape of a region using a shock graph (), while undirected edges represent region adjacency (used to constrain possible merges).", "labels": [], "entities": []}, {"text": "On the text side, each image has an associated caption which is processed by a part-ofspeech tagger and chunker.", "labels": [], "entities": []}, {"text": "The result is a set of noun phrases (nouns with associated modifiers) which mayor may not pertain to image content.", "labels": [], "entities": []}, {"text": "The output of the system is a set of many-to-many (possibly structured) associations between image regions and text words.", "labels": [], "entities": []}, {"text": "This paper represents work in progress, and not all the components have been fully integrated.", "labels": [], "entities": []}, {"text": "Initially, we have focused on the issues of building the structured image models.", "labels": [], "entities": []}, {"text": "We demonstrate the ideas on a set of annotated synthetic scenes with both multi-part objects and oversegmented objects/parts.", "labels": [], "entities": []}, {"text": "The results show that at least on simple scenes, the model can cope with oversegmentation and converge to a set of meaningful many-to-many (regions to words) mappings.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first experiment we report here (Exp. 1) tests our ability to learn a translation model in the presence of Type A and Type P segmentation errors.", "labels": [], "entities": []}, {"text": "We generated 1000 scenes with the following parameters: 1 or 2 objects per image, forced oversegmentation to a depth of 4, maximum 4 background shapes, one relevant word (part or whole descriptor), and maximum 2 meaningless random words per image.", "labels": [], "entities": []}, {"text": ") for this dataset, stopping the algorithm after step 1 (no merging) and after step 3.", "labels": [], "entities": []}, {"text": "For all of the objects, the merging step increased the probability of one word, and decreased the probability of the others, creating a stronger word-shape association.", "labels": [], "entities": []}, {"text": "For 5 of the objects, the highest probability word is a correct identifier of the object (stand, chair, stool, light, lamp), and for the   other object, a word indicating apart of the object has high probability.", "labels": [], "entities": []}, {"text": "Although increasing the strength of one probability has an advantage, we need to explore ways to allow association of more than one \"whole object\" word (such as lamp and light) with a single object (cf.).", "labels": [], "entities": []}, {"text": "Since we maintain the component regions of a merged region, having both apart and a whole word, such as leg and table, associated with the same image is not a problem.", "labels": [], "entities": []}, {"text": "Incorporating these into a structured word hierarchy should help to focus associations appropriately.", "labels": [], "entities": []}, {"text": "Another way to view the data is to see which shapes are most consistently associated with the meaningful words in the captions.", "labels": [], "entities": []}, {"text": ", with the latter normalized overall shapes.", "labels": [], "entities": []}, {"text": "A problem with this formulation is that, due to the Pr \u00a5 r \u00a7 component, high frequency shapes can increase the probability of primitive components.", "labels": [], "entities": [{"text": "Pr \u00a5 r \u00a7 component", "start_pos": 52, "end_pos": 70, "type": "METRIC", "confidence": 0.9214317560195923}]}, {"text": "However, the merging steps (2 and 3) of our algorithm raise the frequencies of complex (multi-region) shapes.", "labels": [], "entities": []}, {"text": "shows the five shapes with the highest values for each meaningful word, again before and after the merging steps in Exp.", "labels": [], "entities": []}, {"text": "-0.00 0.00 0.00 0.00 0.00 0.00 table 0.00 0.00 0.00 0.00 0.00 0.00 stand 0.00 1.00 0.00 0.00 0.00 0.00 chair 0.00 0.00 0.98 0.00 0.00 0.00 stool 0.00 0.00 0.00 0.41 0.00 0.00 lamp 0.00 0.00 0.00 0.00 0.03 0.99 light 0.00 0.00 0.00 0.00 0.97 0.00 leg 1.00 0.00 0.01 0.00 0.00 0.00 base 0.00 0.00 0.00 0.00 0.00 0.00 (b) after merging (steps 2/3)     for the meaningful words, after step 4.", "labels": [], "entities": []}, {"text": "Shape icons (for merged regions) or primitive shapes (indicated by number) have the probability for that word listed below.", "labels": [], "entities": []}, {"text": "eral complex shapes increase in probability after merging, and a number of new complex shapes appear in the lists.", "labels": [], "entities": [{"text": "probability", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.958259105682373}]}, {"text": "We report on one other experiment (Exp. 2) which was designed to test our approach to handling oversegmentations of Type C in step 4 of our algorithm.", "labels": [], "entities": []}, {"text": "Our dataset again had 1000 images; here there was only one object per image, but every object was oversegmented into its primitive parts (that is, an object never appeared as a complete silhouette).", "labels": [], "entities": []}, {"text": "(We did not allow oversegmentation of the primitives here, nor did we include irrelevant words in the captions.)", "labels": [], "entities": []}, {"text": "Because our 6 objects never appear \"whole,\" steps 2 and 3 of our algorithm cannot apply; before step 4, words are associated with primitive shapes only.", "labels": [], "entities": []}, {"text": "After step 4, the highest probability word (Pr \u00a5 w \u00a6 r \u00a7 ) for 4 of the objects is a correct identifier of the object (stand, chair, stool, light); for one object, a word indicating apart of the object had high probability (leg for the rectangular table).", "labels": [], "entities": [{"text": "Pr \u00a5 w \u00a6 r \u00a7 )", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.9092558111463275}]}, {"text": "(One object silhouette-the second lamp-was not fully reconstructed.) shows the five shapes with the highest Pr \u00a5 r \u00a6 w \u00a7 values for each meaningful word, after step 4.", "labels": [], "entities": [{"text": "Pr \u00a5 r \u00a6 w", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9452144384384156}]}, {"text": "For 3 of the whole object words (stand, stool, light), and both part words (leg, base), the best shape is a correct one.", "labels": [], "entities": []}, {"text": "For the remaining whole object words (table, chair, lamp), a correct full silhouette is one of the top five.", "labels": [], "entities": []}, {"text": "Step 4 clearly has high potential for reconstructing objects that are consistently oversegmented into their parts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Exp. 1: Translation tables from shapes to words P\u00a5 w\u00a6 r \u00a7 for the 6 object silhouettes. 1  stand 0.55 0.33 0.13 0.00 0.00", "labels": [], "entities": []}]}