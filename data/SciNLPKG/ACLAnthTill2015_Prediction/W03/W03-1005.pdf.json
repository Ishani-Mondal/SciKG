{"title": [{"text": "Antecedent Recovery: Experiments with a Trace Tagger", "labels": [], "entities": [{"text": "Antecedent Recovery", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9518257975578308}]}], "abstractContent": [{"text": "This paper explores the problem of finding non-local dependencies.", "labels": [], "entities": []}, {"text": "First, we isolate a set of features useful for this task.", "labels": [], "entities": []}, {"text": "Second, we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which finds non-local dependencies while parsing.", "labels": [], "entities": []}, {"text": "We find that the former outperforms the latter because it makes better use of the features we isolate.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many broad-coverage statistical parsers are notable to give a full interpretation for sentences such as: It is difficult to guess what she wants to buy.", "labels": [], "entities": []}, {"text": "Building the semantic interpretation of this sentence requires recovering three non-local relations: (i) the object of buy is what; 1 (ii) the subject of buy is she; and (iii) guess does not have a subject in the sentence.", "labels": [], "entities": []}, {"text": "Three approaches have been proposed to detect such relations: (i) post-processing the output of a parser not designed to detect extraction sites); (ii) integrating antecedent recovery into the parser (henceforth in-processing) by either enriching a syntactically simple model or using a more powerful syntactic framework); and (iii) detecting non-local dependencies as a pre-processing step before parsing.", "labels": [], "entities": []}, {"text": "While the pre-processing approach is reported to give state-of-the-art performance using unlexicalized parsers, it has not been tested using lexicalized models.", "labels": [], "entities": []}, {"text": "Our main claim is that that the pre-processing approach, coupled with a lexicalized parser outperforms both state-of-the-art postprocessing and in-processing.", "labels": [], "entities": []}, {"text": "However, we show that Model 3 of can be generalized to handle all types of long-distance dependencies with performance close to the pre-processing architecture.", "labels": [], "entities": []}, {"text": "A general contribution of this paper is that it gives important insights about the nature of the problem.", "labels": [], "entities": []}, {"text": "Recovering non-local semantic relations is regarded to be a difficult problem.", "labels": [], "entities": []}, {"text": "The successes (and failures) of the simple architecture outlined here help determine what features are to be incorporated into a parser in order to improve recovery of non-local dependencies.", "labels": [], "entities": []}, {"text": "The overall organization of the paper is as follows.", "labels": [], "entities": []}, {"text": "First, Section 2 sketches the material we use for the experiments in the paper.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss a finite-state system, a trace tagger, that detects extraction sites without knowledge of phrasestructure and we isolate important cues for the task.", "labels": [], "entities": []}, {"text": "Section 4 combines the trace tagger with a parser in order to recover antecedents.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we investigate whether and how detection of extraction sites and antecedent recovery can be integrated into a lexicalized stochastic parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on all sentences in the test section of the treebank.", "labels": [], "entities": []}, {"text": "As with trace detection, we use the measure introduced by.", "labels": [], "entities": [{"text": "trace detection", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.8391827046871185}]}, {"text": "This metric works by treating EEs and their antecedents as fourtuples, consisting of the type of the EE, its location, the type of its antecedent and the location(s) (beginning and end) of the antecedent.", "labels": [], "entities": []}, {"text": "An antecedent is correctly recovered if all four values match the gold standard.", "labels": [], "entities": []}, {"text": "We calculate the precision, recall, and Fscore; however for brevity's sake we only report the F-score for most experiments in this section.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9998441934585571}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9997060894966125}, {"text": "Fscore", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9998193383216858}, {"text": "F-score", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9990799427032471}]}, {"text": "In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall.", "labels": [], "entities": [{"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9721627831459045}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9639201760292053}, {"text": "F-Score", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.8733270168304443}, {"text": "PARSEVAL-style labeled bracketing", "start_pos": 123, "end_pos": 156, "type": "TASK", "confidence": 0.602093497912089}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.47983044385910034}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.99700528383255}]}], "tableCaptions": [{"text": " Table 3: F-Scores for parsing and antecedent recov- ery on Section 23.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9927265644073486}, {"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9694008231163025}, {"text": "recov- ery", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.8785165349642435}]}, {"text": " Table 4: Comparison of our antecedent recov- ery results with the lexicalized parser and John- son's (2002).", "labels": [], "entities": [{"text": "John- son's (2002)", "start_pos": 90, "end_pos": 108, "type": "DATASET", "confidence": 0.8828639388084412}]}, {"text": " Table 5: INSERT model unlexicalized parsing results  on Section 23.", "labels": [], "entities": [{"text": "INSERT", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.6257981061935425}]}, {"text": " Table 6: INSERT model lexicalized parsing results  on Section 23.", "labels": [], "entities": [{"text": "INSERT", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.5366465449333191}]}, {"text": " Table 7: Comparison of pre-processing with lexical- ized in-processing (F-scores).", "labels": [], "entities": []}]}