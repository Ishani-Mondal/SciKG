{"title": [{"text": "Noun-Noun Compound Machine Translation: A Feasibility Study on Shallow Processing", "labels": [], "entities": [{"text": "Noun-Noun Compound Machine Translation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5105448290705681}]}], "abstractContent": [{"text": "The translation of compound nouns is a major issue in machine translation due to their frequency of occurrence and high productivity.", "labels": [], "entities": [{"text": "translation of compound nouns", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8749033212661743}, {"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7737029194831848}]}, {"text": "Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word com-positional machine translation.", "labels": [], "entities": [{"text": "translate compound nouns", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.8222700754801432}, {"text": "memory-based machine translation", "start_pos": 98, "end_pos": 130, "type": "TASK", "confidence": 0.6359720428784689}, {"text": "word-to-word com-positional machine translation", "start_pos": 135, "end_pos": 182, "type": "TASK", "confidence": 0.5758598819375038}]}, {"text": "This paper describes the results of a feasibility study on the ability of these methods to translate Japanese and English noun-noun compounds .", "labels": [], "entities": [{"text": "translate Japanese and English noun-noun compounds", "start_pos": 91, "end_pos": 141, "type": "TASK", "confidence": 0.817205419143041}]}], "introductionContent": [{"text": "Multiword expressions are problematic in machine translation (MT) due to the idiomaticity and overgeneration problems ().", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8382597863674164}]}, {"text": "Idiomaticity is the problem of compositional semantic unpredictability and/or syntactic markedness, as seen in expressions such as kick the bucket ) and by and large, respectively.", "labels": [], "entities": []}, {"text": "Overgeneration occurs as a result of a system failing to capture idiosyncratic lexical affinities between words, such as the blocking of seemingly equivalent word combinations (e.g. many thanks vs. *several thanks).", "labels": [], "entities": []}, {"text": "In this paper, we target the particular task of the Japanese \u00a1 English machine translation of noun-noun compounds to outline the various techniques that have been proposed to tackle idiomaticity and overgeneration, and carryout detailed analysis of their viability over naturally-occurring data.", "labels": [], "entities": [{"text": "Japanese \u00a1 English machine translation of noun-noun compounds", "start_pos": 52, "end_pos": 113, "type": "TASK", "confidence": 0.6935693137347698}]}, {"text": "Noun-noun (NN) compounds (e.g. web server, car park) characteristically occur with high frequency and high lexical and semantic variability.", "labels": [], "entities": []}, {"text": "A summary examination of the 90m-word written component of the British National Corpus) unearthed over 400,000 NN compound types, with a combined token frequency of 1.3m; 1 that is, over 1% of words in the BNC are NN compounds.", "labels": [], "entities": [{"text": "British National Corpus)", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.9433900862932205}]}, {"text": "Moreover, if we plot the relative token coverage of the most frequently-occurring NN compound types, we find that the low-frequency types account fora sig- Results based on the method described in nificant proportion of the type count (see ).", "labels": [], "entities": []}, {"text": "To achieve 50% token coverage, e.g., we require coverage of the top 5% most-frequent NN compounds, amounting to roughly 70,000 types with a minimum token frequency of 10.", "labels": [], "entities": []}, {"text": "NN compounds are especially prevalent in technical domains, often with idiosyncratic semantics: found that NN compounds accounted for almost 20% of entries in a Japanese-English financial terminological dictionary.", "labels": [], "entities": []}, {"text": "Various claims have been made about the level of processing complexity required to translate NN compounds, and proposed translation methods range over abroad spectrum of processing complexity.", "labels": [], "entities": []}, {"text": "There is a clear division between the proposed methods based on whether they attempt to interpret the semantics of the NN compound (i.e. use deep processing), or simply use the source language word forms to carryout the translation task (i.e. use shallow processing).", "labels": [], "entities": []}, {"text": "It is not hard to find examples of semantic mismatch in NN compounds to motivate deep translation methods: the Japanese \u00a3 \u00a5 \u00a4 \u00a7 \u00a6 \u00a9 \u00a8 \u00a5 idobata\u00a8kaigiidobata\u00a8kaigi \"(lit.) well-side meeting\", 3 e.g., translates most naturally into English as \"idle gossip\", which a shallow method would be hard put to predict.", "labels": [], "entities": [{"text": "\u00a9", "start_pos": 130, "end_pos": 131, "type": "METRIC", "confidence": 0.8313077688217163}]}, {"text": "Our interest is in the relative occurrence of such NN compounds and their impact on the performance of shallow translation methods.", "labels": [], "entities": [{"text": "shallow translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.6916504204273224}]}, {"text": "In particular, we seek to determine what proportion of NN compounds shallow translation translation methods can reasonably translate and answer the question: do shallow methods perform well enough to preclude the need for deep processing?", "labels": [], "entities": []}, {"text": "The answer to this question takes the form of an estimation of the upper bound on translation performance for shallow translation methods.", "labels": [], "entities": []}, {"text": "In order to answer this question, we have selected the language pair of English and Japanese, due to the high linguistic disparity between the two languages.", "labels": [], "entities": []}, {"text": "We consider the tasks of both English-toJapanese (EJ) and Japanese-to-English (JE) NN compound translation over fixed datasets of NN compounds, and apply representative shallow MT methods to the data.", "labels": [], "entities": [{"text": "Japanese-to-English (JE) NN compound translation", "start_pos": 58, "end_pos": 106, "type": "TASK", "confidence": 0.5645340553351811}, {"text": "MT", "start_pos": 177, "end_pos": 179, "type": "TASK", "confidence": 0.9617151021957397}]}, {"text": "While stating that English and Japanese are highly linguistically differentiated, we recognise that there are strong syntactic parallels between the two languages with respect to the compound noun construction.", "labels": [], "entities": []}, {"text": "At the same time, there are large volumes of subtle lexical and expressional divergences between the two languages, as evidenced between jiteNsha\u00a8seNshujiteNsha\u00a8seNshu \"(lit.) bicycle athelete\" and its translation competitive cyclist.", "labels": [], "entities": []}, {"text": "In this sense, we claim that English and Japanese are representative of the inherent difficulty of NN compound translation.", "labels": [], "entities": [{"text": "NN compound translation", "start_pos": 99, "end_pos": 122, "type": "TASK", "confidence": 0.717771569887797}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the testdata to evaluate MBMT DICT and DMT COMP . Both methods potentially produce multiple translations candidates fora given input, from which a unique translation output must be selected in someway.", "labels": [], "entities": [{"text": "MBMT DICT", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.6014336943626404}, {"text": "DMT COMP", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.6335806548595428}]}, {"text": "So as to establish an upper bound on the feasibility of each method, we focus on the translation candidate generation step in this paper and leave the second step of translation selection as an item for further research.", "labels": [], "entities": [{"text": "translation candidate generation", "start_pos": 85, "end_pos": 117, "type": "TASK", "confidence": 0.8758228222529093}, {"text": "translation selection", "start_pos": 166, "end_pos": 187, "type": "TASK", "confidence": 0.952184647321701}]}, {"text": "With MBMT DICT , we calculate the upper bound by simply checking for the gold-standard translation within the translation candidates.", "labels": [], "entities": [{"text": "MBMT DICT", "start_pos": 5, "end_pos": 14, "type": "DATASET", "confidence": 0.6215744912624359}]}, {"text": "In the case of DMT COMP , rather than generating all translation candidates and checking among them, we take a predetermined set of translation templates and a simplex translation dictionary to test for word alignment.", "labels": [], "entities": [{"text": "DMT COMP", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.5186484009027481}, {"text": "word alignment", "start_pos": 203, "end_pos": 217, "type": "TASK", "confidence": 0.7687564194202423}]}, {"text": "Word alignment is considered to have been achieved if there exists a translation template and set of word translations which lead to an isomorphic mapping onto the gold-standard translation.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7377805709838867}]}, {"text": "For byway of DMT COMP . Note here that derivational morphology is used to convert the nominal translation of territory into the adjective territorial.", "labels": [], "entities": [{"text": "DMT COMP", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.6931916177272797}]}, {"text": "On the first word-alignment pass for DMT COMP , the translation pairs in each dataset were automatically aligned using only ALTDIC.", "labels": [], "entities": [{"text": "DMT COMP", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.6107532978057861}]}, {"text": "We then manual inspected the unaligned translation pairs for translation pairs which were not aligned simply because of patchy coverage in ALTDIC.", "labels": [], "entities": [{"text": "ALTDIC", "start_pos": 139, "end_pos": 145, "type": "DATASET", "confidence": 0.814713180065155}]}, {"text": "In such cases, we manually supplemented ALTDIC with simplex translation pairs taken from the Genius Japanese-English dictionary (Konishi, 1997), resulting in an additional 178 simplex entries.", "labels": [], "entities": [{"text": "Genius Japanese-English dictionary (Konishi, 1997)", "start_pos": 93, "end_pos": 143, "type": "DATASET", "confidence": 0.8513718321919441}]}, {"text": "We then performed a second pass of alignment using the supplemented ALTDIC (ALTDICb ).", "labels": [], "entities": [{"text": "alignment", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.9673925638198853}]}, {"text": "Below, we present the results for both the original ALTDIC and ALTDICb .  The principal evaluatory axes we consider in comparing the different methods are coverage and accuracy: coverage is the relative proportion of a given set of NN compounds that the method can generate some translation for, and accuracy describes the proportion of translated NN compounds for which the goldstandard translation is reproduced (irrespective of how many other translations are generated).", "labels": [], "entities": [{"text": "coverage", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9912319779396057}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9988043308258057}, {"text": "coverage", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9878608584403992}, {"text": "accuracy", "start_pos": 300, "end_pos": 308, "type": "METRIC", "confidence": 0.9991878867149353}]}, {"text": "These two tend to be indirect competition, in that more accurate methods tend to have lower coverage, and conversely higher coverage methods tend to have lower accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9940289258956909}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9966831803321838}]}, {"text": "So as to make cross-system comparison simple, we additionally combine these two measures into an Fscore, that is their harmonic mean.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9972206354141235}]}], "tableCaptions": [{"text": " Table 2: Results for MBMT DICT (F = F-score)", "labels": [], "entities": [{"text": "MBMT DICT", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.4033573120832443}, {"text": "F", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.9919419288635254}, {"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.6718078255653381}]}, {"text": " Table 3: Alignment-based results for DMT COMP", "labels": [], "entities": [{"text": "Alignment-based", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9016898274421692}, {"text": "DMT COMP", "start_pos": 38, "end_pos": 46, "type": "TASK", "confidence": 0.7715610265731812}]}, {"text": " Table 4: Cascaded translation results", "labels": [], "entities": [{"text": "Cascaded translation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7666700482368469}]}, {"text": " Table 5: Performance vs. translation fan-out (JE)", "labels": [], "entities": [{"text": "JE)", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.5022871792316437}]}]}