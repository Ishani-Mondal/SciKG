{"title": [{"text": "Why can't Jos\u00e9 read? The problem of learning semantic associations in a robot environment", "labels": [], "entities": []}], "abstractContent": [{"text": "We study the problem of learning to recognise objects in the context of autonomous agents.", "labels": [], "entities": []}, {"text": "We cast object recognition as the process of attaching meaningful concepts to specific regions of an image.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7635878622531891}]}, {"text": "In other words, given a set of images and their captions, the goal is to segment the image, in either an intelligent or naive fashion, then to find the proper mapping between words and regions.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate that a model that learns spatial relationships between individual words not only provides accurate annotations, but also allows one to perform recognition that respects the real-time constraints of an autonomous, mobile robot.", "labels": [], "entities": []}], "introductionContent": [{"text": "In writing this paper we hope to promote a discussion on the design of an autonomous agent that learns semantic associations in its environment or, more precisely, that learns to associate regions of images with discrete concepts.", "labels": [], "entities": []}, {"text": "When an image region is labeled with a concept in an appropriate and consistent fashion, we say that the object has been recognised ( ).", "labels": [], "entities": []}, {"text": "We use our laboratory robot, Jos\u00e9 (), as a prototype, but the ideas presented here extend to a wide variety of settings and agents.", "labels": [], "entities": []}, {"text": "Before we proceed, we must elucidate on the requirements for achieving semantic learning in an autonomous agent context.", "labels": [], "entities": []}, {"text": "Primarily, we need a model that learns associations between objects given a set of images paired with user input.", "labels": [], "entities": []}, {"text": "Formally, the task is to find a function that separates the space of image patch descriptions into n w semantic concepts, where n w is the total number of concepts in the: The image on the left is Jos\u00e9 (), the mobile robot we used to collect the image data.", "labels": [], "entities": []}, {"text": "The images on the right are examples the robot has captured while roaming in the lab, along with labels used for training.", "labels": [], "entities": []}, {"text": "We depict image region annotations in later figures, but we emphasize that the robot receives only the labels as input for training.", "labels": [], "entities": []}, {"text": "That is, the robot does not know what words correspond to the image regions.", "labels": [], "entities": []}, {"text": "training set (from now on we use the word \"patch\" to refer to a contiguous region in an image).", "labels": [], "entities": []}, {"text": "These supplied concepts could be in the form of text captions, speech, or anything else that might convey semantic information.", "labels": [], "entities": []}, {"text": "For the time being, we restrict the set of concepts to English nouns (e.g. \"face\", \"toothbrush\", \"floor\").", "labels": [], "entities": []}, {"text": "See Figure 1 for examples of images paired with captions composed of nouns.", "labels": [], "entities": []}, {"text": "Despite this restriction, we still leave ourselves open to a great deal of ambiguity and uncertainty, in part because objects can be described at several different levels of specificity, and at the same level using different words (e.g. is it \"sea\", \"ocean\", \"wave\" or \"water\"?).", "labels": [], "entities": []}, {"text": "Ideally, one would like to impose a hierarchy of lexical concepts, as in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9493722915649414}]}, {"text": "We have yet to explore WordNet for our proposed framework, though it has been used successfully for image clustering ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.936834990978241}, {"text": "image clustering", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7972243130207062}]}, {"text": "Image regions, or patches, are described by a set of low-level features such as average and standard deviation of colour, average oriented Gabor filter responses to represent texture, and position in space.", "labels": [], "entities": []}, {"text": "The set of patch descriptions forms an n f -dimensional space of real numbers, where n f is the number of features.", "labels": [], "entities": []}, {"text": "Even complex low-level features are far from adequate for the task of classifying patches as objects -at some point we need to move to representations that include high-level information.", "labels": [], "entities": []}, {"text": "In this paper we take a small step in that direction since our model learns spatial relations between concepts.", "labels": [], "entities": []}, {"text": "Given the uncertainty regarding descriptions of objects and their corresponding concepts, we further require that the model be probabilistic.", "labels": [], "entities": []}, {"text": "In this paper we use Bayesian techniques to construct our object recognition model.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.880537748336792}]}, {"text": "Implicitly, we need a thorough method for decomposing an image into conceptually contiguous regions.", "labels": [], "entities": []}, {"text": "This is not only non-trivial, but also impossible without considering semantic associations.", "labels": [], "entities": []}, {"text": "This motivates the segmentation of images and learning associations between patches and words as tightly coupled processes.", "labels": [], "entities": []}, {"text": "The subject of segmentation brings up another important consideration.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.9801191091537476}]}, {"text": "A good segmentation algorithm such as Normalized Cuts) can take on the order of a minute to complete.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.9646778702735901}]}, {"text": "For many real-time applications this is an unaffordable expense.", "labels": [], "entities": []}, {"text": "It is important to abide by real-time constraints in the case of a mobile robot, since it has to simultaneously recognise and negotiate obstacles while navigating in its environment.", "labels": [], "entities": []}, {"text": "Our experiments suggest that the costly step of a decoupled segmentation can be avoided without imposing a penalty to object recognition performance.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8569945394992828}]}, {"text": "Autonomous semantic learning must be considered a supervised processor, as we will see later on, a partiallysupervised process since the associations are made from the perspective of humans.", "labels": [], "entities": [{"text": "Autonomous semantic learning", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7044806480407715}]}, {"text": "This motivates a second requirement: a system for the collection of data, ideally in an on-line fashion.", "labels": [], "entities": []}, {"text": "As mentioned above, user input could come in the form of text or speech.", "labels": [], "entities": []}, {"text": "However, the collection of data for supervised classification is problematic and time-consuming for the user overseeing the autonomous agent, since the user is required to tediously feed the agent with self-annotated regions of images.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6621358096599579}]}, {"text": "If we relax our requirement on training data acquisition by requesting captions at an image level, not at a patch level, the acquisition of labeled data is suddenly much less challenging.", "labels": [], "entities": [{"text": "training data acquisition", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6750983397165934}]}, {"text": "Throughout this paper, we use manual annotations purely for testing only -we emphasize that the training data includes only the labels paired with images.", "labels": [], "entities": []}, {"text": "We are no longer exploring object recognition as a strict classification problem, and we do so at a cost since we are no longer blessed with the exact associations between image regions and nouns.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.796141505241394}]}, {"text": "As a result, the learning problem is now unsupervised.", "labels": [], "entities": []}, {"text": "For a single training image and a particular word token, we must now learn both the probability of generating that word given an object description and the correct association to one of the regions with the image.", "labels": [], "entities": []}, {"text": "Fortunately, there is a straightforward parallel between our object recognition formulation and the statistical machine translation problem of building a lexicon from an aligned bitext (.", "labels": [], "entities": [{"text": "object recognition formulation", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.9002904494603475}, {"text": "statistical machine translation", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.638383279244105}]}, {"text": "Throughout this paper, we reason about object recognition with this analogy in mind ( ).", "labels": [], "entities": [{"text": "object recognition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8945751190185547}]}, {"text": "What other requirements should we consider?", "labels": [], "entities": []}, {"text": "Since our discussion involves autonomous agents, we should pursue a dynamic data acquisition model.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.7645710110664368}]}, {"text": "We can consider the problem of learning an object recognition model as an on-line conversation between the robot and the user, and it follows the robot should be able to participate.", "labels": [], "entities": [{"text": "object recognition model", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7748987178007761}]}, {"text": "If the agent ventures into \"unexplored territory\", we would like it to make unprompted requests for more assistance.", "labels": [], "entities": []}, {"text": "One could use active learning to implement a scheme for requesting user input based on what information would be most valuable to classification.", "labels": [], "entities": []}, {"text": "This has yet to be explored for object recognition, but it has been applied to the related domain of image retrieval).", "labels": [], "entities": [{"text": "object recognition", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8513253331184387}, {"text": "image retrieval", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7223586440086365}]}, {"text": "Additionally, the learning process could be coupled with reinforcement -in other words, the robot could offer hypotheses for visual input and await feedback from user.", "labels": [], "entities": []}, {"text": "In the next section, we outline our proposed contextual translation model.", "labels": [], "entities": [{"text": "contextual translation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6330777406692505}]}, {"text": "In Section 3, we weigh the merits of several different error measures for the purposes of evaluation.", "labels": [], "entities": []}, {"text": "The experimental results on the robot data are given in Section 4.", "labels": [], "entities": []}, {"text": "We leave discussion of results and future work to the final section of this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before we discuss what makes a good evaluation metric, it will help if we answer this question: \"what makes a good image annotation?\"", "labels": [], "entities": []}, {"text": "As we will see, there is no straightforward answer.", "labels": [], "entities": []}, {"text": "It is fair to say that certain concepts in an image are more prominent than others.", "labels": [], "entities": []}, {"text": "One might take the approach that objects that consume the most space in an image are the most important, and this is roughly the evaluation criterion used in previous papers . Consider the image on the left in.", "labels": [], "entities": []}, {"text": "We claim that \"polar bear\" is at least as important as snow.", "labels": [], "entities": []}, {"text": "There is an easy way to test this assertion -pretend the image is annotated either entirely as \"snow\" or entirely as \"polar bear\".", "labels": [], "entities": []}, {"text": "In our experience, people find the latter annotation as appealing, if not more, than the former.", "labels": [], "entities": []}, {"text": "Therefore, one would conclude that it is better to weight all concepts equally, regardless of size, which brings us to the image on the right.", "labels": [], "entities": []}, {"text": "If we treat all words equally, having many words in a single label obfuscates the goal of getting the most important concept,\"train\", correct.", "labels": [], "entities": []}, {"text": "Ideally, when collecting user-annotated images for the purpose of evaluation, we should tag each word with a weight to specify its prominence in the scene.", "labels": [], "entities": []}, {"text": "In practice, this is problematic because different users focus their attention on different concepts, not to mention the fact that it is an burdensome task.", "labels": [], "entities": []}, {"text": "For lack of a good metric, we evaluate the proposed translation models using two error measures.", "labels": [], "entities": []}, {"text": "Error measure 1 reports an error of 1 if the model annotation with the highest probability results in an incorrect patch annotation.", "labels": [], "entities": []}, {"text": "The error is averaged over the number of patches in each image, and then again over the number of images in the data set.", "labels": [], "entities": []}, {"text": "Error measure 2 is similar, only we average the error over the patches corresponding to word (according to the manual annotations).", "labels": [], "entities": [{"text": "Error measure 2", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9693935712178549}]}, {"text": "The equations are given by where P ni is the set of patches in image n that are manually-annotated using word i, \ud97b\udf59 a nj is the model alignment with the highest probability, \u02dc a nj is the provided \"true\" annotation, and \u03b4 \u02dc anj (\ud97b\udf59 a nj ) is 1 if\u00e3if\u02dcif\u00e3 nj = \ud97b\udf59 a nj . Our intuition is that the metric where we weight all concepts equally, regardless of size, is better overall.", "labels": [], "entities": []}, {"text": "As we will see in the next section, our translation models do not perform as well under this error measure.", "labels": [], "entities": []}, {"text": "This is due to the fact that the joint probability shown in equation 1 maximises the first error metric, not the second.", "labels": [], "entities": []}, {"text": "Since the agent cannot know the true annotations beforehand, it is difficult to construct a model that maximises the second error measure, but we are currently pursuing approximations to this metric.", "labels": [], "entities": []}, {"text": "We built a data set by having Jos\u00e9 the robot roam around the lab taking pictures, and then having laboratory members create captions for the data using a consistent set of words.", "labels": [], "entities": []}, {"text": "For evaluation purposes, we manually annotated the images.", "labels": [], "entities": []}, {"text": "The robomedia data set is composed of 107 training images and 43 test images . The training and test sets contain a combined total of 21 word tokens.", "labels": [], "entities": [{"text": "robomedia data set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8792027433713278}]}, {"text": "The word frequencies in the labels and manual annotations are shown in.", "labels": [], "entities": []}, {"text": "In our experiments, we consider two scenarios.", "labels": [], "entities": []}, {"text": "In the first, we use Normalized Cuts) to segment the images into distinct patches.", "labels": [], "entities": []}, {"text": "In the second scenario, we take on the object recognition task without the aid of a sophisticated segmentation algorithm, and instead construct a uniform grid of patches over the image.", "labels": [], "entities": [{"text": "object recognition task", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8758266369501749}]}, {"text": "Examples of different segmentations are shown along with the anecdotal results in.", "labels": [], "entities": []}, {"text": "For the crude segmentation, we used patches of height and width approximately 1/6th the size of the image.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.7739050388336182}]}, {"text": "We found that smaller patches introduced too much noise to the features and resulted in poor test performance, and larger patches contained too many objects at once.", "labels": [], "entities": []}, {"text": "In future work, we: The first four columns list the probability of finding a particular word in a label and a manually annotated patch, in the robomedia training and test sets.", "labels": [], "entities": []}, {"text": "The final two columns show the precision of the translation model tMRF using the grid segmentation for each token, averaged over the 12 trials.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9993034601211548}]}, {"text": "Precision is defined as the probability the model's prediction is correct fora particular word and patch.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9926677346229553}]}, {"text": "Since precision is 1 minus the error of equation 3, the total precision on both the training and test sets matches the average performance of tMRF-patch on Error measure 2, as shown in in.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9992658495903015}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9965121150016785}]}, {"text": "While not presented in the table, the precision on individual words varies significantly from one one trial to the next.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9993172883987427}]}, {"text": "Note that some words do not appear in both the training and test sets, hence the n/a.", "labels": [], "entities": []}, {"text": "\u2020The model predicts words without access to the test image labels.", "labels": [], "entities": []}, {"text": "We provide this information for completeness.", "labels": [], "entities": []}, {"text": "\u2021We can use the manual annotations for evaluation purposes, but we underline the fact that an agent would not have access to the information presented in the \"Annotation %\" column.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.9821988940238953}]}, {"text": "will investigate a hierarchical patch representation to take into account both short and long range patch interactions, as in).", "labels": [], "entities": []}, {"text": "The first is the translation model where dependencies between alignments are removed for the sake of tractability, called tInd.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9709248542785645}]}, {"text": "The second is the translation model in which we assume dependences \ud97b\udf59: Correct annotations for Normalized Cuts, grid and manual segmentations.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9712138175964355}]}, {"text": "When there are multiple annotations in a single patch, anyone of them is correct.", "labels": [], "entities": []}, {"text": "Even when both are correct, the grid segmentation is usually more precise and, as a result, more closely approximates generic object recognition. between adjacent alignments in the image.", "labels": [], "entities": [{"text": "grid segmentation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7347756922245026}, {"text": "generic object recognition.", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.5870775381724039}]}, {"text": "This model is denoted by tMRF.", "labels": [], "entities": []}, {"text": "We represent the sophisticated and crude segmentation scenarios by -seg and -patch, respectively.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.9593355059623718}]}, {"text": "One admonition regarding the evaluation procedure: a translation is deemed correct if at least one of the patches corresponds to the model's prediction.", "labels": [], "entities": []}, {"text": "Ina manner of speaking, when a segment encompasses several concepts, we are giving the model the benefit of the doubt.", "labels": [], "entities": []}, {"text": "For example, according to our evaluation the annotations for both the grid and Normalized Cuts segmentations shown incorrect.", "labels": [], "entities": [{"text": "Normalized Cuts segmentations", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.5158207416534424}]}, {"text": "However, from observation the grid segmentation provides a more precise object recognition.", "labels": [], "entities": [{"text": "grid segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.721938744187355}, {"text": "object recognition", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7871285974979401}]}, {"text": "As a result, evaluation can be unreliable when Normalized Cuts offers poor segmentations.", "labels": [], "entities": []}, {"text": "It is also important to remember that the true result images shown in the second column of are idealisations.", "labels": [], "entities": []}, {"text": "Experimental results on 12 trials are shown in, and selected annotations predicted by the tMRF model on the test set are shown in.", "labels": [], "entities": []}, {"text": "The most significant result is that the contextual translation model performs the best overall, and performs equally well when supplied with either Normalized Cuts or a naive segmentations.", "labels": [], "entities": [{"text": "contextual translation", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6956373155117035}]}, {"text": "We stress that even though the models trained using both the grid and Normalized Cuts segmentations are displayed on the same plots, in we indicate that object recognition using the grid segmentation is generally more precise, given the same evaluation result in.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.8249194920063019}]}, {"text": "Learning contextual dependencies between alignment appears to improve performance, despite the large amount of noise and the increase in the number of model parameters that have to be learned.", "labels": [], "entities": []}, {"text": "The contex- tual model also tends to produce more visually appealing annotations since they the translations smoothed over neighbourhoods of patches.", "labels": [], "entities": []}, {"text": "The performance of the contextual translation model on individual words on the training and test sets is shown in, averaged over the trials.", "labels": [], "entities": [{"text": "contextual translation", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.6932894438505173}]}, {"text": "Since our approximate EM training a local maximum point estimate for the joint posterior and the initial model parameters are set to random values, we obtain a great deal of variance from one trial to the next, as observed in the Box-andWhisker plots in.", "labels": [], "entities": []}, {"text": "While not shown in, we have noticed considerable variation in what words are predicted with high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9800907969474792}]}, {"text": "For example, the word \"ceiling\" is predicted with an average success rate of 0.347, although the precision on individual trials ranges from 0 to 0.842.: Selected annotations on the robomedia test data predicted by the contextual (tMRF) translation model.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9987108707427979}, {"text": "robomedia test data", "start_pos": 181, "end_pos": 200, "type": "DATASET", "confidence": 0.8175049622853597}]}, {"text": "We show our model's predictions using both sophisticated and crude segmentations.", "labels": [], "entities": []}, {"text": "The \"true\" annotations are shown in the second column.", "labels": [], "entities": []}, {"text": "Notice that the annotations using Normalized Cuts tend to be more visually appealing compared to the rectangular grid, but intuition is probably misleading: the error measures in demonstrate that both segmentations produce equally accurate results.", "labels": [], "entities": []}, {"text": "It is also important to note that these annotations are probabilistic; for clarity we only display results with the highest probability.", "labels": [], "entities": []}, {"text": "From the Bayesian feature weighting priors \u03c4 placed on the word cluster means, we can deduce the relative importance of our feature set.", "labels": [], "entities": []}, {"text": "In our experiments, luminance and vertical position in the image are the two most important features.", "labels": [], "entities": []}], "tableCaptions": []}