{"title": [{"text": "Unsupervised Learning of Bulgarian POS Tags", "labels": [], "entities": [{"text": "Bulgarian POS Tags", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.6841258804003397}]}], "abstractContent": [{"text": "This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information.", "labels": [], "entities": []}, {"text": "While the model is more complex than those which have been employed for un-supervised learning of POS tags in En-glish, which use only syntactic information , the variety of languages in the world requires that we consider morphology as well.", "labels": [], "entities": []}, {"text": "In many languages, morphology provides better clues to a word's category than word order.", "labels": [], "entities": []}, {"text": "We present the computational model for POS learning, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology.", "labels": [], "entities": [{"text": "POS learning", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9584326446056366}]}, {"text": "1 Preliminaries In designing a model to induce parts of speech (POS categories) from a corpus, the first question which arises is exactly what sort of entities the target categories are.", "labels": [], "entities": []}, {"text": "Depending on exactly how these categories are defined, and which words are taken to be members of each, different sorts of linguistic information will clearly be relevant to their identification.", "labels": [], "entities": []}, {"text": "For concreteness, we will be concerned with the part-of-speech categories used in tagging electronic texts such as the Bulgarian Treebank (Simov et al., 2002).", "labels": [], "entities": [{"text": "Bulgarian Treebank (Simov et al., 2002)", "start_pos": 119, "end_pos": 158, "type": "DATASET", "confidence": 0.8911005788379245}]}, {"text": "Since the goal of this paper is to devise a model which will induce POS categories automatically from an untagged text, with no prior knowledge of the structure of the language , we will be using these tagged corpora as a gold standard to evaluate the performance of competing models.", "labels": [], "entities": []}, {"text": "2 Previous approaches While this study is unique in attempting to incorporate both syntactic and morphological factors , previous work by other researchers has explored unsupervised methods of deriving clusters of words based on their linguistic behavior.", "labels": [], "entities": []}, {"text": "(Brown et al., 1992) is one of the first works to use statistical methods of distributional analysis to induce clusters of words.", "labels": [], "entities": []}, {"text": "These authors define an initial, very fine categorization of the vocabulary of a corpus, in which each word is the sole member of its own category, and then iteratively merge these word classes until the desired level of granularity is achieved.", "labels": [], "entities": []}, {"text": "The objective function which they use to determine the optimal set of word classes fora corpus is the inter-class mutual information between adjacent words in the corpus.", "labels": [], "entities": []}, {"text": "Since there is no practical way of determining the classification which maximizes this quantity fora given corpus, (Brown et al., 1992) use a greedy algorithm which proceeds from the initial classification, performing the merge which results in the least loss in mutual information at each stage.", "labels": [], "entities": []}, {"text": "(Lee, 1997) pursues a similar approach in clustering nouns which occur as direct objects to verbs, but uses a soft clustering algorithm in place of the agglomerative clustering algorithm used by Brown et al., and Lee uses the KL divergence between the nouns' distributions as a measure of closeness, rather than the loss in inter-class mutual information.", "labels": [], "entities": [{"text": "clustering nouns which occur as direct objects to verbs", "start_pos": 42, "end_pos": 97, "type": "TASK", "confidence": 0.8184367550743951}]}, {"text": "(McMahon and Smith, 1996) employ a similar algorithm to that of Brown et al., but use a top-down search in determining word clusters, rather than a bottom-up one.", "labels": [], "entities": []}, {"text": "A number of other studies have attempted to use distributional analysis to derive POS categories.", "labels": [], "entities": []}, {"text": "(Brill et al., 1990) use an ad-hoc similarity metric to cluster words into POS-like classes, but the problem is significantly simplified by their pre-processing of the data to replace infrequent open", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results on prediction of Bulgarian POS  tags from morphological information  Training Validation  Test  Baseline  21.7%  23.8%  21.1%  Single-layer  65.6%  66.8%  64.9%  network  Multi-layer  68.0%  67.9%  67.9%  network", "labels": [], "entities": [{"text": "prediction of Bulgarian POS  tags from morphological information  Training Validation", "start_pos": 21, "end_pos": 106, "type": "TASK", "confidence": 0.6499189138412476}]}]}