{"title": [], "abstractContent": [{"text": "We present minimally supervised methods for training and testing geographic name disam-biguation (GND) systems.", "labels": [], "entities": []}, {"text": "We train data-driven place name classifiers using toponyms already disambiguated in the training text-by such existing cues as \"Nashville, Tenn.\" or \"Spring-field, MA\"-and test the system on texts where these cues have been stripped out and on hand-tagged historical texts.", "labels": [], "entities": [{"text": "place name classifiers", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.6708548367023468}, {"text": "Nashville, Tenn.\" or \"Spring-field, MA", "start_pos": 128, "end_pos": 166, "type": "DATASET", "confidence": 0.8015189038382636}]}, {"text": "We experiment on three English-language corpora of varying provenance and complexity: newsfeed from the 1990s, personal narratives from the 19th century American west, and memoirs and records of the U.S. Civil War.", "labels": [], "entities": []}, {"text": "Disambiguation accuracy ranges from 87% for news to 69% for some historical collections.", "labels": [], "entities": [{"text": "Disambiguation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.915427029132843}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8978394269943237}]}], "introductionContent": [], "datasetContent": [{"text": "Dividing the corpora in training and test data, we train Naive Bayes classifiers on all examples of disambiguated toponyms in the training set.", "labels": [], "entities": []}, {"text": "Although it is not uncommon for two places in the same state, for example, to share a name, we define disambiguation for purposes of these experiments as finding the correct U.S. state or foreign country.", "labels": [], "entities": []}, {"text": "This asymmetry is reflected in U.S. news and historical text of the training data, where toponyms are specified by U.S. states or by foreign countries.", "labels": [], "entities": []}, {"text": "We then run the classifiers on the test text with disambiguating labels, such as state or country names that immediately follow the city name, removed.", "labels": [], "entities": []}, {"text": "Since not all toponyms in the test set will have been seen in training, we also train backoff classifiers to guess the states and countries related to a story.", "labels": [], "entities": []}, {"text": "If, for example, we cannot find a classifier for \"Oxford\", but can tell that a story is about Mississippi, we will still be able to disambiguate.", "labels": [], "entities": [{"text": "Oxford\"", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.9217386543750763}]}, {"text": "We use a gazetteer to restrict the set of candidate states and countries fora given place name.", "labels": [], "entities": []}, {"text": "In trying to disambiguate \"Portland\", we would thus consider Oregon, Maine, and England, among other options, but not Maryland.", "labels": [], "entities": []}, {"text": "As in the word sense disambiguation task as usually defined, we are classifying names and not clustering them.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7463874220848083}]}, {"text": "This approach is practical for geographic names, for which broad-coverage gazetteers exist, though less so for personal names.", "labels": [], "entities": []}, {"text": "System performance is measured with reference to the naive baseline where each ambiguous toponym is guessed to be the most commonly occurring place.", "labels": [], "entities": []}, {"text": "London, England, would thus always be guessed rather than London, Ontario.", "labels": [], "entities": []}, {"text": "Bootstrapping methods similar to ours have been shown to be competitive in word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7902157704035441}]}, {"text": "We evaluate our system's performance on geographic name disambiguation using two tasks.", "labels": [], "entities": [{"text": "geographic name disambiguation", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6880883375803629}]}, {"text": "For the first task, we use the same sort of untagged raw text used in training.", "labels": [], "entities": []}, {"text": "We simply find the toponyms with disambiguating labels -e.g., \"Portland, Maine\" -, remove the labels, and see if the system can restore them from context.", "labels": [], "entities": []}, {"text": "For the second task, we use texts all of whose toponyms have been marked and disambiguated.", "labels": [], "entities": []}, {"text": "The earlier heuristic system described in) was run on the texts and all disambiguation choices were reviewed by a human editor.", "labels": [], "entities": []}, {"text": "shows the results of these experiments.", "labels": [], "entities": []}, {"text": "The baseline accuracy was briefly mentioned above: if a toponym has been seen in training, select the state or country with which it was most frequently associated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.978964626789093}]}, {"text": "If a site was not seen, select the most frequent state or country from among the candidates in the gazetteer.", "labels": [], "entities": []}, {"text": "The columns for \"seen\" and \"new\" provide separate accuracy rates for toponyms that were seen in training and for those that were not.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992913007736206}]}, {"text": "Finally, the overall accuracy of the trained system is reported.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9996442794799805}]}, {"text": "For the American Memory and Civil War corpora, we report results on the hand-tagged as well as the raw text.", "labels": [], "entities": [{"text": "American Memory and Civil War corpora", "start_pos": 8, "end_pos": 45, "type": "DATASET", "confidence": 0.8546432058016459}]}, {"text": "Not surprisingly, in light of its lower conditional entropy, disambiguation in news text was the most accurate, at 87.38%.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.9556452631950378}]}, {"text": "Not only was the system accurate on news text overall, but it degraded the least for unseen toponyms.", "labels": [], "entities": []}, {"text": "War texts is also consistent with the entropies presented above.", "labels": [], "entities": [{"text": "War texts", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7653800249099731}]}, {"text": "The classifier shows a more marked degradation when disambiguating toponyms not seen in training.", "labels": [], "entities": []}, {"text": "The accuracy of the classifier on restoring states and countries in raw text is significantly, but not considerably, higher than the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996614456176758}]}, {"text": "It seems that many of toponyms mentioned in text might be only loosely connected to the surrounding discourse.", "labels": [], "entities": []}, {"text": "An obituary, for example, might mention that the deceased left a brother, John Doe, of Arlington, Texas.", "labels": [], "entities": [{"text": "John Doe, of Arlington, Texas", "start_pos": 74, "end_pos": 103, "type": "DATASET", "confidence": 0.8523973907743182}]}, {"text": "Without tagging our test sets to mark such tangential statements, it would be hard to weigh errors in such cases appropriately.", "labels": [], "entities": []}, {"text": "Although accuracy on the hand-tagged data from the American memory corpus was better than for the raw text, performance on the Civil War tagged data (Grant's Memoirs) was abysmal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.998978853225708}, {"text": "American memory corpus", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.7240476806958517}, {"text": "Civil War tagged data (Grant's Memoirs)", "start_pos": 127, "end_pos": 166, "type": "DATASET", "confidence": 0.7667474249998728}]}, {"text": "Most of this error seems came from toponyms unseen in training, for with the accuracy was 9.38%.", "labels": [], "entities": [{"text": "error", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9770470857620239}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9997393488883972}]}, {"text": "In both sets of tagged text, moreover, the full classifier performed below baseline accuracy due to problems with unseen toponyms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9793832898139954}]}, {"text": "The back-off state models are clearly inadequate for the minute topographical references Grant makes in his descriptions of campaigns.", "labels": [], "entities": []}, {"text": "Including proximity to other places mentioned is probably the best way to overcome this difficulty.", "labels": [], "entities": []}, {"text": "These problems suggest that we need to more robustly generalize from the kinds of environments with labelled toponyms to those without.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental corpora with toponym counts in  unsupervised training and test and hand-tagged test sec- tions.", "labels": [], "entities": []}, {"text": " Table 2: Places with multiple names and names applied  to more than one place in the Getty Thesaurus of Geo- graphic Names", "labels": [], "entities": [{"text": "Getty Thesaurus of Geo- graphic Names", "start_pos": 86, "end_pos": 123, "type": "TASK", "confidence": 0.7313131434576852}]}, {"text": " Table 3: Entropy (H) of the state/country classification  task", "labels": [], "entities": [{"text": "Entropy (H)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9504241794347763}]}, {"text": " Table 4: Disambiguation accuracy (%) on test corpora.  Hand-tagged data were available for the American Mem- ory and Civil War corpora.", "labels": [], "entities": [{"text": "Disambiguation", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9754971265792847}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8551084399223328}, {"text": "American Mem- ory and Civil War corpora", "start_pos": 96, "end_pos": 135, "type": "DATASET", "confidence": 0.8591162413358688}]}]}