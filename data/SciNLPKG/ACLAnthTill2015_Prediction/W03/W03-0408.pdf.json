{"title": [{"text": "Updating an NLP System to Fit New Domains: an empirical study on the sentence segmentation problem", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7164061814546585}]}], "abstractContent": [{"text": "Statistical machine learning algorithms have been successfully applied to many natural language processing (NLP) problems.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.7527486284573873}]}, {"text": "Compared to manually constructed systems, statistical NLP systems are often easier to develop and maintain since only annotated training text is required.", "labels": [], "entities": []}, {"text": "From annotated data, the underlying statistical algorithm can build a model so that annotations for future data can be predicted.", "labels": [], "entities": []}, {"text": "However, the performance of a statistical system can also depend heavily on the characteristics of the training data.", "labels": [], "entities": []}, {"text": "If we apply such a system to text with characteristics different from that of the training data, then performance degradation will occur.", "labels": [], "entities": []}, {"text": "In this paper, we examine this issue empirically using the sentence boundary detection problem.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.7447908322016398}]}, {"text": "We propose and compare several methods that can be used to update a statistical NLP system when moving to a different domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important issue fora statistical machine learning based NLP system is that its performance can depend heavily on the characteristics of the training data used to build the system.", "labels": [], "entities": []}, {"text": "Consequently if we train a system on some data but apply it to other data with different characteristics, then the system's performance can degrade significantly.", "labels": [], "entities": []}, {"text": "It is therefore natural to investigate the following related issues: \u00a3 How to detect the change of underlying data characteristics, and to estimate the corresponding system performance degradation.", "labels": [], "entities": []}, {"text": "\u00a3 If performance degradation is detected, how to update a statistical system to improve its performance with as little human effort as possible.", "labels": [], "entities": []}, {"text": "This paper investigates some methodological and practical aspects of the above issues.", "labels": [], "entities": []}, {"text": "Although ideally such a study would include as many different statistical algorithms as possible, and as many different linguistic problems as possible (so that a very general conclusion might be drawn), in reality such an undertaking is not only difficult to carryout, but also can hide essential observations and obscure important effects that may depend on many variables.", "labels": [], "entities": []}, {"text": "An alternative is to study a relatively simple and well-understood problem to try to gain understanding of the fundamental issues.", "labels": [], "entities": []}, {"text": "Causal effects and essential observations can be more easily isolated and identified from simple problems since there are fewer variables that can affect the outcome of the experiments.", "labels": [], "entities": []}, {"text": "In this paper, we take the second approach and focus on a specific problem using a specific underlying statistical algorithm.", "labels": [], "entities": []}, {"text": "However, we try to use only some fundamental properties of the algorithm so that our methods are readily applicable to other systems with similar properties.", "labels": [], "entities": []}, {"text": "Specifically, we use the sentence boundary detection problem to perform experiments since not only is it relatively simple and well-understood, but it also provides the basis for other more advanced linguistic problems.", "labels": [], "entities": [{"text": "sentence boundary detection problem", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.76898393034935}]}, {"text": "Our hope is that some characteristics of this problem are universal to language processing so that they can be generalized to more complicated linguistic tasks.", "labels": [], "entities": [{"text": "language processing", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7091982513666153}]}, {"text": "In this paper we use the generalized Winnow method () for all experiments.", "labels": [], "entities": []}, {"text": "Applied to text chunking, this method resulted instate of the art performance.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.7514289617538452}]}, {"text": "It is thus reasonable to conjecture that it is also suitable to other linguistic problems including sentence segmentation.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.7321968823671341}]}, {"text": "Although issues addressed in this paper are very important for practical applications, there have only been limited studies on this topic in the existing literature.", "labels": [], "entities": []}, {"text": "In speech processing, various adaption techniques have been proposed for language modeling.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7336044609546661}, {"text": "language modeling", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.711442768573761}]}, {"text": "However, the language modeling problem is essentially unsupervised (density estimation) in the sense that it does not require any annotation.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7555016577243805}]}, {"text": "Therefore techniques developed there cannot be applied to our problems.", "labels": [], "entities": []}, {"text": "Motivated from adaptive language modeling, transformation based adaptation techniques have also been proposed for certain supervised learning tasks (.", "labels": [], "entities": []}, {"text": "However, typically they only considered very specific statistical models where the idea is to fit certain transformation parameters.", "labels": [], "entities": []}, {"text": "In particular they did not consider the main issues investigated in this paper as well as generally applicable supervised adaptation methodologies such as what we propose.", "labels": [], "entities": []}, {"text": "In fact, it will be very difficult to extend their methods to natural language processing problems that use different statistical models.", "labels": [], "entities": []}, {"text": "The adaption idea in () is also closely related to the idea of combining supervised and unsupervised learning in the same domain.", "labels": [], "entities": []}, {"text": "In machine learning, this is often referred to as semi-supervised learning or learning with unlabeled data.", "labels": [], "entities": []}, {"text": "Such methods are not always reliable and can often fail).", "labels": [], "entities": []}, {"text": "Although potentially useful for small distributional parameter shifts, they cannot recover labels for examples not (or inadequately) represented in the old training data.", "labels": [], "entities": []}, {"text": "In such cases, it is necessary to use supervised adaption methods which we study in this paper.", "labels": [], "entities": []}, {"text": "Another related idea is socalled active learning paradigm (), which selectively annotates the most informative data (from the same domain) so as to reduce the total number of annotations required to achieve a certain level of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 226, "end_pos": 234, "type": "METRIC", "confidence": 0.9896519780158997}]}, {"text": "See () for related studies in statistical natural language parsing.", "labels": [], "entities": [{"text": "statistical natural language parsing", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.6725432425737381}]}], "datasetContent": [{"text": "In our study of system behavior under domain changes, we have also used manually constructed rules to filter out some of the periods.", "labels": [], "entities": []}, {"text": "The specific set of rules we have used are: \u00a3 If a period terminates a non-capitalized word, and is followed by a blank and a capitalized word, then we predict that it is a sentence boundary.", "labels": [], "entities": []}, {"text": "\u00a3 If a period is both preceded and followed by alphanumerical characters, then we predict that it is not a sentence boundary.", "labels": [], "entities": []}, {"text": "The above rules achieve error rates of less than on both the WSJ and Brown datasets, which is sufficient for our purpose.", "labels": [], "entities": [{"text": "error rates", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9831957519054413}, {"text": "WSJ and Brown datasets", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.7983875572681427}]}, {"text": "Note that we did not try to make the above rules as accurate as possible.", "labels": [], "entities": []}, {"text": "For example, the first rule will misclassifiy situations such as \"A vs. B\".", "labels": [], "entities": []}, {"text": "Eliminating such mistakes is not essential for the purpose of this study.", "labels": [], "entities": []}, {"text": "All of our experiments are performed and reported on the remaining periods that are not filtered out by the above manual rules.", "labels": [], "entities": []}, {"text": "In this study, the filtering scheme serves two purposes.", "labels": [], "entities": []}, {"text": "The first purpose is to magnify the errors.", "labels": [], "entities": []}, {"text": "Roughly speaking, the rules will classify more than half of the periods.", "labels": [], "entities": []}, {"text": "These periods are also relatively easy to classify using a statistical classifier.", "labels": [], "entities": []}, {"text": "Therefore the error rate on the remaining periods is more than doubled.", "labels": [], "entities": [{"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9914376139640808}]}, {"text": "Since the sentence boundary detection problem has a relatively small error rate, this magnification effect is useful for comparing different algorithms.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7271539469559988}]}, {"text": "The second purpose is to reduce our manual labeling effort.", "labels": [], "entities": []}, {"text": "In this study, we had used a number of datasets that are not annotated.", "labels": [], "entities": []}, {"text": "Therefore for experimentation purpose, we have to label each period manually.", "labels": [], "entities": []}, {"text": "After filtering, the WSJ training set contains about twenty seven thousand data points, and the test set contains about five thousand data points.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.8742208282152811}]}, {"text": "The Brown corpus contains about seventeen thousand data points.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.974498838186264}]}, {"text": "In addition, we also manually labeled the following data: It is perhaps not surprising that a sentence boundary classifier trained on WSJ does not perform nearly as well on some of the other data sets.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.8950357437133789}]}, {"text": "However it is useful to examine the source of these extra errors.", "labels": [], "entities": []}, {"text": "We observed that most of the errors are clearly caused by the fact that other domains contain examples that are not represented in the WSJ training set.", "labels": [], "entities": [{"text": "WSJ training set", "start_pos": 135, "end_pos": 151, "type": "DATASET", "confidence": 0.8872201641400655}]}, {"text": "There are two sources for these previously unseen examples: 1.", "labels": [], "entities": []}, {"text": "change of writing style; 2.", "labels": [], "entities": []}, {"text": "For example, quote marks are represented as two single quote (or back quote) characters in WSJ, but typically as one double quote character elsewhere.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9305582046508789}]}, {"text": "In some data sets such as Reuters, phrases such as \"U.S. Economy\" or \"U.S. Dollar\" frequently have the word after the country name capitalized (they also appear in lowercase sometimes, in the same data).", "labels": [], "entities": [{"text": "U.S. Dollar\"", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.8464446663856506}]}, {"text": "The above can be considered as a change of writing style.", "labels": [], "entities": []}, {"text": "In some other cases, new expressions may occur.", "labels": [], "entities": []}, {"text": "For example, in the MedLine data, new expressions such as \"4 degrees C.\" are used to indicate temperature, and expressions such as \"Bioch.", "labels": [], "entities": [{"text": "MedLine data", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9640627503395081}, {"text": "Bioch", "start_pos": 132, "end_pos": 137, "type": "METRIC", "confidence": 0.958980143070221}]}, {"text": "251, 744-747\" are used for citations.", "labels": [], "entities": []}, {"text": "In addition, new acronyms and even formulas containing tokens ending with periods occur in such domains.", "labels": [], "entities": []}, {"text": "It is clear that the majority of errors are caused by data that are not represented in the training set.", "labels": [], "entities": []}, {"text": "This fact suggests that when we apply a statistical system to anew domain, we need to check whether the domain contains a significant number of previously unseen examples which may cause performance deterioration.", "labels": [], "entities": []}, {"text": "This can be achieved by measuring the similarity of the new test domain to the training domain.", "labels": [], "entities": []}, {"text": "One way is to compute statistics on the training domain, and compare them to statistics computed on the new test domain; another way is to calculate a properly defined distance between the test data and the training data.", "labels": [], "entities": []}, {"text": "However, it is not immediately obvious what data statistics are important for determining classification performance.", "labels": [], "entities": [{"text": "classification", "start_pos": 90, "end_pos": 104, "type": "TASK", "confidence": 0.926399827003479}]}, {"text": "Similarly it is not clear what distance metric would be good to use.", "labels": [], "entities": []}, {"text": "To avoid such difficulties, in this paper we assume that the classifier itself can provide a confidence measure for each prediction, and we use this information to estimate the classifier's performance.", "labels": [], "entities": []}, {"text": "As we have mentioned earlier, the generalized Winnow method approximately minimizes the quantity as an estimate of the conditional probability . From simple algebra, we obtain an estimate of the classification error as is only an approximation of the conditional probability, this estimate may not be entirely accurate.", "labels": [], "entities": []}, {"text": "However, one would expect it to give a reasonably indicative measure of the classification performance.", "labels": [], "entities": []}, {"text": "In, we compare the true classification accuracy from the annotated test data to the estimated accuracy using this method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.5801123976707458}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.95377516746521}]}, {"text": "It clearly shows that this estimate indeed correlates very well with the true classification performance.", "labels": [], "entities": []}, {"text": "Note that this estimate does not require knowing the true labels of the data.", "labels": [], "entities": []}, {"text": "Therefore we are able to detect the potential performance degradation of the classifier on anew domain using this metric without the ground truth information.", "labels": [], "entities": []}, {"text": "As pointed out before, a major source of error fora new application domain comes from data that are not represented in the training set.", "labels": [], "entities": []}, {"text": "If we can identify those data, then a natural way to enhance the underlying classifier's performance would be to include them in the training data, and then retrain.", "labels": [], "entities": []}, {"text": "However, a human is required to obtain labels for the new data, but our goal is to reduce the human labeling effort as much as possible.", "labels": [], "entities": []}, {"text": "Therefore we examine the potential of using the classifier to determine which part of the data it has difficulty with, and then ask a human to label that part.", "labels": [], "entities": []}, {"text": "If the underlying classifier can provide confidence information, then it is natural to assume that confidence for unseen data will likely below.", "labels": [], "entities": []}, {"text": "Therefore for labeling purposes, one can choose data from the new domain for which the confidence is low.", "labels": [], "entities": [{"text": "confidence", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9540390968322754}]}, {"text": "This idea is very similar to certain methods used in active learning.", "labels": [], "entities": []}, {"text": "In particular a confidence-based sample selection scheme was proposed in (.", "labels": [], "entities": []}, {"text": "One potential problem for this approach is that by choosing data with lower confidence levels, noisy data that are difficult to classify tend to be chosen; another problem is that it tends to choose similar data multiple times.", "labels": [], "entities": []}, {"text": "However, in this paper we do not investigate methods that solve these issues.", "labels": [], "entities": []}, {"text": "For baseline comparison, we consider the classifier obtained from the old training data (see), as well as classifiers trained on random samples from the new domain (see).", "labels": [], "entities": []}, {"text": "In this study, we explore the following three ideas to improve the performance: One may combine the above ideas.", "labels": [], "entities": []}, {"text": "In particular, we will compare the following methods in this study:  We carried out experiments on the Brown, Reuters, and MedLine datasets.", "labels": [], "entities": [{"text": "Brown, Reuters", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.8692333300908407}, {"text": "MedLine datasets", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.8396011292934418}]}, {"text": "We randomly partition each dataset into training and testing.", "labels": [], "entities": []}, {"text": "All methods are trained using only information from the training set, and their performance are evaluated on the test set.", "labels": [], "entities": []}, {"text": "Each test set contains oi ii data points randomly selected.", "labels": [], "entities": []}, {"text": "This sample size is chosen to make sure that an estimated accuracy based on these empirical samples will be reasonably close to the true accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9768574237823486}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9925663471221924}]}, {"text": "For a binary classifier, the standard deviation between the empirical mean \u00af \u00b0 with a sample size contains the results of using the balancing idea.", "labels": [], "entities": []}, {"text": "With the same amount of newly labeled data, the improvement over the random method is significant.", "labels": [], "entities": []}, {"text": "This shows that even though the domain has changed, training data from the old domain are still very useful.", "labels": [], "entities": []}, {"text": "Observe that not only is the average performance improved, but the variance is also reduced.", "labels": [], "entities": [{"text": "variance", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.986135721206665}]}, {"text": "Note that in this table, we have fixed The performance with different \u00ad values on the MedLine dataset is reported in.", "labels": [], "entities": [{"text": "MedLine dataset", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9763737320899963}]}, {"text": "It shows that different choices of \u00ad make relatively small differences inaccuracy.", "labels": [], "entities": []}, {"text": "At this point, it is interesting to check whether the estimated accuracy (using the method described for) reflects the change in performance improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.981524646282196}]}, {"text": "The result is given in report the performance using confidence based data selection, instead of random sampling.", "labels": [], "entities": []}, {"text": "This method helps to some extent, but not as much as we originally expected.", "labels": [], "entities": []}, {"text": "However, we have only used the simplest version of this method, which is susceptible to two problems mentioned earlier: it tends (a) to select data that are inherently hard to classify, and (b) to select redundant data.", "labels": [], "entities": []}, {"text": "Both problems can be avoided with a more elaborated implementation, but we have not explored this.", "labels": [], "entities": []}, {"text": "Another possible reason that using confidence based sample selection does not result in significant performance improvement is that for our examples, the performance is already quite good with even a small number of new samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: True and estimated accuracy", "labels": [], "entities": [{"text": "True", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9922996163368225}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.990685760974884}]}, {"text": " Table 7: True and estimated accuracy (balancing scheme  with", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9662492871284485}]}]}