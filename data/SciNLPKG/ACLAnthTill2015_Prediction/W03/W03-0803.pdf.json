{"title": [{"text": "OLLIE: On-Line Learning for Information Extraction", "labels": [], "entities": [{"text": "OLLIE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8217988014221191}, {"text": "Information Extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7034595757722855}]}], "abstractContent": [{"text": "This paper reports work aimed at developing an open, distributed learning environment , OLLIE, where researchers can experiment with different Machine Learning (ML) methods for Information Extraction.", "labels": [], "entities": [{"text": "OLLIE", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.8304030895233154}, {"text": "Information Extraction", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.7845935225486755}]}, {"text": "Once the required level of performance is reached, the ML algorithms can be used to speedup the manual annotation process.", "labels": [], "entities": []}, {"text": "OLLIE uses a browser client while data storage and ML training is performed on servers.", "labels": [], "entities": [{"text": "OLLIE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.702300488948822}, {"text": "ML training", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9183477759361267}]}, {"text": "The different ML algorithms use a unified programming interface; the integration of new ones is straightforward.", "labels": [], "entities": [{"text": "ML", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9495358467102051}]}], "introductionContent": [{"text": "OLLIE is an on-line application for corpus annotation that harnesses the power of Machine Learning (ML) and Information Extraction (IE) in order to make the annotator's task easier and more efficient.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.792967677116394}]}, {"text": "A normal OLLIE working session starts with the user uploading a set of documents, selecting which ML method to use from the several supplied by the system, choosing the parameters for the learning module and starting to annotate the texts.", "labels": [], "entities": []}, {"text": "During the initial phase of the manual annotation process, the system learns in the background (i.e. on the server) from the user's actions and, when a certain degree of confidence is reached, it starts making suggestions by pre-annotating the documents.", "labels": [], "entities": []}, {"text": "Initially, some of these suggestions maybe erroneous but, as the user makes the necessary corrections, the system will learn from its mistakes and the performance will increase leading to a reduction in the amount of human input required.", "labels": [], "entities": []}, {"text": "The implementation is based on a client-server architecture where the client is any Java-enabled web browser and the server is responsible for storing data, training ML models and providing access services for the users.", "labels": [], "entities": []}, {"text": "The client side of OLLIE is implemented as a set of Java Server Pages (JSPs) and a small number of Java applets are used for tasks where the user interface capabilities provided by HTML are not enough.", "labels": [], "entities": []}, {"text": "The server side comprises a JSP/servlet server, a relational database server and an instance of the GATE architecture for language engineering which is used for driving all the language-related processing.", "labels": [], "entities": []}, {"text": "The general architecture is presented in.", "labels": [], "entities": []}, {"text": "The next section describes the client side of the OLLIE system while Section 3 details the implementation of the server with a subsection on the integration of Machine Learning.", "labels": [], "entities": []}, {"text": "Section 4 talks about security; Section 6 about future improvements and Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To the end of evaluating the suitability of the ML algorithms provided by WEKA for use in OLLIE we performed several experiments for named entity recognition on the MUC-7 corpus We first tested the ability of the learners to identify correctly the boundaries of named entities.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9028392434120178}, {"text": "named entity recognition", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.6948994000752767}, {"text": "MUC-7 corpus", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9193116724491119}]}, {"text": "Using 10-fold cross-validation on the MUC 7 corpus described above, we experimented with different machine learning algorithms and parameters (using WEKA), and using different attributes for training.", "labels": [], "entities": [{"text": "MUC 7 corpus", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9496337572733561}, {"text": "WEKA", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.7259232401847839}]}, {"text": "5 different algorithms have been evaluated: Zero Rand OneR -as baselines, Naive Bayes, IBK (an implementation of K Nearest Neighbour) and J48 (an implementation of a C4.5 decision tree).", "labels": [], "entities": [{"text": "IBK", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.5232120752334595}]}, {"text": "As expected, the baseline algorithms performed very poorly (at around 1%).", "labels": [], "entities": []}, {"text": "For IBK small windows gave low results, while large windows were very inefficient.", "labels": [], "entities": []}, {"text": "The best results (f-measure of around 60%) were achieved using the J48 algorithm.", "labels": [], "entities": [{"text": "f-measure", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9702487587928772}]}, {"text": "The types of linguistic data used for the attribute collection included part of speech information, orthography (upper case, lowercase, initial uppercase letter, mixture of upper and lower case), token kind (word, symbol, punctuation or number), sentence boundary, the presence of certain known names and keywords from the gazetteer lists provided by the ANNIE system.", "labels": [], "entities": [{"text": "ANNIE system", "start_pos": 355, "end_pos": 367, "type": "DATASET", "confidence": 0.8039379715919495}]}, {"text": "Tokens were used as instance annotations and, for each token, the window used for collecting the attributes was of size 5 (itself plus two other tokens in each direction).", "labels": [], "entities": []}, {"text": "Additional information, such as features on a wider window of tokens, tended to improve the recall marginally, but decreased the precision substantially, resulting in a lower F-measure, and therefore the trade off was not worthwhile.", "labels": [], "entities": [{"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.99956876039505}, {"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9994716048240662}, {"text": "F-measure", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9992021918296814}]}, {"text": "We also tested the algorithms on a smaller news corpus (which contained around 68,000 instances as opposed to 300,000 for the MUC7 corpus).", "labels": [], "entities": [{"text": "MUC7 corpus", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.962317943572998}]}, {"text": "Again, the J48 algorithm scored highest, with the decision table and the K nearest neighbour algorithms both scoring approximately 1 percentage point lower than the J48.", "labels": [], "entities": []}, {"text": "The second set of experiments was to classify the named entities identified into the three ENAMEX categories: Organisations, Persons and Locations.", "labels": [], "entities": []}, {"text": "Using 10-fold cross-validation on the MUC 7 corpus described above, we experimented with the WEKA machine learning algorithms and parameters, and using attributes for training similar to those used for boundary detection.", "labels": [], "entities": [{"text": "MUC 7 corpus", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.950797955195109}, {"text": "WEKA", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.7974755167961121}, {"text": "boundary detection", "start_pos": 202, "end_pos": 220, "type": "TASK", "confidence": 0.7496963441371918}]}, {"text": "The best results were achieved again with the J48 algorithm, and, for this easier task, they were situated at around 90%.", "labels": [], "entities": []}, {"text": "The attributes were chosen on the basis of their information gain, calculated using WEKA's attribute selection facilities.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.898830771446228}]}, {"text": "The named entity recognition experiments were performed mainly to evaluate the WEKA ML algorithms on datasets of different sizes, ranging from small to fairly large ones (300,000 instances).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6169503927230835}, {"text": "WEKA ML", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.5734916925430298}]}, {"text": "The different ML algorithms had different memory requirements and execution speed, tested on a PIII 1.5GHz PC running Windows 2000 with 1GB RAM.", "labels": [], "entities": [{"text": "ML", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9668520092964172}]}, {"text": "From all algorithms tested, the decision table and decision tree were the slowest (325 and 122 seconds respectively on 68,000 instances) and required most memory -up to 800MB on the big datasets.", "labels": [], "entities": []}, {"text": "Naive Bayes was very fast (only 0.25 seconds) with 1R following closely (0.28 seconds).", "labels": [], "entities": []}], "tableCaptions": []}