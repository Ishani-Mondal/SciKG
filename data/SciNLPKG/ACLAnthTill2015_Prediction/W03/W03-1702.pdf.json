{"title": [{"text": "Class Based Sense Definition Model for Word Sense Tagging and Disambiguation", "labels": [], "entities": [{"text": "Word Sense Tagging and Disambiguation", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.663711553812027}]}], "abstractContent": [{"text": "We present an unsupervised learning strategy for word sense disambiguation (WSD) that exploits multiple linguistic resources including a parallel corpus, a bilingual machine readable dictionary, and a thesaurus.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.8202567845582962}]}, {"text": "The approach is based on Class Based Sense Definition Model (CBSDM) that generates the glosses and translations fora class of word senses.", "labels": [], "entities": []}, {"text": "The model can be applied to resolve sense ambiguity for words in a parallel corpus.", "labels": [], "entities": []}, {"text": "That sense tagging procedure, in effect, produces a semantic bilingual concordance, which can be used to train WSD systems for the two languages involved.", "labels": [], "entities": [{"text": "sense tagging", "start_pos": 5, "end_pos": 18, "type": "TASK", "confidence": 0.7095637619495392}]}, {"text": "Experimental results show that CBSDM trained on Longman Dictionary of Contemporary English, English-Chinese Edition (LDOCE E-C) and Longman Lexicon of Contemporary English (LLOCE) is very effectively in turning a Chinese-English parallel corpus into sense tagged data for development of WSD systems.", "labels": [], "entities": [{"text": "Longman Dictionary of Contemporary English", "start_pos": 48, "end_pos": 90, "type": "DATASET", "confidence": 0.9044524788856506}, {"text": "WSD", "start_pos": 287, "end_pos": 290, "type": "TASK", "confidence": 0.975229799747467}]}], "introductionContent": [{"text": "Word sense disambiguation has been an important research area for over 50 years.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7402924001216888}]}, {"text": "WSD is crucial for many applications, including machine translation, information retrieval, part of speech tagging, etc.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7452043890953064}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.836104542016983}, {"text": "information retrieval", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.8259847462177277}, {"text": "part of speech tagging", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.6951056867837906}]}, {"text": "pointed out the two major problems of WSD: sense tagging and data sparseness.", "labels": [], "entities": [{"text": "WSD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.966084361076355}, {"text": "sense tagging", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.7702992856502533}]}, {"text": "On one hand, tagged data are very difficult to come by, since sense tagging is considerably more difficult than other forms of linguistic annotation.", "labels": [], "entities": [{"text": "sense tagging", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.7438713312149048}]}, {"text": "On the other hand, although the data sparseness is a common problem, it is especially severe for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.8998050689697266}]}, {"text": "The problems were attacked in various ways.", "labels": [], "entities": []}, {"text": "showed a class-based approach under which a very large untagged corpus and thesaurus can be used effectively for unsupervised training for noun homograph disambiguation.", "labels": [], "entities": [{"text": "noun homograph disambiguation", "start_pos": 139, "end_pos": 168, "type": "TASK", "confidence": 0.8333262999852499}]}, {"text": "However, the method does not offer a method that explicitly produces sense tagged data for any given sense inventory.", "labels": [], "entities": []}, {"text": "Li and Huang (1999) described a similar unsupervised approach for Chinese text based on a Chinese thesaurus.", "labels": [], "entities": []}, {"text": "As noted in, even minimal hand tagging improved on the results of unsupervised methods.", "labels": [], "entities": [{"text": "hand tagging", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.787868857383728}]}, {"text": "showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods.", "labels": [], "entities": []}, {"text": "extended the approach by using corpora in two languages to bootstrap the learning process.", "labels": [], "entities": []}, {"text": "They showed bilingual bootstrapping is even more effective.", "labels": [], "entities": []}, {"text": "The bootstrapping approach is limited by lack of a systematic procedure of preparing seed data for any word in a given sense inventory.", "labels": [], "entities": []}, {"text": "The approach also suffers from errors propagating from one iteration into the next.", "labels": [], "entities": []}, {"text": "Li and Huang Another alternative involves using a parallel corpus as a surrogate for tagged data.", "labels": [], "entities": []}, {"text": "exploited the so-called one sense per translation constraint for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.5614587664604187}]}, {"text": "They reported high precision rates of a WSD system for two-way disambiguation of six English nouns based on their translations in an English-French Parallel corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.998536229133606}, {"text": "WSD", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.8901026844978333}]}, {"text": "However, when working with a particular sense inventory, there is no obvious way to know whether the one sense per translation constraint holds or how to determine the relevant translations automatically.", "labels": [], "entities": []}, {"text": "extended the translation-based learning strategy with a weakened constraint that many instances of a word in a parallel corpus often correspond to lexically varied but semantically consistent translations.", "labels": [], "entities": [{"text": "translation-based learning", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.9110011756420135}]}, {"text": "They proposed to group those translations into a target set, which can be automatically tagged with correct senses based on the hypernym hierarchy of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 150, "end_pos": 157, "type": "DATASET", "confidence": 0.9689686894416809}]}, {"text": "Diab and Resnik's work represents a departure from previous unsupervised approaches in that no seed data is needed and explicit tagged data are produced fora given sense inventory (WordNet in their case).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.9511648416519165}]}, {"text": "The system trained on the tagged data was shown to be on a par with the best \"supervised training\" systems in SENSEVAL-2 competition.", "labels": [], "entities": []}, {"text": "However, Diab and Resnik's method is only applicable to nominal WordNet senses.", "labels": [], "entities": []}, {"text": "Moreover, the method is seriously hampered by noise and semantic inconsistency in a target set.", "labels": [], "entities": []}, {"text": "Worse still, it is not always possible to rely on the hypernym hierarchy for tagging a target set.", "labels": [], "entities": []}, {"text": "The approach assumes the availability of a parallel corpus of text written in E (the first language, L1 + ) and C (the second language, L2), an L1 to L2 bilingual machine readable dictionary M, and a L1 thesaurus T.", "labels": [], "entities": []}, {"text": "A so-called Mutually Assured Resolution of Sense Algorithm (MARS) and Class Based Sense Definition Model (CBSDM) are proposed to identify the word senses in I for each word in a semantic class of words Lin T. Unlike Diab and Resnik, we do not apply the MARS algorithm directly to target sets to avoid the noisy words therein.", "labels": [], "entities": [{"text": "Mutually Assured Resolution of Sense Algorithm (MARS", "start_pos": 12, "end_pos": 64, "type": "TASK", "confidence": 0.7735336050391197}, {"text": "Class Based Sense Definition Model (CBSDM)", "start_pos": 70, "end_pos": 112, "type": "METRIC", "confidence": 0.6772379837930202}]}, {"text": "The derived classes senses and their relevant glosses in L1 and L2 make it possible to build Class Based Sense Definition and Translation Models (CBSDM and CBSTM), which subsequently can be applied to assign sense tags to words in a parallel corpus.", "labels": [], "entities": [{"text": "Class Based Sense Definition and Translation", "start_pos": 93, "end_pos": 137, "type": "TASK", "confidence": 0.6947323083877563}]}, {"text": "The main idea is to exploit the defining L1 and L2 words in the glosses to resolve the sense ambi-+ This has nothing to do with the direction of translation and is not to be confused with the native and second language distinction made in the literature of Teaching English As a Second Language (TESL) and Computer Assisted Language Learning. guity.", "labels": [], "entities": []}, {"text": "For instance, for the class containing \"serve\" and \"tee off,\" the approach exploits common defining words, including \"ball\" and \"game\" in two relevant serve-15 and tee off-1 to assign the correct senses to \"serve\" and \"tee off.\"", "labels": [], "entities": []}, {"text": "The character bigram [faqiu] in an English-Chinese MRD: serve v 10 [I\u2205; T1] to begin play by striking (the ball) to the opponent (LDOCE E-C p.", "labels": [], "entities": []}, {"text": "1300), would make it possible to align and sense tag \"serve\" or \"tee off\" in a parallel corpus such as the bilingual citations in Example 1: (1C) (1E) drink a capful before teeing off at each hole.", "labels": [], "entities": []}, {"text": "( That effectively attaches semantic information to bilingual citations and turns a parallel corpus into a Bilingual Semantic Concordance (BSC).", "labels": [], "entities": [{"text": "Bilingual Semantic Concordance (BSC)", "start_pos": 107, "end_pos": 143, "type": "TASK", "confidence": 0.6295712490876516}]}, {"text": "The BSC enables us to simultaneously attack two critical WSD problems of sense tagging difficulties and data sparseness, thus provides an effective approach to WSD.", "labels": [], "entities": [{"text": "BSC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9608437418937683}, {"text": "WSD", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9607183337211609}, {"text": "sense tagging", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.7468684315681458}, {"text": "WSD", "start_pos": 160, "end_pos": 163, "type": "TASK", "confidence": 0.9648532867431641}]}, {"text": "BSC also embodies a projection of the sense inventory from L1 onto L2, thus creates anew sense inventory and semantic concordance for L2.", "labels": [], "entities": [{"text": "BSC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9020538926124573}]}, {"text": "If I is based on WordNet for English, it is then possible to obtain an L2 WordNet.", "labels": [], "entities": []}, {"text": "There are many additional applications of BSC, including bilingual lexicography, cross language information retrieval, and computer assisted language learning.", "labels": [], "entities": [{"text": "BSC", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9508617520332336}, {"text": "cross language information retrieval", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.7269186675548553}]}, {"text": "The remainder of the paper is organized as follows: Sections 2 and 3 layout the approach and describe the MARS and SWAT algorithms.", "labels": [], "entities": [{"text": "MARS", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.521245539188385}]}, {"text": "Section 4 describes experiments and evaluation.", "labels": [], "entities": []}, {"text": "Section 5 contains discussion and we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to assess the feasibility of the proposed approach, we carried out experiments and evaluation on an implementation of MARS and SWAT based on LDOCE E-C, LLOCE, and Sinorama.", "labels": [], "entities": [{"text": "MARS", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.47792327404022217}, {"text": "Sinorama", "start_pos": 172, "end_pos": 180, "type": "DATASET", "confidence": 0.9637741446495056}]}, {"text": "First experiment was involved with the trainability of CBSDM and CBSTM via MARS.", "labels": [], "entities": [{"text": "CBSDM", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.8926246762275696}, {"text": "CBSTM", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.9327870607376099}, {"text": "MARS", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8620399236679077}]}, {"text": "The second experiment was involved with the effectiveness of using SWAT and CBSTM to annotate a parallel corpus with sense information.", "labels": [], "entities": []}, {"text": "Evaluation was done on a set of 14 nouns, verbs, adjectives, and adverbs studies in previous work.", "labels": [], "entities": []}, {"text": "The set includes the nouns \"bass,\" \"bow,\" \"cone,\" \"duty,\" \"gallery,\" \"mole,\" \"sentence,\" \"slug,\" \"taste,\" \"star,\" \"interest,\" \"issue,\" the adjective \"hard,\" and the verb \"serve.\"", "labels": [], "entities": []}, {"text": "We applied SWAT to sense tag English words in some 50,000 reliably aligned sentence pairs in Sinorama parallel Corpus based on LDOCE sense inventory.", "labels": [], "entities": [{"text": "Sinorama parallel Corpus", "start_pos": 93, "end_pos": 117, "type": "DATASET", "confidence": 0.9162687659263611}]}, {"text": "The results are shown in Tables 6.", "labels": [], "entities": []}, {"text": "Evaluation indicates an average precision rate of around 90%.", "labels": [], "entities": [{"text": "precision rate", "start_pos": 32, "end_pos": 46, "type": "METRIC", "confidence": 0.9764465689659119}]}], "tableCaptions": [{"text": " Table 1. The initial CBSDM for n-word list {difficult,  hard, stiff, tough, arduous, awkward} based on the rele- vant and irrelevant LDOCE senses, n = 6.  Defining word d  Count, k  P(d) = k/n  Difficult  5  0.83  Effort  3  0.50  Understand  2  0.33  Bad  2  0.33  Bent  2  0.33  Body  2  0.33  Broken  2  0.33  Difficulty  2  0.33  Easy  2  0.33  Firm  2  0.33  Hard  2  0.33  Moving  2  0.33  Needing  2  0.33  Water  2  0.33", "labels": [], "entities": [{"text": "Count", "start_pos": 173, "end_pos": 178, "type": "METRIC", "confidence": 0.8572396636009216}]}, {"text": " Table 2. First round estimates for P(s), the probability of  sense s in S.  Sense*  Definition  P(s)  hard-1 firm and stiff; which can- not easily be broken", "labels": [], "entities": []}, {"text": " Table 5. Evaluation of the MARS Algorithm based on  12 nouns, 1 verb, 1 adjective in LDOCE.", "labels": [], "entities": [{"text": "MARS Algorithm", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.5511359870433807}]}, {"text": " Table 6. Experimental results of sense tagging the Sinorama", "labels": [], "entities": [{"text": "sense tagging", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.8655605614185333}, {"text": "Sinorama", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8491185903549194}]}]}