{"title": [{"text": "An SVM Based Voting Algorithm with Application to Parse Reranking", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces a novel Support Vector Machines (SVMs) based voting algorithm for reranking, which provides away to solve the sequential models indirectly.", "labels": [], "entities": []}, {"text": "We have presented a risk formulation under the PAC framework for this voting algorithm.", "labels": [], "entities": [{"text": "PAC framework", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.8921614289283752}]}, {"text": "We have applied this algorithm to the parse reranking problem, and achieved labeled recall and precision of 89.4%/89.8% on WSJ section 23 of Penn Treebank.", "labels": [], "entities": [{"text": "parse reranking problem", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.9495034615198771}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9224956631660461}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9971925616264343}, {"text": "WSJ section 23 of Penn Treebank", "start_pos": 123, "end_pos": 154, "type": "DATASET", "confidence": 0.9223358035087585}]}], "introductionContent": [{"text": "Support Vector Machines (SVMs) have been successfully used in many machine learning tasks.", "labels": [], "entities": []}, {"text": "Unlike the errordriven algorithms, SVMs search for the hyperplane that separates a set of training samples that contain two distinct classes and maximizes the margin between these two classes.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.9673498272895813}]}, {"text": "The ability to maximize the margin is believed to be the reason for SVMs' superiority over other classifiers.", "labels": [], "entities": [{"text": "maximize the margin", "start_pos": 15, "end_pos": 34, "type": "METRIC", "confidence": 0.5543898145357767}]}, {"text": "In addition, SVMs can achieve high performance even with input data of high dimensional feature space, especially because of the use of the \"kernel trick\".", "labels": [], "entities": []}, {"text": "However, the incorporation of SVMs into sequential models remains a problem.", "labels": [], "entities": []}, {"text": "An obvious reason is that the output of an SVM is the distance to the separating hyperplane, but not a probability.", "labels": [], "entities": []}, {"text": "A possible solution to this problem is to map SVMs' results into probabilities through a Sigmoid function, and use Viterbi search to combine those probabilities).", "labels": [], "entities": []}, {"text": "However, this approach conflicts with SVMs' purpose of achieving the so-called global optimization . First, this approach may constrain SVMs to local features because of the leftto-right scanning strategy.", "labels": [], "entities": []}, {"text": "Furthermore, like other nongenerative Markov models, it suffers from the so-called label bias problem, which means that the transitions leaving a given state compete only against each other, rather than against all other transitions in the model ().", "labels": [], "entities": []}, {"text": "Intuitively, it is the local normalization that results in the label bias problem.", "labels": [], "entities": []}, {"text": "One way of using discriminative machine learning algorithms in sequential models is to rerank the n-best outputs of a generative system.", "labels": [], "entities": []}, {"text": "Reranking uses global features as well as local features, and does not make local normalization.", "labels": [], "entities": [{"text": "Reranking", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7793908715248108}]}, {"text": "If the output set is large enough, the reranking approach may help to alleviate the impact of the label bias problem, because the victim parses (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking.", "labels": [], "entities": []}, {"text": "In recent years, reranking techniques have been successfully applied to the so-called history-based models, especially to parsing).", "labels": [], "entities": [{"text": "parsing", "start_pos": 122, "end_pos": 129, "type": "TASK", "confidence": 0.9868029952049255}]}, {"text": "Ina history-based model, the current decision depends on the decisions made previously.", "labels": [], "entities": []}, {"text": "Therefore, we may regard parsing as a special form of sequential model without losing generality.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9703758358955383}]}, {"text": "Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser.", "labels": [], "entities": []}, {"text": "One is based on Markov Random Fields, and the other is based on a boosting approach.", "labels": [], "entities": []}, {"text": "In), the use of Voted Perceptron (VP)) for the parse reranking problem has been described.", "labels": [], "entities": [{"text": "parse reranking problem", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.927455484867096}]}, {"text": "In that paper, the tree kernel) has been used to efficiently count the number of common subtrees as described in.", "labels": [], "entities": []}, {"text": "In this paper we will follow the reranking approach.", "labels": [], "entities": []}, {"text": "We describe a novel SVM-based voting algorithm for reranking.", "labels": [], "entities": [{"text": "SVM-based voting", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7782934308052063}]}, {"text": "It provides an alternative way of using a large margin classifier for sequential models.", "labels": [], "entities": []}, {"text": "Instead of using the parse tree itself as a training sample, we use a pair of parse trees as a sample, which is analogous to the preference relation used in the context of ordinal regression ().", "labels": [], "entities": []}, {"text": "Furthermore, we justify the algorithm through a modification of the proof of the large margin rank boundaries for ordinal regression.", "labels": [], "entities": []}, {"text": "We then apply this algorithm to the parse reranking problem.", "labels": [], "entities": [{"text": "parse reranking", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.9008381962776184}]}], "datasetContent": [{"text": "We use SV M light as the SVM classifier.", "labels": [], "entities": []}, {"text": "The soft margin parameter C is set to its default value in SV M light . We use the same data set as described in).", "labels": [], "entities": []}, {"text": "Section 2-21 of the Penn WSJ Treebank ( are used as training data, and section 23 is used for final test.", "labels": [], "entities": [{"text": "Penn WSJ Treebank (", "start_pos": 20, "end_pos": 39, "type": "DATASET", "confidence": 0.9566361159086227}]}, {"text": "The training data contains around 40,000 sentences, each of which has 27 distinct parses on average.", "labels": [], "entities": []}, {"text": "Of the 40,000 training sentences, the first 36,000 are used to train SVMs.", "labels": [], "entities": [{"text": "SVMs", "start_pos": 69, "end_pos": 73, "type": "TASK", "confidence": 0.6264760494232178}]}, {"text": "The remaining 4,000 sentences are used as development data.", "labels": [], "entities": []}, {"text": "The training complexity for SV M light is roughly O(n 2.1 ), where n is the number of the training samples.", "labels": [], "entities": [{"text": "O", "start_pos": 50, "end_pos": 51, "type": "METRIC", "confidence": 0.9965130686759949}]}, {"text": "One solution to the scaling difficulties is to use the Kernel Fisher Discriminant as described in.", "labels": [], "entities": []}, {"text": "In this paper, we divide training data into slices to speedup training.", "labels": [], "entities": []}, {"text": "Each slice contains two pairs of parses from each sentence.", "labels": [], "entities": []}, {"text": "Specifically, slice i contains positive samples (( \u02dc pk , p ki ), +1) and negative samples ((p ki , \u02dc pk ), \u22121), where\u02dcpwhere\u02dc where\u02dcp k is the best parse for sentence k, p ki is the parse with the ith highest loglikelihood in all the parses for sentence k and it is not the best parse.", "labels": [], "entities": []}, {"text": "There are about 60000 parses in each slice in average.", "labels": [], "entities": []}, {"text": "For each slice, we train an SVM.", "labels": [], "entities": []}, {"text": "Then results of SVMs are put together with a simple combination.", "labels": [], "entities": []}, {"text": "It takes about 2 days to train a slice on a P3 1.13GHz processor.", "labels": [], "entities": []}, {"text": "As a result of this subdivision of the training data into slices, we cannot take advantage of SVM's global optimization ability.", "labels": [], "entities": []}, {"text": "This seems to nullify our effort to create this new algorithm.", "labels": [], "entities": []}, {"text": "However, our new algorithm is still useful for the following reasons.", "labels": [], "entities": []}, {"text": "Firstly, with the improvement in the computing resources, we will be able to use larger slices so as to utilize more global optimization.", "labels": [], "entities": []}, {"text": "SVMs are superior to other linear classifiers in theory.", "labels": [], "entities": []}, {"text": "On the other hand, the current size of the slice is large enough for other NLP applications like text chunking, although it is not large enough for parse reranking.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 97, "end_pos": 110, "type": "TASK", "confidence": 0.8042968213558197}, {"text": "parse reranking", "start_pos": 148, "end_pos": 163, "type": "TASK", "confidence": 0.8773139417171478}]}, {"text": "The last reason is that we have achieved state-of-the-art results even with the sliced data.", "labels": [], "entities": []}, {"text": "We have used both a linear kernel and a tree kernel.", "labels": [], "entities": []}, {"text": "For the linear kernel test, we have used the same dataset as that in.", "labels": [], "entities": []}, {"text": "In this experiment, we first train 22 SVMs on 22 distinct slices.", "labels": [], "entities": []}, {"text": "In order to combine those SVMs results, we have tried mapping SVMs' results to probabilities via a Sigmoid as described in.", "labels": [], "entities": []}, {"text": "We use the development data to estimate parameter A and B in the Sigmoid where f i is the result of the ith SVM.", "labels": [], "entities": [{"text": "Sigmoid", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9532284736633301}]}, {"text": "The parse with maximal value of i P i (y = 1|f i ) is chosen as the topmost parse.", "labels": [], "entities": []}, {"text": "Experiments on the development data shows that the result is better if Ae \u2212fiB is much larger than 1.", "labels": [], "entities": [{"text": "Ae \u2212fiB", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9350494146347046}]}, {"text": "Therefore Therefore, we may use if i directly, and there is no need to estimate.", "labels": [], "entities": []}, {"text": "Then we combine SVMs' result with the log-likelihood generated by the parser.", "labels": [], "entities": []}, {"text": "Parameter \u03b2 is used as the weight of the log-likelihood.", "labels": [], "entities": [{"text": "Parameter \u03b2", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9762280881404877}]}, {"text": "In addition, we find that our SVM has greater labeled precision than labeled recall, which means that the system prefer parses with less brackets.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.5935887694358826}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.7776516675949097}]}, {"text": "So we take the number of brackets as another feature to be considered, with weight \u03b1. \u03b1 and \u03b2 are estimated on the development data.", "labels": [], "entities": []}, {"text": "The result is shown in.", "labels": [], "entities": []}, {"text": "The performance of our system matches the results of), but is a little lower than the results of the Boosting system in, except that the percentage of sentences with no crossing brackets is 1% higher than that of).", "labels": [], "entities": []}, {"text": "Since we have to divide data into slices, we cannot take full advantage of the margin maximization.", "labels": [], "entities": []}, {"text": "\u03b2 is used to con-  trol the weight of log-likelihood given by the parser.", "labels": [], "entities": []}, {"text": "The proper value of \u03b2 depends on the size of training data.", "labels": [], "entities": []}, {"text": "The best result does not improve much after combining 7 slices of training data.", "labels": [], "entities": []}, {"text": "We think this is due the limitation of local optimization.", "labels": [], "entities": []}, {"text": "Our next experiment is on the tree kernel as it is used in).", "labels": [], "entities": []}, {"text": "We have only trained 5 slices, since each slice takes about 2 weeks to train on a P3 1.13GHz processor.", "labels": [], "entities": []}, {"text": "In addition, the speed of test for the tree kernel is much slower than that for the linear kernel.", "labels": [], "entities": []}, {"text": "The experimental result is shown in The results of our SVM system match the results of the Voted Perceptron algorithm in), although only 5 slices, amounting to less than one fourth of the whole training dataset, have been used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on section 23 of the WSJ Treebank.  LR/LP = labeled recall/precision. CBs = average number  of crossing brackets per sentence. 0 CBs, 2 CBs are the  percentage of sentences with 0 or \u2264 2 crossing brackets  respectively. CO99 = Model 2 of (Collins, 1999). CH00  = (Charniak, 2000). CO00 = (Collins, 2000).", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9859262108802795}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.8561961054801941}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.905449628829956}, {"text": "CO00", "start_pos": 299, "end_pos": 303, "type": "DATASET", "confidence": 0.7269194722175598}]}, {"text": " Table 2: Results on section 23 of the WSJ Treebank.  LR/LP = labeled recall/precision. CBs = average num- ber of crossing brackets per sentence. CO99 = Model 2  of (Collins, 1999). CD02 = (Collins and Duffy, 2002)", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9870668053627014}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.7853384613990784}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.8293181657791138}, {"text": "CBs", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9407128095626831}]}]}