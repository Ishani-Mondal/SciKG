{"title": [{"text": "A Study for Documents Summarization based on Personal Annotation", "labels": [], "entities": [{"text": "Documents Summarization", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7804525792598724}]}], "abstractContent": [{"text": "For one document, current summarization systems produce a uniform version of summary for all users.", "labels": [], "entities": []}, {"text": "Personalized summarizations are necessary in order to represent users' preferences and interests.", "labels": [], "entities": []}, {"text": "Annotation is getting important for document sharing and collaborative filtering, which in fact record users' dynamic behaviors compared to traditional steady profiles.", "labels": [], "entities": [{"text": "document sharing", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7762306332588196}]}, {"text": "In this paper we introduce anew summarization system based on users' annotations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.97797691822052}]}, {"text": "Annotations and their contexts are extracted to represent features of sentences , which are given different weights for representation of the document.", "labels": [], "entities": []}, {"text": "Our system produces two versions of summaries for each document: generic summary without considering annotations and annotation-based summary.", "labels": [], "entities": []}, {"text": "Since annotation is a kind of personal data, annotation-based summary is tailored to user's interests to some extent.", "labels": [], "entities": []}, {"text": "We show by experiments that annotations can help a lot in improving summarization performance compared to no annotation consideration.", "labels": [], "entities": [{"text": "summarization", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.9866710305213928}]}, {"text": "At the same time, we make an extensive study on us-ers' annotating behaviors and annotations distribution , and propose a variety of techniques to evaluate the relationships between annotations and summaries, such as how the number of annotations affects the summarizing performance.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 259, "end_pos": 270, "type": "TASK", "confidence": 0.9805153608322144}]}, {"text": "A study about collaborative filtering is also made to evaluate the summarization based on annotations of similar users.", "labels": [], "entities": []}], "introductionContent": [{"text": "As information explodes in the Internet, it's hard for users to read through all the published materials potentially interesting.", "labels": [], "entities": []}, {"text": "Therefore, it is of great help to present them in a condensed way, i.e. using extracts or abstracts that generalize the content of an article.", "labels": [], "entities": []}, {"text": "Text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version fora particular user (or users) and task (or tasks)).", "labels": [], "entities": [{"text": "Text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version fora particular user (or users) and task (or tasks))", "start_pos": 0, "end_pos": 189, "type": "Description", "confidence": 0.8279197538892428}]}, {"text": "Summary can help the user quickly get a general idea about the article and decide if it deserves more detailed attention.", "labels": [], "entities": [{"text": "Summary", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9483144879341125}]}, {"text": "The problem with present summarization systems is that they produce one uniform summary fora given document without considering users' opinions on it, while different users may have different perspectives on the same text, thus need a different summary.", "labels": [], "entities": []}, {"text": "A good summary should change corresponding to interests and preferences of its reader.", "labels": [], "entities": []}, {"text": "We refer to the adaptation of the summarization process to a particular user as personalized summarization.", "labels": [], "entities": [{"text": "summarization process", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.9054066836833954}]}, {"text": "We wish to extract the kind of personalized summary in this paper.", "labels": [], "entities": []}, {"text": "Annotating is a common behavior while reading, since many users would like to make some remarks or to highlight some important parts of the text.", "labels": [], "entities": [{"text": "Annotating", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9324921369552612}]}, {"text": "So we think that, to some extent, annotations represent users' interests by recording some viewpoints of users on the documents, such as which part is important or interesting, which is not.", "labels": [], "entities": []}, {"text": "With the rapidly technologies improvements on tablet-pc, it is possible to record these behaviors automatically.", "labels": [], "entities": []}, {"text": "From another point, annotations can be seen as a kind of user feedback, but different from traditional explicit relevance feedback) in the sense of that annotation can be collected automatically without user's consciousness.", "labels": [], "entities": []}, {"text": "Since annotation is a kind of personal data, it is expected to help improve the personalization of present *This work was done when the first author visited Microsoft Research Asia.", "labels": [], "entities": [{"text": "Microsoft Research Asia", "start_pos": 157, "end_pos": 180, "type": "DATASET", "confidence": 0.8771922787030538}]}, {"text": "document summarizers; also, intention of users can be learned through this interaction process.", "labels": [], "entities": [{"text": "document summarizers", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5742909163236618}]}, {"text": "Since a user may make annotations freely, not all of them are useful for representing his real preferences.", "labels": [], "entities": []}, {"text": "Therefore it is helpful to find how some of the annotations affect on summarization performance, based on that, we make a study on evaluations about the number of annotations effective.", "labels": [], "entities": [{"text": "summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9780819416046143}]}, {"text": "In spite of summarization based on the user's annotation himself, we want to do more when his annotation on current document is not available.", "labels": [], "entities": []}, {"text": "Suppose he is new to the domain, but other users annotated on the document, we can resort to annotations of similar users to help his summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.9835286140441895}]}, {"text": "Study in such situation is thought of as collaboratively filtering which filter the useful information from similar users to help the summarization of current user.", "labels": [], "entities": [{"text": "summarization", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.98639315366745}]}, {"text": "To our knowledge, no work has been done about such annotation studies.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces some related work in text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8246593177318573}]}, {"text": "Section 3 presents the new annotation based text summarization approach, including annotation, context and keywords extraction, and summary generation.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.6685346066951752}, {"text": "context and keywords extraction", "start_pos": 95, "end_pos": 126, "type": "TASK", "confidence": 0.599489189684391}, {"text": "summary generation", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.8238348066806793}]}, {"text": "Section 4 gives the metrics for evaluation of annotated summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9040536284446716}]}, {"text": "Section 5 discusses the detailed experiments and evaluation of summarization, annotations and collaborative filtering.", "labels": [], "entities": [{"text": "summarization", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9804783463478088}]}, {"text": "Section 6 concludes the paper with a simple summary and future work.", "labels": [], "entities": [{"text": "summary", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.8996398448944092}]}], "datasetContent": [{"text": "The problem of evaluating text summarization is a quite deep one, and some problems remain concerning the appropriate methods and types of evaluation.", "labels": [], "entities": [{"text": "evaluating text summarization", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.7082593441009521}]}, {"text": "There area variety of possible bases for comparison of summarization performance, e.g., summary to source, system to manual summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9572854042053223}]}, {"text": "In general, methods for text summarization can be classified into two categories).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7819997370243073}]}, {"text": "The first is intrinsic evaluation, which judge the quality of the summarization directly based on analysis of the summary, including user judgments of fluency of the summary, coverage of the \"key/essential ideas\", or similarly to an \"ideal\" summary which is hard to establish.", "labels": [], "entities": []}, {"text": "The other is extrinsic evaluation, which judge the quality of the summarization based on how it affects on the completion of other tasks, such as question answering and comprehension tasks.", "labels": [], "entities": [{"text": "question answering", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.812759280204773}]}, {"text": "Here we use intrinsic evaluation for our summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.976239800453186}]}, {"text": "It is to compare the system summary with an ideal manual summary.", "labels": [], "entities": []}, {"text": "Since we need to collect annotations for experimented documents, which require reading through the text, manual summaries can be made consequently after the reading.", "labels": [], "entities": []}, {"text": "The documents dataset to be evaluated are supplied with human annotations and summaries, which will be described in detail in the next section.", "labels": [], "entities": []}, {"text": "For one annotated document, our annotation based summarization (ABS) system produce two versions of summaries: generic summary without considering annotations, and annotated summary considering annotations.", "labels": [], "entities": []}, {"text": "For evaluation, we made comparison between human-made summary and generic summary, and comparisons between human-made summary and annotated summary.", "labels": [], "entities": []}, {"text": "There area lot of measures to make the comparisons (), such as precision, recall, some of which will be used for our evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9996905326843262}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.999647855758667}]}, {"text": "Another measure we are interested in is the cosine similarity for two summaries, which is defined on keywords, and reflects the general similarity of two summaries in global distribution.", "labels": [], "entities": []}, {"text": "For human-made summary S 0 and summary Si generated by ABS, summary similarity is as follows: Precision and recall are generally applied to sentences; in fact they can be applied to keywords too, which reflects the percentage of keywords correctly identified.", "labels": [], "entities": [{"text": "ABS", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.8665826916694641}, {"text": "summary similarity", "start_pos": 60, "end_pos": 78, "type": "METRIC", "confidence": 0.6543249487876892}, {"text": "Precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9981439113616943}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.997750461101532}]}, {"text": "Therefore, in spite of summary similarity, our measures for evaluation also include sentences precision, sentences recall, keywords precision and keywords recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.8407313227653503}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.7579126954078674}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.6084475517272949}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.870442271232605}]}, {"text": "For keywords evaluation, a keyword is correct only if it occurs in human-made summary.", "labels": [], "entities": [{"text": "keywords evaluation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7506835162639618}]}, {"text": "For sentences evaluation, a sentence in summary is correct if it has as many possible keywords as in the corresponding sentence in the human-made summary, that is, their similarity (calculated same as summary similarity) is beyond a certain threshold.", "labels": [], "entities": [{"text": "sentences evaluation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7075243294239044}]}, {"text": "We use two types of sentences match in the experiments: one is perfect match, which means a sentence in summary is correct only if it occurs in manual summary; the other is conditional match, which means most concepts of the two sentences are correct, in this case the match similarity threshold is less than 1.", "labels": [], "entities": [{"text": "match similarity threshold", "start_pos": 269, "end_pos": 295, "type": "METRIC", "confidence": 0.7660120526949564}]}, {"text": "For a set of annotated documents, average values for the above five measures are calculated to show the general performance of the comparison.", "labels": [], "entities": []}, {"text": "Since our approaches are based on annotated documents, we need to collect users' annotations fora set of documents.", "labels": [], "entities": []}, {"text": "In the meantime, users are required to supply a summary consisting of several sentences that reflects the main ideas of the document.", "labels": [], "entities": []}, {"text": "We supply an annotating tool called \"Annotation\" that can be used to highlight words, phrases, or sentences which users are interested in, and to select important sentences into summary.", "labels": [], "entities": []}, {"text": "The original data we used is from Yahoo news, which is interesting to most users.", "labels": [], "entities": [{"text": "Yahoo news", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.9383063912391663}]}, {"text": "The data is composed of eight categories: business, sports, entertainment, world, science, technology, politics, and oddly, with totally about 6000 documents.", "labels": [], "entities": []}, {"text": "Documents are preprocessed and removed graphics and tags before experiments.", "labels": [], "entities": []}, {"text": "We hired five students 10 days to annotate the documents, each student were supplied 20 percent of the documents.", "labels": [], "entities": []}, {"text": "They are allowed to choose articles interesting to do the experiments.", "labels": [], "entities": []}, {"text": "Users are told to make annotations freely and summaries which reflect the main ideas of the text.", "labels": [], "entities": []}, {"text": "The process of annotating and summarizing for one user are independent, that is, they are done at different time.", "labels": [], "entities": [{"text": "summarizing", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9441623687744141}]}, {"text": "At last, we collect totally 1098 different annotated documents, each of which consists of a set of annotations and a human-made summary.", "labels": [], "entities": []}, {"text": "The statistics for five persons is presented in.", "labels": [], "entities": []}, {"text": "Since different users may have different annotation style, we separate the experiments for each individual's data.", "labels": [], "entities": []}, {"text": "In experiments, the keyword threshold \u03b1 is set 2, which is reasonable that most keywords' frequency is at least 2.", "labels": [], "entities": []}, {"text": "The threshold for summary similarity related to sentences precision, is 0.3 (which in fact means its square root 55% sentences are correct).", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9749837517738342}]}, {"text": "The summarizer produce the same number of sentences as are in the corresponding manual summary, as in (, therefore, precision and recall are the same for summaries sentences comparison.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.93696129322052}, {"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9995512366294861}, {"text": "recall", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9984104633331299}, {"text": "summaries sentences comparison", "start_pos": 154, "end_pos": 184, "type": "TASK", "confidence": 0.8802353342374166}]}, {"text": "First, we make experiments with different annotations and context weights.", "labels": [], "entities": []}, {"text": "The results are presented in2.", "labels": [], "entities": []}, {"text": "The first column contains different combinations of annotation weight \u03b2 and context weight \u03b3, as described in Section 3.2.", "labels": [], "entities": [{"text": "annotation weight \u03b2", "start_pos": 52, "end_pos": 71, "type": "METRIC", "confidence": 0.8486767411231995}]}, {"text": "See 2 for the meanings of other columns.", "labels": [], "entities": []}, {"text": "It is obvious that context can help to improve the summarization performance than no context consideration, so in our later experiments, we set the context weight \u03b3=1, and annotation weight \u03b2=1..", "labels": [], "entities": [{"text": "summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.9883720278739929}, {"text": "annotation weight \u03b2", "start_pos": 172, "end_pos": 191, "type": "METRIC", "confidence": 0.8760214249293009}]}, {"text": "Comparison of annotated summary with different parameters.", "labels": [], "entities": []}, {"text": "Next, we make formal experiments for generic summarization and annotation based summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.805709719657898}]}, {"text": "presents the average performance results for five users' data 3 . From the above table and figure, we found that annotation-based summarization is much better than generic summarization.", "labels": [], "entities": []}, {"text": "The improvements are quite inspiring.", "labels": [], "entities": []}, {"text": "In the case of user P4, the cosine similarity is increased by 10.1%; the sentences precision for conditional match is increased by 13.57%; precision for perfect match is increased by 17.59%; keywords precision is increased by 11.6%; keywords recall is increased by 13.18%, which shows that annotations can help a lotto improve the summarization performance..", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 28, "end_pos": 45, "type": "METRIC", "confidence": 0.7522964179515839}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.8395522832870483}, {"text": "precision", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9988033771514893}, {"text": "precision", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.718008279800415}, {"text": "recall", "start_pos": 242, "end_pos": 248, "type": "METRIC", "confidence": 0.9646134972572327}]}, {"text": "Comparison of generic summary and annotated summary for 5 users' data.", "labels": [], "entities": []}, {"text": "shows the average performance comparison for total 1098 documents.", "labels": [], "entities": []}, {"text": "Compared with generic summarization, cosine similarity of annotation based summarization is increased by 6.47%; sentences precision for conditional match is increased by 7.99%; precision for perfect match increased by 10.62%; keywords precision increased by 7.14%; keywords recall increased by 8.61%.", "labels": [], "entities": [{"text": "generic summarization", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.5836881995201111}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.8500808477401733}, {"text": "precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9987069368362427}, {"text": "precision", "start_pos": 235, "end_pos": 244, "type": "METRIC", "confidence": 0.7621947526931763}, {"text": "recall", "start_pos": 274, "end_pos": 280, "type": "METRIC", "confidence": 0.966696560382843}]}, {"text": "The most significant is that the improvement of sentences precision for perfect match is higher than 10%, since perfect matches require two sentences totally matched, it is very important to gain this point, showing that in general annotations are able to contribute much to the performance of summarization.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9819000959396362}, {"text": "summarization", "start_pos": 294, "end_pos": 307, "type": "TASK", "confidence": 0.9783571362495422}]}, {"text": "In, \"BA\" means 'first Best' Annotated summary, which will be explained in the next subsection..", "labels": [], "entities": [{"text": "BA", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.9845987558364868}, {"text": "first Best' Annotated summary", "start_pos": 16, "end_pos": 45, "type": "METRIC", "confidence": 0.8862407684326172}]}, {"text": "Performance comparison of generic summary, annotated summary and 'first best' summary.", "labels": [], "entities": [{"text": "first best' summary", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.8493183851242065}]}, {"text": "In our last experiments, we found that the average annotation number was 11.86, which was much higher than summary length.", "labels": [], "entities": [{"text": "annotation number", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.8524731397628784}, {"text": "summary length", "start_pos": 107, "end_pos": 121, "type": "METRIC", "confidence": 0.6684443950653076}]}, {"text": "We wonder whether there are some relations between the number of annotations and summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9713115692138672}]}, {"text": "Thus we make such annotations evaluations to study how the number of annotations affects on summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.9796062111854553}]}, {"text": "The first experiment is to find the best summary by selecting the first k (k\u2264n, n is total number of annotations) annotations, that is \"first Best Annotated summary\".", "labels": [], "entities": []}, {"text": "In fact, from we can see that when using all of the annotations, the performance falls about in the middle of generic summary and first Best Annotated summary (labeled by \"BA\").", "labels": [], "entities": [{"text": "BA", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.937747597694397}]}, {"text": "However we found that, in some annotated documents, some of annotations are beyond the scope of the manual summary.", "labels": [], "entities": []}, {"text": "This means that some of them are noisy to summarization; using all of them cannot reach the best performance; there must have a changing process of performance as more annotations are considered, which confirms us to explore the relationship between the annotation number and the summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9927639365196228}]}, {"text": "So the next experiment is to observe how the average summarization performance evolves as we select any of the k annotations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9636112451553345}]}, {"text": "That is, for any annotation number k\u2264n, to average the summarization performance by enumerating all possible annotations combinations.", "labels": [], "entities": []}, {"text": "presents such a plot fora document \"sports_78.html\", which indicates how the number of annotations affects summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 107, "end_pos": 120, "type": "TASK", "confidence": 0.9663029313087463}]}, {"text": "It has totally 15 annotations, \"0\" stands for generic summary performance.", "labels": [], "entities": []}, {"text": "When considering only one annotation, the performance drops a bit down (We found in this document some of the single annotations are faraway from corresponding summary), but as more annotations considered, the performance begins to increase slowly and reaches the best at annotation number 12, then again begins to drop.", "labels": [], "entities": []}, {"text": "For other documents, we find similar GA BA situations that at beginning the performance increases along the number of annotations, but after it reaches a certain degree, the performance will fluctuate and sometimes drop slightly down.", "labels": [], "entities": [{"text": "GA BA", "start_pos": 37, "end_pos": 42, "type": "TASK", "confidence": 0.7950661182403564}]}, {"text": "These are all true for our 5 evaluation measures, which are consistent and reasonable since too many annotations will introduce some degree of noise, which would bias the summarization process.", "labels": [], "entities": [{"text": "summarization", "start_pos": 171, "end_pos": 184, "type": "TASK", "confidence": 0.9722050428390503}]}, {"text": "We conclude from this evolving process that not all of the annotations are valuable for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.987672746181488}]}, {"text": "In, the best and worst performance point can be identified, for example, we get the best point at annotation number 12 and the worst point at annotation number 1.", "labels": [], "entities": []}, {"text": "For a subset of 10 documents, summary similarity comparisons with generic, best, worst, and all annotations-based summarization are shown in.", "labels": [], "entities": []}, {"text": "Different documents have different annotations, which have different influences on summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.9777314066886902}]}, {"text": "In most cases, best summaries are better than all annotations-based summaries, which are better than generic and worst summaries.", "labels": [], "entities": []}, {"text": "There is an exception in at Document 5, we found in this document some of the annotations are irrelevant to the summary.", "labels": [], "entities": [{"text": "Document 5", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9372090101242065}]}, {"text": "For example, in this document, percentage of summary annotated sentences in annotated sentences is 28.57%; and percentage of summary annotated keywords in summary keywords is only 17.19%.", "labels": [], "entities": []}, {"text": "While for Document 6, the corresponding values are 50.00% and 32.73%.", "labels": [], "entities": [{"text": "Document 6", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8011398017406464}]}, {"text": "This indicates that the user's interests drifted as he read the document.", "labels": [], "entities": []}, {"text": "When annotations are consistent with users' summaries, they help to improve the personalization of summarization, the more the better, otherwise, when the annotations are far from users' summaries, the influence of annotations maybe negative, for example some annotations subset make the performance worse.", "labels": [], "entities": []}, {"text": "But generally the former has a much larger proportion than the latter.", "labels": [], "entities": []}, {"text": "Along the documents in, the main trend is that annotation-based summaries are better than summaries with no annotations consideration; the average improvement for 12 documents is 13.49%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. A user study.", "labels": [], "entities": []}, {"text": " Table 2. Comparison of annotated summary with  different parameters.", "labels": [], "entities": []}, {"text": " Table 3. Comparison of generic summary and an- notated summary for 5 users' data.", "labels": [], "entities": []}, {"text": " Table 4. Users annotation similarities.", "labels": [], "entities": []}]}