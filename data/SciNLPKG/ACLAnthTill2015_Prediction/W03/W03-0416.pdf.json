{"title": [{"text": "An Efficient Clustering Algorithm for Class-based Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper defines a general form for class-based probabilistic language models and proposes an efficient algorithm for clustering based on this.", "labels": [], "entities": []}, {"text": "Our evaluation experiments revealed that our method decreased computation time drastically, while retaining accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9983526468276978}]}], "introductionContent": [{"text": "Clustering algorithms have been extensively studied in the research area of natural language processing because many researchers have proved that \"classes\" obtained by clustering can improve the performance of various NLP tasks.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.6694692373275757}]}, {"text": "Examples have been class-based \u00d2-gram models, smoothing techniques for structural disambiguation) and word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.7206393082936605}]}, {"text": "In this paper, we define a general form for class-based probabilistic language models, and propose an efficient and model-theoretic algorithm for clustering based on this.", "labels": [], "entities": []}, {"text": "The algorithm involves three operations, CLAS-SIFY, MERGE, and SPLIT, all of which decreases the optimization function based on the MDL principle, and can efficiently find a point near the local optimum.", "labels": [], "entities": [{"text": "MERGE", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9740895628929138}, {"text": "SPLIT", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9499795436859131}]}, {"text": "The algorithm is applicable to more general tasks than existing studies (, and computational costs are significantly small, which allows its application to very large corpora.", "labels": [], "entities": []}, {"text": "Clustering algorithms maybe classified into three types.", "labels": [], "entities": []}, {"text": "The first is a type that uses various heuristic measure of similarity between the elements to be clustered and has no interpretation as a probability model).", "labels": [], "entities": []}, {"text": "The resulting clusters from this type of method are not guaranteed to work effectively as a component of a statistical language model, because the similarity used in clustering is not derived from the criterion in the learning process of the statistical model, e.g. likelihood.", "labels": [], "entities": []}, {"text": "The second type has clear interpretation as a probability model, but no criteria to determine the number of clusters (.", "labels": [], "entities": []}, {"text": "The performance of methods of this type depend on the number of clusters that must be specified before the clustering process.", "labels": [], "entities": []}, {"text": "It may prove rather troublesome to determine the proper number of clusters in this type of method.", "labels": [], "entities": []}, {"text": "The third has interpretation as a probability model and uses some statistically motivated model selection criteria to determine the proper number of clusters.", "labels": [], "entities": []}, {"text": "This type has a clear advantage compared to the second.", "labels": [], "entities": []}, {"text": "AutoClass), the Bayesian model merging method) and Li's method) are examples of this type.", "labels": [], "entities": [{"text": "Bayesian model merging", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.5757814745108286}]}, {"text": "AutoClass and the Bayesian model merging are based on soft clustering models and Li's method is based on a hard clustering model.", "labels": [], "entities": [{"text": "AutoClass", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9215027093887329}]}, {"text": "In general, computational costs for hard clustering models are lower than that for soft clustering models.", "labels": [], "entities": []}, {"text": "However, the time complexity of Li's method is of cubic order in the size of the vocabulary.", "labels": [], "entities": []}, {"text": "Therefore, it is not practical to apply it to large corpora.", "labels": [], "entities": []}, {"text": "Our model and clustering algorithm provide a solution to these problems with existing clustering algorithms.", "labels": [], "entities": []}, {"text": "Since the model has clear interpretation as a probability model, the clustering algorithm uses MDL as clustering criteria and using a combination of top-down clustering, bottom-up clustering, and a K-means style exchange algorithm, the method we propose can perform the clustering efficiently.", "labels": [], "entities": []}, {"text": "We evaluated the algorithm through experiments on a disambiguation task of Japanese dependency analysis.", "labels": [], "entities": [{"text": "Japanese dependency analysis", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.5913437902927399}]}, {"text": "In the experiments, we observed that the proposed algorithm's computation time is roughly linear to the size of the vocabulary, and it performed slightly better than the existing method.", "labels": [], "entities": []}, {"text": "Our main intention in the experiments was to see improvements in terms of computational cost, not in performance in the test task.", "labels": [], "entities": []}, {"text": "We will show, in Sections 2 and 3, that the proposed method can be applied to a broader range of tasks than the test task we evaluate in the experiments in Section 4.", "labels": [], "entities": []}, {"text": "We need further experiments to determine the performance of the proposed method with more general tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section discusses the results of the evaluation experiment where we compared three clustering methods: i.e., our method, Li's agglomerative method described in, and a restricted version of our method that only uses CLASSIFY.", "labels": [], "entities": []}, {"text": "We used a simplified version of the dependency analysis task for Japanese for the evaluation experiment.", "labels": [], "entities": [{"text": "dependency analysis", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8008641004562378}]}, {"text": "In Japanese, a sentence can bethought of as an array of phrasal units called 'bunsetsu' and the dependency structure of a sentence can be represented by the relationships between these bunsetsus.", "labels": [], "entities": []}, {"text": "A bunsetsu consists of one or more content words and zero or more function words that follow these.", "labels": [], "entities": []}, {"text": "For example, the Japanese sentence Ryoushi-ga kawa-de oyogu nezumi-wo utta.", "labels": [], "entities": []}, {"text": "hunter-SUBJ river-in swim mouse-OBJ shot (A hunter shot a mouse which swam in the river.) contains five bunsetsus \ud97b\udf59 Ryoushi-ga, kawa-de, oyogu, nezumi-wo, utta \ud97b\udf59 and their dependency relations are as follows: Ryoushi-ga \ud97b\udf59 utta kawa-de \ud97b\udf59 oyogu oyogu \ud97b\udf59 nezumi-wo nezumi-wo \ud97b\udf59 utta Our task is, given an input bunsetsu, to output the correct bunsetsu on which the input bunsetsu depends.", "labels": [], "entities": []}, {"text": "In this task, we considered the dependency relations of limited types.", "labels": [], "entities": []}, {"text": "That is the dependency of types: noun-pp \ud97b\udf59 pred , where noun is a noun, or the head of a compound noun, pp is one of 9 postpositions \ud97b\udf59ga, wo, ni, de, to, he, made, kara, yori\ud97b\udf59 and pred is a bunsetsu which contains a verb or an adjective as its content word part.", "labels": [], "entities": []}, {"text": "We restricted possible dependee bunsetsus to be those to the right of the input bunsetsus because in Japanese, basically all dependency relations are from left to right.", "labels": [], "entities": []}, {"text": "Thus, our test data is in the form \ud97b\udf59 noun-pp\ud97b\udf59 \ud97b\udf59pred \u00bd pred \u00d2 \ud97b\udf59 \ud97b\udf59\ud97b\udf59 where \ud97b\udf59pred \u00bd ,...,pred \u00d2 \ud97b\udf59 is the set of all candidate dependee bunsetsus that are to the right of the input dependent bunsetsu noun-pp in a sentence.", "labels": [], "entities": []}, {"text": "The task is to select the correct dependee of noun-pp from \ud97b\udf59pred \u00bd ,..,pred \u00d2 \ud97b\udf59.", "labels": [], "entities": [{"text": "\ud97b\udf59pred \u00bd", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.7186957001686096}]}, {"text": "Our training data is in the form \ud97b\udf59\u00d6, noun, pp, pred\ud97b\udf59.", "labels": [], "entities": [{"text": "\ud97b\udf59", "start_pos": 33, "end_pos": 34, "type": "METRIC", "confidence": 0.9915621876716614}, {"text": "\u00d6", "start_pos": 34, "end_pos": 35, "type": "METRIC", "confidence": 0.5990471839904785}]}, {"text": "A sample of this form represents two bunsetsus, nounpp and pred within a sentence, in this order, and \u00d6 \u00be \ud97b\udf59\u00b7\ud97b\udf59 denotes whether they are in a dependency relation (\u00d6 \ud97b\udf59 \u00b7 ), or not (\u00d6 \ud97b\udf59 )..", "labels": [], "entities": [{"text": "\u00d6 \u00be \ud97b\udf59\u00b7\ud97b\udf59", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9786158005396525}]}, {"text": "We extracted all the positive (i.e., \u00d6 \ud97b\udf59 \u00b7 ) and negative (\u00d6 \ud97b\udf59 ) relation samples and divided them into 10 disjunctive sets for 10-fold cross validation.", "labels": [], "entities": []}, {"text": "When we divided the samples, all the relations extracted from one sentence were put together in one of 10 sets.", "labels": [], "entities": []}, {"text": "When a set was used as the test data, these relations from one sentence were used as the test data of the form (Eq.8).", "labels": [], "entities": []}, {"text": "Of course, we did not use samples with only one pred.", "labels": [], "entities": []}, {"text": "In the results in the next subsection, the 'training data of size \u00d7' means where we used a subset of positive samples that were covered by the most frequent \u00d7 nouns and the most frequent \u00d7 pp:pred pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Description length in training data sets (unit: \u00bd \u00a2 \u00bd\u00bc \ud97b\udf59 )", "labels": [], "entities": []}, {"text": " Table 2: Performance of each method in the evaluation task", "labels": [], "entities": []}]}