{"title": [], "abstractContent": [{"text": "Simple baselines provide insights into the value of scoring functions and give starting points for measuring the performance improvements of technological advances.", "labels": [], "entities": []}, {"text": "This paper presents baseline unsupervised techniques for performing word alignment based on geometric and word edit distances as well as supervised fusion of the results of these techniques using the nearest neighbor rule.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.8056716918945312}]}], "introductionContent": [{"text": "Simple baselines provide insights into the value of scoring functions and give starting points for measuring the performance improvements of technological advances.", "labels": [], "entities": []}, {"text": "This paper presents baseline unsupervised techniques for performing word alignment based on geometric and word edit distances as well as supervised fusion of the results of these techniques using the nearest neighbor rule.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.8056716918945312}]}], "datasetContent": [{"text": "Two datasets of different language pairs were used to evaluate these measures: Romanian-English and EnglishFrench.", "labels": [], "entities": [{"text": "EnglishFrench", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9414343237876892}]}, {"text": "The measures were optimized on atrial dataset and then evaluated blind on a test set.", "labels": [], "entities": []}, {"text": "The RomanianEnglish trial data was 17 sentences long and the EnglishFrench trial dataset was 37 sentences.", "labels": [], "entities": [{"text": "RomanianEnglish trial data", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.9542620579401652}, {"text": "EnglishFrench trial dataset", "start_pos": 61, "end_pos": 88, "type": "DATASET", "confidence": 0.9778618216514587}]}, {"text": "Additionally, approximately 1.1 million aligned English-French sentences and 48,000 Romanian-English sentences were used for the set of supervised experiments.", "labels": [], "entities": []}, {"text": "Four measures were used to evaluate the classifiers: precision, recall, F-measure, and alignment error rate (AER).", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9997084736824036}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9989691972732544}, {"text": "F-measure", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9986275434494019}, {"text": "alignment error rate (AER)", "start_pos": 87, "end_pos": 113, "type": "METRIC", "confidence": 0.9615092277526855}]}, {"text": "Precision and recall are the ratios of matching aligned pairs to the number of predicted pairs and the number of reference pairs respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9885565042495728}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9990161657333374}]}, {"text": "F-measure is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9710832834243774}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9993749260902405}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9960999488830566}]}, {"text": "AER differentiates between \"sure\" and \"possible\" aligned pairs in the reference, requiring hypotheses to match those that are \"sure\" and permitting them to match those that are \"possible\".).", "labels": [], "entities": [{"text": "AER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8732442855834961}]}, {"text": "shows results of the explored methods on the trial data, ordered by degree of supervision and AER on the Romanian-English dataset.", "labels": [], "entities": [{"text": "AER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9997976422309875}, {"text": "Romanian-English dataset", "start_pos": 105, "end_pos": 129, "type": "DATASET", "confidence": 0.8769164979457855}]}, {"text": "The biased coin random aligner is indicated as random and the final punctuation aligner is fpunct.", "labels": [], "entities": []}, {"text": "The classifier based on relative length is len.", "labels": [], "entities": []}, {"text": "The three edit distance measures are exact match (exact), edit distance (wedit), and lower-case edit distance (lcedit).", "labels": [], "entities": [{"text": "exact match (exact)", "start_pos": 37, "end_pos": 56, "type": "METRIC", "confidence": 0.8833658814430236}, {"text": "edit distance (wedit)", "start_pos": 58, "end_pos": 79, "type": "METRIC", "confidence": 0.9539638161659241}, {"text": "edit distance (lcedit)", "start_pos": 96, "end_pos": 118, "type": "METRIC", "confidence": 0.8669014215469361}]}, {"text": "The geometric measures are word distance to the diagonal (wdiag), distance to the character diagonal, (cdiag), and distance from the character box made by the word pair to the character diagonal, (cbox).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Trial set results.", "labels": [], "entities": []}, {"text": " Table 2: NON-OFFICIAL test set results (ignoring elements aligned with null).", "labels": [], "entities": [{"text": "NON-OFFICIAL test set", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.7641621232032776}]}]}