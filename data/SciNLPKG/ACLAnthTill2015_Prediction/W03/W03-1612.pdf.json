{"title": [{"text": "Paraphrasing Rules for Automatic Evaluation of Translation into Japanese", "labels": [], "entities": [{"text": "Evaluation of Translation into Japanese", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.6812230229377747}]}], "abstractContent": [{"text": "Automatic evaluation of translation quality has proved to be useful when the target language is English.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9269660711288452}]}, {"text": "In this paper the evaluation of translation into Japanese is studied.", "labels": [], "entities": [{"text": "translation into Japanese", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.8812010685602824}]}, {"text": "An existing method based on n-gram similarity between translations and reference sentences is difficult to apply to the evaluation of Japanese because of the ag-glutinativeness and variation of semantically similar expressions in Japanese.", "labels": [], "entities": []}, {"text": "The proposed method applies a set of paraphrasing rules to the reference sentences in order to increase the similarity score for the expressions that differ only in their writing styles.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 108, "end_pos": 124, "type": "METRIC", "confidence": 0.9830321371555328}]}, {"text": "Experimental results show the paraphrasing rules improved the correlation between automatic evaluation and human evaluation from 0.80 to 0.93.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating natural language processing applications' output is important both for users and developers.", "labels": [], "entities": []}, {"text": "Tasks such as sentential parsing, morphological analysis and named entity recognition are easy to evaluate automatically because the \"right answer\" can be defined deterministically under a specific grammar or assumed criterion.", "labels": [], "entities": [{"text": "sentential parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.8306638896465302}, {"text": "morphological analysis", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.771759033203125}, {"text": "named entity recognition", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.6330757935841879}]}, {"text": "The evaluation of machine translation is not so straightforward since there are infinite ways to output similar meanings and one cannot enumerate the right answers exhaustively.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7756871283054352}]}, {"text": "In spite of that, automatic translation evaluation is practically important because the evaluation is laborious work for humans and evaluation by humans tends to be arbitrary.", "labels": [], "entities": [{"text": "automatic translation evaluation", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.6598605016867319}]}, {"text": "Automatic evaluation is more reliable than human evaluation because of its consistency for the same translations.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7327706515789032}]}, {"text": "BLEU () is one of the methods for automatic evaluation of translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9775224328041077}, {"text": "translation", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.8306066989898682}]}, {"text": "It uses the ratio of co-occurring n-grams between a translation and single or multiple reference sentences.", "labels": [], "entities": []}, {"text": "High correlation is reported between the BLEU score and human evaluations for translations from Arabic, Chinese, French, and Spanish to English (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9699433743953705}]}, {"text": "This paper investigates how to apply BLEU to the evaluation of English-to-Japanese translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9940699934959412}, {"text": "English-to-Japanese translation", "start_pos": 63, "end_pos": 94, "type": "TASK", "confidence": 0.6520594954490662}]}, {"text": "The main goal of this paper is to design a reliable method of evaluation for translations from another language to Japanese (henceforth we call this Japanese translation evaluation).", "labels": [], "entities": [{"text": "Japanese translation evaluation", "start_pos": 149, "end_pos": 180, "type": "TASK", "confidence": 0.6093404392401377}]}, {"text": "There are some difficulties in adjusting BLEU for Japanese: BLEU uses n-grams of words, so words in a sentence are assumed to be separated by spaces, while Japanese does not use spaces between words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9941920638084412}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9952704310417175}]}, {"text": "Moreover, Japanese has more variation in writing styles than English.", "labels": [], "entities": []}, {"text": "A major difference in these languages is that Japanese has polite forms expressed by inflections or auxiliary verbs.", "labels": [], "entities": []}, {"text": "If the style of the translations is not the same as that of the reference sentences, the evaluation score becomes low even though the translations are accurate in their meanings and grammar.", "labels": [], "entities": []}, {"text": "To solve these problems, we apply paraphrasing rules to the reference sentences so that the differences in writing styles do not affect the evaluation score.", "labels": [], "entities": []}, {"text": "Another goal is derived from this application of paraphrasing: to define a \"good paraphrase\".", "labels": [], "entities": []}, {"text": "Here paraphrasing means rewriting sentences without changing their semantics.", "labels": [], "entities": []}, {"text": "Several methods of paraphrasing have been studied.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.9764326214790344}]}, {"text": "Some of them aim at the preprocessing of machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7196893990039825}]}, {"text": "They use paraphrasing to transform the input sentences so that the language-transferring routines can handle them easily.", "labels": [], "entities": []}, {"text": "Another application of paraphrasing is to canonicalize many expressions that have the same semantics, supporting information retrieval or question answering).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7372584939002991}, {"text": "question answering", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7821782231330872}]}, {"text": "Paraphrasing techniques in these studies are considered to be useful, but they are difficult to evaluate.", "labels": [], "entities": []}, {"text": "Machine translation evaluation requires methods to judge whether two sentences have the same meaning even when they are syntactically different.", "labels": [], "entities": [{"text": "Machine translation evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8793151179949442}]}, {"text": "Therefore if a set of paraphrasing rules contributes to more reliable translation evaluation, it can be said to be \"good\" paraphrasing.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9371049702167511}]}, {"text": "Thus the study in this paper also presents anew paradigm for evaluating paraphrases.", "labels": [], "entities": []}, {"text": "Section 2 overviews the BLEU metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9867135286331177}]}, {"text": "Section 3 presents the proposed method of Japanese translation evaluation, and its performance is evaluated in Section 4.", "labels": [], "entities": [{"text": "Japanese translation evaluation", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.8116946220397949}]}, {"text": "Based on the experimental results, Section 5 discusses qualitative and quantitative features of paraphrasing.", "labels": [], "entities": []}], "datasetContent": [{"text": "The contribution of the paraphrasing was measured by the increase of reliability of the translation evaluation, as described in Section 4.2.", "labels": [], "entities": [{"text": "reliability", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9939411878585815}, {"text": "translation evaluation", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.9068402051925659}]}, {"text": "In the same way, the effect of each single paraphrasing rule can be also evaluated quantitatively.", "labels": [], "entities": []}, {"text": "shows the three best paraphrasing rules which contributed to the translation evaluation.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.9285507500171661}]}, {"text": "Here the contribution of a rule to the automatic evaluation is measured by the increase of correlation with the human evaluation when the rule is used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The three best paraphrasing rules which  contributed to the translation evaluation. The col- umn '\u2206correl' means the decrease of the correla- tion in the translation evaluation when the rule is re- moved. '(verb-v)' denotes a vowel verb.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9117010235786438}]}]}