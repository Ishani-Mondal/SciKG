{"title": [], "abstractContent": [{"text": "The production of accurate and complete multiple-document summaries is challenged by the complexity of judging the usefulness of information to the user.", "labels": [], "entities": []}, {"text": "Our aim is to determine whether identifying sub-events in a news topic could help us capture essential information to produce better summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 133, "end_pos": 142, "type": "TASK", "confidence": 0.9624230265617371}]}, {"text": "In our first experiment, we asked human judges to determine the relative utility of sentences as they related to the sub-events of a larger topic.", "labels": [], "entities": []}, {"text": "We used this data to create summaries by three different methods, and we then compared these summaries with three automatically created summaries.", "labels": [], "entities": []}, {"text": "In our second experiment, we show how the results of our first experiment can be applied to a cluster-based automatic summarization system.", "labels": [], "entities": []}, {"text": "Through both experiments, we examine the use of inter-judge agreement and a relative utility metric that accounts for the complexity of determining sentence quality in relation to a topic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiple articles on a particular topic tend to contain redundant information as well as information that is unique to each article.", "labels": [], "entities": []}, {"text": "For instance, different news sources covering the same topic may take different angles, or new information may become available in a later report.", "labels": [], "entities": []}, {"text": "So, while all the articles are related to the larger topic, each article maybe associated with any of several sub-events.", "labels": [], "entities": []}, {"text": "We wanted to find away to capture the unique sub-event information that is characteristic in multiple-document coverage of a single topic.", "labels": [], "entities": []}, {"text": "We predicted that breaking documents down to their sub-events and capturing those sentences in each sub-event with the highest utility would produce an accurate, thorough, and diverse multidocument summary.", "labels": [], "entities": []}, {"text": "In our first experiment, we compared six methods of summarization to see which produces the best summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9884366989135742}]}, {"text": "The methods included three automatic and three manual methods of producing summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 75, "end_pos": 84, "type": "TASK", "confidence": 0.8886469006538391}]}, {"text": "We used relative utility to capture and measure subtleties in determining sentence relevance.", "labels": [], "entities": []}, {"text": "We created multiple document summaries using both a sub-event based approach and a topic-based approach.", "labels": [], "entities": []}, {"text": "Generally, we expected to find that the manual summaries performed better than the automatic summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.9381855726242065}]}, {"text": "In our second experiment, we designed a multi-document summarizer which relied on a clustering method, and we tested the three policies we devised for creating summaries from the manual summarization technique developed in our first experiment.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experiment involved having human judges analyze the sentences in our corpus for degree of saliency to a series of sub-events comprising the topic.", "labels": [], "entities": []}, {"text": "Some of our results met our expectations, while others surprised us (see).", "labels": [], "entities": []}, {"text": "The Sum of All Scores manual algorithm produces the best summaries at the twenty percent compression rate.", "labels": [], "entities": [{"text": "Sum of", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.8958134353160858}, {"text": "compression", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.8505483269691467}]}, {"text": "At the ten percent compression rate, data shows Lead-based summaries performing best, with the Sum of All Scores algorithm coming in right behind.", "labels": [], "entities": [{"text": "compression", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9234356880187988}, {"text": "Sum of All Scores algorithm", "start_pos": 95, "end_pos": 122, "type": "METRIC", "confidence": 0.740359616279602}]}, {"text": "Mead scores in the mid-range as expected, for both compression rates, just behind the Round Robin Algorithm.", "labels": [], "entities": [{"text": "compression", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9796145558357239}, {"text": "Round Robin Algorithm", "start_pos": 86, "end_pos": 107, "type": "DATASET", "confidence": 0.7766362627347311}]}, {"text": "In contrast, the random method leads in low scores, with the Highest Score Anywhere algorithm coming in only slightly higher.", "labels": [], "entities": [{"text": "Highest Score Anywhere", "start_pos": 61, "end_pos": 83, "type": "METRIC", "confidence": 0.815717856089274}]}, {"text": "Random sets the lower bound.", "labels": [], "entities": []}, {"text": "Here, we discuss the details of our findings and their significance in more detail.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. First ten sentences of article 30, shown with scores given by three judges for three sub-events. Judges often disagree  on the degree of sentence relevancy. Some sentences are used in more than one sub-event.", "labels": [], "entities": []}, {"text": " Table 3. Results: Best performing algorithm at each cluster/compression rate shown in bold.", "labels": [], "entities": []}, {"text": " Table 4: Results from our automatic, cluster-based summarizer", "labels": [], "entities": [{"text": "summarizer", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.6080043911933899}]}, {"text": " Table 5: Top 10 parameters for the both rates of summarization", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9821173548698425}]}]}