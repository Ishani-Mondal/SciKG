{"title": [{"text": "Learning a Perceptron-Based Named Entity Chunker via Online Recognition Feedback", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "We present a novel approach for the problem of Named Entity Recognition and Classification (NERC), in the context of the CoNLL-2003 Shared Task.", "labels": [], "entities": [{"text": "Named Entity Recognition and Classification (NERC)", "start_pos": 47, "end_pos": 97, "type": "TASK", "confidence": 0.806157797574997}]}, {"text": "Our work is framed into the learning and inference paradigm for recognizing structures in Natural Language).", "labels": [], "entities": []}, {"text": "We make use of several learned functions which, applied at local contexts, discriminatively select optimal partial structures.", "labels": [], "entities": []}, {"text": "On the top of this local recognition, an inference layer explores the partial structures and builds the optimal global structure for the problem.", "labels": [], "entities": []}, {"text": "For the NERC problem, the structures to be recognized are the named entity phrases (NE) of a sentence.", "labels": [], "entities": [{"text": "NERC", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.9541032910346985}]}, {"text": "First, we apply learning at word level to identify NE candidates by means of a Begin-Inside classification.", "labels": [], "entities": []}, {"text": "Then, we make use of functions learned at phrase level -one for each NE category-to discriminate among competing NEs.", "labels": [], "entities": []}, {"text": "We propose a simple online learning algorithm for training all the involved functions together.", "labels": [], "entities": []}, {"text": "Each function is modeled as a voted perceptron).", "labels": [], "entities": []}, {"text": "The learning strategy works online at sentence level.", "labels": [], "entities": []}, {"text": "When visiting a sentence, the functions being learned are first used to recognize the NE phrases, and then updated according to the correctness of their solution.", "labels": [], "entities": []}, {"text": "We analyze the dependencies among the involved perceptrons and a global solution in order to design a global update rule based on the recognition of namedentities, which reflects to each individual perceptron its committed errors from a global perspective.", "labels": [], "entities": []}, {"text": "The learning approach presented here is closely related to -and inspired by-some recent works in the area of NLP and Machine Learning.", "labels": [], "entities": []}, {"text": "adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback.", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9227377772331238}]}, {"text": "presented an online topic-ranking algorithm involving several perceptrons and ranking-based update rules for training them.", "labels": [], "entities": []}], "datasetContent": [{"text": "A list of functional words was automatically extracted from each language training set, selecting those lowercased words within NEs appearing 3 times or more.", "labels": [], "entities": []}, {"text": "For each language, we also constructed a gazetteer with the NEs in the training set.", "labels": [], "entities": []}, {"text": "When training, only a random 40% of the entries was considered.", "labels": [], "entities": []}, {"text": "We performed parameter tuning on the English language.", "labels": [], "entities": []}, {"text": "Concerning the features, we set the window sizes (L wand L p ) to 3 (we tested 2 and 3) , and we did not considered features occurring less than 5 times in the data.", "labels": [], "entities": []}, {"text": "When moving to German, we found better to work with lemmas instead of word forms.", "labels": [], "entities": []}, {"text": "Concerning the learning algorithm, we evaluated kernel degrees from 1 to 5.", "labels": [], "entities": []}, {"text": "Degrees 2 and 3 performed somewhat better than others, and we chose degree 2.", "labels": [], "entities": []}, {"text": "We then ran the algorithm through the English training set for up to five epochs, and through the German training set for up to 3 epochs.", "labels": [], "entities": [{"text": "English training set", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.8838828404744467}, {"text": "German training set", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.7857123215993246}]}, {"text": "On both languages, the performance was still slightly increasing while visiting more training sentences.", "labels": [], "entities": []}, {"text": "Unfortunately, we were notable to run the algorithm until performance was stable.", "labels": [], "entities": []}, {"text": "summarizes the obtained results on all sets.", "labels": [], "entities": []}, {"text": "Clearly, the NERC task on English is much easier than on German.", "labels": [], "entities": [{"text": "NERC task", "start_pos": 13, "end_pos": 22, "type": "TASK", "confidence": 0.8453044891357422}]}, {"text": "Figures indicate that the moderate performance on German is mainly caused by the low recall, specially for ORG and MISC entities.", "labels": [], "entities": [{"text": "German", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.94553542137146}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9994237422943115}, {"text": "ORG", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.7798237800598145}, {"text": "MISC", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.6191123127937317}]}, {"text": "It is interesting to note that while in English the performance is much better on the development set, in German we achieve better results on the test set.", "labels": [], "entities": []}, {"text": "This seems to indicate that the difference in performance between development and test sets is due to irregularities in the NEs that appear in each set, rather than overfitting problems of our learning strategy.", "labels": [], "entities": []}, {"text": "The general performance of phrase recognition system we present is fairly good, and we think it is competitive with state-of-the-art named entity extraction systems.", "labels": [], "entities": [{"text": "phrase recognition", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8719914853572845}, {"text": "entity extraction", "start_pos": 139, "end_pos": 156, "type": "TASK", "confidence": 0.7565472722053528}]}], "tableCaptions": []}