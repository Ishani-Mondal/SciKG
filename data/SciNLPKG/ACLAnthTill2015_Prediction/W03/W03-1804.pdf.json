{"title": [{"text": "Using Masks, Suffix Array-based Data Structures and Multidimensional Arrays to Compute Positional Ngram Statistics from Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an implementation to compute positional ngram statistics (i.e. Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays.", "labels": [], "entities": []}, {"text": "Positional ngrams are ordered sequences of words that represent continuous or discontinuous substrings of a corpus.", "labels": [], "entities": []}, {"text": "In particular, the positional ngram model has shown successful results for the extraction of discontinuous collocations from large corpora.", "labels": [], "entities": []}, {"text": "However, its computation is heavy.", "labels": [], "entities": []}, {"text": "For instance, 4.299.742 positional ngrams (n=1..7) can be generated from a 100.000-word size corpus in a seven-word size window context.", "labels": [], "entities": []}, {"text": "In comparison, only 700.000 ngrams would be computed for the classical ngram model.", "labels": [], "entities": []}, {"text": "It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space.", "labels": [], "entities": []}, {"text": "Our solution shows O(h(F) N log N) time complexity where N is the corpus size and h(F) a function of the window context.", "labels": [], "entities": [{"text": "O", "start_pos": 19, "end_pos": 20, "type": "METRIC", "confidence": 0.9876492619514465}]}], "introductionContent": [{"text": "Many models have been proposed to evaluate word dependencies.", "labels": [], "entities": []}, {"text": "One of the most successful statistical models is certainly the ngram model).", "labels": [], "entities": []}, {"text": "However, in order to overcome its conceptual rigidity, T. have defined the polygram model that estimates the probability of an ngram by interpolating the relative frequencies of all its kgrams (k \u2264 n).", "labels": [], "entities": []}, {"text": "Another way to account for variable length dependencies is the n-multigram model designed by.", "labels": [], "entities": []}, {"text": "All these models have in common the fact that they need to compute continuous string frequencies.", "labels": [], "entities": []}, {"text": "This task can be colossal when gigabytes of data need to be processed.", "labels": [], "entities": []}, {"text": "Indeed, show that there exist N(N+1)/2 substrings in a N-size corpus.", "labels": [], "entities": []}, {"text": "That is the reason why low order ngrams have been commonly used in Natural Language Processing applications.", "labels": [], "entities": []}, {"text": "In the specific field of multiword unit extraction, has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora.", "labels": [], "entities": [{"text": "multiword unit extraction", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7544552683830261}]}, {"text": "Unlikely previous models, positional ngrams are ordered sequences of tokens that represent continuous or discontinuous substrings of a corpus computed in a (2.F+1)-word size window context (F represents the context in terms of words on the right and on the left of any word in the corpus).", "labels": [], "entities": []}, {"text": "As a consequence, the number of generated substrings rapidly explodes and reaches astronomic figures.", "labels": [], "entities": []}, {"text": "shows that \u2206 (Equation 1) positional ngrams can be computed for an N-size corpus in a (2.F+1)-size window context.", "labels": [], "entities": []}, {"text": "In order to illustrate this equation, 4.299.742 positional ngrams (n=1..7) would be generated from a 100.000-word size corpus in a seven-word size window context.", "labels": [], "entities": []}, {"text": "In comparison, only 700.000 ngrams would be computed for the classical ngram model.", "labels": [], "entities": []}, {"text": "It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space.", "labels": [], "entities": []}, {"text": "In this paper, we describe an implementation that computes the Frequency and the Mutual Expectation () of any positional ngram with time complexity O(h(F) N log N).", "labels": [], "entities": [{"text": "Frequency", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9972080588340759}, {"text": "Mutual Expectation", "start_pos": 81, "end_pos": 99, "type": "METRIC", "confidence": 0.9003917276859283}, {"text": "time complexity O", "start_pos": 132, "end_pos": 149, "type": "METRIC", "confidence": 0.7493524154027303}]}, {"text": "The global architecture is based on the definition of masks that allow virtually representing any positional ngram in the corpus.", "labels": [], "entities": []}, {"text": "Thus, we follow the Virtual Corpus approach introduced by and apply a suffix-array-like method, coupled to the Multikey Quicksort algorithm), to compute positional ngram frequencies.", "labels": [], "entities": []}, {"text": "Finally, a multidimensional array is built to easily process the Mutual Expectation, an association measure for collocation extraction.", "labels": [], "entities": [{"text": "collocation extraction", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.7935529947280884}]}, {"text": "The evaluation of our C++ implementation has been realized over the CETEMP\u00fablico 2 corpus and shows satisfactory results.", "labels": [], "entities": [{"text": "CETEMP\u00fablico 2 corpus", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.9599197308222452}]}, {"text": "For example, it takes 8.59 minutes to compute both frequency and Mutual Expectation fora 1.092.723 3 -word corpus on an Intel Pentium III 900 MHz Personal Computer fora seven-word size window context.", "labels": [], "entities": [{"text": "frequency", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9744666218757629}, {"text": "Mutual Expectation", "start_pos": 65, "end_pos": 83, "type": "METRIC", "confidence": 0.8868808150291443}]}, {"text": "This article is divided into four sections: (1) we explain the basic principles of positional ngrams and the mask representation to build the Virtual Corpus; (2) we present the suffix-array-based data structure that allows counting occurrences of positional ngrams; (3) we show how a multidimensional array eases the efficient computation of the Mutual Expectation; (4) we present results over different size sub-corpora of the CETEMP\u00fablico corpus.", "labels": [], "entities": [{"text": "CETEMP\u00fablico corpus", "start_pos": 428, "end_pos": 447, "type": "DATASET", "confidence": 0.9791290462017059}]}], "datasetContent": [{"text": "We have conducted a number of experiments of our C++ implementation on the CETEMP\u00fablico Portuguese corpus to derive positional ngram statistics (Frequency and Mutual Expectation).", "labels": [], "entities": [{"text": "CETEMP\u00fablico Portuguese corpus", "start_pos": 75, "end_pos": 105, "type": "DATASET", "confidence": 0.9774744311968485}]}, {"text": "The experiments have been realized on an Intel Pentium 900 MHz PC with 390MB of RAM.", "labels": [], "entities": []}, {"text": "From the original corpus, we have randomly defined 5 different size sub-corpora that we present in  For each sub-corpus we have calculated the execution time of different stages of the process: (1) the tokenization that transforms the corpus into a set of integers; (2) the preparation of the mask structure and the construction of the suffix-array data structure; (3) the sorting of the suffix-array data structure and the creation of the Matrix; (4) the calculation of the ME.", "labels": [], "entities": [{"text": "ME", "start_pos": 475, "end_pos": 477, "type": "METRIC", "confidence": 0.8794763088226318}]}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "The window context of the experiment is F=3.", "labels": [], "entities": [{"text": "F", "start_pos": 40, "end_pos": 41, "type": "METRIC", "confidence": 0.9914869070053101}]}, {"text": "The results clearly show that the construction of the Matrix and the sort operation over the suffix-array data structure are the most time consuming procedures.", "labels": [], "entities": []}, {"text": "On the contrary, the computation of the Mutual Expectation is quick due to the direct access to sub-ngrams frequencies enabled by the Matrix.", "labels": [], "entities": []}, {"text": "In order to understand the evolution of the results, we present, in, a graphical representation of the results.", "labels": [], "entities": []}, {"text": "The graphical representation illustrates a linear time complexity.", "labels": [], "entities": []}, {"text": "In fact, Alexandre Gil (2002) has proved that, mainly due to the implementation of the Multikey Quicksort algorithm, our implementation evidences a time complexity of O(h(F) N log N) where N is the size of the corpus and h(F) a function of the window context.", "labels": [], "entities": [{"text": "O", "start_pos": 167, "end_pos": 168, "type": "METRIC", "confidence": 0.9836655259132385}]}], "tableCaptions": [{"text": " Table 3: Number of masks", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9562566876411438}]}, {"text": " Table 5: Execution Time in (hh:mm:ss)", "labels": [], "entities": [{"text": "Execution Time", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8697455823421478}]}]}