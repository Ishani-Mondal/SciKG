{"title": [], "abstractContent": [{"text": "We propose a computational model of visually-grounded spatial language understanding , based on a study of how people verbally describe objects in visual scenes.", "labels": [], "entities": [{"text": "visually-grounded spatial language understanding", "start_pos": 36, "end_pos": 84, "type": "TASK", "confidence": 0.7340157479047775}]}, {"text": "We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework.", "labels": [], "entities": []}, {"text": "The implemented system selects the correct referents in response to abroad range of referring expressions fora large percentage of test cases.", "labels": [], "entities": []}, {"text": "In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a study of how people describe objects in visual scenes of the kind shown in.", "labels": [], "entities": []}, {"text": "Based on this study, we propose a computational model of visuallygrounded language understanding.", "labels": [], "entities": [{"text": "visuallygrounded language understanding", "start_pos": 57, "end_pos": 96, "type": "TASK", "confidence": 0.6506139934062958}]}, {"text": "A typical referring expression for Figure 1 might be, \"the far back purple cone that's behind a row of green ones\".", "labels": [], "entities": []}, {"text": "In such tasks, speakers construct expressions to guide listeners' attention to intended objects.", "labels": [], "entities": []}, {"text": "Such referring expressions succeed in communication because speakers and listeners find similar features of the visual scene to be salient, and share an understanding of how language is grounded in terms of these features.", "labels": [], "entities": []}, {"text": "This work is a step towards our longer term goals to develop a conversational robot () that can fluidly connect language to perception and action.", "labels": [], "entities": []}, {"text": "To study the characteristics of descriptive spatial language, we collected several hundred referring expressions based on scenes similar to.", "labels": [], "entities": []}, {"text": "We analysed the descriptions by cataloguing the visual features that they referred to within a scene, and the range of linguistic devices (words or grammatical patterns) that they used to refer to those features.", "labels": [], "entities": []}, {"text": "The combination of a visual feature and corresponding linguistic device is referred to as a descriptive strategy.", "labels": [], "entities": []}, {"text": "We propose a set of computational mechanisms that correspond to the most commonly used descriptive strategies from our study.", "labels": [], "entities": []}, {"text": "The resulting model has been implemented as a set of visual feature extraction algorithms, a lexicon that is grounded in terms of these visual features, a robust parser to capture the syntax of spoken utterances, and a compositional engine driven by the parser that combines visual groundings of lexical units.", "labels": [], "entities": []}, {"text": "We use the term grounded semantic composition to highlight that both the semantics of individual words and the word composition process itself are visually-grounded.", "labels": [], "entities": []}, {"text": "We propose processes that combine the visual models of words, governed by rules of syntax.", "labels": [], "entities": []}, {"text": "In designing our system, we made several simplifying assumptions.", "labels": [], "entities": []}, {"text": "We assumed that word meanings are independent of the visual scene, and that semantic composition is a purely incremental process.", "labels": [], "entities": []}, {"text": "As we will show, neither of these assumptions holds in all of our data, but our system still understands most utterances correctly.", "labels": [], "entities": []}, {"text": "To evaluate the system, we collected a set of spoken utterances from three speakers.", "labels": [], "entities": []}, {"text": "The model was able to correctly understand the visual referents of 59% of the expressions (chance performance was 1/30 30 i=1 1/i = 13%).", "labels": [], "entities": []}, {"text": "The system was able to resolve a range of linguistic phenomena that made use of relatively complex compositions of spatial semantics.", "labels": [], "entities": []}, {"text": "We provide an analysis of the sources of failure in this evaluation, based on which we propose a number of improvements that are required to achieve human level performance.", "labels": [], "entities": []}, {"text": "An extended report on this work can be found in).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}