{"title": [], "abstractContent": [{"text": "The Duluth Word Alignment System participated in the 2003 HLT-NAACL Workshop on Parallel Text shared task on word alignment for both English-French and Romanian-English.", "labels": [], "entities": [{"text": "Duluth Word Alignment", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.5634442667166392}, {"text": "HLT-NAACL Workshop on Parallel Text shared task", "start_pos": 58, "end_pos": 105, "type": "TASK", "confidence": 0.5801177961485726}, {"text": "word alignment", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.6767490953207016}]}, {"text": "It is a Perl implementation of IBM Model 2.", "labels": [], "entities": []}, {"text": "We used approximately 50,000 aligned sentences as training data for each language pair, and found the results for Romanian-English to be somewhat better.", "labels": [], "entities": []}, {"text": "We also varied the Model 2 distortion parameters among the values 2, 4, and 6, but did not observe any significant differences in performance as a result.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word alignment is a crucial part of any Machine Translation system, since it is the process of determining which words in a given source and target language sentence pair are translations of each other.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7469691336154938}, {"text": "Machine Translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.805357813835144}]}, {"text": "This is a token level task, meaning that each word (token) in the source text is aligned with its corresponding translation in the target text.", "labels": [], "entities": []}, {"text": "The Duluth Word Alignment System is a Perl implementation of IBM Model 2 (.", "labels": [], "entities": [{"text": "Duluth Word Alignment", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7496107220649719}]}, {"text": "It learns a probabilistic model from sentence aligned parallel text that can then be used to align the words in another such text (that was not apart of the training process).", "labels": [], "entities": []}, {"text": "A parallel text consists of a source language text and its translation into some target language.", "labels": [], "entities": []}, {"text": "If we have determined which sentences are translations of each other then the text is said to be sentence aligned, where we calla source and target language sentence that are translations of each other a sentence pair.", "labels": [], "entities": []}, {"text": "() introduced five statistical translation models (IBM Models 1 -5).", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.7317827939987183}]}, {"text": "In general a statistical machine translation system is composed of three components: a language model, a translation model, and a decoder ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6296398937702179}]}, {"text": "The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output.", "labels": [], "entities": []}, {"text": "Our focus is on translation models, since that is where word alignment is carried out.", "labels": [], "entities": [{"text": "translation models", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9506425857543945}, {"text": "word alignment", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.7673422992229462}]}, {"text": "The IBM Models start very simply and grow steadily more complex.", "labels": [], "entities": [{"text": "IBM Models", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9430576860904694}]}, {"text": "IBM Model 1 is based solely on the probability that a given word in the source language translates as a particular word in the target language.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9065727194150289}]}, {"text": "Thus, a word in the first position of the source sentence is just as likely to translate to a word in the target sentence that is in the first position versus one at the last position.", "labels": [], "entities": []}, {"text": "IBM Model 2 augments these translation probabilities by taking into account how likely it is for words at particular positions in a sentence pair to be alignments of each other.", "labels": [], "entities": []}, {"text": "This paper continues with a more detailed description of IBM Model 2.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9079450567563375}]}, {"text": "It goes onto present the implementation details of the Duluth Word Alignment System.", "labels": [], "entities": [{"text": "Duluth Word Alignment", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7558594147364298}]}, {"text": "Then we describe the data and the parameters that were used during the training and testing stages of the shared task on word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.785902589559555}]}, {"text": "Finally, we discuss our experimental results and briefly outline our future plans.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Duluth Word Alignment System participated in both the English-French (UMD-EF) and Romanian-English (UMD-RE) portions of the shared task on word alignment.", "labels": [], "entities": [{"text": "Duluth Word Alignment", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.5465142925580343}, {"text": "word alignment", "start_pos": 143, "end_pos": 157, "type": "TASK", "confidence": 0.7653729021549225}]}, {"text": "The UMD-RE models were trained using 49,284 sentence pairs of Romanian-English, which was the complete set of training data as provided by the shared task organizers.", "labels": [], "entities": []}, {"text": "It is made up of three different types of text: the novel 1984, by George Orwell, which contains 6,429 sentence pairs, the Romanian Constitution which contains 967 sentence pairs, and a set of selected newspaper articles collected from the Internet that contain 41,889 sentences pairs.", "labels": [], "entities": [{"text": "Romanian Constitution", "start_pos": 123, "end_pos": 144, "type": "DATASET", "confidence": 0.8462828695774078}]}, {"text": "The gold standard data used in the shared task consists of 248 manually word aligned sentence pairs that were held out of the training process.", "labels": [], "entities": []}, {"text": "The UMD-EF models were trained using a 5% subset of the Aligned Hansards of the 36th Parliament of Canada (Hansards).", "labels": [], "entities": [{"text": "Aligned Hansards of the 36th Parliament of Canada (Hansards)", "start_pos": 56, "end_pos": 116, "type": "DATASET", "confidence": 0.9016678820956837}]}, {"text": "The Hansards contains 1,254,001 sentence pairs, which is well beyond the quantity of data that our current system can train with.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.7414359450340271}]}, {"text": "UMD-EF is trained on a balanced mixture of House and Senate debates and contains 49,393 sentence pairs.", "labels": [], "entities": [{"text": "UMD-EF", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9374872446060181}]}, {"text": "The gold standard data used in the shared task consists of 447 manually word aligned sentence pairs that were held out of the training process.", "labels": [], "entities": []}, {"text": "The UMD-RE and UMD-EF models were trained for thirty iterations.", "labels": [], "entities": []}, {"text": "Three different models for each language pair were trained.", "labels": [], "entities": []}, {"text": "These were based on distortion factors of two, four, and six.", "labels": [], "entities": [{"text": "distortion", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.980004608631134}]}, {"text": "The resulting models will be referred to as UMD-XX-2, UMD-XX-4 and UMD-XX-6, where 2, 4, and 6 are the distortion factor and XX is the language pair (either RE or EF).", "labels": [], "entities": []}, {"text": "The shared task allowed for two different types of alignments, Sure and Probable.", "labels": [], "entities": []}, {"text": "As their names suggest, a sure alignment is one that is judged to be very likely, while a probable is somewhat less certain.", "labels": [], "entities": []}, {"text": "The English-French gold standard data included Sand P alignments, but our system does not make this distinction, and only outputs S alignments.", "labels": [], "entities": [{"text": "English-French gold standard data", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.8601669073104858}]}, {"text": "Submissions to the shared task evaluation were scored using precision, recall, the F-measure and the alignment error rate (AER).", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9996757507324219}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9995585083961487}, {"text": "F-measure", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9977537989616394}, {"text": "alignment error rate (AER)", "start_pos": 101, "end_pos": 127, "type": "METRIC", "confidence": 0.9548888802528381}]}, {"text": "Precision is the number of correct alignments (C) out of the total number of alignments attempted by the system (S), while recall is the number of correct alignments (C) out of the total number of correct alignments (A) as given in the gold standard.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9910298585891724}, {"text": "correct alignments (C)", "start_pos": 27, "end_pos": 49, "type": "METRIC", "confidence": 0.8399210572242737}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.999477207660675}]}, {"text": "That is, The F-measure is the harmonic mean of precision and recall: AER is defined by (Och and Ney, 2000a) and accounts for both Sure and Probable alignments in scoring.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9930583238601685}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9993639588356018}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9981310963630676}, {"text": "AER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9994619488716125}]}, {"text": "The word alignment results attained by our models are shown in.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7338864803314209}]}, {"text": "We score and report our results as nonull, since our system does not include null alignments (source words that don't have a target translation).", "labels": [], "entities": []}, {"text": "We also score relative to sure alignments only.", "labels": [], "entities": []}, {"text": "During the shared task systems were scored with and without null alignments in the gold standard, so our results correspond to those without.", "labels": [], "entities": []}, {"text": "It is apparent from that the precision and recall of the models were not significantly affected by the distortion factor.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9995893836021423}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.999194324016571}]}, {"text": "Also, we note that the precision of the two language pairs is relatively similar.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9992666840553284}]}, {"text": "This may reflect that fact that we used approximately the same amount of training data for each language pair.", "labels": [], "entities": []}, {"text": "However, note that the recall for English-French is much lower.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.999675989151001}]}, {"text": "We continue to investigate why this might be the case, but believe it maybe due to the fact that the training data we randomly selected for the Hansards may not have been representative of the gold standard data.", "labels": [], "entities": [{"text": "Hansards", "start_pos": 144, "end_pos": 152, "type": "DATASET", "confidence": 0.9282561540603638}, {"text": "gold standard data", "start_pos": 193, "end_pos": 211, "type": "DATASET", "confidence": 0.7036960025628408}]}, {"text": "Finally, the alignment error rate (AER) is lower (and hence better) for English-French than RomanianEnglish.", "labels": [], "entities": [{"text": "alignment error rate (AER)", "start_pos": 13, "end_pos": 39, "type": "METRIC", "confidence": 0.8748763899008433}]}, {"text": "However, note that the F-measure for Romanian-English is higher (and therefore better) than English-French.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.999091625213623}]}, {"text": "While this may seem contradictory, AER factors in both Sure and Probable alignments into is scoring while only the English-French data included such alignments in its gold standard.", "labels": [], "entities": [{"text": "AER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9927272796630859}]}, {"text": "The models used for our official submission to the shared task led to somewhat puzzling results, since as the number of iterations increased the precision and recall continued to fall.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9997765421867371}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9996187686920166}]}, {"text": "Upon further investigation, an error was found.", "labels": [], "entities": [{"text": "error", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9754276871681213}]}, {"text": "Rather than estimating as shown in Equation 1, our system did the following: The results shown in are based on a corrected version of the model.", "labels": [], "entities": []}, {"text": "Thereafter as the number of iterations increased the accuracy of the results rose and then reached a plateau that was near what is reported here.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9995866417884827}]}, {"text": "results are actually slightly better with respect to the Fmeasure and AER than our newer results.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9587141871452332}, {"text": "AER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9970768690109253}]}], "tableCaptions": [{"text": " Table 1: No-null Alignment Results", "labels": [], "entities": [{"text": "No-null Alignment", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.48834727704524994}]}, {"text": " Table 2: No-null Alignment Results (original)", "labels": [], "entities": [{"text": "No-null Alignment", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.44468531012535095}]}]}