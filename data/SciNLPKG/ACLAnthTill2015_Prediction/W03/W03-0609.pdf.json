{"title": [{"text": "Grounding Word Meanings in Sensor Data: Dealing with Referential Uncertainty", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider the problem of how the meanings of words can be grounded in sensor data.", "labels": [], "entities": []}, {"text": "A probabilistic representation for the meanings of words is defined, a method for recovering meanings from observational information about word use in the face of referential uncertainty is described, and empirical results with real utterances and robot sensor data are presented .", "labels": [], "entities": []}], "introductionContent": [{"text": "We are interested in how robots might learn language given qualitatively the same inputs available to childrennatural language utterances paired with sensory access to the environment.", "labels": [], "entities": []}, {"text": "This paper focuses on the sub-problem of learning word meanings.", "labels": [], "entities": [{"text": "learning word meanings", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7809623678525289}]}, {"text": "Suppose a robot has acquired a set of sound patterns that mayor may not correspond to words.", "labels": [], "entities": []}, {"text": "How is it possible to separate the words from the non-words, and to learn the meanings of the words?", "labels": [], "entities": []}, {"text": "We assume the robot's sensory access to its environment is through a collection of primitive sensors organized into sensor groups, where each sensor group is a set of related sensors.", "labels": [], "entities": []}, {"text": "For example, the sensor group \u00a2 \u00a1 might return a single value representing the mean grayscale intensity of a set of pixels corresponding to an object in the visual field.", "labels": [], "entities": []}, {"text": "The sensor group \u00a4 \u00a3 \u00a6 \u00a5 might return two values representing the height and width of the bounding box around the object.", "labels": [], "entities": []}, {"text": "Learning the meanings of words requires a representation for meaning.", "labels": [], "entities": []}, {"text": "We use a representation that we calla conditional probability field (CPF), which is a type of scalar field.", "labels": [], "entities": []}, {"text": "A scalar field is a map of the following form: \u00a7 \u00a9 \u00a8 \u00a6 The mapping assigns to each vector a scalar value \u00a7 ! # \" . A conditional probability field assigns to each , which corresponds to a point in an $ -dimensional sensor group, a conditional probability of the form", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the results of experiments in which word meanings are grounded in the sensor data of a mobile robot.", "labels": [], "entities": []}, {"text": "The domain of discourse was a set of blocks.", "labels": [], "entities": []}, {"text": "There were 32 individual blocks with one block for each possible combination of two sizes (small and large), four colors (red, blue, green and yellow) and four shapes (cone, cube, sphere and rectangle).", "labels": [], "entities": []}, {"text": "To generate sensor data for the robot, one set of human subjects played with the blocks, repeatedly selecting a subset of the blocks and placing them in some configuration in the robot's visual field.", "labels": [], "entities": []}, {"text": "The only restrictions placed on this activity were that there could be no more than three blocks visible atone time, two blocks of the same color could not touch, and occlusion from the perspective of the robot was not allowed.", "labels": [], "entities": []}, {"text": "Given a configuration of blocks, the robot generated a digital image of the configuration using a color CCD camera and identified objects in the image as contiguous regions of uniform color.", "labels": [], "entities": []}, {"text": "Given a set of objects, i.e. a set of regions of uniform color in the robot's visual field, virtual sensor groups implemented in software extracted the following information about each object: \u00a2 returned a vector of three numbers that represented the shape of the object ().", "labels": [], "entities": []}, {"text": "In addition, the \u00a2 sensor group returned the proximal orientation, center of mass orientation and distance for the pair of objects as described in.", "labels": [], "entities": [{"text": "distance", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9676614999771118}]}, {"text": "These sensor groups constitute the entirety of the robot's sensorimotor experience of the configurations of blocks created by the human subjects.", "labels": [], "entities": []}, {"text": "From the 120 block configurations created by the four subjects, a random sample of 50 of configurations was shown to a different set of subjects who were asked to generate natural language utterances describing what they saw.", "labels": [], "entities": []}, {"text": "The only restriction placed on the utterances was that they had to be truthful statements about the scenes.", "labels": [], "entities": []}, {"text": "Recurring patterns were discovered in the audio waveforms corresponding to the utterances) and these patterns were used as candidate words.", "labels": [], "entities": []}, {"text": "Recall that a sensor group is semantically associated with a word when the mutual information between occurrences of the word and values in the sensor groups are statistically significant.", "labels": [], "entities": []}, {"text": "shows the % values for the mutual information fora number of combinations of words and sensor groups.", "labels": [], "entities": []}, {"text": "Note from the first column that it is clear that the meaning of the word \"red\" is grounded in the \u00a3 \u00a9 sensor group.", "labels": [], "entities": [{"text": "\u00a3 \u00a9 sensor group", "start_pos": 98, "end_pos": 114, "type": "DATASET", "confidence": 0.8636756241321564}]}, {"text": "It is the only one with a statistically significant mutual information value.", "labels": [], "entities": []}, {"text": "As the second column indicates, the mutual information between the word \"small\" and the \u00a1 sensor group is significant at the 0.05 level, sensor group overestimates the area of non-rectangular objects because it returns the height and width of a bounding box around an object.", "labels": [], "entities": []}, {"text": "Finally, note from the third column that the denotation of the word \"above\" is correctly determined to lie in the sensor group, yet there appears to be some relationship between this word and the \u00a2 \u00a3 \u00a6 \u00a5 sensor group.", "labels": [], "entities": []}, {"text": "The reason for this is that objects that are said to be \"above\" tend to be much higher in the robot's visual field than all of the other objects.", "labels": [], "entities": []}, {"text": "How is it possible to determine the extent to which a machine has discovered and represented the semantics of a set of words?", "labels": [], "entities": []}, {"text": "We are trying to capture semantic distinctions made by humans in natural language communication, so it makes sense to ask a human how successful the system has been.", "labels": [], "entities": []}, {"text": "This was accomplished as follows.", "labels": [], "entities": []}, {"text": "For each word for which a semantic association was discovered, each of the training utterances that used the word were identified.", "labels": [], "entities": []}, {"text": "For the scene associated with each utterance, the CPF underlying the word was used to identify the most probable referent of the word.", "labels": [], "entities": []}, {"text": "For example, if the word in question was \"red\", then the mean HSI values of all objects in the scene would be computed and the object for which the underlying CPF defined over HSI space yielded the highest probability would be deemed to be the referent of that word in that scene.", "labels": [], "entities": []}, {"text": "A human subject was then asked if it made sense for that word to refer to that object in that scene.", "labels": [], "entities": []}, {"text": "The percentage of content words (i.e. words like \"red\" and \"large\" as opposed to \"oh\" and \"there\") for which a semantic association was discovered was \u00a1 \u00a3 \u00a2 \u00a5 \u00a4\u00a2 \u00a2 . Given a semantic association, the two ways that it can be in error are as follows: either the wrong sensor group is selected or the conditional probability field defined over that sensor group is wrong.", "labels": [], "entities": []}, {"text": "Given all of the configurations for which a particular word was used, the semantic accuracy is the percentage of configurations that the meaning component of the word selects an aspect of the configuration that a native speaker of the language says is appropriate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9257103204727173}]}, {"text": "The semantic accuracy was \u00a1 \u00a2 \u00a8 \u00a6 \u00a4 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9772945642471313}]}], "tableCaptions": [{"text": " Table 1: For each sensor group and several words, the  cells of the table show the probability of making an error  in rejecting the null hypothesis that occurrences of the  word and values in the sensor group are independent.", "labels": [], "entities": []}]}