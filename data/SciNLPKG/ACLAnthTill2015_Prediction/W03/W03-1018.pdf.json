{"title": [{"text": "Evaluation and Extension of Maximum Entropy Models with Inequality Constraints", "labels": [], "entities": []}], "abstractContent": [{"text": "A maximum entropy (ME) model is usually estimated so that it conforms to equality constraints on feature expectations.", "labels": [], "entities": []}, {"text": "However, the equality constraint is inappropriate for sparse and therefore unreliable features.", "labels": [], "entities": []}, {"text": "This study explores an ME model with box-type inequality constraints , where the equality can be violated to reflect this unreliability.", "labels": [], "entities": []}, {"text": "We evaluate the inequality ME model using text categorization datasets.", "labels": [], "entities": []}, {"text": "We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the advantage of the inequality models and the proposed extension.", "labels": [], "entities": []}], "introductionContent": [{"text": "The maximum entropy model) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks).", "labels": [], "entities": []}, {"text": "In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: for each feature.", "labels": [], "entities": [{"text": "ME estimation", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.6105246543884277}]}, {"text": "E \u02dc p [f i ] represents the expectation of feature f i in the training data (empirical expectation), and E p [f i ] is the expectation with respect to the model being estimated.", "labels": [], "entities": []}, {"text": "A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform model avoids overfitting the training data.", "labels": [], "entities": []}, {"text": "In spite of these advantages, the ME model still suffers from alack of data as long as it imposes the equality constraint (1), since the empirical expectation calculated from the training data of limited size is inevitably unreliable.", "labels": [], "entities": [{"text": "ME", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.822087824344635}, {"text": "equality constraint", "start_pos": 102, "end_pos": 121, "type": "METRIC", "confidence": 0.9567346274852753}]}, {"text": "A careful treatment is required especially in NLP applications since the features are usually very sparse.", "labels": [], "entities": []}, {"text": "In this study, text categorization is used as an example of such tasks with sparse features.", "labels": [], "entities": []}, {"text": "Previous work on NLP proposed several solutions for this unreliability such as the cut-off, which simply omits rare features, the MAP estimation with the Gaussian prior), the fuzzy maximum entropy model, and fat constraints.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 130, "end_pos": 144, "type": "METRIC", "confidence": 0.7281058728694916}]}, {"text": "Currently, the Gaussian MAP estimation (combined with the cut-off) seems to be the most promising method from the empirical results.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.5946111977100372}]}, {"text": "It succeeded in language modeling) and text categorization).", "labels": [], "entities": [{"text": "language modeling", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7638940513134003}]}, {"text": "As described later, it relaxes constraints like , where \u03bb i is the model's parameter.", "labels": [], "entities": []}, {"text": "This study follows this line, but explores the following box-type inequality constraints: Here, the equality can be violated by the widths A i and Bi . We refer to the ME model with the above inequality constraints as the inequality ME model.", "labels": [], "entities": []}, {"text": "This inequality constraint falls into a type of fat constraints, a i \u2264 E p [f i ] \u2264 bi , as suggested by.", "labels": [], "entities": []}, {"text": "However, as noted in), this type of constraint has not yet been applied nor evaluated for NLPs.", "labels": [], "entities": []}, {"text": "The inequality ME model differs from the Gaussian MAP estimation in that its solution becomes sparse (i.e., many parameters become zero) as a result of optimization with inequality constraints.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.8474093675613403}]}, {"text": "The features with a zero parameter can be removed from the model without changing its prediction behavior.", "labels": [], "entities": []}, {"text": "Therefore, we can consider that the inequality ME model embeds feature selection in its estimation.", "labels": [], "entities": []}, {"text": "Recently, the sparseness of the solution has been recognized as an important concept in constructing robust classifiers such as SVMs.", "labels": [], "entities": []}, {"text": "We believe that the sparse solution improves the robustness of the ME model as well.", "labels": [], "entities": []}, {"text": "We also extend the inequality ME model so that the constraint widths can move using slack variables.", "labels": [], "entities": []}, {"text": "If we penalize the slack variables by their 2-norm, we obtain a natural integration of the inequality ME model and the Gaussian MAP estimation.", "labels": [], "entities": []}, {"text": "While it incorporates the quadratic stabilization of the parameters as in the Gaussian MAP estimation, the sparseness of the solution is preserved.", "labels": [], "entities": []}, {"text": "We evaluate the inequality ME models empirically, using two text categorization datasets.", "labels": [], "entities": []}, {"text": "The results show that the inequality ME models outperform the cut-off and the Gaussian MAP estimation.", "labels": [], "entities": [{"text": "ME", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.9103442430496216}, {"text": "Gaussian MAP estimation", "start_pos": 78, "end_pos": 101, "type": "METRIC", "confidence": 0.7356895605723063}]}, {"text": "Such high accuracies are achieved with a fairly small number of active features, indicating that the sparse solution can effectively enhance the performance.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9721997380256653}]}, {"text": "In addition, the 2-norm extended model is shown to be more robust in several situations.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the evaluation, we use the \"Reuters-21578, Distribution 1.0\" dataset and the \"OHSUMED\" dataset.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 32, "end_pos": 45, "type": "DATASET", "confidence": 0.9263190031051636}, {"text": "Distribution 1.0\" dataset", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.7494631037116051}, {"text": "OHSUMED\" dataset", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.854708214600881}]}, {"text": "The Reuters dataset developed by David D. Lewis is a collection of labeled newswire articles.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8943552672863007}]}, {"text": "We adopted \"ModApte\" split to split the collection, and we obtained 7, 048 documents for training, and 2, 991 documents for testing.", "labels": [], "entities": []}, {"text": "We used 112 \"TOP-ICS\" that actually occurred in the training set as the target categories.", "labels": [], "entities": []}, {"text": "The OHSUMED dataset) is a collection of clinical paper abstracts from the MED-LINE database.", "labels": [], "entities": [{"text": "OHSUMED dataset)", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9289183815320333}, {"text": "MED-LINE database", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.9098685681819916}]}, {"text": "Each abstract is manually assigned MeSH terms.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.7218309640884399}]}, {"text": "We simplified a MeSH term, like \"A/B/C \ud97b\udf59 \u2192 A\", and used the most frequent 100 simplified terms as the target categories.", "labels": [], "entities": []}, {"text": "We extracted 9, 947 abstracts for training, and 9, 948 abstracts for testing from the file \"ohsumed.91.\"", "labels": [], "entities": [{"text": "ohsumed.91", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.7081930041313171}]}, {"text": "A documents is converted to a bag-of-words vector representation with TFIDF values, after the stop words are removed and all the words are downcased.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.8871968984603882}]}, {"text": "Since the text categorization task requires that multiple categories are assigned if appropriate, we constructed a binary categorizer, p c (y \u2208 {+1, \u22121}|d), for each category c.", "labels": [], "entities": []}, {"text": "If the probability p c (+1|d) is greater than 0.5, the category is assigned.", "labels": [], "entities": []}, {"text": "To construct a conditional maximum entropy model, we used the feature function of the form, where hi (d) returns the TFIDF value of the i-th word of the document vector.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 117, "end_pos": 122, "type": "METRIC", "confidence": 0.9853496551513672}]}, {"text": "We implemented the estimation algorithms as an extension of an ME estimation tool, Amis, 8 using the Toolkit for Advanced Optimization (TAO)), which provides the LMVM and the BLMVM optimization modules.", "labels": [], "entities": [{"text": "BLMVM optimization", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.7116840183734894}]}, {"text": "For the inequality ME estimation, we added a hook that checks the KKT conditions after the normal convergence test.", "labels": [], "entities": [{"text": "ME", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9659969210624695}, {"text": "KKT", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.5141698122024536}]}, {"text": "We compared the following models: \u2022 ME models only with cut-off (cut-off ), \u2022 ME models with cut-off and the Gaussian MAP estimation (gaussian), \u2022 Inequality ME models (ineq), \u2022 Inequality ME models with 2-norm extension described in Section 4 (2-norm), For the inequality ME models, we compared the two methods to determine the widths, single and bayes, as described in Section 5.", "labels": [], "entities": [{"text": "Gaussian MAP estimation", "start_pos": 109, "end_pos": 132, "type": "METRIC", "confidence": 0.7180387874444326}]}, {"text": "Although the Gaussian MAP estimation can use different \u03c3 i for each feature, we used a common variance \u03c3 for gaussian.", "labels": [], "entities": [{"text": "MAP estimation", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8887222409248352}]}, {"text": "Thus, gaussian roughly corresponds to single in the way of dealing with the unreliability of features.", "labels": [], "entities": []}, {"text": "Note that, for inequality models, we started with all possible features and rely on their ability to remove unnecessary features automatically by solution sparseness.", "labels": [], "entities": []}, {"text": "The average maximum number of features in a categorizer is 63, 150.0 for the Reuters dataset and 116, 452.0 for the OHSUMED dataset.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 77, "end_pos": 92, "type": "DATASET", "confidence": 0.9856695532798767}, {"text": "OHSUMED dataset", "start_pos": 116, "end_pos": 131, "type": "DATASET", "confidence": 0.9910144209861755}]}, {"text": "Developed by Yusuke Miyao so as to support various ME estimations such as the efficient estimation with complicated event structures ().", "labels": [], "entities": [{"text": "ME estimations", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.779682844877243}]}, {"text": "Available at http://www-tsujii.is.s.u-tokyo.ac.jp/ \u02dcyusuke/amis The tolerance for the normal convergence test (relative improvement) and the KKT check is 10 \u22124 . We stop the training if the KKT check has been failed many times and the ratio of the bad (upper and lower active) features among the active features is lower than 0.01.", "labels": [], "entities": [{"text": "normal convergence test", "start_pos": 86, "end_pos": 109, "type": "METRIC", "confidence": 0.77285103003184}, {"text": "KKT check", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.599609449505806}]}, {"text": "Here, we fix the penalty constants C1 = C2 = 10 16 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The summary of the experiments.", "labels": [], "entities": []}]}