{"title": [{"text": "Combining Segmenter and Chunker for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.5484126607577006}]}], "abstractContent": [{"text": "Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation.", "labels": [], "entities": [{"text": "Hidden Markov Model-based word segmenter", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.6643568456172944}, {"text": "Chinese word segmentation", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.5800508658091227}]}, {"text": "Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter.", "labels": [], "entities": [{"text": "Hidden Markov Model-based word segmenter", "start_pos": 45, "end_pos": 85, "type": "TASK", "confidence": 0.6474967479705811}]}, {"text": "The word seg-menter produces n-best word candidates together with some class information and confidence measures.", "labels": [], "entities": []}, {"text": "Secondly, the extracted words are broken into character units and each character is annotated with the possible word class and the position in the word, which are then used as the features for the chunker.", "labels": [], "entities": []}, {"text": "Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries.", "labels": [], "entities": []}, {"text": "1 Methods We participate in the closed test for all four sets of data in Chinese Word Segmentation Bakeoff.", "labels": [], "entities": [{"text": "Chinese Word Segmentation Bakeoff", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.6784582510590553}]}, {"text": "Our method is based on the following two steps: 1.", "labels": [], "entities": []}, {"text": "The input sentence is segmented into a word sequence by Hidden Markov Model-based word seg-menter.", "labels": [], "entities": []}, {"text": "The segmenter assigns a word class with a confidence measure for each word at the hidden states.", "labels": [], "entities": []}, {"text": "The model is trained by Baum-Welch algorithm.", "labels": [], "entities": []}, {"text": "2. Each character in the sentence is annotated with the word class tag and the position in the word.", "labels": [], "entities": []}, {"text": "The n-best word candidates derived from the word seg-menter are also extracted as the features.", "labels": [], "entities": []}, {"text": "A support vector machine-based chunker corrects the errors made by the segmenter using the extracted features.", "labels": [], "entities": []}, {"text": "We will describe each of these steps in more details.", "labels": [], "entities": []}, {"text": "1.1 Hidden Markov Model-based Word Segmenter Our word segmenter is based on Hidden Markov Model (HMM).", "labels": [], "entities": [{"text": "Word Segmenter", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.6258696168661118}, {"text": "word segmenter", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.7378770112991333}]}, {"text": "We first decide the number of hidden states (classes) and assume that the each word can belong to all the classes with some probability.", "labels": [], "entities": []}, {"text": "The problem is defined as a search for the sequence of word classes C = c 1 ,.", "labels": [], "entities": []}, {"text": ".. , c n given a word sequence W = w 1 ,.", "labels": [], "entities": []}, {"text": "The target is to find W and C fora given input S that maximizes the following probability: arg max W,C P (W |C)P (C) We assume that the word probability P (W |C) is constrained only by its word class, and that the class probability P (C) is constrained only by the class of the preceding word.", "labels": [], "entities": []}, {"text": "These probabilities are estimated by the Baum-Welch algorithm using the training material (See (Manning and Sch\u00fctze., 1999)).", "labels": [], "entities": []}, {"text": "The learning process is based on the Baum-Welch algorithm and is the same as the well-known use of HMM for part-of-speech tagging problem, except that the number of states are arbitrarily determined and the initial probabilities are randomly assigned in our model.", "labels": [], "entities": [{"text": "part-of-speech tagging problem", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.7932846347490946}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Validation Results for HMM  Data # of classes  Rec.  Prec.  F  AS  5  0.845 0.768 0.804  AS  10  0.900 0.857 0.878  CTB  5  0.909 0.844 0.875  CTB  10  0.912 0.848 0.879  HK  5  0.867 0.742 0.799  HK  10  0.867 0.741 0.799  PK  5  0.942 0.902 0.921  PK  10  0.944 0.905 0.924", "labels": [], "entities": [{"text": "Rec.  Prec.  F  AS  5  0.845 0.768 0.804  AS  10", "start_pos": 57, "end_pos": 105, "type": "DATASET", "confidence": 0.8436722904443741}]}, {"text": " Table 2: Validation Results (CTB) for Chunking  # of classes n-best  Rec.  Prec.  F  5  1  0.957 0.930 0.943  5  2  0.957 0.931 0.944  5  3  0.957 0.930 0.943  5  4  0.957 0.930 0.943  10  1  0.956 0.929 0.943  10  2  0.957 0.928 0.942  10  3  0.956 0.929 0.942  10  4  0.955 0.928 0.941", "labels": [], "entities": [{"text": "Validation Results (CTB)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7738428711891174}, {"text": "Rec.  Prec.  F  5", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.8479887147744497}]}, {"text": " Table 3: Validation Results (HK) for Chunking  # of classes n-best  Rec.  Prec.  F  5  1  0.853 0.793 0.822  5  2  0.859 0.799 0.828  5  3  0.859 0.799 0.828  5  4  0.859 0.800 0.828  10  1  0.856 0.793 0.823  10  2  0.858 0.797 0.826  10  3  0.857 0.796 0.826  10  4  0.858 0.797 0.826", "labels": [], "entities": [{"text": "Validation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9855892062187195}, {"text": "HK", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.8583468198776245}, {"text": "Rec.  Prec.  F  5", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8211886088053385}]}, {"text": " Table 4: Validation Results (PK) for Chunking  # of classes n-best  Rec.  Prec.  F  5  1  0.960 0.934 0.947  5  2  0.961 0.935 0.948  5  3  0.962 0.936 0.949  5  4  0.962 0.935 0.948  10  1  0.961 0.932 0.946  10  2  0.962 0.935 0.948  10  3  0.961 0.934 0.947  10  4  0.961 0.934 0.947", "labels": [], "entities": [{"text": "Validation Results (PK)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8176238656044006}, {"text": "Rec.  Prec.  F  5", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.8408109049002329}]}, {"text": " Table 7: The Models for the Test Material  -with respect to F-Measure in Our Validation Test", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.983607828617096}]}, {"text": " Table 5: Throughput Speeds (characters per second)  Data Word Seg. (# of words) Fea. Ext.  (n-best) Chunker (# of SV) Total Speed  AS  57000  (462750)  7640  (Only Best)  279  (96452)  241  CTB  54400  (77324)  4040  (to 2nd Best)  894  (16736)  671  HK  38900  (93231)  3870  (to 4th Best)  649  (14904)  524  PK  57400  (215865)  6209  (to 3rd Best)  254  (49736)  200", "labels": [], "entities": [{"text": "Fea. Ext", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9021347761154175}, {"text": "Chunker", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9193974137306213}, {"text": "Total Speed  AS", "start_pos": 119, "end_pos": 134, "type": "METRIC", "confidence": 0.8788193265597025}]}, {"text": " Table 6: Results for the Test Materials  Data T. Rec. T. Prec.  F  OOV Rec. IV Rec. Ranking  AS  0.944  0.945  0.945  0.574  0.952  3rd/6  CTB  0.852  0.807  0.829  0.412  0.949  8th/10  HK  0.940  0.908  0.924  0.415  0.980  5th/6  PK  0.933  0.916  0.924  0.357  0.975  2nd/4", "labels": [], "entities": [{"text": "Test Materials  Data T. Rec. T. Prec.  F  OOV Rec. IV Rec. Ranking", "start_pos": 26, "end_pos": 92, "type": "DATASET", "confidence": 0.8666062109610614}, {"text": "AS  0.944  0.945  0.945  0.574  0.952  3rd/6  CTB  0.852  0.807  0.829  0.412  0.949  8th/10  HK  0.940  0.908  0.924  0.415  0.980  5th/6  PK  0.933  0.916  0.924  0.357  0.975  2nd/4", "start_pos": 94, "end_pos": 278, "type": "METRIC", "confidence": 0.7168631189399295}]}]}