{"title": [{"text": "Utterance Segmentation Using Combined Approach Based on Bi-directional N-gram and Maximum Entropy", "labels": [], "entities": [{"text": "Utterance Segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9106155335903168}, {"text": "Approach", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.5885395407676697}]}], "abstractContent": [{"text": "This paper proposes anew approach to segmentation of utterances into sentences using anew linguistic model based upon Maximum-entropy-weighted Bi-directional N-grams.", "labels": [], "entities": [{"text": "segmentation of utterances into sentences", "start_pos": 37, "end_pos": 78, "type": "TASK", "confidence": 0.8801787853240967}]}, {"text": "The usual N-gram algorithm searches for sentence boundaries in a text from left to right only.", "labels": [], "entities": []}, {"text": "Thus a candidate sentence boundary in the text is evaluated mainly with respect to its left context, without fully considering its right context.", "labels": [], "entities": []}, {"text": "Using this approach, utterances are often divided into incomplete sentences or fragments.", "labels": [], "entities": []}, {"text": "In order to make use of both the right and left contexts of candidate sentence boundaries, we propose anew linguistic modeling approach based on Maximum-entropy-weighted Bi-directional N-grams.", "labels": [], "entities": []}, {"text": "Experimental results indicate that the new approach significantly outperforms the usual N-gram algorithm for segmenting both Chinese and English utterances.", "labels": [], "entities": [{"text": "segmenting both Chinese and English utterances", "start_pos": 109, "end_pos": 155, "type": "TASK", "confidence": 0.6911266148090363}]}], "introductionContent": [{"text": "Due to the improvement of speech recognition technology, spoken language user interfaces, spoken dialogue systems, and speech translation systems are no longer only laboratory dreams.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7663317024707794}, {"text": "speech translation", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7734493613243103}]}, {"text": "Roughly speaking, such systems have the structure shown in.", "labels": [], "entities": []}, {"text": "In these systems, the language analysis module takes the output of speech recognition as its input, representing the current utterance exactly as pronounced, without any punctuation symbols marking the boundaries of sentences.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.758355051279068}]}, {"text": "Here is an example: \u8fd9\u8fb9\u8bf7\u60a8\u5750\u7535\u68af\u5230 9 \u697c\u670d\u52a1\u751f\u5c06\u5728\u90a3 \u91cc\u7b49\u60a8\u5e76\u5c06\u60a8\u5e26\u5230 913 \u53f7\u623f\u95f4 . (this way please please take this elevator to the ninth floor the floor attendant will meet you at your elevator entrance there and show you to room 913.)", "labels": [], "entities": []}, {"text": "As the example shows, it will be difficult fora text analysis module to parse the input if the utterance is not segmented.", "labels": [], "entities": []}, {"text": "Further, the output utterance from the speech recognizer usually contains wrongly recognized words or noise words.", "labels": [], "entities": []}, {"text": "Thus it is crucial to segment the utterance before further language processing.", "labels": [], "entities": []}, {"text": "We believe that accurate segmentation can greatly improve the performance of language analysis modules.", "labels": [], "entities": []}, {"text": "Stevenson et al. have demonstrated the difficulties of text segmentation through an experiment in which six people, educated to at least the Bachelor's degree level, were required to segment into sentences broadcast transcripts from which all punctuation symbols had been removed.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7580232918262482}]}, {"text": "The experimental results show that humans do not always agree on the insertion of punctuation symbols, and that their segmentation performance is not very good).", "labels": [], "entities": []}, {"text": "Thus it is a great challenge for computers to perform the task automatically.", "labels": [], "entities": []}, {"text": "To solve this problem, many methods have been proposed, which can be roughly classified into two categories.", "labels": [], "entities": []}, {"text": "One approach is based on simple acoustic criteria, such as nonspeech intervals (e.g. pauses), pitch and energy.", "labels": [], "entities": []}, {"text": "We can call this approach acoustic segmentation.", "labels": [], "entities": [{"text": "acoustic segmentation", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7227237522602081}]}, {"text": "The other approach, which can be called linguistic segmentation, is based on linguistic clues, including lexical knowledge, syntactic structure, semantic information etc.", "labels": [], "entities": [{"text": "linguistic segmentation", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.7327248454093933}]}, {"text": "Acoustic segmentation cannot always work well, because utterance boundaries do not always correspond to acoustic criteria.", "labels": [], "entities": [{"text": "Acoustic segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6580482572317123}]}, {"text": "For example: \u60a8\u597d<pause>\u8bf7\u95ee<pause>\u660e\u5929\u7684\u5355\u4eba\u95f4 \u8fd8\u6709\u5417<pause>\u6216\u8005<pause>\u6807\u51c6\u95f4\u4e5f\u884c.", "labels": [], "entities": []}, {"text": "Since the simple acoustic criteria are inadequate, linguistic clues play an indispensable role in utterance segmentation, and many methods relying on them have been proposed.", "labels": [], "entities": [{"text": "utterance segmentation", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8756446242332458}]}, {"text": "This paper proposes anew approach to linguistic segmentation using a Maximum-entropyweighted Bi-directional N-gram-based algorithm (MEBN).", "labels": [], "entities": [{"text": "linguistic segmentation", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.7436440289020538}]}, {"text": "To evaluate the performance of MEBN, we conducted experiments in both Chinese and English.", "labels": [], "entities": [{"text": "MEBN", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8217970132827759}]}, {"text": "All the results show that MEBN outperforms the normal N-gram algorithm.", "labels": [], "entities": [{"text": "MEBN", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.6445754170417786}]}, {"text": "The remainder of this paper will focus on description of our new approach for linguistic segmentation.", "labels": [], "entities": [{"text": "linguistic segmentation", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7070610374212265}]}, {"text": "In Section 2, some related work on utterance segmentation is briefly reviewed, and our motivations are described.", "labels": [], "entities": [{"text": "utterance segmentation", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.9017089307308197}]}, {"text": "Section 3 describes MEBN in detail.", "labels": [], "entities": [{"text": "MEBN", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.6699458360671997}]}, {"text": "The experimental results are presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, Section 5 gives our conclusion.", "labels": [], "entities": []}, {"text": "proposed an approach to detection of sentence boundaries and disfluency locations in speech transcribed by an automatic recognizer, based on a combination of prosodic cues modeled by decision trees and N-gram language models.", "labels": [], "entities": [{"text": "detection of sentence boundaries and disfluency locations in speech transcribed", "start_pos": 24, "end_pos": 103, "type": "TASK", "confidence": 0.8143330693244935}]}, {"text": "Their N-gram language model is mainly based on part of speech, and retains some words which are particularly relevant to segmentation.", "labels": [], "entities": []}, {"text": "Of course, most part-of-speech taggers require sentence boundaries to be pre-determined; so to require the use of part-of-speech information in utterance segmentation would risk circularity.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7234957367181778}, {"text": "utterance segmentation", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.7302401214838028}]}, {"text": "approach to sentence boundary detection is somewhat similar to Stolcke et al.'s.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.784534732500712}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 gives an overview of the training  corpus.", "labels": [], "entities": []}, {"text": " Table 2. Overview of the Testing Corpus.  We have implemented four segmentation algo- rithms using NN, RN, BN and MEBN respectively.", "labels": [], "entities": [{"text": "Testing Corpus", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8597307801246643}, {"text": "BN", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.9745995998382568}]}, {"text": " Table 4. Experimental Results for English Utter- ance Segmentation.  From the result tables it is clear that RN, BN, and  MEBN all outperforms the normal N-gram algo- rithm in the F-score for both Chinese and English  utterance segmentation. MEBN achieved the best  performance which improves the precision by  7.3% and the recall by 1.5% in the Chinese ex- periment, and improves the precision by 5.4% and  the recall by 1.9% in the English experiment.", "labels": [], "entities": [{"text": "English Utter- ance Segmentation", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.6585932016372681}, {"text": "BN", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9598705172538757}, {"text": "MEBN", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9098663926124573}, {"text": "F-score", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9895575642585754}, {"text": "Chinese and English  utterance segmentation", "start_pos": 198, "end_pos": 241, "type": "TASK", "confidence": 0.6429868459701538}, {"text": "precision", "start_pos": 298, "end_pos": 307, "type": "METRIC", "confidence": 0.9992886781692505}, {"text": "recall", "start_pos": 325, "end_pos": 331, "type": "METRIC", "confidence": 0.9993200302124023}, {"text": "precision", "start_pos": 386, "end_pos": 395, "type": "METRIC", "confidence": 0.9994693398475647}, {"text": "recall", "start_pos": 413, "end_pos": 419, "type": "METRIC", "confidence": 0.9993141889572144}]}, {"text": " Table 5 and Table 6.", "labels": [], "entities": []}, {"text": " Table 5. Chinese Utterance Segmentation Results  Comparison.", "labels": [], "entities": [{"text": "Chinese Utterance Segmentation", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7431878844896952}]}]}