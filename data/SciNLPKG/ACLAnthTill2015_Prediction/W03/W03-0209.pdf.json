{"title": [], "abstractContent": [{"text": "To date, traditional NLP parsers have not been widely successful in TESOL-oriented applications, particularly in scoring written compositions.", "labels": [], "entities": [{"text": "NLP parsers", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.7501586675643921}]}, {"text": "Re-engineering such applications to provide the necessary robustness for handling ungrammat-ical English has proven a formidable obstacle.", "labels": [], "entities": []}, {"text": "We discuss the use of a non-traditional parser for rating compositions that attenuates some of these difficulties.", "labels": [], "entities": []}, {"text": "Its dependency-based shallow parsing approach provides significant robustness in the face of language learners' ungrammat-ical compositions.", "labels": [], "entities": [{"text": "dependency-based shallow parsing", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.55301300684611}]}, {"text": "This paper discusses how a corpus of L2 essays for English was rated using the parser, and how the automatic evaulations compared to those obtained by manual methods.", "labels": [], "entities": []}, {"text": "The types of modifications that were made to the system are discussed.", "labels": [], "entities": []}, {"text": "Limitations to the current system are described, future plans for developing the system are sketched, and further applications beyond English essay rating are mentioned.", "labels": [], "entities": [{"text": "English essay rating", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.36439791321754456}]}], "introductionContent": [{"text": "Rating constructed response items, particularly essays, is a time-consuming effort.", "labels": [], "entities": [{"text": "Rating constructed response items", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8728308379650116}]}, {"text": "This is true in rating essays written by second-language speakers.", "labels": [], "entities": []}, {"text": "To make this process more manageable, researchers have investigated how to involve computers in the grading process.", "labels": [], "entities": []}, {"text": "Several factors suggest why automating scoring might be desirable: (i) practicality: essay grading is costly and time-consuming; (ii) consistency: essay grading is somewhat subjective in nature, and consistency may sometimes suffer; and (iii) feedback: Providing feedback to a student is important, and automated scoring can provide ways of generating specific suggestions tailored to the needs of the author.", "labels": [], "entities": [{"text": "consistency", "start_pos": 134, "end_pos": 145, "type": "METRIC", "confidence": 0.991363525390625}]}, {"text": "However, computerized rating of essays written by second-language speakers poses unique dilemmas, particularly for responses written by examinees at low levels of language proficiency.", "labels": [], "entities": [{"text": "rating of essays written by second-language speakers", "start_pos": 22, "end_pos": 74, "type": "TASK", "confidence": 0.7383829355239868}]}, {"text": "Where we expect generally well-formed sentences from native English speaker responses, we find that the majority of the responses by lower proficiency secondlanguage English speakers will be made up of illformed sentences.", "labels": [], "entities": []}, {"text": "Previous work in automated essay grading and related technologies has been surveyed and discussed in several different forums, and a thorough survey of the field has recently been published.", "labels": [], "entities": []}, {"text": "Typically these approaches have borrowed techniques and tools from several natural language processing (NLP) fields.", "labels": [], "entities": []}, {"text": "For example, the knowledge-based engines have been used for analyzing essays: parsers), grammar and spelling checkers (), discourse processing analyzers), and other hand-crafted linguistic knowledge sources.", "labels": [], "entities": []}, {"text": "On the other hand, much work has leveraged statistical methods in detecting properties of student essays via stylometrics) 1 , latent semantic indexing, and feature analysis.", "labels": [], "entities": [{"text": "latent semantic indexing", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.5850049555301666}, {"text": "feature analysis", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.7369190454483032}]}, {"text": "Finally, mirroring noteworthy progress in other NLP fields involving data-driven methods, recent work has involved essay grading via exemplar-based machine learning techniques).", "labels": [], "entities": []}, {"text": "The most visible systems implement one (or more) of these approaches.", "labels": [], "entities": []}, {"text": "The Project Essay Grade (PEG) system, for example, uses lexicallybased metrics in scoring.", "labels": [], "entities": []}, {"text": "The Intelligent Essay Assessor (IEA) uses latent semantic analysis in calculating its metrics (.", "labels": [], "entities": [{"text": "Intelligent Essay Assessor (IEA", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7225487053394317}]}, {"text": "The E-Rater system by Educational Testing Services uses syntactic, discourse, and topical (i.e. vocabulary-based) data analysis.", "labels": [], "entities": []}, {"text": "Several criticisms have been aimed at automatic scoring systems on both theoretical and implementational grounds.", "labels": [], "entities": []}, {"text": "For example, many systems exhibit an inherent Achilles' heel since it is possible to trick them into evaluating a nonsensical text purely by reverse-engineering the scoring mechanism and designing a text that responds to the criteria.", "labels": [], "entities": []}, {"text": "Another problem is the cost of development, which can be substantial.", "labels": [], "entities": []}, {"text": "In addition, most systems are designed around certain specific topics in order to focus terminology and vocabulary in limited subdomains; introducing new subject areas requires building anew model, often a nontrivial process.", "labels": [], "entities": []}, {"text": "Thus, many systems are often not adaptable enough to meet the particular needs of an individual, class, teacher, or institution.", "labels": [], "entities": []}, {"text": "This work also uses the Link Grammar parser.", "labels": [], "entities": []}, {"text": "The purpose of our research is to explore the use of a particular natural language processing (NLP) approach for automated scoring of essays written by ESL students at lower levels of language proficiency Our goal for the system reflects commonsense (though ambitious) criteria: to have the system's scores agree with those assigned by human raters at least as often as human raters agree among themselves.", "labels": [], "entities": [{"text": "automated scoring of essays written by ESL students", "start_pos": 113, "end_pos": 164, "type": "TASK", "confidence": 0.7731303833425045}]}], "datasetContent": [], "tableCaptions": []}