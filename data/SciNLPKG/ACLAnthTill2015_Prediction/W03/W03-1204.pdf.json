{"title": [{"text": "Evaluation of Features for Sentence Extraction on Different Types of Corpora", "labels": [], "entities": [{"text": "Sentence Extraction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.9476778507232666}]}], "abstractContent": [{"text": "We report evaluation results for our sum-marization system and analyze the resulting summarization data for three different types of corpora.", "labels": [], "entities": []}, {"text": "To develop a robust summarization system, we have created a system based on sentence extraction and applied it to summarize Japanese and English newspaper articles, obtained some of the top results at two evaluation workshops.", "labels": [], "entities": [{"text": "summarization", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9899185299873352}, {"text": "sentence extraction", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7226276099681854}, {"text": "summarize Japanese and English newspaper articles", "start_pos": 114, "end_pos": 163, "type": "TASK", "confidence": 0.9076637228329977}]}, {"text": "We have also created sentence extraction data from Japanese lectures and evaluated our system with these data.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7153110951185226}]}, {"text": "In addition to the evaluation results, we analyze the relationships between key sentences and the features used in sentence extraction.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7362390607595444}]}, {"text": "We find that discrete combinations of features match distributions of key sentences better than sequential combinations .", "labels": [], "entities": []}], "introductionContent": [{"text": "Our ultimate goal is to create a robust summarization system that can handle different types of documents in a uniform way.", "labels": [], "entities": [{"text": "summarization", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9645460844039917}]}, {"text": "To achieve this goal, we have developed a summarization system based on sentence extraction.", "labels": [], "entities": [{"text": "summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9722475409507751}, {"text": "sentence extraction", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7479616105556488}]}, {"text": "We have participated in evaluation workshops on automatic summarization for both Japanese and English written corpora.", "labels": [], "entities": [{"text": "summarization", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.8104518055915833}]}, {"text": "We have also evaluated the performance of the sentence extraction system for Japanese lectures.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7376495003700256}]}, {"text": "At both workshops we obtained some of the top results, and for the speech corpus we obtained results comparable with those for the written corpora.", "labels": [], "entities": []}, {"text": "This means that the features we use are worth analyzing.", "labels": [], "entities": []}, {"text": "Sentence extraction is one of the main methods required fora summarization system to reduce the size of a document.", "labels": [], "entities": [{"text": "Sentence extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9241881668567657}]}, {"text": "proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences.", "labels": [], "entities": []}, {"text": "He manually assigned parameter values to integrate features for estimating the significance scores of sentences.", "labels": [], "entities": []}, {"text": "On the other hand, machine learning methods can also be applied to integrate features.", "labels": [], "entities": []}, {"text": "For sentence extraction from training data, and used Bayes' rule, and generated a decision tree, and generated an SVM.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7767128348350525}]}, {"text": "In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7274125516414642}]}, {"text": "The analysis gives us some indication about how to use these features and how to combine them.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we show our evaluation results on the three sets of data for the sentence extraction system described in the previous section.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7368707358837128}]}, {"text": "shows the evaluation results for our system and some baseline systems on the task of sentence extraction at TSC-2001.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7561955153942108}, {"text": "TSC-2001", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.7976922988891602}]}, {"text": "The figures in are values of the F-measure 1 . The 'System' column shows the performance of our system and its rank among the nine systems that were applied to the task, and the 'Lead' column shows the performance of a baseline system which extracts as many sentences as the threshold from the beginning of a document.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9859834313392639}]}, {"text": "Since all participants could output as many sentences as the allowed upper limit, the values of the recall, precision, and F-measure were the same.", "labels": [], "entities": [{"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9996720552444458}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9993703961372375}, {"text": "F-measure", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9990932941436768}]}, {"text": "Our system obtained better results than the baseline systems, especially when the compression ratio was 10%.", "labels": [], "entities": [{"text": "compression ratio", "start_pos": 82, "end_pos": 99, "type": "METRIC", "confidence": 0.9769329130649567}]}, {"text": "The average performance was second among the nine systems.", "labels": [], "entities": []}, {"text": "shows the results of a subjective evaluation in the SDS task at DUC-2001.", "labels": [], "entities": [{"text": "SDS task", "start_pos": 52, "end_pos": 60, "type": "TASK", "confidence": 0.8496001958847046}, {"text": "DUC-2001", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.564098060131073}]}, {"text": "In this subjective evaluation, assessors gave a score to each system's outputs, on a zero-to-four scale (where four is the best), as compared with summaries made by humans.", "labels": [], "entities": []}, {"text": "The figures shown are the average scores overall documents.", "labels": [], "entities": []}, {"text": "The 'System' column shows the performance of our system and its rank among the 12 systems that were applied to this task.", "labels": [], "entities": []}, {"text": "The 'Lead' The definitions of each measurement are as follows:   where COR is the number of correct sentences marked by the system, GLD is the total number of correct sentences marked by humans, and SYS is the total number of sentences marked by the system.", "labels": [], "entities": [{"text": "COR", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9951866269111633}, {"text": "GLD", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.988331139087677}, {"text": "SYS", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.9852979779243469}]}, {"text": "After calculating these scores for each transcription, the average is calculated as the final score.", "labels": [], "entities": []}, {"text": "column shows the performance of a baseline system that always outputs the first 100 words of a given document, while the 'Avg.'", "labels": [], "entities": []}, {"text": "column shows the average for all systems.", "labels": [], "entities": []}, {"text": "Our system ranked 5th in grammaticality and was ranked at the top for the other measurements, including the total value.", "labels": [], "entities": []}, {"text": "The evaluation results for sentence extraction with the CSJ data are shown in.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7922565340995789}, {"text": "CSJ data", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8924291729927063}]}, {"text": "We compared the system's results with each annotator's key data.", "labels": [], "entities": []}, {"text": "As mentioned previously, we used 50 transcriptions for training and 10 for testing.", "labels": [], "entities": []}, {"text": "These results are comparable with the performance on sentence segmentation for written documents, because the system's performance for the TSC data was 0.363 when the compression ratio was set to 10%.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.7266581058502197}, {"text": "TSC data", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.8460918962955475}]}, {"text": "The results of our experiments thus show that for transcriptions, sentence extraction achieves results comparable to those for written documents, if the are well defined.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.8135119080543518}]}, {"text": "shows the contribution vectors for each set of training data.", "labels": [], "entities": []}, {"text": "The contribution here means the product of the optimized weight and the standard deviation of the score for the test data.", "labels": [], "entities": []}, {"text": "The vectors were normalized so that the sum of the components is equal to 1, and the selected function types for the features are also shown in the table.", "labels": [], "entities": []}, {"text": "Our system used the NE-based headline function (HL (N)) for the DUC data and the word-based function (HL We can see that the feature with the biggest contribution varies among the data sets.", "labels": [], "entities": [{"text": "DUC data", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.8633207678794861}]}, {"text": "While the position feature was the most effective for the TSC and DUC data, the length feature was dominant for the CSJ data.", "labels": [], "entities": [{"text": "DUC data", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.7587802708148956}, {"text": "length", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9964956641197205}, {"text": "CSJ data", "start_pos": 116, "end_pos": 124, "type": "DATASET", "confidence": 0.886647641658783}]}, {"text": "Most of the short sentences in the lectures were specific expressions, such as \"This is the result of the experiment.\" or \"Let me summarize my presentation.\".", "labels": [], "entities": []}, {"text": "Since these sentences were not extracted as key sentences by the annotators, it is believed that the function giving short sentences a penalty score matched the manual extraction results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results for the TSC data.", "labels": [], "entities": [{"text": "TSC data", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.7443791478872299}]}, {"text": " Table 2: Evaluation results for the DUC data (sub- jective evaluation).", "labels": [], "entities": [{"text": "DUC data", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9338438212871552}]}, {"text": " Table 3: Evaluation results for the CSJ data.", "labels": [], "entities": [{"text": "CSJ data", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9560294449329376}]}, {"text": " Table 4: Contribution (weight \u00d7 s.d.) of each feature  for each set of summarization data.", "labels": [], "entities": [{"text": "Contribution", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9807250499725342}]}, {"text": " Table 5: Rank correlation coefficients between fea- tures.", "labels": [], "entities": [{"text": "Rank correlation", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8707887828350067}]}, {"text": " Table 6: Distributions of key sentences based on the combination of the sentence position (Pst.) and tf*idf  features.", "labels": [], "entities": []}, {"text": " Table 7: Distributions of key sentences based on  the combination of the sentence position (Pst.) and  headline features.", "labels": [], "entities": []}]}