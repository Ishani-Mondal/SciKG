{"title": [], "abstractContent": [{"text": "This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora.", "labels": [], "entities": []}, {"text": "While classical hybrid systems manually define local part-of-speech patterns that lead to the identification of well-known multiword units (mainly compound nouns), our solution automatically identifies relevant syntactical patterns from the corpus.", "labels": [], "entities": []}, {"text": "Word statistics are then combined with the endogenously acquired linguistic information in order to extract the most relevant sequences of words.", "labels": [], "entities": []}, {"text": "As a result, (1) human intervention is avoided providing total flexibility of use of the system and (2) different multiword units like phrasal verbs, adverbial locutions and prepositional locutions maybe identified.", "labels": [], "entities": []}, {"text": "The system has been tested on the Brown Corpus leading to encouraging results.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9900083541870117}]}], "introductionContent": [{"text": "Multiword units (MWUs) include a large range of linguistic phenomena, such as compound nouns (e.g. interior designer), phrasal verbs (e.g. run through), adverbial locutions (e.g. on purpose), compound determinants (e.g. an amount of), prepositional locutions (e.g. in front of) and institutionalized phrases (e.g. con carne).", "labels": [], "entities": []}, {"text": "MWUs are frequently used in everyday language, usually to precisely express ideas and concepts that cannot be compressed into a single word.", "labels": [], "entities": []}, {"text": "As a consequence, their identification is a crucial issue for applications that require some degree of semantic processing (e.g. machine translation, summarization, information retrieval).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7921293675899506}, {"text": "summarization", "start_pos": 150, "end_pos": 163, "type": "TASK", "confidence": 0.9624985456466675}, {"text": "information retrieval", "start_pos": 165, "end_pos": 186, "type": "TASK", "confidence": 0.7306483387947083}]}, {"text": "In recent years, there has been a growing awareness in the Natural Language Processing (NLP) community of the problems that MWUs pose and the need for their robust handling.", "labels": [], "entities": []}, {"text": "For that purpose, syntactical, statistical) and hybrid syntaxicostatistical methodologies) have been proposed.", "labels": [], "entities": []}, {"text": "In this paper, we propose an original hybrid system called HELAS 1 that extracts MWU candidates from part-of-speech tagged corpora.", "labels": [], "entities": []}, {"text": "Unlike classical hybrid systems that manually pre-define local part-of-speech patterns of interest, our solution automatically identifies relevant syntactical patterns from the corpus.", "labels": [], "entities": []}, {"text": "Word statistics are then combined with the endogenously acquired linguistic information in order to extract the most relevant sequences of words i.e. MWU candidates.", "labels": [], "entities": []}, {"text": "Technically, we conjugate the Mutual Expectation (ME) association measure with the acquisition process called GenLocalMaxs () in a five step process.", "labels": [], "entities": [{"text": "Mutual Expectation (ME) association measure", "start_pos": 30, "end_pos": 73, "type": "METRIC", "confidence": 0.8837035553795951}, {"text": "GenLocalMaxs", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.8635825514793396}]}, {"text": "First, the part-of-speech tagged corpus is divided into two sub-corpora: one containing words and one containing part-of-speech tags.", "labels": [], "entities": []}, {"text": "Each sub-corpus is then segmented into a set of positional ngrams i.e. ordered vectors of textual units.", "labels": [], "entities": []}, {"text": "Third, the ME independently evaluates the degree of cohesiveness of each positional ngram i.e. any positional ngram of words and any positional ngram of part-of-speech tags.", "labels": [], "entities": []}, {"text": "A combination of both MEs is then used to evaluate the global degree of cohesiveness of any sequence of words associated with its respective part-of-speech tag sequence.", "labels": [], "entities": []}, {"text": "Finally, the GenLocalMaxs retrieves all the MWU candidates by evidencing local maxima of association measure values thus avoiding the definition of global thresholds.", "labels": [], "entities": []}, {"text": "The overall architecture can be seen in.", "labels": [], "entities": []}, {"text": "Compared to existing hybrid systems, the benefits of HELAS are clear.", "labels": [], "entities": []}, {"text": "By avoiding human intervention in the definition of syntactical patterns, it provides total flexibility of use.", "labels": [], "entities": []}, {"text": "Indeed, the system can be used for any language without any specific tuning.", "labels": [], "entities": []}, {"text": "HELAS also allows the identification of various MWUs like phrasal verbs, adverbial locutions, compound determinants, prepositional locutions and institutionalized phrases.", "labels": [], "entities": []}, {"text": "Finally, it responds to some extent to the affirmation of Beno\u00eet Habert and Christian Jacquemin (1993) that claim that \"existing hybrid systems do not sufficiently tackle the problem of the interdependency between the filtering stage [the definition of syntactical patterns] and the acquisition process [the scoring and the election of relevant sequences of words] as they propose that these two steps should be independent\".", "labels": [], "entities": [{"text": "scoring and the election of relevant sequences of words", "start_pos": 308, "end_pos": 363, "type": "TASK", "confidence": 0.5997400912973616}]}, {"text": "The article is divided into five main sections: (1) we introduce the related work; (2) we present the text corpus segmentation into positional ngrams; (3) we define the Mutual Expectation and anew combined association measure; (4) we propose the GenLocalMaxs algorithm as the acquisition process; Finally, in (5), we present some results over the Brown Corpus.", "labels": [], "entities": [{"text": "Mutual Expectation", "start_pos": 169, "end_pos": 187, "type": "METRIC", "confidence": 0.8717041611671448}, {"text": "anew combined association measure", "start_pos": 192, "end_pos": 225, "type": "METRIC", "confidence": 0.888258308172226}, {"text": "Brown Corpus", "start_pos": 347, "end_pos": 359, "type": "DATASET", "confidence": 0.9842164814472198}]}], "datasetContent": [{"text": "The Mutual Expectation (ME) has been introduced by Ga\u00ebl and evaluates the degree of cohesiveness that links together all the textual units contained in a positional ngram (\u2200n, n \u2265 2) based on the concept of Normalized Expectation and relative frequency.", "labels": [], "entities": [{"text": "Mutual Expectation (ME)", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.673265141248703}]}, {"text": "In order to test our architecture, we have conducted a number of experiments with 11 different values of \u03b1 fora portion of the Brown Corpus containing 249 578 words i.e. 249 578 words plus its 249 578 part-ofspeech tags.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 127, "end_pos": 139, "type": "DATASET", "confidence": 0.9776909053325653}]}, {"text": "The limited size of our corpus is mainly due to the space complexity of our system.", "labels": [], "entities": []}, {"text": "Indeed, the number of computed positional ngrams is huge even fora small corpus.", "labels": [], "entities": []}, {"text": "For instance, 21 463 192 positional ngrams are computed for this particular corpus fora 7-word size window context.", "labels": [], "entities": []}, {"text": "As a consequence, computation is hard.", "labels": [], "entities": []}, {"text": "For this experiment, HELAS has been tested on a personal computer with 128 Mb of RAM, 20 Gb of Hard Disk and an AMD 1.4 Ghz processor under Linux Mandrake 7.2.", "labels": [], "entities": [{"text": "HELAS", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9650505185127258}, {"text": "RAM", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9602144360542297}]}, {"text": "On average, each experiment (i.e. fora given \u03b1) took 4 hours and 20 minutes.", "labels": [], "entities": []}, {"text": "Knowing that our system increases proportionally with the size of the corpus, it was unmanageable, for this particular experiment, to test our architecture over a bigger corpus.", "labels": [], "entities": []}, {"text": "Even though, the whole processing stage lasted almost 48 hours 4 . We will divide our experiment into two main parts.", "labels": [], "entities": []}, {"text": "First, we will do a quantitative analysis and then we will lead a qualitative analysis.", "labels": [], "entities": []}, {"text": "All results will only tackle contiguous multiword units although non-contiguous sequences maybe extracted.", "labels": [], "entities": []}, {"text": "This decision is due to the lack of space.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Number of extracted MWU candidates", "labels": [], "entities": []}, {"text": " Table 8: Precision in % by alpha", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9691587686538696}]}]}