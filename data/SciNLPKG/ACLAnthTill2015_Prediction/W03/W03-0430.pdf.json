{"title": [{"text": "Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.7073366045951843}]}], "abstractContent": [], "introductionContent": [{"text": "Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features.", "labels": [], "entities": []}, {"text": "For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character ngrams, and capitalization patterns.", "labels": [], "entities": []}, {"text": "While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well.", "labels": [], "entities": []}, {"text": "There has been significant work with such models for greedy sequence modeling in NLP.", "labels": [], "entities": [{"text": "greedy sequence modeling", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6628861526648203}]}, {"text": "Conditional Random Fields (CRFs) () are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines.", "labels": [], "entities": []}, {"text": "While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training.", "labels": [], "entities": []}, {"text": "CRFs have shown empirical successes recently in POS tagging (), noun phrase segmentation and Chinese word segmentation.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.8854262232780457}, {"text": "noun phrase segmentation", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.7871148983637491}, {"text": "Chinese word segmentation", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.6737371484438578}]}, {"text": "Given these models' great flexibility to include a wide array of features, an important question that remains is what features should be used?", "labels": [], "entities": []}, {"text": "For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams.", "labels": [], "entities": []}, {"text": "As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows.", "labels": [], "entities": []}, {"text": "This paper presents a feature induction method for CRFs.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.9494694471359253}]}, {"text": "Founded on the principle of constructing only those feature conjunctions that significantly increase loglikelihood, the approach builds on that of Della Pietra et al, but is altered to work with conditional rather than joint probabilities, and with a mean-field approximation and other additional modifications that improve efficiency specifically fora sequence model.", "labels": [], "entities": []}, {"text": "In comparison with traditional approaches, automated feature induction offers both improved accuracy and significant reduction in feature count; it enables the use of richer, higherorder Markov models, and offers more freedom to liberally guess about which atomic features maybe relevant to a task.", "labels": [], "entities": [{"text": "automated feature induction", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6240865290164948}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9985448122024536}]}, {"text": "Feature induction methods still require the user to create the building-block atomic features.", "labels": [], "entities": [{"text": "Feature induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6879796087741852}]}, {"text": "Lexicon membership tests are particularly powerful features in natural language tasks.", "labels": [], "entities": [{"text": "Lexicon membership", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8196601867675781}]}, {"text": "The question is whereto get lexicons that are relevant for the particular task at hand?", "labels": [], "entities": []}, {"text": "This paper describes WebListing, a method that obtains seeds for the lexicons from the labeled data, then uses the Web, HTML formatting regularities and a search engine service to significantly augment those lexicons.", "labels": [], "entities": []}, {"text": "For example, based on the appearance of Arnold Palmer in the labeled data, we gather from the Web a large list of other golf players, including Tiger Woods (a phrase that is difficult to detect as a name without a good lexicon).", "labels": [], "entities": []}, {"text": "We present results on the CoNLL-2003 named entity recognition (NER) shared task, consisting of news articles with tagged entities PERSON, LOCATION, ORGANI-ZATION and MISC.", "labels": [], "entities": [{"text": "CoNLL-2003 named entity recognition (NER) shared task", "start_pos": 26, "end_pos": 79, "type": "TASK", "confidence": 0.8184113237592909}, {"text": "PERSON", "start_pos": 130, "end_pos": 136, "type": "METRIC", "confidence": 0.9784093499183655}, {"text": "ORGANI-ZATION", "start_pos": 148, "end_pos": 161, "type": "METRIC", "confidence": 0.9781337380409241}, {"text": "MISC", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.6095043420791626}]}, {"text": "On this, our first attempt at a NER task, with just a few person-weeks of effort and little work on developmentset error analysis, our method currently obtains overall English F1 of 84.04% on the test set by using CRFs, feature induction and Web-augmented lexicons.", "labels": [], "entities": [{"text": "NER task", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.9128850400447845}, {"text": "F1", "start_pos": 176, "end_pos": 178, "type": "METRIC", "confidence": 0.618393063545227}]}, {"text": "German F1 using very limited lexicons is 68.11%.", "labels": [], "entities": [{"text": "German", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8453690409660339}, {"text": "F1", "start_pos": 7, "end_pos": 9, "type": "METRIC", "confidence": 0.9513842463493347}]}], "datasetContent": [], "tableCaptions": []}