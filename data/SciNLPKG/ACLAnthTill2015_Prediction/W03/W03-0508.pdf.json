{"title": [{"text": "Examining the consensus between human summaries: initial experiments with factoid analysis", "labels": [], "entities": [{"text": "factoid analysis", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.8596101701259613}]}], "abstractContent": [{"text": "We present anew approach to summary evaluation which combines two novel aspects, namely (a) content comparison between gold standard summary and system summary via factoids, a pseudo-semantic representation based on atomic information units which can be robustly marked in text, and (b) use of a gold standard consensus summary, in our case based on 50 individual summaries of one text.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8470372259616852}]}], "introductionContent": [{"text": "It is an understatement to say that measuring the quality of summaries is hard.", "labels": [], "entities": []}, {"text": "In fact, there is unanimous consensus in the summarisation community that evaluation of summaries is a monstrously difficult task.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9841980338096619}, {"text": "evaluation of summaries", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.8334222833315531}]}, {"text": "In the past years, there has been quite a lot of summarisation work that has effectively aimed at finding viable evaluation strategies).", "labels": [], "entities": []}, {"text": "Largescale conferences like SUMMAC ( and have unfortunately shown weak results in that current evaluation measures could not distinguish between automatic summaries -though they are effective enough to distinguish them from human-written summaries.", "labels": [], "entities": []}, {"text": "In principle, the best way to evaluate a summary is to try to perform the task for which the summary was meant in the first place, and measure the quality of the summary on the basis of degree of success in executing the task.", "labels": [], "entities": []}, {"text": "However, such extrinsic evaluations are so time-consuming to setup that they cannot be used for the day-to-day evaluation needed during system development.", "labels": [], "entities": []}, {"text": "So in practice, a method for intrinsic evaluation is needed, where the properties of the summary itself are examined, independent of its application.", "labels": [], "entities": []}, {"text": "We think one of the reasons for the difficulty of an intrinsic evaluation is that summarisation has to call upon at least two hard subtasks: selection of information and production of new text.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9853817224502563}]}, {"text": "Both tasks are known from various NLP fields (e.g. information retrieval and information extraction for selection; generation and machine translation (MT) for production) to be not only hard to execute, but also hard to evaluate.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.7779966592788696}, {"text": "information extraction", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7428959757089615}, {"text": "machine translation (MT)", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.8484894037246704}]}, {"text": "This is caused fora large part by the fact that in both cases there is no single \"best\" result, but rather various \"good\" results.", "labels": [], "entities": []}, {"text": "It is hence no wonder that the evaluation of summarisation, combining these two, is even harder.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9670791625976562}]}, {"text": "The general approach for intrinsic evaluations, then, is to separate the evaluation of the form of the text (quality) and its information content (informativeness).", "labels": [], "entities": []}, {"text": "In this paper, we will focus on the latter, the intrinsic evaluation of informativeness, and we will address two aspects: the (in)sufficiency of the single human summary to measure against, and the information unit on which similarity measures are based.", "labels": [], "entities": []}], "datasetContent": [{"text": "If we plan to use human summaries as a reference point for the evaluation of machine-made summaries, we are assuming that there is some consensus between the human summarisers as to which information is important enough to include in a summary.", "labels": [], "entities": []}, {"text": "Whether such consensus actually exists is uncertain.", "labels": [], "entities": []}, {"text": "In very broad terms, we can distinguish four possible scenarios: 1.", "labels": [], "entities": []}, {"text": "There is a good consensus between all human summarisers.", "labels": [], "entities": []}, {"text": "A large percentage of the factoids present in the summaries is in fact present in a large percentage of the summaries.", "labels": [], "entities": []}, {"text": "We can determine whether this is so by measuring factoid overlap.", "labels": [], "entities": []}, {"text": "2. There is no such overall consensus between all summarisers, but there are subsets of summarisers between whom consensus exists.", "labels": [], "entities": []}, {"text": "Each of these subsets has summarised from a particular point of view, even though a generic summary was requested, and the point of view has led to group consensus.", "labels": [], "entities": []}, {"text": "We can determine whether this is so by doing a cluster analysis on the factoid presence vectors.", "labels": [], "entities": []}, {"text": "We should find clusters if and only if group consensus exists.", "labels": [], "entities": []}, {"text": "3. There is no such thing as overall consensus, but there is a difference in perceived importance between the various factoids.", "labels": [], "entities": []}, {"text": "We can determine whether this is the case by examining how often each factoid is used in the summaries.", "labels": [], "entities": []}, {"text": "Factoids that are more important ought to be included more often.", "labels": [], "entities": []}, {"text": "In that case, it is still possible to create a consensus-like reference summary for any desired summary size.", "labels": [], "entities": []}, {"text": "4. There is no difference in perceived importance of the various factoids at all.", "labels": [], "entities": []}, {"text": "Inclusion of factoids in summaries appears to be random.", "labels": [], "entities": []}, {"text": "Two of the main demands on a gold standard generic summary for evaluation are: a) that it contains the information deemed most important in the document and b) that two gold standard summaries constructed along the same lines lead to the same, or at least very similar, ranking of a set of summaries which are evaluated.", "labels": [], "entities": []}, {"text": "If we decide to use a single human summary as a gold standard, we in fact assume that this human's choice of important material is acceptable for all other summary users, which it the wrong assumption, as the lack of consensus between the various human summaries shows.", "labels": [], "entities": []}, {"text": "We propose that the use of a reference summary which is based on the factoid importance hierarchy described above, as it uses a less subjective indication of the relative importance of the information units in the text across a population of summary writers.", "labels": [], "entities": []}, {"text": "The reference summary would then take the form of a consensus summary, in our case the 100-word compound summary on the basis of factoids over the 30% threshold.", "labels": [], "entities": []}, {"text": "The construction of the consensus summary would indicate that demand a) will be catered for, but we still have to check demand b).", "labels": [], "entities": []}, {"text": "We can do this by computing rankings based on the F-measure for included factoids, and measuring the correlation coefficient \u03c1 between them.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.986885666847229}, {"text": "correlation coefficient \u03c1", "start_pos": 101, "end_pos": 126, "type": "METRIC", "confidence": 0.9323001901308695}]}, {"text": "As we do not have a large number of automatic summaries of our text available, we use our 50 human summaries as data, pretending that they are summaries we wish to rank (evaluate).", "labels": [], "entities": []}, {"text": "If we compare the rankings on the basis of single human summaries as gold standard, it turns out that the ranking correlation \u03c1 between two \"gold\" standards is indeed very low at an average of 0.20 (variation between -0.51 and 0.85).", "labels": [], "entities": [{"text": "ranking correlation \u03c1", "start_pos": 106, "end_pos": 127, "type": "METRIC", "confidence": 0.7305674354235331}]}, {"text": "For the consensus summary, we can compare rankings for various numbers of base summaries.", "labels": [], "entities": []}, {"text": "After all, the consensus summary should improve with the number of contributing base summaries and ought to approach an ideal consensus summary, which would be demonstrated by a stabilizing derived ranking.", "labels": [], "entities": []}, {"text": "We investigate if this assumption is correct by creating pairs of samples of N=5 to 200 base summaries, drawn (in away similar to bootstrapping) from our original sample of 50.", "labels": [], "entities": []}, {"text": "For each pair of samples, we automatically create a pair of consensus summaries and then determine how well these two agree in their ranking.", "labels": [], "entities": []}, {"text": "shows how \u03c1 increases with N (based on 1000 trials per N).", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9767012000083923}]}, {"text": "At N=5 and 10, \u03c1 has a still clearly unacceptable average 0.40 or 0.53.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 15, "end_pos": 16, "type": "METRIC", "confidence": 0.9570691585540771}]}, {"text": "The average reaches 0.80 at 45, 0.90 at 95 and 0.95 at a staggering 180 base summaries.", "labels": [], "entities": []}, {"text": "We must note, however, that we have to be careful with these measurements, since 40 of our 50 starting summaries were made by less experienced non-natives.", "labels": [], "entities": []}, {"text": "In fact, if we bootstrap pairs of N=10 base summary samples (100 trials) on just the 10 higher-quality summaries (created by natives and near-natives), we get an average \u03c1 of 0.74.", "labels": [], "entities": []}, {"text": "The same experiment on 10 different summaries from the other 40 (100 trials for choosing the 10, and for each 100 trials to estimate average \u03c1) yields average \u03c1's ranging from 0.55 to 0.63.", "labels": [], "entities": []}, {"text": "So clearly the difference in experience has its effect.", "labels": [], "entities": []}, {"text": "Even so, even the 'better' summaries lead to a ranking correlation of \u03c1=0.74 at N=10, which still is much lower than we would like to see.", "labels": [], "entities": []}, {"text": "We estimate that with this type of summaries an acceptably stable ranking (\u03c1 around 0.90) would be reached somewhere between 30 and 40 summaries.", "labels": [], "entities": []}], "tableCaptions": []}