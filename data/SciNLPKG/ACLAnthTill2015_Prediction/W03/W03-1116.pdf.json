{"title": [{"text": "Extraction of User Preferences from a Few Positive Documents", "labels": [], "entities": [{"text": "Extraction of User Preferences from a Few Positive Documents", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.8671885000334846}]}], "abstractContent": [{"text": "In this work, we propose anew method for extracting user preferences from a few documents that might interest users.", "labels": [], "entities": []}, {"text": "For this end, we first extract candidate terms and choose a number of terms called initial representative keywords (IRKs) from them through fuzzy inference.", "labels": [], "entities": []}, {"text": "Then, by expanding IRKs and reweighting them using term co-occurrence similarity, the final representative keywords are extracted.", "labels": [], "entities": []}, {"text": "Performance of our approach is heavily influenced by effectiveness of selection method for IRKs so we choose fuzzy inference because it is more effective in handling the uncertainty inherent in selecting representative keywords of documents.", "labels": [], "entities": []}, {"text": "The problem addressed in this paper can be viewed as the one of finding a representative vector of documents in the linear text classification literature.", "labels": [], "entities": [{"text": "linear text classification", "start_pos": 116, "end_pos": 142, "type": "TASK", "confidence": 0.6969399650891622}]}, {"text": "So, to show the usefulness of our approach, we compare it with two famous methods-Rocchio and Widrow-Hoff-on the Reuters-21578 collection.", "labels": [], "entities": [{"text": "Reuters-21578 collection", "start_pos": 113, "end_pos": 137, "type": "DATASET", "confidence": 0.8583553731441498}]}, {"text": "The results show that our approach outperforms the other approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Agent technology is able to provide increasingly more services for individuals, groups, and organizations.", "labels": [], "entities": []}, {"text": "Agents, which have been developed for Internet, have addressed many tasks such as information finding, filtering and presentation, contract negotiation, and electronic commerce).", "labels": [], "entities": [{"text": "information finding, filtering and presentation", "start_pos": 82, "end_pos": 129, "type": "TASK", "confidence": 0.7619941234588623}, {"text": "contract negotiation", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.7030004262924194}]}, {"text": "Most of them rely on the knowledge of the user.", "labels": [], "entities": []}, {"text": "The inclusion of user information becomes a key area.", "labels": [], "entities": []}, {"text": "A user model that represents some aspects of a user's information needs or preferences can be useful in any information system design, and in the case of information filtering ().", "labels": [], "entities": [{"text": "information filtering", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7646801471710205}]}, {"text": "User models can be constructed by hand, or learned automatically based on feedback provided by the users.", "labels": [], "entities": []}, {"text": "Some systems require users to explicitly specify their profiles, often as a set of keywords or categories.", "labels": [], "entities": []}, {"text": "But it is difficult fora user to exactly and correctly specify their information needs.", "labels": [], "entities": []}, {"text": "The machine learning techniques offer the potential to automatic construction and continuous refinement of user model.", "labels": [], "entities": []}, {"text": "The research systems adopting the machine learning techniques have been applied feedback techniques that explicitly provide relevance judgments on documents.", "labels": [], "entities": []}, {"text": "Studies have shown that such explicit feedback from the user is clearly useful, but, in practice, many users are unwilling to provide relevance judgments on documents (Pazzani, M.,) . Users may have problems to decide about some documents.", "labels": [], "entities": []}, {"text": "An alternative is to use implicit feedback where document relevance is inferred from user's behavior, which has received increased attention in recent years (Nichols, 1997; This paper focuses upon the extraction of user preferences from a few documents that might interest a user.", "labels": [], "entities": []}, {"text": "It does not consider how to provide relevance judgment on documents, i.e. it assumes that relevant documents are given explicitly or implicitly.", "labels": [], "entities": []}, {"text": "Our approach is based on the vector space model), where text-based documents are represented as vectors of term weights.", "labels": [], "entities": []}, {"text": "So, the problem addressed in this paper is how to extract representative keywords from documents provided by a user and what weights should be assigned to these keywords.", "labels": [], "entities": []}, {"text": "We present anew technique to solve this problem.", "labels": [], "entities": []}, {"text": "The proposed method is composed of two parts, one is to select initial representative keywords (IRKs) and the other is to automatically expand and reweight IRKs.", "labels": [], "entities": []}, {"text": "For the first part, we can consider feature selection methods) that focus on performance improvement and dimensionality reduction of document classifiers fora huge amount of documents covering various categories.", "labels": [], "entities": []}, {"text": "However, since this kind of methods select features using information of other categories and negative document sets as well as positive ones, it is impossible to apply these to the target problem in this paper that extract feature keywords from only few positive documents in the same category.", "labels": [], "entities": []}, {"text": "As alternatives, we can consider the Rocchio algorithm and Widrow-Hoff algorithm used as a training algorithm for linear text classifier since these algorithms can extract keywords and assign weights to them effectively with only positive document sets.", "labels": [], "entities": [{"text": "linear text classifier", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.610977570215861}]}, {"text": "However, here, anew technique that adopts fuzzy inference to extractor generate IRKs from a few example documents (the set of documents judged relevant by the users) is suggested since the existing algorithms did not show good results as we expected.", "labels": [], "entities": []}, {"text": "For the second part, we can choose one of query term expansion and term weight modification methods based on vector model ().", "labels": [], "entities": [{"text": "query term expansion", "start_pos": 42, "end_pos": 62, "type": "TASK", "confidence": 0.6040236552556356}]}, {"text": "Instead, we take anew approach where the term co-occurrence similarity is introduced as a measure of similarity between the distributions within the feedbacked documents of a given term and the initial query.", "labels": [], "entities": [{"text": "similarity", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9560560584068298}]}, {"text": "With this similarity and the document frequency in feedbacked documents, the weight of the term in the new query was calculated.", "labels": [], "entities": []}, {"text": "In the next section, Rocchio and Widrow-Hoff algorithms are reviewed.", "labels": [], "entities": []}, {"text": "Section 3 presents a method for user's preference extraction.", "labels": [], "entities": [{"text": "user's preference extraction", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.5762717500329018}]}, {"text": "The experiments to test the proposed method will be outlined in Section 4.", "labels": [], "entities": []}, {"text": "Finally, conclusion is followed.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Reuters-21578 data as an experimental document set.", "labels": [], "entities": [{"text": "Reuters-21578 data", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.9746261835098267}]}, {"text": "This collection has five different sets of contents related categories.", "labels": [], "entities": []}, {"text": "They are EXCHANGES, ORGS, PEOPLE, PLACES and TOPICS.", "labels": [], "entities": [{"text": "EXCHANGES", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.7995806932449341}, {"text": "ORGS", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9915964007377625}, {"text": "PEOPLE", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9387786984443665}, {"text": "PLACES", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9690214991569519}, {"text": "TOPICS", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.5748552680015564}]}, {"text": "Some of the categories set have up to 265 categories, but some of them have just 39 categories.", "labels": [], "entities": []}, {"text": "We chose the TOPICS categories set which has 135 categories.", "labels": [], "entities": [{"text": "TOPICS categories set", "start_pos": 13, "end_pos": 34, "type": "DATASET", "confidence": 0.9038974642753601}]}, {"text": "We divided the documents according to the \"ModeApte\" split.", "labels": [], "entities": [{"text": "ModeApte\" split", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9022378126780192}]}, {"text": "There are 9603 training documents and 3299 test documents.", "labels": [], "entities": []}, {"text": "Among the 135 categories, we first chose only 90 ones that have at least one training example and one testing example.", "labels": [], "entities": []}, {"text": "Then, we finally selected 21 categories that have from 10 to 30 training documents.", "labels": [], "entities": []}, {"text": "The 3019 documents of those categories are used as testing documents.", "labels": [], "entities": []}, {"text": "The document frequency information from 7770 training documents in 90 categories is used to calculate IDF values of terms.", "labels": [], "entities": [{"text": "IDF", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.929985523223877}]}, {"text": "We did not consider negative documents under the assumption that only positive documents coincident with users' preferences were given implicitly or explicitly . Documents are ranked by the cosine similarity and the following F-measure), which is a weighted combination of recall and precision and popularly used for performance evaluation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 226, "end_pos": 235, "type": "METRIC", "confidence": 0.995987594127655}, {"text": "recall", "start_pos": 273, "end_pos": 279, "type": "METRIC", "confidence": 0.998745322227478}, {"text": "precision", "start_pos": 284, "end_pos": 293, "type": "METRIC", "confidence": 0.9927847385406494}]}, {"text": "Since the maximum value for F can be interpreted as the best possible compromise between recall and precision, we use this maximum value.", "labels": [], "entities": [{"text": "F", "start_pos": 28, "end_pos": 29, "type": "METRIC", "confidence": 0.9785552620887756}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9986802935600281}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9927213788032532}]}, {"text": "where, R j and P j are the recall and precision for the j'th document in the ranking and F j is their harmonic mean.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.999506950378418}, {"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9919575452804565}, {"text": "F j", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9699483215808868}]}, {"text": "First, our method was compared to the Rocchio and Widrow-Hoff algorithms.", "labels": [], "entities": []}, {"text": "To seethe effect of the number of FRKs, we made experiments by varying it from 5 to 30 in increment 5 and for the case that all terms are used.", "labels": [], "entities": [{"text": "FRKs", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8434773087501526}]}, {"text": "shows the overall or summary result of the proposed method compared to the two existing algorithms for 21categories.", "labels": [], "entities": []}, {"text": "The result shows that our method is better than the others in all cases, especially when 10 terms are used to represent user preferences.", "labels": [], "entities": []}, {"text": "shows the detail result in that case, i.e. the F-values and the performance improvement ratios when 10 terms are used.", "labels": [], "entities": [{"text": "detail", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9983612895011902}, {"text": "F-values", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9982625842094421}, {"text": "performance improvement ratios", "start_pos": 64, "end_pos": 94, "type": "METRIC", "confidence": 0.8450877467791239}]}, {"text": "The proposed method has achieved about 20% over Rocchio algorithm and 10% over Widrow-Hoff algorithm on the average.", "labels": [], "entities": []}, {"text": "When 5 terms are used to represent user preferences, 19 categories among 21 categories are used because \"strategic-metal\" and \"pet-chem\" categories do not satisfy the constraint in Section 3.2, i.e., 5 terms are too few to coverall training documents.", "labels": [], "entities": []}, {"text": "It is not clear which component of our method mainly contributes to such improvement since our method consists of two main components -one is for extracting IRKs, the other for expanding and reweighting of IRKs.", "labels": [], "entities": []}, {"text": "To analyze our method, we made several variants of the proposed method and did experiments with them.", "labels": [], "entities": []}, {"text": "The variants are named by the sequence of the following symbols.", "labels": [], "entities": []}, {"text": "IF, IR, IW: mean that IRKs are selected based on the weight obtained by the method in Section 3.1, the Rocchio algorithm, and the Widrow-Hoff algorithm, respectively.", "labels": [], "entities": [{"text": "IF", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9554986953735352}, {"text": "IR", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9391326308250427}, {"text": "IW", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.985032856464386}, {"text": "IRKs", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.8939628601074219}]}, {"text": "RC, RR, RW: mean that terms are reweighted by the method in Section 3.3, the Rocchio algorithm, and the Widrow-Hoff algorithm, respectively.", "labels": [], "entities": []}, {"text": "EC, EF, ER, EW: mean that expanded terms are selected based on the weight obtained by applying the method in Section 3.3, the method in Section 3.1, the Rocchio algorithm, and the Widrow-Hoff algorithm, respectively.", "labels": [], "entities": [{"text": "EC", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.889292299747467}, {"text": "ER", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9694110155105591}]}, {"text": "For example, the proposed method in Section 3 is named as IF_EF_RC, which means IRKs, and expanded terms are selected based on the weight calculated by the method in Section 3.1 and then reweighted by the method in Section 3.3.", "labels": [], "entities": [{"text": "IF_EF_RC", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.810115647315979}, {"text": "IRKs", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9710115790367126}]}, {"text": "For another example, the method called by IF_RC_EC means that IRKs are selected based on the weight obtained by the method in Section 3.1 and then all terms are reweighed by the method in Section 3.3 before expanded terms are selected.", "labels": [], "entities": [{"text": "IF_RC_EC", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.47392202615737916}, {"text": "IRKs", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9144738912582397}]}, {"text": "In the proposed method, fuzzy inference technique is used to extract IRKs.", "labels": [], "entities": []}, {"text": "So, we tried two variants, IR_ER_RC and IW_EW_RC, where the Rocchio and Widrow-Hoff algorithms are used respectively to calculate the representativeness (or weights) of terms instead of the method in Section 3.1, and then IRKs and expanded terms are selected based on these weights.", "labels": [], "entities": [{"text": "IR_ER_RC", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.8447357177734375}, {"text": "IW_EW_RC", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.7866149187088013}]}, {"text": "The variants all use the reweighting scheme in Section 3.3.", "labels": [], "entities": [{"text": "reweighting", "start_pos": 25, "end_pos": 36, "type": "METRIC", "confidence": 0.9798535108566284}]}, {"text": "shows that other keyword extraction algorithms do not show any benefit over the fuzzy inference approach.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7600346803665161}]}, {"text": "We can also observe that when one of the existing algorithms is combined with the second component of our method, the performance improvement over the case that the algorithm solely is used is negligible.", "labels": [], "entities": []}, {"text": "The method to extract IRKs reflecting user's preference directly affects the result of the term reweighting process because the process is based on the term co-occurrence similarity with the IRKs.", "labels": [], "entities": []}, {"text": "If the terms that are far from user's preference are extracted as IRKs, then some terms that actually are improper in representing user's information needs maybe assigned with high weights during the reweighting process and then the final vector generated from the results maybe disqualified from representing user's preferences.", "labels": [], "entities": [{"text": "IRKs", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.7323781251907349}]}, {"text": "So, we can know that our fuzzy inference technique is effective to extract IRKs from the results in.", "labels": [], "entities": [{"text": "IRKs", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.7465817332267761}]}, {"text": "To demonstrate the usefulness of the second part of our method, i.e., the expansion and reweighting technique, we also tried the 5 variants of our method (IF_RC_EC, IF_RR_ER, IF_RW_EW, IF_EF_RR, IF_EF_RW).", "labels": [], "entities": [{"text": "IF_RC_EC", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.6276161849498749}, {"text": "IF_RR_ER", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.6100065529346466}, {"text": "IF_EF_RW", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.5971376776695252}]}, {"text": "shows the all variants are not better than the original though they outperform Rocchio and Widrow-Hoff algorithms.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Performance of 21 categories in the  REUTERS corpus and comparison with two exist- ing algorithms.", "labels": [], "entities": [{"text": "REUTERS corpus", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.7779945731163025}]}, {"text": " Table 3. The detail result when 10 terms are used  for user preferences", "labels": [], "entities": [{"text": "detail", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9960538148880005}]}, {"text": " Table 4. The performance of our method and its  two variants that use Rocchio and Widrow-Hoff  algorithms instead of fuzzy inference, respectively.", "labels": [], "entities": []}, {"text": " Table 5. The performance of our method and its  five variants that use different reweighting and ex- panding approaches. Xu Jinix and Croft W. B.. 1996. Query Expansion Us- ing Local and Global Document Analysis, In Pro- ceeding of ACM SIGIR International Conference on  Research and Development in Information Retrieval,  p4-11.", "labels": [], "entities": [{"text": "Query Expansion Us- ing Local and Global Document Analysis", "start_pos": 154, "end_pos": 212, "type": "TASK", "confidence": 0.5862709373235703}, {"text": "ACM SIGIR International Conference on  Research and Development in Information Retrieval,  p4-11", "start_pos": 233, "end_pos": 329, "type": "TASK", "confidence": 0.5770300076558039}]}]}