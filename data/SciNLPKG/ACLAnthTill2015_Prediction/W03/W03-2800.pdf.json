{"title": [], "abstractContent": [], "introductionContent": [{"text": "Systems that accomplish different Natural Language Processing (NLP) tasks have different characteristics and therefore, it would seem, different requirements for evaluation.", "labels": [], "entities": []}, {"text": "However, are there common features in evaluation methods used in various language technologies?", "labels": [], "entities": []}, {"text": "Could the evaluation methods established for one type of systems be ported/adapted to another NLP research area?", "labels": [], "entities": []}, {"text": "Could automatic evaluation metrics be ported?", "labels": [], "entities": []}, {"text": "For instance, could Papineni's MT evaluation metric be used for the evaluation of generated summaries?", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.844873458147049}]}, {"text": "Could the extrinsic evaluation method used within SUMMAC be applied to the evaluation of Natural Language Generation systems?", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.6471888720989227}]}, {"text": "What are the reusability obstacles encountered and how could they be overcome?", "labels": [], "entities": []}, {"text": "What are the evaluation needs of system types such as dialogue systems, which have been less strenuously evaluated till now, and how could they benefit from current practices in evaluating Language Engineering technologies?", "labels": [], "entities": []}, {"text": "What are the evaluation challenges that emerge from systems that integrate a number of different language processing functions (e.g. multimodal dialogue systems such as Smartkom)?", "labels": [], "entities": []}, {"text": "Could resources (e.g. corpora) used fora specific NLP task, be reused for the evaluation of an NLP system and if so, what adaptations would this require?", "labels": [], "entities": []}, {"text": "Cross-fertilization of evaluation resources has taken place to some extent: in MUC, the extractionspecific adaptation of the standard Information Retrieval precision metric has been accepted as a standard for the evaluation of Information Extraction systems.", "labels": [], "entities": [{"text": "MUC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8008026480674744}, {"text": "Information Extraction", "start_pos": 227, "end_pos": 249, "type": "TASK", "confidence": 0.678458496928215}]}, {"text": "In SUMMAC, parts of the TREC collection (documents, relevance assessments and even assessment software) have been reused.", "labels": [], "entities": [{"text": "TREC collection", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.8114480376243591}]}, {"text": "Both MTEval and SUMMAC have used conceptually similar approaches to evaluation (i.e. subject-based evaluation by testing reading comprehension).", "labels": [], "entities": [{"text": "MTEval", "start_pos": 5, "end_pos": 11, "type": "DATASET", "confidence": 0.753658652305603}]}, {"text": "Many U.S. and European funding initiatives have been devoted to the evaluation of specific NLP systems, such as: MUC, SUMMAC, TREC and its followup initiative CLEF, MTEval and DUC.", "labels": [], "entities": [{"text": "MUC", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9148727059364319}, {"text": "TREC", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.6390851140022278}, {"text": "MTEval", "start_pos": 165, "end_pos": 171, "type": "DATASET", "confidence": 0.842168390750885}, {"text": "DUC", "start_pos": 176, "end_pos": 179, "type": "DATASET", "confidence": 0.8816738128662109}]}, {"text": "ISLE, the European initiative for establishing standards in Language Engineering has a working group on the evaluation of Machine Translation systems and its predecessor, EAGLES, has addressed evaluation issues for Language Engineering in general.", "labels": [], "entities": [{"text": "ISLE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8141672611236572}, {"text": "Machine Translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7726438939571381}]}, {"text": "The ELSE project was concerned with the evaluation infrastructure that could be deployed within the scope of the IST Key Actions of the 5th Framework Program of the European Community and indeed, the funding of evaluation activities has been addressed within the 5th Framework (as reported by.", "labels": [], "entities": []}, {"text": "Transatlantic co-operation for the evaluation of Human Language Technologies has also been stressed, among other issues, within an extensive report that was submitted to both the U.S. National Science Foundation and the European Commission's Language Engineering Office in 1999.", "labels": [], "entities": []}, {"text": "This report mentions that evaluation techniques in the different Language Engineering areas grow more similar, a fact that emphasizes the need for co-ordinated and reusable evaluation resources.", "labels": [], "entities": []}, {"text": "The time has come to bring together all the above attempts to address the evaluation of NLP systems as a whole and explore ways for reusing established evaluation methods, metrics and other resources, thus, contributing to a more co-ordinated approach to the evaluation of language technology.", "labels": [], "entities": []}, {"text": "This is exactly what this workshop has achieved: to bring together leading researchers from various NLP areas (such as Machine Translation, Information Extraction, Information Retrieval, Automatic Summarization, Question-Answering, Dialogue Systems and Natural Language Generation) in order to discuss this topic.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.8170265853404999}, {"text": "Information Extraction", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.7734796404838562}, {"text": "Information Retrieval", "start_pos": 164, "end_pos": 185, "type": "TASK", "confidence": 0.8088639378547668}, {"text": "Automatic Summarization", "start_pos": 187, "end_pos": 210, "type": "TASK", "confidence": 0.7721305191516876}, {"text": "Natural Language Generation)", "start_pos": 253, "end_pos": 281, "type": "TASK", "confidence": 0.7127943560481071}]}, {"text": "The papers included in this volume address issues of reuse of evaluation resources within and across NLP research areas.", "labels": [], "entities": []}, {"text": "We cordially thank the authors and the members of the Programme Committee whose significant contributions made this workshop possible.", "labels": [], "entities": []}, {"text": "We are especially grateful to our invited speakers: Donna Harman and Kevin McTait and ELSNET for its support and endorsement.", "labels": [], "entities": [{"text": "McTait", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.791000247001648}, {"text": "ELSNET", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.5991317629814148}]}], "datasetContent": [], "tableCaptions": []}