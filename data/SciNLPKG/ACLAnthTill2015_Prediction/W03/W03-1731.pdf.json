{"title": [], "abstractContent": [{"text": "This paper introduces a Chinese word tokenization system through HMM-based chunking.", "labels": [], "entities": [{"text": "Chinese word tokenization", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.4840095341205597}, {"text": "HMM-based chunking", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6948146224021912}]}, {"text": "Experiments show that such a system can well deal with the unknown word problem in Chinese word tokenization.", "labels": [], "entities": [{"text": "Chinese word tokenization", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.5673628747463226}]}, {"text": "The second term in (2-1) is the mutual information between T and . In order to simplify the computation of this term, we assume mutual information independence (2-2):", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Tokenization is regarded as one of major bottlenecks in Chinese Language Processing.", "labels": [], "entities": [{"text": "Word Tokenization", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6455188989639282}, {"text": "Chinese Language Processing", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.571987529595693}]}, {"text": "Normally, word tokenization is implemented through word segmentation in Chinese Language Processing literature.", "labels": [], "entities": [{"text": "word tokenization", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.745111346244812}, {"text": "word segmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7081668227910995}, {"text": "Chinese Language Processing literature", "start_pos": 72, "end_pos": 110, "type": "DATASET", "confidence": 0.5944148749113083}]}, {"text": "This is also affected in the title of this competition.", "labels": [], "entities": []}, {"text": "That is, an individual tag is only dependent on the token sequence G and independent on other tags in the tag sequence T . This assumption is reasonable because the dependence among the tags in the tag sequence T has already been captured by the first term in equation (2-1).", "labels": [], "entities": []}, {"text": "Applying it to equation (2-1), we have (2-3): There exists two major problems in Chinese word segmentation: ambiguity and unknown word detection.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.596850574016571}, {"text": "unknown word detection", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.6537929673989614}]}, {"text": "While ngarm modeling and/or word co-ocurrence has been successfully applied to deal with ambiguity problem, unknown word detection has become major bottleneck in word tokenization.", "labels": [], "entities": [{"text": "ngarm modeling", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7628360986709595}, {"text": "unknown word detection", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.6553231676419576}, {"text": "word tokenization", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.7153319418430328}]}, {"text": "This paper proposes a HMM-based chunking scheme to cope with unkown words in Chinese word tokenization.", "labels": [], "entities": [{"text": "HMM-based chunking", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8127444386482239}, {"text": "Chinese word tokenization", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.5924733281135559}]}, {"text": "The unknown word detection is re-casted as chunking several words (single-character word or multi-character word) together to form anew word.", "labels": [], "entities": [{"text": "unknown word detection", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6695331037044525}]}, {"text": "From equation (2-3), we can see that: \u2022 The first term can be computed by applying chain rules.", "labels": [], "entities": []}, {"text": "In ngram modeling, each tag is assumed to be probabilistically dependent on the N-1 previous tags.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}