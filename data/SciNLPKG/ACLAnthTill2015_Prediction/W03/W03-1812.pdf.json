{"title": [{"text": "An Empirical Model of Multiword Expression Decomposability", "labels": [], "entities": [{"text": "Multiword Expression Decomposability", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.7626205285390218}]}], "abstractContent": [{"text": "This paper presents a construction-inspecific model of multiword expression decomposability based on latent semantic analysis.", "labels": [], "entities": []}, {"text": "We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.", "labels": [], "entities": []}, {"text": "We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9654626846313477}]}, {"text": "Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.9533771276473999}]}], "introductionContent": [{"text": "This paper is concerned with an empirical model of multiword expression decomposability.", "labels": [], "entities": []}, {"text": "Multiword expressions (MWEs) are defined to be cohesive lexemes that crossword boundaries ().", "labels": [], "entities": [{"text": "Multiword expressions (MWEs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.684008264541626}]}, {"text": "They occur in a wide variety of syntactic configurations in different languages (e.g. in the case of English, compound nouns: post office, verbal idioms: pull strings, verb-particle constructions: push on, etc.).", "labels": [], "entities": []}, {"text": "Decomposability is a description of the degree to which the semantics of an MWE can be ascribed to those of its parts ().", "labels": [], "entities": []}, {"text": "Analysis of the semantic correlation between the constituent parts and whole of an MWE is perhaps more commonly discussed under the banner of compositionality (.", "labels": [], "entities": []}, {"text": "Our claim here is that the semantics of the MWE are deconstructed and the parts coerced into often idiosyncratic interpretations to attain semantic alignment, rather than the other way around.", "labels": [], "entities": []}, {"text": "One idiom which illustrates this process is spill the beans, where the semantics of reveal \ud97b\udf59 (secret \ud97b\udf59 ) are decomposed such that spill is coerced into the idiosyncratic interpretation of reveal \ud97b\udf59 and beans into the idiosyncratic interpretation of secret \ud97b\udf59 . Given that these senses for spill and beans are not readily available at the simplex level other than in the context of this particular MWE, it seems fallacious to talk about them composing together to form the semantics of the idiom.", "labels": [], "entities": []}, {"text": "Ideally, we would like to be able to differentiate between three classes of MWEs: nondecomposable, idiosyncratically decomposable and simple decomposable (derived from Nunberg et al.'s sub-classification of idioms).", "labels": [], "entities": []}, {"text": "With nondecomposable MWEs (e.g. kick the bucket, shoot the breeze, hot dog), no decompositional analysis is possible, and the MWE is semantically impenetrable.", "labels": [], "entities": []}, {"text": "The only syntactic variation that non-decomposable MWEs undergo is verbal inflection (e.g. kicked the bucket, kicks the bucket) and pronominal reflexivisation (e.g. wet oneself , wet themselves).", "labels": [], "entities": []}, {"text": "Idiosyncratically decomposable MWEs (e.g. spill the beans, let the cat out of the bag, radar footprint) are decomposable but coerce their parts into taking semantics unavailable outside the MWE.", "labels": [], "entities": []}, {"text": "They undergo a certain degree of syntactic variation (e.g. the cat was let out of the bag).", "labels": [], "entities": []}, {"text": "Finally, simple decomposable MWEs (also known as \"institutionalised\" MWEs, e.g. kindle excitement, traffic light) decompose into simplex senses and generally display high syntactic variability.", "labels": [], "entities": []}, {"text": "What makes simple decomposable expressions true MWEs rather than productive word combinations is that they tend to block compositional alternates with the expected semantics (termed anticollocations by).", "labels": [], "entities": []}, {"text": "For example, motorcar cannot be rephrased as *engine car or *motor automobile.", "labels": [], "entities": []}, {"text": "Note that the existence of anticollocations is also a test for non-decomposable and idiosyncratically decomposable MWEs (e.g. hot dog vs. #warm dog or #hot canine).", "labels": [], "entities": []}, {"text": "Our particular interest in decomposability stems from ongoing work on grammatical means for capturing MWEs.", "labels": [], "entities": []}, {"text": "observed that idiosyncratically decomposable MWEs (in particular idioms) undergo much greater syntactic variation than non-decomposable MWEs, and that the variability can be partially predicted from the decompositional analysis.", "labels": [], "entities": []}, {"text": "We thus aim to capture the decomposability of MWEs in the grammar and use this to constrain the syntax of MWEs in parsing and generation.", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.6930139561494192}]}, {"text": "Note that it is arguable whether simple decomposable MWEs belong in the grammar proper, or should be described instead as lexical affinities between particular word combinations.", "labels": [], "entities": []}, {"text": "As the first step down the path toward an empirical model of decomposability, we focus on demarcating simple decomposable MWEs from idiosyncratically decomposable and non-decomposable MWEs.", "labels": [], "entities": []}, {"text": "This is largely equivalent to classifying MWEs as being endocentric (i.e., a hyponym of their head) or exocentric (i.e., not a hyponym of their head:).", "labels": [], "entities": []}, {"text": "We attempt to achieve this by looking at the semantic similarity between an MWE and its constituent words, and hypothesising that where the similarity between the constituents of an MWE and the whole is sufficiently high, the MWE must be of simple decomposable type.", "labels": [], "entities": []}, {"text": "The particular similarity method we adopt is latent semantic analysis, or LSA (.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6725269357363383}]}, {"text": "LSA allows us to calculate the similarity between an arbitrary word pair, offering the advantage of being able to measure the similarity between the MWE and each of its constituent words.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7745333313941956}]}, {"text": "For MWEs such as houseboat, therefore, we can expect to capture the fact that the MWE is highly similar in meaning to both constituent words (i.e. the modifier house and head noun boat).", "labels": [], "entities": []}, {"text": "More importantly, LSA makes no assumptions about the lexical or syntactic composition of the inputs, and thus constitutes a fully construction-and language-inspecific method of modelling decomposability.", "labels": [], "entities": []}, {"text": "This has clear advantages over a more conventional supervised classifierstyle approach, where training data would have to be customised to a particular language and construction type.", "labels": [], "entities": []}, {"text": "Evaluation is inevitably a difficulty when it comes to the analysis of MWEs, due to the lack of concise consistency checks on what MWEs should and should not be incorporated into dictionaries.", "labels": [], "entities": [{"text": "analysis of MWEs", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.5461927254994711}]}, {"text": "While recognising the dangers associated with dictionarybased evaluation, we commit ourselves to this paradigm and focus on searching for appropriate means of demonstrating the correlation between dictionary-and corpus-based similarities.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes past research on MWE compositionality of relevance to this effort.", "labels": [], "entities": [{"text": "MWE compositionality", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.9771024584770203}]}, {"text": "Section 3 provides a basic outline of the resources used in this research, LSA, the MWE extraction methods, and measures used to evaluate our method.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.9382499158382416}]}, {"text": "Section 4 then provides evaluation of the proposed method, and the paper is concluded with a brief discussion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "LSA was used to build models in which MWEs could be compared with their constituent words.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7464961409568787}]}, {"text": "Two models were built, one from the WSJ corpus (indexing NN compounds) and one from the BNC (indexing verb-particles).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9718105494976044}, {"text": "BNC", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.7044031620025635}]}, {"text": "After removing stopwords, the 50,000 most frequent terms were indexed in each model.", "labels": [], "entities": []}, {"text": "From the WSJ, these 50,000 terms included 1,710 NN compounds (with corpus frequency of at least 13) and from the BNC, 461 verbparticles (with corpus frequency of at least 49).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9735551476478577}, {"text": "BNC", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9249404668807983}]}, {"text": "We used these models to compare different words, and to find their neighbours.", "labels": [], "entities": []}, {"text": "For example, the neighbours of the simplex verb cut and the verb-particles cutout and cutoff (from the BNC model) are shown in.", "labels": [], "entities": []}, {"text": "As can be seen, several of the neighbours of cutout are from similar semantic areas as those of cut, whereas those of cutoff are quite different.", "labels": [], "entities": []}, {"text": "This reflects the fact that inmost of its instances the verb cutoff is used to mean \"forcibly isolate\".", "labels": [], "entities": []}, {"text": "In order to measure this effect quantitatively, we can simply take the cosine similarities between these verbs, finding that sim(cut, cut out) = 0.433 and sim(cut, cut off) = 0.183 from which we infer directly that, relative to the sense of cut, cutout is a clearer case of a simple decomposable MWE than cutoff .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Semantic neighbours of fire with different parts-of-speech. The scores are cosine similarities", "labels": [], "entities": []}, {"text": " Table 2: Semantic neighbours of the verbs cut, cut out, and cut off .", "labels": [], "entities": []}, {"text": " Table 3: Correlation between LSA and WordNet  similarities", "labels": [], "entities": [{"text": "WordNet", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.9474937915802002}]}]}