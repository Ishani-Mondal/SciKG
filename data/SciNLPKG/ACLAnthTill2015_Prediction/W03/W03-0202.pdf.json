{"title": [{"text": "Learning to Identify Student Preconceptions from Text", "labels": [], "entities": [{"text": "Learning to Identify Student Preconceptions from Text", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.7415866724082402}]}], "abstractContent": [{"text": "Automatic classification of short textual answers by students to questions about topics in physics, computing, etc., is an attractive approach to diagnostic assessment of learning.", "labels": [], "entities": [{"text": "Automatic classification of short textual answers by students to questions about topics in physics", "start_pos": 0, "end_pos": 98, "type": "TASK", "confidence": 0.7994179895945958}]}, {"text": "We present a language for expressing rules that can classify text based on the presence and relative positions of words, lists of synonyms and other abstractions of a single word.", "labels": [], "entities": [{"text": "classify text based on the presence and relative positions of words, lists of synonyms and other abstractions of a single word", "start_pos": 52, "end_pos": 178, "type": "Description", "confidence": 0.7432276782664385}]}, {"text": "We also describe a system, based on Mitchell's version spaces algorithm, that learns rules in this language.", "labels": [], "entities": []}, {"text": "These rules can be used to categorize student responses to short-answer questions.", "labels": [], "entities": []}, {"text": "The system is trained on written responses captured by an online assessment system that poses multiple choice questions and asks the student to justify their answers with textual explanations of their reasoning.", "labels": [], "entities": []}, {"text": "Several experiments are described that examine the effects of the use of negative data and tagging students explanations with their answer to the original multiple choice question.", "labels": [], "entities": []}], "introductionContent": [{"text": "We are building INFACT, a software system to support teachers in performing diagnostic assessment of their students' learning.", "labels": [], "entities": [{"text": "INFACT", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.5602211952209473}]}, {"text": "Our work is guided by the principle that assessment should be a ubiquitous and unobtrusive part of the learning process.", "labels": [], "entities": []}, {"text": "Since many learning experiences involve writing, we focus on the analysis of free natural language text and certain other representations of student expression and behavior.", "labels": [], "entities": []}, {"text": "We also believe that rich assessment, which informs teachers about the belief states of their students, is a valuable addition to tests with a single numeric grade.", "labels": [], "entities": []}, {"text": "There are several parts to our system, including an online textual forum for class discussions, an annotation interface for teachers, and tools for displaying assessment data in various formats.", "labels": [], "entities": []}, {"text": "The philosophy behind the system is described in ().", "labels": [], "entities": []}, {"text": "The system facilitates small-group discussions which the teacher can monitor and intervene if there is an obvious impasse.", "labels": [], "entities": []}, {"text": "An astute teacher with enough time can follow the discussions closely and observe as students make conceptual transitions.", "labels": [], "entities": []}, {"text": "A major motivation for the work described in this paper is to find away to reduce the burden on teachers who want such diagnostic information but who cannot afford the time needed to follow each discussion closely.", "labels": [], "entities": []}, {"text": "Our system analyzes small selections of student writing, on the order of one or two sentences, and learns rules that can be used to identify common student preconceptions.", "labels": [], "entities": []}, {"text": "Our approach to partially-automated analysis uses text markup rules consisting of patterns in a \"rule language\" and classifications that maybe as general as \"may be of interest\" to \"suggests preconception P17.\"", "labels": [], "entities": []}, {"text": "In addition to learning text markup rules for identifying preconceptions in online discussions, we are also learning rules for assessing short textual answers in an online diagnostic testing environment.", "labels": [], "entities": []}, {"text": "This system poses questions to the student and uses the results to report student preconceptions to teachers and recommend resources to the student.", "labels": [], "entities": []}, {"text": "The system asks multiple choice or numeric content questions and then, based on the response asks a short-answer follow-up question allowing the student to explain their reasoning.", "labels": [], "entities": []}, {"text": "In this paper, we describe the results of applying our rule learning system to classifying the responses to these follow-up questions.", "labels": [], "entities": []}, {"text": "In the following sections we discuss other work on automated essay grading, we then describe the language with which rules are represented in our system, followed by a description of the version space learning technique and our specific adaptations to allow it to learn text classification rules.", "labels": [], "entities": [{"text": "text classification", "start_pos": 270, "end_pos": 289, "type": "TASK", "confidence": 0.6935250759124756}]}, {"text": "Finally, we describe the empirical results of our experiments with this technique.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use data from Diagnoser, a web-based system for diagnosing student preconceptions to test our rule learner.", "labels": [], "entities": []}, {"text": "This assessment system has two types of questions, domain-specific base questions, which can be multiple choice or numeric, and secondary follow-up questions, which can be multiple choice or free text.", "labels": [], "entities": []}, {"text": "The answers to the base questions are designed to correlate with common student preconceptions and the secondary questions are used to confirm the system's diagnosis.", "labels": [], "entities": []}, {"text": "The system includes a database of common preconceptions that has been developed over a period of years (Hunt and Minstrell, 1996).", "labels": [], "entities": []}, {"text": "The system primarily uses multiple choice follow-up questions, with just a handful of text-based ones.", "labels": [], "entities": []}, {"text": "The developers would like to use more textual questions, but don't currently do so due to alack of automatic analysis tools.", "labels": [], "entities": []}, {"text": "Our data consist of student answers to one of these short-answer questions.", "labels": [], "entities": []}, {"text": "The base question is shown in.", "labels": [], "entities": []}, {"text": "The follow-up question just asks the student to explain their reasoning.", "labels": [], "entities": []}, {"text": "We used the students' answers to the base question to classify the responses into three categories, one for each of the three possible answers to the base question.", "labels": [], "entities": []}, {"text": "According to the system documentation, the first answer is predictive of students who fail to distinguish position and speed (Ppos-speed).", "labels": [], "entities": [{"text": "Ppos-speed", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9694199562072754}]}, {"text": "Presumably, these students reported that the motion represented by the top line had a higher speed because the line is physically higher.", "labels": [], "entities": []}, {"text": "The second answer indicates that students haven't understood the notion of average speed and are just reporting a comparison of the final speeds.", "labels": [], "entities": [{"text": "average speed", "start_pos": 75, "end_pos": 88, "type": "METRIC", "confidence": 0.8708185255527496}]}, {"text": "The third answer corresponds to the correct analysis of the question (Pcorrect).", "labels": [], "entities": [{"text": "Pcorrect", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9457389712333679}]}, {"text": "Both objects travel the same total distance in the same time, and neither ever moves backwards, so they have the same average speed.", "labels": [], "entities": []}, {"text": "We analyzed the text of responses to confirm that the students' descriptions of their reasoning matched the preconception predicted by system based on their multiple choice answer.", "labels": [], "entities": []}, {"text": "We found that it was necessary to create two additional classes.", "labels": [], "entities": []}, {"text": "One class was added for students who wrote that they had guessed their answer or otherwise gave an irrelevant answer in the free text (Pmisc).", "labels": [], "entities": []}, {"text": "Another class corresponded to a preconception that wasn't explicitly being tested for but which was clearly indicated by some students' responses.", "labels": [], "entities": []}, {"text": "The explanations of several students who chose answer A indicate that they didn't confuse position and speed.", "labels": [], "entities": []}, {"text": "Instead, they tried to compute the average speed of each object, but ignored the initial conditions of the system, in which object A is already 3 units ahead of object B (Pinitial).", "labels": [], "entities": []}, {"text": "Thus simply relying on the multiple choice answers may lead to incorrect attribution of preconceptions to students.", "labels": [], "entities": []}, {"text": "Furthermore, although it is true that students who answered B tended to be confused about the notion of average speed, few of them specifically reported considering the final speeds.", "labels": [], "entities": [{"text": "average speed", "start_pos": 104, "end_pos": 117, "type": "METRIC", "confidence": 0.8839380145072937}]}, {"text": "Rather, many of them commented that object A's motion was smooth, while object B moved in fits and starts.", "labels": [], "entities": []}, {"text": "The system explictly predicts a confusion of average speed with final speed.", "labels": [], "entities": []}, {"text": "This shows that the vocabulary of the textual description of the preconception (e.g. \"final speed\") isn't necessarily a good indicator of the way student's will express their beliefs.", "labels": [], "entities": [{"text": "final speed", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.7432738542556763}]}, {"text": "There were 88 responses to the secondary question.", "labels": [], "entities": []}, {"text": "Based solely on the answers to the base question, there were 61 answers classified as Ppos-speed, 15 were Pfinal-avg and 12 were Pcorrect.", "labels": [], "entities": [{"text": "Ppos-speed", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9825853109359741}, {"text": "Pfinal-avg", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.9531033039093018}, {"text": "Pcorrect", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.8986414670944214}]}, {"text": "After our manual analysis, the breakdown was 43 Ppos-speed answers, 10 Pfinal-avg answers, 5 Pinitial answers, 9 Pcorrect answers and 21 Pmisc answers.", "labels": [], "entities": [{"text": "Ppos-speed", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9501984119415283}]}, {"text": "As a baseline for comparison with the performance of our learned rules, we computed precision, recall and F-score measures for simply labeling each textual response with the preconception predicted by the student's answer to the base question.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9995031356811523}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9984979629516602}, {"text": "F-score", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9978891015052795}]}, {"text": "Precision is correct positives over correct positives plus incorrect negatives (i.e. false positives).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9887185096740723}]}, {"text": "Recall is correct positives overall positives (correct + incorrect.)", "labels": [], "entities": [{"text": "Recall is correct positives overall positives", "start_pos": 0, "end_pos": 45, "type": "METRIC", "confidence": 0.8152550806601843}]}, {"text": "The F-score is 2*precision*recall/(precision+recall).", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9993433356285095}, {"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.998794674873352}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.7961603999137878}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9890034794807434}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.5491839051246643}]}, {"text": "These results are shown in table 1.", "labels": [], "entities": []}, {"text": "Note that each row of the table shows the breakdown of all 88 examples with respect to the classification of a particular preconception.", "labels": [], "entities": []}, {"text": "Thus each row represents the performance of a single binary classifier on the entire dataset.", "labels": [], "entities": []}, {"text": "The recall is always 1.000 or 0.000 because of the way the data are generated.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9995458722114563}]}, {"text": "The predictions implied by the students' answers to the base question are used and only when their explanation indicated otherwise are they reassigned to a different preconception class.", "labels": [], "entities": []}, {"text": "Thus for those classes that were contemplated by the creator of the base question, all positive examples were correctly labeled.", "labels": [], "entities": []}, {"text": "Conversely, for preconceptions that weren't included in the base question formulation, no positive examples are correctly identified.", "labels": [], "entities": []}, {"text": "Because we have very little data in some categoriesas few as five examples for one class and nine for another -we use a leave-one-out training and testing regime.", "labels": [], "entities": []}, {"text": "For each class, we construct a data set in which examples from that class are labeled positive and all other examples are labeled negative.", "labels": [], "entities": []}, {"text": "We then cycle through every example, training on all but that example and testing that example.", "labels": [], "entities": []}, {"text": "Since our goal is to identify answers that indicate a particular preconception, we're primarily concerned with true and false positives.", "labels": [], "entities": []}, {"text": "We report the number of examples correctly and incorrectly labeled as well as the number of examples that the version space was unable to classify.", "labels": [], "entities": []}, {"text": "Precision is calculated the same way, but recall is now calculated as correct positives over the sum of correct, incorrect and unclassified positive examples.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9861375689506531}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.99893718957901}]}, {"text": "Our initial results, shown in table 2, show that the algorithm is able to correctly label 48 of the 88 examples and mislabeled none.", "labels": [], "entities": []}, {"text": "While the precision of the algorithm is excellent, the recall needs improvement.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9994714856147766}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9996843338012695}]}, {"text": "The results also show that the behavior varies widely from one class to another.", "labels": [], "entities": []}, {"text": "Clearly, for some preconceptions, the algorithm isn't generalizing enough.", "labels": [], "entities": []}, {"text": "Examining the rules produced by the algorithm, we found that part of the problem is the existence of very similar answers in different classes.", "labels": [], "entities": []}, {"text": "In particular, the Pinitial class consists of answers where the student claimed that Object A had a higher average speed, but not because they confused position and speed, as the automated diagnostic system had inferred.", "labels": [], "entities": []}, {"text": "These students not only understood the difference between position and speed, but knew that the formula for speed was change in position over elapsed time, though they misapplied that formula due to a different misconception.", "labels": [], "entities": [{"text": "speed", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9576336145401001}, {"text": "speed", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9650697112083435}]}, {"text": "It was their explanations of their reasoning that led us to separate them into a different class.", "labels": [], "entities": []}, {"text": "However, those explanations are extremely similar to those students who knew the formula and applied it correctly.", "labels": [], "entities": []}, {"text": "Since the answers were very similar, any generalization in one class would likely be restricted by negative examples from the other class.", "labels": [], "entities": []}, {"text": "In order to test this hypothesis, we reran the trials for these two without including Pcorrect examples as negative evidence for Pinitial, and vice versa.", "labels": [], "entities": [{"text": "Pinitial", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.730091392993927}]}, {"text": "These results are shown in: Results from the first leave-one-out experiment.", "labels": [], "entities": []}, {"text": "The first three columns are the number of correctly and incorrectly classified and unclassifiable positive examples, the next three columns are the same for negative examples.", "labels": [], "entities": []}, {"text": "The final columns show the precision, recall and F-score of the system on the positive examples only.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9998249411582947}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9996418952941895}, {"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9994865655899048}]}, {"text": "to three, which, while not much in absolute terms, represents a recall of 60% with no reduction in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9996463060379028}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9992766976356506}]}, {"text": "The Pcorrect class had more limited gains, going from one to two correctly labeled examples, again with no loss of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9980756044387817}]}, {"text": "These improvements led us to ask whether negative examples were limiting generalization in other cases as well.", "labels": [], "entities": []}, {"text": "In order to test this, we ran the same leave-one-out experiment using only positive examples to test the recall of the rules we were producing and then used all the positive examples with no negative examples to learn a set of rules and tested those rules on all the negative examples.", "labels": [], "entities": [{"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9983265995979309}]}, {"text": "The results of this experiment are shown in table 4.", "labels": [], "entities": []}, {"text": "The performance of the algorithm has improved significantly.", "labels": [], "entities": []}, {"text": "The recall on positive examples for this trial is 89% and there are still no false positives.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9993914365768433}]}, {"text": "While these results are promising, we would like to be able to make use of negative examples in our system.", "labels": [], "entities": []}, {"text": "In the process of analyzing student response data by hand, we found that it was often helpful to look at the student's answer to the base question associated with a given follow-up question.", "labels": [], "entities": []}, {"text": "It seemed likely that this information would also be useful to the rule learner.", "labels": [], "entities": [{"text": "rule learner", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.8961167633533478}]}, {"text": "We added to each text response a pseudo-word indicating the student's base question response and reran the algorithm using negative examples.", "labels": [], "entities": []}, {"text": "We included Pcorrect data as negative examples for Pinitial and vice versa because our hope was that the use of the base response tags would allow the algorithm to create rules that wouldn't conflict with negative examples from another class because the examples would have different tags.", "labels": [], "entities": [{"text": "Pcorrect data", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.7920940816402435}]}, {"text": "The results are shown in table 5.", "labels": [], "entities": []}, {"text": "For most classes, the addition of the tags improved the performance over untagged data.", "labels": [], "entities": []}, {"text": "This is even true in Pinitial, where all the tags were wrong (since those data came from students whose base response indicated Ppos-speed.)", "labels": [], "entities": [{"text": "Ppos-speed", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9033443331718445}]}, {"text": "In this class, the addition of tags allowed the same number of positive answers to be identified as the removal of negative evidence from the Pcorrect class did, implying that the tags served to avoid the trap of generalization-quashing negative evidence.", "labels": [], "entities": []}, {"text": "However, in both these classes, the addition of tags led to some examples being incorrectly classified instead of just remaining unclassified.", "labels": [], "entities": []}, {"text": "The only class where tag data posed a problem was the Pmisc class.", "labels": [], "entities": []}, {"text": "This is not surprising as this class contains data with a variety of tags.", "labels": [], "entities": []}, {"text": "In some cases responses that were exactly the same (e.g. two students who wrote \"I guessed.\") were associated with different base question answers.", "labels": [], "entities": []}, {"text": "This meant the addition of different tags resulting in non-matching answers.", "labels": [], "entities": []}, {"text": "However, this doesn't pose a great problem for the system.", "labels": [], "entities": []}, {"text": "The Pmisc class is unusual in that it doesn't really correspond to a specific misconception and the examples in that class come from students responding to the base question in many   different ways.", "labels": [], "entities": []}, {"text": "Classes of this type are easy to spot and can easily be trained on untagged data.", "labels": [], "entities": []}, {"text": "This was, in fact, the class that did the best when trained on untagged data.", "labels": [], "entities": []}, {"text": "Had this been done, the total number of correctly classified positive examples would have been 66, fora recall of 75%.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9997311234474182}]}, {"text": "The use of tag data also increases the performance of the system on negative examples to over 99%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results obtained by only using the students' answer on the base question to label their short answer question.  The first two columns show the number of positive examples that are correctly and incorrectly labeled, the second two  columns show the number of negative examples that are correctly and incorrectly labeled.", "labels": [], "entities": []}, {"text": " Table 2: Results from the first leave-one-out experiment. The first three columns are the number of correctly and  incorrectly classified and unclassifiable positive examples, the next three columns are the same for negative examples.  The final columns show the precision, recall and F-score of the system on the positive examples only.", "labels": [], "entities": [{"text": "precision", "start_pos": 264, "end_pos": 273, "type": "METRIC", "confidence": 0.9997445940971375}, {"text": "recall", "start_pos": 275, "end_pos": 281, "type": "METRIC", "confidence": 0.9991355538368225}, {"text": "F-score", "start_pos": 286, "end_pos": 293, "type": "METRIC", "confidence": 0.9990936517715454}]}, {"text": " Table 3: Retest of Pcorrect and Pinitial without conflicting negative evidence. Totals are carried over and revised from  Table 2.", "labels": [], "entities": [{"text": "Retest", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9777572751045227}, {"text": "Pcorrect", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9537926316261292}, {"text": "Pinitial", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8614162802696228}]}, {"text": " Table 4: The learner is trained using only positive examples. Positive examples are tested with the leave-one-out  methodology. Negative examples are tested on rules learned with all positive examples.", "labels": [], "entities": []}, {"text": " Table 5: Leave-one-out trial using data tagged with the students' responses to the corresponding base question.", "labels": [], "entities": []}]}