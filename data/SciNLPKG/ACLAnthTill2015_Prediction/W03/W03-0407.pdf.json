{"title": [{"text": "Bootstrapping POS taggers using Unlabelled Data", "labels": [], "entities": [{"text": "Bootstrapping POS taggers", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6092068354288737}]}], "abstractContent": [{"text": "This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively retrained on each other's output.", "labels": [], "entities": [{"text": "booststrapping part-of-speech taggers", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.737660676240921}]}, {"text": "Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set.", "labels": [], "entities": []}, {"text": "We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature.", "labels": [], "entities": []}, {"text": "Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets.", "labels": [], "entities": [{"text": "tagging", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.971707284450531}]}, {"text": "Further results show that this form of co-training considerably out-performs self-training.", "labels": [], "entities": []}, {"text": "However, we find that simply retraining on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Co-training, and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation, named entity recognition), noun phrase bracketing) and statistical parsing.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.6640758415063223}, {"text": "named entity recognition", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.6273847917715708}, {"text": "noun phrase bracketing", "start_pos": 161, "end_pos": 183, "type": "TASK", "confidence": 0.7254116336504618}, {"text": "statistical parsing", "start_pos": 189, "end_pos": 208, "type": "TASK", "confidence": 0.8312708139419556}]}, {"text": "In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data.", "labels": [], "entities": []}, {"text": "Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example.", "labels": [], "entities": []}, {"text": "In this paper we take a different approach, based on theoretical work by and, in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers' agreement on unlabelled data.", "labels": [], "entities": [{"text": "POS taggers'", "start_pos": 189, "end_pos": 201, "type": "TASK", "confidence": 0.6912859380245209}]}, {"text": "We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger) and the maximum entropy C&C tagger.", "labels": [], "entities": []}, {"text": "There has been some previous work on boostrapping POS taggers (e.g., and), but to our knowledge no previous work on co-training POS taggers.", "labels": [], "entities": [{"text": "boostrapping POS taggers", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.7896438837051392}, {"text": "POS taggers", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.7938329875469208}]}, {"text": "The idea behind co-training the POS taggers is very simple: use output from the TNT tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.7462812066078186}]}, {"text": "Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set.", "labels": [], "entities": []}, {"text": "The additional data should be accurate, but also useful, providing the tagger with new information.", "labels": [], "entities": []}, {"text": "Our work differs from the formulation of co-training by using two different learning algorithms rather than two independent feature sets).", "labels": [], "entities": []}, {"text": "Our results show that, when using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 187, "end_pos": 197, "type": "TASK", "confidence": 0.9569393396377563}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.874826192855835}]}, {"text": "We also show that simply re-training on all of the newly labelled data is surprisingly effective, with performance depending on the amount of newly labelled data added at each iteration.", "labels": [], "entities": []}, {"text": "For certain sizes of newly labelled data, this simple approach is just as effective as the agreement-based method.", "labels": [], "entities": []}, {"text": "We also show that co-training can still benefit both taggers when the performance of one tagger is initially much better than the other.", "labels": [], "entities": []}, {"text": "We have also investigated whether co-training can improve the taggers already trained on large amounts of manually annotated data.", "labels": [], "entities": []}, {"text": "Using standard sections of the WSJ Penn Treebank as seed data, we have been unable to improve the performance of the taggers using selftraining or co-training.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9128424525260925}]}, {"text": "Manually tagged data for English exists in large quantities, which means that there is no need to create taggers from small amounts of labelled material.", "labels": [], "entities": []}, {"text": "However, our experiments are relevant for languages for which there is little or no annotated data.", "labels": [], "entities": []}, {"text": "We only perform the experiments in English for convenience.", "labels": [], "entities": []}, {"text": "Our experiments can also be seen as a vehicle for exploring aspects of cotraining.", "labels": [], "entities": []}], "datasetContent": [{"text": "The co-training framework uses labelled examples from one tagger as additional training data for the other.", "labels": [], "entities": []}, {"text": "For the purposes of this paper, a labelled example is a tagged sentence.", "labels": [], "entities": []}, {"text": "We chose complete sentences, rather than smaller units, because this simplifies the experiments and the publicly available version of TNT requires complete tagged sentences for training.", "labels": [], "entities": [{"text": "TNT", "start_pos": 134, "end_pos": 137, "type": "DATASET", "confidence": 0.9327875375747681}]}, {"text": "It is possible that cotraining with sub-sentential units might be more effective, but we leave this as future work.", "labels": [], "entities": []}, {"text": "The co-training process is given in.", "labels": [], "entities": []}, {"text": "At each stage in the process there is a cache of unlabelled sentences (selected from the total pool of unlabelled sentences) which is labelled by each tagger.", "labels": [], "entities": []}, {"text": "The cache size could be increased at each iteration, which is a common practice in the co-training literature.", "labels": [], "entities": []}, {"text": "A subset of those sentences labelled by TNT is then added to the training data for C&C, and vice versa.", "labels": [], "entities": []}, {"text": "use the combined set of newly labelled examples for training each view, but we follow Goldman and Zhou (2000) in using separate labelled sets.", "labels": [], "entities": []}, {"text": "In the remainder of this section we consider two possible methods for selecting a subset.", "labels": [], "entities": []}, {"text": "The cache is cleared after each iteration.", "labels": [], "entities": []}, {"text": "There are various ways to select the labelled examples for each tagger.", "labels": [], "entities": []}, {"text": "A typical approach is to select those examples assigned a high score by the relevant classifier, under the assumption that these examples will be the most reliable.", "labels": [], "entities": []}, {"text": "A score-based selection method is difficult to apply in our experiments, however, since TNT does not provide scores for tagged sentences.", "labels": [], "entities": []}, {"text": "We therefore tried two alternative selection methods.", "labels": [], "entities": []}, {"text": "The first is to simply add all of the cache labelled by one tagger to the training data of the other.", "labels": [], "entities": []}, {"text": "We refer to this method as naive co-training.", "labels": [], "entities": []}, {"text": "The second, more sophisticated, method is to select that subset of the labelled cache which maximises the agreement of the two taggers on unlabelled data.", "labels": [], "entities": []}, {"text": "We call this method agreement-based cotraining.", "labels": [], "entities": []}, {"text": "For a large cache the number of possible subsets makes exhaustive search intractable, and so we randomly sample the subsets.", "labels": [], "entities": []}, {"text": "S is a seed set of labelled sentences L T is labelled training data for TNT LC is labelled training data for C&C U is a large set of unlabelled sentences C is a cache holding a small subset of U initialise:  The pseudo-code for the agreement-based selection method is given in.", "labels": [], "entities": [{"text": "TNT", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9577106833457947}]}, {"text": "The current tagger is the one being retrained, while the other tagger is kept static.", "labels": [], "entities": []}, {"text": "The co-training process uses the selection method for selecting sentences from the cache (which has been labelled by one of the taggers).", "labels": [], "entities": []}, {"text": "Note that during the selection process, we repeatedly sample from all possible subsets of the cache; this is done by first randomly choosing the size of the subset and then randomly choosing sentences based on the size.", "labels": [], "entities": []}, {"text": "The number of subsets we consider is determined by the number of times the loop is traversed in.", "labels": [], "entities": []}, {"text": "If TNT is being trained on the output of C&C, then the most recent version of C&C is used to measure agreement (and vice versa); so we first attempt to improve one tagger, then the other, rather than both at the same time.", "labels": [], "entities": []}, {"text": "The agreement rate of the taggers on unlabelled sentences is the per-token agreement rate; that is, the number of times each word in the unlabelled set of sentences is assigned the same tag by both taggers.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9531887471675873}]}, {"text": "For the small seed set experiments, the seed data was an arbitrarily chosen subset of sections 10-19 of the WSJ Penn Treebank; the unlabelled training data was taken from 50, 000 sentences of the 1994 WSJ section of the North American News Corpus (NANC); and the unlabelled data used to measure agreement was around 10, 000 sentences from sections 1-5 of the Treebank.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 108, "end_pos": 125, "type": "DATASET", "confidence": 0.8867650628089905}, {"text": "WSJ section of the North American News Corpus (NANC)", "start_pos": 201, "end_pos": 253, "type": "DATASET", "confidence": 0.9482886411926963}, {"text": "the Treebank", "start_pos": 355, "end_pos": 367, "type": "DATASET", "confidence": 0.822546511888504}]}, {"text": "Section 00 of the Treebank was used to measure the accuracy of the taggers.", "labels": [], "entities": [{"text": "the Treebank", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.7613110244274139}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9995287656784058}]}, {"text": "The cache size was 500 sentences.", "labels": [], "entities": []}, {"text": "shows the results for self-training, in which each tagger is simply retrained on its own labelled cache at each round.", "labels": [], "entities": []}, {"text": "(By round we mean the re-training of a single tagger, so there are two rounds per co-training iteration.)", "labels": [], "entities": []}, {"text": "TNT does improve using self-training, from 81.4% to 82.2%, but C&C is unaffected.", "labels": [], "entities": [{"text": "TNT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7998855710029602}, {"text": "C&C", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.6800756454467773}]}, {"text": "Re-running these experiments using a range of unlabelled training sets, from a variety of sources, showed similar behaviour.", "labels": [], "entities": []}, {"text": "gives the results for the greedy agreement cotraining, using a cache size of 500 and searching through 100 subsets of the labelled cache to find the one that maximises agreement.", "labels": [], "entities": []}, {"text": "Co-training improves the performance of both taggers: TNT improves from 81.4% to 84.9%, and C&C improves from 73.2% to 84.3% (an error reduction of over 40%).", "labels": [], "entities": [{"text": "TNT", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.7927668690681458}, {"text": "error reduction", "start_pos": 129, "end_pos": 144, "type": "METRIC", "confidence": 0.9701263010501862}]}, {"text": "show the self-training results and agreement-based results when a larger seed set, of 500 sentences, is used for each tagger.", "labels": [], "entities": []}, {"text": "In this case, selftraining harms TNT and C&C is again unaffected.", "labels": [], "entities": [{"text": "TNT", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.6273959875106812}, {"text": "C&C", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.7953638831774393}]}, {"text": "Cotraining continues to be beneficial.", "labels": [], "entities": []}, {"text": "shows how the size of the labelled data set (the number of sentences) grows for each tagger per round.", "labels": [], "entities": []}, {"text": "Towards the end of the co-training run, more material is being selected for C&C than TNT.", "labels": [], "entities": [{"text": "C&C", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9225951433181763}, {"text": "TNT", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9332994818687439}]}, {"text": "The experiments using a seed set size of 50 showed a similar trend, but the difference between the two taggers was less marked.", "labels": [], "entities": []}, {"text": "By examining the subsets chosen from the labelled cache at each round, we also observed that a large proportion of the cache was being selected for both taggers.", "labels": [], "entities": []}, {"text": "We also performed a number of experiments using much more unlabelled training material than before.", "labels": [], "entities": []}, {"text": "Instead of using 50, 000 sentences from the 1994 WSJ section of the North American News Corpus, we used 417, 000 sentences (from the same section) and ran the experiments until the unlabelled data had been exhausted.", "labels": [], "entities": [{"text": "WSJ section of the North American News Corpus", "start_pos": 49, "end_pos": 94, "type": "DATASET", "confidence": 0.9511115476489067}]}, {"text": "One experiment used naive co-training, with 50 seed sentences and a cache of size 500.", "labels": [], "entities": []}, {"text": "This led to an agreement rate of 99%, with performance levels of 85.4% and 85.4% for TNT and C&C respectively.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 15, "end_pos": 29, "type": "METRIC", "confidence": 0.9652351140975952}, {"text": "TNT and C&C", "start_pos": 85, "end_pos": 96, "type": "DATASET", "confidence": 0.7533237636089325}]}, {"text": "230, 000 sentences (\u2248 5 million words) had been processed and were used as training material by the taggers.", "labels": [], "entities": []}, {"text": "The other experiment used our agreement-based co-training approach (50 seed sentences, cache size of 1, 000 sentences, exploring at most 10 subsets in the maximisation process per round).", "labels": [], "entities": []}, {"text": "The agreement rate was 98%, with performance levels of 86.0% and 85.9% for both taggers.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9399314820766449}]}, {"text": "124, 000 sentences had been processed, of which 30, 000 labelled sentences were selected for training TNT and 44, 000 labelled sentences were selected for training C&C.", "labels": [], "entities": [{"text": "C&C", "start_pos": 164, "end_pos": 167, "type": "DATASET", "confidence": 0.8446124394734701}]}, {"text": "Co-training using this much larger amount of unlabelled material did improve our previously mentioned results, but not by a large margin.", "labels": [], "entities": []}, {"text": "Although bootstrapping from unlabelled data is particularly valuable when only small amounts of training material are available, it is also interesting to see if selftraining or co-training can improve state of the art POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 219, "end_pos": 230, "type": "TASK", "confidence": 0.9362269043922424}]}, {"text": "For these experiments, both C&C and TNT were initially trained on sections 00-18 of the WSJ Penn Treebank, and sections 19-21 and 22-24 were used as the development and test sets.", "labels": [], "entities": [{"text": "TNT", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8266600966453552}, {"text": "WSJ Penn Treebank", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.8812602361043295}]}, {"text": "The 1994-1996 WSJ text from the NANC was used as unlabelled material to fill the cache.", "labels": [], "entities": [{"text": "WSJ text from the NANC", "start_pos": 14, "end_pos": 36, "type": "DATASET", "confidence": 0.9014172792434693}]}, {"text": "The cache size started out at 8000 sentences and increased by 10% in each round to match the increasing labelled training data.", "labels": [], "entities": []}, {"text": "In each round of self-training or naive co-training 10% of the cache was randomly selected and added to the labelled training data.", "labels": [], "entities": []}, {"text": "The experiments ran for 40 rounds.", "labels": [], "entities": []}, {"text": "The performance of the different training regimes is listed in: Performance with large seed sets improvement for C&C 1 while naive co-training performance is always worse.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tagger performance for different seed sets", "labels": [], "entities": []}, {"text": " Table 2: Naive co-training accuracy results when varying  the amount added after each round (50 seed sentences)", "labels": [], "entities": [{"text": "Naive co-training", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8764482140541077}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9546548128128052}]}, {"text": " Table 3: Naive co-training accuracy results when varying  the amount added after each round (500 seed sentences)", "labels": [], "entities": [{"text": "Naive co-training", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8749603629112244}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9570070505142212}]}, {"text": " Table 4: Co-training Results for Imbalanced Views", "labels": [], "entities": []}, {"text": " Table 5. These results show no significant im- provement using either self-training or co-training with  very large seed datasets. Self-training shows only a slight", "labels": [], "entities": []}]}